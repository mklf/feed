<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-25T01:30:00Z">05-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Ignore Adversarial Attacks. (arXiv:2205.11551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11551">
<div class="article-summary-box-inner">
<span><p>Despite the strong performance of current NLP models, they can be brittle
against adversarial attacks. To enable effective learning against adversarial
inputs, we introduce the use of rationale models that can explicitly learn to
ignore attack tokens. We find that the rationale models can successfully ignore
over 90\% of attack tokens. This approach leads to consistent sizable
improvements ($\sim$10\%) over baseline models in robustness on three datasets
for both BERT and RoBERTa, and also reliably outperforms data augmentation with
adversarial examples alone. In many cases, we find that our method is able to
close the gap between model performance on a clean test set and an attacked
test set and hence reduce the effect of adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Recurrence Improves Masked Language Models. (arXiv:2205.11588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11588">
<div class="article-summary-box-inner">
<span><p>In this work, we explore whether modeling recurrence into the Transformer
architecture can both be beneficial and efficient, by building an extremely
simple recurrent module into the Transformer. We compare our model to baselines
following the training and evaluation recipe of BERT. Our results confirm that
recurrence can indeed improve Transformer models by a consistent margin,
without requiring low-level performance optimizations, and while keeping the
number of parameters constant. For example, our base model achieves an absolute
improvement of 2.1 points averaged across 10 tasks and also demonstrates
increased stability in fine-tuning over a range of learning rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Measuring Bias via Open-Ended Language Generation. (arXiv:2205.11601v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11601">
<div class="article-summary-box-inner">
<span><p>Researchers have devised numerous ways to quantify social biases vested in
pretrained language models. As some language models are capable of generating
coherent completions given a set of textual prompts, several prompting datasets
have been proposed to measure biases between social groups -- posing language
generation as a way of identifying biases. In this opinion paper, we analyze
how specific choices of prompt sets, metrics, automatic tools and sampling
strategies affect bias results. We find out that the practice of measuring
biases through text completion is prone to yielding contradicting results under
different experiment settings. We additionally provide recommendations for
reporting biases in open-ended language generation for a more complete outlook
of biases exhibited by a given language model. Code to reproduce the results is
released under https://github.com/feyzaakyurek/bias-textgen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeded Hierarchical Clustering for Expert-Crafted Taxonomies. (arXiv:2205.11602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11602">
<div class="article-summary-box-inner">
<span><p>Practitioners from many disciplines (e.g., political science) use
expert-crafted taxonomies to make sense of large, unlabeled corpora. In this
work, we study Seeded Hierarchical Clustering (SHC): the task of automatically
fitting unlabeled data to such taxonomies using only a small set of labeled
examples. We propose HierSeed, a novel weakly supervised algorithm for this
task that uses only a small set of labeled seed examples. It is both data and
computationally efficient. HierSeed assigns documents to topics by weighing
document density against topic hierarchical structure. It outperforms both
unsupervised and supervised baselines for the SHC task on three real-world
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving language models fine-tuning with representation consistency targets. (arXiv:2205.11603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11603">
<div class="article-summary-box-inner">
<span><p>Fine-tuning contextualized representations learned by pre-trained language
models has become a standard practice in the NLP field. However, pre-trained
representations are prone to degradation (also known as representation
collapse) during fine-tuning, which leads to instability, suboptimal
performance, and weak generalization. In this paper, we propose a novel
fine-tuning method that avoids representation collapse during fine-tuning by
discouraging undesirable changes in the representations. We show that our
approach matches or exceeds the performance of the existing
regularization-based fine-tuning methods across 13 language understanding tasks
(GLUE benchmark and six additional datasets). We also demonstrate its
effectiveness in low-data settings and robustness to label perturbation.
Furthermore, we extend previous studies of representation collapse and propose
several metrics to quantify it. Using these metrics and previously proposed
experiments, we show that our approach obtains significant improvements in
retaining the expressive power of representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Measuring Social Biases in Prompt-Based Multi-Task Learning. (arXiv:2205.11605v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11605">
<div class="article-summary-box-inner">
<span><p>Large language models trained on a mixture of NLP tasks that are converted
into a text-to-text format using prompts, can generalize into novel forms of
language and handle novel tasks. A large body of work within prompt engineering
attempts to understand the effects of input forms and prompts in achieving
superior performance. We consider an alternative measure and inquire whether
the way in which an input is encoded affects social biases promoted in outputs.
In this paper, we study T0, a large-scale multi-task text-to-text language
model trained using prompt-based learning. We consider two different forms of
semantically equivalent inputs: question-answer format and premise-hypothesis
format. We use an existing bias benchmark for the former BBQ and create the
first bias benchmark in natural language inference BBNLI with hand-written
hypotheses while also converting each benchmark into the other form. The
results on two benchmarks suggest that given two different formulations of
essentially the same input, T0 conspicuously acts more biased in question
answering form, which is seen during training, compared to premise-hypothesis
form which is unlike its training examples. Code and data are released under
https://github.com/feyzaakyurek/bbnli.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment. (arXiv:2205.11616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11616">
<div class="article-summary-box-inner">
<span><p>Word translation without parallel corpora has become feasible, rivaling the
performance of supervised methods. Recent findings have shown that the accuracy
and robustness of unsupervised word translation (UWT) can be improved by making
use of visual observations, which are universal representations across
languages. In this work, we investigate the potential of using not only visual
observations but also pretrained language-image models for enabling a more
efficient and robust UWT. Specifically, we develop a novel UWT method dubbed
Word Alignment using Language-Image Pretraining (WALIP), which leverages visual
observations via the shared embedding space of images and texts provided by
CLIP models (Radford et al., 2021). WALIP has a two-step procedure. First, we
retrieve word pairs with high confidences of similarity, computed using our
proposed image-based fingerprints, which define the initial pivot for the word
alignment. Second, we apply our robust Procrustes algorithm to estimate the
linear mapping between two embedding spaces, which iteratively corrects and
refines the estimated alignment. Our extensive experiments show that WALIP
improves upon the state-of-the-art performance of bilingual word alignment for
a few language pairs across different word embeddings and displays great
robustness to the dissimilarity of language pairs or training corpora for two
word embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer. (arXiv:2205.11631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11631">
<div class="article-summary-box-inner">
<span><p>In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
focused solely on source sentence tokens attributions. Therefore, we lack a
full understanding of the influences of every input token (source sentence and
target prefix) in the model predictions. In this work, we propose an
interpretability method that tracks complete input token attributions. Our
method, which can be extended to any encoder-decoder Transformer-based model,
allows us to better comprehend the inner workings of current NMT models. We
apply the proposed method to both bilingual and multilingual Transformers and
present insights into their behaviour.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Natural Language Processing Pipeline for Detecting Informal Data References in Academic Literature. (arXiv:2205.11651v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11651">
<div class="article-summary-box-inner">
<span><p>Discovering authoritative links between publications and the datasets that
they use can be a labor-intensive process. We introduce a natural language
processing pipeline that retrieves and reviews publications for informal
references to research datasets, which complements the work of data librarians.
We first describe the components of the pipeline and then apply it to expand an
authoritative bibliography linking thousands of social science studies to the
data-related publications in which they are used. The pipeline increases recall
for literature to review for inclusion in data-related collections of
publications and makes it possible to detect informal data references at scale.
We contribute (1) a novel Named Entity Recognition (NER) model that reliably
detects informal data references and (2) a dataset connecting items from social
science literature with datasets they reference. Together, these contributions
enable future work on data reference, data citation networks, and data reuse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexiBERT: Are Current Transformer Architectures too Homogeneous and Rigid?. (arXiv:2205.11656v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11656">
<div class="article-summary-box-inner">
<span><p>The existence of a plethora of language models makes the problem of selecting
the best one for a custom task challenging. Most state-of-the-art methods
leverage transformer-based models (e.g., BERT) or their variants. Training such
models and exploring their hyperparameter space, however, is computationally
expensive. Prior work proposes several neural architecture search (NAS) methods
that employ performance predictors (e.g., surrogate models) to address this
issue; however, analysis has been limited to homogeneous models that use fixed
dimensionality throughout the network. This leads to sub-optimal architectures.
To address this limitation, we propose a suite of heterogeneous and flexible
models, namely FlexiBERT, that have varied encoder layers with a diverse set of
possible operations and different hidden dimensions. For better-posed surrogate
modeling in this expanded design space, we propose a new graph-similarity-based
embedding scheme. We also propose a novel NAS policy, called BOSHNAS, that
leverages this new scheme, Bayesian modeling, and second-order optimization, to
quickly train and use a neural surrogate model to converge to the optimal
architecture. A comprehensive set of experiments shows that the proposed
policy, when applied to the FlexiBERT design space, pushes the performance
frontier upwards compared to traditional models. FlexiBERT-Mini, one of our
proposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9%
higher GLUE score. A FlexiBERT model with equivalent performance as the best
homogeneous model achieves 2.6x smaller size. FlexiBERT-Large, another proposed
model, achieves state-of-the-art results, outperforming the baseline models by
at least 5.7% on the GLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions. (arXiv:2205.11658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11658">
<div class="article-summary-box-inner">
<span><p>Generics express generalizations about the world (e.g., "birds can fly").
However, they are not universally true -- while sparrows and penguins are both
birds, only sparrows can fly and penguins cannot. Commonsense knowledge bases,
which are used extensively in many NLP tasks as a source of world-knowledge,
can often encode generic knowledge but, by-design, cannot encode such
exceptions. Therefore, it is crucial to realize the specific instances when a
generic statement is true or false. In this work, we present a novel framework
to generate pragmatically relevant true and false instances of a generic. We
use pre-trained language models, constraining the generation based on insights
from linguistic theory, and produce ${\sim}20k$ exemplars for ${\sim}650$
generics. Our system outperforms few-shot generation from GPT-3 (by 12.5
precision points) and our analysis highlights the importance of constrained
decoding for this task and the implications of generics exemplars for language
inference tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization. (arXiv:2205.11686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11686">
<div class="article-summary-box-inner">
<span><p>Integrating vision and language has gained notable attention following the
success of pretrained language models. Despite that, a fraction of emerging
multimodal models is suitable for text generation conditioned on images. This
minority is typically developed and evaluated for image captioning, a text
generation task conditioned solely on images with the goal to describe what is
explicitly visible in an image. In this paper, we take a step back and ask: How
do these models work for more complex generative tasks, conditioned on both
text and images? Are models based on joint multimodal pretraining, visually
adapted pretrained language models, or models that combine these two
approaches, more promising for such tasks? We address these questions in the
context of self-rationalization (jointly generating task labels/answers and
free-text explanations) of three tasks: (i) visual question answering in VQA-X,
(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment
in E-SNLI-VE. We show that recent advances in each modality, CLIP image
representations and scaling of language models, do not consistently improve
multimodal self-rationalization of tasks with multimodal inputs. We also
observe that no model type works universally the best across tasks/datasets and
finetuning data sizes. Our findings call for a backbone modelling approach that
can be built on to advance text generation from images and text beyond image
captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Workflow Discovery from Dialogues in the Low Data Regime. (arXiv:2205.11690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11690">
<div class="article-summary-box-inner">
<span><p>Text-based dialogues are now widely used to solve real-world problems. In
cases where solution strategies are already known, they can sometimes be
codified into workflows and used to guide humans or artificial agents through
the task of helping clients. We are interested in the situation where a formal
workflow may not yet exist, but we wish to discover the steps of actions that
have been taken to resolve problems. We examine a novel transformer-based
approach for this situation and we present experiments where we summarize
dialogues in the Action-Based Conversations Dataset (ABCD) with workflows.
Since the ABCD dialogues were generated using known workflows to guide agents
we can evaluate our ability to extract such workflows using ground truth
sequences of action steps, organized as workflows. We propose and evaluate an
approach that conditions models on the set of allowable action steps and we
show that using this strategy we can improve workflow discovery (WD)
performance. Our conditioning approach also improves zero-shot and few-shot WD
performance when transferring learned models to entirely new domains (i.e. the
MultiWOZ setting). Further, a modified variant of our architecture achieves
state-of-the-art performance on the related but different problems of Action
State Tracking (AST) and Cascading Dialogue Success (CDS) on the ABCD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Neural Open Information Extraction: Current Status and Future Directions. (arXiv:2205.11725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11725">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OpenIE) facilitates domain-independent discovery
of relational facts from large corpora. The technique well suits many
open-world natural language understanding scenarios, such as automatic
knowledge base construction, open-domain question answering, and explicit
reasoning. Thanks to the rapid development in deep learning technologies,
numerous neural OpenIE architectures have been proposed and achieve
considerable performance improvement. In this survey, we provide an extensive
overview of the-state-of-the-art neural OpenIE models, their key design
decisions, strengths and weakness. Then, we discuss limitations of current
solutions and the open issues in OpenIE problem itself. Finally we list recent
trends that could help expand its scope and applicability, setting up promising
directions for future research in OpenIE. To our best knowledge, this paper is
the first review on this specific topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Role of Bidirectionality in Language Model Pre-Training. (arXiv:2205.11726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11726">
<div class="article-summary-box-inner">
<span><p>Prior work on language model pre-training has explored different
architectures and learning objectives, but differences in data, hyperparameters
and evaluation make a principled comparison difficult. In this work, we focus
on bidirectionality as a key factor that differentiates existing approaches,
and present a comprehensive study of its role in next token prediction, text
infilling, zero-shot priming and fine-tuning. We propose a new framework that
generalizes prior approaches, including fully unidirectional models like GPT,
fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.
Our framework distinguishes between two notions of bidirectionality
(bidirectional context and bidirectional attention) and allows us to control
each of them separately. We find that the optimal configuration is largely
application-dependent (e.g., bidirectional attention is beneficial for
fine-tuning and infilling, but harmful for next token prediction and zero-shot
priming). We train models with up to 6.7B parameters, and find differences to
remain consistent at scale. While prior work on scaling has focused on
left-to-right autoregressive models, our results suggest that this approach
comes with some trade-offs, and it might be worthwhile to develop very large
bidirectional models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question Answering. (arXiv:2205.11729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11729">
<div class="article-summary-box-inner">
<span><p>Multi-hop question answering (QA) is a challenging task requiring QA systems
to perform complex reasoning over multiple documents and provide supporting
facts together with the exact answer. Existing works tend to utilize
graph-based reasoning and question decomposition to obtain the reasoning chain,
which inevitably introduces additional complexity and cumulative error to the
system. To address the above issue, we propose a simple yet effective novel
framework, From Easy to Hard (FE2H), to remove distracting information and
obtain better contextual representations for the multi-hop QA task. Inspired by
the iterative document selection process and the progressive learning custom of
humans, FE2H divides both the document selector and reader into two stages
following an easy-to-hard manner. Specifically, we first select the document
most relevant to the question and then utilize the question together with this
document to select other pertinent documents. As for the QA phase, our reader
is first trained on a single-hop QA dataset and then transferred into the
multi-hop QA task. We comprehensively evaluate our model on the popular
multi-hop QA benchmark HotpotQA. Experimental results demonstrate that our
method ourperforms all other methods in the leaderboard of HotpotQA (distractor
setting).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PERT: A New Solution to Pinyin to Character Conversion Task. (arXiv:2205.11737v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11737">
<div class="article-summary-box-inner">
<span><p>Pinyin to Character conversion (P2C) task is the key task of Input Method
Engine (IME) in commercial input software for Asian languages, such as Chinese,
Japanese, Thai language and so on. It's usually treated as sequence labelling
task and resolved by language model, i.e. n-gram or RNN. However, the low
capacity of the n-gram or RNN limits its performance. This paper introduces a
new solution named PERT which stands for bidirectional Pinyin Encoder
Representations from Transformers. It achieves significant improvement of
performance over baselines. Furthermore, we combine PERT with n-gram under a
Markov framework, and improve performance further. Lastly, the external lexicon
is incorporated into PERT so as to resolve the OOD issue of IME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BabyBear: Cheap inference triage for expensive language models. (arXiv:2205.11747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11747">
<div class="article-summary-box-inner">
<span><p>Transformer language models provide superior accuracy over previous models
but they are computationally and environmentally expensive. Borrowing the
concept of model cascading from computer vision, we introduce BabyBear, a
framework for cascading models for natural language processing (NLP) tasks to
minimize cost. The core strategy is inference triage, exiting early when the
least expensive model in the cascade achieves a sufficiently high-confidence
prediction. We test BabyBear on several open source data sets related to
document classification and entity recognition. We find that for common NLP
tasks a high proportion of the inference load can be accomplished with cheap,
fast models that have learned by observing a deep learning model. This allows
us to reduce the compute cost of large-scale classification jobs by more than
50% while retaining overall accuracy. For named entity recognition, we save 33%
of the deep learning compute while maintaining an F1 score higher than 95% on
the CoNLL benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models. (arXiv:2205.11758v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11758">
<div class="article-summary-box-inner">
<span><p>The emergent cross-lingual transfer seen in multilingual pretrained models
has sparked significant interest in studying their behavior. However, because
these analyses have focused on fully trained multilingual models, little is
known about the dynamics of the multilingual pretraining process. We
investigate when these models acquire their in-language and cross-lingual
abilities by probing checkpoints taken from throughout XLM-R pretraining, using
a suite of linguistic tasks. Our analysis shows that the model achieves high
in-language performance early on, with lower-level linguistic skills acquired
before more complex ones. In contrast, when the model learns to transfer
cross-lingually depends on the language pair. Interestingly, we also observe
that, across many languages and tasks, the final, converged model checkpoint
exhibits significant performance degradation and that no one checkpoint
performs best on all languages. Taken together with our other findings, these
insights highlight the complexity and interconnectedness of multilingual
pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat. (arXiv:2205.11764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11764">
<div class="article-summary-box-inner">
<span><p>In a depression-diagnosis-directed clinical session, doctors initiate a
conversation with ample emotional support that guides the patients to expose
their symptoms based on clinical diagnosis criteria. Such a dialog is a
combination of task-oriented and chitchat, different from traditional
single-purpose human-machine dialog systems. However, due to the social stigma
associated with mental illness, the dialogue data related to depression
consultation and diagnosis are rarely disclosed. Though automatic
dialogue-based diagnosis foresees great application potential, data sparsity
has become one of the major bottlenecks restricting research on such
task-oriented chat dialogues. Based on clinical depression diagnostic criteria
ICD-11 and DSM-5, we construct the D$^4$: a Chinese Dialogue Dataset for
Depression-Diagnosis-Oriented Chat which simulates the dialogue between doctors
and patients during the diagnosis of depression, including diagnosis results
and symptom summary given by professional psychiatrists for each
dialogue.Finally, we finetune on state-of-the-art pre-training models and
respectively present our dataset baselines on four tasks including response
generation, topic prediction, dialog summary, and severity classification of
depressive episode and suicide risk. Multi-scale evaluation results demonstrate
that a more empathy-driven and diagnostic-accurate consultation dialogue system
trained on our dataset can be achieved compared to rule-based bots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition. (arXiv:2205.11799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11799">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models has recently become a common practice
in building NLP models for various tasks, especially few-shot tasks. We argue
that under the few-shot setting, formulating fine-tuning closer to the
pre-training objectives shall be able to unleash more benefits from the
pre-trained language models. In this work, we take few-shot named entity
recognition (NER) for a pilot study, where existing fine-tuning strategies are
much different from pre-training. We propose a novel few-shot fine-tuning
framework for NER, FFF-NER. Specifically, we introduce three new types of
tokens, "is-entity", "which-type" and bracket, so we can formulate the NER
fine-tuning as (masked) token prediction or generation, depending on the choice
of pre-trained language models. In our experiments, we apply FFF-NER to
fine-tune both BERT and BART for few-shot NER on several benchmark datasets and
observe significant improvements over existing fine-tuning strategies,
including sequence labeling, prototype meta-learning, and prompt-based
approaches. We further perform a series of ablation studies, showing few-shot
NER performance is strongly correlated with the similarity between fine-tuning
and pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeDef: Weakly Supervised Backdoor Defense for Text Classification. (arXiv:2205.11803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11803">
<div class="article-summary-box-inner">
<span><p>Existing backdoor defense methods are only effective for limited trigger
types. To defend different trigger types at once, we start from the
class-irrelevant nature of the poisoning process and propose a novel weakly
supervised backdoor defense framework WeDef. Recent advances in weak
supervision make it possible to train a reasonably accurate text classifier
using only a small number of user-provided, class-indicative seed words. Such
seed words shall be considered independent of the triggers. Therefore, a weakly
supervised text classifier trained by only the poisoned documents without their
labels will likely have no backdoor. Inspired by this observation, in WeDef, we
define the reliability of samples based on whether the predictions of the weak
classifier agree with their labels in the poisoned training set. We further
improve the results through a two-phase sanitization: (1) iteratively refine
the weak classifier based on the reliable samples and (2) train a binary poison
classifier by distinguishing the most unreliable samples from the most reliable
samples. Finally, we train the sanitized model on the samples that the poison
classifier predicts as benign. Extensive experiments show that WeDefis
effective against popular trigger-based attacks (e.g., words, sentences, and
paraphrases), outperforming existing defense methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. (arXiv:2205.11822v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11822">
<div class="article-summary-box-inner">
<span><p>Despite their impressive capabilities, large pre-trained language models
(LMs) struggle with consistent reasoning; recently, prompting LMs to generate
explanations that self-guide the inference has emerged as a promising direction
to amend this. However, these approaches are fundamentally bounded by the
correctness of explanations, which themselves are often noisy and inconsistent.
In this work, we develop Maieutic Prompting, which infers a correct answer to a
question even from the noisy and inconsistent generations of LM. Maieutic
Prompting induces a tree of explanations abductively (e.g. X is true, because
...) and recursively, then frames the inference as a satisfiability problem
over these explanations and their logical relations. We test Maieutic Prompting
for true/false QA on three challenging benchmarks that require complex
commonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy
than state-of-the-art prompting methods, and as a fully unsupervised approach,
performs competitively with supervised models. We also show that Maieutic
Prompting improves robustness in inference while providing interpretable
rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lack of Fluency is Hurting Your Translation Model. (arXiv:2205.11826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11826">
<div class="article-summary-box-inner">
<span><p>Many machine translation models are trained on bilingual corpus, which
consist of aligned sentence pairs from two different languages with same
semantic. However, there is a qualitative discrepancy between train and test
set in bilingual corpus. While the most train sentences are created via
automatic techniques such as crawling and sentence-alignment methods, the test
sentences are annotated with the consideration of fluency by human. We suppose
this discrepancy in training corpus will yield performance drop of translation
model. In this work, we define \textit{fluency noise} to determine which parts
of train sentences cause them to seem unnatural. We show that \textit{fluency
noise} can be detected by simple gradient-based method with pre-trained
classifier. By removing \textit{fluency noise} in train sentences, our final
model outperforms the baseline on WMT-14 DE$\rightarrow$EN and
RU$\rightarrow$EN. We also show the compatibility with back-translation
augmentation, which has been commonly used to improve the fluency of the
translation model. At last, the qualitative analysis of \textit{fluency noise}
provides the insight of what points we should focus on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Lottery Tickets Boost Ensemble from a Single Pretrained Model. (arXiv:2205.11833v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11833">
<div class="article-summary-box-inner">
<span><p>Ensembling is a popular method used to improve performance as a last resort.
However, ensembling multiple models finetuned from a single pretrained model
has been not very effective; this could be due to the lack of diversity among
ensemble members. This paper proposes Multi-Ticket Ensemble, which finetunes
different subnetworks of a single pretrained model and ensembles them. We
empirically demonstrated that winning-ticket subnetworks produced more diverse
predictions than dense networks, and their ensemble outperformed the standard
ensemble on some tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Charon: a FrameNet Annotation Tool for Multimodal Corpora. (arXiv:2205.11836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11836">
<div class="article-summary-box-inner">
<span><p>This paper presents Charon, a web tool for annotating multimodal corpora with
FrameNet categories. Annotation can be made for corpora containing both static
images and video sequences paired - or not - with text sequences. The pipeline
features, besides the annotation interface, corpus import and pre-processing
tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lutma: a Frame-Making Tool for Collaborative FrameNet Development. (arXiv:2205.11840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11840">
<div class="article-summary-box-inner">
<span><p>This paper presents Lutma, a collaborative, semi-constrained, tutorial-based
tool for contributing frames and lexical units to the Global FrameNet
initiative. The tool parameterizes the process of frame creation, avoiding
consistency violations and promoting the integration of frames contributed by
the community with existing frames. Lutma is structured in a wizard-like
fashion so as to provide users with text and video tutorials relevant for each
step in the frame creation process. We argue that this tool will allow for a
sensible expansion of FrameNet coverage in terms of both languages and cultural
perspectives encoded by them, positioning frames as a viable alternative for
representing perspective in language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of STEM Science as Process, Method, Material, and Data Named Entities. (arXiv:2205.11863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11863">
<div class="article-summary-box-inner">
<span><p>We are faced with an unprecedented production in scholarly publications
worldwide. Stakeholders in the digital libraries posit that the document-based
publishing paradigm has reached the limits of adequacy. Instead, structured,
machine-interpretable, fine-grained scholarly knowledge publishing as Knowledge
Graphs (KG) is strongly advocated. In this work, we develop and analyze a
large-scale structured dataset of STEM articles across 10 different
disciplines, viz. Agriculture, Astronomy, Biology, Chemistry, Computer Science,
Earth Science, Engineering, Material Science, Mathematics, and Medicine. Our
analysis is defined over a large-scale corpus comprising 60K abstracts
structured as four scientific entities process, method, material, and data.
Thus our study presents, for the first-time, an analysis of a large-scale
multidisciplinary corpus under the construct of four named entity labels that
are specifically defined and selected to be domain-independent as opposed to
domain-specific. The work is then inadvertently a feasibility test of
characterizing multidisciplinary science with domain-independent concepts.
Further, to summarize the distinct facets of scientific knowledge per concept
per discipline, a set of word cloud visualizations are offered. The
STEM-NER-60k corpus, created in this work, comprises over 1M extracted entities
from 60k STEM articles obtained from a major publishing platform and is
publicly released https://github.com/jd-coderepos/stem-ner-60k.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Dialogue Corpus Annotated with Expressed and Experienced Emotions. (arXiv:2205.11867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11867">
<div class="article-summary-box-inner">
<span><p>In communication, a human would recognize the emotion of an interlocutor and
respond with an appropriate emotion, such as empathy and comfort. Toward
developing a dialogue system with such a human-like ability, we propose a
method to build a dialogue corpus annotated with two kinds of emotions. We
collect dialogues from Twitter and annotate each utterance with the emotion
that a speaker put into the utterance (expressed emotion) and the emotion that
a listener felt after listening to the utterance (experienced emotion). We
built a dialogue corpus in Japanese using this method, and its statistical
analysis revealed the differences between expressed and experienced emotions.
We conducted experiments on recognition of the two kinds of emotions. The
experimental results indicated the difficulty in recognizing experienced
emotions and the effectiveness of multi-task learning of the two kinds of
emotions. We hope that the constructed corpus will facilitate the study on
emotion recognition in a dialogue and emotion-aware dialogue response
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accuracy on In-Domain Samples Matters When Building Out-of-Domain detectors: A Reply to Marek et al. (2021). (arXiv:2205.11887v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11887">
<div class="article-summary-box-inner">
<span><p>We have noticed that Marek et al. (2021) try to re-implement our paper Zheng
et al. (2020a) in their work "OodGAN: Generative Adversarial Network for
Out-of-Domain Data Generation". Our paper proposes a model to generate pseudo
OOD samples that are akin to IN-Domain (IND) input utterances. These pseudo OOD
samples can be used to improve the OOD detection performance by optimizing an
entropy regularization term when building the IND classifier. Marek et al.
(2021) report a large gap between their re-implemented results and ours on the
CLINC150 dataset (Larson et al., 2019). This paper discusses some key
observations that may have led to such a large gap. Most of these observations
originate from our experiments because Marek et al. (2021) have not released
their codes1. One of the most important observations is that stronger IND
classifiers usually exhibit a more robust ability to detect OOD samples. We
hope these observations help other researchers, including Marek et al. (2021),
to develop better OOD detectors in their applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building an Effective Automated Assessment System for C/C++ Introductory Programming Courses in ODL Environment. (arXiv:2205.11915v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11915">
<div class="article-summary-box-inner">
<span><p>Assessments help in evaluating the knowledge gained by a learner at any
specific point as well as in continuous improvement of the curriculum design
and the whole learning process. However, with the increase in students'
enrollment at University level in either conventional or distance education
environment, traditional ways of assessing students' work are becoming
insufficient in terms of both time and effort. In distance education
environment, such assessments become additionally more challenging in terms of
hefty remuneration for hiring large number of tutors. The availability of
automated tools to assist the evaluation of students' work and providing
students with appropriate and timely feedback can really help in overcoming
these problems. We believe that building such tools for assessing students'
work for all kinds of courses in not yet possible. However, courses that
involve some formal language of expression can be automated, such as,
programming courses in Computer Science (CS) discipline. Instructors provide
various practical exercises to students as assignments to build these skills.
Usually, instructors manually grade and provide feedbacks on these assignments.
Although in literature, various tools have been reported to automate this
process, but most of these tools have been developed by the host institutions
themselves for their own use. We at COMSATS Institute of Information
Technology, Lahore are conducting a pioneer effort in Pakistan to automate the
marking of assignments of introductory programming courses that involve C or
C++ languages with the capability of associating appropriate feedbacks for
students. In this paper, we basically identify different components that we
believe are necessary in building an effective automated assessment system in
the context of introductory programming courses that involve C/C++ programming.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models are Zero-Shot Reasoners. (arXiv:2205.11916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11916">
<div class="article-summary-box-inner">
<span><p>Pretrained large language models (LLMs) are widely used in many sub-fields of
natural language processing (NLP) and generally known as excellent few-shot
learners with task-specific exemplars. Notably, chain of thought (CoT)
prompting, a recent technique for eliciting complex multi-step reasoning
through step-by-step answer examples, achieved the state-of-the-art
performances in arithmetics and symbolic reasoning, difficult system-2 tasks
that do not follow the standard scaling laws for LLMs. While these successes
are often attributed to LLMs' ability for few-shot learning, we show that LLMs
are decent zero-shot reasoners by simply adding ``Let's think step by step''
before each answer. Experimental results demonstrate that our Zero-shot-CoT,
using the same single prompt template, significantly outperforms zero-shot LLM
performances on diverse benchmark reasoning tasks including arithmetics
(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin
Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled
Objects), without any hand-crafted few-shot examples, e.g. increasing the
accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with
an off-the-shelf 175B parameter model. The versatility of this single prompt
across very diverse reasoning tasks hints at untapped and understudied
fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task
broad cognitive capabilities may be extracted through simple prompting. We hope
our work not only serves as the minimal strongest zero-shot baseline for the
challenging reasoning benchmarks, but also highlights the importance of
carefully exploring and analyzing the enormous zero-shot knowledge hidden
inside LLMs before crafting finetuning datasets or few-shot exemplars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Community Question Answering Entity Linking via Leveraging Auxiliary Data. (arXiv:2205.11917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11917">
<div class="article-summary-box-inner">
<span><p>Community Question Answering (CQA) platforms contain plenty of CQA texts
(i.e., questions and answers corresponding to the question) where named
entities appear ubiquitously. In this paper, we define a new task of CQA entity
linking (CQAEL) as linking the textual entity mentions detected from CQA texts
with their corresponding entities in a knowledge base. This task can facilitate
many downstream applications including expert finding and knowledge base
enrichment. Traditional entity linking methods mainly focus on linking entities
in news documents, and are suboptimal over this new task of CQAEL since they
cannot effectively leverage various informative auxiliary data involved in the
CQA platform to aid entity linking, such as parallel answers and two types of
meta-data (i.e., topic tags and users). To remedy this crucial issue, we
propose a novel transformer-based framework to effectively harness the
knowledge delivered by different kinds of auxiliary data to promote the linking
performance. We validate the superiority of our framework through extensive
experiments over a newly released CQAEL data set against state-of-the-art
entity linking methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Human is Human Evaluation? Improving the Gold Standard for NLG with Utility Theory. (arXiv:2205.11930v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11930">
<div class="article-summary-box-inner">
<span><p>Human ratings are treated as the gold standard in NLG evaluation. The
standard protocol is to collect ratings of generated text, average across
annotators, and then rank NLG systems by their average scores. However, little
consideration has been given as to whether this approach faithfully captures
human preferences. In this work, we analyze this standard protocol through the
lens of utility theory in economics. We first identify the implicit assumptions
it makes about annotators and find that these assumptions are often violated in
practice, in which case annotator ratings become an unfaithful reflection of
their preferences. The most egregious violations come from using Likert scales,
which provably reverse the direction of the true preference in certain cases.
We suggest improvements to the standard protocol to make it more theoretically
sound, but even in its improved form, it cannot be used to evaluate open-ended
tasks like story generation. For the latter, we propose a new evaluation
protocol called $\textit{system-level probabilistic assessment}$ (SPA). In our
experiments, we find that according to SPA, annotators prefer larger GPT-3
variants to smaller ones -- as expected -- with all comparisons being
statistically significant. In contrast, the standard protocol only yields
significant results half the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentional Mixtures of Soft Prompt Tuning for Parameter-efficient Multi-task Knowledge Sharing. (arXiv:2205.11961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11961">
<div class="article-summary-box-inner">
<span><p>This work introduces ATTEMPT (Attentional Mixture of Prompt Tuning), a new
modular, multi-task, and parameter-efficient language model (LM) tuning
approach that combines knowledge transferred across different tasks via a
mixture of soft prompts while keeping original LM unchanged. ATTEMPT
interpolates a set of prompts trained on large-scale source tasks and a newly
initialized target task prompt using instance-wise attention computed by a
lightweight sub-network trained on multiple target tasks. ATTEMPT is
parameter-efficient (e.g., updates 1,600 times fewer parameters than
fine-tuning) and enables multi-task learning and flexible extensions;
importantly, it is also more interpretable because it demonstrates which source
tasks affect the final model decision on target tasks. Experimental results
across 17 diverse datasets show that ATTEMPT improves prompt tuning by up to a
22% absolute performance gain and outperforms or matches fully fine-tuned or
other parameter-efficient tuning approaches that use over ten times more
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmark Data and Evaluation Framework for Intent Discovery Around COVID-19 Vaccine Hesitancy. (arXiv:2205.11966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11966">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has made a huge global impact and cost millions of
lives. As COVID-19 vaccines were rolled out, they were quickly met with
widespread hesitancy. To address the concerns of hesitant people, we launched
VIRA, a public dialogue system aimed at addressing questions and concerns
surrounding the COVID-19 vaccines. Here, we release VIRADialogs, a dataset of
over 8k dialogues conducted by actual users with VIRA, providing a unique
real-world conversational dataset. In light of rapid changes in users' intents,
due to updates in guidelines or as a response to new information, we highlight
the important task of intent discovery in this use-case. We introduce a novel
automatic evaluation framework for intent discovery, leveraging the existing
intent classifier of a given dialogue system. We use this framework to report
baseline intent-discovery results over VIRADialogs, that highlight the
difficulty of this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Dependency Treebank for Odia Language. (arXiv:2205.11976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11976">
<div class="article-summary-box-inner">
<span><p>This paper presents the first publicly available treebank of Odia, a
morphologically rich low resource Indian language. The treebank contains
approx. 1082 tokens (100 sentences) in Odia selected from "Samantar", the
largest available parallel corpora collection for Indic languages. All the
selected sentences are manually annotated following the ``Universal Dependency
(UD)" guidelines. The morphological analysis of the Odia treebank was performed
using machine learning techniques. The Odia annotated treebank will enrich the
Odia language resource and will help in building language technology tools for
cross-lingual learning and typological research. We also build a preliminary
Odia parser using a machine learning approach. The accuracy of the parser is
86.6% Tokenization, 64.1% UPOS, 63.78% XPOS, 42.04% UAS and 21.34% LAS.
Finally, the paper briefly discusses the linguistic analysis of the Odia UD
treebank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word-order typology in Multilingual BERT: A case study in subordinate-clause detection. (arXiv:2205.11987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11987">
<div class="article-summary-box-inner">
<span><p>The capabilities and limitations of BERT and similar models are still unclear
when it comes to learning syntactic abstractions, in particular across
languages. In this paper, we use the task of subordinate-clause detection
within and across languages to probe these properties. We show that this task
is deceptively simple, with easy gains offset by a long tail of harder cases,
and that BERT's zero-shot performance is dominated by word-order effects,
mirroring the SVO/VSO/SOV typology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition. (arXiv:2205.11998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11998">
<div class="article-summary-box-inner">
<span><p>The choice of modeling units affects the performance of the acoustic modeling
and plays an important role in automatic speech recognition (ASR). In mandarin
scenarios, the Chinese characters represent meaning but are not directly
related to the pronunciation. Thus only considering the writing of Chinese
characters as modeling units is insufficient to capture speech features. In
this paper, we present a novel method involves with multi-level modeling units,
which integrates multi-level information for mandarin speech recognition.
Specifically, the encoder block considers syllables as modeling units, and the
decoder block deals with character modeling units. During inference, the input
feature sequences are converted into syllable sequences by the encoder block
and then converted into Chinese characters by the decoder block. This process
is conducted by a unified end-to-end model without introducing additional
conversion models. By introducing InterCE auxiliary task, our method achieves
competitive results with CER of 4.1%/4.6% and 4.6%/5.2% on the widely used
AISHELL-1 benchmark without a language model, using the Conformer and the
Transformer backbones respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections. (arXiv:2205.12005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12005">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained foundation models have been an emerging paradigm for
building artificial intelligence (AI) systems, which can be quickly adapted to
a wide range of downstream tasks. This paper presents mPLUG, a new
vision-language foundation model for both cross-modal understanding and
generation. Most existing pre-trained models suffer from the problems of low
computational efficiency and information asymmetry brought by the long visual
sequence in cross-modal alignment. To address these problems, mPLUG introduces
an effective and efficient vision-language architecture with novel cross-modal
skip-connections, which creates inter-layer shortcuts that skip a certain
number of layers for time-consuming full self-attention on the vision side.
mPLUG is pre-trained end-to-end on large-scale image-text pairs with both
discriminative and generative objectives. It achieves state-of-the-art results
on a wide range of vision-language downstream tasks, such as image captioning,
image-text retrieval, visual grounding and visual question answering. mPLUG
also demonstrates strong zero-shot transferability when directly transferred to
multiple video-language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysing the Greek Parliament Records with Emotion Classification. (arXiv:2205.12012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12012">
<div class="article-summary-box-inner">
<span><p>In this project, we tackle emotion classification for the Greek language,
presenting and releasing a new dataset in Greek. We fine-tune and assess
Transformer-based masked language models that were pre-trained on monolingual
and multilingual resources, and we present the results per emotion and by
aggregating at the sentiment and subjectivity level. The potential of the
presented resources is investigated by detecting and studying the emotion of
`disgust' in the Greek Parliament records. We: (a) locate the months with the
highest values from 1989 to present, (b) rank the Greek political parties based
on the presence of this emotion in their speeches, and (c) study the emotional
context shift of words used to stigmatise people.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetroMAE: Pre-training Retrieval-oriented Transformers via Masked Auto-Encoder. (arXiv:2205.12035v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12035">
<div class="article-summary-box-inner">
<span><p>Pre-trained models have demonstrated superior power on many important tasks.
However, it is still an open problem of designing effective pre-training
strategies so as to promote the models' usability on dense retrieval. In this
paper, we propose a novel pre-training framework for dense retrieval based on
the Masked Auto-Encoder, known as RetroMAE. Our proposed framework is
highlighted for the following critical designs: 1) a MAE based pre-training
workflow, where the input sentence is polluted on both encoder and decoder side
with different masks, and original sentence is reconstructed based on both
sentence embedding and masked sentence; 2) asymmetric model architectures, with
a large-scale expressive transformer for sentence encoding and a extremely
simplified transformer for sentence reconstruction; 3) asymmetric masking
ratios, with a moderate masking on the encoder side (15%) and an aggressive
masking ratio on the decoder side (50~90%). We pre-train a BERT like encoder on
English Wikipedia and BookCorpus, where it notably outperforms the existing
pre-trained models on a wide range of dense retrieval benchmarks, like MS
MARCO, Open-domain Question Answering, and BEIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Phonological Parameters in Sign Languages. (arXiv:2205.12072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12072">
<div class="article-summary-box-inner">
<span><p>Signers compose sign language phonemes that enable communication by combining
phonological parameters such as handshape, orientation, location, movement, and
non-manual features. Linguistic research often breaks down signs into their
constituent parts to study sign languages and often a lot of effort is invested
into the annotation of the videos. In this work we show how a single model can
be used to recognise the individual phonological parameters within sign
languages with the aim of either to assist linguistic annotations or to
describe the signs for the sign recognition models. We use Danish Sign Language
data set `Ordbog over Dansk Tegnsprog' to generate multiple data sets using
pose estimation model, which are then used for training the multi-label Fast
R-CNN model to support multi-label modelling. Moreover, we show that there is a
significant co-dependence between the orientation and location phonological
parameters in the generated data and we incorporate this co-dependence in the
model to achieve better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphQ IR: Unifying Semantic Parsing of Graph Query Language with Intermediate Representation. (arXiv:2205.12078v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12078">
<div class="article-summary-box-inner">
<span><p>Subject to the semantic gap lying between natural and formal language, neural
semantic parsing is typically bottlenecked by the paucity and imbalance of
data. In this paper, we propose a unified intermediate representation (IR) for
graph query languages, namely GraphQ IR. With the IR's natural-language-like
representation that bridges the semantic gap and its formally defined syntax
that maintains the graph structure, neural semantic parser can more effectively
convert user queries into our GraphQ IR, which can be later automatically
compiled into different downstream graph query languages. Extensive experiments
show that our approach can consistently achieve state-of-the-art performance on
benchmarks KQA Pro, Overnight and MetaQA. Evaluations under compositional
generalization and few-shot learning settings also validate the promising
generalization ability of GraphQ IR with at most 11% accuracy improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12105">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of vision-language pre-training (VLP)
has brought cross-modal retrieval to a new era. However, due to the latency and
computation demand, it is commonly challenging to apply VLP in a real-time
online retrieval system. To alleviate the defect, this paper proposes a
\textbf{Hi}erarchical \textbf{V}ision-\textbf{}Language \textbf{P}re-Training
(\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a
novel hierarchical retrieval objective, which uses the representation of
different dimensions for coarse-to-fine ITR, i.e., using low-dimensional
representation for large-scale coarse retrieval and high-dimensional
representation for small-scale fine retrieval. We evaluate our proposed HiVLP
on two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.
Extensive experiments demonstrate that our HiVLP not only has fast inference
speed but also can be easily scaled to large-scale ITR scenarios. The detailed
results show that HiVLP is $1,427$$\sim$$120,649\times$ faster than the
fusion-based model UNITER and 2$\sim$5 faster than the fastest embedding-based
model LightingDot in different candidate scenarios. It also achieves about +4.9
AR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable
performance with the state-of-the-art (SOTA) fusion-based model METER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Curious Case of Control. (arXiv:2205.12113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12113">
<div class="article-summary-box-inner">
<span><p>Children acquiring English make systematic errors on subject control
sentences even after they have reached near-adult competence (C. Chomsky,
1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).
Given the advanced fluency of large generative language models, we ask whether
model outputs are consistent with these heuristics, and to what degree
different models are consistent with each other. We find that models can be
categorized by behavior into three separate groups, with broad differences
between the groups. The outputs of models in the largest group are consistent
with positional heuristics that succeed on subject control but fail on object
control. This result is surprising, given that object control is orders of
magnitude more frequent in the text data used to train such models. We examine
to what degree the models are sensitive to prompting with agent-patient
information, finding that raising the salience of agent and patient relations
results in significant changes in the outputs of most models. Based on this
observation, we leverage an existing dataset of semantic proto-role annotations
(White, et al. 2020) to explore the connections between control and labeling
event participants with properties typically associated with agents and
patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer. (arXiv:2205.12148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12148">
<div class="article-summary-box-inner">
<span><p>Massively multilingual models are promising for transfer learning across
tasks and languages. However, existing methods are unable to fully leverage
training data when it is available in different task-language combinations. To
exploit such heterogeneous supervision we propose Hyper-X, a unified
hypernetwork that generates weights for parameter-efficient adapter modules
conditioned on both tasks and language embeddings. By learning to combine task
and language-specific knowledge our model enables zero-shot transfer for unseen
languages and task-language combinations. Our experiments on a diverse set of
languages demonstrate that Hyper-X achieves the best gain when a mixture of
multiple resources is available while performing on par with strong baselines
in the standard scenario. Finally, Hyper-X consistently produces strong results
in few-shot scenarios for new languages and tasks showing the effectiveness of
our approach beyond zero-shot transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamic, Interpreted CheckList for Meaning-oriented NLG Metric Evaluation -- through the Lens of Semantic Similarity Rating. (arXiv:2205.12176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12176">
<div class="article-summary-box-inner">
<span><p>Evaluating the quality of generated text is difficult, since traditional NLG
evaluation metrics, focusing more on surface form than meaning, often fail to
assign appropriate scores. This is especially problematic for AMR-to-text
evaluation, given the abstract nature of AMR. Our work aims to support the
development and improvement of NLG evaluation metrics that focus on meaning, by
developing a dynamic CheckList for NLG metrics that is interpreted by being
organized around meaning-relevant linguistic phenomena. Each test instance
consists of a pair of sentences with their AMR graphs and a human-produced
textual semantic similarity or relatedness score. Our CheckList facilitates
comparative evaluation of metrics and reveals strengths and weaknesses of novel
and traditional metrics. We demonstrate the usefulness of CheckList by
designing a new metric GraCo that computes lexical cohesion graphs over AMR
concepts. Our analysis suggests that GraCo presents an interesting NLG metric
worth future investigation and that meaning-oriented NLG metrics can profit
from graph-based metric components using AMR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial-input baselines show that NLI models can ignore context, but they don't. (arXiv:2205.12181v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12181">
<div class="article-summary-box-inner">
<span><p>When strong partial-input baselines reveal artifacts in crowdsourced NLI
datasets, the performance of full-input models trained on such datasets is
often dismissed as reliance on spurious correlations. We investigate whether
state-of-the-art NLI models are capable of overriding default inferences made
by a partial-input baseline. We introduce an evaluation set of 600 examples
consisting of perturbed premises to examine a RoBERTa model's sensitivity to
edited contexts. Our results indicate that NLI models are still capable of
learning to condition on context--a necessary component of inferential
reasoning--despite being trained on artifact-ridden datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning for Expressive Task-Related Sentence Representations. (arXiv:2205.12186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12186">
<div class="article-summary-box-inner">
<span><p>NLP models learn sentence representations for downstream tasks by tuning a
model which is pre-trained by masked language modeling. However, after tuning,
the learned sentence representations may be skewed heavily toward label space
and thus are not expressive enough to represent whole samples, which should
contain task-related information of both sentence inputs and labels. In this
work, we learn expressive sentence representations for supervised tasks which
(1). contain task-related information in the sentence inputs, and (2). enable
correct label predictions. To achieve this goal, we first propose a new
objective which explicitly points out the label token space in the input, and
predicts categories of labels via an added [MASK] token. This objective
encourages fusing the semantic information of both the label and sentence. Then
we develop a neighbor attention module, added on a frozen pre-trained model, to
build connections between label/sentence tokens via their neighbors. The
propagation can be further guided by the regularization on neighborhood
representations to encourage expressiveness. Experimental results show that,
despite tuning only 5% additional parameters over a frozen pre-trained model,
our model can achieve classification results comparable to the SOTA while
maintaining strong expressiveness as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization. (arXiv:2205.12191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12191">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, we observe that these
models exhibit poor out-of-distribution (OOD) generalization on the task of
VQA. To better understand the underlying causes of poor generalization, we
comprehensively investigate performance of two pretrained V&amp;L models under
different settings (i.e. classification and open-ended text generation) by
conducting cross-dataset evaluations. We find that these models tend to learn
to solve the benchmark, rather than learning the high-level skills required by
the VQA task. We also argue that in most cases generative models are less
susceptible to shifts in data distribution, while frequently performing better
on our tested benchmarks. Moreover, we find that multimodal pretraining
improves OOD performance in most settings. Finally, we revisit assumptions
underlying the use of automatic VQA evaluation metrics, and empirically show
that their stringent nature repeatedly penalizes models for correct responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel's Weekly Video Podcasts. (arXiv:2205.12194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12194">
<div class="article-summary-box-inner">
<span><p>We introduce the Merkel Podcast Corpus, an audio-visual-text corpus in German
collected from 16 years of (almost) weekly Internet podcasts of former German
chancellor Angela Merkel. To the best of our knowledge, this is the first
single speaker corpus in the German language consisting of audio, visual and
text modalities of comparable size and temporal extent. We describe the methods
used with which we have collected and edited the data which involves
downloading the videos, transcripts and other metadata, forced alignment,
performing active speaker recognition and face detection to finally curate the
single speaker dataset consisting of utterances spoken by Angela Merkel. The
proposed pipeline is general and can be used to curate other datasets of
similar nature, such as talk show contents. Through various statistical
analyses and applications of the dataset in talking face generation and TTS, we
show the utility of the dataset. We argue that it is a valuable contribution to
the research community, in particular, due to its realistic and challenging
material at the boundary between prepared and spontaneous speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation. (arXiv:2205.12206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12206">
<div class="article-summary-box-inner">
<span><p>Formal verse poetry imposes strict constraints on the meter and rhyme scheme
of poems. Most prior work on generating this type of poetry uses existing poems
for supervision, which are difficult to obtain for most languages and poetic
forms. In this work, we propose an unsupervised approach to generate poems
following any given meter and rhyme scheme, without requiring any poetic text
for training. Our method works by splitting a regular, non-poetic corpus into
phrases, prepending control codes that describe the length and end rhyme of
each phrase, and training a transformer language model in the augmented corpus.
During inference, we build control codes for the desired meter and rhyme
scheme, and condition our language model on them to generate formal verse
poetry. Experiments in Spanish and Basque show that our approach is able to
generate valid poems, which are often comparable in quality to those written by
humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start. (arXiv:2205.12209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12209">
<div class="article-summary-box-inner">
<span><p>We present EdiT5 - a novel semi-autoregressive text-editing approach designed
to combine the strengths of non-autoregressive text-editing and autoregressive
decoding. EdiT5 is faster at inference times than conventional
sequence-to-sequence (seq2seq) models, while being capable of modeling flexible
input-output transformations.
</p>
<p>This is achieved by decomposing the generation process into three sub-tasks:
(1) tagging to decide on the subset of input tokens to be preserved in the
output, (2) re-ordering to define their order in the output text, and (3)
insertion to infill the missing tokens that are not present in the input. The
tagging and re-ordering steps, which are responsible for generating the largest
portion of the output, are non-autoregressive, while the insertion uses an
autoregressive decoder.
</p>
<p>Depending on the task, EdiT5 requires significantly fewer autoregressive
steps demonstrating speedups of up to 25x when compared to classic seq2seq
models. Quality-wise, EdiT5 is initialized with a pre-trained T5 checkpoint
yielding comparable performance to T5 in high-resource settings and clearly
outperforms it on low-resource settings when evaluated on three NLG tasks:
Sentence Fusion, Grammatical Error Correction, and Decontextualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Principled Paraphrase Generation with Parallel Corpora. (arXiv:2205.12213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12213">
<div class="article-summary-box-inner">
<span><p>Round-trip Machine Translation (MT) is a popular choice for paraphrase
generation, which leverages readily available parallel corpora for supervision.
In this paper, we formalize the implicit similarity function induced by this
approach, and show that it is susceptible to non-paraphrase pairs sharing a
single ambiguous translation. Based on these insights, we design an alternative
similarity metric that mitigates this issue by requiring the entire translation
distribution to match, and implement a relaxation of it through the Information
Bottleneck method. Our approach incorporates an adversarial term into MT
training in order to learn representations that encode as much information
about the reference translation as possible, while keeping as little
information about the input as possible. Paraphrases can be generated by
decoding back to the source from this representation, without having to
generate pivot translations. In addition to being more principled and efficient
than round-trip MT, our approach offers an adjustable parameter to control the
fidelity-diversity trade-off, and obtains better results in our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages. (arXiv:2205.12215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12215">
<div class="article-summary-box-inner">
<span><p>We introduce DivEMT, the first publicly available post-editing study of
Neural Machine Translation (NMT) over a typologically diverse set of target
languages. Using a strictly controlled setup, 18 professional translators were
instructed to translate or post-edit the same set of English documents into
Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process,
their edits, keystrokes, editing times, pauses, and perceived effort were
recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and its
post-editing process. Using this new dataset, we assess the impact on
translation productivity of two state-of-the-art NMT systems, namely: Google
Translate and the open-source multilingual model mBART50. We find that, while
post-editing is consistently faster than translation from scratch, the
magnitude of its contribution varies largely across systems and languages,
ranging from doubled productivity in Dutch and Italian to marginal gains in
Arabic, Turkish and Ukrainian, for some of the evaluated modalities. Moreover,
the observed cross-language variability appears to partly reflect source-target
relatedness and type of target morphology, while remaining hard to predict even
based on state-of-the-art automatic MT quality metrics. We publicly release the
complete dataset, including all collected behavioural data, to foster new
research on the ability of state-of-the-art NMT systems to generate text in
typologically diverse languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation. (arXiv:2205.12216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12216">
<div class="article-summary-box-inner">
<span><p>We present a new approach to perform zero-shot cross-modal transfer between
speech and text for translation tasks. Multilingual speech and text are encoded
in a joint fixed-size representation space. Then, we compare different
approaches to decode these multimodal and multilingual fixed-size
representations, enabling zero-shot translation between languages and
modalities. All our models are trained without the need of cross-modal labeled
translation data. Despite a fixed-size representation, we achieve very
competitive results on several text and speech translation tasks. In
particular, we significantly improve the state-of-the-art for zero-shot speech
translation on Must-C. Incorporating a speech decoder in our framework, we
introduce the first results for zero-shot direct speech-to-speech and
text-to-speech translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12219">
<div class="article-summary-box-inner">
<span><p>The ability to converse with humans and follow commands in natural language
is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can
relieve people's burden of holding a controller all the time, allow
multitasking, and make drone control more accessible for people with
disabilities or with their hands occupied. To this end, we introduce Aerial
Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language
conversation. We build a drone simulator with a continuous photorealistic
environment and collect a new AVDN dataset of over 3k recorded navigation
trajectories with asynchronous human-human dialogs between commanders and
followers. The commander provides initial navigation instruction and further
guidance by request, while the follower navigates the drone in the simulator
and asks questions when needed. During data collection, followers' attention on
the drone's visual observation is also recorded. Based on the AVDN dataset, we
study the tasks of aerial navigation from (full) dialog history and propose an
effective Human Attention Aided (HAA) baseline model, which learns to predict
both navigation waypoints and human attention. Dataset and code will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Fact Verification: Comparing and Contrasting Claims on Contentious Topics. (arXiv:2205.12221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12221">
<div class="article-summary-box-inner">
<span><p>As the importance of identifying misinformation is increasing, many
researchers focus on verifying textual claims on the web. One of the most
popular tasks to achieve this is fact verification, which retrieves an evidence
sentence from a large knowledge source such as Wikipedia to either verify or
refute each factual claim. However, while such problem formulation is helpful
for detecting false claims and fake news, it is not applicable to catching
subtle differences in factually consistent claims which still might implicitly
bias the readers, especially in contentious topics such as political, gender,
or racial issues. In this study, we propose ClaimDiff, a novel dataset to
compare the nuance between claim pairs in both a discriminative and a
generative manner, with the underlying assumption that one is not necessarily
more true than the other. This differs from existing fact verification datasets
that verify the target sentence with respect to an absolute truth. We hope this
task assists people in making more informed decisions among various sources of
media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems. (arXiv:2205.12228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12228">
<div class="article-summary-box-inner">
<span><p>In natural language understanding (NLU) production systems, users' evolving
needs necessitate the addition of new features over time, indexed by new
symbols added to the meaning representation space. This requires additional
training data and results in ever-growing datasets. We present the first
systematic investigation into this incremental symbol learning scenario. Our
analyses reveal a troubling quirk in building (broad-coverage) NLU systems: as
the training dataset grows, more data is needed to learn new symbols, forming a
vicious cycle. We show that this trend holds for multiple mainstream models on
two common NLU tasks: intent recognition and semantic parsing. Rejecting class
imbalance as the sole culprit, we reveal that the trend is closely associated
with an effect we call source signal dilution, where strong lexical cues for
the new symbol become diluted as the training dataset grows. Selectively
dropping training examples to prevent dilution often reverses the trend,
showing the over-reliance of mainstream neural NLU models on simple lexical
cues and their lack of contextual understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chunk-based Nearest Neighbor Machine Translation. (arXiv:2205.12230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12230">
<div class="article-summary-box-inner">
<span><p>Semi-parametric models, which augment generation with retrieval, have led to
impressive results in language modeling and machine translation, due to their
ability to leverage information retrieved from a datastore of examples. One of
the most prominent approaches, $k$NN-MT, has an outstanding performance on
domain adaptation by retrieving tokens from a domain-specific datastore
\citep{khandelwal2020nearest}. However, $k$NN-MT requires retrieval for every
single generated token, leading to a very low decoding speed (around 8 times
slower than a parametric model). In this paper, we introduce a
\textit{chunk-based} $k$NN-MT model which retrieves chunks of tokens from the
datastore, instead of a single token. We propose several strategies for
incorporating the retrieved chunks into the generation process, and for
selecting the steps at which the model needs to search for neighbors in the
datastore. Experiments on machine translation in two settings, static domain
adaptation and ``on-the-fly'' adaptation, show that the chunk-based $k$NN-MT
model leads to a significant speed-up (up to 4 times) with only a small drop in
translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VIRATrustData: A Trust-Annotated Corpus of Human-Chatbot Conversations About COVID-19 Vaccines. (arXiv:2205.12240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12240">
<div class="article-summary-box-inner">
<span><p>Public trust in medical information is crucial for successful application of
public health policies such as vaccine uptake. This is especially true when the
information is offered remotely, by chatbots, which have become increasingly
popular in recent years. Here, we explore the challenging task of human-bot
turn-level trust classification. We rely on a recently released data of
observationally-collected (rather than crowdsourced) dialogs with VIRA chatbot,
a COVID-19 Vaccine Information Resource Assistant. These dialogs are centered
around questions and concerns about COVID-19 vaccines, where trust is
particularly acute. We annotated $3k$ VIRA system-user conversational turns for
Low Institutional Trust or Low Agent Trust vs. Neutral or High Trust. We
release the labeled dataset, VIRATrustData, the first of its kind to the best
of our knowledge. We demonstrate how this task is non-trivial and compare
several models that predict the different levels of trust.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of Hierarchical Conversation Structure. (arXiv:2205.12244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12244">
<div class="article-summary-box-inner">
<span><p>Human conversations can evolve in many different ways, creating challenges
for automatic understanding and summarization. Goal-oriented conversations
often have meaningful sub-dialogue structure, but it can be highly
domain-dependent. This work introduces an unsupervised approach to learning
hierarchical conversation structure, including turn and sub-dialogue segment
labels, corresponding roughly to dialogue acts and sub-tasks, respectively. The
decoded structure is shown to be useful in enhancing neural models of language
for three conversation-level understanding tasks. Further, the learned
finite-state sub-dialogue network is made interpretable through automatic
summarization. Our code and trained models are available at
\url{https://github.com/boru-roylu/THETA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models. (arXiv:2205.12247v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12247">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that Pre-trained Language Models (PLMs) have the
ability to store the relational knowledge from pre-training data in their model
parameters. However, it is not clear up to what extent do PLMs store
geo-diverse commonsense knowledge, the knowledge associated with a culture and
only shared locally. For instance, the color of bridal dress is white in
American weddings whereas it is red in Chinese weddings. Here, we wish to probe
if PLMs can predict red and white as the color of the bridal dress when queried
for American and Chinese weddings, respectively. To this end, we introduce a
framework for geo-diverse commonsense probing on multilingual PLMs (mPLMs) and
introduce a corresponding benchmark Geo-diverse Commonsense Multilingual
Language Model Analysis (GeoMLAMA) dataset. GeoMLAMA contains 3125 prompts in
English, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts
shared by people from American, Chinese, Indian, Iranian and Kenyan cultures.
We benchmark 11 standard mPLMs which include variants of mBERT, XLM, mT5, and
XGLM on GeoMLAMA. Interestingly, we find that 1) larger mPLM variants do not
necessarily store geo-diverse concepts better than its smaller variant; 2)
mPLMs are not intrinsically biased towards knowledge from the Western countries
(the United States); 3) the native language of a country may not be the best
language to probe its knowledge and 4) a language may better probe knowledge
about a non-native country than its native country.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RevUp: Revise and Update Information Bottleneck for Event Representation. (arXiv:2205.12248v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12248">
<div class="article-summary-box-inner">
<span><p>In machine learning, latent variables play a key role to capture the
underlying structure of data, but they are often unsupervised. When we have
side knowledge that already has high-level information about the input data, we
can use that source to guide latent variables and capture the available
background information in a process called "parameter injection." In that
regard, we propose a semi-supervised information bottleneck-based model that
enables the use of side knowledge, even if it is noisy and imperfect, to direct
the learning of discrete latent variables. Fundamentally, we introduce an
auxiliary continuous latent variable as a way to reparameterize the model's
discrete variables with a light-weight hierarchical structure. With this
reparameterization, the model's discrete latent variables are learned to
minimize the mutual information between the observed data and optional side
knowledge that is not already captured by the new, auxiliary variables. We
theoretically show that our approach generalizes an existing method of
parameter injection, and perform an empirical case study of our approach on
language-based event modeling. We corroborate our theoretical results with
strong empirical experiments, showing that the proposed method outperforms
previous proposed approaches on multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing. (arXiv:2205.12253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12253">
<div class="article-summary-box-inner">
<span><p>Despite their strong performance on many tasks, pre-trained language models
have been shown to struggle on out-of-distribution compositional
generalization. Meanwhile, recent work has shown considerable improvements on
many NLP tasks from model scaling. Can scaling up model size also improve
compositional generalization in semantic parsing? We evaluate encoder-decoder
models up to 11B parameters and decoder-only models up to 540B parameters, and
compare model scaling curves for three different methods for transfer learning:
fine-tuning all parameters, prompt tuning, and in-context learning. We observe
that fine-tuning generally has flat or negative scaling curves on
out-of-distribution compositional generalization in semantic parsing
evaluations. In-context learning has positive scaling curves, but is generally
outperformed by much smaller fine-tuned models. Prompt-tuning can outperform
fine-tuning, suggesting further potential improvements from scaling as it
exhibits a more positive scaling curve. Additionally, we identify several error
trends that vary with model scale. For example, larger models are generally
better at modeling the syntax of the output space, but are also more prone to
certain types of overfitting. Overall, our study highlights limitations of
current techniques for effectively leveraging model scale for compositional
generalization, while our analysis also suggests promising directions for
future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretation Quality Score for Measuring the Quality of interpretability methods. (arXiv:2205.12254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12254">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) models have been applied to a wide range of natural
language processing (NLP) tasks in recent years. In addition to making accurate
decisions, the necessity of understanding how models make their decisions has
become apparent in many applications. To that end, many interpretability
methods that help explain the decision processes of ML models have been
developed. Yet, there currently exists no widely-accepted metric to evaluate
the quality of explanations generated by these methods. As a result, there
currently is no standard way of measuring to what degree an interpretability
method achieves an intended objective. Moreover, there is no accepted standard
of performance by which we can compare and rank the current existing
interpretability methods. In this paper, we propose a novel metric for
quantifying the quality of explanations generated by interpretability methods.
We compute the metric on three NLP tasks using six interpretability methods and
present our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TALM: Tool Augmented Language Models. (arXiv:2205.12255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12255">
<div class="article-summary-box-inner">
<span><p>Transformer based language models (LMs) demonstrate increasing performance
with scale across a wide variety of tasks. Scale alone however cannot enable
models to solve tasks that require access to ephemeral, changing, or private
data that was unavailable at training time. Many useful tasks may also benefit
from LMs being able to access APIs that read or modify state. In this work, we
present Tool Augmented Language Models (TALM), combining a text-only approach
to augment language models with non-differentiable tools, and an iterative
"self-play" technique to bootstrap performance starting from few tool
demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA
task and a reasoning oriented math task with simple tools. At a given model
scale, TALM significantly outperforms non-augmented LMs. We further demonstrate
that TALM successfully performs out-of-distribution inferences on both QA and
math tasks, where non-augmented LMs fail. Our results suggest that Tool
Augmented Language Models are a promising direction to enrich LMs'
capabilities, with less dependence on scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">History Compression via Language Models in Reinforcement Learning. (arXiv:2205.12258v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12258">
<div class="article-summary-box-inner">
<span><p>In a partially observable Markov decision process (POMDP), an agent typically
uses a representation of the past to approximate the underlying MDP. We propose
to utilize a frozen Pretrained Language Transformer (PLT) for history
representation and compression to improve sample efficiency. To avoid training
of the Transformer, we introduce FrozenHopfield, which automatically associates
observations with original token embeddings. To form these associations, a
modern Hopfield network stores the original token embeddings, which are
retrieved by queries that are obtained by a random but fixed projection of
observations. Our new method, HELM, enables actor-critic network architectures
that contain a pretrained language Transformer for history representation as a
memory module. Since a representation of the past need not be learned, HELM is
much more sample efficient than competitors. On Minigrid and Procgen
environments HELM achieves new state-of-the-art results. Our code is available
at https://github.com/ml-jku/helm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Policy Compliance Detection via Expression Tree Inference. (arXiv:2205.12259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12259">
<div class="article-summary-box-inner">
<span><p>Policy Compliance Detection (PCD) is a task we encounter when reasoning over
texts, e.g. legal frameworks. Previous work to address PCD relies heavily on
modeling the task as a special case of Recognizing Textual Entailment.
Entailment is applicable to the problem of PCD, however viewing the policy as a
single proposition, as opposed to multiple interlinked propositions, yields
poor performance and lacks explainability. To address this challenge, more
recent proposals for PCD have argued for decomposing policies into expression
trees consisting of questions connected with logic operators. Question
answering is used to obtain answers to these questions with respect to a
scenario. Finally, the expression tree is evaluated in order to arrive at an
overall solution. However, this work assumes expression trees are provided by
experts, thus limiting its applicability to new policies. In this work, we
learn how to infer expression trees automatically from policy texts. We ensure
the validity of the inferred trees by introducing constrained decoding using a
finite state automaton to ensure the generation of valid trees. We determine
through automatic evaluation that 63% of the expression trees generated by our
constrained generation model are logically equivalent to gold trees. Human
evaluation shows that 88% of trees generated by our model are correct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-domain Detection for Natural Language Understanding in Dialog Systems. (arXiv:1909.03862v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.03862">
<div class="article-summary-box-inner">
<span><p>Natural Language Understanding (NLU) is a vital component of dialogue
systems, and its ability to detect Out-of-Domain (OOD) inputs is critical in
practical applications, since the acceptance of the OOD input that is
unsupported by the current system may lead to catastrophic failure. However,
most existing OOD detection methods rely heavily on manually labeled OOD
samples and cannot take full advantage of unlabeled data. This limits the
feasibility of these models in practical applications.
</p>
<p>In this paper, we propose a novel model to generate high-quality pseudo OOD
samples that are akin to IN-Domain (IND) input utterances, and thereby improves
the performance of OOD detection. To this end, an autoencoder is trained to map
an input utterance into a latent code. and the codes of IND and OOD samples are
trained to be indistinguishable by utilizing a generative adversarial network.
To provide more supervision signals, an auxiliary classifier is introduced to
regularize the generated OOD samples to have indistinguishable intent labels.
Experiments show that these pseudo OOD samples generated by our model can be
used to effectively improve OOD detection in NLU. Besides, we also demonstrate
that the effectiveness of these pseudo OOD data can be further improved by
efficiently utilizing unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy and Efficient Transformer : Scalable Inference Solution For large NLP model. (arXiv:2104.12470v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12470">
<div class="article-summary-box-inner">
<span><p>Recently, large-scale transformer-based models have been proven to be
effective over various tasks across many domains. Nevertheless, applying them
in industrial production requires tedious and heavy works to reduce inference
costs. To fill such a gap, we introduce a scalable inference solution: Easy and
Efficient Transformer (EET), including a series of transformer inference
optimization at the algorithm and implementation levels. First, we design
highly optimized kernels for long inputs and large hidden sizes. Second, we
propose a flexible CUDA memory manager to reduce the memory footprint when
deploying a large model. Compared with the state-of-the-art transformer
inference library (Faster Transformer v4.0), EET can achieve an average of
1.40-4.20x speedup on the transformer decoder layer with an A100 GPU
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues. (arXiv:2106.03065v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03065">
<div class="article-summary-box-inner">
<span><p>This paper propose to combine pretrained language models with the modular
dialogue paradigm for open-domain dialogue modeling. Our method,
semantic-enhanced finetuning, instantiates conversation understanding,
planning, and response generation as a language model finetuning task. At
inference, we disentangle semantic and token variations by specifying sampling
methods and constraints for each module separately. For training and
evaluation, we present X-Weibo, a Chinese multi-turn open-domain dialogue
dataset with automatic annotation for emotions, DAs, and topical words.
Experiments show that semantic-enhanced finetuning outperforms strong baselines
on non-semantic and semantic metrics, improves the human-evaluated relevance,
coherence, and informativeness, and exhibits considerable controllability over
semantic variables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear-time calculation of the expected sum of edge lengths in random projective linearizations of trees. (arXiv:2107.03277v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03277">
<div class="article-summary-box-inner">
<span><p>The syntactic structure of a sentence is often represented using syntactic
dependency trees. The sum of the distances between syntactically related words
has been in the limelight for the past decades. Research on dependency
distances led to the formulation of the principle of dependency distance
minimization whereby words in sentences are ordered so as to minimize that sum.
Numerous random baselines have been defined to carry out related quantitative
studies on languages. The simplest random baseline is the expected value of the
sum in unconstrained random permutations of the words in the sentence, namely
when all the shufflings of the words of a sentence are allowed and equally
likely. Here we focus on a popular baseline: random projective permutations of
the words of the sentence, that is, permutations where the syntactic dependency
structure is projective, a formal constraint that sentences satisfy often in
languages. Thus far, the expectation of the sum of dependency distances in
random projective shufflings of a sentence has been estimated approximately
with a Monte Carlo procedure whose cost is of the order of $Rn$, where $n$ is
the number of words of the sentence and $R$ is the number of samples; it is
well known that the larger $R$, the lower the error of the estimation but the
larger the time cost. Here we present formulae to compute that expectation
without error in time of the order of $n$. Furthermore, we show that star trees
maximize it, and give an algorithm to retrieve the trees that minimize it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks. (arXiv:2107.07610v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07610">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an approach to improve the robustness of BERT
language models against word substitution-based adversarial attacks by
leveraging adversarial perturbations for self-supervised contrastive learning.
We create a word-level adversarial attack generating hard positives on-the-fly
as adversarial examples during contrastive learning. In contrast to previous
works, our method improves model robustness without using any labeled data.
Experimental results show that our method improves robustness of BERT against
four different word substitution-based adversarial attacks, and combining our
method with adversarial training gives higher robustness than adversarial
training alone. As our method improves the robustness of BERT purely with
unlabeled data, it opens up the possibility of using large text datasets to
train robust language models against word substitution-based adversarial
attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06013">
<div class="article-summary-box-inner">
<span><p>Visual dialog is challenging since it needs to answer a series of coherent
questions based on understanding the visual environment. How to ground related
visual objects is one of the key problems. Previous studies utilize the
question and history to attend to the image and achieve satisfactory
performance, however these methods are not sufficient to locate related visual
objects without any guidance. The inappropriate grounding of visual objects
prohibits the performance of visual dialog models. In this paper, we propose a
novel approach to Learn to Ground visual objects for visual dialog, which
employs a novel visual objects grounding mechanism where both prior and
posterior distributions over visual objects are used to facilitate visual
objects grounding. Specifically, a posterior distribution over visual objects
is inferred from both context (history and questions) and answers, and it
ensures the appropriate grounding of visual objects during the training
process. Meanwhile, a prior distribution, which is inferred from context only,
is used to approximate the posterior distribution so that appropriate visual
objects can be grounded even without answers during the inference process.
Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our
approach improves the previous strong models in both generative and
discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03215">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LMs) are known to encode world knowledge in their
parameters as they pretrain on a vast amount of web corpus, which is often
utilized for performing knowledge-dependent downstream tasks such as question
answering, fact-checking, and open dialogue. In real-world scenarios, the world
knowledge stored in the LMs can quickly become outdated as the world changes,
but it is non-trivial to avoid catastrophic forgetting and reliably acquire new
knowledge while preserving invariant knowledge. To push the community towards
better maintenance of ever-changing LMs, we formulate a new continual learning
(CL) problem called Continual Knowledge Learning (CKL). We construct a new
benchmark and metric to quantify the retention of time-invariant world
knowledge, the update of outdated knowledge, and the acquisition of new
knowledge. We adopt applicable recent methods from literature to create several
strong baselines. Through extensive experiments, we find that CKL exhibits
unique challenges that are not addressed in previous CL setups, where parameter
expansion is necessary to reliably retain and learn knowledge simultaneously.
By highlighting the critical causes of knowledge forgetting, we show that CKL
is a challenging and important problem that helps us better understand and
train ever-changing LMs. The benchmark datasets, evaluation script, and
baseline code to reproduce our results are available at
https://github.com/joeljang/continual-knowledge-learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantics-aware Attention Improves Neural Machine Translation. (arXiv:2110.06920v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06920">
<div class="article-summary-box-inner">
<span><p>The integration of syntactic structures into Transformer machine translation
has shown positive results, but to our knowledge, no work has attempted to do
so with semantic structures. In this work we propose two novel parameter-free
methods for injecting semantic information into Transformers, both rely on
semantics-aware masking of (some of) the attention heads. One such method
operates on the encoder, through a Scene-Aware Self-Attention (SASA) head.
Another on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We
show a consistent improvement over the vanilla Transformer and syntax-aware
models for four language pairs. We further show an additional gain when using
both semantic and syntactic structures in some language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing. (arXiv:2110.07205v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07205">
<div class="article-summary-box-inner">
<span><p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in
pre-trained natural language processing models, we propose a unified-modal
SpeechT5 framework that explores the encoder-decoder pre-training for
self-supervised speech/text representation learning. The SpeechT5 framework
consists of a shared encoder-decoder network and six modal-specific
(speech/text) pre/post-nets. After preprocessing the input speech/text through
the pre-nets, the shared encoder-decoder network models the
sequence-to-sequence transformation, and then the post-nets generate the output
in the speech/text modality based on the output of the decoder. Leveraging
large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a
unified-modal representation, hoping to improve the modeling capability for
both speech and text. To align the textual and speech information into this
unified semantic space, we propose a cross-modal vector quantization approach
that randomly mixes up speech/text states with latent units as the interface
between encoder and decoder. Extensive evaluations show the superiority of the
proposed SpeechT5 framework on a wide variety of spoken language processing
tasks, including automatic speech recognition, speech synthesis, speech
translation, voice conversion, speech enhancement, and speaker identification.
We release our code and model at https://github.com/microsoft/SpeechT5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why KDAC? A general activation function for knowledge discovery. (arXiv:2111.13858v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13858">
<div class="article-summary-box-inner">
<span><p>Deep learning oriented named entity recognition (DNER) has gradually become
the paradigm of knowledge discovery, which greatly promotes domain
intelligence. However, the current activation function of DNER fails to treat
gradient vanishing, no negative output or non-differentiable existence, which
may impede knowledge exploration caused by the omission and incomplete
representation of latent semantics. To break through the dilemma, we present a
novel activation function termed KDAC. Detailly, KDAC is an aggregation
function with multiple conversion modes. The backbone of the activation region
is the interaction between exponent and linearity, and the both ends extend
through adaptive linear divergence, which surmounts the obstacle of gradient
vanishing and no negative output. Crucially, the non-differentiable points are
alerted and eliminated by an approximate smoothing algorithm. KDAC has a series
of brilliant properties, including nonlinear, stable near-linear transformation
and derivative, as well as dynamic style, etc. We perform experiments based on
BERT-BiLSTM-CNN-CRF model on six benchmark datasets containing different domain
knowledge, such as Weibo, Clinical, E-commerce, Resume, HAZOP and People's
daily. The evaluation results show that KDAC is advanced and effective, and can
provide more generalized activation to stimulate the performance of DNER. We
hope that KDAC can be exploited as a promising activation function to devote
itself to the construction of knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge. (arXiv:2112.08544v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08544">
<div class="article-summary-box-inner">
<span><p>Claim detection and verification are crucial for news understanding and have
emerged as promising technologies for mitigating news misinformation. However,
most existing work has focused on claim sentence analysis while overlooking
crucial background attributes (e.g., claimer, claim objects). In this work, we
present NewsClaims, a new benchmark for knowledge-aware claim detection in the
news domain. We redefine the claim detection problem to include extraction of
additional background attributes related to each claim and release 889 claims
annotated over 143 news articles. NewsClaims aims to benchmark claim detection
systems in emerging scenarios, comprising unseen topics with little or no
training data. To this end, we provide a comprehensive evaluation of zero-shot
and prompt-based baselines for NewsClaims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs. (arXiv:2112.08804v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08804">
<div class="article-summary-box-inner">
<span><p>We present CrossSum, a large-scale cross-lingual abstractive summarization
dataset comprising 1.7 million article-summary samples in 1500+ language pairs.
We create CrossSum by aligning identical articles written in different
languages via cross-lingual retrieval from a multilingual summarization
dataset. We propose a multi-stage data sampling algorithm to effectively train
a cross-lingual summarization model capable of summarizing an article in any
target language. We also propose LaSE, a new metric for automatically
evaluating model-generated summaries and showing a strong correlation with
ROUGE. Performance on ROUGE and LaSE indicate that pretrained models fine-tuned
on CrossSum consistently outperform baseline models, even when the source and
target language pairs are linguistically distant. To the best of our knowledge,
CrossSum is the largest cross-lingual summarization dataset and the first-ever
that does not rely solely on English as the pivot language. We are releasing
the dataset, alignment and training scripts, and the models to spur future
research on cross-lingual abstractive summarization. The resources can be found
at https://github.com/csebuetnlp/CrossSum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Questions Generate Named Entity Recognition Datasets. (arXiv:2112.08808v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08808">
<div class="article-summary-box-inner">
<span><p>Recent named entity recognition (NER) models often rely on human-annotated
datasets requiring the vast engagement of professional knowledge on the target
domain and entities. This work introduces an ask-to-generate approach, which
automatically generates NER datasets by asking simple natural language
questions to an open-domain question answering system (e.g., "Which disease?").
Despite using fewer training resources, our models solely trained on the
generated datasets largely outperform strong low-resource models by 20.8 F1
score on average across six popular NER benchmarks. Our models also show
competitive performance with rich-resource models that additionally leverage
in-domain dictionaries provided by domain experts. In few-shot NER, we
outperform the previous best model by 5.2 F1 score on three benchmarks and
achieve new state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RuMedBench: A Russian Medical Language Understanding Benchmark. (arXiv:2201.06499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06499">
<div class="article-summary-box-inner">
<span><p>The paper describes the open Russian medical language understanding benchmark
covering several task types (classification, question answering, natural
language inference, named entity recognition) on a number of novel text sets.
Given the sensitive nature of the data in healthcare, such a benchmark
partially closes the problem of Russian medical dataset absence. We prepare the
unified format labeling, data split, and evaluation metrics for new tasks. The
remaining tasks are from existing datasets with a few modifications. A
single-number metric expresses a model's ability to cope with the benchmark.
Moreover, we implement several baseline models, from simple ones to neural
networks with transformer architecture, and release the code. Expectedly, the
more advanced models yield better performance, but even a simple model is
enough for a decent result in some tasks. Furthermore, for all tasks, we
provide a human evaluation. Interestingly the models outperform humans in the
large-scale classification tasks. However, the advantage of natural
intelligence remains in the tasks requiring more knowledge and reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12114">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms are dominating the explainability of deep models. They
produce probability distributions over the input, which are widely deemed as
feature-importance indicators. However, in this paper, we find one critical
limitation in attention explanations: weakness in identifying the polarity of
feature impact. This would be somehow misleading -- features with higher
attention weights may not faithfully contribute to model predictions; instead,
they can impose suppression effects. With this finding, we reflect on the
explainability of current attention-based techniques, such as
Attentio$\odot$Gradient and LRP-based attention explanations. We first propose
an actionable diagnostic methodology (henceforth faithfulness violation test)
to measure the consistency between explanation weights and the impact polarity.
Through the extensive experiments, we then show that most tested explanation
methods are unexpectedly hindered by the faithfulness violation issue,
especially the raw attention. Empirical analyses on the factors affecting
violation issues further provide useful observations for adopting explanation
methods in attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Pretraining Term Frequencies on Few-Shot Reasoning. (arXiv:2202.07206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07206">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (LMs) have demonstrated ability to perform
numerical reasoning by extrapolating from a few examples in few-shot settings.
However, the extent to which this extrapolation relies on robust reasoning is
unclear. In this paper, we investigate how well these models reason with terms
that are less frequent in the pretraining data. In particular, we examine the
correlations between the model performance on test instances and the frequency
of terms from those instances in the pretraining data. We measure the strength
of this correlation for a number of GPT-based language models (pretrained on
the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and
unit conversion). Our results consistently demonstrate that models are more
accurate on instances whose terms are more prevalent, in some cases above
$70\%$ (absolute) more accurate on the top 10\% frequent terms in comparison to
the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot
numerical reasoning tasks, our results raise the question of how much models
actually generalize beyond pretraining data, and we encourage researchers to
take the pretraining data into account when interpreting evaluation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logical Fallacy Detection. (arXiv:2202.13758v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13758">
<div class="article-summary-box-inner">
<span><p>Reasoning is central to human intelligence. However, fallacious arguments are
common, and some exacerbate problems such as spreading misinformation about
climate change. In this paper, we propose the task of logical fallacy
detection, and provide a new dataset (Logic) of logical fallacies generally
found in text, together with an additional challenge set for detecting logical
fallacies in climate change claims (LogicClimate). Detecting logical fallacies
is a hard problem as the model must understand the underlying logical structure
of the argument. We find that existing pretrained large language models perform
poorly on this task. In contrast, we show that a simple structure-aware
classifier outperforms the best language model by 5.46% on Logic and 4.51% on
LogicClimate. We encourage future work to explore this task as (a) it can serve
as a new reasoning challenge for language models, and (b) it can have potential
applications in tackling the spread of misinformation. Our dataset and code are
available at https://github.com/causalNLP/logical-fallacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Resource-Constrained Keyphrase Generation. (arXiv:2203.08118v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08118">
<div class="article-summary-box-inner">
<span><p>State-of-the-art keyphrase generation methods generally depend on large
annotated datasets, limiting their performance in domains with limited
annotated data. To overcome this challenge, we design a data-oriented approach
that first identifies salient information using unsupervised corpus-level
statistics, and then learns a task-specific intermediate representation based
on a pre-trained language model. We introduce salient span recovery and salient
span prediction as denoising training objectives that condense the
intra-article and inter-article knowledge essential for keyphrase generation.
Through experiments on multiple keyphrase generation benchmarks, we show the
effectiveness of the proposed approach for facilitating low-resource and
zero-shot keyphrase generation. We further observe that the method especially
benefits the generation of absent keyphrases, approaching the performance of
models trained with large training sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine. (arXiv:2203.10232v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10232">
<div class="article-summary-box-inner">
<span><p>In this paper, we present DuReader-retrieval, a large-scale Chinese dataset
for passage retrieval. DuReader-retrieval contains more than 90K queries and
over 8M unique passages from Baidu search. To ensure the quality of our
benchmark and address the shortcomings in other existing datasets, we (1)
reduce the false negatives in development and testing sets by pooling the
results from multiple retrievers with human annotations, (2) and de-duplicate
the semantically similar questions between training with development and
testing sets. Additionally, we provide two out-of-domain testing sets for
cross-domain evaluation, as well as a cross-lingual set that has been manually
translated for cross-lingual retrieval. The experiments demonstrate that
DuReader-retrieval is challenging and there is still plenty of room for
improvement, e.g. salient phrase and syntax mismatch between query and
paragraph. These experimental results show that the dense retriever does not
generalize well across domains, and cross-lingual retrieval is essentially
challenging. DuReader-retrieval will be publicly available at
https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Ranking and Aggregation of Label Descriptions for Zero-Shot Classifiers. (arXiv:2204.09481v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09481">
<div class="article-summary-box-inner">
<span><p>Zero-shot text classifiers based on label descriptions embed an input text
and a set of labels into the same space: measures such as cosine similarity can
then be used to select the most similar label description to the input text as
the predicted label. In a true zero-shot setup, designing good label
descriptions is challenging because no development set is available. Inspired
by the literature on Learning with Disagreements, we look at how probabilistic
models of repeated rating analysis can be used for selecting the best label
descriptions in an unsupervised fashion. We evaluate our method on a set of
diverse datasets and tasks (sentiment, topic and stance). Furthermore, we show
that multiple, noisy label descriptions can be aggregated to boost the
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AiSocrates: Towards Answering Ethical Quandary Questions. (arXiv:2205.05989v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05989">
<div class="article-summary-box-inner">
<span><p>Considerable advancements have been made in various NLP tasks based on the
impressive power of large pre-trained language models (LLMs). These results
have inspired efforts to understand the limits of LLMs so as to evaluate how
far we are from achieving human level general natural language understanding.
In this work, we challenge the capability of LLMs with the new task of Ethical
Quandary Generative Question Answering. Ethical quandary questions are more
challenging to address because multiple conflicting answers may exist to a
single quandary. We propose a system, AiSocrates, that provides an answer with
a deliberative exchange of different perspectives to an ethical quandary, in
the approach of Socratic philosophy, instead of providing a closed answer like
an oracle. AiSocrates searches for different ethical principles applicable to
the ethical quandary and generates an answer conditioned on the chosen
principles through prompt-based few-shot learning. We also address safety
concerns by providing a human controllability option in choosing ethical
principles. We show that AiSocrates generates promising answers to ethical
quandary questions with multiple perspectives, 6.92% more often than answers
written by human philosophers by one measure, but the system still needs
improvement to match the coherence of human philosophers fully. We argue that
AiSocrates is a promising step toward developing an NLP system that
incorporates human values explicitly by prompt instructions. We are releasing
the code for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Budge programming language. (arXiv:2205.07979v3 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07979">
<div class="article-summary-box-inner">
<span><p>We present a simple, esoteric programming language based on G\"odel numbering
and prime factorization, enhanced with explicit, scoped loops, allowing for
easy program composition. We will show the syntax and semantics and then
provide a few example programs and their evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator. (arXiv:2205.09058v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09058">
<div class="article-summary-box-inner">
<span><p>Contextual knowledge is essential for reducing speech recognition errors on
high-valued long-tail words. This paper proposes a novel tree-constrained
pointer generator (TCPGen) component that enables end-to-end ASR models to bias
towards a list of long-tail words obtained using external contextual
information. With only a small overhead in memory use and computation cost,
TCPGen can structure thousands of biasing words efficiently into a symbolic
prefix-tree and creates a neural shortcut between the tree and the final ASR
output to facilitate the recognition of the biasing words. To enhance TCPGen,
we further propose a novel minimum biasing word error (MBWE) loss that directly
optimises biasing word errors during training, along with a biasing-word-driven
language model discounting (BLMD) method during the test. All contextual ASR
systems were evaluated on the public Librispeech audiobook corpus and the data
from the dialogue state tracking challenges (DSTC) with the biasing lists
extracted from the dialogue-system ontology. Consistent word error rate (WER)
reductions were achieved with TCPGen, which were particularly significant on
the biasing words with around 40\% relative reductions in the recognition error
rates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved
more significant WER reductions on the biasing words. TCPGen also achieved
zero-shot learning of words not in the audio training set with large WER
reductions on the out-of-vocabulary words in the biasing list.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deeper vs Wider: A Revisit of Transformer Configuration. (arXiv:2205.10505v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10505">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have delivered impressive results on many tasks,
particularly vision and language tasks. In many model training situations,
conventional configurations are typically adopted. For example, we often set
the base model with hidden dimensions (i.e. model width) to be 768 and the
number of transformer layers (i.e. model depth) to be 12. In this paper, we
revisit these conventional configurations. Through theoretical analysis and
experimental evaluation, we show that the masked autoencoder is effective in
alleviating the over-smoothing issue in deep transformer training. Based on
this finding, we propose Bamboo, an idea of using deeper and narrower
transformer configurations, for masked autoencoder training. On ImageNet, with
such a simple change in configuration, re-designed model achieves 87.1% top-1
accuracy and outperforms SoTA models like MAE and BEiT. On language tasks,
re-designed model outperforms BERT with default setting by 1.1 points on
average, on GLUE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase-level Textual Adversarial Attack with Label Preservation. (arXiv:2205.10710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10710">
<div class="article-summary-box-inner">
<span><p>Generating high-quality textual adversarial examples is critical for
investigating the pitfalls of natural language processing (NLP) models and
further promoting their robustness. Existing attacks are usually realized
through word-level or sentence-level perturbations, which either limit the
perturbation space or sacrifice fluency and textual quality, both affecting the
attack effectiveness. In this paper, we propose Phrase-Level Textual
Adversarial aTtack (PLAT) that generates adversarial samples through
phrase-level perturbations. PLAT first extracts the vulnerable phrases as
attack targets by a syntactic parser, and then perturbs them by a pre-trained
blank-infilling model. Such flexible perturbation design substantially expands
the search space for more effective attacks without introducing too many
modifications, and meanwhile maintaining the textual fluency and grammaticality
via contextualized generation using surrounding texts. Moreover, we develop a
label-preservation filter leveraging the likelihoods of language models
fine-tuned on each class, rather than textual similarity, to rule out those
perturbations that potentially alter the original class label for humans.
Extensive experiments and human evaluation demonstrate that PLAT has a superior
attack effectiveness as well as a better label consistency than strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Proof Generation via Iterative Backward Reasoning. (arXiv:2205.10714v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10714">
<div class="article-summary-box-inner">
<span><p>We present IBR, an Iterative Backward Reasoning model to solve the proof
generation tasks on rule-based Question Answering (QA), where models are
required to reason over a series of textual rules and facts to find out the
related proof path and derive the final answer. We handle the limitations of
existed works in two folds: 1) enhance the interpretability of reasoning
procedures with detailed tracking, by predicting nodes and edges in the proof
path iteratively backward from the question; 2) promote the efficiency and
accuracy via reasoning on the elaborate representations of nodes and history
paths, without any intermediate texts that may introduce external noise during
proof generation. There are three main modules in IBR, QA and proof strategy
prediction to obtain the answer and offer guidance for the following procedure;
parent node prediction to determine a node in the existing proof that a new
child node will link to; child node prediction to find out which new node will
be added to the proof. Experiments on both synthetic and paraphrased datasets
demonstrate that IBR has better in-domain performance as well as cross-domain
transferability than several strong baselines. Our code and models are
available at https://github.com/find-knowledge/IBR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relphormer: Relational Graph Transformer for Knowledge Graph Representation. (arXiv:2205.10852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10852">
<div class="article-summary-box-inner">
<span><p>Transformers have achieved remarkable performance in widespread fields,
including natural language processing, computer vision and graph mining.
However, in the knowledge graph representation, where translational distance
paradigm dominates this area, vanilla Transformer architectures have not
yielded promising improvements. Note that vanilla Transformer architectures
struggle to capture the intrinsically semantic and structural information of
knowledge graphs and can hardly scale to long-distance neighbors due to
quadratic dependency. To this end, we propose a new variant of Transformer for
knowledge graph representation dubbed Relphormer. Specifically, we introduce
Triple2Seq which can dynamically sample contextualized sub-graph sequences as
the input of the Transformer to alleviate the scalability issue. We then
propose a novel structure-enhanced self-attention mechanism to encode the
relational information and keep the globally semantic information among
sub-graphs. Moreover, we propose masked knowledge modeling as a new paradigm
for knowledge graph representation learning to unify different link prediction
tasks. Experimental results show that our approach can obtain better
performance on benchmark datasets compared with baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BanglaNLG: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla. (arXiv:2205.11081v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11081">
<div class="article-summary-box-inner">
<span><p>This work presents BanglaNLG, a comprehensive benchmark for evaluating
natural language generation (NLG) models in Bangla, a widely spoken yet
low-resource language in the web domain. We aggregate three challenging
conditional text generation tasks under the BanglaNLG benchmark. Then, using a
clean corpus of 27.5 GB of Bangla data, we pretrain BanglaT5, a
sequence-to-sequence Transformer model for Bangla. BanglaT5 achieves
state-of-the-art performance in all of these tasks, outperforming mT5 (base) by
up to 5.4%. We are making the BanglaT5 language model and a leaderboard
publicly available in the hope of advancing future research and evaluation on
Bangla NLG. The resources can be found at
https://github.com/csebuetnlp/BanglaNLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PASH at TREC 2021 Deep Learning Track: Generative Enhanced Model for Multi-stage Ranking. (arXiv:2205.11245v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11245">
<div class="article-summary-box-inner">
<span><p>This paper describes the PASH participation in TREC 2021 Deep Learning Track.
In the recall stage, we adopt a scheme combining sparse and dense retrieval
method. In the multi-stage ranking phase, point-wise and pair-wise ranking
strategies are used one after another based on model continual pre-trained on
general knowledge and document-level data. Compared to TREC 2020 Deep Learning
Track, we have additionally introduced the generative model T5 to further
enhance the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracing Knowledge in Language Models Back to the Training Data. (arXiv:2205.11482v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11482">
<div class="article-summary-box-inner">
<span><p>Neural language models (LMs) have been shown to memorize a great deal of
factual knowledge. But when an LM generates an assertion, it is often difficult
to determine where it learned this information and whether it is true. In this
paper, we introduce a new benchmark for fact tracing: tracing language models'
assertions back to the training examples that provided evidence for those
predictions. Prior work has suggested that dataset-level influence methods
might offer an effective framework for tracing predictions back to training
data. However, such methods have not been evaluated for fact tracing, and
researchers primarily have studied them through qualitative analysis or as a
data cleaning technique for classification/regression tasks. We present the
first experiments that evaluate influence methods for fact tracing, using
well-understood information retrieval (IR) metrics. We compare two popular
families of influence methods -- gradient-based and embedding-based -- and show
that neither can fact-trace reliably; indeed, both methods fail to outperform
an IR baseline (BM25) that does not even access the LM. We explore why this
occurs (e.g., gradient saturation) and demonstrate that existing influence
methods must be improved significantly before they can reliably attribute
factual predictions in LMs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Concept Contribution Spatially: Hidden Layer Interpretation with Spatial Activation Concept Vector. (arXiv:2205.11511v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11511">
<div class="article-summary-box-inner">
<span><p>To interpret deep learning models, one mainstream is to explore the learned
concepts by networks. Testing with Concept Activation Vector (TCAV) presents a
powerful tool to quantify the contribution of query concepts (represented by
user-defined guidance images) to a target class. For example, we can
quantitatively evaluate whether and to what extent concept striped contributes
to model prediction zebra with TCAV. Therefore, TCAV whitens the reasoning
process of deep networks. And it has been applied to solve practical problems
such as diagnosis. However, for some images where the target object only
occupies a small fraction of the region, TCAV evaluation may be interfered with
by redundant background features because TCAV calculates concept contribution
to a target class based on a whole hidden layer. To tackle this problem, based
on TCAV, we propose Spatial Activation Concept Vector (SACV) which identifies
the relevant spatial locations to the query concept while evaluating their
contributions to the model prediction of the target class. Experiment shows
that SACV generates a more fine-grained explanation map for a hidden layer and
quantifies concepts' contributions spatially. Moreover, it avoids interference
from background features. The code is available on
https://github.com/AntonotnaWang/Spatial-Activation-Concept-Vector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cardiomegaly Detection using Deep Convolutional Neural Network with U-Net. (arXiv:2205.11515v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11515">
<div class="article-summary-box-inner">
<span><p>Cardiomegaly is indeed a medical disease in which the heart is enlarged.
Cardiomegaly is better to handle if caught early, so early detection is
critical. The chest X-ray, being one of the most often used radiography
examinations, has been used to detect and visualize abnormalities of human
organs for decades. X-ray is also a significant medical diagnosis tool for
cardiomegaly. Even for domain experts, distinguishing the many types of
diseases from the X-ray is a difficult and time-consuming task. Deep learning
models are also most effective when used on huge data sets, yet due to privacy
concerns, large datasets are rarely available inside the medical industry. A
Deep learning-based customized retrained U-Net model for detecting Cardiomegaly
disease is presented in this research. In the training phase, chest X-ray
images from the "ChestX-ray8" open source real dataset are used. To reduce
computing time, this model performs data preprocessing, picture improvement,
image compression, and classification before moving on to the training step.
The work used a chest x-ray image dataset to simulate and produced a diagnostic
accuracy of 94%, a sensitivity of 96.2 percent, and a specificity of 92.5
percent, which beats prior pre-trained model findings for identifying
Cardiomegaly disease.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging via Differentiable Microscopy. (arXiv:2205.11521v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11521">
<div class="article-summary-box-inner">
<span><p>With applications ranging from metabolomics to histopathology, quantitative
phase microscopy (QPM) is a powerful label-free imaging modality. Despite
significant advances in fast multiplexed imaging sensors and
deep-learning-based inverse solvers, the throughput of QPM is currently limited
by the speed of electronic hardware. Complementarily, to improve throughput
further, here we propose to acquire images in a compressed form such that more
information can be transferred beyond the existing electronic hardware
bottleneck. To this end, we present a learnable optical
compression-decompression framework that learns content-specific features. The
proposed differentiable optical-electronic quantitative phase microscopy
($\partial \mu$) first uses learnable optical feature extractors as image
compressors. The intensity representation produced by these networks is then
captured by the imaging sensor. Finally, a reconstruction network running on
electronic hardware decompresses the QPM images. The proposed system achieves
compression of $\times$ 64 while maintaining the SSIM of $\sim 0.90$ and PSNR
of $\sim 30$ dB. The promising results demonstrated by our experiments open up
a new pathway for achieving end-to-end optimized (i.e., optics and electronic)
compact QPM systems that provide unprecedented throughput improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating the creation of instance segmentation training sets through bounding box annotation. (arXiv:2205.11563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11563">
<div class="article-summary-box-inner">
<span><p>Collecting image annotations remains a significant burden when deploying CNN
in a specific applicative context. This is especially the case when the
annotation consists in binary masks covering object instances. Our work
proposes to delineate instances in three steps, based on a semi-automatic
approach: (1) the extreme points of an object (left-most, right-most, top,
bottom pixels) are manually defined, thereby providing the object bounding-box,
(2) a universal automatic segmentation tool like Deep Extreme Cut is used to
turn the bounded object into a segmentation mask that matches the extreme
points; and (3) the predicted mask is manually corrected. Various strategies
are then investigated to balance the human manual annotation resources between
bounding-box definition and mask correction, including when the correction of
instance masks is prioritized based on their overlap with other instance
bounding-boxes, or the outcome of an instance segmentation model trained on a
partially annotated dataset. Our experimental study considers a teamsport
player segmentation task, and measures how the accuracy of the Panoptic-Deeplab
instance segmentation model depends on the human annotation resources
allocation strategy. It reveals that the sole definition of extreme points
results in a model accuracy that would require up to 10 times more resources if
the masks were defined through fully manual delineation of instances. When
targeting higher accuracies, prioritizing the mask correction among the
training set instances is also shown to save up to 80\% of correction
annotation resources compared to a systematic frame by frame correction of
instances, for a same trained instance segmentation model accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VPAIR -- Aerial Visual Place Recognition and Localization in Large-scale Outdoor Environments. (arXiv:2205.11567v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11567">
<div class="article-summary-box-inner">
<span><p>Visual Place Recognition and Visual Localization are essential components in
navigation and mapping for autonomous vehicles especially in GNSS-denied
navigation scenarios. Recent work has focused on ground or close to ground
applications such as self-driving cars or indoor-scenarios and low-altitude
drone flights. However, applications such as Urban Air Mobility require
operations in large-scale outdoor environments at medium to high altitudes. We
present a new dataset named VPAIR. The dataset was recorded on board a light
aircraft flying at an altitude of more than 300 meters above ground capturing
images with a downwardfacing camera. Each image is paired with a high
resolution reference render including dense depth information and 6-DoF
reference poses. The dataset covers a more than one hundred kilometers long
trajectory over various types of challenging landscapes, e.g. urban, farmland
and forests. Experiments on this dataset illustrate the challenges introduced
by the change in perspective to a bird's eye view such as in-plane rotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminative Feature Learning through Feature Distance Loss. (arXiv:2205.11606v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11606">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks have shown remarkable ability to learn
discriminative semantic features in image recognition tasks. Though, for
classification they often concentrate on specific regions in images. This work
proposes a novel method that combines variant rich base models to concentrate
on different important image regions for classification. A feature distance
loss is implemented while training an ensemble of base models to force them to
learn discriminative feature concepts. The experiments on benchmark
convolutional neural networks (VGG16, ResNet, AlexNet), popular datasets
(Cifar10, Cifar100, miniImageNet, NEU, BSD, TEX), and different training
samples (3, 5, 10, 20, 50, 100 per class) show our methods effectiveness and
generalization ability. Our method outperforms ensemble versions of the base
models without feature distance loss, and the Class Activation Maps explicitly
proves the ability to learn different discriminative feature concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Contrario multi-scale anomaly detection method for industrial quality inspection. (arXiv:2205.11611v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11611">
<div class="article-summary-box-inner">
<span><p>Anomalies can be defined as any non-random structure which deviates from
normality. Anomaly detection methods reported in the literature are numerous
and diverse, as what is considered anomalous usually varies depending on
particular scenarios and applications. In this work we propose an a contrario
framework to detect anomalies in images applying statistical analysis to
feature maps obtained via convolutions. We evaluate filters learned from the
image under analysis via patch PCA, Gabor filters and the feature maps obtained
from a pre-trained deep neural network (Resnet). The proposed method is
multi-scale and fully unsupervised and is able to detect anomalies in a wide
variety of scenarios. While the end goal of this work is the detection of
subtle defects in leather samples for the automotive industry, we show that the
same algorithm achieves state-of-the-art results in public anomalies datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransforMatcher: Match-to-Match Attention for Semantic Correspondence. (arXiv:2205.11634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11634">
<div class="article-summary-box-inner">
<span><p>Establishing correspondences between images remains a challenging task,
especially under large appearance changes due to different viewpoints or
intra-class variations. In this work, we introduce a strong semantic image
matching learner, dubbed TransforMatcher, which builds on the success of
transformer networks in vision domains. Unlike existing convolution- or
attention-based schemes for correspondence, TransforMatcher performs global
match-to-match attention for precise match localization and dynamic refinement.
To handle a large number of matches in a dense correlation map, we develop a
light-weight attention architecture to consider the global match-to-match
interactions. We also propose to utilize a multi-channel correlation map for
refinement, treating the multi-level scores as features instead of a single
score to fully exploit the richer layer-wise semantics. In experiments,
TransforMatcher sets a new state of the art on SPair-71k while performing on
par with existing SOTA methods on the PF-PASCAL dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11664">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithm Development for Controlling Movement of a Robotic Platform by Digital Image Processing. (arXiv:2205.11666v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11666">
<div class="article-summary-box-inner">
<span><p>The following work shows an algorithm that can process images digitally with
the goal of control the movement of a mobile robotic platform in a certain
environment. The platform is identified with a specific color, and displacement
environment of the platform shift has identified obstacles with different
colors, for both cases it worked with the RGB color scale. To obtain the
control's movement of the robotic platform, the algorithm was developed in C
programming language, and used the Open CV libraries for processing images
captured by a video camera on the Dev-platform C + +. The video camera was
previously calibrated using ZHANG technique where parameters were obtained
focal length and tilt focal pixel. In the algorithm histogram analysis and
segmentation of the image were developed, allowing to determine exactly the
relative position of the platform with respect to the obstacles and movement
strategy to follow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning multi-scale functional representations of proteins from single-cell microscopy data. (arXiv:2205.11676v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11676">
<div class="article-summary-box-inner">
<span><p>Protein function is inherently linked to its localization within the cell,
and fluorescent microscopy data is an indispensable resource for learning
representations of proteins. Despite major developments in molecular
representation learning, extracting functional information from biological
images remains a non-trivial computational task. Current state-of-the-art
approaches use autoencoder models to learn high-quality features by
reconstructing images. However, such methods are prone to capturing noise and
imaging artifacts. In this work, we revisit deep learning models used for
classifying major subcellular localizations, and evaluate representations
extracted from their final layers. We show that simple convolutional networks
trained on localization classification can learn protein representations that
encapsulate diverse functional information, and significantly outperform
autoencoder-based models. We also propose a robust evaluation strategy to
assess quality of protein representations across different scales of biological
function.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization. (arXiv:2205.11686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11686">
<div class="article-summary-box-inner">
<span><p>Integrating vision and language has gained notable attention following the
success of pretrained language models. Despite that, a fraction of emerging
multimodal models is suitable for text generation conditioned on images. This
minority is typically developed and evaluated for image captioning, a text
generation task conditioned solely on images with the goal to describe what is
explicitly visible in an image. In this paper, we take a step back and ask: How
do these models work for more complex generative tasks, conditioned on both
text and images? Are models based on joint multimodal pretraining, visually
adapted pretrained language models, or models that combine these two
approaches, more promising for such tasks? We address these questions in the
context of self-rationalization (jointly generating task labels/answers and
free-text explanations) of three tasks: (i) visual question answering in VQA-X,
(ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment
in E-SNLI-VE. We show that recent advances in each modality, CLIP image
representations and scaling of language models, do not consistently improve
multimodal self-rationalization of tasks with multimodal inputs. We also
observe that no model type works universally the best across tasks/datasets and
finetuning data sizes. Our findings call for a backbone modelling approach that
can be built on to advance text generation from images and text beyond image
captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M6-Fashion: High-Fidelity Multi-modal Image Generation and Editing. (arXiv:2205.11705v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11705">
<div class="article-summary-box-inner">
<span><p>The fashion industry has diverse applications in multi-modal image generation
and editing. It aims to create a desired high-fidelity image with the
multi-modal conditional signal as guidance. Most existing methods learn
different condition guidance controls by introducing extra models or ignoring
the style prior knowledge, which is difficult to handle multiple signal
combinations and faces a low-fidelity problem. In this paper, we adapt both
style prior knowledge and flexibility of multi-modal control into one unified
two-stage framework, M6-Fashion, focusing on the practical AI-aided Fashion
design. It decouples style codes in both spatial and semantic dimensions to
guarantee high-fidelity image generation in the first stage. M6-Fashion
utilizes self-correction for the non-autoregressive generation to improve
inference speed, enhance holistic consistency, and support various signal
controls. Extensive experiments on a large-scale clothing dataset M2C-Fashion
demonstrate superior performances on various image generation and editing
tasks. M6-Fashion model serves as a highly potential AI designer for the
fashion industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCVRL: Shuffled Contrastive Video Representation Learning. (arXiv:2205.11710v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11710">
<div class="article-summary-box-inner">
<span><p>We propose SCVRL, a novel contrastive-based framework for self-supervised
learning for videos. Differently from previous contrast learning based methods
that mostly focus on learning visual semantics (e.g., CVRL), SCVRL is capable
of learning both semantic and motion patterns. For that, we reformulate the
popular shuffling pretext task within a modern contrastive learning paradigm.
We show that our transformer-based network has a natural capacity to learn
motion in self-supervised settings and achieves strong performance,
outperforming CVRL on four benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Geometric Moment. (arXiv:2205.11722v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11722">
<div class="article-summary-box-inner">
<span><p>Deep networks for image classification often rely more on texture information
than object shape. While efforts have been made to make deep-models
shape-aware, it is often difficult to make such models simple, interpretable,
or rooted in known mathematical definitions of shape. This paper presents a
deep-learning model inspired by geometric moments, a classically well
understood approach to measure shape-related properties. The proposed method
consists of a trainable network for generating coordinate bases and affine
parameters for making the features geometrically invariant, yet in a
task-specific manner. The proposed model improves the final feature's
interpretation. We demonstrate the effectiveness of our method on standard
image classification datasets. The proposed model achieves higher
classification performance as compared to the baseline and standard ResNet
models while substantially improving interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images. (arXiv:2205.11733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11733">
<div class="article-summary-box-inner">
<span><p>This paper deals with the challenging task of synthesizing novel views for
in-the-wild photographs. Existing methods have shown promising results
leveraging monocular depth estimation and color inpainting with layered depth
representations. However, these methods still have limited capability to handle
scenes with complex 3D geometry. We propose a new method based on the
multiplane image (MPI) representation. To accommodate diverse scene layouts in
the wild and tackle the difficulty in producing high-dimensional MPI contents,
we design a network structure that consists of two novel modules, one for plane
depth adjustment and another for depth-aware color prediction. The former
adjusts the initial plane positions using the RGBD context feature and an
attention mechanism. Given adjusted depth values, the latter predicts the color
and density for each plane separately with proper inter-plane interactions
achieved via a feature masking strategy. To train our method, we construct
large-scale stereo training data using only unconstrained single-view image
collections by a simple yet effective warp-back strategy. The experiments on
both synthetic and real datasets demonstrate that our trained model works
remarkably well and achieves state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alleviating Robust Overfitting of Adversarial Training With Consistency Regularization. (arXiv:2205.11744v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11744">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) has proven to be one of the most effective ways to
defend Deep Neural Networks (DNNs) against adversarial attacks. However, the
phenomenon of robust overfitting, i.e., the robustness will drop sharply at a
certain stage, always exists during AT. It is of great importance to decrease
this robust generalization gap in order to obtain a robust model. In this
paper, we present an in-depth study towards the robust overfitting from a new
angle. We observe that consistency regularization, a popular technique in
semi-supervised learning, has a similar goal as AT and can be used to alleviate
robust overfitting. We empirically validate this observation, and find a
majority of prior solutions have implicit connections to consistency
regularization. Motivated by this, we introduce a new AT solution, which
integrates the consistency regularization and Mean Teacher (MT) strategy into
AT. Specifically, we introduce a teacher model, coming from the average weights
of the student models over the training steps. Then we design a consistency
loss function to make the prediction distribution of the student models over
adversarial examples consistent with that of the teacher model over clean
samples. Experiments show that our proposed method can effectively alleviate
robust overfitting and improve the robustness of DNN models against common
adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UMSNet: An Universal Multi-sensor Network for Human Activity Recognition. (arXiv:2205.11756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11756">
<div class="article-summary-box-inner">
<span><p>Human activity recognition (HAR) based on multimodal sensors has become a
rapidly growing branch of biometric recognition and artificial intelligence.
However, how to fully mine multimodal time series data and effectively learn
accurate behavioral features has always been a hot topic in this field.
Practical applications also require a well-generalized framework that can
quickly process a variety of raw sensor data and learn better feature
representations. This paper proposes a universal multi-sensor network (UMSNet)
for human activity recognition. In particular, we propose a new lightweight
sensor residual block (called LSR block), which improves the performance by
reducing the number of activation function and normalization layers, and adding
inverted bottleneck structure and grouping convolution. Then, the Transformer
is used to extract the relationship of series features to realize the
classification and recognition of human activities. Our framework has a clear
structure and can be directly applied to various types of multi-modal Time
Series Classification (TSC) tasks after simple specialization. Extensive
experiments show that the proposed UMSNet outperforms other state-of-the-art
methods on two popular multi-sensor human activity recognition datasets (i.e.
HHAR dataset and MHEALTH dataset).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNet#: A UNet-like Redesigning Skip Connections for Medical Image Segmentation. (arXiv:2205.11759v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11759">
<div class="article-summary-box-inner">
<span><p>As an essential prerequisite for developing a medical intelligent assistant
system, medical image segmentation has received extensive research and
concentration from the neural network community. A series of UNet-like networks
with encoder-decoder architecture has achieved extraordinary success, in which
UNet2+ and UNet3+ redesign skip connections, respectively proposing dense skip
connection and full-scale skip connection and dramatically improving compared
with UNet in medical image segmentation. However, UNet2+ lacks sufficient
information explored from the full scale, which will affect the learning of
organs' location and boundary. Although UNet3+ can obtain the full-scale
aggregation feature map, owing to the small number of neurons in the structure,
it does not satisfy the segmentation of tiny objects when the number of samples
is small. This paper proposes a novel network structure combining dense skip
connections and full-scale skip connections, named UNet-sharp (UNet\#) for its
shape similar to symbol \#. The proposed UNet\# can aggregate feature maps of
different scales in the decoder sub-network and capture fine-grained details
and coarse-grained semantics from the full scale, which benefits learning the
exact location and accurately segmenting the boundary of organs or lesions. We
perform deep supervision for model pruning to speed up testing and make it
possible for the model to run on mobile devices; furthermore, designing two
classification-guided modules to reduce false positives achieves more accurate
segmentation results. Various experiments of semantic segmentation and instance
segmentation on different modalities (EM, CT, MRI) and dimensions (2D, 3D)
datasets, including the nuclei, brain tumor, liver, and lung, demonstrate that
the proposed method outperforms state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ranking-Based Siamese Visual Tracking. (arXiv:2205.11761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11761">
<div class="article-summary-box-inner">
<span><p>Current Siamese-based trackers mainly formulate the visual tracking into two
independent subtasks, including classification and localization. They learn the
classification subnetwork by processing each sample separately and neglect the
relationship among positive and negative samples. Moreover, such tracking
paradigm takes only the classification confidence of proposals for the final
prediction, which may yield the misalignment between classification and
localization. To resolve these issues, this paper proposes a ranking-based
optimization algorithm to explore the relationship among different proposals.
To this end, we introduce two ranking losses, including the classification one
and the IoU-guided one, as optimization constraints. The classification ranking
loss can ensure that positive samples rank higher than hard negative ones,
i.e., distractors, so that the trackers can select the foreground samples
successfully without being fooled by the distractors. The IoU-guided ranking
loss aims to align classification confidence scores with the Intersection over
Union(IoU) of the corresponding localization prediction for positive samples,
enabling the well-localized prediction to be represented by high classification
confidence. Specifically, the proposed two ranking losses are compatible with
most Siamese trackers and incur no additional computation for inference.
Extensive experiments on seven tracking benchmarks, including OTB100, UAV123,
TC128, VOT2016, NFS30, GOT-10k and LaSOT, demonstrate the effectiveness of the
proposed ranking-based optimization algorithm. The code and raw results are
available at https://github.com/sansanfree/RBO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Augmentation for Efficient Visual Representation Learning for Self-supervised Pre-training. (arXiv:2205.11772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11772">
<div class="article-summary-box-inner">
<span><p>In recent years, self-supervised learning has been studied to deal with the
limitation of available labeled-dataset. Among the major components of
self-supervised learning, the data augmentation pipeline is one key factor in
enhancing the resulting performance. However, most researchers manually
designed the augmentation pipeline, and the limited collections of
transformation may cause the lack of robustness of the learned feature
representation. In this work, we proposed Multi-Augmentations for
Self-Supervised Representation Learning (MA-SSRL), which fully searched for
various augmentation policies to build the entire pipeline to improve the
robustness of the learned feature representation. MA-SSRL successfully learns
the invariant feature representation and presents an efficient, effective, and
adaptable data augmentation pipeline for self-supervised pre-training on
different distribution and domain datasets. MA-SSRL outperforms the previous
state-of-the-art methods on transfer and semi-supervised benchmarks while
requiring fewer training epochs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFNet-M: Adaptive Fusion Network with Masks for 2D+3D Facial Expression Recognition. (arXiv:2205.11785v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11785">
<div class="article-summary-box-inner">
<span><p>2D+3D facial expression recognition (FER) can effectively cope with
illumination changes and pose variations by simultaneously merging 2D texture
and more robust 3D depth information. Most deep learning-based approaches
employ the simple fusion strategy that concatenates the multimodal features
directly after fully-connected layers, without considering the different
degrees of significance for each modality. Meanwhile, how to focus on both 2D
and 3D local features in salient regions is still a great challenge. In this
letter, we propose the adaptive fusion network with masks (AFNet-M) for 2D+3D
FER. To enhance 2D and 3D local features, we take the masks annotating salient
regions of the face as prior knowledge and design the mask attention module
(MA) which can automatically learn two modulation vectors to adjust the feature
maps. Moreover, we introduce a novel fusion strategy that can perform adaptive
fusion at convolutional layers through the designed importance weights
computing module (IWC). Experimental results demonstrate that our AFNet-M
achieves the state-of-the-art performance on BU-3DFE and Bosphorus datasets and
requires fewer parameters in comparison with other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">G-Rep: Gaussian Representation for Arbitrary-Oriented Object Detection. (arXiv:2205.11796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11796">
<div class="article-summary-box-inner">
<span><p>Arbitrary-oriented object representations contain the oriented bounding box
(OBB), quadrilateral bounding box (QBB), and point set (PointSet). Each
representation encounters problems that correspond to its characteristics, such
as the boundary discontinuity, square-like problem, representation ambiguity,
and isolated points, which lead to inaccurate detection. Although many
effective strategies have been proposed for various representations, there is
still no unified solution. Current detection methods based on Gaussian modeling
have demonstrated the possibility of breaking this dilemma; however, they
remain limited to OBB. To go further, in this paper, we propose a unified
Gaussian representation called G-Rep to construct Gaussian distributions for
OBB, QBB, and PointSet, which achieves a unified solution to various
representations and problems. Specifically, PointSet or QBB-based objects are
converted into Gaussian distributions, and their parameters are optimized using
the maximum likelihood estimation algorithm. Then, three optional Gaussian
metrics are explored to optimize the regression loss of the detector because of
their excellent parameter optimization mechanisms. Furthermore, we also use
Gaussian metrics for sampling to align label assignment and regression loss.
Experimental results on several public available datasets, DOTA, HRSC2016,
UCAS-AOD, and ICDAR2015 show the excellent performance of the proposed method
for arbitrary-oriented object detection. The code has been open sourced at
https://github.com/open-mmlab/mmrotate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Expression Transformer: A Computer Vision Approach for Symbolic Regression. (arXiv:2205.11798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11798">
<div class="article-summary-box-inner">
<span><p>Symbolic Regression (SR) is a type of regression analysis to automatically
find the mathematical expression that best fits the data. Currently, SR still
basically relies on various searching strategies so that a sample-specific
model is required to be optimized for every expression, which significantly
limits the model's generalization and efficiency. Inspired by the fact that
human beings can infer a mathematical expression based on the curve of it, we
propose Symbolic Expression Transformer (SET), a sample-agnostic model from the
perspective of computer vision for SR. Specifically, the collected data is
represented as images and an image caption model is employed for translating
images to symbolic expressions. A large-scale dataset without overlap between
training and testing sets in the image domain is released. Our results
demonstrate the effectiveness of SET and suggest the promising direction of
image-based model for solving the challenging SR problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Package Theft Detection from Smart Home Security Cameras. (arXiv:2205.11804v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11804">
<div class="article-summary-box-inner">
<span><p>Package theft detection has been a challenging task mainly due to lack of
training data and a wide variety of package theft cases in reality. In this
paper, we propose a new Global and Local Fusion Package Theft Detection
Embedding (GLF-PTDE) framework to generate package theft scores for each
segment within a video to fulfill the real-world requirements on package theft
detection. Moreover, we construct a novel Package Theft Detection dataset to
facilitate the research on this task. Our method achieves 80% AUC performance
on the newly proposed dataset, showing the effectiveness of the proposed
GLF-PTDE framework and its robustness in different real scenes for package
theft detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Assemble Geometric Shapes. (arXiv:2205.11809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11809">
<div class="article-summary-box-inner">
<span><p>Assembling parts into an object is a combinatorial problem that arises in a
variety of contexts in the real world and involves numerous applications in
science and engineering. Previous related work tackles limited cases with
identical unit parts or jigsaw-style parts of textured shapes, which greatly
mitigate combinatorial challenges of the problem. In this work, we introduce
the more challenging problem of shape assembly, which involves textureless
fragments of arbitrary shapes with indistinctive junctions, and then propose a
learning-based approach to solving it. We demonstrate the effectiveness on
shape assembly tasks with various scenarios, including the ones with abnormal
fragments (e.g., missing and distorted), the different number of fragments, and
different rotation discretization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thunder: Thumbnail based Fast Lightweight Image Denoising Network. (arXiv:2205.11823v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11823">
<div class="article-summary-box-inner">
<span><p>To achieve promising results on removing noise from real-world images, most
of existing denoising networks are formulated with complex network structure,
making them impractical for deployment. Some attempts focused on reducing the
number of filters and feature channels but suffered from large performance
loss, and a more practical and lightweight denoising network with fast
inference speed is of high demand.
</p>
<p>To this end, a \textbf{Thu}mb\textbf{n}ail based \textbf{D}\textbf{e}noising
Netwo\textbf{r}k dubbed Thunder, is proposed and implemented as a lightweight
structure for fast restoration without comprising the denoising capabilities.
Specifically, the Thunder model contains two newly-established modules:
</p>
<p>(1) a wavelet-based Thumbnail Subspace Encoder (TSE) which can leverage
sub-bands correlation to provide an approximate thumbnail based on the
low-frequent feature; (2) a Subspace Projection based Refine Module (SPR) which
can restore the details for thumbnail progressively based on the subspace
projection approach.
</p>
<p>Extensive experiments have been carried out on two real-world denoising
benchmarks, demonstrating that the proposed Thunder outperforms the existing
lightweight models and achieves competitive performance on PSNR and SSIM when
compared with the complex designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Difference Learning for Noisy Rigid Image Alignment. (arXiv:2205.11829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11829">
<div class="article-summary-box-inner">
<span><p>Rigid image alignment is a fundamental task in computer vision, while the
traditional algorithms are either too sensitive to noise or time-consuming.
Recent unsupervised image alignment methods developed based on spatial
transformer networks show an improved performance on clean images but will not
achieve satisfactory performance on noisy images due to its heavy reliance on
pixel value comparations. To handle such challenging applications, we report a
new unsupervised difference learning (UDL) strategy and apply it to rigid image
alignment. UDL exploits the quantitative properties of regression tasks and
converts the original unsupervised problem to pseudo supervised problem. Under
the new UDL-based image alignment pipeline, rotation can be accurately
estimated on both clean and noisy images and translations can then be easily
solved. Experimental results on both nature and cryo-EM images demonstrate the
efficacy of our UDL-based unsupervised rigid image alignment method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TraCon: A novel dataset for real-time traffic cones detection using deep learning. (arXiv:2205.11830v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11830">
<div class="article-summary-box-inner">
<span><p>Substantial progress has been made in the field of object detection in road
scenes. However, it is mainly focused on vehicles and pedestrians. To this end,
we investigate traffic cone detection, an object category crucial for road
effects and maintenance. In this work, the YOLOv5 algorithm is employed, in
order to find a solution for the efficient and fast detection of traffic cones.
The YOLOv5 can achieve a high detection accuracy with the score of IoU up to
91.31%. The proposed method is been applied to an RGB roadwork image dataset,
collected from various sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDFKD-MFS: Collaborative Data-free Knowledge Distillation via Multi-level Feature Sharing. (arXiv:2205.11845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11845">
<div class="article-summary-box-inner">
<span><p>Recently, the compression and deployment of powerful deep neural networks
(DNNs) on resource-limited edge devices to provide intelligent services have
become attractive tasks. Although knowledge distillation (KD) is a feasible
solution for compression, its requirement on the original dataset raises
privacy concerns. In addition, it is common to integrate multiple pretrained
models to achieve satisfactory performance. How to compress multiple models
into a tiny model is challenging, especially when the original data are
unavailable. To tackle this challenge, we propose a framework termed
collaborative data-free knowledge distillation via multi-level feature sharing
(CDFKD-MFS), which consists of a multi-header student module, an asymmetric
adversarial data-free KD module, and an attention-based aggregation module. In
this framework, the student model equipped with a multi-level feature-sharing
structure learns from multiple teacher models and is trained together with a
generator in an asymmetric adversarial manner. When some real samples are
available, the attention module adaptively aggregates predictions of the
student headers, which can further improve performance. We conduct extensive
experiments on three popular computer visual datasets. In particular, compared
with the most competitive alternative, the accuracy of the proposed framework
is 1.18\% higher on the CIFAR-100 dataset, 1.67\% higher on the Caltech-101
dataset, and 2.99\% higher on the mini-ImageNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative 3D Object Detection for Automatic Vehicle Systems via Learnable Communications. (arXiv:2205.11849v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11849">
<div class="article-summary-box-inner">
<span><p>Accurate detection of objects in 3D point clouds is a key problem in
autonomous driving systems. Collaborative perception can incorporate
information from spatially diverse sensors and provide significant benefits for
improving the perception accuracy of autonomous driving systems. In this work,
we consider that the autonomous vehicle uses local point cloud data and
combines information from neighboring infrastructures through wireless links
for cooperative 3D object detection. However, information sharing among vehicle
and infrastructures in predefined communication schemes may result in
communication congestion and/or bring limited performance improvement. To this
end, we propose a novel collaborative 3D object detection framework that
consists of three components: feature learning networks that map point clouds
into feature maps; an efficient communication block that propagates compact and
fine-grained query feature maps from vehicle to support infrastructures and
optimizes attention weights between query and key to refine support feature
maps; a region proposal network that fuses local feature maps and weighted
support feature maps for 3D object detection. We evaluate the performance of
the proposed framework using a synthetic cooperative dataset created in two
complex driving scenarios: a roundabout and a T-junction. Experiment results
and bandwidth usage analysis demonstrate that our approach can save
communication and computation costs and significantly improve detection
performance under different detection difficulties in all scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration. (arXiv:2205.11876v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11876">
<div class="article-summary-box-inner">
<span><p>Recent learning-based image fusion methods have marked numerous progress in
pre-registered multi-modality data, but suffered serious ghosts dealing with
misaligned multi-modality data, due to the spatial deformation and the
difficulty narrowing cross-modality discrepancy. To overcome the obstacles, in
this paper, we present a robust cross-modality generation-registration paradigm
for unsupervised misaligned infrared and visible image fusion (IVIF).
Specifically, we propose a Cross-modality Perceptual Style Transfer Network
(CPSTN) to generate a pseudo infrared image taking a visible image as input.
Benefiting from the favorable geometry preservation ability of the CPSTN, the
generated pseudo infrared image embraces a sharp structure, which is more
conducive to transforming cross-modality image alignment into mono-modality
registration coupled with the structure-sensitive of the infrared image. In
this case, we introduce a Multi-level Refinement Registration Network (MRRN) to
predict the displacement vector field between distorted and pseudo infrared
images and reconstruct registered infrared image under the mono-modality
setting. Moreover, to better fuse the registered infrared images and visible
images, we present a feature Interaction Fusion Module (IFM) to adaptively
select more meaningful features for fusion in the Dual-path Interaction Fusion
Network (DIFN). Extensive experimental results suggest that the proposed method
performs superior capability on misaligned cross-modality image fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Vectorization for Portrait Images. (arXiv:2205.11880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11880">
<div class="article-summary-box-inner">
<span><p>Aiming at developing intuitive and easy-to-use portrait editing tools, we
propose a novel vectorization method that can automatically convert raster
images into a 3-tier hierarchical representation. The base layer consists of a
set of sparse diffusion curves (DC) which characterize salient geometric
features and low-frequency colors and provide means for semantic color transfer
and facial expression editing. The middle level encodes specular highlights and
shadows to large and editable Poisson regions (PR) and allows the user to
directly adjust illumination via tuning the strength and/or changing shape of
PR. The top level contains two types of pixel-sized PRs for high-frequency
residuals and fine details such as pimples and pigmentation. We also train a
deep generative model that can produce high-frequency residuals automatically.
Thanks to the meaningful organization of vector primitives, editing portraits
becomes easy and intuitive. In particular, our method supports color transfer,
facial expression editing, highlight and shadow editing and automatic
retouching. Thanks to the linearity of the Laplace operator, we introduce alpha
blending, linear dodge and linear burn to vector editing and show that they are
effective in editing highlights and shadows. To quantitatively evaluate the
results, we extend the commonly used FLIP metric (which measures differences
between two images) by considering illumination. The new metric, called
illumination-sensitive FLIP or IS-FLIP, can effectively capture the salient
changes in color transfer results, and is more consistent with human perception
than FLIP and other quality measures on portrait images. We evaluate our method
on the FFHQR dataset and show that our method is effective for common portrait
editing tasks, such as retouching, light editing, color transfer and expression
editing. We will make the code and trained models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind The Gap: Alleviating Local Imbalance for Unsupervised Cross-Modality Medical Image Segmentation. (arXiv:2205.11888v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11888">
<div class="article-summary-box-inner">
<span><p>Unsupervised cross-modality medical image adaptation aims to alleviate the
severe domain gap between different imaging modalities without using the target
domain label. A key in this campaign relies upon aligning the distributions of
source and target domain. One common attempt is to enforce the global alignment
between two domains, which, however, ignores the fatal local-imbalance domain
gap problem, i.e., some local features with larger domain gap are harder to
transfer. Recently, some methods conduct alignment focusing on local regions to
improve the efficiency of model learning. While this operation may cause a
deficiency of critical information from contexts. To tackle this limitation, we
propose a novel strategy to alleviate the domain gap imbalance considering the
characteristics of medical images, namely Global-Local Union Alignment.
Specifically, a feature-disentanglement style-transfer module first synthesizes
the target-like source-content images to reduce the global domain gap. Then, a
local feature mask is integrated to reduce the 'inter-gap' for local features
by prioritizing those discriminative features with larger domain gap. This
combination of global and local alignment can precisely localize the crucial
regions in segmentation target while preserving the overall semantic
consistency. We conduct a series of experiments with two cross-modality
adaptation tasks, i,e. cardiac substructure and abdominal multi-organ
segmentation. Experimental results indicate that our method exceeds the SOTA
methods by 3.92% Dice score in MRI-CT cardiac segmentation and 3.33% in the
reverse direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An interpretation of the final fully connected layer. (arXiv:2205.11908v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11908">
<div class="article-summary-box-inner">
<span><p>In recent years neural networks have achieved state-of-the-art accuracy for
various tasks but the the interpretation of the generated outputs still remains
difficult. In this work we attempt to provide a method to understand the learnt
weights in the final fully connected layer in image classification models. We
motivate our method by drawing a connection between the policy gradient
objective in RL and supervised learning objective. We suggest that the commonly
used cross entropy based supervised learning objective can be regarded as a
special case of the policy gradient objective. Using this insight we propose a
method to find the most discriminative and confusing parts of an image. Our
method does not make any prior assumption about neural network achitecture and
has low computational cost. We apply our method on publicly available
pre-trained models and report the generated results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust 3D Object Detection in Cold Weather Conditions. (arXiv:2205.11925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11925">
<div class="article-summary-box-inner">
<span><p>Adverse weather conditions can negatively affect LiDAR-based object
detectors. In this work, we focus on the phenomenon of vehicle gas exhaust
condensation in cold weather conditions. This everyday effect can influence the
estimation of object sizes, orientations and introduce ghost object detections,
compromising the reliability of the state of the art object detectors. We
propose to solve this problem by using data augmentation and a novel training
loss term. To effectively train deep neural networks, a large set of labeled
data is needed. In case of adverse weather conditions, this process can be
extremely laborious and expensive. We address this issue in two steps: First,
we present a gas exhaust data generation method based on 3D surface
reconstruction and sampling which allows us to generate large sets of gas
exhaust clouds from a small pool of labeled data. Second, we introduce a point
cloud augmentation process that can be used to add gas exhaust to datasets
recorded in good weather conditions. Finally, we formulate a new training loss
term that leverages the augmented point cloud to increase object detection
robustness by penalizing predictions that include noise. In contrast to other
works, our method can be used with both grid-based and point-based detectors.
Moreover, since our approach does not require any network architecture changes,
inference times remain unchanged. Experimental results on real data show that
our proposed method greatly increases robustness to gas exhaust and noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Trinarization Using a Partial Differential Equations: A Novel Approach to Automatic Sperm Image Analysis. (arXiv:2205.11927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11927">
<div class="article-summary-box-inner">
<span><p>Partial differential equations have recently garnered substantial attention
as an image processing framework due to their extensibility, the ability to
rigorously engineer and analyse the governing dynamics as well as the ease of
implementation using numerical methods. This paper explores a novel approach to
image trinarization with a concrete real-world application of classifying
regions of sperm images used in the automatic analysis of sperm morphology. The
proposed methodology engineers a diffusion equation with non-linear source
term, exhibiting three steady-states. The model is implemented as an image
processor using a standard finite difference method to illustrate the efficacy
of the proposed approach. The performance of the proposed approach is
benchmarked against standard image clustering/segmentation methods and shown to
be highly effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraSens: A Gabor Residual Anti-aliasing Sensing Framework for Action Recognition using WiFi. (arXiv:2205.11945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11945">
<div class="article-summary-box-inner">
<span><p>WiFi-based human action recognition (HAR) has been regarded as a promising
solution in applications such as smart living and remote monitoring due to the
pervasive and unobtrusive nature of WiFi signals. However, the efficacy of WiFi
signals is prone to be influenced by the change in the ambient environment and
varies over different sub-carriers. To remedy this issue, we propose an
end-to-end Gabor residual anti-aliasing sensing network (GraSens) to directly
recognize the actions using the WiFi signals from the wireless devices in
diverse scenarios. In particular, a new Gabor residual block is designed to
address the impact of the changing surrounding environment with a focus on
learning reliable and robust temporal-frequency representations of WiFi
signals. In each block, the Gabor layer is integrated with the anti-aliasing
layer in a residual manner to gain the shift-invariant features. Furthermore,
fractal temporal and frequency self-attention are proposed in a joint effort to
explicitly concentrate on the efficacy of WiFi signals and thus enhance the
quality of output features scattered in different subcarriers. Experimental
results throughout our wireless-vision action recognition dataset (WVAR) and
three public datasets demonstrate that our proposed GraSens scheme outperforms
state-of-the-art methods with respect to recognition accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHARP: Shape-Aware Reconstruction of People in Loose Clothing. (arXiv:2205.11948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11948">
<div class="article-summary-box-inner">
<span><p>Recent advancements in deep learning have enabled 3D human body
reconstruction from a monocular image, which has broad applications in multiple
domains. In this paper, we propose SHARP (SHape Aware Reconstruction of People
in loose clothing), a novel end-to-end trainable network that accurately
recovers the 3D geometry and appearance of humans in loose clothing from a
monocular image. SHARP uses a sparse and efficient fusion strategy to combine
parametric body prior with a non-parametric 2D representation of clothed
humans. The parametric body prior enforces geometrical consistency on the body
shape and pose, while the non-parametric representation models loose clothing
and handle self-occlusions as well. We also leverage the sparseness of the
non-parametric representation for faster training of our network while using
losses on 2D maps. Another key contribution is 3DHumans, our new life-like
dataset of 3D human body scans with rich geometrical and textural details. We
evaluate SHARP on 3DHumans and other publicly available datasets and show
superior qualitative and quantitative performance than existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffuse Map Guiding Unsupervised Generative Adversarial Network for SVBRDF Estimation. (arXiv:2205.11951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11951">
<div class="article-summary-box-inner">
<span><p>Reconstructing materials in the real world has always been a difficult
problem in computer graphics. Accurately reconstructing the material in the
real world is critical in the field of realistic rendering. Traditionally,
materials in computer graphics are mapped by an artist, then mapped onto a
geometric model by coordinate transformation, and finally rendered with a
rendering engine to get realistic materials. For opaque objects, the industry
commonly uses physical-based bidirectional reflectance distribution function
(BRDF) rendering models for material modeling. The commonly used physical-based
rendering models are Cook-Torrance BRDF, Disney BRDF. In this paper, we use the
Cook-Torrance model to reconstruct the materials. The SVBRDF material
parameters include Normal, Diffuse, Specular and Roughness. This paper presents
a Diffuse map guiding material estimation method based on the Generative
Adversarial Network(GAN). This method can predict plausible SVBRDF maps with
global features using only a few pictures taken by the mobile phone. The main
contributions of this paper are: 1) We preprocess a small number of input
pictures to produce a large number of non-repeating pictures for training to
reduce over-fitting. 2) We use a novel method to directly obtain the guessed
diffuse map with global characteristics, which provides more prior information
for the training process. 3) We improve the network architecture of the
generator so that it can generate fine details of normal maps and reduce the
possibility to generate over-flat normal maps. The method used in this paper
can obtain prior knowledge without using dataset training, which greatly
reduces the difficulty of material reconstruction and saves a lot of time to
generate and calibrate datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D helical CT reconstruction with memory efficient invertible Learned Primal-Dual method. (arXiv:2205.11952v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11952">
<div class="article-summary-box-inner">
<span><p>Helical acquisition geometry is the most common geometry used in computed
tomography (CT) scanners for medical imaging. We adapt the invertible Learned
Primal-Dual (iLPD) deep neural network architecture so that it can be applied
to helical 3D CT reconstruction. We achieve this by splitting the geometry and
the data in parts that fit the memory and by splitting images into
corresponding sub-volumes. The architecture can be applied to images different
in size along the rotation axis. We perform the experiments on tomographic data
simulated from realistic helical geometries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Wireless-Vision Dataset for Privacy Preserving Human Activity Recognition. (arXiv:2205.11962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11962">
<div class="article-summary-box-inner">
<span><p>Human Activity Recognition (HAR) has recently received remarkable attention
in numerous applications such as assisted living and remote monitoring.
Existing solutions based on sensors and vision technologies have obtained
achievements but still suffering from considerable limitations in the
environmental requirement. Wireless signals like WiFi-based sensing have
emerged as a new paradigm since it is convenient and not restricted in the
environment. In this paper, a new WiFi-based and video-based neural network
(WiNN) is proposed to improve the robustness of activity recognition where the
synchronized video serves as the supplement for the wireless data. Moreover, a
wireless-vision benchmark (WiVi) is collected for 9 class actions recognition
in three different visual conditions, including the scenes without occlusion,
with partial occlusion, and with full occlusion. Both machine learning methods
- support vector machine (SVM) as well as deep learning methods are used for
the accuracy verification of the data set. Our results show that WiVi data set
satisfies the primary demand and all three branches in the proposed pipeline
keep more than $80\%$ of activity recognition accuracy over multiple action
segmentation from 1s to 3s. In particular, WiNN is the most robust method in
terms of all the actions on three action segmentation compared to the others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Models for Reproducible Coronary Calcium Scoring. (arXiv:2205.11967v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11967">
<div class="article-summary-box-inner">
<span><p>Purpose: Coronary artery calcium (CAC) score, i.e. the amount of CAC
quantified in CT, is a strong and independent predictor of coronary heart
disease (CHD) events. However, CAC scoring suffers from limited interscan
reproducibility, which is mainly due to the clinical definition requiring
application of a fixed intensity level threshold for segmentation of
calcifications. This limitation is especially pronounced in
non-ECG-synchronized CT where lesions are more impacted by cardiac motion and
partial volume effects. Therefore, we propose a CAC quantification method that
does not require a threshold for segmentation of CAC. Approach: Our method
utilizes a generative adversarial network where a CT with CAC is decomposed
into an image without CAC and an image showing only CAC. The method, using a
CycleGAN, was trained using 626 low-dose chest CTs and 514 radiotherapy
treatment planning CTs. Interscan reproducibility was compared to clinical
calcium scoring in radiotherapy treatment planning CTs of 1,662 patients, each
having two scans. Results: A lower relative interscan difference in CAC mass
was achieved by the proposed method: 47% compared to 89% manual clinical
calcium scoring. The intraclass correlation coefficient of Agatston scores was
0.96 for the proposed method compared to 0.91 for automatic clinical calcium
scoring. Conclusions: The increased interscan reproducibility achieved by our
method may lead to increased reliability of CHD risk categorization and
improved accuracy of CHD event prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPOM: Customized Invisible Cloak towards Face Privacy Protection. (arXiv:2205.11981v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11981">
<div class="article-summary-box-inner">
<span><p>While convenient in daily life, face recognition technologies also raise
privacy concerns for regular users on the social media since they could be used
to analyze face images and videos, efficiently and surreptitiously without any
security restrictions. In this paper, we investigate the face privacy
protection from a technology standpoint based on a new type of customized
cloak, which can be applied to all the images of a regular user, to prevent
malicious face recognition systems from uncovering their identity.
Specifically, we propose a new method, named one person one mask (OPOM), to
generate person-specific (class-wise) universal masks by optimizing each
training sample in the direction away from the feature subspace of the source
identity. To make full use of the limited training images, we investigate
several modeling methods, including affine hulls, class centers, and convex
hulls, to obtain a better description of the feature subspace of source
identities. The effectiveness of the proposed method is evaluated on both
common and celebrity datasets against black-box face recognition models with
different loss functions and network architectures. In addition, we discuss the
advantages and potential problems of the proposed method. In particular, we
conduct an application study on the privacy protection of a video dataset,
Sherlock, to demonstrate the potential practical usage of the proposed method.
Datasets and code are available at https://github.com/zhongyy/OPOM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections. (arXiv:2205.12005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12005">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained foundation models have been an emerging paradigm for
building artificial intelligence (AI) systems, which can be quickly adapted to
a wide range of downstream tasks. This paper presents mPLUG, a new
vision-language foundation model for both cross-modal understanding and
generation. Most existing pre-trained models suffer from the problems of low
computational efficiency and information asymmetry brought by the long visual
sequence in cross-modal alignment. To address these problems, mPLUG introduces
an effective and efficient vision-language architecture with novel cross-modal
skip-connections, which creates inter-layer shortcuts that skip a certain
number of layers for time-consuming full self-attention on the vision side.
mPLUG is pre-trained end-to-end on large-scale image-text pairs with both
discriminative and generative objectives. It achieves state-of-the-art results
on a wide range of vision-language downstream tasks, such as image captioning,
image-text retrieval, visual grounding and visual question answering. mPLUG
also demonstrates strong zero-shot transferability when directly transferred to
multiple video-language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SFace: Sigmoid-Constrained Hypersphere Loss for Robust Face Recognition. (arXiv:2205.12010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12010">
<div class="article-summary-box-inner">
<span><p>Deep face recognition has achieved great success due to large-scale training
databases and rapidly developing loss functions. The existing algorithms devote
to realizing an ideal idea: minimizing the intra-class distance and maximizing
the inter-class distance. However, they may neglect that there are also low
quality training images which should not be optimized in this strict way.
Considering the imperfection of training databases, we propose that intra-class
and inter-class objectives can be optimized in a moderate way to mitigate
overfitting problem, and further propose a novel loss function, named
sigmoid-constrained hypersphere loss (SFace). Specifically, SFace imposes
intra-class and inter-class constraints on a hypersphere manifold, which are
controlled by two sigmoid gradient re-scale functions respectively. The sigmoid
curves precisely re-scale the intra-class and inter-class gradients so that
training samples can be optimized to some degree. Therefore, SFace can make a
better balance between decreasing the intra-class distances for clean examples
and preventing overfitting to the label noise, and contributes more robust deep
face recognition models. Extensive experiments of models trained on
CASIA-WebFace, VGGFace2, and MS-Celeb-1M databases, and evaluated on several
face recognition benchmarks, such as LFW, MegaFace and IJB-C databases, have
demonstrated the superiority of SFace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Naive Few-Shot Learning: Sequence Consistency Evaluation. (arXiv:2205.12013v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12013">
<div class="article-summary-box-inner">
<span><p>Cognitive psychologists often use the term $\textit{fluid intelligence}$ to
describe the ability of humans to solve novel tasks without any prior training.
In contrast to humans, deep neural networks can perform cognitive tasks only
after extensive (pre-)training with a large number of relevant examples.
Motivated by fluid intelligence research in the cognitive sciences, we built a
benchmark task which we call sequence consistency evaluation (SCE) that can be
used to address this gap. Solving the SCE task requires the ability to extract
simple rules from sequences, a basic computation that is required for solving
various intelligence tests in humans. We tested $\textit{untrained}$ (naive)
deep learning models in the SCE task. Specifically, we compared Relation
Networks (RN) and Contrastive Predictive Coding (CPC), two models that can
extract simple rules from sequences, and found that the latter, which imposes a
structure on the predictable rule does better. We further found that simple
networks fare better in this task than complex ones. Finally, we show that this
approach can be used for security camera anomaly detection without any prior
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance. (arXiv:2205.12022v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12022">
<div class="article-summary-box-inner">
<span><p>With the rapid development of the Metaverse, virtual humans have emerged, and
human image synthesis and editing techniques, such as pose transfer, have
recently become popular. Most of the existing techniques rely on GANs, which
can generate good human images even with large variants and occlusions. But
from our best knowledge, the existing state-of-the-art method still has the
following problems: the first is that the rendering effect of the synthetic
image is not realistic, such as poor rendering of some regions. And the second
is that the training of GAN is unstable and slow to converge, such as model
collapse. Based on the above two problems, we propose several methods to solve
them. To improve the rendering effect, we use the Residual Fast Fourier
Transform Block to replace the traditional Residual Block. Then, spectral
normalization and Wasserstein distance are used to improve the speed and
stability of GAN training. Experiments demonstrate that the methods we offer
are effective at solving the problems listed above, and we get state-of-the-art
scores in LPIPS and PSNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Gender, Pose and Camera Distance on Human Body Dimensions Estimation. (arXiv:2205.12028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12028">
<div class="article-summary-box-inner">
<span><p>Human Body Dimensions Estimation (HBDE) is a task that an intelligent agent
can perform to attempt to determine human body information from images (2D) or
point clouds or meshes (3D). More specifically, if we define the HBDE problem
as inferring human body measurements from images, then HBDE is a difficult,
inverse, multi-task regression problem that can be tackled with machine
learning techniques, particularly convolutional neural networks (CNN). Despite
the community's tremendous effort to advance human shape analysis, there is a
lack of systematic experiments to assess CNNs estimation of human body
dimensions from images. Our contribution lies in assessing a CNN estimation
performance in a series of controlled experiments. To that end, we augment our
recently published neural anthropometer dataset by rendering images with
different camera distance. We evaluate the network inference absolute and
relative mean error between the estimated and actual HBDs. We train and
evaluate the CNN in four scenarios: (1) training with subjects of a specific
gender, (2) in a specific pose, (3) sparse camera distance and (4) dense camera
distance. Not only our experiments demonstrate that the network can perform the
task successfully, but also reveal a number of relevant facts that contribute
to better understand the task of HBDE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification. (arXiv:2205.12029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12029">
<div class="article-summary-box-inner">
<span><p>Multimodal learning from document data has achieved great success lately as
it allows to pre-train semantically meaningful features as a prior into a
learnable downstream approach. In this paper, we approach the document
classification problem by learning cross-modal representations through language
and vision cues, considering intra- and inter-modality relationships. Instead
of merging features from different modalities into a common representation
space, the proposed method exploits high-level interactions and learns relevant
semantic information from effective attention flows within and across
modalities. The proposed learning objective is devised between intra- and
inter-modality alignment tasks, where the similarity distribution per task is
computed by contracting positive sample pairs while simultaneously contrasting
negative ones in the common feature representation space}. Extensive
experiments on public document classification datasets demonstrate the
effectiveness and the generalization capacity of our model on both low-scale
and large-scale datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Image Classification Using Vision Transformer. (arXiv:2205.12041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12041">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a privacy-preserving image classification method
that is based on the combined use of encrypted images and the vision
transformer (ViT). The proposed method allows us not only to apply images
without visual information to ViT models for both training and testing but to
also maintain a high classification accuracy. ViT utilizes patch embedding and
position embedding for image patches, so this architecture is shown to reduce
the influence of block-wise image transformation. In an experiment, the
proposed method for privacy-preserving image classification is demonstrated to
outperform state-of-the-art methods in terms of classification accuracy and
robustness against various attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Efficient CNNS: Tweaking the Nuts and Bolts of Neural Networks for Lighter, Faster and Robust Models. (arXiv:2205.12050v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12050">
<div class="article-summary-box-inner">
<span><p>Deep Learning has revolutionized the fields of computer vision, natural
language understanding, speech recognition, information retrieval and more.
Many techniques have evolved over the past decade that made models lighter,
faster, and robust with better generalization. However, many deep learning
practitioners persist with pre-trained models and architectures trained mostly
on standard datasets such as Imagenet, MS-COCO, IMDB-Wiki Dataset, and
Kinetics-700 and are either hesitant or unaware of redesigning the architecture
from scratch that will lead to better performance. This scenario leads to
inefficient models that are not suitable on various devices such as mobile,
edge, and fog. In addition, these conventional training methods are of concern
as they consume a lot of computing power. In this paper, we revisit various
SOTA techniques that deal with architecture efficiency (Global Average Pooling,
depth-wise convolutions &amp; squeeze and excitation, Blurpool), learning rate
(Cyclical Learning Rate), data augmentation (Mixup, Cutout), label manipulation
(label smoothing), weight space manipulation (stochastic weight averaging), and
optimizer (sharpness aware minimization). We demonstrate how an efficient deep
convolution network can be built in a phased manner by sequentially reducing
the number of training parameters and using the techniques mentioned above. We
achieved a SOTA accuracy of 99.2% on MNIST data with just 1500 parameters and
an accuracy of 86.01% with just over 140K parameters on the CIFAR-10 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Attention Network for Skeleton Extraction. (arXiv:2205.12066v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12066">
<div class="article-summary-box-inner">
<span><p>Skeleton extraction is a task focused on providing a simple representation of
an object by extracting the skeleton from the given binary or RGB image. In
recent years many attractive works in skeleton extraction have been made. But
as far as we know, there is little research on how to utilize the context
information in the binary shape of objects. In this paper, we propose an
attention-based model called Context Attention Network (CANet), which
integrates the context extraction module in a UNet architecture and can
effectively improve the ability of network to extract the skeleton pixels.
Meanwhile, we also use some novel techniques including distance transform,
weight focal loss to achieve good results on the given dataset. Finally,
without model ensemble and with only 80% of the training images, our method
achieves 0.822 F1 score during the development phase and 0.8507 F1 score during
the final phase of the Pixel SkelNetOn Competition, ranking 1st place on the
leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Phonological Parameters in Sign Languages. (arXiv:2205.12072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12072">
<div class="article-summary-box-inner">
<span><p>Signers compose sign language phonemes that enable communication by combining
phonological parameters such as handshape, orientation, location, movement, and
non-manual features. Linguistic research often breaks down signs into their
constituent parts to study sign languages and often a lot of effort is invested
into the annotation of the videos. In this work we show how a single model can
be used to recognise the individual phonological parameters within sign
languages with the aim of either to assist linguistic annotations or to
describe the signs for the sign recognition models. We use Danish Sign Language
data set `Ordbog over Dansk Tegnsprog' to generate multiple data sets using
pose estimation model, which are then used for training the multi-label Fast
R-CNN model to support multi-label modelling. Moreover, we show that there is a
significant co-dependence between the orientation and location phonological
parameters in the generated data and we incorporate this co-dependence in the
model to achieve better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution. (arXiv:2205.12089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12089">
<div class="article-summary-box-inner">
<span><p>Service robots should be able to interact naturally with non-expert human
users, not only to help them in various tasks but also to receive guidance in
order to resolve ambiguities that might be present in the instruction. We
consider the task of visual grounding, where the agent segments an object from
a crowded scene given a natural language description. Modern holistic
approaches to visual grounding usually ignore language structure and struggle
to cover generic domains, therefore relying heavily on large datasets.
Additionally, their transfer performance in RGB-D datasets suffers due to high
visual discrepancy between the benchmark and the target domains. Modular
approaches marry learning with domain modeling and exploit the compositional
nature of language to decouple visual representation from language parsing, but
either rely on external parsers or are trained in an end-to-end fashion due to
the lack of strong supervision. In this work, we seek to tackle these
limitations by introducing a fully decoupled modular framework for
compositional visual grounding of entities, attributes, and spatial relations.
We exploit rich scene graph annotations generated in a synthetic domain and
train each module independently. Our approach is evaluated both in simulation
and in two real RGB-D scene datasets. Experimental results show that the
decoupled nature of our framework allows for easy integration with domain
adaptation approaches for Sim-To-Real visual recognition, offering a
data-efficient, robust, and interpretable solution to visual grounding in
robotic applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval. (arXiv:2205.12105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12105">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of vision-language pre-training (VLP)
has brought cross-modal retrieval to a new era. However, due to the latency and
computation demand, it is commonly challenging to apply VLP in a real-time
online retrieval system. To alleviate the defect, this paper proposes a
\textbf{Hi}erarchical \textbf{V}ision-\textbf{}Language \textbf{P}re-Training
(\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a
novel hierarchical retrieval objective, which uses the representation of
different dimensions for coarse-to-fine ITR, i.e., using low-dimensional
representation for large-scale coarse retrieval and high-dimensional
representation for small-scale fine retrieval. We evaluate our proposed HiVLP
on two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.
Extensive experiments demonstrate that our HiVLP not only has fast inference
speed but also can be easily scaled to large-scale ITR scenarios. The detailed
results show that HiVLP is $1,427$$\sim$$120,649\times$ faster than the
fusion-based model UNITER and 2$\sim$5 faster than the fastest embedding-based
model LightingDot in different candidate scenarios. It also achieves about +4.9
AR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable
performance with the state-of-the-art (SOTA) fusion-based model METER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Drive Using Sparse Imitation Reinforcement Learning. (arXiv:2205.12128v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12128">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Sparse Imitation Reinforcement Learning (SIRL), a
hybrid end-to-end control policy that combines the sparse expert driving
knowledge with reinforcement learning (RL) policy for autonomous driving (AD)
task in CARLA simulation environment. The sparse expert is designed based on
hand-crafted rules which is suboptimal but provides a risk-averse strategy by
enforcing experience for critical scenarios such as pedestrian and vehicle
avoidance, and traffic light detection. As it has been demonstrated, training a
RL agent from scratch is data-inefficient and time consuming particularly for
the urban driving task, due to the complexity of situations stemming from the
vast size of state space. Our SIRL strategy provides a solution to solve these
problems by fusing the output distribution of the sparse expert policy and the
RL policy to generate a composite driving policy. With the guidance of the
sparse expert during the early training stage, SIRL strategy accelerates the
training process and keeps the RL exploration from causing a catastrophe
outcome, and ensures safe exploration. To some extent, the SIRL agent is
imitating the driving expert's behavior. At the same time, it continuously
gains knowledge during training therefore it keeps making improvement beyond
the sparse expert, and can surpass both the sparse expert and a traditional RL
agent. We experimentally validate the efficacy of proposed SIRL approach in a
complex urban scenario within the CARLA simulator. Besides, we compare the SIRL
agent's performance for risk-averse exploration and high learning efficiency
with the traditional RL approach. We additionally demonstrate the SIRL agent's
generalization ability to transfer the driving skill to unseen environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full-Reference Calibration-Free Image Quality Assessment. (arXiv:2205.12129v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12129">
<div class="article-summary-box-inner">
<span><p>One major problem of objective Image Quality Assessment (IQA) methods is the
lack of linearity of their quality estimates with respect to scores expressed
by human subjects. For this reason, usually IQA metrics undergo a calibration
process based on subjective quality examples. However, example-based training
makes generalization problematic, hampering result comparison across different
applications and operative conditions. In this paper, new Full Reference (FR)
techniques, providing estimates linearly correlated with human scores without
using calibration are introduced. To reach this objective, these techniques are
deeply rooted on principles and theoretical constraints. Restricting the
interest on the IQA of the set of natural images, it is first recognized that
application of estimation theory and psycho physical principles to images
degraded by Gaussian blur leads to a so-called canonical IQA method, whose
estimates are not only highly linearly correlated to subjective scores, but are
also straightforwardly related to the Viewing Distance (VD). Then, it is shown
that mainstream IQA methods can be reconducted to the canonical method applying
a preliminary metric conversion based on a unique specimen image. The
application of this scheme is then extended to a significant class of degraded
images other than Gaussian blur, including noisy and compressed images. The
resulting calibration-free FR IQA methods are suited for applications where
comparability and interoperability across different imaging systems and on
different VDs is a major requirement. A comparison of their statistical
performance with respect to some conventional calibration prone methods is
finally provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Deforestation from Sentinel-1 Data in the Absence of Reliable Reference Data. (arXiv:2205.12131v1 [stat.ME])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12131">
<div class="article-summary-box-inner">
<span><p>Forests are vital for the wellbeing of our planet. Large and small scale
deforestation across the globe is threatening the stability of our climate,
forest biodiversity, and therefore the preservation of fragile ecosystems and
our natural habitat as a whole. With increasing public interest in climate
change issues and forest preservation, a large demand for carbon offsetting,
carbon footprint ratings, and environmental impact assessments is emerging.
Most often, deforestation maps are created from optical data such as Landsat
and MODIS. These maps are not typically available at less than annual intervals
due to persistent cloud cover in many parts of the world, especially the
tropics where most of the world's forest biomass is concentrated. Synthetic
Aperture Radar (SAR) can fill this gap as it penetrates clouds. We propose and
evaluate a novel method for deforestation detection in the absence of reliable
reference data which often constitutes the largest practical hurdle. This
method achieves a change detection sensitivity (producer's accuracy) of 96.5%
in the study area, although false positives lead to a lower user's accuracy of
about 75.7%, with a total balanced accuracy of 90.4%. The change detection
accuracy is maintained when adding up to 20% noise to the reference labels.
While further work is required to reduce the false positive rate, improve
detection delay, and validate this method in additional circumstances, the
results show that Sentinel-1 data have the potential to advance the timeliness
of global deforestation monitoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Latent Space of Image Style Transfer. (arXiv:2205.12135v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12135">
<div class="article-summary-box-inner">
<span><p>Existing neural style transfer researches have studied to match statistical
information between the deep features of content and style images, which were
extracted by a pre-trained VGG, and achieved significant improvement in
synthesizing artistic images. However, in some cases, the feature statistics
from the pre-trained encoder may not be consistent with the visual style we
perceived. For example, the style distance between images of different styles
is less than that of the same style. In such an inappropriate latent space, the
objective function of the existing methods will be optimized in the wrong
direction, resulting in bad stylization results. In addition, the lack of
content details in the features extracted by the pre-trained encoder also leads
to the content leak problem. In order to solve these issues in the latent space
used by style transfer, we propose two contrastive training schemes to get a
refined encoder that is more suitable for this task. The style contrastive loss
pulls the stylized result closer to the same visual style image and pushes it
away from the content image. The content contrastive loss enables the encoder
to retain more available details. We can directly add our training scheme to
some existing style transfer methods and significantly improve their results.
Extensive experimental results demonstrate the effectiveness and superiority of
our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis. (arXiv:2205.12144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12144">
<div class="article-summary-box-inner">
<span><p>The increasingly stringent data privacy regulations limit the development of
person re-identification (ReID) because person ReID training requires
centralizing an enormous amount of data that contains sensitive personal
information. To address this problem, we introduce federated person
re-identification (FedReID) -- implementing federated learning, an emerging
distributed training method, to person ReID. FedReID preserves data privacy by
aggregating model updates, instead of raw data, from clients to a central
server. Furthermore, we optimize the performance of FedReID under statistical
heterogeneity via benchmark analysis. We first construct a benchmark with an
enhanced algorithm, two architectures, and nine person ReID datasets with large
variances to simulate the real-world statistical heterogeneity. The benchmark
results present insights and bottlenecks of FedReID under statistical
heterogeneity, including challenges in convergence and poor performance on
datasets with large volumes. Based on these insights, we propose three
optimization approaches: (1) We adopt knowledge distillation to facilitate the
convergence of FedReID by better transferring knowledge from clients to the
server; (2) We introduce client clustering to improve the performance of large
datasets by aggregating clients with similar data distributions; (3) We propose
cosine distance weight to elevate performance by dynamically updating the
weights for aggregation depending on how well models are trained in clients.
Extensive experiments demonstrate that these approaches achieve satisfying
convergence with much better performance on all datasets. We believe that
FedReID will shed light on implementing and optimizing federated learning on
more computer vision applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StylizedNeRF: Consistent 3D Scene Stylization as Stylized NeRF via 2D-3D Mutual Learning. (arXiv:2205.12183v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12183">
<div class="article-summary-box-inner">
<span><p>3D scene stylization aims at generating stylized images of the scene from
arbitrary novel views following a given set of style examples, while ensuring
consistency when rendered from different views. Directly applying methods for
image or video stylization to 3D scenes cannot achieve such consistency. Thanks
to recently proposed neural radiance fields (NeRF), we are able to represent a
3D scene in a consistent way. Consistent 3D scene stylization can be
effectively achieved by stylizing the corresponding NeRF. However, there is a
significant domain gap between style examples which are 2D images and NeRF
which is an implicit volumetric representation. To address this problem, we
propose a novel mutual learning framework for 3D scene stylization that
combines a 2D image stylization network and NeRF to fuse the stylization
ability of 2D stylization network with the 3D consistency of NeRF. We first
pre-train a standard NeRF of the 3D scene to be stylized and replace its color
prediction module with a style network to obtain a stylized NeRF.It is followed
by distilling the prior knowledge of spatial consistency from NeRF to the 2D
stylization network through an introduced consistency loss. We also introduce a
mimic loss to supervise the mutual learning of the NeRF style module and
fine-tune the 2D stylization decoder. In order to further make our model handle
ambiguities of 2D stylization results, we introduce learnable latent codes that
obey the probability distributions conditioned on the style. They are attached
to training samples as conditional inputs to better learn the style module in
our novel stylized NeRF. Experimental results demonstrate that our method is
superior to existing approaches in both visual quality and long-range
consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization. (arXiv:2205.12191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12191">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data
have demonstrated strong performance on various tasks such as image captioning
and visual question answering (VQA). The quality of such models is commonly
assessed by measuring their performance on unseen data that typically comes
from the same distribution as the training data. However, we observe that these
models exhibit poor out-of-distribution (OOD) generalization on the task of
VQA. To better understand the underlying causes of poor generalization, we
comprehensively investigate performance of two pretrained V&amp;L models under
different settings (i.e. classification and open-ended text generation) by
conducting cross-dataset evaluations. We find that these models tend to learn
to solve the benchmark, rather than learning the high-level skills required by
the VQA task. We also argue that in most cases generative models are less
susceptible to shifts in data distribution, while frequently performing better
on our tested benchmarks. Moreover, we find that multimodal pretraining
improves OOD performance in most settings. Finally, we revisit assumptions
underlying the use of automatic VQA evaluation metrics, and empirically show
that their stringent nature repeatedly penalizes models for correct responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Absolute Triangulation Algorithms for Space Exploration. (arXiv:2205.12197v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12197">
<div class="article-summary-box-inner">
<span><p>Images are an important source of information for spacecraft navigation and
for three-dimensional reconstruction of observed space objects. Both of these
applications take the form of a triangulation problem when the camera has a
known attitude and the measurements extracted from the image are line of sight
(LOS) directions. This work provides a comprehensive review of the history and
theoretical foundations of triangulation. A variety of classical triangulation
algorithms are reviewed, including a number of suboptimal linear methods (many
LOS measurements) and the optimal method of Hartley and Sturm (only two LOS
measurements). Two new optimal non-iterative triangulation algorithms are
introduced that provide the same solution as Hartley and Sturm. The optimal
two-measurement case can be solved as a quadratic equation in many common
situations. The optimal many-measurement case may be solved without iteration
as a linear system using the new Linear Optimal Sine Triangulation (LOST)
method. The various triangulation algorithms are assessed with a few numerical
examples, including planetary terrain relative navigation, angles-only optical
navigation at Uranus, 3-D reconstruction of Notre-Dame de Paris, and
angles-only relative navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aerial Vision-and-Dialog Navigation. (arXiv:2205.12219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12219">
<div class="article-summary-box-inner">
<span><p>The ability to converse with humans and follow commands in natural language
is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can
relieve people's burden of holding a controller all the time, allow
multitasking, and make drone control more accessible for people with
disabilities or with their hands occupied. To this end, we introduce Aerial
Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language
conversation. We build a drone simulator with a continuous photorealistic
environment and collect a new AVDN dataset of over 3k recorded navigation
trajectories with asynchronous human-human dialogs between commanders and
followers. The commander provides initial navigation instruction and further
guidance by request, while the follower navigates the drone in the simulator
and asks questions when needed. During data collection, followers' attention on
the drone's visual observation is also recorded. Based on the AVDN dataset, we
study the tasks of aerial navigation from (full) dialog history and propose an
effective Human Attention Aided (HAA) baseline model, which learns to predict
both navigation waypoints and human attention. Dataset and code will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLOBUS: GLObal Building heights for Urban Studies. (arXiv:2205.12224v1 [physics.ao-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12224">
<div class="article-summary-box-inner">
<span><p>Urban weather and climate studies continue to be important as extreme events
cause economic loss and impact public health. Weather models seek to represent
urban areas but are oversimplified due to data availability, especially
building information. This paper introduces a novel Level of Detail-1 (LoD-1)
building dataset derived from a Deep Neural Network (DNN) called GLObal
Building heights for Urban Studies (GLOBUS). GLOBUS uses open-source datasets
as predictors: Advanced Land Observation Satellite (ALOS) Digital Surface Model
(DSM) normalized using Shuttle Radar Topography Mission (SRTM) Digital
Elevation Model (DEM), Landscan population density, and building footprints.
The building information from GLOBUS can be ingested in Numerical Weather
Prediction (NWP) and urban energy-water balance models to study localized
phenomena such as the Urban Heat Island (UHI) effect. GLOBUS has been trained
and validated using the United States Geological Survey (USGS) 3DEP Light
Detection and Ranging (LiDAR) data. We used data from 5 US cities for training
and the model was validated over 6 cities. Performance metrics are computed at
a spatial resolution of 300-meter. The Root Mean Squared Error (RMSE) and Mean
Absolute Percentage Error (MAPE) were 5.15 meters and 28.8 %, respectively. The
standard deviation and histogram of building heights over a 300-meter grid are
well represented using GLOBUS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions. (arXiv:2205.12231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12231">
<div class="article-summary-box-inner">
<span><p>We present ASSET, a neural architecture for automatically modifying an input
high-resolution image according to a user's edits on its semantic segmentation
map. Our architecture is based on a transformer with a novel attention
mechanism. Our key idea is to sparsify the transformer's attention matrix at
high resolutions, guided by dense attention extracted at lower image
resolutions. While previous attention mechanisms are computationally too
expensive for handling high-resolution images or are overly constrained within
specific image regions hampering long-range interactions, our novel attention
mechanism is both computationally efficient and effective. Our sparsified
attention mechanism is able to capture long-range interactions and context,
leading to synthesizing interesting phenomena in scenes, such as reflections of
landscapes onto water or flora consistent with the rest of the landscape, that
were not possible to generate reliably with previous convnets and transformer
approaches. We present qualitative and quantitative results, along with user
studies, demonstrating the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gacs-Korner Common Information Variational Autoencoder. (arXiv:2205.12239v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12239">
<div class="article-summary-box-inner">
<span><p>We propose a notion of common information that allows one to quantify and
separate the information that is shared between two random variables from the
information that is unique to each. Our notion of common information is a
variational relaxation of the G\'acs-K\"orner common information, which we
recover as a special case, but is more amenable to optimization and can be
approximated empirically using samples from the underlying distribution. We
then provide a method to partition and quantify the common and unique
information using a simple modification of a traditional variational
auto-encoder. Empirically, we demonstrate that our formulation allows us to
learn semantically meaningful common and unique factors of variation even on
high-dimensional data such as images and videos. Moreover, on datasets where
ground-truth latent factors are known, we show that we can accurately quantify
the common information between the random variables. Additionally, we show that
the auto-encoder that we learn recovers semantically meaningful disentangled
factors of variation, even though we do not explicitly optimize for it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Dynamics for Articulated 3d Human Motion Reconstruction. (arXiv:2205.12256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12256">
<div class="article-summary-box-inner">
<span><p>We introduce DiffPhy, a differentiable physics-based model for articulated 3d
human motion reconstruction from video. Applications of physics-based reasoning
in human motion analysis have so far been limited, both by the complexity of
constructing adequate physical models of articulated human motion, and by the
formidable challenges of performing stable and efficient inference with physics
in the loop. We jointly address such modeling and inference challenges by
proposing an approach that combines a physically plausible body representation
with anatomical joint limits, a differentiable physics simulator, and
optimization techniques that ensure good performance and robustness to
suboptimal local optima. In contrast to several recent methods, our approach
readily supports full-body contact including interactions with objects in the
scene. Most importantly, our model connects end-to-end with images, thus
supporting direct gradient-based physics optimization by means of image-based
loss functions. We validate the model by demonstrating that it can accurately
reconstruct physically plausible 3d human motion from monocular video, both on
public benchmarks with available 3d ground-truth, and on videos from the
internet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OnePose: One-Shot Object Pose Estimation without CAD Models. (arXiv:2205.12257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12257">
<div class="article-summary-box-inner">
<span><p>We propose a new method named OnePose for object pose estimation. Unlike
existing instance-level or category-level methods, OnePose does not rely on CAD
models and can handle objects in arbitrary categories without instance- or
category-specific network training. OnePose draws the idea from visual
localization and only requires a simple RGB video scan of the object to build a
sparse SfM model of the object. Then, this model is registered to new query
images with a generic feature matching network. To mitigate the slow runtime of
existing visual localization methods, we propose a new graph attention network
that directly matches 2D interest points in the query image with the 3D points
in the SfM model, resulting in efficient and robust pose estimation. Combined
with a feature-based pose tracker, OnePose is able to stably detect and track
6D poses of everyday household objects in real-time. We also collected a
large-scale dataset that consists of 450 sequences of 150 objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the competition between detection and ReID in Multi-Object Tracking. (arXiv:2010.12138v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12138">
<div class="article-summary-box-inner">
<span><p>Due to balanced accuracy and speed, one-shot models which jointly learn
detection and identification embeddings, have drawn great attention in
multi-object tracking (MOT). However, the inherent differences and relations
between detection and re-identification (ReID) are unconsciously overlooked
because of treating them as two isolated tasks in the one-shot tracking
paradigm. This leads to inferior performance compared with existing two-stage
methods. In this paper, we first dissect the reasoning process for these two
tasks, which reveals that the competition between them inevitably would destroy
task-dependent representations learning. To tackle this problem, we propose a
novel reciprocal network (REN) with a self-relation and cross-relation design
so that to impel each branch to better learn task-dependent representations.
The proposed model aims to alleviate the deleterious tasks competition,
meanwhile improve the cooperation between detection and ReID. Furthermore, we
introduce a scale-aware attention network (SAAN) that prevents semantic level
misalignment to improve the association capability of ID embeddings. By
integrating the two delicately designed networks into a one-shot online MOT
system, we construct a strong MOT tracker, namely CSTrack. Our tracker achieves
the state-of-the-art performance on MOT16, MOT17 and MOT20 datasets, without
other bells and whistles. Moreover, CSTrack is efficient and runs at 16.4 FPS
on a single modern GPU, and its lightweight version even runs at 34.6 FPS. The
complete code has been released at https://github.com/JudasDie/SOTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey. (arXiv:2101.00734v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00734">
<div class="article-summary-box-inner">
<span><p>This is a tutorial and survey paper on factor analysis, probabilistic
Principal Component Analysis (PCA), variational inference, and Variational
Autoencoder (VAE). These methods, which are tightly related, are dimensionality
reduction and generative models. They assume that every data point is generated
from or caused by a low-dimensional latent factor. By learning the parameters
of distribution of latent space, the corresponding low-dimensional factors are
found for the sake of dimensionality reduction. For their stochastic and
generative behaviour, these models can also be used for generation of new data
points in the data space. In this paper, we first start with variational
inference where we derive the Evidence Lower Bound (ELBO) and Expectation
Maximization (EM) for learning the parameters. Then, we introduce factor
analysis, derive its joint and marginal distributions, and work out its EM
steps. Probabilistic PCA is then explained, as a special case of factor
analysis, and its closed-form solutions are derived. Finally, VAE is explained
where the encoder, decoder and sampling from the latent space are introduced.
Training VAE using both EM and backpropagation are explained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effects of Image Size on Deep Learning. (arXiv:2101.11508v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11508">
<div class="article-summary-box-inner">
<span><p>This paper presents the evaluation of the effects of image size on deep
learning performance via semantic segmentation of magnetic resonance heart
images with U-net for fully automated quantification of myocardial infarction.
Both non-extra pixel and extra pixel interpolation algorithms are used to
change the size of images in datasets of interest. Extra class labels, in
interpolated ground truth segmentation images, are removed using thresholding,
median filtering, and subtraction strategies. Common class metrics are used to
evaluate the quality of semantic segmentation with U-net against the ground
truth segmentation while arbitrary threshold, comparison of the sums, and sums
of differences between medical experts and fully automated results are options
used to estimate the relationship between medical experts-based quantification
and fully automated quantification results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03097">
<div class="article-summary-box-inner">
<span><p>Machine learning systems generally assume that the training and testing
distributions are the same. To this end, a key requirement is to develop models
that can generalize to unseen distributions. Domain generalization (DG), i.e.,
out-of-distribution generalization, has attracted increasing interests in
recent years. Domain generalization deals with a challenging setting where one
or several different but related domain(s) are given, and the goal is to learn
a model that can generalize to an unseen test domain. Great progress has been
made in the area of domain generalization for years. This paper presents the
first review of recent advances in this area. First, we provide a formal
definition of domain generalization and discuss several related fields. We then
thoroughly review the theories related to domain generalization and carefully
analyze the theory behind generalization. We categorize recent algorithms into
three classes: data manipulation, representation learning, and learning
strategy, and present several popular algorithms in detail for each category.
Third, we introduce the commonly used datasets, applications, and our
open-sourced codebase for fair evaluation. Finally, we summarize existing
literature and present some potential research topics for the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances on Neural Network Pruning at Initialization. (arXiv:2103.06460v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06460">
<div class="article-summary-box-inner">
<span><p>Neural network pruning typically removes connections or neurons from a
pretrained converged model; while a new pruning paradigm, pruning at
initialization (PaI), attempts to prune a randomly initialized network. This
paper offers the first survey concentrated on this emerging pruning fashion. We
first introduce a generic formulation of neural network pruning, followed by
the major classic pruning topics. Then, as the main body of this paper, a
thorough and structured literature review of PaI methods is presented,
consisting of two major tracks (sparse training and sparse selection). Finally,
we summarize the surge of PaI compared to PaT and discuss the open problems.
Apart from the dedicated literature review, this paper also offers a code base
for easy sanity-checking and benchmarking of different PaI methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BADet: Boundary-Aware 3D Object Detection from Point Clouds. (arXiv:2104.10330v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10330">
<div class="article-summary-box-inner">
<span><p>Currently, existing state-of-the-art 3D object detectors are in two-stage
paradigm. These methods typically comprise two steps: 1) Utilize a region
proposal network to propose a handful of high-quality proposals in a bottom-up
fashion. 2) Resize and pool the semantic features from the proposed regions to
summarize RoI-wise representations for further refinement. Note that these
RoI-wise representations in step 2) are considered individually as uncorrelated
entries when fed to following detection headers. Nevertheless, we observe these
proposals generated by step 1) offset from ground truth somehow, emerging in
local neighborhood densely with an underlying probability. Challenges arise in
the case where a proposal largely forsakes its boundary information due to
coordinate offset while existing networks lack corresponding information
compensation mechanism. In this paper, we propose $BADet$ for 3D object
detection from point clouds. Specifically, instead of refining each proposal
independently as previous works do, we represent each proposal as a node for
graph construction within a given cut-off threshold, associating proposals in
the form of local neighborhood graph, with boundary correlations of an object
being explicitly exploited. Besides, we devise a lightweight Region Feature
Aggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise
features with expanding receptive fields for more informative RoI-wise
representations. We validate BADet both on widely used KITTI Dataset and highly
challenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par
performance on KITTI 3D detection leaderboard and ranks $1^{st}$ on $Moderate$
difficulty of $Car$ category on KITTI BEV detection leaderboard. The source
code is available at https://github.com/rui-qian/BADet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Real-Time Monocular SLAM Using Semantic Segmentation on Selective Frames. (arXiv:2105.00114v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00114">
<div class="article-summary-box-inner">
<span><p>Monocular simultaneous localization and mapping (SLAM) is emerging in
advanced driver assistance systems and autonomous driving, because a single
camera is cheap and easy to install. Conventional monocular SLAM has two major
challenges leading inaccurate localization and mapping. First, it is
challenging to estimate scales in localization and mapping. Second,
conventional monocular SLAM uses inappropriate mapping factors such as dynamic
objects and low-parallax areas in mapping. This paper proposes an improved
real-time monocular SLAM that resolves the aforementioned challenges by
efficiently using deep learning-based semantic segmentation. To achieve the
real-time execution of the proposed method, we apply semantic segmentation only
to downsampled keyframes in parallel with mapping processes. In addition, the
proposed method corrects scales of camera poses and three-dimensional (3D)
points, using estimated ground plane from road-labeled 3D points and the real
camera height. The proposed method also removes inappropriate corner features
labeled as moving objects and low parallax areas. Experiments with eight video
sequences demonstrate that the proposed monocular SLAM system achieves
significantly improved and comparable trajectory tracking accuracy, compared to
existing state-of-the-art monocular and stereo SLAM systems, respectively. The
proposed system can achieve real-time tracking on a standard CPU potentially
with a standard GPU support, whereas existing segmentation-aided monocular SLAM
does not.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01100">
<div class="article-summary-box-inner">
<span><p>During lung radiotherapy, the position of infrared reflective objects on the
chest can be recorded to estimate the tumor location. However, radiotherapy
systems have a latency inherent to robot control limitations that impedes the
radiation delivery precision. Prediction with online learning of recurrent
neural networks (RNN) allows for adaptation to non-stationary respiratory
signals, but classical methods such as RTRL and truncated BPTT are respectively
slow and biased. This study investigates the capabilities of unbiased online
recurrent optimization (UORO) to forecast respiratory motion and enhance safety
in lung radiotherapy.
</p>
<p>We used 9 observation records of the 3D position of 3 external markers on the
chest and abdomen of healthy individuals breathing during intervals from 73s to
222s. The sampling frequency was 10Hz, and the amplitudes of the recorded
trajectories range from 6mm to 40mm in the superior-inferior direction. We
forecast the 3D location of each marker simultaneously with a horizon value
between 0.1s and 2.0s, using an RNN trained with UORO. We compare its
performance with an RNN trained with RTRL, LMS, and offline linear regression.
We provide closed-form expressions for quantities involved in the gradient loss
calculation in UORO, thereby making its implementation efficient. Training and
cross-validation were performed during the first minute of each sequence.
</p>
<p>On average over the horizon values considered and the 9 sequences, UORO
achieves the lowest root-mean-square (RMS) error and maximum error among the
compared algorithms. These errors are respectively equal to 1.3mm and 8.8mm,
and the prediction time per time step was lower than 2.8ms (Dell Intel core
i9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon
values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,
and UORO for horizon values greater than 0.6s.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02229">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the fundamental question: To what extent are
gradient-based neural architecture search (NAS) techniques applicable to RL?
Using the original DARTS as a convenient baseline, we discover that the
discrete architectures found can achieve up to 250% performance compared to
manual architecture designs on both discrete and continuous action space
environments across off-policy and on-policy RL algorithms, at only 3x more
computation time. Furthermore, through numerous ablation studies, we
systematically verify that not only does DARTS correctly upweight operations
during its supernet phrase, but also gradually improves resulting discrete
cells up to 30x more efficiently than random search, suggesting DARTS is
surprisingly an effective tool for improving architectures in RL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Object Detection for Autonomous Driving: A Survey. (arXiv:2106.10823v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10823">
<div class="article-summary-box-inner">
<span><p>Autonomous driving is regarded as one of the most promising remedies to
shield human beings from severe crashes. To this end, 3D object detection
serves as the core basis of perception stack especially for the sake of path
planning, motion prediction, and collision avoidance etc. Taking a quick glance
at the progress we have made, we attribute challenges to visual appearance
recovery in the absence of depth information from images, representation
learning from partially occluded unstructured point clouds, and semantic
alignments over heterogeneous features from cross modalities. Despite existing
efforts, 3D object detection for autonomous driving is still in its infancy.
Recently, a large body of literature have been investigated to address this 3D
vision task. Nevertheless, rather a few investigations have looked into
collecting and structuring this growing knowledge. We therefore aim to fill
this gap in a comprehensive survey, encompassing all the main concerns
including sensors, datasets, performance metrics and the recent
state-of-the-art detection methods, together with their pros and cons.
Furthermore, we provide quantitative comparisons with the state of the art. A
case study on fifteen selected representative methods is presented, involved
with runtime analysis, error analysis, and robustness analysis. Finally, we
provide concluding remarks after an in-depth analysis of the surveyed works and
identify promising directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Top-Down Just Noticeable Difference Estimation of Natural Images. (arXiv:2108.05058v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05058">
<div class="article-summary-box-inner">
<span><p>Just noticeable difference (JND) of natural images refers to the maximum
pixel intensity change magnitude that typical human visual system (HVS) cannot
perceive. Existing efforts on JND estimation mainly dedicate to modeling the
diverse masking effects in either/both spatial or/and frequency domains, and
then fusing them into an overall JND estimate. In this work, we turn to a
dramatically different way to address this problem with a top-down design
philosophy. Instead of explicitly formulating and fusing different masking
effects in a bottom-up way, the proposed JND estimation model dedicates to
first predicting a critical perceptual lossless (CPL) counterpart of the
original image and then calculating the difference map between the original
image and the predicted CPL image as the JND map. We conduct subjective
experiments to determine the critical points of 500 images and find that the
distribution of cumulative normalized KLT coefficient energy values over all
500 images at these critical points can be well characterized by a Weibull
distribution. Given a testing image, its corresponding critical point is
determined by a simple weighted average scheme where the weights are determined
by a fitted Weibull distribution function. The performance of the proposed JND
model is evaluated explicitly with direct JND prediction and implicitly with
two applications including JND-guided noise injection and JND-guided image
compression. Experimental results have demonstrated that our proposed JND model
can achieve better performance than several latest JND models. In addition, we
also compare the proposed JND model with existing visual difference predicator
(VDP) metrics in terms of the capability in distortion detection and
discrimination. The results indicate that our JND model also has a good
performance in this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Ground Visual Objects for Visual Dialog. (arXiv:2109.06013v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06013">
<div class="article-summary-box-inner">
<span><p>Visual dialog is challenging since it needs to answer a series of coherent
questions based on understanding the visual environment. How to ground related
visual objects is one of the key problems. Previous studies utilize the
question and history to attend to the image and achieve satisfactory
performance, however these methods are not sufficient to locate related visual
objects without any guidance. The inappropriate grounding of visual objects
prohibits the performance of visual dialog models. In this paper, we propose a
novel approach to Learn to Ground visual objects for visual dialog, which
employs a novel visual objects grounding mechanism where both prior and
posterior distributions over visual objects are used to facilitate visual
objects grounding. Specifically, a posterior distribution over visual objects
is inferred from both context (history and questions) and answers, and it
ensures the appropriate grounding of visual objects during the training
process. Meanwhile, a prior distribution, which is inferred from context only,
is used to approximate the posterior distribution so that appropriate visual
objects can be grounded even without answers during the inference process.
Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate that our
approach improves the previous strong models in both generative and
discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How much human-like visual experience do current self-supervised learning algorithms need in order to achieve human-level object recognition?. (arXiv:2109.11523v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11523">
<div class="article-summary-box-inner">
<span><p>This paper addresses a fundamental question: how good are our current
self-supervised visual representation learning algorithms relative to humans?
More concretely, how much "human-like" natural visual experience would these
algorithms need in order to reach human-level performance in a complex,
realistic visual object recognition task such as ImageNet? Using a scaling
experiment, here we estimate that the answer is several orders of magnitude
longer than a human lifetime: typically on the order of a million to a billion
years of natural visual experience (depending on the algorithm used). We obtain
even larger estimates for achieving human-level performance in ImageNet-derived
robustness benchmarks. The exact values of these estimates are sensitive to
some underlying assumptions, however even in the most optimistic scenarios they
remain orders of magnitude larger than a human lifetime. We discuss the main
caveats surrounding our estimates and the implications of these surprising
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Splits: Improved K-Means Clustering Algorithm to Automatically Detect the Number of Clusters. (arXiv:2110.04660v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04660">
<div class="article-summary-box-inner">
<span><p>This paper introduces k-splits, an improved hierarchical algorithm based on
k-means to cluster data without prior knowledge of the number of clusters.
K-splits starts from a small number of clusters and uses the most significant
data distribution axis to split these clusters incrementally into better fits
if needed. Accuracy and speed are two main advantages of the proposed method.
We experiment on six synthetic benchmark datasets plus two real-world datasets
MNIST and Fashion-MNIST, to prove that our algorithm has excellent accuracy in
finding the correct number of clusters under different conditions. We also show
that k-splits is faster than similar methods and can even be faster than the
standard k-means in lower dimensions. Finally, we suggest using k-splits to
uncover the exact position of centroids and then input them as initial points
to the k-means algorithm to fine-tune the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10832">
<div class="article-summary-box-inner">
<span><p>In Domain Generalization (DG) settings, models trained independently on a
given set of training domains have notoriously chaotic performance on
distribution shifted test domains, and stochasticity in optimization (e.g.
seed) plays a big role. This makes deep learning models unreliable in real
world settings. We first show that this chaotic behavior exists even along the
training optimization trajectory of a single model, and propose a simple model
averaging protocol that both significantly boosts domain generalization and
diminishes the impact of stochasticity by improving the rank correlation
between the in-domain validation accuracy and out-domain test accuracy, which
is crucial for reliable early stopping. Taking advantage of our observation, we
show that instead of ensembling unaveraged models (that is typical in
practice), ensembling moving average models (EoA) from independent runs further
boosts performance. We theoretically explain the boost in performance of
ensembling and model averaging by adapting the well known Bias-Variance
trade-off to the domain generalization setting. On the DomainBed benchmark,
when using a pre-trained ResNet-50, this ensemble of averages achieves an
average of $68.0\%$, beating vanilla ERM (w/o averaging/ensembling) by $\sim
4\%$, and when using a pre-trained RegNetY-16GF, achieves an average of
$76.6\%$, beating vanilla ERM by $6\%$. Our code is available at
\url{https://github.com/salesforce/ensemble-of-averages}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deep Representation with Energy-Based Self-Expressiveness for Subspace Clustering. (arXiv:2110.15037v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15037">
<div class="article-summary-box-inner">
<span><p>Deep subspace clustering has attracted increasing attention in recent years.
Almost all the existing works are required to load the whole training data into
one batch for learning the self-expressive coefficients in the framework of
deep learning. Although these methods achieve promising results, such a
learning fashion severely prevents from the usage of deeper neural network
architectures (e.g., ResNet), leading to the limited representation abilities
of the models. In this paper, we propose a new deep subspace clustering
framework, motivated by the energy-based models. In contrast to previous
approaches taking the weights of a fully connected layer as the self-expressive
coefficients, we propose to learn an energy-based network to obtain the
self-expressive coefficients by mini-batch training. By this means, it is no
longer necessary to load all data into one batch for learning, and it thus
becomes a reality that we can utilize deeper neural network models for subspace
clustering. Considering the powerful representation ability of the recently
popular self-supervised learning, we attempt to leverage self-supervised
representation learning to learn the dictionary. Finally, we propose a joint
framework to learn both the self-expressive coefficients and dictionary
simultaneously, and train the model in an end-to-end manner. The experiments
are performed on three publicly available datasets, and extensive experimental
results demonstrate our method can significantly outperform the other related
approaches. For instance, on the three datasets, our method can averagely
achieve $13.8\%$, $15.4\%$, $20.8\%$ improvements in terms of Accuracy, NMI,
and ARI over SENet which is proposed very recently and obtains the second best
results in the experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DanceTrack: Multi-Object Tracking in Uniform Appearance and Diverse Motion. (arXiv:2111.14690v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14690">
<div class="article-summary-box-inner">
<span><p>A typical pipeline for multi-object tracking (MOT) is to use a detector for
object localization, and following re-identification (re-ID) for object
association. This pipeline is partially motivated by recent progress in both
object detection and re-ID, and partially motivated by biases in existing
tracking datasets, where most objects tend to have distinguishing appearance
and re-ID models are sufficient for establishing associations. In response to
such bias, we would like to re-emphasize that methods for multi-object tracking
should also work when object appearance is not sufficiently discriminative. To
this end, we propose a large-scale dataset for multi-human tracking, where
humans have similar appearance, diverse motion and extreme articulation. As the
dataset contains mostly group dancing videos, we name it "DanceTrack". We
expect DanceTrack to provide a better platform to develop more MOT algorithms
that rely less on visual discrimination and depend more on motion analysis. We
benchmark several state-of-the-art trackers on our dataset and observe a
significant performance drop on DanceTrack when compared against existing
benchmarks. The dataset, project code and competition server are released at:
\url{https://github.com/DanceTrack}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v3 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10065">
<div class="article-summary-box-inner">
<span><p>As emerging deep neural network (DNN) models continue to grow in size, using
large GPU clusters to train DNNs is becoming an essential requirement to
achieving acceptable training times. In this paper, we consider the case where
future increases in cluster size will cause the global batch size that can be
used to train models to reach a fundamental limit: beyond a certain point,
larger global batch sizes cause sample efficiency to degrade, increasing
overall time to accuracy. As a result, to achieve further improvements in
training performance, we must instead consider "strong scaling" strategies that
hold the global batch size constant and allocate smaller batches to each GPU.
Unfortunately, this makes it significantly more difficult to use cluster
resources efficiently. We present DeepPool, a system that addresses this
efficiency challenge through two key ideas. First, burst parallelism allocates
large numbers of GPUs to foreground jobs in bursts to exploit the unevenness in
parallelism across layers. Second, GPU multiplexing prioritizes throughput for
foreground training jobs, while packing in background training jobs to reclaim
underutilized GPU resources, thereby improving cluster-wide utilization.
Together, these two ideas enable DeepPool to deliver a 1.2 - 2.3x improvement
in total cluster throughput over standard data parallelism with a single task
when the cluster scale is large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer with Deformable Attention. (arXiv:2201.00520v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00520">
<div class="article-summary-box-inner">
<span><p>Transformers have recently shown superior performances on various vision
tasks. The large, sometimes even global, receptive field endows Transformer
models with higher representation power over their CNN counterparts.
Nevertheless, simply enlarging receptive field also gives rise to several
concerns. On the one hand, using dense attention e.g., in ViT, leads to
excessive memory and computational cost, and features can be influenced by
irrelevant parts which are beyond the region of interests. On the other hand,
the sparse attention adopted in PVT or Swin Transformer is data agnostic and
may limit the ability to model long range relations. To mitigate these issues,
we propose a novel deformable self-attention module, where the positions of key
and value pairs in self-attention are selected in a data-dependent way. This
flexible scheme enables the self-attention module to focus on relevant regions
and capture more informative features. On this basis, we present Deformable
Attention Transformer, a general backbone model with deformable attention for
both image classification and dense prediction tasks. Extensive experiments
show that our models achieve consistently improved results on comprehensive
benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jacobian Computation for Cumulative B-Splines on SE(3) and Application to Continuous-Time Object Tracking. (arXiv:2201.10602v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10602">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a method that estimates the $SE(3)$ continuous
trajectories (orientation and translation) of the dynamic rigid objects present
in a scene, from multiple RGB-D views. Specifically, we fit the object
trajectories to cumulative B-Splines curves, which allow us to interpolate, at
any intermediate time stamp, not only their poses but also their linear and
angular velocities and accelerations. Additionally, we derive in this work the
analytical $SE(3)$ Jacobians needed by the optimization, being applicable to
any other approach that uses this type of curves. To the best of our knowledge
this is the first work that proposes 6-DoF continuous-time object tracking,
which we endorse with significant computational cost reduction thanks to our
analytical derivations. We evaluate our proposal in synthetic data and in a
public benchmark, showing competitive results in localization and significant
improvements in velocity estimation in comparison to discrete-time approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12114">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms are dominating the explainability of deep models. They
produce probability distributions over the input, which are widely deemed as
feature-importance indicators. However, in this paper, we find one critical
limitation in attention explanations: weakness in identifying the polarity of
feature impact. This would be somehow misleading -- features with higher
attention weights may not faithfully contribute to model predictions; instead,
they can impose suppression effects. With this finding, we reflect on the
explainability of current attention-based techniques, such as
Attentio$\odot$Gradient and LRP-based attention explanations. We first propose
an actionable diagnostic methodology (henceforth faithfulness violation test)
to measure the consistency between explanation weights and the impact polarity.
Through the extensive experiments, we then show that most tested explanation
methods are unexpectedly hindered by the faithfulness violation issue,
especially the raw attention. Empirical analyses on the factors affecting
violation issues further provide useful observations for adopting explanation
methods in attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frame-level Prediction of Facial Expressions, Valence, Arousal and Action Units for Mobile Devices. (arXiv:2203.13436v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13436">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of real-time video-based facial
emotion analytics, namely, facial expression recognition, prediction of valence
and arousal and detection of action unit points. We propose the novel
frame-level emotion recognition algorithm by extracting facial features with
the single EfficientNet model pre-trained on AffectNet. As a result, our
approach may be implemented even for video analytics on mobile devices.
Experimental results for the large scale Aff-Wild2 database from the third
Affective Behavior Analysis in-the-wild (ABAW) Competition demonstrate that our
simple model is significantly better when compared to the VggFace baseline. In
particular, our method is characterized by 0.15-0.2 higher performance measures
for validation sets in uni-task Expression Classification, Valence-Arousal
Estimation and Expression Classification. Due to simplicity, our approach may
be considered as a new baseline for all four sub-challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stack operation of tensor networks. (arXiv:2203.16338v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16338">
<div class="article-summary-box-inner">
<span><p>The tensor network, as a facterization of tensors, aims at performing the
operations that are common for normal tensors, such as addition, contraction
and stacking. However, due to its non-unique network structure, only the tensor
network contraction is so far well defined. In this paper, we propose a
mathematically rigorous definition for the tensor network stack approach, that
compress a large amount of tensor networks into a single one without changing
their structures and configurations. We illustrate the main ideas with the
matrix product states based machine learning as an example. Our results are
compared with the for loop and the efficient coding method on both CPU and GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Compositional Consistency for Video Question Answering. (arXiv:2204.07190v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07190">
<div class="article-summary-box-inner">
<span><p>Recent video question answering benchmarks indicate that state-of-the-art
models struggle to answer compositional questions. However, it remains unclear
which types of compositional reasoning cause models to mispredict. Furthermore,
it is difficult to discern whether models arrive at answers using compositional
reasoning or by leveraging data biases. In this paper, we develop a question
decomposition engine that programmatically deconstructs a compositional
question into a directed acyclic graph of sub-questions. The graph is designed
such that each parent question is a composition of its children. We present
AGQA-Decomp, a benchmark containing $2.3M$ question graphs, with an average of
$11.49$ sub-questions per graph, and $4.55M$ total new sub-questions. Using
question graphs, we evaluate three state-of-the-art models with a suite of
novel compositional consistency metrics. We find that models either cannot
reason correctly through most compositions or are reliant on incorrect
reasoning to reach answers, frequently contradicting themselves or achieving
high accuracies when failing at intermediate reasoning steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imposing Consistency for Optical Flow Estimation. (arXiv:2204.07262v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07262">
<div class="article-summary-box-inner">
<span><p>Imposing consistency through proxy tasks has been shown to enhance
data-driven learning and enable self-supervision in various tasks. This paper
introduces novel and effective consistency strategies for optical flow
estimation, a problem where labels from real-world data are very challenging to
derive. More specifically, we propose occlusion consistency and zero forcing in
the forms of self-supervised learning and transformation consistency in the
form of semi-supervised learning. We apply these consistency techniques in a
way that the network model learns to describe pixel-level motions better while
requiring no additional annotations. We demonstrate that our consistency
strategies applied to a strong baseline network model using the original
datasets and labels provide further improvements, attaining the
state-of-the-art results on the KITTI-2015 scene flow benchmark in the
non-stereo category. Our method achieves the best foreground accuracy (4.33% in
Fl-all) over both the stereo and non-stereo categories, even though using only
monocular image inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08499">
<div class="article-summary-box-inner">
<span><p>Coreset selection, which aims to select a subset of the most informative
training samples, is a long-standing learning problem that can benefit many
downstream tasks such as data-efficient learning, continual learning, neural
architecture search, active learning, etc. However, many existing coreset
selection methods are not designed for deep learning, which may have high
complexity and poor generalization performance. In addition, the recently
proposed methods are evaluated on models, datasets, and settings of different
complexities. To advance the research of coreset selection in deep learning, we
contribute a comprehensive code library, namely DeepCore, and provide an
empirical study on popular coreset selection methods on CIFAR10 and ImageNet
datasets. Extensive experiments on CIFAR10 and ImageNet datasets verify that,
although various methods have advantages in certain experiment settings, random
selection is still a strong baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation. (arXiv:2204.10762v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10762">
<div class="article-summary-box-inner">
<span><p>A high-resolution network exhibits remarkable capability in extracting
multi-scale features for human pose estimation, but fails to capture long-range
interactions between joints and has high computational complexity. To address
these problems, we present a Dynamic lightweight High-Resolution Network
(Dite-HRNet), which can efficiently extract multi-scale contextual information
and model long-range spatial dependency for human pose estimation.
Specifically, we propose two methods, dynamic split convolution and adaptive
context modeling, and embed them into two novel lightweight blocks, which are
named dynamic multi-scale context block and dynamic global context block. These
two blocks, as the basic component units of our Dite-HRNet, are specially
designed for the high-resolution networks to make full use of the parallel
multi-resolution architecture. Experimental results show that the proposed
network achieves superior performance on both COCO and MPII human pose
estimation datasets, surpassing the state-of-the-art lightweight networks. Code
is available at: https://github.com/ZiyiZhang27/Dite-HRNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation. (arXiv:2204.12484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12484">
<div class="article-summary-box-inner">
<span><p>Although no specific domain knowledge is considered in the design, plain
vision transformers have shown excellent performance in visual recognition
tasks. However, little effort has been made to reveal the potential of such
simple structures for pose estimation tasks. In this paper, we show the
surprisingly good capabilities of plain vision transformers for pose estimation
from various aspects, namely simplicity in model structure, scalability in
model size, flexibility in training paradigm, and transferability of knowledge
between models, through a simple baseline model called ViTPose. Specifically,
ViTPose employs plain and non-hierarchical vision transformers as backbones to
extract features for a given person instance and a lightweight decoder for pose
estimation. It can be scaled up from 100M to 1B parameters by taking the
advantages of the scalable model capacity and high parallelism of transformers,
setting a new Pareto front between throughput and performance. Besides, ViTPose
is very flexible regarding the attention type, input resolution, pre-training
and finetuning strategy, as well as dealing with multiple pose tasks. We also
empirically demonstrate that the knowledge of large ViTPose models can be
easily transferred to small ones via a simple knowledge token. Experimental
results show that our basic ViTPose model outperforms representative methods on
the challenging MS COCO Keypoint Detection benchmark, while the largest model
sets a new state-of-the-art. The code and models are available at
https://github.com/ViTAE-Transformer/ViTPose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13170">
<div class="article-summary-box-inner">
<span><p>In Federated Learning (FL), a number of clients or devices collaborate to
train a model without sharing their data. Models are optimized locally at each
client and further communicated to a central hub called the server for
aggregation. While FL serves as an attractive decentralized training paradigm,
heterogeneity amongst clients' data can cause the local optimization to drift
away with respect to the global objective. In order to estimate and therefore
remove this drift, variance reduction techniques have been incorporated into FL
optimization recently. However, these approaches inaccurately estimate the
clients' drift and ultimately fail to remove it properly. In this work, we
address this challenge by introducing an adaptive algorithm that efficiently
reduces clients' drift. In this work, we propose an adaptive algorithm that
accurately estimates drift across clients. Further, unlike previous works, our
approach uses less or the same level of communication bandwidth, compute or
memory. Additionally, our proposed methodology induces stability by
constraining the norm of estimates for client drift, making it more practical
for large scale FL settings. Experimental findings demonstrate that the
proposed algorithm converges significantly faster and achieves higher accuracy
compared to the baselines across various benchmarks for FL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01550">
<div class="article-summary-box-inner">
<span><p>In recent years, with the development of computing resources and LiDAR, point
cloud semantic segmentation has attracted many researchers. For the sparsity of
point clouds, although there is already a way to deal with sparse convolution,
multi-scale features are not considered. In this letter, we propose a feature
extraction module based on multi-scale sparse convolution and a feature
selection module based on channel attention and build a point cloud
segmentation network framework based on this. By introducing multi-scale sparse
convolution, the network could capture richer feature information based on
convolution kernels with different sizes, improving the segmentation result of
point cloud segmentation. Experimental results on Stanford large-scale 3-D
Indoor Spaces(S3DIS) dataset and outdoor dataset(SemanticKITTI), demonstrate
effectiveness and superiority of the proposed mothod.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05677">
<div class="article-summary-box-inner">
<span><p>Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BronchusNet: Region and Structure Prior Embedded Representation Learning for Bronchus Segmentation and Classification. (arXiv:2205.06947v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06947">
<div class="article-summary-box-inner">
<span><p>CT-based bronchial tree analysis plays an important role in the
computer-aided diagnosis for respiratory diseases, as it could provide
structured information for clinicians. The basis of airway analysis is
bronchial tree reconstruction, which consists of bronchus segmentation and
classification. However, there remains a challenge for accurate bronchial
analysis due to the individual variations and the severe class imbalance. In
this paper, we propose a region and structure prior embedded framework named
BronchusNet to achieve accurate segmentation and classification of bronchial
regions in CT images. For bronchus segmentation, we propose an adaptive hard
region-aware UNet that incorporates multi-level prior guidance of hard
pixel-wise samples in the general Unet segmentation network to achieve better
hierarchical feature learning. For the classification of bronchial branches, we
propose a hybrid point-voxel graph learning module to fully exploit bronchial
structure priors and to support simultaneous feature interactions across
different branches. To facilitate the study of bronchial analysis, we
contribute~\textbf{BRSC}: an open-access benchmark of \textbf{BR}onchus imaging
analysis with high-quality pixel-wise \textbf{S}egmentation masks and the
\textbf{C}lass of bronchial segments. Experimental results on BRSC show that
our proposed method not only achieves the state-of-the-art performance for
binary segmentation of bronchial region but also exceeds the best existing
method on bronchial branches classification by 6.9\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. (arXiv:2205.09853v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09853">
<div class="article-summary-box-inner">
<span><p>Video prediction is a challenging task. The quality of video frames from
current state-of-the-art (SOTA) generative models tends to be poor and
generalization beyond the training data is difficult. Furthermore, existing
prediction frameworks are typically not capable of simultaneously handling
other video-related tasks such as unconditional generation or interpolation. In
this work, we devise a general-purpose framework called Masked Conditional
Video Diffusion (MCVD) for all of these video synthesis tasks using a
probabilistic conditional score-based denoising diffusion model, conditioned on
past and/or future frames. We train the model in a manner where we randomly and
independently mask all the past frames or all the future frames. This novel but
straightforward setup allows us to train a single model that is capable of
executing a broad range of video tasks, specifically: future/past prediction --
when only future/past frames are masked; unconditional generation -- when both
past and future frames are masked; and interpolation -- when neither past nor
future frames are masked. Our experiments show that this approach can generate
high-quality frames for diverse types of videos. Our MCVD models are built from
simple non-recurrent 2D-convolutional architectures, conditioning on blocks of
frames and generating blocks of frames. We generate videos of arbitrary lengths
autoregressively in a block-wise manner. Our approach yields SOTA results
across standard video prediction and interpolation benchmarks, with computation
times for training models measured in 1-12 days using $\le$ 4 GPUs.
https://mask-cond-video-diffusion.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EXPANSE: A Deep Continual / Progressive Learning System for Deep Transfer Learning. (arXiv:2205.10356v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10356">
<div class="article-summary-box-inner">
<span><p>Deep transfer learning techniques try to tackle the limitations of deep
learning, the dependency on extensive training data and the training costs, by
reusing obtained knowledge. However, the current DTL techniques suffer from
either catastrophic forgetting dilemma (losing the previously obtained
knowledge) or overly biased pre-trained models (harder to adapt to target data)
in finetuning pre-trained models or freezing a part of the pre-trained model,
respectively. Progressive learning, a sub-category of DTL, reduces the effect
of the overly biased model in the case of freezing earlier layers by adding a
new layer to the end of a frozen pre-trained model. Even though it has been
successful in many cases, it cannot yet handle distant source and target data.
We propose a new continual/progressive learning approach for deep transfer
learning to tackle these limitations. To avoid both catastrophic forgetting and
overly biased-model problems, we expand the pre-trained model by expanding
pre-trained layers (adding new nodes to each layer) in the model instead of
only adding new layers. Hence the method is named EXPANSE. Our experimental
results confirm that we can tackle distant source and target data using this
technique. At the same time, the final model is still valid on the source data,
achieving a promising deep continual learning approach. Moreover, we offer a
new way of training deep learning models inspired by the human education
system. We termed this two-step training: learning basics first, then adding
complexities and uncertainties. The evaluation implies that the two-step
training extracts more meaningful features and a finer basin on the error
surface since it can achieve better accuracy in comparison to regular training.
EXPANSE (model expansion and two-step training) is a systematic continual
learning approach applicable to different problems and DL models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Omnidirectional Vision: A Survey and New Perspectives. (arXiv:2205.10468v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10468">
<div class="article-summary-box-inner">
<span><p>Omnidirectional image (ODI) data is captured with a 360x180 field-of-view,
which is much wider than the pinhole cameras and contains richer spatial
information than the conventional planar images. Accordingly, omnidirectional
vision has attracted booming attention due to its more advantageous performance
in numerous applications, such as autonomous driving and virtual reality. In
recent years, the availability of customer-level 360 cameras has made
omnidirectional vision more popular, and the advance of deep learning (DL) has
significantly sparked its research and applications. This paper presents a
systematic and comprehensive review and analysis of the recent progress in DL
methods for omnidirectional vision. Our work covers four main contents: (i) An
introduction to the principle of omnidirectional imaging, the convolution
methods on the ODI, and datasets to highlight the differences and difficulties
compared with the 2D planar image data; (ii) A structural and hierarchical
taxonomy of the DL methods for omnidirectional vision; (iii) A summarization of
the latest novel learning strategies and applications; (iv) An insightful
discussion of the challenges and open problems by highlighting the potential
research directions to trigger more research in the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deeper vs Wider: A Revisit of Transformer Configuration. (arXiv:2205.10505v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10505">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have delivered impressive results on many tasks,
particularly vision and language tasks. In many model training situations,
conventional configurations are typically adopted. For example, we often set
the base model with hidden dimensions (i.e. model width) to be 768 and the
number of transformer layers (i.e. model depth) to be 12. In this paper, we
revisit these conventional configurations. Through theoretical analysis and
experimental evaluation, we show that the masked autoencoder is effective in
alleviating the over-smoothing issue in deep transformer training. Based on
this finding, we propose Bamboo, an idea of using deeper and narrower
transformer configurations, for masked autoencoder training. On ImageNet, with
such a simple change in configuration, re-designed model achieves 87.1% top-1
accuracy and outperforms SoTA models like MAE and BEiT. On language tasks,
re-designed model outperforms BERT with default setting by 1.1 points on
average, on GLUE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10667">
<div class="article-summary-box-inner">
<span><p>Traditionally, extracting patterns from eye movement data relies on
statistics of different macro-events such as fixations and saccades. This
requires an additional preprocessing step to separate the eye movement
subtypes, often with a number of parameters on which the classification results
depend. Besides that, definitions of such macro events are formulated in
different ways by different researchers.
</p>
<p>We propose an application of a new class of features to the quantitative
analysis of personal eye movement trajectories structure. This new class of
features based on algebraic topology allows extracting patterns from different
modalities of gaze such as time series of coordinates and amplitudes, heatmaps,
and point clouds in a unified way at all scales from micro to macro. We
experimentally demonstrate the competitiveness of the new class of features
with the traditional ones and their significant synergy while being used
together for the person authentication task on the recently published eye
movement trajectories dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners. (arXiv:2205.10747v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10747">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to build flexible video-language models that can
generalize to various video-to-text tasks from few examples, such as
domain-specific captioning, question answering, and future event prediction.
Existing few-shot video-language learners focus exclusively on the encoder,
resulting in the absence of a video-to-text decoder to handle generative tasks.
Video captioners have been pretrained on large-scale video-language datasets,
but they rely heavily on finetuning and lack the ability to generate text for
unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language
Learner via Image and Language models, which demonstrates strong performance on
few-shot video-to-text tasks without the necessity of pretraining or finetuning
on any video datasets. We use the image-language models to translate the video
content into frame captions, object, attribute, and event phrases, and compose
them into a temporal structure template. We then instruct a language model,
with a prompt containing a few in-context examples, to generate a target output
from the composed content. The flexibility of prompting allows the model to
capture any form of text input, such as automatic speech recognition (ASR)
transcripts. Our experiments demonstrate the power of language models in
understanding videos on a wide variety of video-language tasks, including video
captioning, video question answering, video caption retrieval, and video future
event prediction. Especially, on video future event prediction, our few-shot
model significantly outperforms state-of-the-art supervised models trained on
large-scale video datasets. Code and resources are publicly available for
research purposes at https://github.com/MikeWangWZHL/VidIL .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real Time Detection Free Tracking of Multiple Objects Via Equilibrium Optimizer. (arXiv:2205.10756v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10756">
<div class="article-summary-box-inner">
<span><p>Multiple objects tracking (MOT) is a difficult task, as it usually requires
special hardware and higher computation complexity. In this work, we present a
new framework of MOT by using of equilibrium optimizer (EO) algorithm and
reducing the resolution of the bounding boxes of the objects to solve such
problems in the detection free framework. First, in the first frame the target
objects are initialized and its size is computed, then its resolution is
reduced if it is higher than a threshold, and then modeled by their kernel
color histogram to establish a feature model. The Bhattacharya distances
between the histogram of object models and other candidates are used as the
fitness function to be optimized. Multiple agents are generated by EO,
according to the number of the target objects to be tracked. EO algorithm is
used because of its efficiency and lower computation cost compared to other
algorithms in global optimization. Experimental results confirm that EO
multi-object tracker achieves satisfying tracking results then other trackers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCP: Recurrent Closest Point for Scene Flow Estimation on 3D Point Clouds. (arXiv:2205.11028v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11028">
<div class="article-summary-box-inner">
<span><p>3D motion estimation including scene flow and point cloud registration has
drawn increasing interest. Inspired by 2D flow estimation, recent methods
employ deep neural networks to construct the cost volume for estimating
accurate 3D flow. However, these methods are limited by the fact that it is
difficult to define a search window on point clouds because of the irregular
data structure. In this paper, we avoid this irregularity by a simple yet
effective method.We decompose the problem into two interlaced stages, where the
3D flows are optimized point-wisely at the first stage and then globally
regularized in a recurrent network at the second stage. Therefore, the
recurrent network only receives the regular point-wise information as the
input. In the experiments, we evaluate the proposed method on both the 3D scene
flow estimation and the point cloud registration task. For 3D scene flow
estimation, we make comparisons on the widely used FlyingThings3D and
KITTIdatasets. For point cloud registration, we follow previous works and
evaluate the data pairs with large pose and partially overlapping from
ModelNet40. The results show that our method outperforms the previous method
and achieves a new state-of-the-art performance on both 3D scene flow
estimation and point cloud registration, which demonstrates the superiority of
the proposed zero-order method on irregular point cloud data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Network approaches for Analysing Videos of Music Performances. (arXiv:2205.11232v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11232">
<div class="article-summary-box-inner">
<span><p>This paper presents a framework to automate the labelling process for
gestures in musical performance videos with a 3D Convolutional Neural Network
(CNN). While this idea was proposed in a previous study, this paper introduces
several novelties: (i) Presents a novel method to overcome the class imbalance
challenge and make learning possible for co-existent gestures by batch
balancing approach and spatial-temporal representations of gestures. (ii)
Performs a detailed study on 7 and 18 categories of gestures generated during
the performance (guitar play) of musical pieces that have been video-recorded.
(iii) Investigates the possibility to use audio features. (iv) Extends the
analysis to multiple videos. The novel methods significantly improve the
performance of gesture identification by 12 %, when compared to the previous
work (51 % in this study over 39 % in previous work). We successfully validate
the proposed methods on 7 super classes (72 %), an ensemble of the 18
gestures/classes, and additional videos (75 %).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer: Vit and its Derivatives. (arXiv:2205.11239v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11239">
<div class="article-summary-box-inner">
<span><p>Transformer, an attention-based encoder-decoder architecture, has not only
revolutionized the field of natural language processing (NLP), but has also
done some pioneering work in the field of computer vision (CV). Compared to
convolutional neural networks (CNNs), the Vision Transformer (ViT) relies on
excellent modeling capabilities to achieve very good performance on several
benchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the
self-attention mechanism in natural language processing, where word embeddings
are replaced with patch embeddings.
</p>
<p>This paper reviews the derivatives of ViT and the cross-applications of ViT
with other fields.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-25 23:08:52.908322856 UTC">2022-05-25 23:08:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>