<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-11T00:30:00Z">03-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition. (arXiv:2203.05008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05008">
<div class="article-summary-box-inner">
<span><p>Language model fusion helps smart assistants recognize words which are rare
in acoustic data but abundant in text-only corpora (typed search logs).
However, such corpora have properties that hinder downstream performance,
including being (1) too large, (2) beset with domain-mismatched content, and
(3) heavy-headed rather than heavy-tailed (excessively many duplicate search
queries such as "weather"). We show that three simple strategies for selecting
language modeling data can dramatically improve rare-word recognition without
harming overall performance. First, to address the heavy-headedness, we
downsample the data according to a soft log function, which tunably reduces
high frequency (head) sentences. Second, to encourage rare-word exposure, we
explicitly filter for words rare in the acoustic data. Finally, we tackle
domain-mismatch via perplexity-based contrastive selection, filtering for
examples matched to the target domain. We down-select a large corpus of web
search queries by a factor of 53x and achieve better LM perplexities than
without down-selection. When shallow-fused with a state-of-the-art, production
speech engine, our LM achieves WER reductions of up to 24% relative on
rare-word sentences (without changing overall WER) compared to a baseline LM
trained on the raw corpus. These gains are further validated through favorable
side-by-side evaluations on live voice search traffic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing. (arXiv:2203.05061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05061">
<div class="article-summary-box-inner">
<span><p>Deep learning algorithms are dependent on the availability of large-scale
annotated clinical text datasets. The lack of such publicly available datasets
is the biggest bottleneck for the development of clinical Natural Language
Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep
learning models to classify instances from new classes of which no training
data have been seen before. Prompt-based learning is an emerging ZSL technique
where we define task-based templates for NLP tasks. We developed a novel
prompt-based clinical NLP framework called HealthPrompt and applied the
paradigm of prompt-based learning on clinical texts. In this technique, rather
than fine-tuning a Pre-trained Language Model(PLM), the task definitions are
tuned by defining a prompt template. We performed an in-depth analysis of
HealthPrompt on six different PLMs in a no-data setting. Our experiments prove
that prompts effectively capture the context of clinical texts and perform
remarkably well without any training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks. (arXiv:2203.05081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05081">
<div class="article-summary-box-inner">
<span><p>Natural language explanation (NLE) models aim at explaining the
decision-making process of a black box system via generating natural language
sentences which are human-friendly, high-level and fine-grained. Current NLE
models explain the decision-making process of a vision or vision-language model
(a.k.a., task model), e.g., a VQA model, via a language model (a.k.a.,
explanation model), e.g., GPT. Other than the additional memory resources and
inference time required by the task model, the task and explanation models are
completely independent, which disassociates the explanation from the reasoning
process made to predict the answer. We introduce NLX-GPT, a general, compact
and faithful language model that can simultaneously predict an answer and
explain it. We first conduct pre-training on large scale data of image-caption
pairs for general understanding of images, and then formulate the answer as a
text prediction task along with the explanation. Without region proposals nor a
task model, our resulting overall framework attains better evaluation scores,
contains much less parameters and is 15$\times$ faster than the current SoA
model. We then address the problem of evaluating the explanations which can be
in many times generic, data-biased and can come in several forms. We therefore
design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based
attack, a self-evaluation framework that requires no labels. Code is at:
https://github.com/fawazsammani/nlxgpt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Librarian-in-the-Loop: A Natural Language Processing Paradigm for Detecting Informal Mentions of Research Data in Academic Literature. (arXiv:2203.05112v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05112">
<div class="article-summary-box-inner">
<span><p>Data citations provide a foundation for studying research data impact.
Collecting and managing data citations is a new frontier in archival science
and scholarly communication. However, the discovery and curation of research
data citations is labor intensive. Data citations that reference unique
identifiers (i.e. DOIs) are readily findable; however, informal mentions made
to research data are more challenging to infer. We propose a natural language
processing (NLP) paradigm to support the human task of identifying informal
mentions made to research datasets. The work of discovering informal data
mentions is currently performed by librarians and their staff in the
Inter-university Consortium for Political and Social Research (ICPSR), a large
social science data archive that maintains a large bibliography of data-related
literature. The NLP model is bootstrapped from data citations actively
collected by librarians at ICPSR. The model combines pattern matching with
multiple iterations of human annotations to learn additional rules for
detecting informal data mentions. These examples are then used to train an NLP
pipeline. The librarian-in-the-loop paradigm is centered in the data work
performed by ICPSR librarians, supporting broader efforts to build a more
comprehensive bibliography of data-related literature that reflects the
scholarly communities of research data users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Internet-augmented language models through few-shot prompting for open-domain question answering. (arXiv:2203.05115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05115">
<div class="article-summary-box-inner">
<span><p>In this work, we aim to capitalize on the unique few-shot capabilities
offered by large-scale language models to overcome some of their challenges
with respect to grounding to factual and up-to-date information. Motivated by
semi-parametric language models, which ground their decisions in external
retrieved evidence, we use few-shot prompting to learn to condition language
models on information returned from the web using Google Search, a broad and
constantly updated knowledge source. Our approach does not involve fine-tuning
or learning additional parameters, thus making it applicable to any language
model, offering like this a strong baseline. Indeed, we find that language
models conditioned on the web surpass performance of closed-book models of
similar, or even larger, model sizes in open-domain question answering.
Finally, we find that increasing the inference-time compute of models, achieved
via using multiple retrieved evidences to generate multiple answers followed by
a reranking stage, alleviates generally decreased performance of smaller
few-shot language models. All in all, our findings suggest that it might be
beneficial to slow down the race towards the biggest model and instead shift
the attention towards finding more effective ways to use models, including but
not limited to better prompting or increasing inference-time compute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compilable Neural Code Generation with Compiler Feedback. (arXiv:2203.05132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05132">
<div class="article-summary-box-inner">
<span><p>Automatically generating compilable programs with (or without) natural
language descriptions has always been a touchstone problem for computational
linguistics and automated software engineering. Existing deep-learning
approaches model code generation as text generation, either constrained by
grammar structures in decoder, or driven by pre-trained language models on
large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of
them account for compilability of the generated programs. To improve
compilability of the generated programs, this paper proposes COMPCODER, a
three-stage pipeline utilizing compiler feedback for compilable code
generation, including language model fine-tuning, compilability reinforcement,
and compilability discrimination. Comprehensive experiments on two code
generation tasks demonstrate the effectiveness of our proposed approach,
improving the success rate of compilation from 44.18 to 89.18 in code
completion on average and from 70.3 to 96.2 in text-to-code generation,
respectively, when comparing with the state-of-the-art CodeGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speciesist Language and Nonhuman Animal Bias in English Masked Language Models. (arXiv:2203.05140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05140">
<div class="article-summary-box-inner">
<span><p>Various existing studies have analyzed what social biases are inherited by
NLP models. These biases may directly or indirectly harm people, therefore
previous studies have focused only on human attributes. If the social biases in
NLP models can be indirectly harmful to humans involved, then the models can
also indirectly harm nonhuman animals. However, until recently no research on
social biases in NLP regarding nonhumans existed. In this paper, we analyze
biases to nonhuman animals, i.e. speciesist bias, inherent in English Masked
Language Models. We analyze this bias using template-based and corpus-extracted
sentences which contain speciesist (or non-speciesist) language, to show that
these models tend to associate harmful words with nonhuman animals. Our code
for reproducing the experiments will be made available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection. (arXiv:2203.05147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05147">
<div class="article-summary-box-inner">
<span><p>Knowledge graph integration typically suffers from the widely existing
dangling entities that cannot find alignment cross knowledge graphs (KGs). The
dangling entity set is unavailable in most real-world scenarios, and manually
mining the entity pairs that consist of entities with the same meaning is
labor-consuming. In this paper, we propose a novel accurate Unsupervised method
for joint Entity alignment (EA) and Dangling entity detection (DED), called
UED. The UED mines the literal semantic information to generate pseudo entity
pairs and globally guided alignment information for EA and then utilizes the EA
results to assist the DED. We construct a medical cross-lingual knowledge graph
dataset, MedED, providing data for both the EA and DED tasks. Extensive
experiments demonstrate that in the EA task, UED achieves EA results comparable
to those of state-of-the-art supervised EA baselines and outperforms the
current state-of-the-art EA methods by combining supervised EA data. For the
DED task, UED obtains high-quality results without supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextConvoNet:A Convolutional Neural Network based Architecture for Text Classification. (arXiv:2203.05173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05173">
<div class="article-summary-box-inner">
<span><p>In recent years, deep learning-based models have significantly improved the
Natural Language Processing (NLP) tasks. Specifically, the Convolutional Neural
Network (CNN), initially used for computer vision, has shown remarkable
performance for text data in various NLP problems. Most of the existing
CNN-based models use 1-dimensional convolving filters n-gram detectors), where
each filter specialises in extracting n-grams features of a particular input
word embedding. The input word embeddings, also called sentence matrix, is
treated as a matrix where each row is a word vector. Thus, it allows the model
to apply one-dimensional convolution and only extract n-gram based features
from a sentence matrix. These features can be termed as intra-sentence n-gram
features. To the extent of our knowledge, all the existing CNN models are based
on the aforementioned concept. In this paper, we present a CNN-based
architecture TextConvoNet that not only extracts the intra-sentence n-gram
features but also captures the inter-sentence n-gram features in input text
data. It uses an alternative approach for input matrix representation and
applies a two-dimensional multi-scale convolutional operation on the input. To
evaluate the performance of TextConvoNet, we perform an experimental study on
five text classification datasets. The results are evaluated by using various
performance metrics. The experimental results show that the presented
TextConvoNet outperforms state-of-the-art machine learning and deep learning
models for text classification purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05203">
<div class="article-summary-box-inner">
<span><p>3D dense captioning is a recently-proposed novel task, where point clouds
contain more geometric information than the 2D counterpart. However, it is also
more challenging due to the higher complexity and wider variety of inter-object
relations. Existing methods only treat such relations as by-products of object
feature learning in graphs without specifically encoding them, which leads to
sub-optimal results. In this paper, aiming at improving 3D dense captioning via
capturing and utilizing the complex relations in the 3D scene, we propose MORE,
a Multi-Order RElation mining model, to support generating more descriptive and
comprehensive captions. Technically, our MORE encodes object relations in a
progressive manner since complex relations can be deduced from a limited number
of basic ones. We first devise a novel Spatial Layout Graph Convolution (SLGC),
which semantically encodes several first-order relations as edges of a graph
constructed over 3D object proposals. Next, from the resulting graph, we
further extract multiple triplets which encapsulate basic first-order relations
as the basic unit and construct several Object-centric Triplet Attention Graphs
(OTAG) to infer multi-order relations for every target object. The updated node
features from OTAG are aggregated and fed into the caption decoder to provide
abundant relational cues so that captions including diverse relations with
context objects can be generated. Extensive experiments on the Scan2Cap dataset
prove the effectiveness of our proposed MORE and its components, and we also
outperform the current state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods. (arXiv:2203.05227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05227">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) has made great progress in recent years due
to the development of deep learning techniques such as pre-trained language
models. This advancement has resulted in more fluent, coherent and even
properties controllable (e.g. stylistic, sentiment, length etc.) generation,
naturally leading to development in downstream tasks such as abstractive
summarization, dialogue generation, machine translation, and data-to-text
generation. However, the faithfulness problem that the generated text usually
contains unfaithful or non-factual information has become the biggest
challenge, which makes the performance of text generation unsatisfactory for
practical applications in many real-world scenarios. Many studies on analysis,
evaluation, and optimization methods for faithfulness problems have been
proposed for various tasks, but have not been organized, compared and discussed
in a combined manner. In this survey, we provide a systematic overview of the
research progress on the faithfulness problem of NLG, including problem
analysis, evaluation metrics and optimization methods. We organize the
evaluation and optimization methods for different tasks into a unified taxonomy
to facilitate comparison and learning across tasks. Several research trends are
discussed further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach. (arXiv:2203.05243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05243">
<div class="article-summary-box-inner">
<span><p>Temporal Sentence Grounding in Videos (TSGV), which aims to ground a natural
language sentence in an untrimmed video, has drawn widespread attention over
the past few years. However, recent studies have found that current benchmark
datasets may have obvious moment annotation biases, enabling several simple
baselines even without training to achieve SOTA performance. In this paper, we
take a closer look at existing evaluation protocols, and find both the
prevailing dataset and evaluation metrics are the devils that lead to
untrustworthy benchmarking. Therefore, we propose to re-organize the two
widely-used datasets, making the ground-truth moment distributions different in
the training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile,
we introduce a new evaluation metric "dR@n,IoU@m" that discounts the basic
recall scores to alleviate the inflating evaluation caused by biased datasets.
New benchmarking results indicate that our proposed evaluation protocols can
better monitor the research progress. Furthermore, we propose a novel
causality-based Multi-branch Deconfounding Debiasing (MDD) framework for
unbiased moment prediction. Specifically, we design a multi-branch deconfounder
to eliminate the effects caused by multiple confounders with causal
intervention. In order to help the model better align the semantics between
sentence queries and video moments, we enhance the representations during
feature encoding. Specifically, for textual information, the query is parsed
into several verb-centered phrases to obtain a more fine-grained textual
feature. For visual information, the positional information has been decomposed
from moment features to enhance representations of moments with diverse
locations. Extensive experiments demonstrate that our proposed approach can
achieve competitive results among existing SOTA approaches and outperform the
base model with great gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Backward and Forward: Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation. (arXiv:2203.05248v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05248">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation(NMT) models are usually trained via unidirectional
decoder which corresponds to optimizing one-step-ahead prediction. However,
this kind of unidirectional decoding framework may incline to focus on local
structure rather than global coherence. To alleviate this problem, we propose a
novel method, Self-Knowledge Distillation with Bidirectional Decoder for Neural
Machine Translation(SBD-NMT). We deploy a backward decoder which can act as an
effective regularization method to the forward decoder. By leveraging the
backward decoder's information about the longer-term future, distilling
knowledge learned in the backward decoder can encourage auto-regressive NMT
models to plan ahead. Experiments show that our method is significantly better
than the strong Transformer baselines on multiple machine translation data
sets. Our codes will be released on github soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting Neural Response measurements & Computational Models of language: a non-comprehensive guide. (arXiv:2203.05300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05300">
<div class="article-summary-box-inner">
<span><p>Understanding the neural basis of language comprehension in the brain has
been a long-standing goal of various scientific research programs. Recent
advances in language modelling and in neuroimaging methodology promise
potential improvements in both the investigation of language's neurobiology and
in the building of better and more human-like language models. This survey
traces a line from early research linking Event Related Potentials and
complexity measures derived from simple language models to contemporary studies
employing Artificial Neural Network models trained on large corpora in
combination with neural response recordings from multiple modalities using
naturalistic stimuli.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05321">
<div class="article-summary-box-inner">
<span><p>We present StyleBabel, a unique open access dataset of natural language
captions and free-form tags describing the artistic style of over 135K digital
artworks, collected via a novel participatory method from experts studying at
specialist art and design schools. StyleBabel was collected via an iterative
method, inspired by `Grounded Theory': a qualitative approach that enables
annotation while co-evolving a shared language for fine-grained artistic style
attribute description. We demonstrate several downstream tasks for StyleBabel,
adapting the recent ALADIN architecture for fine-grained style similarity, to
train cross-modal embeddings for: 1) free-form tag generation; 2) natural
language description of artistic style; 3) fine-grained text search of style.
To do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and
cross-modal representation learning, achieving a state of the art accuracy in
fine-grained style retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities. (arXiv:2203.05325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05325">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an end-to-end joint entity and relation extraction
approach based on transformer-based language models. We apply the model to the
task of linking mathematical symbols to their descriptions in LaTeX documents.
In contrast to existing approaches, which perform entity and relation
extraction in sequence, our system incorporates information from relation
extraction into entity extraction. This means that the system can be trained
even on data sets where only a subset of all valid entity spans is annotated.
We provide an extensive evaluation of the proposed system and its strengths and
weaknesses. Our approach, which can be scaled dynamically in computational
complexity at inference time, produces predictions with high precision and
reaches 3rd place in the leaderboard of SemEval-2022 Task 12. For inputs in the
domain of physics and math, it achieves high relation extraction macro f1
scores of 95.43% and 79.17%, respectively. The code used for training and
evaluating our models is available at: https://github.com/nicpopovic/RE1st
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-enriched Attention Network with Group-wise Semantic for Visual Storytelling. (arXiv:2203.05346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05346">
<div class="article-summary-box-inner">
<span><p>As a technically challenging topic, visual storytelling aims at generating an
imaginary and coherent story with narrative multi-sentences from a group of
relevant images. Existing methods often generate direct and rigid descriptions
of apparent image-based contents, because they are not capable of exploring
implicit information beyond images. Hence, these schemes could not capture
consistent dependencies from holistic representation, impairing the generation
of reasonable and fluent story. To address these problems, a novel
knowledge-enriched attention network with group-wise semantic model is
proposed. Three main novel components are designed and supported by substantial
experiments to reveal practical advantages. First, a knowledge-enriched
attention network is designed to extract implicit concepts from external
knowledge system, and these concepts are followed by a cascade cross-modal
attention mechanism to characterize imaginative and concrete representations.
Second, a group-wise semantic module with second-order pooling is developed to
explore the globally consistent guidance. Third, a unified one-stage story
generation model with encoder-decoder structure is proposed to simultaneously
train and infer the knowledge-enriched attention network, group-wise semantic
module and multi-modal story generation decoder in an end-to-end fashion.
Substantial experiments on the popular Visual Storytelling dataset with both
objective and subjective evaluation metrics demonstrate the superior
performance of the proposed scheme as compared with other state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SATLab at SemEval-2022 Task 4: Trying to Detect Patronizing and Condescending Language with only Character and Word N-grams. (arXiv:2203.05355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05355">
<div class="article-summary-box-inner">
<span><p>A logistic regression model only fed with character and word n-grams is
proposed for the SemEval-2022 Task 4 on Patronizing and Condescending Language
Detection (PCL). It obtained an average level of performance, well above the
performance of a system that tries to guess without using any knowledge about
the task, but much lower than the best teams. As the proposed model is very
similar to the one that performed well on a task requiring to automatically
identify hate speech and offensive content, this paper confirms the difficulty
of PCL detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset of Stuttering. (arXiv:2203.05383v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05383">
<div class="article-summary-box-inner">
<span><p>Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation. (arXiv:2203.05386v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05386">
<div class="article-summary-box-inner">
<span><p>While there has been a lot of research and many recent advances in neural
fake news detection, defending against human-written disinformation remains
underexplored. Upon analyzing current approaches for fake news generation and
human-crafted articles, we found that there is a gap between them, which can
explain the poor performance on detecting human-written fake news for detectors
trained on automatically generated data. To address this issue, we propose a
novel framework for generating articles closer to human-written ones.
Specifically, we perform self-critical sequence training with natural language
inference to ensure the validity of the generated articles. We then explicitly
incorporate propaganda techniques into the generated articles to mimic how
humans craft fake news. Eventually, we create a fake news detection training
dataset, PropaNews, which includes 2,256 examples. Our experimental results
show that detectors trained on PropaNews are 7.3% to 12.0% more accurate for
detecting human-written disinformation than for counterparts trained on data
generated by state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OneRel:Joint Entity and Relation Extraction with One Module in One Step. (arXiv:2203.05412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05412">
<div class="article-summary-box-inner">
<span><p>Joint entity and relation extraction is an essential task in natural language
processing and knowledge graph construction. Existing approaches usually
decompose the joint extraction task into several basic modules or processing
steps to make it easy to conduct. However, such a paradigm ignores the fact
that the three elements of a triple are interdependent and indivisible.
Therefore, previous joint methods suffer from the problems of cascading errors
and redundant information. To address these issues, in this paper, we propose a
novel joint entity and relation extraction model, named OneRel, which casts
joint extraction as a fine-grained triple classification problem. Specifically,
our model consists of a scoring-based classifier and a relation-specific horns
tagging strategy. The former evaluates whether a token pair and a relation
belong to a factual triple. The latter ensures a simple but effective decoding
process. Extensive experimental results on two widely used datasets demonstrate
that the proposed method performs better than the state-of-the-art baselines,
and delivers consistent performance gain on complex scenarios of various
overlapping patterns and multiple triples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Norm Recognition and its application to Portuguese Law. (arXiv:2203.05425v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05425">
<div class="article-summary-box-inner">
<span><p>Being able to clearly interpret legal texts and fully understanding our
rights, obligations and other legal norms has become progressively more
important in the digital society. However, simply giving citizens access to the
laws is not enough, as there is a need to provide meaningful information that
cater to their specific queries and needs. For this, it is necessary to extract
the relevant semantic information present in legal texts. Thus, we introduce
the SNR (Semantic Norm Recognition) system, an automatic semantic information
extraction system trained on a domain-specific (legal) text corpus taken from
Portuguese Consumer Law. The SNR system uses the Portuguese Bert (BERTimbau)
and was trained on a legislative Portuguese corpus. We demonstrate how our
system achieved good results (81.44\% F1-score) on this domain-specific corpus,
despite existing noise, and how it can be used to improve downstream tasks such
as information retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages. (arXiv:2203.05437v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05437">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the IndicNLG suite, a collection of datasets for
benchmarking Natural Language Generation (NLG) for 11 Indic languages. We focus
on five diverse tasks, namely, biography generation using Wikipedia infoboxes
(WikiBio), news headline generation, sentence summarization, question
generation and paraphrase generation. We describe the process of creating the
datasets and present statistics of the dataset, following which we train and
report a variety of strong monolingual and multilingual baselines that leverage
pre-trained sequence-to-sequence models and analyze the results to understand
the challenges involved in Indic language NLG. To the best of our knowledge,
this is the first NLG dataset for Indic languages and also the largest
multilingual NLG dataset. Our methods can also be easily applied to
modest-resource languages with reasonable monolingual and parallel corpora, as
well as corpora containing structured data like Wikipedia. We hope this dataset
spurs research in NLG on diverse languages and tasks, particularly for Indic
languages. The datasets and models are publicly available at
https://indicnlp.ai4bharat.org/indicnlg-suite.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval. (arXiv:2203.05465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05465">
<div class="article-summary-box-inner">
<span><p>Dual encoders and cross encoders have been widely used for image-text
retrieval. Between the two, the dual encoder encodes the image and text
independently followed by a dot product, while the cross encoder jointly feeds
image and text as the input and performs dense multi-modal fusion. These two
architectures are typically modeled separately without interaction. In this
work, we propose LoopITR, which combines them in the same network for joint
learning. Specifically, we let the dual encoder provide hard negatives to the
cross encoder, and use the more discriminative cross encoder to distill its
predictions back to the dual encoder. Both steps are efficiently performed
together in the same model. Our work centers on empirical analyses of this
combined architecture, putting the main focus on the design of the distillation
objective. Our experimental results highlight the benefits of training the two
encoders in the same network, and demonstrate that distillation can be quite
effective with just a few hard negative examples. Experiments on two standard
datasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual
encoder performance when compared with approaches using a similar amount of
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05482">
<div class="article-summary-box-inner">
<span><p>The conventional recipe for maximizing model accuracy is to (1) train
multiple models with various hyperparameters and (2) pick the individual model
which performs best on a held-out validation set, discarding the remainder. In
this paper, we revisit the second step of this procedure in the context of
fine-tuning large pre-trained models, where fine-tuned models often appear to
lie in a single low error basin. We show that averaging the weights of multiple
models fine-tuned with different hyperparameter configurations often improves
accuracy and robustness. Unlike a conventional ensemble, we may average many
models without incurring any additional inference or memory costs -- we call
the results "model soups." When fine-tuning large pre-trained models such as
CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides
significant improvements over the best model in a hyperparameter sweep on
ImageNet. As a highlight, the resulting ViT-G model attains 90.94% top-1
accuracy on ImageNet, a new state of the art. Furthermore, we show that the
model soup approach extends to multiple image classification and natural
language processing tasks, improves out-of-distribution performance, and
improves zero-shot performance on new downstream tasks. Finally, we
analytically relate the performance similarity of weight-averaging and
logit-ensembling to flatness of the loss and confidence of the predictions, and
validate this relation empirically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Prompt Learning for Vision-Language Models. (arXiv:2203.05557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05557">
<div class="article-summary-box-inner">
<span><p>With the rise of powerful pre-trained vision-language models like CLIP, it
becomes essential to investigate ways to adapt these models to downstream
datasets. A recently proposed method named Context Optimization (CoOp)
introduces the concept of prompt learning -- a recent trend in NLP -- to the
vision domain for adapting pre-trained vision-language models. Specifically,
CoOp turns context words in a prompt into a set of learnable vectors and, with
only a few labeled images for learning, can achieve huge improvements over
intensively-tuned manual prompts. In our study we identify a critical problem
of CoOp: the learned context is not generalizable to wider unseen classes
within the same dataset, suggesting that CoOp overfits base classes observed
during training. To address the problem, we propose Conditional Context
Optimization (CoCoOp), which extends CoOp by further learning a lightweight
neural network to generate for each image an input-conditional token (vector).
Compared to CoOp's static prompts, our dynamic prompts adapt to each instance
and are thus less sensitive to class shift. Extensive experiments show that
CoCoOp generalizes much better than CoOp to unseen classes, even showing
promising transferability beyond a single dataset; and yields stronger domain
generalization performance as well. Code is available at
https://github.com/KaiyangZhou/CoOp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base. (arXiv:2007.03875v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03875">
<div class="article-summary-box-inner">
<span><p>Complex question answering over knowledge base (Complex KBQA) is challenging
because it requires various compositional reasoning capabilities, such as
multi-hop inference, attribute comparison, set operation. Existing benchmarks
have some shortcomings that limit the development of Complex KBQA: 1) they only
provide QA pairs without explicit reasoning processes; 2) questions are poor in
diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex
KBQA including ~120K diverse natural language questions. We introduce a
compositional and interpretable programming language KoPL to represent the
reasoning process of complex questions. For each question, we provide the
corresponding KoPL program and SPARQL query, so that KQA Pro serves for both
KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA
methods cannot achieve promising results on KQA Pro as on current datasets,
which suggests that KQA Pro is challenging and Complex KBQA requires further
research efforts. We also treat KQA Pro as a diagnostic dataset for testing
multiple reasoning skills, conduct a thorough evaluation of existing models and
discuss further directions for Complex KBQA. Our codes and datasets can be
obtained from https://github.com/shijx12/KQAPro_Baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Extreme Multi-label to Multi-class: A Hierarchical Approach for Automated ICD-10 Coding Using Phrase-level Attention. (arXiv:2102.09136v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09136">
<div class="article-summary-box-inner">
<span><p>Clinical coding is the task of assigning a set of alphanumeric codes,
referred to as ICD (International Classification of Diseases), to a medical
event based on the context captured in a clinical narrative. The latest version
of ICD, ICD-10, includes more than 70,000 codes. As this is a labor-intensive
and error-prone task, automatic ICD coding of medical reports using machine
learning has gained significant interest in the last decade. Existing
literature has modeled this problem as a multi-label task. Nevertheless, such
multi-label approach is challenging due to the extremely large label set size.
Furthermore, the interpretability of the predictions is essential for the
endusers (e.g., healthcare providers and insurance companies). In this paper,
we propose a novel approach for automatic ICD coding by reformulating the
extreme multi-label problem into a simpler multi-class problem using a
hierarchical solution. We made this approach viable through extensive data
collection to acquire phrase-level human coder annotations to supervise our
models on learning the specific relations between the input text and predicted
ICD codes. Our approach employs two independently trained networks, the
sentence tagger and the ICD classifier, stacked hierarchically to predict a
codeset for a medical report. The sentence tagger identifies focus sentences
containing a medical event or concept relevant to an ICD coding. Using a
supervised attention mechanism, the ICD classifier then assigns each focus
sentence with an ICD code. The proposed approach outperforms strong baselines
by large margins of 23% in subset accuracy, 18% in micro-F1, and 15% in
instance based F-1. With our proposed approach, interpretability is achieved
not through implicitly learned attention scores but by attributing each
prediction to a particular sentence and words selected by human coders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SummScreen: A Dataset for Abstractive Screenplay Summarization. (arXiv:2104.07091v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07091">
<div class="article-summary-box-inner">
<span><p>We introduce SummScreen, a summarization dataset comprised of pairs of TV
series transcripts and human written recaps. The dataset provides a challenging
testbed for abstractive summarization for several reasons. Plot details are
often expressed indirectly in character dialogues and may be scattered across
the entirety of the transcript. These details must be found and integrated to
form the succinct plot descriptions in the recaps. Also, TV scripts contain
content that does not directly pertain to the central plot but rather serves to
develop characters or provide comic relief. This information is rarely
contained in recaps. Since characters are fundamental to TV series, we also
propose two entity-centric evaluation metrics. Empirically, we characterize the
dataset by evaluating several methods, including neural models and those based
on nearest neighbors. An oracle extractive approach outperforms all benchmarked
models according to automatic metrics, showing that the neural models are
unable to fully exploit the input transcripts. Human evaluation and qualitative
analysis reveal that our non-oracle models are competitive with their oracle
counterparts in terms of generating faithful plot events and can benefit from
better content selectors. Both oracle and non-oracle models generate unfaithful
facts, suggesting future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Neurons in Pretrained Transformers. (arXiv:2104.08696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08696">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained language models are surprisingly good at recalling
factual knowledge presented in the training corpus. In this paper, we present
preliminary studies on how factual knowledge is stored in pretrained
Transformers by introducing the concept of knowledge neurons. Specifically, we
examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we
propose a knowledge attribution method to identify the neurons that express the
fact. We find that the activation of such knowledge neurons is positively
correlated to the expression of their corresponding facts. In our case studies,
we attempt to leverage knowledge neurons to edit (such as update, and erase)
specific factual knowledge without fine-tuning. Our results shed light on
understanding the storage of knowledge within pretrained Transformers. The code
is available at https://github.com/Hunter-DDM/knowledge-neurons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation Of Word Embeddings From Large-Scale French Web Content. (arXiv:2105.01990v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01990">
<div class="article-summary-box-inner">
<span><p>Distributed word representations are popularly used in many tasks in natural
language processing. Adding that pretrained word vectors on huge text corpus
achieved high performance in many different NLP tasks. This paper introduces
multiple high-quality word vectors for the French language where two of them
are trained on massive crawled French data during this study and the others are
trained on an already existing French corpus. We also evaluate the quality of
our proposed word vectors and the existing French word vectors on the French
word analogy task. In addition, we do the evaluation on multiple real NLP tasks
that shows the important performance enhancement of the pre-trained word
vectors compared to the existing and random ones. Finally, we created a demo
web application to test and visualize the obtained word embeddings. The
produced French word embeddings are available to the public, along with the
finetuning code on the NLU tasks and the demo code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing to New Domains by Mapping Natural Language to Lifted LTL. (arXiv:2110.05603v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05603">
<div class="article-summary-box-inner">
<span><p>Recent work on using natural language to specify commands to robots has
grounded that language to LTL. However, mapping natural language task
specifications to LTL task specifications using language models require
probability distributions over finite vocabulary. Existing state-of-the-art
methods have extended this finite vocabulary to include unseen terms from the
input sequence to improve output generalization. However, novel
out-of-vocabulary atomic propositions cannot be generated using these methods.
To overcome this, we introduce an intermediate contextual query representation
which can be learned from single positive task specification examples,
associating a contextual query with an LTL template. We demonstrate that this
intermediate representation allows for generalization over unseen object
references, assuming accurate groundings are available. We compare our method
of mapping natural language task specifications to intermediate contextual
queries against state-of-the-art CopyNet models capable of translating natural
language to LTL, by evaluating whether correct LTL for manipulation and
navigation task specifications can be output, and show that our method
outperforms the CopyNet model on unseen object references. We demonstrate that
the grounded LTL our method outputs can be used for planning in a simulated
OO-MDP environment. Finally, we discuss some common failure modes encountered
when translating natural language task specifications to grounded LTL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07855">
<div class="article-summary-box-inner">
<span><p>Abstract Meaning Representation (AMR) parsing aims to translate sentences to
semantic representation with a hierarchical structure, and is recently
empowered by pretrained sequence-to-sequence models. However, there exists a
gap between their flat training objective (i.e., equally treats all output
tokens) and the hierarchical AMR structure, which limits the model
generalization. To bridge this gap, we propose a Hierarchical Curriculum
Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula
(IC). SC switches progressively from core to detail AMR semantic elements while
IC transits from structure-simple to -complex AMR instances during training.
Through these two warming-up processes, HCL reduces the difficulty of learning
complex structures, thus the flat model can better adapt to the AMR hierarchy.
Extensive experiments on AMR2.0, AMR3.0, structure-complex and
out-of-distribution situations verify the effectiveness of HCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail. (arXiv:2110.08300v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08300">
<div class="article-summary-box-inner">
<span><p>Researchers in NLP often frame and discuss research results in ways that
serve to deemphasize the field's successes, often in response to the field's
widespread hype. Though well-meaning, this has yielded many misleading or false
claims about the limits of our best technology. This is a problem, and it may
be more serious than it looks: It harms our credibility in ways that can make
it harder to mitigate present-day harms, like those involving biased systems
for content moderation or resume screening. It also limits our ability to
prepare for the potentially enormous impacts of more distant future advances.
This paper urges researchers to be careful about these claims and suggests some
research directions and communication strategies that will make it easier to
avoid or rebut them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems. (arXiv:2110.08464v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08464">
<div class="article-summary-box-inner">
<span><p>Math Word Problem (MWP) solving needs to discover the quantitative
relationships over natural language narratives. Recent work shows that existing
models memorize procedures from context and rely on shallow heuristics to solve
MWPs. In this paper, we look at this issue and argue that the cause is a lack
of overall understanding of MWP patterns. We first investigate how a neural
network understands patterns only from semantics, and observe that, if the
prototype equations are the same, most problems get closer representations and
those representations apart from them or close to other prototypes tend to
produce wrong solutions. Inspired by it, we propose a contrastive learning
approach, where the neural network perceives the divergence of patterns. We
collect contrastive examples by converting the prototype equation into a tree
and seeking similar tree structures. The solving model is trained with an
auxiliary objective on the collected examples, resulting in the representations
of problems with similar prototypes being pulled closer. We conduct experiments
on the Chinese dataset Math23k and the English dataset MathQA. Our method
greatly improves the performance in monolingual and multilingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An unsupervised extractive summarization method based on multi-round computation. (arXiv:2112.03203v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03203">
<div class="article-summary-box-inner">
<span><p>Text summarization methods have attracted much attention all the time. In
recent years, deep learning has been applied to text summarization, and it
turned out to be pretty effective. Most of the current text summarization
methods based on deep learning are supervised methods which need large-scale
datasets. However, large-scale datasets are difficult to obtain in practical
applications. In this paper, an unsupervised extractive text summarization
method based on multi-round calculation is proposed. Based on the directed
graph algorithm, we change the common method which calculates the sentence
ranking at one time to multi-round calculation, and we dynamically optimize the
relation of sentences after each round of calculation to reduce the redundancy
of summarization. Experiments are carried out on four data sets, each
separately containing Chinese, English, long and short texts. The experiment
results show that our method has better performance than other unsupervised
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatically Generating Counterfactuals for Relation Classification. (arXiv:2202.10668v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10668">
<div class="article-summary-box-inner">
<span><p>The goal of relation classification (RC) is to extract the semantic relations
between/among entities in the text. As a fundamental task in natural language
processing, it is crucial to ensure the robustness of RC models. Despite the
high accuracy current deep neural models have achieved in RC tasks, they are
easily affected by spurious correlations. One solution to this problem is to
train the model with counterfactually augmented data (CAD) such that it can
learn the causation rather than the confounding. However, no attempt has been
made on generating counterfactuals for RC tasks. In this paper, we formulate
the problem of automatically generating CAD for RC tasks from an entity-centric
viewpoint, and develop a novel approach to derive contextual counterfactuals
for entities. Specifically, we exploit two elementary topological properties,
i.e., the centrality and the shortest path, in syntactic and semantic
dependency graphs, to first identify and then intervene on the contextual
causal features for entities. We conduct a comprehensive evaluation on four RC
datasets by combining our proposed approach with a variety of backbone RC
models. The results demonstrate that our approach not only improves the
performance of the backbones, but also makes them more robust in the
out-of-domain test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Simultaneous to Streaming Machine Translation by Leveraging Streaming History. (arXiv:2203.02459v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02459">
<div class="article-summary-box-inner">
<span><p>Simultaneous Machine Translation is the task of incrementally translating an
input sentence before it is fully available. Currently, simultaneous
translation is carried out by translating each sentence independently of the
previously translated text. More generally, Streaming MT can be understood as
an extension of Simultaneous MT to the incremental translation of a continuous
input text stream. In this work, a state-of-the-art simultaneous sentence-level
MT system is extended to the streaming setup by leveraging the streaming
history. Extensive empirical results are reported on IWSLT Translation Tasks,
showing that leveraging the streaming history leads to significant quality
gains. In particular, the proposed system proves to compare favorably to the
best performing systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02932">
<div class="article-summary-box-inner">
<span><p>Huge volumes of patient queries are daily generated on online health forums,
rendering manual doctor allocation a labor-intensive task. To better help
patients, this paper studies a novel task of doctor recommendation to enable
automatic pairing of a patient to a doctor with relevant expertise. While most
prior work in recommendation focuses on modeling target users from their past
behavior, we can only rely on the limited words in a query to infer a patient's
needs for privacy reasons. For doctor modeling, we study the joint effects of
their profiles and previous dialogues with other patients and explore their
interactions via self-learning. The learned doctor embeddings are further
employed to estimate their capabilities of handling a patient query with a
multi-head attention mechanism. For experiments, a large-scale dataset is
collected from Chunyu Yisheng, a Chinese online health forum, where our model
exhibits the state-of-the-art results, outperforming baselines only consider
profiles and past dialogues to characterize a doctor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building and curating conversational corpora for diversity-aware language science and technology. (arXiv:2203.03399v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03399">
<div class="article-summary-box-inner">
<span><p>We present a pipeline and tools to build a maximally natural data set of
conversational interaction that covers 66 languages and varieties from 32
phyla. We describe the curation and compilation process moving from diverse
language documentation corpora to a unified format and describe an open-source
tool "convo-parse" to help in quality control and assessment of conversational
data. We conclude with two case studies of how diverse data sets can inform
interactional linguistics and speech recognition technology and thus contribute
to broadening the empirical foundations of language sciences and technologies
of the future.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Fluid registration between lung CT and stationary chest tomosynthesis images. (arXiv:2203.04958v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04958">
<div class="article-summary-box-inner">
<span><p>Registration is widely used in image-guided therapy and image-guided surgery
to estimate spatial correspondences between organs of interest between planning
and treatment images. However, while high-quality computed tomography (CT)
images are often available at planning time, limited angle acquisitions are
frequently used during treatment because of radiation concerns or imaging time
constraints. This requires algorithms to register CT images based on limited
angle acquisitions. We, therefore, formulate a 3D/2D registration approach
which infers a 3D deformation based on measured projections and digitally
reconstructed radiographs of the CT. Most 3D/2D registration approaches use
simple transformation models or require complex mathematical derivations to
formulate the underlying optimization problem. Instead, our approach entirely
relies on differentiable operations which can be combined with modern
computational toolboxes supporting automatic differentiation. This then allows
for rapid prototyping, integration with deep neural networks, and to support a
variety of transformation models including fluid flow models. We demonstrate
our approach for the registration between CT and stationary chest tomosynthesis
(sDCT) images and show how it naturally leads to an iterative image
reconstruction approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities. (arXiv:2203.04959v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04959">
<div class="article-summary-box-inner">
<span><p>Multiple Sclerosis (MS) is a chronic neuroinflammatory disease and
multi-modality MRIs are routinely used to monitor MS lesions. Many automatic MS
lesion segmentation models have been developed and have reached human-level
performance. However, most established methods assume the MRI modalities used
during training are also available during testing, which is not guaranteed in
clinical practice. A training strategy termed Modality Dropout (ModDrop) has
been applied to MS lesion segmentation to achieve the state-of-the-art
performance for missing modality. We present a novel method dubbed ModDrop++ to
train a unified network adaptive to an arbitrary number of input MRI sequences.
Moreover, ModDrop++ can be easily applied to any existing model architectures.
Specifically, ModDrop++ upgrades the main idea of ModDrop in two key ways.
First, we devise a plug-and-play dynamic head and adopt a filter scaling
strategy to improve the expressiveness of the network. Second, we design a
co-training strategy to leverage the intra-subject relation between full
modality and missing modality. In particular, the intra-subject co-training
strategy aims to guide the dynamic head to generate similar feature
representations between the full- and missing-modality data from the same
subject. We use two public MS datasets to show the superiority of ModDrop++.
Source code and trained models are available at
https://github.com/han-liu/ModDropPlusPlus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-augmented Deep Unfolding Network for Guided Image Super-resolution. (arXiv:2203.04960v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04960">
<div class="article-summary-box-inner">
<span><p>Guided image super-resolution (GISR) aims to obtain a high-resolution (HR)
target image by enhancing the spatial resolution of a low-resolution (LR)
target image under the guidance of a HR image. However, previous model-based
methods mainly takes the entire image as a whole, and assume the prior
distribution between the HR target image and the HR guidance image, simply
ignoring many non-local common characteristics between them. To alleviate this
issue, we firstly propose a maximal a posterior (MAP) estimation model for GISR
with two types of prior on the HR target image, i.e., local implicit prior and
global implicit prior. The local implicit prior aims to model the complex
relationship between the HR target image and the HR guidance image from a local
perspective, and the global implicit prior considers the non-local
auto-regression property between the two images from a global perspective.
Secondly, we design a novel alternating optimization algorithm to solve this
model for GISR. The algorithm is in a concise framework that facilitates to be
replicated into commonly used deep network structures. Thirdly, to reduce the
information loss across iterative stages, the persistent memory mechanism is
introduced to augment the information representation by exploiting the Long
short-term memory unit (LSTM) in the image and feature spaces. In this way, a
deep network with certain interpretation and high representation ability is
built. Extensive experimental results validate the superiority of our method on
a variety of GISR tasks, including Pan-sharpening, depth image
super-resolution, and MR image super-resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04961">
<div class="article-summary-box-inner">
<span><p>Early detection of breast cancer in mammography screening via deep-learning
based computer-aided detection systems shows promising potential in improving
the curability and mortality rates of breast cancer. However, many clinical
centres are restricted in the amount and heterogeneity of available data to
train such models to (i) achieve promising performance and to (ii) generalise
well across acquisition protocols and domains. As sharing data between centres
is restricted due to patient privacy concerns, we propose a potential solution:
sharing trained generative models between centres as substitute for real
patient data. In this work, we use three well known mammography datasets to
simulate three different centres, where one centre receives the trained
generator of Generative Adversarial Networks (GANs) from the two remaining
centres in order to augment the size and heterogeneity of its training dataset.
We evaluate the utility of this approach on mammography patch classification on
the test set of the GAN-receiving centre using two different classification
models, (a) a convolutional neural network and (b) a transformer neural
network. Our experiments demonstrate that shared GANs notably increase the
performance of both transformer and convolutional classification models and
highlight this approach as a viable alternative to inter-centre data sharing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Degradation Distribution for Blind Image Super-Resolution. (arXiv:2203.04962v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04962">
<div class="article-summary-box-inner">
<span><p>Synthetic high-resolution (HR) \&amp; low-resolution (LR) pairs are widely used
in existing super-resolution (SR) methods. To avoid the domain gap between
synthetic and test images, most previous methods try to adaptively learn the
synthesizing (degrading) process via a deterministic model. However, some
degradations in real scenarios are stochastic and cannot be determined by the
content of the image. These deterministic models may fail to model the random
factors and content-independent parts of degradations, which will limit the
performance of the following SR models. In this paper, we propose a
probabilistic degradation model (PDM), which studies the degradation
$\mathbf{D}$ as a random variable, and learns its distribution by modeling the
mapping from a priori random variable $\mathbf{z}$ to $\mathbf{D}$. Compared
with previous deterministic degradation models, PDM could model more diverse
degradations and generate HR-LR pairs that may better cover the various
degradations of test images, and thus prevent the SR model from over-fitting to
specific ones. Extensive experiments have demonstrated that our degradation
model can help the SR model achieve better performance on different datasets.
The source codes are released at \url{git@github.com:greatlog/UnpairedSR.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04963">
<div class="article-summary-box-inner">
<span><p>Learned image compression has achieved great success due to its excellent
modeling capacity, but seldom further considers the Rate-Distortion
Optimization (RDO) of each input image. To explore this potential in the
learned codec, we make the first attempt to build a neural data-dependent
transform and introduce a continuous online mode decision mechanism to jointly
optimize the coding efficiency for each individual image. Specifically, apart
from the image content stream, we employ an additional model stream to generate
the transform parameters at the decoder side. The presence of a model stream
enables our model to learn more abstract neural-syntax, which helps cluster the
latent representations of images more compactly. Beyond the transform stage, we
also adopt neural-syntax based post-processing for the scenarios that require
higher quality reconstructions regardless of extra decoding overhead. Moreover,
the involvement of the model stream further makes it possible to optimize both
the representation and the decoder in an online way, i.e. RDO at the testing
time. It is equivalent to a continuous online mode decision, like coding modes
in the traditional codecs, to improve the coding efficiency based on the
individual input image. The experimental results show the effectiveness of the
proposed neural-syntax design and the continuous online mode decision
mechanism, demonstrating the superiority of our method in coding efficiency
compared to the latest conventional standard Versatile Video Coding (VVC) and
other state-of-the-art learning-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metastatic Cancer Outcome Prediction with Injective Multiple Instance Pooling. (arXiv:2203.04964v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04964">
<div class="article-summary-box-inner">
<span><p>Cancer stage is a large determinant of patient prognosis and management in
many cancer types, and is often assessed using medical imaging modalities, such
as CT and MRI. These medical images contain rich information that can be
explored to stratify patients within each stage group to further improve
prognostic algorithms. Although the majority of cancer deaths result from
metastatic and multifocal disease, building imaging biomarkers for patients
with multiple tumors has been a challenging task due to the lack of annotated
datasets and standard study framework. In this paper, we process two public
datasets to set up a benchmark cohort of 341 patient in total for studying
outcome prediction of multifocal metastatic cancer. We identify the lack of
expressiveness in common multiple instance classification networks and propose
two injective multiple instance pooling functions that are better suited to
outcome prediction. Our results show that multiple instance learning with
injective pooling functions can achieve state-of-the-art performance in the
non-small-cell lung cancer CT and head and neck CT outcome prediction
benchmarking tasks. We will release the processed multifocal datasets, our code
and the intermediate files i.e. extracted radiomic features to support further
transparent and reproducible research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNeXt: MLP-based Rapid Medical Image Segmentation Network. (arXiv:2203.04967v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04967">
<div class="article-summary-box-inner">
<span><p>UNet and its latest extensions like TransUNet have been the leading medical
image segmentation methods in recent years. However, these networks cannot be
effectively adopted for rapid image segmentation in point-of-care applications
as they are parameter-heavy, computationally complex and slow to use. To this
end, we propose UNeXt which is a Convolutional multilayer perceptron (MLP)
based network for image segmentation. We design UNeXt in an effective way with
an early convolutional stage and a MLP stage in the latent stage. We propose a
tokenized MLP block where we efficiently tokenize and project the convolutional
features and use MLPs to model the representation. To further boost the
performance, we propose shifting the channels of the inputs while feeding in to
MLPs so as to focus on learning local dependencies. Using tokenized MLPs in
latent space reduces the number of parameters and computational complexity
while being able to result in a better representation to help segmentation. The
network also consists of skip connections between various levels of encoder and
decoder. We test UNeXt on multiple medical image segmentation datasets and show
that we reduce the number of parameters by 72x, decrease the computational
complexity by 68x, and improve the inference speed by 10x while also obtaining
better segmentation performance over the state-of-the-art medical image
segmentation architectures. Code is available at
https://github.com/jeya-maria-jose/UNeXt-pytorch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resource-Efficient Invariant Networks: Exponential Gains by Unrolled Optimization. (arXiv:2203.05006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05006">
<div class="article-summary-box-inner">
<span><p>Achieving invariance to nuisance transformations is a fundamental challenge
in the construction of robust and reliable vision systems. Existing approaches
to invariance scale exponentially with the dimension of the family of
transformations, making them unable to cope with natural variabilities in
visual data such as changes in pose and perspective. We identify a common
limitation of these approaches--they rely on sampling to traverse the
high-dimensional space of transformations--and propose a new computational
primitive for building invariant networks based instead on optimization, which
in many scenarios provides a provably more efficient method for
high-dimensional exploration than sampling. We provide empirical and
theoretical corroboration of the efficiency gains and soundness of our proposed
method, and demonstrate its utility in constructing an efficient invariant
network for a simple hierarchical object detection task when combined with
unrolled optimization. Code for our networks and experiments is available at
https://github.com/sdbuch/refine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Instance Domain Adaptation. (arXiv:2203.05028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05028">
<div class="article-summary-box-inner">
<span><p>Most existing studies on unsupervised domain adaptation (UDA) assume that
each domain's training samples come with domain labels (e.g., painting, photo).
Samples from each domain are assumed to follow the same distribution and the
domain labels are exploited to learn domain-invariant features via feature
alignment. However, such an assumption often does not hold true -- there often
exist numerous finer-grained domains (e.g., dozens of modern painting styles
have been developed, each differing dramatically from those of the classic
styles). Therefore, forcing feature distribution alignment across each
artificially-defined and coarse-grained domain can be ineffective. In this
paper, we address both single-source and multi-source UDA from a completely
different perspective, which is to view each instance as a fine domain. Feature
alignment across domains is thus redundant. Instead, we propose to perform
dynamic instance domain adaptation (DIDA). Concretely, a dynamic neural network
with adaptive convolutional kernels is developed to generate instance-adaptive
residuals to adapt domain-agnostic deep features to each individual instance.
This enables a shared classifier to be applied to both source and target domain
data without relying on any domain annotation. Further, instead of imposing
intricate feature alignment losses, we adopt a simple semi-supervised learning
paradigm using only a cross-entropy loss for both labeled source and pseudo
labeled target data. Our model, dubbed DIDA-Net, achieves state-of-the-art
performance on several commonly used single-source and multi-source UDA
datasets including Digits, Office-Home, DomainNet, Digit-Five, and PACS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Trajectory Prediction via Transferable GNN. (arXiv:2203.05046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05046">
<div class="article-summary-box-inner">
<span><p>Pedestrian trajectory prediction is an essential component in a wide range of
AI applications such as autonomous driving and robotics. Existing methods
usually assume the training and testing motions follow the same pattern while
ignoring the potential distribution differences (e.g., shopping mall and
street). This issue results in inevitable performance decrease. To address this
issue, we propose a novel Transferable Graph Neural Network (T-GNN) framework,
which jointly conducts trajectory prediction as well as domain alignment in a
unified framework. Specifically, a domain invariant GNN is proposed to explore
the structural motion knowledge where the domain specific knowledge is reduced.
Moreover, an attention-based adaptive knowledge learning module is further
proposed to explore fine-grained individual-level feature representation for
knowledge transfer. By this way, disparities across different trajectory
domains will be better alleviated. More challenging while practical trajectory
prediction experiments are designed, and the experimental results verify the
superior performance of our proposed model. To the best of our knowledge, our
work is the pioneer which fills the gap in benchmarks and techniques for
practical pedestrian trajectory prediction across different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Proposed Fairness Models for Face Recognition Algorithms. (arXiv:2203.05051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05051">
<div class="article-summary-box-inner">
<span><p>The development of face recognition algorithms by academic and commercial
organizations is growing rapidly due to the onset of deep learning and the
widespread availability of training data. Though tests of face recognition
algorithm performance indicate yearly performance gains, error rates for many
of these systems differ based on the demographic composition of the test set.
These "demographic differentials" in algorithm performance can contribute to
unequal or unfair outcomes for certain groups of people, raising concerns with
increased worldwide adoption of face recognition systems. Consequently,
regulatory bodies in both the United States and Europe have proposed new rules
requiring audits of biometric systems for "discriminatory impacts" (European
Union Artificial Intelligence Act) and "fairness" (U.S. Federal Trade
Commission). However, no standard for measuring fairness in biometric systems
yet exists. This paper characterizes two proposed measures of face recognition
algorithm fairness (fairness measures) from scientists in the U.S. and Europe.
We find that both proposed methods are challenging to interpret when applied to
disaggregated face recognition error rates as they are commonly experienced in
practice. To address this, we propose a set of interpretability criteria,
termed the Functional Fairness Measure Criteria (FFMC), that outlines a set of
properties desirable in a face recognition algorithm fairness measure. We
further develop a new fairness measure, the Gini Aggregation Rate for Biometric
Equitability (GARBE), and show how, in conjunction with the Pareto
optimization, this measure can be used to select among alternative algorithms
based on the accuracy/fairness trade-space. Finally, we have open-sourced our
dataset of machine-readable, demographically disaggregated error rates. We
believe this is currently the largest open-source dataset of its kind.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Flow Training under Limited Label Budget via Active Learning. (arXiv:2203.05053v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05053">
<div class="article-summary-box-inner">
<span><p>Supervised training of optical flow predictors generally yields better
accuracy than unsupervised training. However, the improved performance comes at
an often high annotation cost. Semi-supervised training trades off accuracy
against annotation cost. We use a simple yet effective semi-supervised training
method to show that even a small fraction of labels can improve flow accuracy
by a significant margin over unsupervised training. In addition, we propose
active learning methods based on simple heuristics to further reduce the number
of labels required to achieve the same target accuracy. Our experiments on both
synthetic and real optical flow datasets show that our semi-supervised networks
generally need around 50% of the labels to achieve close to full-label
accuracy, and only around 20% with active learning on Sintel. We also analyze
and show insights on the factors that may influence our active learning
performance. Code will be made available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving. (arXiv:2203.05056v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05056">
<div class="article-summary-box-inner">
<span><p>Surround-view cameras are a primary sensor for automated driving, used for
near field perception. It is one of the most commonly used sensors in
commercial vehicles. Four fisheye cameras with a 190{\deg} field of view cover
the 360{\deg} around the vehicle. Due to its high radial distortion, the
standard algorithms do not extend easily. Previously, we released the first
public fisheye surround-view dataset named WoodScape. In this work, we release
a synthetic version of the surround-view dataset, covering many of its
weaknesses and extending it. Firstly, it is not possible to obtain ground truth
for pixel-wise optical flow and depth. Secondly, WoodScape did not have all
four cameras simultaneously in order to sample diverse frames. However, this
means that multi-camera algorithms cannot be designed, which is enabled in the
new dataset. We implemented surround-view fisheye geometric projections in
CARLA Simulator matching WoodScape's configuration and created SynWoodScape. We
release 80k images from the synthetic dataset with annotations for 10+ tasks.
We also release the baseline code and supporting scripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05074">
<div class="article-summary-box-inner">
<span><p>Paradoxically, a Variational Autoencoder (VAE) could be pushed in two
opposite directions, utilizing powerful decoder model for generating realistic
images but collapsing the learned representation, or increasing regularization
coefficient for disentangling representation but ultimately generating blurry
examples. Existing methods narrow the issues to the rate-distortion trade-off
between compression and reconstruction. We argue that a good reconstruction
model does learn high capacity latents that encode more details, however, its
use is hindered by two major issues: the prior is random noise which is
completely detached from the posterior and allow no controllability in the
generation; mean-field variational inference doesn't enforce hierarchy
structure which makes the task of recombining those units into plausible novel
output infeasible. As a result, we develop a system that learns a hierarchy of
disentangled representation together with a mechanism for recombining the
learned representation for generalization. This is achieved by introducing a
minimal amount of inductive bias to learn controllable prior for the VAE. The
idea is supported by here developed transitive information theory, that is, the
mutual information between two target variables could alternately be maximized
through the mutual information to the third variable, thus bypassing the
rate-distortion bottleneck in VAE design. In particular, we show that our
model, named SemafoVAE (inspired by the similar concept in computer science),
could generate high-quality examples in a controllable manner, perform smooth
traversals of the disentangled factors and intervention at a different level of
representation hierarchy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks. (arXiv:2203.05081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05081">
<div class="article-summary-box-inner">
<span><p>Natural language explanation (NLE) models aim at explaining the
decision-making process of a black box system via generating natural language
sentences which are human-friendly, high-level and fine-grained. Current NLE
models explain the decision-making process of a vision or vision-language model
(a.k.a., task model), e.g., a VQA model, via a language model (a.k.a.,
explanation model), e.g., GPT. Other than the additional memory resources and
inference time required by the task model, the task and explanation models are
completely independent, which disassociates the explanation from the reasoning
process made to predict the answer. We introduce NLX-GPT, a general, compact
and faithful language model that can simultaneously predict an answer and
explain it. We first conduct pre-training on large scale data of image-caption
pairs for general understanding of images, and then formulate the answer as a
text prediction task along with the explanation. Without region proposals nor a
task model, our resulting overall framework attains better evaluation scores,
contains much less parameters and is 15$\times$ faster than the current SoA
model. We then address the problem of evaluating the explanations which can be
in many times generic, data-biased and can come in several forms. We therefore
design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based
attack, a self-evaluation framework that requires no labels. Code is at:
https://github.com/fawazsammani/nlxgpt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Tree-Structured Multi-Task Model Recommender. (arXiv:2203.05092v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05092">
<div class="article-summary-box-inner">
<span><p>Tree-structured multi-task architectures have been employed to jointly tackle
multiple vision tasks in the context of multi-task learning (MTL). The major
challenge is to determine where to branch out for each task given a backbone
model to optimize for both task accuracy and computation efficiency. To address
the challenge, this paper proposes a recommender that, given a set of tasks and
a convolutional neural network-based backbone model, automatically suggests
tree-structured multi-task architectures that could achieve a high task
performance while meeting a user-specified computation budget without
performing model training. Extensive evaluations on popular MTL benchmarks show
that the recommended architectures could achieve competitive task accuracy and
computation efficiency compared with state-of-the-art MTL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural ODEs via Knowledge Distillation. (arXiv:2203.05103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05103">
<div class="article-summary-box-inner">
<span><p>Neural Ordinary Differential Equations (Neural ODEs) construct the continuous
dynamics of hidden units using ordinary differential equations specified by a
neural network, demonstrating promising results on many tasks. However, Neural
ODEs still do not perform well on image recognition tasks. The possible reason
is that the one-hot encoding vector commonly used in Neural ODEs can not
provide enough supervised information. We propose a new training based on
knowledge distillation to construct more powerful and robust Neural ODEs
fitting image recognition tasks. Specially, we model the training of Neural
ODEs into a teacher-student learning process, in which we propose ResNets as
the teacher model to provide richer supervised information. The experimental
results show that the new training manner can improve the classification
accuracy of Neural ODEs by 24% on CIFAR10 and 5% on SVHN. In addition, we also
quantitatively discuss the effect of both knowledge distillation and time
horizon in Neural ODEs on robustness against adversarial examples. The
experimental analysis concludes that introducing the knowledge distillation and
increasing the time horizon can improve the robustness of Neural ODEs against
adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenTAL: Towards Open Set Temporal Action Localization. (arXiv:2203.05114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05114">
<div class="article-summary-box-inner">
<span><p>Temporal Action Localization (TAL) has experienced remarkable success under
the supervised learning paradigm. However, existing TAL methods are rooted in
the closed set assumption, which cannot handle the inevitable unknown actions
in open-world scenarios. In this paper, we, for the first time, step toward the
Open Set TAL (OSTAL) problem and propose a general framework OpenTAL based on
Evidential Deep Learning (EDL). Specifically, the OpenTAL consists of
uncertainty-aware action classification, actionness prediction, and temporal
location regression. With the proposed importance-balanced EDL method,
classification uncertainty is learned by collecting categorical evidence
majorly from important samples. To distinguish the unknown actions from
background video frames, the actionness is learned by the positive-unlabeled
learning. The classification uncertainty is further calibrated by leveraging
the guidance from the temporal localization quality. The OpenTAL is general to
enable existing TAL models for open set scenarios, and experimental results on
THUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method.
The code and pre-trained models are released at
https://www.rit.edu/actionlab/opental.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervision semantic segmentation with uncertainty-guided self cross supervision. (arXiv:2203.05118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05118">
<div class="article-summary-box-inner">
<span><p>As a powerful way of realizing semi-supervised segmentation, the cross
supervision method learns cross consistency based on independent ensemble
models using abundant unlabeled images. However, the wrong pseudo labeling
information generated by cross supervision would confuse the training process
and negatively affect the effectiveness of the segmentation model. Besides, the
training process of ensemble models in such methods also multiplies the cost of
computation resources and decreases the training efficiency. To solve these
problems, we propose a novel cross supervision method, namely
uncertainty-guided self cross supervision (USCS). In addition to ensemble
models, we first design a multi-input multi-output (MIMO) segmentation model
which can generate multiple outputs with shared model and consequently impose
consistency over the outputs, saving the cost on parameters and calculations.
On the other hand, we employ uncertainty as guided information to encourage the
model to focus on the high confident regions of pseudo labels and mitigate the
effects of wrong pseudo labeling in self cross supervision, improving the
performance of the segmentation model. Extensive experiments show that our
method achieves state-of-the-art performance while saving 40.5% and 49.1% cost
on parameters and calculations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetAug: Contrastive Learning via Meta Feature Augmentation. (arXiv:2203.05119v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05119">
<div class="article-summary-box-inner">
<span><p>What matters for contrastive learning? We argue that contrastive learning
heavily relies on informative features, or "hard" (positive or negative)
features. Early works include more informative features by applying complex
data augmentations and large batch size or memory bank, and recent works design
elaborate sampling approaches to explore informative features. The key
challenge toward exploring such features is that the source multi-view data is
generated by applying random data augmentations, making it infeasible to always
add useful information in the augmented data. Consequently, the informativeness
of features learned from such augmented data is limited. In response, we
propose to directly augment the features in latent space, thereby learning
discriminative representations without a large amount of input data. We perform
a meta learning technique to build the augmentation generator that updates its
network parameters by considering the performance of the encoder. However,
insufficient input data may lead the encoder to learn collapsed features and
therefore malfunction the augmentation generator. A new margin-injected
regularization is further added in the objective function to avoid the encoder
learning a degenerate mapping. To contrast all features in one gradient
back-propagation step, we adopt the proposed optimization-driven unified
contrastive loss instead of the conventional contrastive loss. Empirically, our
method achieves state-of-the-art results on several benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEER: Detection-agnostic End-to-End Recognizer for Scene Text Spotting. (arXiv:2203.05122v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05122">
<div class="article-summary-box-inner">
<span><p>Recent end-to-end scene text spotters have achieved great improvement in
recognizing arbitrary-shaped text instances. Common approaches for text
spotting use region of interest pooling or segmentation masks to restrict
features to single text instances. However, this makes it hard for the
recognizer to decode correct sequences when the detection is not accurate i.e.
one or more characters are cropped out. Considering that it is hard to
accurately decide word boundaries with only the detector, we propose a novel
Detection-agnostic End-to-End Recognizer, DEER, framework. The proposed method
reduces the tight dependency between detection and recognition modules by
bridging them with a single reference point for each text instance, instead of
using detected regions. The proposed method allows the decoder to recognize the
texts that are indicated by the reference point, with features from the whole
image. Since only a single point is required to recognize the text, the
proposed method enables text spotting without an arbitrarily-shaped detector or
bounding polygon annotations. Experimental results present that the proposed
method achieves competitive results on regular and arbitrarily-shaped text
spotting benchmarks. Further analysis shows that DEER is robust to the
detection errors. The code and dataset will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifold Modeling in Quotient Space: Learning An Invariant Mapping with Decodability of Image Patches. (arXiv:2203.05134v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05134">
<div class="article-summary-box-inner">
<span><p>This study proposes a framework for manifold learning of image patches using
the concept of equivalence classes: manifold modeling in quotient space (MMQS).
In MMQS, we do not consider a set of local patches of the image as it is, but
rather the set of their canonical patches obtained by introducing the concept
of equivalence classes and performing manifold learning on their canonical
patches. Canonical patches represent equivalence classes, and their
auto-encoder constructs a manifold in the quotient space. Based on this
framework, we produce a novel manifold-based image model by introducing
rotation-flip-equivalence relations. In addition, we formulate an image
reconstruction problem by fitting the proposed image model to a corrupted
observed image and derive an algorithm to solve it. Our experiments show that
the proposed image model is effective for various self-supervised image
reconstruction tasks, such as image inpainting, deblurring, super-resolution,
and denoising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05137">
<div class="article-summary-box-inner">
<span><p>We consider the problem of Vision-and-Language Navigation (VLN). The majority
of current methods for VLN are trained end-to-end using either unstructured
memory such as LSTM, or using cross-modal attention over the egocentric
observations of the agent. In contrast to other works, our key insight is that
the association between language and vision is stronger when it occurs in
explicit spatial representations. In this work, we propose a cross-modal map
learning model for vision-and-language navigation that first learns to predict
the top-down semantics on an egocentric map for both observed and unobserved
regions, and then predicts a path towards the goal as a set of waypoints. In
both cases, the prediction is informed by the language through cross-modal
attention mechanisms. We experimentally test the basic hypothesis that
language-driven navigation can be solved given a map, and then show competitive
results on the full VLN-CE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intention-aware Feature Propagation Network for Interactive Segmentation. (arXiv:2203.05145v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05145">
<div class="article-summary-box-inner">
<span><p>We aim to tackle the problem of point-based interactive segmentation, in
which two key challenges are to infer user's intention correctly and to
propagate the user-provided annotations to unlabeled regions efficiently. To
address those challenges, we propose a novel intention-aware feature
propagation strategy that performs explicit user intention estimation and
learns an efficient click-augmented feature representation for high-resolution
foreground segmentation. Specifically, we develop a coarse-to-fine sparse
propagation network for each interactive segmentation step, which consists of a
coarse-level network for more effective tracking of user's interest, and a
fine-level network for zooming to the target object and performing fine-level
segmentation. Moreover, we design a new sparse graph network module for both
levels to enable efficient long-range propagation of click information.
Extensive experiments show that our method surpasses the previous
state-of-the-art methods on all popular benchmarks, demonstrating its efficacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05151">
<div class="article-summary-box-inner">
<span><p>Current adversarial attack research reveals the vulnerability of
learning-based classifiers against carefully crafted perturbations. However,
most existing attack methods have inherent limitations in cross-dataset
generalization as they rely on a classification layer with a closed set of
categories. Furthermore, the perturbations generated by these methods may
appear in regions easily perceptible to the human visual system (HVS). To
circumvent the former problem, we propose a novel algorithm that attacks
semantic similarity on feature representations. In this way, we are able to
fool classifiers without limiting attacks to a specific dataset. For
imperceptibility, we introduce the low-frequency constraint to limit
perturbations within high-frequency components, ensuring perceptual similarity
between adversarial examples and originals. Extensive experiments on three
datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online
platforms indicate that our attack can yield misleading and transferable
adversarial examples across architectures and datasets. Additionally,
visualization results and quantitative performance (in terms of four different
metrics) show that the proposed algorithm generates more imperceptible
perturbations than the state-of-the-art methods. Code is made available at.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05154">
<div class="article-summary-box-inner">
<span><p>Defense models against adversarial attacks have grown significantly, but the
lack of practical evaluation methods has hindered progress. Evaluation can be
defined as looking for defense models' lower bound of robustness given a budget
number of iterations and a test dataset. A practical evaluation method should
be convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and
reliable (i.e., approaching the lower bound of robustness). Towards this
target, we propose a parameter-free Adaptive Auto Attack (A$^3$) evaluation
method which addresses the efficiency and reliability in a test-time-training
fashion. Specifically, by observing that adversarial examples to a specific
defense model follow some regularities in their starting points, we design an
Adaptive Direction Initialization strategy to speed up the evaluation.
Furthermore, to approach the lower bound of robustness under the budget number
of iterations, we propose an online statistics-based discarding strategy that
automatically identifies and abandons hard-to-attack images. Extensive
experiments demonstrate the effectiveness of our A$^3$. Particularly, we apply
A$^3$ to nearly 50 widely-used defense models. By consuming much fewer
iterations than existing methods, i.e., $1/10$ on average (10$\times$ speed
up), we achieve lower robust accuracy in all cases. Notably, we won
$\textbf{first place}$ out of 1681 teams in CVPR 2021 White-box Adversarial
Attacks on Defense Models competitions with this method. Code is available at:
$\href{https://github.com/liuye6666/adaptive_auto_attack}{https://github.com/liuye6666/adaptive\_auto\_attack}$
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Action Recognition with Transformer-based Video Semantic Embedding. (arXiv:2203.05156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05156">
<div class="article-summary-box-inner">
<span><p>While video action recognition has been an active area of research for
several years, zero-shot action recognition has only recently started gaining
traction. However, there is a lack of a formal definition for the zero-shot
learning paradigm leading to uncertainty about classes that can be considered
as previously unseen. In this work, we take a new comprehensive look at the
inductive zero-shot action recognition problem from a realistic standpoint.
Specifically, we advocate for a concrete formulation for zero-shot action
recognition that avoids an exact overlap between the training and testing
classes and also limits the intra-class variance; and propose a novel
end-to-end trained transformer model which is capable of capturing long range
spatiotemporal dependencies efficiently, contrary to existing approaches which
use 3D-CNNs. The proposed approach outperforms the existing state-of-the-art
algorithms in many settings on all benchmark datasets by a wide margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP: Multimodality-guided Visual Pre-training. (arXiv:2203.05175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05175">
<div class="article-summary-box-inner">
<span><p>Recently, masked image modeling (MIM) has become a promising direction for
visual pre-training. In the context of vision transformers, MIM learns
effective visual representation by aligning the token-level features with a
pre-defined space (e.g., BEIT used a d-VAE trained on a large image corpus as
the tokenizer). In this paper, we go one step further by introducing guidance
from other modalities and validating that such additional knowledge leads to
impressive gains for visual pre-training. The proposed approach is named
Multimodality-guided Visual Pre-training (MVP), in which we replace the
tokenizer with the vision branch of CLIP, a vision-language model pre-trained
on 400 million image-text pairs. We demonstrate the effectiveness of MVP by
performing standard experiments, i.e., pre-training the ViT models on ImageNet
and fine-tuning them on a series of downstream visual recognition tasks. In
particular, pre-training ViT-Base/16 for 300 epochs, MVP reports a 52.4% mIoU
on ADE20K, surpassing BEIT (the baseline and previous state-of-the-art) with an
impressive margin of 6.8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Audio-Visual Attention Based Multimodal Network for Fake Talking Face Videos Detection. (arXiv:2203.05178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05178">
<div class="article-summary-box-inner">
<span><p>DeepFake based digital facial forgery is threatening the public media
security, especially when lip manipulation has been used in talking face
generation, the difficulty of fake video detection is further improved. By only
changing lip shape to match the given speech, the facial features of identity
is hard to be discriminated in such fake talking face videos. Together with the
lack of attention on audio stream as the prior knowledge, the detection failure
of fake talking face generation also becomes inevitable. Inspired by the
decision-making mechanism of human multisensory perception system, which
enables the auditory information to enhance post-sensory visual evidence for
informed decisions output, in this study, a fake talking face detection
framework FTFDNet is proposed by incorporating audio and visual representation
to achieve more accurate fake talking face videos detection. Furthermore, an
audio-visual attention mechanism (AVAM) is proposed to discover more
informative features, which can be seamlessly integrated into any audio-visual
CNN architectures by modularization. With the additional AVAM, the proposed
FTFDNet is able to achieve a better detection performance on the established
dataset (FTFDD). The evaluation of the proposed work has shown an excellent
performance on the detection of fake talking face videos, which is able to
arrive at a detection rate above 97%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Open-Set Text Recognition via Label-to-Prototype Learning. (arXiv:2203.05179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05179">
<div class="article-summary-box-inner">
<span><p>Scene text recognition is a popular topic and can benefit various tasks.
Although many methods have been proposed for the close-set text recognition
challenges, they cannot be directly applied to open-set scenarios, where the
evaluation set contains novel characters not appearing in the training set.
Conventional methods require collecting new data and retraining the model to
handle these novel characters, which is an expensive and tedious process. In
this paper, we propose a label-to-prototype learning framework to handle novel
characters without retraining the model. In the proposed framework, novel
characters are effectively mapped to their corresponding prototypes with a
label-to-prototype learning module. This module is trained on characters with
seen labels and can be easily generalized to novel characters. Additionally,
feature-level rectification is conducted via topology-preserving
transformation, resulting in better alignments between visual features and
constructed prototypes while having a reasonably small impact on model speed. A
lot of experiments show that our method achieves promising performance on a
variety of zero-shot, close-set, and open-set text recognition datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability. (arXiv:2203.05180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05180">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-training has been proven to be crucial for various computer
vision tasks. However, with the increase of pre-training data amount, model
architecture amount, and the private/inaccessible data, it is not very
efficient or possible to pre-train all the model architectures on large-scale
datasets. In this work, we investigate an alternative strategy for
pre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),
aiming to efficiently transfer the learned feature representation from existing
pre-trained models to new student models for future downstream tasks. We
observe that existing Knowledge Distillation (KD) methods are unsuitable
towards pre-training since they normally distill the logits that are going to
be discarded when transferred to downstream tasks. To resolve this problem, we
propose a feature-based KD method with non-parametric feature dimension
aligning. Notably, our method performs comparably with supervised pre-training
counterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less
data and 5x less pre-training time. Code is available at
https://github.com/CVMI-Lab/KDEP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding. (arXiv:2203.05186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05186">
<div class="article-summary-box-inner">
<span><p>Recently, one-stage visual grounders attract high attention due to the
comparable accuracy but significantly higher efficiency than two-stage
grounders. However, inter-object relation modeling has not been well studied
for one-stage grounders. Inter-object relationship modeling, though important,
is not necessarily performed among all the objects within the image, as only a
part of them are related to the text query and may confuse the model. We call
these objects "suspected objects". However, exploring relationships among these
suspected objects in the one-stage visual grounding paradigm is non-trivial due
to two core problems: (1) no object proposals are available as the basis on
which to select suspected objects and perform relationship modeling; (2)
compared with those irrelevant to the text query, suspected objects are more
confusing, as they may share similar semantics, be entangled with certain
relationships, etc, and thereby more easily mislead the model's prediction. To
address the above issues, this paper proposes a Suspected Object Graph (SOG)
approach to encourage the correct referred object selection among the suspected
ones in the one-stage visual grounding. Suspected objects are dynamically
selected from a learned activation map as nodes to adapt to the current
discrimination ability of the model during training. Afterward, on top of the
suspected objects, a Keyword-aware Node Representation module (KNR) and an
Exploration by Random Connection strategy (ERC) are concurrently proposed
within the SOG to help the model rethink its initial prediction. Extensive
ablation studies and comparison with state-of-the-art approaches on prevalent
visual grounding benchmarks demonstrate the effectiveness of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection. (arXiv:2203.05187v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05187">
<div class="article-summary-box-inner">
<span><p>The food packaging industry handles an immense variety of food products with
wide-ranging shapes and sizes, even within one kind of food. Menus are also
diverse and change frequently, making automation of pick-and-place difficult. A
popular approach to bin-picking is to first identify each piece of food in the
tray by using an instance segmentation method. However, human annotations to
train these methods are unreliable and error-prone since foods are packed close
together with unclear boundaries and visual similarity making separation of
pieces difficult. To address this problem, we propose a method that trains
purely on synthetic data and successfully transfers to the real world using
sim2real methods by creating datasets of filled food trays using high-quality
3d models of real pieces of food for the training instance segmentation models.
Another concern is that foods are easily damaged during grasping. We address
this by introducing two additional methods -- a novel adaptive finger mechanism
to passively retract when a collision occurs, and a method to filter grasps
that are likely to cause damage to neighbouring pieces of food during a grasp.
We demonstrate the effectiveness of the proposed method on several kinds of
real foods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRFocus: Neural Radiance Field for 3D Synthetic Defocus. (arXiv:2203.05189v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05189">
<div class="article-summary-box-inner">
<span><p>Neural radiance fields (NeRF) bring a new wave for 3D interactive
experiences. However, as an important part of the immersive experiences, the
defocus effects have not been fully explored within NeRF. Some recent
NeRF-based methods generate 3D defocus effects in a post-process fashion by
utilizing multiplane technology. Still, they are either time-consuming or
memory-consuming. This paper proposes a novel thin-lens-imaging-based NeRF
framework that can directly render various 3D defocus effects, dubbed NeRFocus.
Unlike the pinhole, the thin lens refracts rays of a scene point, so its
imaging on the sensor plane is scattered as a circle of confusion (CoC). A
direct solution sampling enough rays to approximate this process is
computationally expensive. Instead, we propose to inverse the thin lens imaging
to explicitly model the beam path for each point on the sensor plane and
generalize this paradigm to the beam path of each pixel, then use the
frustum-based volume rendering to render each pixel's beam path. We further
design an efficient probabilistic training (p-training) strategy to simplify
the training process vastly. Extensive experiments demonstrate that our
NeRFocus can achieve various 3D defocus effects with adjustable camera pose,
focus distance, and aperture size. Existing NeRF can be regarded as our special
case by setting aperture size as zero to render large depth-of-field images.
Despite such merits, NeRFocus does not sacrifice NeRF's original performance
(e.g., training and inference time, parameter consumption, rendering quality),
which implies its great potential for broader application and further
improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Background Matting Using Background Matching. (arXiv:2203.05193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05193">
<div class="article-summary-box-inner">
<span><p>Due to the difficulty of solving the matting problem, lots of methods use
some kinds of assistance to acquire high quality alpha matte. Green screen
matting methods rely on physical equipment. Trimap-based methods take manual
interactions as external input. Background-based methods require a
pre-captured, static background. The methods are not flexible and convenient
enough to use widely. Trimap-free methods are flexible but not stable in
complicated video applications. To be stable and flexible in real applications,
we propose an adaptive background matting method. The user first captures their
videos freely, moving the cameras. Then the user captures the background video
afterwards, roughly covering the previous captured regions. We use dynamic
background video instead of static background for accurate matting. The
proposed method is convenient to use in any scenes as the static camera and
background is no more the limitation. To achieve this goal, we use background
matching network to find the best-matched background frame by frame from
dynamic backgrounds. Then, robust semantic estimation network is used to
estimate the coarse alpha matte. Finally, we crop and zoom the target region
according to the coarse alpha matte, and estimate the final accurate alpha
matte. In experiments, the proposed method is able to perform comparably
against the state-of-the-art matting methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network. (arXiv:2203.05198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05198">
<div class="article-summary-box-inner">
<span><p>With the advent of the screen-reading era, the confidential documents
displayed on the screen can be easily captured by a camera without leaving any
traces. Thus, this paper proposes a novel screen-shooting resilient
watermarking scheme for document image using deep neural network. By applying
this scheme, when the watermarked image is displayed on the screen and captured
by a camera, the watermark can be still extracted from the captured
photographs. Specifically, our scheme is an end-to-end neural network with an
encoder to embed watermark and a decoder to extract watermark. During the
training process, a distortion layer between encoder and decoder is added to
simulate the distortions introduced by screen-shooting process in real scenes,
such as camera distortion, shooting distortion, light source distortion.
Besides, an embedding strength adjustment strategy is designed to improve the
visual quality of the watermarked image with little loss of extraction
accuracy. The experimental results show that the scheme has higher robustness
and visual quality than other three recent state-of-the-arts. Specially, even
if the shooting distances and angles are in extreme, our scheme can also obtain
high extraction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperspectral Imaging for cherry tomato. (arXiv:2203.05199v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05199">
<div class="article-summary-box-inner">
<span><p>Cherry tomato (Solanum Lycopersicum) is popular with consumers over the world
due to its special flavor. Soluble solids content (SSC) and firmness are two
key metrics for evaluating the product qualities. In this work, we develop
non-destructive testing techniques for SSC and fruit firmness based on
hyperspectral images and a corresponding deep learning regression model.
Hyperspectral reflectance images of over 200 tomato fruits are derived with
spectrum ranging from 400 to 1000 nm. The acquired hyperspectral images are
corrected and the spectral information is extracted. A novel
one-dimensional(1D) convolutional ResNet (Con1dResNet) based regression model
is prosed and compared with the state of art techniques. Experimental results
show that, with a relatively large number of samples our technique is 26.4\%
better than state of art technique for SSC and 33.7\% for firmness. The results
of this study indicate the application potential of hyperspectral imaging
technique in the SSC and firmness detection, which provides a new option for
non-destructive testing of cherry tomato fruit quality in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Deep Metric Learning via Mutual Distillation. (arXiv:2203.05201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05201">
<div class="article-summary-box-inner">
<span><p>Deep metric learning aims to transform input data into an embedding space,
where similar samples are close while dissimilar samples are far apart from
each other. In practice, samples of new categories arrive incrementally, which
requires the periodical augmentation of the learned model. The fine-tuning on
the new categories usually leads to poor performance on the old, which is known
as "catastrophic forgetting". Existing solutions either retrain the model from
scratch or require the replay of old samples during the training. In this
paper, a complete online deep metric learning framework is proposed based on
mutual distillation for both one-task and multi-task scenarios. Different from
the teacher-student framework, the proposed approach treats the old and new
learning tasks with equal importance. No preference over the old or new
knowledge is caused. In addition, a novel virtual feature estimation approach
is proposed to recover the features assumed to be extracted by the old models.
It allows the distillation between the new and the old models without the
replay of old training samples or the holding of old models during the
training. A comprehensive study shows the superior performance of our approach
with the support of different backbones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05203">
<div class="article-summary-box-inner">
<span><p>3D dense captioning is a recently-proposed novel task, where point clouds
contain more geometric information than the 2D counterpart. However, it is also
more challenging due to the higher complexity and wider variety of inter-object
relations. Existing methods only treat such relations as by-products of object
feature learning in graphs without specifically encoding them, which leads to
sub-optimal results. In this paper, aiming at improving 3D dense captioning via
capturing and utilizing the complex relations in the 3D scene, we propose MORE,
a Multi-Order RElation mining model, to support generating more descriptive and
comprehensive captions. Technically, our MORE encodes object relations in a
progressive manner since complex relations can be deduced from a limited number
of basic ones. We first devise a novel Spatial Layout Graph Convolution (SLGC),
which semantically encodes several first-order relations as edges of a graph
constructed over 3D object proposals. Next, from the resulting graph, we
further extract multiple triplets which encapsulate basic first-order relations
as the basic unit and construct several Object-centric Triplet Attention Graphs
(OTAG) to infer multi-order relations for every target object. The updated node
features from OTAG are aggregated and fed into the caption decoder to provide
abundant relational cues so that captions including diverse relations with
context objects can be generated. Extensive experiments on the Scan2Cap dataset
prove the effectiveness of our proposed MORE and its components, and we also
outperform the current state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crowd Source Scene Change Detection and Local Map Update. (arXiv:2203.05205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05205">
<div class="article-summary-box-inner">
<span><p>As scene changes with time map descriptors become outdated, affecting VPS
localization accuracy. In this work, we propose an approach to detect
structural and texture scene changes to be followed by map update. In our
method - map includes 3D points with descriptors generated either via LiDAR or
SFM. Common approaches suffer from shortcomings: 1) Direct comparison of the
two point-clouds for change detection is slow due to the need to build new
point-cloud every time we want to compare; 2) Image based comparison requires
to keep the map images adding substantial storage overhead. To circumvent this
problems, we propose an approach based on point-clouds descriptors comparison:
1) Based on VPS poses select close query and map images pairs, 2) Registration
of query images to map image descriptors, 3) Use segmentation to filter out
dynamic or short term temporal changes, 4) Compare the descriptors between
corresponding segments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReF -- Rotation Equivariant Features for Local Feature Matching. (arXiv:2203.05206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05206">
<div class="article-summary-box-inner">
<span><p>Sparse local feature matching is pivotal for many computer vision and
robotics tasks. To improve their invariance to challenging appearance
conditions and viewing angles, and hence their usefulness, existing
learning-based methods have primarily focused on data augmentation-based
training. In this work, we propose an alternative, complementary approach that
centers on inducing bias in the model architecture itself to generate
`rotation-specific' features using Steerable E2-CNNs, that are then
group-pooled to achieve rotation-invariant local features. We demonstrate that
this high performance, rotation-specific coverage from the steerable CNNs can
be expanded to all rotation angles by combining it with augmentation-trained
standard CNNs which have broader coverage but are often inaccurate, thus
creating a state-of-the-art rotation-robust local feature matcher. We benchmark
our proposed methods against existing techniques on HPatches and a newly
proposed UrbanScenes3D-Air dataset for visual place recognition. Furthermore,
we present a detailed analysis of the performance effects of ensembling, robust
estimation, network architecture variations, and the use of rotation priors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Dual Stochastic Graph Convolutional Network for Facial Micro-expression Recognition. (arXiv:2203.05208v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05208">
<div class="article-summary-box-inner">
<span><p>Micro-expression recognition has drawn increasing attention due to its wide
application in lie detection, criminal detection and psychological
consultation. To improve the recognition performance of the small
micro-expression data, this paper presents a transferring dual stochastic Graph
Convolutional Network (TDSGCN) model. We propose a stochastic graph
construction method and dual graph convolutional network to extract more
discriminative features from the micro-expression images. We use transfer
learning to pre-train SGCNs from macro expression data. Optical flow algorithm
is also integrated to extract their temporal features. We fuse both spatial and
temporal features to improve the recognition performance. To the best of our
knowledge, this is the first attempt to utilize the transferring learning and
graph convolutional network in micro-expression recognition task. In addition,
to handle the class imbalance problem of dataset, we focus on the design of
focal loss function. Through extensive evaluation, our proposed method achieves
state-of-the-art performance on SAMM and recently released MMEW benchmarks. Our
code will be publicly available accompanying this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation. (arXiv:2203.05212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05212">
<div class="article-summary-box-inner">
<span><p>Image-to-image translation models are shown to be vulnerable to the
Membership Inference Attack (MIA), in which the adversary's goal is to identify
whether a sample is used to train the model or not. With daily increasing
applications based on image-to-image translation models, it is crucial to
protect the privacy of these models against MIAs.
</p>
<p>We propose adversarial knowledge distillation (AKD) as a defense method
against MIAs for image-to-image translation models. The proposed method
protects the privacy of the training samples by improving the generalizability
of the model. We conduct experiments on the image-to-image translation models
and show that AKD achieves the state-of-the-art utility-privacy tradeoff by
reducing the attack performance up to 38.9% compared with the regular training
model at the cost of a slight drop in the quality of the generated output
images. The experimental results also indicate that the models trained by AKD
generalize better than the regular training models. Furthermore, compared with
existing defense methods, the results show that at the same privacy protection
level, image translation models trained by AKD generate outputs with higher
quality; while at the same quality of outputs, AKD enhances the privacy
protection over 30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement. (arXiv:2203.05238v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05238">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a weakly-supervised approach for 3D object
detection, which makes it possible to train strong 3D detector with
position-level annotations (i.e. annotations of object centers). In order to
remedy the information loss from box annotations to centers, our method, namely
Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak
labels into fully-annotated virtual scenes as stronger supervision, and in turn
utilizes the perfect virtual labels to complement and refine the real labels.
Specifically, we first assemble 3D shapes into physically reasonable virtual
scenes according to the coarse scene layout extracted from position-level
annotations. Then we go back to reality by applying a virtual-to-real domain
adaptation method, which refine the weak labels and additionally supervise the
training of detector with the virtual scenes. Furthermore, we propose a more
challenging benckmark for indoor 3D object detection with more diversity in
object sizes to better show the potential of BR. With less than 5% of the
labeling labor, we achieve comparable detection performance with some popular
fully-supervised approaches on the widely used ScanNet dataset. Code is
available at: https://github.com/xuxw98/BackToReality
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach. (arXiv:2203.05243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05243">
<div class="article-summary-box-inner">
<span><p>Temporal Sentence Grounding in Videos (TSGV), which aims to ground a natural
language sentence in an untrimmed video, has drawn widespread attention over
the past few years. However, recent studies have found that current benchmark
datasets may have obvious moment annotation biases, enabling several simple
baselines even without training to achieve SOTA performance. In this paper, we
take a closer look at existing evaluation protocols, and find both the
prevailing dataset and evaluation metrics are the devils that lead to
untrustworthy benchmarking. Therefore, we propose to re-organize the two
widely-used datasets, making the ground-truth moment distributions different in
the training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile,
we introduce a new evaluation metric "dR@n,IoU@m" that discounts the basic
recall scores to alleviate the inflating evaluation caused by biased datasets.
New benchmarking results indicate that our proposed evaluation protocols can
better monitor the research progress. Furthermore, we propose a novel
causality-based Multi-branch Deconfounding Debiasing (MDD) framework for
unbiased moment prediction. Specifically, we design a multi-branch deconfounder
to eliminate the effects caused by multiple confounders with causal
intervention. In order to help the model better align the semantics between
sentence queries and video moments, we enhance the representations during
feature encoding. Specifically, for textual information, the query is parsed
into several verb-centered phrases to obtain a more fine-grained textual
feature. For visual information, the positional information has been decomposed
from moment features to enhance representations of moments with diverse
locations. Extensive experiments demonstrate that our proposed approach can
achieve competitive results among existing SOTA approaches and outperform the
base model with great gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Scene Text Detection Based on Global Level and Word Level Features. (arXiv:2203.05251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05251">
<div class="article-summary-box-inner">
<span><p>It is an extremely challenging task to detect arbitrary shape text in natural
scenes on high accuracy and efficiency. In this paper, we propose a scene text
detection framework, namely GWNet, which mainly includes two modules: Global
module and RCNN module. Specifically, Global module improves the adaptive
performance of the DB (Differentiable Binarization) module by adding k
submodule and shift submodule. Two submodules enhance the adaptability of
amplifying factor k, accelerate the convergence of models and help to produce
more accurate detection results. RCNN module fuses global-level and word-level
features. The word-level label is generated by obtaining the minimum
axis-aligned rectangle boxes of the shrunk polygon. In the inference period,
GWNet only uses global-level features to output simple polygon detections.
Experiments on four benchmark datasets, including the MSRA-TD500, Total-Text,
ICDAR2015 and CTW-1500, demonstrate that our GWNet outperforms the
state-of-the-art detectors. Specifically, with a backbone of ResNet-50, we
achieve an F-measure of 88.6% on MSRA- TD500, 87.9% on Total-Text, 89.2% on
ICDAR2015 and 87.5% on CTW-1500.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Boundary Learning for Point Cloud Segmentation. (arXiv:2203.05272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05272">
<div class="article-summary-box-inner">
<span><p>Point cloud segmentation is fundamental in understanding 3D environments.
However, current 3D point cloud segmentation methods usually perform poorly on
scene boundaries, which degenerates the overall segmentation performance. In
this paper, we focus on the segmentation of scene boundaries. Accordingly, we
first explore metrics to evaluate the segmentation performance on scene
boundaries. To address the unsatisfactory performance on boundaries, we then
propose a novel contrastive boundary learning (CBL) framework for point cloud
segmentation. Specifically, the proposed CBL enhances feature discrimination
between points across boundaries by contrasting their representations with the
assistance of scene contexts at multiple scales. By applying CBL on three
different baseline methods, we experimentally show that CBL consistently
improves different baselines and assists them to achieve compelling performance
on boundaries, as well as the overall performance, eg in mIoU. The experimental
results demonstrate the effectiveness of our method and the importance of
boundaries for 3D point cloud segmentation. Code and model will be made
publicly available at https://github.com/LiyaoTang/contrastBoundary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalisation for Object Detection. (arXiv:2203.05294v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05294">
<div class="article-summary-box-inner">
<span><p>Domain generalisation aims to promote the learning of domain-invariant
features while suppressing domain specific features, so that a model can
generalise well on previously unseen target domains. This paper studies domain
generalisation in the object detection setting. We propose new terms for
handling both the bounding box detector and domain belonging, and incorporate
them with consistency regularisation. This allows us to learn a domain agnostic
feature representation for object detection, applicable to the problem of
domain generalisation. The proposed approach is evaluated using four standard
object detection datasets with available domain metadata, namely GWHD,
Cityscapes, BDD100K, Sim10K and exhibits consistently superior generalisation
performance over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GrainSpace: A Large-scale Dataset for Fine-grained and Domain-adaptive Recognition of Cereal Grains. (arXiv:2203.05306v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05306">
<div class="article-summary-box-inner">
<span><p>Cereal grains are a vital part of human diets and are important commodities
for people's livelihood and international trade. Grain Appearance Inspection
(GAI) serves as one of the crucial steps for the determination of grain quality
and grain stratification for proper circulation, storage and food processing,
etc. GAI is routinely performed manually by qualified inspectors with the aid
of some hand tools. Automated GAI has the benefit of greatly assisting
inspectors with their jobs but has been limited due to the lack of datasets and
clear definitions of the tasks.
</p>
<p>In this paper we formulate GAI as three ubiquitous computer vision tasks:
fine-grained recognition, domain adaptation and out-of-distribution
recognition. We present a large-scale and publicly available cereal grains
dataset called GrainSpace. Specifically, we construct three types of device
prototypes for data acquisition, and a total of 5.25 million images determined
by professional inspectors. The grain samples including wheat, maize and rice
are collected from five countries and more than 30 regions. We also develop a
comprehensive benchmark based on semi-supervised learning and self-supervised
learning techniques. To the best of our knowledge, GrainSpace is the first
publicly released dataset for cereal grain inspection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05321">
<div class="article-summary-box-inner">
<span><p>We present StyleBabel, a unique open access dataset of natural language
captions and free-form tags describing the artistic style of over 135K digital
artworks, collected via a novel participatory method from experts studying at
specialist art and design schools. StyleBabel was collected via an iterative
method, inspired by `Grounded Theory': a qualitative approach that enables
annotation while co-evolving a shared language for fine-grained artistic style
attribute description. We demonstrate several downstream tasks for StyleBabel,
adapting the recent ALADIN architecture for fine-grained style similarity, to
train cross-modal embeddings for: 1) free-form tag generation; 2) natural
language description of artistic style; 3) fine-grained text search of style.
To do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and
cross-modal representation learning, achieving a state of the art accuracy in
fine-grained style retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking. (arXiv:2203.05328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05328">
<div class="article-summary-box-inner">
<span><p>Exploiting a general-purpose neural architecture to replace hand-wired
designs or inductive biases has recently drawn extensive interest. However,
existing tracking approaches rely on customized sub-modules and need prior
knowledge for architecture selection, hindering the tracking development in a
more general system. This paper presents a Simplified Tracking architecture
(SimTrack) by leveraging a transformer backbone for joint feature extraction
and interaction. Unlike existing Siamese trackers, we serialize the input
images and concatenate them directly before the one-branch backbone. Feature
interaction in the backbone helps to remove well-designed interaction modules
and produce a more efficient and effective framework. To reduce the information
loss from down-sampling in vision transformers, we further propose a foveal
window strategy, providing more diverse input patches with acceptable
computational costs. Our SimTrack improves the baseline with 2.5%/2.6% AUC
gains on LaSOT/TNL2K and gets results competitive with other specialized
tracking algorithms without bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfTune: Metrically Scaled Monocular Depth Estimation through Self-Supervised Learning. (arXiv:2203.05332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05332">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation in the wild inherently predicts depth up to an
unknown scale. To resolve scale ambiguity issue, we present a learning
algorithm that leverages monocular simultaneous localization and mapping (SLAM)
with proprioceptive sensors. Such monocular SLAM systems can provide metrically
scaled camera poses. Given these metric poses and monocular sequences, we
propose a self-supervised learning method for the pre-trained supervised
monocular depth networks to enable metrically scaled depth estimation. Our
approach is based on a teacher-student formulation which guides our network to
predict high-quality depths. We demonstrate that our approach is useful for
various applications such as mobile robot navigation and is applicable to
diverse environments. Our full system shows improvements over recent
self-supervised depth estimation and completion methods on EuRoC, OpenLORIS,
and ScanNet datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects. (arXiv:2203.05334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05334">
<div class="article-summary-box-inner">
<span><p>Tracking objects in 3D space and predicting their 6DoF pose is an essential
task in computer vision. State-of-the-art approaches often rely on object
texture to tackle this problem. However, while they achieve impressive results,
many objects do not contain sufficient texture, violating the main underlying
assumption. In the following, we thus propose ICG, a novel probabilistic
tracker that fuses region and depth information and only requires the object
geometry. Our method deploys correspondence lines and points to iteratively
refine the pose. We also implement robust occlusion handling to improve
performance in real-world settings. Experiments on the YCB-Video, OPT, and Choi
datasets demonstrate that, even for textured objects, our approach outperforms
the current state of the art with respect to accuracy and robustness. At the
same time, ICG shows fast convergence and outstanding efficiency, requiring
only 1.3 ms per frame on a single CPU core. Finally, we analyze the influence
of individual components and discuss our performance compared to deep
learning-based methods. The source code of our tracker is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis. (arXiv:2203.05335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05335">
<div class="article-summary-box-inner">
<span><p>Synthesizing pseudo samples is currently the most effective way to solve the
Generalized Zero Shot Learning (GZSL) problem. Most models achieve competitive
performance but still suffer from two problems: (1) feature confounding, that
task-correlated and task-independent features are confounded in overall
representations, which is unreasonable to synthesize reliable pseudo samples;
and (2) distribution uncertainty, that massive data is needed when existing
models synthesize samples from the uncertain distribution, which causes poor
performance in limited samples of seen classes. In this paper, we propose a
non-generative model to address these problems correspondingly in two modules:
(1) Task-correlated feature disentanglement, to exclude the task-correlated
features from task-independent ones by adversarial learning of domain adaption
towards reasonable synthesis; and (2) Controllable pseudo sample synthesis, to
synthesize edge-pseudo and center-pseudo samples with certain characteristics
towards more diversity generated and intuitive transfer. To describe the new
scene that is the limit seen class samples in the training process, we further
formulate a new ZSL task named the 'Few-shot Seen class and Zero-shot Unseen
class learning' (FSZU). Extensive experiments on four benchmarks verify that
the proposed method is competitive in the GZSL and the FSZU tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrueType Transformer: Character and Font Style Recognition in Outline Format. (arXiv:2203.05338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05338">
<div class="article-summary-box-inner">
<span><p>We propose TrueType Transformer (T3), which can perform character and font
style recognition in an outline format. The outline format, such as TrueType,
represents each character as a sequence of control points of stroke contours
and is frequently used in born-digital documents. T3 is organized by a deep
neural network, so-called Transformer. Transformer is originally proposed for
sequential data, such as text, and therefore appropriate for handling the
outline data. In other words, T3 directly accepts the outline data without
converting it into a bitmap image. Consequently, T3 realizes a
resolution-independent classification. Moreover, since the locations of the
control points represent the fine and local structures of the font style, T3 is
suitable for font style classification, where such structures are very
important. In this paper, we experimentally show the applicability of T3 in
character and font style recognition tasks, while observing how the individual
control points contribute to classification results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05340">
<div class="article-summary-box-inner">
<span><p>With diverse presentation attacks emerging continually, generalizable face
anti-spoofing (FAS) has drawn growing attention. Most existing methods
implement domain generalization (DG) on the complete representations. However,
different image statistics may have unique properties for the FAS tasks. In
this work, we separate the complete representation into content and style ones.
A novel Shuffled Style Assembly Network (SSAN) is proposed to extract and
reassemble different content and style features for a stylized feature space.
Then, to obtain a generalized representation, a contrastive learning strategy
is developed to emphasize liveness-related style information while suppress the
domain-specific one. Finally, the representations of the correct assemblies are
used to distinguish between living and spoofing during the inferring. On the
other hand, despite the decent performance, there still exists a gap between
academia and industry, due to the difference in data quantity and distribution.
Thus, a new large-scale benchmark for FAS is built up to further evaluate the
performance of algorithms in reality. Both qualitative and quantitative results
on existing and proposed benchmarks demonstrate the effectiveness of our
methods. The codes will be available at https://github.com/wangzhuo2019/SSAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EyeLoveGAN: Exploiting domain-shifts to boost network learning with cycleGANs. (arXiv:2203.05344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05344">
<div class="article-summary-box-inner">
<span><p>This paper presents our contribution to the REFUGE challenge 2020. The
challenge consisted of three tasks based on a dataset of retinal images:
Segmentation of optic disc and cup, classification of glaucoma, and
localization of fovea. We propose employing convolutional neural networks for
all three tasks. Segmentation is performed using a U-Net, classification is
performed by a pre-trained InceptionV3 network, and fovea detection is
performed by employing stacked hour-glass for heatmap prediction. The challenge
dataset contains images from three different data sources. To enhance
performance, cycleGANs were utilized to create a domain-shift between the data
sources. These cycleGANs move images across domains, thus creating artificial
images which can be used for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-enriched Attention Network with Group-wise Semantic for Visual Storytelling. (arXiv:2203.05346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05346">
<div class="article-summary-box-inner">
<span><p>As a technically challenging topic, visual storytelling aims at generating an
imaginary and coherent story with narrative multi-sentences from a group of
relevant images. Existing methods often generate direct and rigid descriptions
of apparent image-based contents, because they are not capable of exploring
implicit information beyond images. Hence, these schemes could not capture
consistent dependencies from holistic representation, impairing the generation
of reasonable and fluent story. To address these problems, a novel
knowledge-enriched attention network with group-wise semantic model is
proposed. Three main novel components are designed and supported by substantial
experiments to reveal practical advantages. First, a knowledge-enriched
attention network is designed to extract implicit concepts from external
knowledge system, and these concepts are followed by a cascade cross-modal
attention mechanism to characterize imaginative and concrete representations.
Second, a group-wise semantic module with second-order pooling is developed to
explore the globally consistent guidance. Third, a unified one-stage story
generation model with encoder-decoder structure is proposed to simultaneously
train and infer the knowledge-enriched attention network, group-wise semantic
module and multi-modal story generation decoder in an end-to-end fashion.
Substantial experiments on the popular Visual Storytelling dataset with both
objective and subjective evaluation metrics demonstrate the superior
performance of the proposed scheme as compared with other state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stream Hierarchical Similarity Reasoning for Image-text Matching. (arXiv:2203.05349v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05349">
<div class="article-summary-box-inner">
<span><p>Reasoning-based approaches have demonstrated their powerful ability for the
task of image-text matching. In this work, two issues are addressed for
image-text matching. First, for reasoning processing, conventional approaches
have no ability to find and use multi-level hierarchical similarity
information. To solve this problem, a hierarchical similarity reasoning module
is proposed to automatically extract context information, which is then
co-exploited with local interaction information for efficient reasoning.
Second, previous approaches only consider learning single-stream similarity
alignment (i.e., image-to-text level or text-to-image level), which is
inadequate to fully use similarity information for image-text matching. To
address this issue, a two-stream architecture is developed to decompose
image-text matching into image-to-text level and text-to-image level similarity
computation. These two issues are investigated by a unifying framework that is
trained in an end-to-end manner, namely two-stream hierarchical similarity
reasoning network. The extensive experiments performed on the two benchmark
datasets of MSCOCO and Flickr30K show the superiority of the proposed approach
as compared to existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Context for Robust Maritime Obstacle Detection. (arXiv:2203.05352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05352">
<div class="article-summary-box-inner">
<span><p>Robust maritime obstacle detection is essential for fully autonomous unmanned
surface vehicles (USVs). The currently widely adopted segmentation-based
obstacle detection methods are prone to misclassification of object reflections
and sun glitter as obstacles, producing many false positive detections,
effectively rendering the methods impractical for USV navigation. However,
water-turbulence-induced temporal appearance changes on object reflections are
very distinctive from the appearance dynamics of true objects. We harness this
property to design WaSR-T, a novel maritime obstacle detection network, that
extracts the temporal context from a sequence of recent frames to reduce
ambiguity. By learning the local temporal characteristics of object reflection
on the water surface, WaSR-T substantially improves obstacle detection accuracy
in the presence of reflections and glitter. Compared with existing single-frame
methods, WaSR-T reduces the number of false positive detections by 41% overall
and by over 53% within the danger zone of the boat, while preserving a high
recall, and achieving new state-of-the-art performance on the challenging MODS
maritime obstacle detection benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Commonsense Graph for Object Localisation in Partial Scenes. (arXiv:2203.05380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05380">
<div class="article-summary-box-inner">
<span><p>We solve object localisation in partial scenes, a new problem of estimating
the unknown position of an object (e.g. where is the bag?) given a partial 3D
scan of a scene. The proposed solution is based on a novel scene graph model,
the Spatial Commonsense Graph (SCG), where objects are the nodes and edges
define pairwise distances between them, enriched by concept nodes and
relationships from a commonsense knowledge base. This allows SCG to better
generalise its spatial inference over unknown 3D scenes. The SCG is used to
estimate the unknown position of the target object in two steps: first, we feed
the SCG into a novel Proximity Prediction Network, a graph neural network that
uses attention to perform distance prediction between the node representing the
target object and the nodes representing the observed objects in the SCG;
second, we propose a Localisation Module based on circular intersection to
estimate the object position using all the predicted pairwise distances in
order to be independent of any reference system. We create a new dataset of
partially reconstructed scenes to benchmark our method and baselines for object
localisation in partial scenes, where our proposed method achieves the best
localisation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Efficient Person Re-Identification with Diverse Cluster-Based Pair Selection. (arXiv:2203.05395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05395">
<div class="article-summary-box-inner">
<span><p>Person Re-identification (Re-ID) has attracted great attention due to its
promising real-world applications. However, in practice, it is always costly to
annotate the training data to train a Re-ID model, and it still remains
challenging to reduce the annotation cost while maintaining the performance for
the Re-ID task. To solve this problem, we propose the Annotation Efficient
Person Re-Identification method to select image pairs from an alternative pair
set according to the fallibility and diversity of pairs, and train the Re-ID
model based on the annotation. Specifically, we design an annotation and
training framework to firstly reduce the size of the alternative pair set by
clustering all images considering the locality of features, secondly select
images pairs from intra-/inter-cluster samples for human to annotate, thirdly
re-assign clusters according to the annotation, and finally train the model
with the re-assigned clusters. During the pair selection, we seek for valuable
pairs according to pairs' fallibility and diversity, which includes an
intra-cluster criterion to construct image pairs with the most chaotic samples
and the representative samples within clusters, an inter-cluster criterion to
construct image pairs between clusters based on the second-order Wasserstein
distance, and a diversity criterion for clusterbased pair selection. Combining
all criteria above, a greedy strategy is developed to solve the pair selection
problem. Finally, the above
clustering-selecting-annotating-reassigning-training procedure will be repeated
until the annotation budget is reached. Extensive experiments on three widely
adopted Re-ID datasets show that we can greatly reduce the annotation cost
while achieving better performance compared with state-of-the-art works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Compensation Networks for Continual Semantic Segmentation. (arXiv:2203.05402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05402">
<div class="article-summary-box-inner">
<span><p>In this work, we study the continual semantic segmentation problem, where the
deep neural networks are required to incorporate new classes continually
without catastrophic forgetting. We propose to use a structural
re-parameterization mechanism, named representation compensation (RC) module,
to decouple the representation learning of both old and new knowledge. The RC
module consists of two dynamically evolved branches with one frozen and one
trainable. Besides, we design a pooled cube knowledge distillation strategy on
both spatial and channel dimensions to further enhance the plasticity and
stability of the model. We conduct experiments on two challenging continual
semantic segmentation scenarios, continual class segmentation and continual
domain segmentation. Without any extra computational overhead and parameters
during inference, our method outperforms state-of-the-art performance. The code
is available at \url{https://github.com/zhangchbin/RCIL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval. (arXiv:2203.05465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05465">
<div class="article-summary-box-inner">
<span><p>Dual encoders and cross encoders have been widely used for image-text
retrieval. Between the two, the dual encoder encodes the image and text
independently followed by a dot product, while the cross encoder jointly feeds
image and text as the input and performs dense multi-modal fusion. These two
architectures are typically modeled separately without interaction. In this
work, we propose LoopITR, which combines them in the same network for joint
learning. Specifically, we let the dual encoder provide hard negatives to the
cross encoder, and use the more discriminative cross encoder to distill its
predictions back to the dual encoder. Both steps are efficiently performed
together in the same model. Our work centers on empirical analyses of this
combined architecture, putting the main focus on the design of the distillation
objective. Our experimental results highlight the benefits of training the two
encoders in the same network, and demonstrate that distillation can be quite
effective with just a few hard negative examples. Experiments on two standard
datasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual
encoder performance when compared with approaches using a similar amount of
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction-Guided Distillation for Dense Object Detection. (arXiv:2203.05469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05469">
<div class="article-summary-box-inner">
<span><p>Real-world object detection models should be cheap and accurate. Knowledge
distillation (KD) can boost the accuracy of a small, cheap detection model by
leveraging useful information from a larger teacher model. However, a key
challenge is identifying the most informative features produced by the teacher
for distillation. In this work, we show that only a very small fraction of
features within a ground-truth bounding box are responsible for a teacher's
high detection performance. Based on this, we propose Prediction-Guided
Distillation (PGD), which focuses distillation on these key predictive regions
of the teacher and yields considerable gains in performance over many existing
KD baselines. In addition, we propose an adaptive weighting scheme over the key
regions to smooth out their influence and achieve even better performance. Our
proposed approach outperforms current state-of-the-art KD baselines on a
variety of advanced one-stage detection architectures. Specifically, on the
COCO dataset, our method achieves between +3.1% and +4.6% AP improvement using
ResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On
the CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,
also using these backbones. Our code is available at
https://github.com/ChenhongyiYang/PGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05482">
<div class="article-summary-box-inner">
<span><p>The conventional recipe for maximizing model accuracy is to (1) train
multiple models with various hyperparameters and (2) pick the individual model
which performs best on a held-out validation set, discarding the remainder. In
this paper, we revisit the second step of this procedure in the context of
fine-tuning large pre-trained models, where fine-tuned models often appear to
lie in a single low error basin. We show that averaging the weights of multiple
models fine-tuned with different hyperparameter configurations often improves
accuracy and robustness. Unlike a conventional ensemble, we may average many
models without incurring any additional inference or memory costs -- we call
the results "model soups." When fine-tuning large pre-trained models such as
CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides
significant improvements over the best model in a hyperparameter sweep on
ImageNet. As a highlight, the resulting ViT-G model attains 90.94% top-1
accuracy on ImageNet, a new state of the art. Furthermore, we show that the
model soup approach extends to multiple image classification and natural
language processing tasks, improves out-of-distribution performance, and
improves zero-shot performance on new downstream tasks. Finally, we
analytically relate the performance similarity of weight-averaging and
logit-ensembling to flatness of the loss and confidence of the predictions, and
validate this relation empirically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Less Constrained Macro-Neural Architecture Search. (arXiv:2203.05508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05508">
<div class="article-summary-box-inner">
<span><p>Networks found with Neural Architecture Search (NAS) achieve state-of-the-art
performance in a variety of tasks, out-performing human-designed networks.
However, most NAS methods heavily rely on human-defined assumptions that
constrain the search: architecture's outer-skeletons, number of layers,
parameter heuristics and search spaces. Additionally, common search spaces
consist of repeatable modules (cells) instead of fully exploring the
architecture's search space by designing entire architectures (macro-search).
Imposing such constraints requires deep human expertise and restricts the
search to pre-defined settings. In this paper, we propose LCMNAS, a method that
pushes NAS to less constrained search spaces by performing macro-search without
relying on pre-defined heuristics or bounded search spaces. LCMNAS introduces
three components for the NAS pipeline: i) a method that leverages information
about well-known architectures to autonomously generate complex search spaces
based on Weighted Directed Graphs with hidden properties, ii) a evolutionary
search strategy that generates complete architectures from scratch, and iii) a
mixed-performance estimation approach that combines information about
architectures at initialization stage and lower fidelity estimates to infer
their trainability and capacity to model complex functions. We present
experiments showing that LCMNAS generates state-of-the-art architectures from
scratch with minimal GPU computation. We study the importance of different NAS
components on a macro-search setting. Code for reproducibility is public at
\url{https://github.com/VascoLopes/LCMNAS}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGCN: Augmented Graph Convolutional Network for Lifelong Multi-label Image Recognition. (arXiv:2203.05534v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05534">
<div class="article-summary-box-inner">
<span><p>The Lifelong Multi-Label (LML) image recognition builds an online
class-incremental classifier in a sequential multi-label image recognition data
stream. The key challenges of LML image recognition are the construction of
label relationships on Partial Labels of training data and the Catastrophic
Forgetting on old classes, resulting in poor generalization. To solve the
problems, the study proposes an Augmented Graph Convolutional Network (AGCN)
model that can construct the label relationships across the sequential
recognition tasks and sustain the catastrophic forgetting. First, we build an
Augmented Correlation Matrix (ACM) across all seen classes, where the
intra-task relationships derive from the hard label statistics while the
inter-task relationships leverage both hard and soft labels from data and a
constructed expert network. Then, based on the ACM, the proposed AGCN captures
label dependencies with dynamic augmented structure and yields effective class
representations. Last, to suppress the forgetting of label dependencies across
old tasks, we propose a relationship-preserving loss as a constraint to the
construction of label relationships. The proposed method is evaluated using two
multi-label image benchmarks and the experimental results show that the
proposed method is effective for LML image recognition and can build convincing
correlation across tasks even if the labels of previous tasks are missing. Our
code is available at https://github.com/Kaile-Du/AGCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Investigation of 3D Anomaly Detection and Segmentation. (arXiv:2203.05550v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05550">
<div class="article-summary-box-inner">
<span><p>Anomaly detection and segmentation in images has made tremendous progress in
recent years while 3D information has often been ignored. The objective of this
paper is to further understand the benefit and role of 3D as opposed to color
in image anomaly detection. Our study begins by presenting a surprising
finding: standard color-only anomaly segmentation methods, when applied to 3D
datasets, significantly outperform all current methods. On the other hand, we
observe that color-only methods are insufficient for images containing
geometric anomalies where shape cannot be unambiguously inferred from 2D. This
suggests that better 3D methods are needed. We investigate different
representations for 3D anomaly detection and discover that handcrafted
orientation-invariant representations are unreasonably effective on this task.
We uncover a simple 3D-only method that outperforms all recent approaches while
not using deep learning, external pretraining datasets, or color information.
As the 3D-only method cannot detect color and texture anomalies, we combine it
with 2D color features, granting us the best current results by a large margin
(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by
discussing future challenges for 3D anomaly detection and segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer of Representations to Video Label Propagation: Implementation Factors Matter. (arXiv:2203.05553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05553">
<div class="article-summary-box-inner">
<span><p>This work studies feature representations for dense label propagation in
video, with a focus on recently proposed methods that learn video
correspondence using self-supervised signals such as colorization or temporal
cycle consistency. In the literature, these methods have been evaluated with an
array of inconsistent settings, making it difficult to discern trends or
compare performance fairly. Starting with a unified formulation of the label
propagation algorithm that encompasses most existing variations, we
systematically study the impact of important implementation factors in feature
extraction and label propagation. Along the way, we report the accuracies of
properly tuned supervised and unsupervised still image baselines, which are
higher than those found in previous works. We also demonstrate that augmenting
video-based correspondence cues with still-image-based ones can further improve
performance. We then attempt a fair comparison of recent video-based methods on
the DAVIS benchmark, showing convergence of best methods to performance levels
near our strong ImageNet baseline, despite the usage of a variety of
specialized video-based losses and training particulars. Additional comparisons
on JHMDB and VIP datasets confirm the similar performance of current methods.
We hope that this study will help to improve evaluation practices and better
inform future research directions in temporal correspondence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Prompt Learning for Vision-Language Models. (arXiv:2203.05557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05557">
<div class="article-summary-box-inner">
<span><p>With the rise of powerful pre-trained vision-language models like CLIP, it
becomes essential to investigate ways to adapt these models to downstream
datasets. A recently proposed method named Context Optimization (CoOp)
introduces the concept of prompt learning -- a recent trend in NLP -- to the
vision domain for adapting pre-trained vision-language models. Specifically,
CoOp turns context words in a prompt into a set of learnable vectors and, with
only a few labeled images for learning, can achieve huge improvements over
intensively-tuned manual prompts. In our study we identify a critical problem
of CoOp: the learned context is not generalizable to wider unseen classes
within the same dataset, suggesting that CoOp overfits base classes observed
during training. To address the problem, we propose Conditional Context
Optimization (CoCoOp), which extends CoOp by further learning a lightweight
neural network to generate for each image an input-conditional token (vector).
Compared to CoOp's static prompts, our dynamic prompts adapt to each instance
and are thus less sensitive to class shift. Extensive experiments show that
CoCoOp generalizes much better than CoOp to unseen classes, even showing
promising transferability beyond a single dataset; and yields stronger domain
generalization performance as well. Code is available at
https://github.com/KaiyangZhou/CoOp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08199">
<div class="article-summary-box-inner">
<span><p>Deep learning has achieved remarkable success in numerous domains with help
from large amounts of big data. However, the quality of data labels is a
concern because of the lack of high-quality labels in many real-world
scenarios. As noisy labels severely degrade the generalization performance of
deep neural networks, learning from noisy labels (robust training) is becoming
an important task in modern deep learning applications. In this survey, we
first describe the problem of learning with label noise from a supervised
learning perspective. Next, we provide a comprehensive review of 62
state-of-the-art robust training methods, all of which are categorized into
five groups according to their methodological difference, followed by a
systematic comparison of six properties used to evaluate their superiority.
Subsequently, we perform an in-depth analysis of noise rate estimation and
summarize the typically used evaluation methodology, including public noisy
datasets and evaluation metrics. Finally, we present several promising research
directions that can serve as a guideline for future studies. All the contents
will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conceptual Compression via Deep Structure and Texture Synthesis. (arXiv:2011.04976v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04976">
<div class="article-summary-box-inner">
<span><p>Existing compression methods typically focus on the removal of signal-level
redundancies, while the potential and versatility of decomposing visual data
into compact conceptual components still lack further study. To this end, we
propose a novel conceptual compression framework that encodes visual data into
compact structure and texture representations, then decodes in a deep synthesis
fashion, aiming to achieve better visual reconstruction quality, flexible
content manipulation, and potential support for various vision tasks. In
particular, we propose to compress images by a dual-layered model consisting of
two complementary visual features: 1) structure layer represented by structural
maps and 2) texture layer characterized by low-dimensional deep
representations. At the encoder side, the structural maps and texture
representations are individually extracted and compressed, generating the
compact, interpretable, inter-operable bitstreams. During the decoding stage, a
hierarchical fusion GAN (HF-GAN) is proposed to learn the synthesis paradigm
where the textures are rendered into the decoded structural maps, leading to
high-quality reconstruction with remarkable visual realism. Extensive
experiments on diverse images have demonstrated the superiority of our
framework with lower bitrates, higher reconstruction quality, and increased
versatility towards visual analysis and content manipulation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery. (arXiv:2011.13913v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13913">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) offers the flexibility to image a given
anatomic volume under a multitude of tissue contrasts. Yet, scan time
considerations put stringent limits on the quality and diversity of MRI data.
The gold-standard approach to alleviate this limitation is to recover
high-quality images from data undersampled across various dimensions, most
commonly the Fourier domain or contrast sets. A primary distinction among
recovery methods is whether the anatomy is processed per volume or per
cross-section. Volumetric models offer enhanced capture of global contextual
information, but they can suffer from suboptimal learning due to elevated model
complexity. Cross-sectional models with lower complexity offer improved
learning behavior, yet they ignore contextual information across the
longitudinal dimension of the volume. Here, we introduce a novel progressive
volumetrization strategy for generative models (ProvoGAN) that serially
decomposes complex volumetric image recovery tasks into successive
cross-sectional mappings task-optimally ordered across individual rectilinear
dimensions. ProvoGAN effectively captures global context and recovers
fine-structural details across all dimensions, while maintaining low model
complexity and improved learning behaviour. Comprehensive demonstrations on
mainstream MRI reconstruction and synthesis tasks show that ProvoGAN yields
superior performance to state-of-the-art volumetric and cross-sectional models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HANA: A HAndwritten NAme Database for Offline Handwritten Text Recognition. (arXiv:2101.10862v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10862">
<div class="article-summary-box-inner">
<span><p>Methods for linking individuals across historical data sets, typically in
combination with AI based transcription models, are developing rapidly.
Probably the single most important identifier for linking is personal names.
However, personal names are prone to enumeration and transcription errors and
although modern linking methods are designed to handle such challenges, these
sources of errors are critical and should be minimized. For this purpose,
improved transcription methods and large-scale databases are crucial
components. This paper describes and provides documentation for HANA, a newly
constructed large-scale database which consists of more than 3.3 million names.
The database contain more than 105 thousand unique names with a total of more
than 1.1 million images of personal names, which proves useful for transfer
learning to other settings. We provide three examples hereof, obtaining
significantly improved transcription accuracy on both Danish and US census
data. In addition, we present benchmark results for deep learning models
automatically transcribing the personal names from the scanned documents.
Through making more challenging large-scale databases publicly available we
hope to foster more sophisticated, accurate, and robust models for handwritten
text recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Compact Deep Neural Networks via Energy-Aware Pruning. (arXiv:2103.10858v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10858">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable performance, modern deep neural networks are
inevitably accompanied by a significant amount of computational cost for
learning and deployment, which may be incompatible with their usage on edge
devices. Recent efforts to reduce these overheads involve pruning and
decomposing the parameters of various layers without performance deterioration.
Inspired by several decomposition studies, in this paper, we propose a novel
energy-aware pruning method that quantifies the importance of each filter in
the network using nuclear-norm (NN). Proposed energy-aware pruning leads to
state-of-the-art performance for Top-1 accuracy, FLOPs, and parameter reduction
across a wide range of scenarios with multiple network architectures on
CIFAR-10 and ImageNet after fine-grained classification tasks. On toy
experiment, without fine-tuning, we can visually observe that NN has a minute
change in decision boundaries across classes and outperforms the previous
popular criteria. We achieve competitive results with 40.4/49.8% of FLOPs and
45.9/52.9% of parameter reduction with 94.13/94.61% in the Top-1 accuracy with
ResNet-56/110 on CIFAR-10, respectively. In addition, our observations are
consistent for a variety of different pruning setting in terms of data size as
well as data quality which can be emphasized in the stability of the
acceleration and compression with negligible accuracy loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11022">
<div class="article-summary-box-inner">
<span><p>We consider unsupervised cell nuclei segmentation in this paper. Exploiting
the recently-proposed unpaired image-to-image translation between cell nuclei
images and randomly synthetic masks, existing approaches, e.g., CycleGAN, have
achieved encouraging results. However, these methods usually take a two-stage
pipeline and fail to learn end-to-end in cell nuclei images. More seriously,
they could lead to the lossy transformation problem, i.e., the content
inconsistency between the original images and the corresponding segmentation
output. To address these limitations, we propose a novel end-to-end
unsupervised framework called Aligned Disentangling Generative Adversarial
Network (AD-GAN). Distinctively, AD-GAN introduces representation
disentanglement to separate content representation (the underling spatial
structure) from style representation (the rendering of the structure). With
this framework, spatial structure can be preserved explicitly, enabling a
significant reduction of macro-level lossy transformation. We also propose a
novel training algorithm able to align the disentangled content in the latent
space to reduce micro-level lossy transformation. Evaluations on real-world 2D
and 3D datasets show that AD-GAN substantially outperforms the other comparison
methods and the professional software both quantitatively and qualitatively.
Specifically, the proposed AD-GAN leads to significant improvement over the
current best unsupervised methods by an average 17.8% relatively (w.r.t. the
metric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN
even performs competitive with the best supervised models, taking a further
leap towards end-to-end unsupervised nuclei segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patchwork: Concentric Zone-based Region-wise Ground Segmentation with Ground Likelihood Estimation Using a 3D LiDAR Sensor. (arXiv:2108.05560v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05560">
<div class="article-summary-box-inner">
<span><p>Ground segmentation is crucial for terrestrial mobile platforms to perform
navigation or neighboring object recognition. Unfortunately, the ground is not
flat, as it features steep slopes; bumpy roads; or objects, such as curbs,
flower beds, and so forth. To tackle the problem, this paper presents a novel
ground segmentation method called \textit{Patchwork}, which is robust for
addressing the under-segmentation problem and operates at more than 40 Hz. In
this paper, a point cloud is encoded into a Concentric Zone Model-based
representation to assign an appropriate density of cloud points among bins in a
way that is not computationally complex. This is followed by Region-wise Ground
Plane Fitting, which is performed to estimate the partial ground for each bin.
Finally, Ground Likelihood Estimation is introduced to dramatically reduce
false positives. As experimentally verified on SemanticKITTI and rough terrain
datasets, our proposed method yields promising performance compared with the
state-of-the-art methods, showing faster speed compared with existing plane
fitting--based methods. Code is available:
https://github.com/LimHyungTae/patchwork
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review and experimental evaluation of deep learning methods for MRI reconstruction. (arXiv:2109.08618v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08618">
<div class="article-summary-box-inner">
<span><p>Following the success of deep learning in a wide range of applications,
neural network-based machine-learning techniques have received significant
interest for accelerating magnetic resonance imaging (MRI) acquisition and
reconstruction strategies. A number of ideas inspired by deep learning
techniques for computer vision and image processing have been successfully
applied to nonlinear image reconstruction in the spirit of compressed sensing
for accelerated MRI. Given the rapidly growing nature of the field, it is
imperative to consolidate and summarize the large number of deep learning
methods that have been reported in the literature, to obtain a better
understanding of the field in general. This article provides an overview of the
recent developments in neural-network based approaches that have been proposed
specifically for improving parallel imaging. A general background and
introduction to parallel MRI is also given from a classical view of k-space
based reconstruction methods. Image domain based techniques that introduce
improved regularizers are covered along with k-space based methods which focus
on better interpolation strategies using neural networks. While the field is
rapidly evolving with plenty of papers published each year, in this review, we
attempt to cover broad categories of methods that have shown good performance
on publicly available data sets. Limitations and open problems are also
discussed and recent efforts for producing open data sets and benchmarks for
the community are examined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Contrastive Representation for Semantic Correspondence. (arXiv:2109.10967v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10967">
<div class="article-summary-box-inner">
<span><p>Dense correspondence across semantically related images has been extensively
studied, but still faces two challenges: 1) large variations in appearance,
scale and pose exist even for objects from the same category, and 2) labeling
pixel-level dense correspondences is labor intensive and infeasible to scale.
Most existing approaches focus on designing various matching approaches with
fully-supervised ImageNet pretrained networks. On the other hand, while a
variety of self-supervised approaches are proposed to explicitly measure
image-level similarities, correspondence matching the pixel level remains
under-explored. In this work, we propose a multi-level contrastive learning
approach for semantic matching, which does not rely on any ImageNet pretrained
model. We show that image-level contrastive learning is a key component to
encourage the convolutional features to find correspondence between similar
objects, while the performance can be further enhanced by regularizing
cross-instance cycle-consistency at intermediate feature levels. Experimental
results on the PF-PASCAL, PF-WILLOW, and SPair-71k benchmark datasets
demonstrate that our method performs favorably against the state-of-the-art
approaches. The source code and trained models will be made available to the
public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cartoon Explanations of Image Classifiers. (arXiv:2110.03485v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03485">
<div class="article-summary-box-inner">
<span><p>We present {CartoonX} (Cartoon Explanation), a novel model-agnostic
explanation method tailored towards image classifiers and based on the
rate-distortion explanation (RDE) framework. Natural images are roughly
piece-wise smooth signals -- also called cartoon-like images -- and tend to be
sparse in the wavelet domain. CartoonX is the first explanation method to
exploit this by requiring its explanations to be sparse in the wavelet domain,
thus extracting the {relevant piece-wise smooth} part of an image instead of
relevant pixel-sparse regions. We demonstrate that CartoonX can reveal novel
valuable explanatory information, particularly for misclassifications.
Moreover, we show that CartoonX achieves a lower distortion with fewer
coefficients than other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals Measurement. (arXiv:2110.04447v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04447">
<div class="article-summary-box-inner">
<span><p>Camera-based physiological measurement is a growing field with neural models
providing state-the-art-performance. Prior research have explored various
"end-to-end" models; however these methods still require several preprocessing
steps. These additional operations are often non-trivial to implement making
replication and deployment difficult and can even have a higher computational
budget than the "core" network itself. In this paper, we propose two novel and
efficient neural models for camera-based physiological measurement called
EfficientPhys that remove the need for face detection, segmentation,
normalization, color space transformation or any other preprocessing steps.
Using an input of raw video frames, our models achieve strong performance on
three public datasets. We show that this is the case whether using a
transformer or convolutional backbone. We further evaluate the latency of the
proposed networks and show that our most light weight network also achieves a
33% improvement in efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisit Dictionary Learning for Video Compressive Sensing under the Plug-and-Play Framework. (arXiv:2110.04966v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04966">
<div class="article-summary-box-inner">
<span><p>Aiming at high-dimensional (HD) data acquisition and analysis, snapshot
compressive imaging (SCI) obtains the 2D compressed measurement of HD data with
optical imaging systems and reconstructs HD data using compressive sensing
algorithms. While the Plug-and-Play (PnP) framework offers an emerging solution
to SCI reconstruction, its intrinsic denoising process is still a challenging
problem. Unfortunately, existing denoisers in the PnP framework either suffer
limited performance or require extensive training data. In this paper, we
propose an efficient and effective shallow-learning-based algorithm for video
SCI reconstruction. Revisiting dictionary learning methods, we empower the PnP
framework with a new denoiser, the kernel singular value decomposition (KSVD).
Benefited from the advent of KSVD, our algorithm retains a good trade-off among
quality, speed, and training difficulty. On a variety of datasets, both
quantitative and qualitative evaluations of our simulation results demonstrate
the effectiveness of our proposed method. In comparison to a typical baseline
using total variation, our method achieves around $2$ dB improvement in PSNR
and 0.2 in SSIM. We expect that our proposed PnP-KSVD algorithm can serve as a
new baseline for video SCI reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00232">
<div class="article-summary-box-inner">
<span><p>In visual recognition tasks, few-shot learning requires the ability to learn
object categories with few support examples. Its re-popularity in light of the
deep learning development is mainly in image classification. This work focuses
on few-shot semantic segmentation, which is still a largely unexplored field. A
few recent advances are often restricted to single-class few-shot segmentation.
In this paper, we first present a novel multi-way (class) encoding and decoding
architecture which effectively fuses multi-scale query information and
multi-class support information into one query-support embedding. Multi-class
segmentation is directly decoded upon this embedding. For better feature
fusion, a multi-level attention mechanism is proposed within the architecture,
which includes the attention for support feature modulation and attention for
multi-scale combination. Last, to enhance the embedding space learning, an
additional pixel-wise metric learning module is introduced with triplet loss
formulated on the pixel-level embedding of the input image. Extensive
experiments on standard benchmarks PASCAL-5i and COCO-20i show clear benefits
of our method over the state of the art in few-shot segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAC-ReconNet: A Multiple Acquisition Context based Convolutional Neural Network for MR Image Reconstruction using Dynamic Weight Prediction. (arXiv:2111.05055v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05055">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural network-based MR reconstruction methods have shown to
provide fast and high quality reconstructions. A primary drawback with a
CNN-based model is that it lacks flexibility and can effectively operate only
for a specific acquisition context limiting practical applicability. By
acquisition context, we mean a specific combination of three input settings
considered namely, the anatomy under study, undersampling mask pattern and
acceleration factor for undersampling. The model could be trained jointly on
images combining multiple contexts. However the model does not meet the
performance of context specific models nor extensible to contexts unseen at
train time. This necessitates a modification to the existing architecture in
generating context specific weights so as to incorporate flexibility to
multiple contexts. We propose a multiple acquisition context based network,
called MAC-ReconNet for MRI reconstruction, flexible to multiple acquisition
contexts and generalizable to unseen contexts for applicability in real
scenarios. The proposed network has an MRI reconstruction module and a dynamic
weight prediction (DWP) module. The DWP module takes the corresponding
acquisition context information as input and learns the context-specific
weights of the reconstruction module which changes dynamically with context at
run time. We show that the proposed approach can handle multiple contexts based
on cardiac and brain datasets, Gaussian and Cartesian undersampling patterns
and five acceleration factors. The proposed network outperforms the naive
jointly trained model and gives competitive results with the context-specific
models both quantitatively and qualitatively. We also demonstrate the
generalizability of our model by testing on contexts unseen at train time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SequentialPointNet: A strong frame-level parallel point cloud sequence network for 3D action recognition. (arXiv:2111.08492v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08492">
<div class="article-summary-box-inner">
<span><p>The point cloud sequence of 3D human actions consists of a set of ordered
point cloud frames. Compared to static point clouds, point cloud sequences have
huge data sizes proportional to the time dimension. Therefore, developing an
efficient and lightweight point cloud sequence model is pivotal for 3D action
recognition. In this paper, we propose a strong frame-level parallel point
cloud sequence network referred to as SequentialPointNet for 3D action
recognition. The key to our approach is to divide the main modeling operations
into frame-level units executed in parallel, which greatly improves the
efficiency of modeling point cloud sequences.Moreover, we propose to flatten
the point cloud sequence into a new point data type named hyperpoint sequence
that preserves the complete spatial structure of each frame. Then, a novel
Hyperpoint-Mixer module is introduced to mix intra-frame spatial features and
inter-frame temporal features of the hyperpoint sequence. By doing so,
SequentialPointNet maximizes the appearance encoding ability and extracts
sufficient motion information for effective human action recognition. Extensive
experiments show that SequentialPointNet achieves up to 10X faster than
existing point cloud sequence models. Additionally, our SequentialPointNet
surpasses state-of-the-art approaches for human action recognition on both
large-scale datasets (i.e., NTU RGB+D 60 and NTU RGB+D 120) and small-scale
datasets (i.e., MSR Action3D and UTD-MHAD).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TraSw: Tracklet-Switch Adversarial Attacks against Multi-Object Tracking. (arXiv:2111.08954v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08954">
<div class="article-summary-box-inner">
<span><p>Multi-Object Tracking (MOT) has achieved aggressive progress and derives many
excellent deep learning models. However, the robustness of the trackers is
rarely studied, and it is challenging to attack the MOT system since its mature
association algorithms are designed to be robust against errors during the
tracking. In this work, we analyze the vulnerability of popular pedestrian MOT
trackers and propose a novel adversarial attack method called Tracklet-Switch
(TraSw) against the complete tracking pipeline of MOT. TraSw can fool the
advanced deep trackers (i.e., FairMOT and ByteTrack) to fail to track the
targets in the subsequent frames by attacking very few frames. Experiments on
the MOT-Challenge datasets (i.e., 2DMOT15, MOT17, and MOT20) show that TraSw
can achieve an extraordinarily high success rate of over 95% by attacking only
four frames on average. To our knowledge, this is the first work on the
adversarial attack against pedestrian MOT trackers. The code is available at
https://github.com/DerryHub/FairMOT-attack .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization. (arXiv:2111.09136v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09136">
<div class="article-summary-box-inner">
<span><p>Learning to synthesize data has emerged as a promising direction in zero-shot
quantization (ZSQ), which represents neural networks by low-bit integer without
accessing any of the real data. In this paper, we observe an interesting
phenomenon of intra-class heterogeneity in real data and show that existing
methods fail to retain this property in their synthetic images, which causes a
limited performance increase. To address this issue, we propose a novel
zero-shot quantization method referred to as IntraQ. First, we propose a local
object reinforcement that locates the target objects at different scales and
positions of the synthetic images. Second, we introduce a marginal distance
constraint to form class-related features distributed in a coarse area. Lastly,
we devise a soft inception loss which injects a soft prior label to prevent the
synthetic images from being overfitting to a fixed object. Our IntraQ is
demonstrated to well retain the intra-class heterogeneity in the synthetic
images and also observed to perform state-of-the-art. For example, compared to
the advanced ZSQ, our IntraQ obtains 9.17\% increase of the top-1 accuracy on
ImageNet when all layers of MobileNetV1 are quantized to 4-bit. Code is at
https://github.com/zysxmu/IntraQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11191">
<div class="article-summary-box-inner">
<span><p>The paper presents a Convolutional Neural Networks (CNN) model for image
classification, aiming at increasing predictive performance for COVID-19
diagnosis while avoiding deeper and thus more complex alternatives. The
proposed model includes four similar convolutional layers followed by a
flattening and two dense layers. This work proposes a less complex solution
based on simply classifying 2D CT-Scan slices of images using their pixels via
a 2D CNN model. Despite the simplicity in architecture, the proposed model
showed improved quantitative results exceeding state-of-the-art on the same
dataset of images, in terms of the macro f1 score. In this case study,
extracting features from images, segmenting parts of the images, or other more
complex techniques, ultimately aiming at images classification, do not yield
better results. With that, this paper introduces a simple yet powerful deep
learning based solution for automated COVID-19 classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Modular Network for Video Captioning. (arXiv:2111.12476v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12476">
<div class="article-summary-box-inner">
<span><p>Video captioning aims to generate natural language descriptions according to
the content, where representation learning plays a crucial role. Existing
methods are mainly developed within the supervised learning framework via
word-by-word comparison of the generated caption against the ground-truth text
without fully exploiting linguistic semantics. In this work, we propose a
hierarchical modular network to bridge video representations and linguistic
semantics from three levels before generating captions. In particular, the
hierarchy is composed of: (I) Entity level, which highlights objects that are
most likely to be mentioned in captions. (II) Predicate level, which learns the
actions conditioned on highlighted objects and is supervised by the predicate
in captions. (III) Sentence level, which learns the global semantic
representation and is supervised by the whole caption. Each level is
implemented by one module. Extensive experimental results show that the
proposed method performs favorably against the state-of-the-art models on the
two widely-used benchmarks: MSVD 104.0% and MSR-VTT 51.5% in CIDEr score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Collaborative Graph Machines for Table Structure Recognition. (arXiv:2111.13359v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13359">
<div class="article-summary-box-inner">
<span><p>Recently, table structure recognition has achieved impressive progress with
the help of deep graph models. Most of them exploit single visual cues of
tabular elements or simply combine visual cues with other modalities via early
fusion to reason their graph relationships. However, neither early fusion nor
individually reasoning in terms of multiple modalities can be appropriate for
all varieties of table structures with great diversity. Instead, different
modalities are expected to collaborate with each other in different patterns
for different table cases. In the community, the importance of intra-inter
modality interactions for table structure reasoning is still unexplored. In
this paper, we define it as heterogeneous table structure recognition
(Hetero-TSR) problem. With the aim of filling this gap, we present a novel
Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative
blocks, which alternatively extracts intra-modality context and models
inter-modality interactions in a hierarchical way. It can represent the
intra-inter modality relationships of tabular elements more robustly, which
significantly improves the recognition performance. We also show that the
proposed NCGM can modulate collaborative pattern of different modalities
conditioned on the context of intra-modality cues, which is vital for
diversified table cases. Experimental results on benchmarks demonstrate our
proposed NCGM achieves state-of-the-art performance and beats other
contemporary methods by a large margin especially under challenging scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14820">
<div class="article-summary-box-inner">
<span><p>Learning behavioral patterns from observational data has been a de-facto
approach to motion forecasting. Yet, the current paradigm suffers from two
shortcomings: brittle under distribution shifts and inefficient for knowledge
transfer. In this work, we propose to address these challenges from a causal
representation perspective. We first introduce a causal formalism of motion
forecasting, which casts the problem as a dynamic process with three groups of
latent variables, namely invariant variables, style confounders, and spurious
features. We then introduce a learning framework that treats each group
separately: (i) unlike the common practice mixing datasets collected from
different locations, we exploit their subtle distinctions by means of an
invariance loss encouraging the model to suppress spurious correlations; (ii)
we devise a modular architecture that factorizes the representations of
invariant mechanisms and style confounders to approximate a sparse causal
graph; (iii) we introduce a style contrastive loss that not only enforces the
structure of style representations but also serves as a self-supervisory signal
for test-time refinement on the fly. Experiments on synthetic and real datasets
show that our proposed method improves the robustness and reusability of
learned motion representations, significantly outperforming prior
state-of-the-art motion forecasting models for out-of-distribution
generalization and low-shot transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. (arXiv:2111.15640v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15640">
<div class="article-summary-box-inner">
<span><p>Diffusion probabilistic models (DPMs) have achieved remarkable quality in
image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent
variables that lack semantic meaning and cannot serve as a useful
representation for other tasks. This paper explores the possibility of using
DPMs for representation learning and seeks to extract a meaningful and
decodable representation of an input image via autoencoding. Our key idea is to
use a learnable encoder for discovering the high-level semantics, and a DPM as
the decoder for modeling the remaining stochastic variations. Our method can
encode any image into a two-part latent code, where the first part is
semantically meaningful and linear, and the second part captures stochastic
details, allowing near-exact reconstruction. This capability enables
challenging applications that currently foil GAN-based methods, such as
attribute manipulation on real images. We also show that this two-level
encoding improves denoising efficiency and naturally facilitates various
downstream tasks including few-shot conditional sampling. Please visit our
project page: https://Diff-AE.github.io/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curvature-guided dynamic scale networks for Multi-view Stereo. (arXiv:2112.05999v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05999">
<div class="article-summary-box-inner">
<span><p>Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most
recent studies tried to improve the performance of matching cost volume in MVS
by designing aggregated 3D cost volumes and their regularization. This paper
focuses on learning a robust feature extraction network to enhance the
performance of matching costs without heavy computation in the other steps. In
particular, we present a dynamic scale feature extraction network, namely,
CDSFNet. It is composed of multiple novel convolution layers, each of which can
select a proper patch scale for each pixel guided by the normal curvature of
the image surface. As a result, CDFSNet can estimate the optimal patch scales
to learn discriminative features for accurate matching computation between
reference and source images. By combining the robust extracted features with an
appropriate cost formulation strategy, our resulting MVS architecture can
estimate depth maps more precisely. Extensive experiments showed that the
proposed method outperforms other state-of-the-art methods on complex outdoor
scenes. It significantly improves the completeness of reconstructed models. As
a result, the method can process higher resolution inputs within faster
run-time and lower memory than other MVS methods. Our source code is available
at url{https://github.com/TruongKhang/cds-mvsnet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking. (arXiv:2112.14016v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14016">
<div class="article-summary-box-inner">
<span><p>Tracking visual objects from a single initial exemplar in the testing phase
has been broadly cast as a one-/few-shot problem, i.e., one-shot learning for
initial adaptation and few-shot learning for online adaptation. The recent
few-shot online adaptation methods incorporate the prior knowledge from large
amounts of annotated training data via complex meta-learning optimization in
the offline phase. This helps the online deep trackers to achieve fast
adaptation and reduce overfitting risk in tracking. In this paper, we propose a
simple yet effective recursive least-squares estimator-aided online learning
approach for few-shot online adaptation without requiring offline training. It
allows an in-built memory retention mechanism for the model to remember the
knowledge about the object seen before, and thus the seen data can be safely
removed from training. This also bears certain similarities to the emerging
continual learning field in preventing catastrophic forgetting. This mechanism
enables us to unveil the power of modern online deep trackers without incurring
too much extra computational cost. We evaluate our approach based on two
networks in the online learning families for tracking, i.e., multi-layer
perceptrons in RT-MDNet and convolutional neural networks in DiMP. The
consistent improvements on several challenging tracking benchmarks demonstrate
its effectiveness and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth. (arXiv:2112.14931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14931">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a dense depth estimation pipeline for multiview
360{\deg} images. The proposed pipeline leverages a spherical camera model that
compensates for radial distortion in 360{\deg} images. The key contribution of
this paper is the extension of a spherical camera model to multiview by
introducing a translation scaling scheme. Moreover, we propose an effective
dense depth estimation method by setting virtual depth and minimizing photonic
reprojection error. We validate the performance of the proposed pipeline using
the images of natural scenes as well as the synthesized dataset for quantitive
evaluation. The experimental results verify that the proposed pipeline improves
estimation accuracy compared to the current state-of-art dense depth estimation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Non-Local Contrastive Attention for Image Super-Resolution. (arXiv:2201.03794v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03794">
<div class="article-summary-box-inner">
<span><p>Non-Local Attention (NLA) brings significant improvement for Single Image
Super-Resolution (SISR) by leveraging intrinsic feature correlation in natural
images. However, NLA gives noisy information large weights and consumes
quadratic computation resources with respect to the input size, limiting its
performance and application. In this paper, we propose a novel Efficient
Non-Local Contrastive Attention (ENLCA) to perform long-range visual modeling
and leverage more relevant non-local features. Specifically, ENLCA consists of
two parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA
adopts the kernel method to approximate exponential function and obtains linear
computation complexity. For Sparse Aggregation, we multiply inputs by an
amplification factor to focus on informative features, yet the variance of
approximation increases exponentially. Therefore, contrastive learning is
applied to further separate relevant and irrelevant features. To demonstrate
the effectiveness of ENLCA, we build an architecture called Efficient Non-Local
Contrastive Network (ENLCN) by adding a few of our modules in a simple
backbone. Extensive experimental results show that ENLCN reaches superior
performance over state-of-the-art approaches on both quantitative and
qualitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution. (arXiv:2201.04358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04358">
<div class="article-summary-box-inner">
<span><p>Reference-based super-resolution (RefSR) has made significant progress in
producing realistic textures using an external reference (Ref) image. However,
existing RefSR methods obtain high-quality correspondence matchings consuming
quadratic computation resources with respect to the input size, limiting its
application. Moreover, these approaches usually suffer from scale misalignments
between the low-resolution (LR) image and Ref image. In this paper, we propose
an Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based
Super-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch)
and Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching
efficiency, we design a novel Embedded PatchMacth scheme with random samples
propagation, which involves end-to-end training with asymptotic linear
computational cost to the input size. To further reduce computational cost and
speed up convergence, we apply the coarse-to-fine strategy on Embedded
PatchMacth constituting CFE-PatchMatch. To fully leverage reference information
across multiple scales and enhance robustness to scale misalignment, we develop
the MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation.
The Dynamic Aggregation corrects minor scale misalignment by dynamically
aggregating features, and the Multi-Scale Aggregation brings robustness to
large scale misalignment by fusing multi-scale information. Experimental
results show that the proposed AMSA achieves superior performance over
state-of-the-art approaches on both quantitative and qualitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Data-Driven STAP Radar. (arXiv:2201.10712v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10712">
<div class="article-summary-box-inner">
<span><p>Using an amalgamation of techniques from classical radar, computer vision,
and deep learning, we characterize our ongoing data-driven approach to
space-time adaptive processing (STAP) radar. We generate a rich example dataset
of received radar signals by randomly placing targets of variable strengths in
a predetermined region using RFView, a site-specific radio frequency modeling
and simulation tool developed by ISL Inc. For each data sample within this
region, we generate heatmap tensors in range, azimuth, and elevation of the
output power of a minimum variance distortionless response (MVDR) beamformer,
which can be replaced with a desired test statistic. These heatmap tensors can
be thought of as stacked images, and in an airborne scenario, the moving radar
creates a sequence of these time-indexed image stacks, resembling a video. Our
goal is to use these images and videos to detect targets and estimate their
locations, a procedure reminiscent of computer vision algorithms for object
detection$-$namely, the Faster Region-Based Convolutional Neural Network
(Faster R-CNN). The Faster R-CNN consists of a proposal generating network for
determining regions of interest (ROI), a regression network for positioning
anchor boxes around targets, and an object classification algorithm; it is
developed and optimized for natural images. Our ongoing research will develop
analogous tools for heatmap images of radar data. In this regard, we will
generate a large, representative adaptive radar signal processing database for
training and testing, analogous in spirit to the COCO dataset for natural
images. As a preliminary example, we present a regression network in this paper
for estimating target locations to demonstrate the feasibility of and
significant improvements provided by our data-driven approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Access Control of Object Detection Models Using Encrypted Feature Maps. (arXiv:2202.00265v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00265">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an access control method for object detection
models. The use of encrypted images or encrypted feature maps has been
demonstrated to be effective in access control of models from unauthorized
access. However, the effectiveness of the approach has been confirmed in only
image classification models and semantic segmentation models, but not in object
detection models. In this paper, the use of encrypted feature maps is shown to
be effective in access control of object detection models for the first time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. (arXiv:2202.06767v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06767">
<div class="article-summary-box-inner">
<span><p>Vision-Language Pre-training (VLP) models have shown remarkable performance
on various downstream tasks. Their success heavily relies on the scale of
pre-trained cross-modal datasets. However, the lack of large-scale datasets and
benchmarks in Chinese hinders the development of Chinese VLP models and broader
multilingual applications. In this work, we release a large-scale Chinese
cross-modal dataset named Wukong, containing 100 million Chinese image-text
pairs from the web. Wukong aims to benchmark different multi-modal pre-training
methods to facilitate the VLP research and community development. Furthermore,
we release a group of models pre-trained with various image encoders
(ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP
such as locked-image text tuning, token-wise similarity in contrastive
learning, and reduced-token interaction. Extensive experiments and a deep
benchmarking of different downstream tasks are also provided. Experiments show
that Wukong can serve as a promising Chinese pre-training dataset and benchmark
for different cross-modal learning methods. For the zero-shot image
classification task on 10 datasets, our model achieves an average accuracy of
73.03%. For the image-text retrieval task,our model achieves a mean recall of
71.6% on AIC-ICC which is 12.9% higher than the result of WenLan 2.0. More
information can refer to https://wukong-dataset.github.io/wukong-dataset/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When, Why, and Which Pretrained GANs Are Useful?. (arXiv:2202.08937v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08937">
<div class="article-summary-box-inner">
<span><p>The literature has proposed several methods to finetune pretrained GANs on
new datasets, which typically results in higher performance compared to
training from scratch, especially in the limited-data regime. However, despite
the apparent empirical benefits of GAN pretraining, its inner mechanisms were
not analyzed in-depth, and understanding of its role is not entirely clear.
Moreover, the essential practical details, e.g., selecting a proper pretrained
GAN checkpoint, currently do not have rigorous grounding and are typically
determined by trial and error.
</p>
<p>This work aims to dissect the process of GAN finetuning. First, we show that
initializing the GAN training process by a pretrained checkpoint primarily
affects the model's coverage rather than the fidelity of individual samples.
Second, we explicitly describe how pretrained generators and discriminators
contribute to the finetuning process and explain the previous evidence on the
importance of pretraining both of them. Finally, as an immediate practical
benefit of our analysis, we describe a simple recipe to choose an appropriate
GAN checkpoint that is the most suitable for finetuning to a particular target
task. Importantly, for most of the target tasks, Imagenet-pretrained GAN,
despite having poor visual quality, appears to be an excellent starting point
for finetuning, resembling the typical pretraining scenario of discriminative
computer vision models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outlier-based Autism Detection using Longitudinal Structural MRI. (arXiv:2202.09988v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09988">
<div class="article-summary-box-inner">
<span><p>Diagnosis of Autism Spectrum Disorder (ASD) using clinical evaluation
(cognitive tests) is challenging due to wide variations amongst individuals.
Since no effective treatment exists, prompt and reliable ASD diagnosis can
enable the effective preparation of treatment regimens. This paper proposes
structural Magnetic Resonance Imaging (sMRI)-based ASD diagnosis via an outlier
detection approach. To learn Spatio-temporal patterns in structural brain
connectivity, a Generative Adversarial Network (GAN) is trained exclusively
with sMRI scans of healthy subjects. Given a stack of three adjacent slices as
input, the GAN generator reconstructs the next three adjacent slices; the GAN
discriminator then identifies ASD sMRI scan reconstructions as outliers. This
model is compared against two other baselines -- a simpler UNet and a
sophisticated Self-Attention GAN. Axial, Coronal, and Sagittal sMRI slices from
the multi-site ABIDE II dataset are used for evaluation. Extensive experiments
reveal that our ASD detection framework performs comparably with the
state-of-the-art with far fewer training data. Furthermore, longitudinal data
(two scans per subject over time) achieve 17-28% higher accuracy than
cross-sectional data (one scan per subject). Among other findings, metrics
employed for model training as well as reconstruction loss computation impact
detection performance, and the coronal modality is found to best encode
structural information for ASD detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13541">
<div class="article-summary-box-inner">
<span><p>We propose a deep learning methodology for multivariate regression that is
based on pattern recognition that triggers fast learning over sensor data. We
used a conversion of sensors-to-image which enables us to take advantage of
Computer Vision architectures and training processes. In addition to this data
preparation methodology, we explore the use of state-of-the-art architectures
to generate regression outputs to predict agricultural crop continuous yield
information. Finally, we compare with some of the top models reported in
MLCAS2021. We found that using a straightforward training process, we were able
to accomplish an MAE of 4.394, RMSE of 5.945, and R^2 of 0.861.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00190">
<div class="article-summary-box-inner">
<span><p>Deep learning methodologies have been employed in several different fields,
with an outstanding success in image recognition applications, such as material
quality control, medical imaging, autonomous driving, etc. Deep learning models
rely on the abundance of labelled observations to train a prospective model.
These models are composed of millions of parameters to estimate, increasing the
need of more training observations. Frequently it is expensive to gather
labelled observations of data, making the usage of deep learning models not
ideal, as the model might over-fit data. In a semi-supervised setting,
unlabelled data is used to improve the levels of accuracy and generalization of
a model with small labelled datasets. Nevertheless, in many situations
different unlabelled data sources might be available. This raises the risk of a
significant distribution mismatch between the labelled and unlabelled datasets.
Such phenomena can cause a considerable performance hit to typical
semi-supervised deep learning frameworks, which often assume that both labelled
and unlabelled datasets are drawn from similar distributions. Therefore, in
this paper we study the latest approaches for semi-supervised deep learning for
image recognition. Emphasis is made in semi-supervised deep learning models
designed to deal with a distribution mismatch between the labelled and
unlabelled datasets. We address open challenges with the aim to encourage the
community to tackle them, and overcome the high data demand of traditional deep
learning pipelines under real-world usage settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. (arXiv:2203.01536v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01536">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) are becoming more popular and dominating technique
for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a
demanding technique in computer vision, ViTs have been successfully solved
various vision problems while focusing on long-range relationships. In this
paper, we begin by introducing the fundamental concepts and background of the
self-attention mechanism. Next, we provide a comprehensive overview of recent
top-performing ViT methods describing in terms of strength and weakness,
computational cost as well as training and testing dataset. We thoroughly
compare the performance of various ViT algorithms and most representative CNN
methods on popular benchmark datasets. Finally, we explore some limitations
with insightful observations and provide further research direction. The
project page along with the collections of papers are available at
https://github.com/khawar512/ViT-Survey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02104">
<div class="article-summary-box-inner">
<span><p>Interactive image synthesis from user-guided input is a challenging task when
users wish to control the scene structure of a generated image with
ease.Although remarkable progress has been made on layout-based image synthesis
approaches, in order to get realistic fake image in interactive scene, existing
methods require high-precision inputs, which probably need adjustment several
times and are unfriendly to novice users. When placement of bounding boxes is
subject to perturbation, layout-based models suffer from "missing regions" in
the constructed semantic layouts and hence undesirable artifacts in the
generated images. In this work, we propose Panoptic Layout Generative
Adversarial Networks (PLGAN) to address this challenge. The PLGAN employs
panoptic theory which distinguishes object categories between "stuff" with
amorphous boundaries and "things" with well-defined shapes, such that stuff and
instance layouts are constructed through separate branches and later fused into
panoptic layouts. In particular, the stuff layouts can take amorphous shapes
and fill up the missing regions left out by the instance layouts. We
experimentally compare our PLGAN with state-of-the-art layout-based models on
the COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN
are not only visually demonstrated but quantitatively verified in terms of
inception score, Fr\'echet inception distance, classification accuracy score,
and coverage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02503">
<div class="article-summary-box-inner">
<span><p>Pansharpening aims to fuse a registered high-resolution panchromatic image
(PAN) with a low-resolution hyperspectral image (LR-HSI) to generate an
enhanced HSI with high spectral and spatial resolution. Existing pansharpening
approaches neglect using an attention mechanism to transfer HR texture features
from PAN to LR-HSI features, resulting in spatial and spectral distortions. In
this paper, we present a novel attention mechanism for pansharpening called
HyperTransformer, in which features of LR-HSI and PAN are formulated as queries
and keys in a transformer, respectively. HyperTransformer consists of three
main modules, namely two separate feature extractors for PAN and HSI, a
multi-head feature soft attention module, and a spatial-spectral feature fusion
module. Such a network improves both spatial and spectral quality measures of
the pansharpened HSI by learning cross-feature space dependencies and
long-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be
utilized across multiple spatial scales at the backbone for obtaining improved
performance. Extensive experiments conducted on three widely used datasets
demonstrate that HyperTransformer achieves significant improvement over the
state-of-the-art methods on both spatial and spectral quality measures.
Implementation code and pre-trained weights can be accessed at
https://github.com/wgcban/HyperTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02925">
<div class="article-summary-box-inner">
<span><p>Weakly supervised temporal action localization aims to localize temporal
boundaries of actions and simultaneously identify their categories with only
video-level category labels. Many existing methods seek to generate pseudo
labels for bridging the discrepancy between classification and localization,
but usually only make use of limited contextual information for pseudo label
generation. To alleviate this problem, we propose a representative snippet
summarization and propagation framework. Our method seeks to mine the
representative snippets in each video for propagating information between video
snippets to generate better pseudo labels. For each video, its own
representative snippets and the representative snippets from a memory bank are
propagated to update the input features in an intra- and inter-video manner.
The pseudo labels are generated from the temporal class activation maps of the
updated features to rectify the predictions of the main branch. Our method
obtains superior performance in comparison to the existing methods on two
benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in
terms of average mAP on THUMOS14.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SingleSketch2Mesh : Generating 3D Mesh model from Sketch. (arXiv:2203.03157v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03157">
<div class="article-summary-box-inner">
<span><p>Sketching is an important activity in any design process. Designers and
stakeholders share their ideas through hand-drawn sketches. These sketches are
further used to create 3D models. Current methods to generate 3D models from
sketches are either manual or tightly coupled with 3D modeling platforms.
Therefore, it requires users to have an experience of sketching on such
platform. Moreover, most of the existing approaches are based on geometric
manipulation and thus cannot be generalized. We propose a novel AI based
ensemble approach, SingleSketch2Mesh, for generating 3D models from hand-drawn
sketches. Our approach is based on Generative Networks and Encoder-Decoder
Architecture to generate 3D mesh model from a hand-drawn sketch. We evaluate
our solution with existing solutions. Our approach outperforms existing
approaches on both - quantitative and qualitative evaluation criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03844">
<div class="article-summary-box-inner">
<span><p>Light-weight super-resolution (SR) models have received considerable
attention for their serviceability in mobile devices. Many efforts employ
network quantization to compress SR models. However, these methods suffer from
severe performance degradation when quantizing the SR models to ultra-low
precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In
this paper, we identify that the performance drop comes from the contradiction
between the layer-wise symmetric quantizer and the highly asymmetric activation
distribution in SR models. This discrepancy leads to either a waste on the
quantization levels or detail loss in reconstructed images. Therefore, we
propose a novel activation quantizer, referred to as Dynamic Dual Trainable
Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically,
DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower
bounds to tackle the highly asymmetric activations. 2) A dynamic gate
controller to adaptively adjust the upper and lower bounds at runtime to
overcome the drastically varying activation ranges over different samples.To
reduce the extra overhead, the dynamic gate controller is quantized to 2-bit
and applied to only part of the SR networks according to the introduced dynamic
intensity. Extensive experiments demonstrate that our DDTB exhibits significant
performance improvements in ultra-low precision. For example, our DDTB achieves
a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and
scaling up output images to x4. Code is at
\url{https://github.com/zysxmu/DDTB}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition. (arXiv:2203.04623v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04623">
<div class="article-summary-box-inner">
<span><p>Recent studies have revealed the vulnerability of face recognition models
against physical adversarial patches, which raises security concerns about the
deployed face recognition systems. However, it is still challenging to ensure
the reproducibility for most attack algorithms under complex physical
conditions, which leads to the lack of a systematic evaluation of the existing
methods. It is therefore imperative to develop a framework that can enable a
comprehensive evaluation of the vulnerability of face recognition in the
physical world. To this end, we propose to simulate the complex transformations
of faces in the physical world via 3D-face modeling, which serves as a digital
counterpart of physical faces. The generic framework allows us to control
different face variations and physical conditions to conduct reproducible
evaluations comprehensively. With this digital simulator, we further propose a
Face3DAdv method considering the 3D face transformations and realistic physical
variations. Extensive experiments validate that Face3DAdv can significantly
improve the effectiveness of diverse physically realizable adversarial patches
in both simulated and physical environments, against various white-box and
black-box face recognition models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-DIAE: Degradation Invariant Autoencoders for Text Recognition and Document Enhancement. (arXiv:2203.04814v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04814">
<div class="article-summary-box-inner">
<span><p>In this work, we propose Text-Degradation Invariant Auto Encoder (Text-DIAE)
aimed to solve two tasks, text recognition (handwritten or scene-text) and
document image enhancement. We define three pretext tasks as learning
objectives to be optimized during pre-training without the usage of labelled
data. Each of the pre-text objectives is specifically tailored for the final
downstream tasks. We conduct several ablation experiments that show the
importance of each degradation for a specific domain. Exhaustive
experimentation shows that our method does not have limitations of previous
state-of-the-art based on contrastive losses while at the same time requiring
essentially fewer data samples to converge. Finally, we demonstrate that our
method surpasses the state-of-the-art significantly in existing supervised and
self-supervised settings in handwritten and scene text recognition and document
image enhancement. Our code and trained models will be made publicly available
at~\url{ <a href="http://Upon_Acceptance">this http URL</a>}.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-13 23:08:09.339043646 UTC">2022-03-13 23:08:09 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>