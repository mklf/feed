<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-25T01:30:00Z">01-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">GreaseLM: Graph REASoning Enhanced Language Models for Question Answering. (arXiv:2201.08860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08860">
<div class="article-summary-box-inner">
<span><p>Answering complex questions about textual narratives requires reasoning over
both stated context and the world knowledge that underlies it. However,
pretrained language models (LM), the foundation of most modern QA systems, do
not robustly represent latent relationships between concepts, which is
necessary for reasoning. While knowledge graphs (KG) are often used to augment
LMs with structured representations of world knowledge, it remains an open
question how to effectively fuse and reason over the KG representations and the
language context, which provides situational constraints and nuances. In this
work, we propose GreaseLM, a new model that fuses encoded representations from
pretrained LMs and graph neural networks over multiple layers of modality
interaction operations. Information from both modalities propagates to the
other, allowing language context representations to be grounded by structured
world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in
the context to inform the graph representations of knowledge. Our results on
three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA)
and medical question answering (i.e., MedQA-USMLE) domains demonstrate that
GreaseLM can more reliably answer questions that require reasoning over both
situational constraints and structured knowledge, even outperforming models 8x
larger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Description-Driven Task-Oriented Dialog Modeling. (arXiv:2201.08904v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08904">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue (TOD) systems are required to identify key information
from conversations for the completion of given tasks. Such information is
conventionally specified in terms of intents and slots contained in
task-specific ontology or schemata. Since these schemata are designed by system
developers, the naming convention for slots and intents is not uniform across
tasks, and may not convey their semantics effectively. This can lead to models
memorizing arbitrary patterns in data, resulting in suboptimal performance and
generalization. In this paper, we propose that schemata should be modified by
replacing names or notations entirely with natural language descriptions. We
show that a language description-driven system exhibits better understanding of
task specifications, higher performance on state tracking, improved data
efficiency, and effective zero-shot transfer to unseen tasks. Following this
paradigm, we present a simple yet effective Description-Driven Dialog State
Tracking (D3ST) model, which relies purely on schema descriptions and an
"index-picking" mechanism. We demonstrate the superiority in quality, data
efficiency and robustness of our approach as measured on the MultiWOZ
(Budzianowski et al.,2018), SGD (Rastogi et al., 2020), and the recent SGD-X
(Lee et al., 2021) benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Neural Networks with Mixed Hierarchical Structures and EM Algorithm for Natural Language Processing. (arXiv:2201.08919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08919">
<div class="article-summary-box-inner">
<span><p>How to obtain hierarchical representations with an increasing level of
abstraction becomes one of the key issues of learning with deep neural
networks. A variety of RNN models have recently been proposed to incorporate
both explicit and implicit hierarchical information in modeling languages in
the literature. In this paper, we propose a novel approach called the latent
indicator layer to identify and learn implicit hierarchical information (e.g.,
phrases), and further develop an EM algorithm to handle the latent indicator
layer in training. The latent indicator layer further simplifies a text's
hierarchical structure, which allows us to seamlessly integrate different
levels of attention mechanisms into the structure. We called the resulting
architecture as the EM-HRNN model. Furthermore, we develop two bootstrap
strategies to effectively and efficiently train the EM-HRNN model on long text
documents. Simulation studies and real data applications demonstrate that the
EM-HRNN model with bootstrap training outperforms other RNN-based models in
document classification tasks. The performance of the EM-HRNN model is
comparable to a Transformer-based method called Bert-base, though the former is
much smaller model and does not require pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Word Segmentation with Heterogeneous Graph Neural Network. (arXiv:2201.08975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08975">
<div class="article-summary-box-inner">
<span><p>In recent years, deep learning has achieved significant success in the
Chinese word segmentation (CWS) task. Most of these methods improve the
performance of CWS by leveraging external information, e.g., words, sub-words,
syntax. However, existing approaches fail to effectively integrate the
multi-level linguistic information and also ignore the structural feature of
the external information. Therefore, in this paper, we proposed a framework to
improve CWS, named HGNSeg. It exploits multi-level external information
sufficiently with the pre-trained language model and heterogeneous graph neural
network. The experimental results on six benchmark datasets (e.g., Bakeoff
2005, Bakeoff 2008) validate that our approach can effectively improve the
performance of Chinese word segmentation. Importantly, in cross-domain
scenarios, our method also shows a strong ability to alleviate the
out-of-vocabulary (OOV) problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leaf: Multiple-Choice Question Generation. (arXiv:2201.09012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09012">
<div class="article-summary-box-inner">
<span><p>Testing with quiz questions has proven to be an effective way to assess and
improve the educational process. However, manually creating quizzes is tedious
and time-consuming. To address this challenge, we present Leaf, a system for
generating multiple-choice questions from factual text. In addition to being
very well suited for the classroom, Leaf could also be used in an industrial
setting, e.g., to facilitate onboarding and knowledge sharing, or as a
component of chatbots, question answering systems, or Massive Open Online
Courses (MOOCs). The code and the demo are available on
https://github.com/KristiyanVachev/Leaf-Question-Generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solvability of orbit-finite systems of linear equations. (arXiv:2201.09060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09060">
<div class="article-summary-box-inner">
<span><p>We study orbit-finite systems of linear equations, in the setting of sets
with atoms. Our principal contribution is a decision procedure for solvability
of such systems. The procedure works for every field (and even commutative
ring) under mild effectiveness assumptions, and reduces a given orbit-finite
system to a number of finite ones: exponentially many in general, but
polynomially many when atom dimension of input systems is fixed. Towards
obtaining the procedure we push further the theory of vector spaces generated
by orbit-finite sets, and show that each such vector space admits an
orbit-finite basis. This fundamental property is a key tool in our development,
but should be also of wider interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Information Guided Zero-Shot Paraphrase Generation. (arXiv:2201.09107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09107">
<div class="article-summary-box-inner">
<span><p>Zero-shot paraphrase generation has drawn much attention as the large-scale
high-quality paraphrase corpus is limited. Back-translation, also known as the
pivot-based method, is typical to this end. Several works leverage different
information as "pivot" such as language, semantic representation and so on. In
this paper, we explore using visual information such as image as the "pivot" of
back-translation. Different with the pipeline back-translation method, we
propose visual information guided zero-shot paraphrase generation (ViPG) based
only on paired image-caption data. It jointly trains an image captioning model
and a paraphrasing model and leverage the image captioning model to guide the
training of the paraphrasing model. Both automatic evaluation and human
evaluation show our model can generate paraphrase with good relevancy, fluency
and diversity, and image is a promising kind of pivot for zero-shot paraphrase
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Causal Lens for Controllable Text Generation. (arXiv:2201.09119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09119">
<div class="article-summary-box-inner">
<span><p>Controllable text generation concerns two fundamental tasks of wide
applications, namely generating text of given attributes (i.e.,
attribute-conditional generation), and minimally editing existing text to
possess desired attributes (i.e., text attribute transfer). Extensive prior
work has largely studied the two problems separately, and developed different
conditional models which, however, are prone to producing biased text (e.g.,
various gender stereotypes). This paper proposes to formulate controllable text
generation from a principled causal perspective which models the two tasks with
a unified framework. A direct advantage of the causal formulation is the use of
rich causality tools to mitigate generation biases and improve control. We
treat the two tasks as interventional and counterfactual causal inference based
on a structural causal model, respectively. We then apply the framework to the
challenging practical setting where confounding factors (that induce spurious
correlations) are observable only on a small fraction of data. Experiments show
significant superiority of the causal approach over previous conditional models
for improved control accuracy and reduced bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question rewriting? Assessing its importance for conversational question answering. (arXiv:2201.09146v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09146">
<div class="article-summary-box-inner">
<span><p>In conversational question answering, systems must correctly interpret the
interconnected interactions and generate knowledgeable answers, which may
require the retrieval of relevant information from a background repository.
Recent approaches to this problem leverage neural language models, although
different alternatives can be considered in terms of modules for (a)
representing user questions in context, (b) retrieving the relevant background
information, and (c) generating the answer. This work presents a conversational
question answering system designed specifically for the Search-Oriented
Conversational AI (SCAI) shared task, and reports on a detailed analysis of its
question rewriting module. In particular, we considered different variations of
the question rewriting module to evaluate the influence on the subsequent
components, and performed a careful analysis of the results obtained with the
best system configuration. Our system achieved the best performance in the
shared task and our analysis emphasizes the importance of the conversation
context representation for the overall system performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large and Diverse Arabic Corpus for Language Modeling. (arXiv:2201.09227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09227">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have introduced a major paradigm shift in Natural
Language Processing (NLP) modeling where large pre-trained LMs became integral
to most of the NLP tasks. The LMs are intelligent enough to find useful and
relevant representations of the language without any supervision. Perhaps,
these models are used to fine-tune typical NLP tasks with significantly high
accuracy as compared to the traditional approaches. Conversely, the training of
these models requires a massively large corpus that is a good representation of
the language. English LMs generally perform better than their other language
counterparts, due to the availability of massive English corpora.
</p>
<p>This work elaborates on the design and development of a large Arabic corpus.
It consists of over 500 GB of Arabic cleaned text targeted at improving
cross-domain knowledge and downstream generalization capability of large-scale
language models. Moreover, the corpus is utilized in the training of a large
Arabic LM. In order to evaluate the effectiveness of the LM, a number of
typical NLP tasks are fine-tuned. The tasks demonstrate a significant boost
from 4.5 to 8.5% when compared to tasks fine-tuned on multi-lingual BERT
(mBERT). To the best of my knowledge, this is currently the largest clean and
diverse Arabic corpus ever collected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WIDAR -- Weighted Input Document Augmented ROUGE. (arXiv:2201.09282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09282">
<div class="article-summary-box-inner">
<span><p>The task of automatic text summarization has gained a lot of traction due to
the recent advancements in machine learning techniques. However, evaluating the
quality of a generated summary remains to be an open problem. The literature
has widely adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as
the standard evaluation metric for summarization. However, ROUGE has some
long-established limitations; a major one being its dependence on the
availability of good quality reference summary. In this work, we propose the
metric WIDAR which in addition to utilizing the reference summary uses also the
input document in order to evaluate the quality of the generated summary. The
proposed metric is versatile, since it is designed to adapt the evaluation
score according to the quality of the reference summary. The proposed metric
correlates better than ROUGE by 26%, 76%, 82%, and 15%, respectively, in
coherence, consistency, fluency, and relevance on human judgement scores
provided in the SummEval dataset. The proposed metric is able to obtain
comparable results with other state-of-the-art metrics while requiring a
relatively short computational time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Visual Attention for Simultaneous Multimodal Machine Translation. (arXiv:2201.09324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09324">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a surge in research in multimodal machine
translation (MMT), where additional modalities such as images are used to
improve translation quality of textual systems. A particular use for such
multimodal systems is the task of simultaneous machine translation, where
visual context has been shown to complement the partial information provided by
the source sentence, especially in the early phases of translation (Caglayanet
al., 2020a; Imankulova et al., 2020). In this paper, we propose the first
Transformer-based simultaneous MMT architecture, which has not been previously
explored in the field. Additionally, we extend this model with an auxiliary
supervision signal that guides its visual attention mechanism using labelled
phrase-region alignments. We perform comprehensive experiments on three
language directions and conduct thorough quantitative and qualitative analyses
using both automatic metrics and manual inspection. Our results show that (i)
supervised visual attention consistently improves the translation quality of
the MMT models, and (ii) fine-tuning the MMT with supervision loss enabled
leads to better performance than training the MMT from scratch. Compared to the
state-of-the-art, our proposed model achieves improvements of up to 2.3 BLEU
and 3.5 METEOR points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Application of Pseudo-Log-Likelihoods to Natural Language Scoring. (arXiv:2201.09377v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09377">
<div class="article-summary-box-inner">
<span><p>Language models built using semi-supervised machine learning on large corpora
of natural language have very quickly enveloped the fields of natural language
generation and understanding. In this paper we apply a zero-shot approach
independently developed by a number of researchers now gaining recognition as a
significant alternative to fine-tuning for evaluation on common sense tasks. A
language model with relatively few parameters and training steps compared to a
more recent language model (T5) can outperform it on a recent large data set
(TimeDial), while displaying robustness in its performance across a similar
class of language tasks. Surprisingly, this result is achieved by using a
hyperparameter-free zero-shot method with the smaller model, compared to
fine-tuning to the larger model. We argue that robustness of the smaller model
ought to be understood in terms of compositionality, in a sense that we draw
from recent literature on a class of similar models. We identify a practical
cost for our method and model: high GPU-time for natural language evaluation.
The zero-shot measurement technique that produces remarkable stability, both
for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods
to masked language models for the relative measurement of probability for
substitution alternatives in forced choice language tasks such as the Winograd
Schema Challenge, Winogrande, and others. One contribution of this paper is to
bring together a number of similar, but independent strands of research. We
produce some absolute state-of-the-art results for common sense reasoning in
binary choice tasks, performing better than any published result in the
literature, including fine-tuned efforts. We show a remarkable consistency of
the model's performance under adversarial settings, which we argue is best
explained by the model's compositionality of representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion-based Modeling of Mental Disorders on Social Media. (arXiv:2201.09451v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09451">
<div class="article-summary-box-inner">
<span><p>According to the World Health Organization (WHO), one in four people will be
affected by mental disorders at some point in their lives. However, in many
parts of the world, patients do not actively seek professional diagnosis
because of stigma attached to mental illness, ignorance of mental health and
its associated symptoms. In this paper, we propose a model for passively
detecting mental disorders using conversations on Reddit. Specifically, we
focus on a subset of mental disorders that are characterized by distinct
emotional patterns (henceforth called emotional disorders): major depressive,
anxiety, and bipolar disorders. Through passive (i.e., unprompted) detection,
we can encourage patients to seek diagnosis and treatment for mental disorders.
Our proposed model is different from other work in this area in that our model
is based entirely on the emotional states, and the transition between these
states of users on Reddit, whereas prior work is typically based on
content-based representations (e.g., n-grams, language model embeddings, etc).
We show that content-based representation is affected by domain and topic bias
and thus does not generalize, while our model, on the other hand, suppresses
topic-specific information and thus generalizes well across different topics
and times. We conduct experiments on our model's ability to detect different
emotional disorders and on the generalizability of our model. Our experiments
show that while our model performs comparably to content-based models, such as
BERT, it generalizes much better across time and topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias in Automated Speaker Recognition. (arXiv:2201.09486v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09486">
<div class="article-summary-box-inner">
<span><p>Automated speaker recognition uses data processing to identify speakers by
their voice. Today, automated speaker recognition technologies are deployed on
billions of smart devices and in services such as call centres. Despite their
wide-scale deployment and known sources of bias in face recognition and natural
language processing, bias in automated speaker recognition has not been studied
systematically. We present an in-depth empirical and analytical study of bias
in the machine learning development workflow of speaker verification, a voice
biometric and core task in automated speaker recognition. Drawing on an
established framework for understanding sources of harm in machine learning, we
show that bias exists at every development stage in the well-known VoxCeleb
Speaker Recognition Challenge, including model building, implementation, and
data generation. Most affected are female speakers and non-US nationalities,
who experience significant performance degradation. Leveraging the insights
from our findings, we make practical recommendations for mitigating bias in
automated speaker recognition, and outline future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data and knowledge-driven approaches for multilingual training to improve the performance of speech recognition systems of Indian languages. (arXiv:2201.09494v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09494">
<div class="article-summary-box-inner">
<span><p>We propose data and knowledge-driven approaches for multilingual training of
the automated speech recognition (ASR) system for a target language by pooling
speech data from multiple source languages. Exploiting the acoustic
similarities between Indian languages, we implement two approaches. In
phone/senone mapping, deep neural network (DNN) learns to map senones or phones
from one language to the others, and the transcriptions of the source languages
are modified such that they can be used along with the target language data to
train and fine-tune the target language ASR system. In the other approach, we
model the acoustic information for all the languages simultaneously by training
a multitask DNN (MTDNN) to predict the senones of each language in different
output layers. The cross-entropy loss and the weight update procedure are
modified such that only the shared layers and the output layer responsible for
predicting the senone classes of a language are updated during training, if the
feature vector belongs to that particular language. In the low-resource setting
(LRS), 40 hours of transcribed data each for Tamil, Telugu and Gujarati
languages are used for training. The DNN based senone mapping technique gives
relative improvements in word error rates (WER) of 9.66%, 7.2% and 15.21% over
the baseline system for Tamil, Gujarati and Telugu languages, respectively. In
medium-resourced setting (MRS), 160, 275 and 135 hours of data for Tamil,
Kannada and Hindi languages are used, where, the same technique gives better
relative improvements of 13.94%, 10.28% and 27.24% for Tamil, Kannada and
Hindi, respectively. The MTDNN with senone mapping based training in LRS, gives
higher relative WER improvements of 15.0%, 17.54% and 16.06%, respectively for
Tamil, Gujarati and Telugu, whereas in MRS, we see improvements of 21.24%
21.05% and 30.17% for Tamil, Kannada and Hindi languages, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Books. (arXiv:2201.09518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09518">
<div class="article-summary-box-inner">
<span><p>The article explores new ways of written language aided by AI technologies,
like GPT-2 and GPT-3. The question that is stated in the paper is not about
whether these novel technologies will eventually replace authored books, but
how to relate to and contextualize such publications and what kind of new
tools, processes, and ideas are behind them. For that purpose, a new concept of
synthetic books is introduced in the article. It stands for the publications
created by deploying AI technology, more precisely autoregressive language
models that are able to generate human-like text. Supported by the case
studies, the value and reasoning of the synthetic books are discussed. The
paper emphasizes that artistic quality is an issue when it comes to
AI-generated content. The article introduces projects that demonstrate an
interactive input by an artist and/or audience combined with the
deep-learning-based language models. In the end, the paper focuses on
understanding the neural aesthetics of written language in the art context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BTPK-based learning: An Interpretable Method for Named Entity Recognition. (arXiv:2201.09523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09523">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is an essential task in natural language
processing, but the internal mechanism of most NER models is a black box for
users. In some high-stake decision-making areas, improving the interpretability
of an NER method is crucial but challenging. In this paper, based on the
existing Deterministic Talmudic Public announcement logic (TPK) model, we
propose a novel binary tree model (called BTPK) and apply it to two widely used
Bi-RNNs to obtain BTPK-based interpretable ones. Then, we design a
counterfactual verification module to verify the BTPK-based learning method.
Experimental results on three public datasets show that the BTPK-based learning
outperform two classical Bi-RNNs with self-attention, especially on small,
simple data and relatively large, complex data. Moreover, the counterfactual
verification demonstrates that the explanations provided by the BTPK-based
learning method are reasonable and accurate in NER tasks. Besides, the logical
reasoning based on BTPK shows how Bi-RNNs handle NER tasks, with different
distance of public announcements on long and complex sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals. (arXiv:2201.09555v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09555">
<div class="article-summary-box-inner">
<span><p>Scholarly data is growing continuously containing information about the
articles from plethora of venues including conferences, journals, etc. Many
initiatives have been taken to make scholarly data available in the for of
Knowledge Graphs (KGs). These efforts to standardize these data and make them
accessible have also lead to many challenges such as exploration of scholarly
articles, ambiguous authors, etc. This study more specifically targets the
problem of Author Name Disambiguation (AND) on Scholarly KGs and presents a
novel framework, Literally Author Name Disambiguation (LAND), which utilizes
Knowledge Graph Embeddings (KGEs) using multimodal literal information
generated from these KGs. This framework is based on three components: 1)
Multimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical
Agglomerative Clustering. Extensive experiments have been conducted on two
newly created KGs: (i) KG containing information from Scientometrics Journal
from 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known
benchmark for AND provided by AMiner (AMiner-534K). The results show that our
proposed architecture outperforms our baselines of 8-14\% in terms of F$_1$
score and shows competitive performances on a challenging benchmark such as
AMiner. The code and the datasets are publicly available through Github
(https://github.com/sntcristian/and-kge) and Zenodo
(https://zenodo.org/record/5675787\#.YcCJzL3MJTY) respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEPOR: An Augmented Machine Translation Evaluation Metric. (arXiv:1703.08748v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1703.08748">
<div class="article-summary-box-inner">
<span><p>Machine translation (MT) was developed as one of the hottest research topics
in the natural language processing (NLP) literature. One important issue in MT
is that how to evaluate the MT system reasonably and tell us whether the
translation system makes an improvement or not. The traditional manual judgment
methods are expensive, time-consuming, unrepeatable, and sometimes with low
agreement. On the other hand, the popular automatic MT evaluation methods have
some weaknesses. Firstly, they tend to perform well on the language pairs with
English as the target language, but weak when English is used as source.
Secondly, some methods rely on many additional linguistic features to achieve
good performance, which makes the metric unable to replicate and apply to other
language pairs easily. Thirdly, some popular metrics utilize incomprehensive
factors, which result in low performance on some practical tasks. In this
thesis, to address the existing problems, we design novel MT evaluation methods
and investigate their performances on different languages. Firstly, we design
augmented factors to yield highly accurate evaluation. Secondly, we design a
tunable evaluation model where weighting of factors can be optimized according
to the characteristics of languages. Thirdly, in the enhanced version of our
methods, we design concise linguistic feature using part-of-speech (POS) to
show that our methods can yield even higher performance when using some
external linguistic resources. Finally, we introduce the practical performance
of our metrics in the ACL-WMT workshop shared tasks, which show that the
proposed methods are robust across different languages. In addition, we also
present some novel work on quality estimation of MT without using reference
translations including the usage of probability models of Na\"ive Bayes (NB),
support vector machine (SVM) classification algorithms, and CRFs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v11 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge-Enhanced Text Generation. (arXiv:2010.04389v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04389">
<div class="article-summary-box-inner">
<span><p>The goal of text generation is to make machines express in human language. It
is one of the most important yet challenging tasks in natural language
processing (NLP). Since 2014, various neural encoder-decoder models pioneered
by Seq2Seq have been proposed to achieve the goal by learning to map input text
to output text. However, the input text alone often provides limited knowledge
to generate the desired output, so the performance of text generation is still
far from satisfaction in many real-world scenarios. To address this issue,
researchers have considered incorporating various forms of knowledge beyond the
input text into the generation models. This research direction is known as
knowledge-enhanced text generation. In this survey, we present a comprehensive
review of the research on knowledge enhanced text generation over the past five
years. The main content includes two parts: (i) general methods and
architectures for integrating knowledge into text generation; (ii) specific
techniques and applications according to different forms of knowledge data.
This survey can have broad audiences, researchers and practitioners, in
academia and industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Sparse Transformer for Multilingual Translation. (arXiv:2104.07358v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07358">
<div class="article-summary-box-inner">
<span><p>Multilingual machine translation has attracted much attention recently due to
its support of knowledge transfer among languages and the low cost of training
and deployment compared with numerous bilingual models. A known challenge of
multilingual models is the negative language interference. In order to enhance
the translation quality, deeper and wider architectures are applied to
multilingual modeling for larger model capacity, which suffers from the
increased inference cost at the same time. It has been pointed out in recent
studies that parameters shared among languages are the cause of interference
while they may also enable positive transfer. Based on these insights, we
propose an adaptive and sparse architecture for multilingual modeling, and
train the model to learn shared and language-specific parameters to improve the
positive transfer and mitigate the interference. The sparse architecture only
activates a sub-network which preserves inference efficiency, and the adaptive
design selects different sub-networks based on the input languages. Our model
outperforms strong baselines across multiple benchmarks. On the large-scale
OPUS dataset with $100$ languages, we achieve $+2.1$, $+1.3$ and $+6.2$ BLEU
improvements in one-to-many, many-to-one and zero-shot tasks respectively
compared to standard Transformer without increasing the inference cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results for specific few-shot
classification tasks. The core idea of prompt-tuning is to insert text pieces
(i.e., templates) into the input and transform a classification task into a
masked language modeling problem. However, for relation extraction, determining
an appropriate prompt template requires domain expertise, and it is cumbersome
and time-consuming to obtain a suitable label word. Furthermore, there exists
abundant semantic and prior knowledge among the relation labels that cannot be
ignored. To this end, we focus on incorporating knowledge among relation labels
into prompt-tuning for relation extraction and propose a Knowledge-aware
Prompt-tuning approach with synergistic optimization (KnowPrompt).
Specifically, we inject latent knowledge contained in relation labels into
prompt construction with learnable virtual type words and answer words. Then,
we synergistically optimize their representation with structured constraints.
Extensive experimental results on five datasets with standard and low-resource
settings demonstrate the effectiveness of our approach. Our code and datasets
are available in https://github.com/zjunlp/KnowPrompt for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duplex Sequence-to-Sequence Learning for Reversible Machine Translation. (arXiv:2105.03458v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03458">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence learning naturally has two directions. How to
effectively utilize supervision signals from both directions? Existing
approaches either require two separate models, or a multitask-learned model but
with inferior performance. In this paper, we propose REDER (Reversible Duplex
Transformer), a parameter-efficient model and apply it to machine translation.
Either end of REDER can simultaneously input and output a distinct language.
Thus REDER enables reversible machine translation by simply flipping the input
and output ends. Experiments verify that REDER achieves the first success of
reversible machine translation, which helps outperform its multitask-trained
baselines by up to 1.3 BLEU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13948">
<div class="article-summary-box-inner">
<span><p>Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation. (arXiv:2109.06379v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06379">
<div class="article-summary-box-inner">
<span><p>Natural language generation (NLG) spans a broad range of tasks, each of which
serves for specific objectives and desires different properties of generated
text. The complexity makes automatic evaluation of NLG particularly
challenging. Previous work has typically focused on a single task and developed
individual evaluation metrics based on specific intuitions. In this paper, we
propose a unifying perspective that facilitates the design of metrics for a
wide range of language generation tasks and quality aspects. Based on the
nature of information change from input to output, we classify NLG tasks into
compression (e.g., summarization), transduction (e.g., text rewriting), and
creation (e.g., dialog). The information alignment, or overlap, between input,
context, and output text plays a common central role in characterizing the
generation. Using the uniform concept of information alignment, we develop a
family of interpretable metrics for various NLG tasks and aspects, often
without need of gold reference data. To operationalize the metrics, we train
self-supervised models to approximate information alignment as a prediction
task. Experiments show the uniformly designed metrics achieve stronger or
comparable correlations with human judgement compared to state-of-the-art
metrics in each of diverse tasks, including text summarization, style transfer,
and knowledge-grounded dialog. With information alignment as the intermediate
representation, we deliver a composable library for easy NLG evaluation and
future metric design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14927">
<div class="article-summary-box-inner">
<span><p>Temporal expressions in text play a significant role in language
understanding and correctly identifying them is fundamental to various
retrieval and natural language processing systems. Previous works have slowly
shifted from rule-based to neural architectures, capable of tagging expressions
with higher accuracy. However, neural models can not yet distinguish between
different expression types at the same level as their rule-based counterparts.
In this work, we aim to identify the most suitable transformer architecture for
joint temporal tagging and type classification, as well as, investigating the
effect of semi-supervised training on the performance of these systems. Based
on our study of token classification variants and encoder-decoder
architectures, we present a transformer encoder-decoder model using the RoBERTa
language model as our best performing system. By supplementing training
resources with weakly labeled data from rule-based systems, our model surpasses
previous works in temporal tagging and type classification, especially on rare
classes. Our code and pre-trained experiments are available at:
https://github.com/satya77/Transformer_Temporal_Tagger
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01900">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech representation learning methods like wav2vec 2.0 and
Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and
offer good representations for numerous speech processing tasks. Despite the
success of these methods, they require large memory and high pre-training
costs, making them inaccessible for researchers in academia and small
companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task
learning framework to distill hidden representations from a HuBERT model
directly. This method reduces HuBERT's size by 75% and 73% faster while
retaining most performance in ten different tasks. Moreover, DistilHuBERT
required little training time and data, opening the possibilities of
pre-training personal and on-device SSL models for speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR. (arXiv:2110.03151v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03151">
<div class="article-summary-box-inner">
<span><p>This paper presents Transcribe-to-Diarize, a new approach for neural speaker
diarization that uses an end-to-end (E2E) speaker-attributed automatic speech
recognition (SA-ASR). The E2E SA-ASR is a joint model that was recently
proposed for speaker counting, multi-talker speech recognition, and speaker
identification from monaural audio that contains overlapping speech. Although
the E2E SA-ASR model originally does not estimate any time-related information,
we show that the start and end times of each word can be estimated with
sufficient accuracy from the internal state of the E2E SA-ASR by adding a small
number of learnable parameters. Similar to the target-speaker voice activity
detection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to
estimate speech activity of each speaker while it has the advantages of (i)
handling unlimited number of speakers, (ii) leveraging linguistic information
for speaker diarization, and (iii) simultaneously generating speaker-attributed
transcriptions. Experimental results on the LibriCSS and AMI corpora show that
the proposed method achieves significantly better diarization error rate than
various existing speaker diarization methods when the number of speakers is
unknown, and achieves a comparable performance to TS-VAD when the number of
speakers is given in advance. The proposed method simultaneously generates
speaker-attributed transcription with state-of-the-art accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Pitfalls of Analyzing Individual Neurons in Language Models. (arXiv:2110.07483v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07483">
<div class="article-summary-box-inner">
<span><p>While many studies have shown that linguistic information is encoded in
hidden word representations, few have studied individual neurons, to show how
and in which neurons it is encoded. Among these, the common approach is to use
an external probe to rank neurons according to their relevance to some
linguistic attribute, and to evaluate the obtained ranking using the same probe
that produced it. We show two pitfalls in this methodology: 1. It confounds
distinct factors: probe quality and ranking quality. We separate them and draw
conclusions on each. 2. It focuses on encoded information, rather than
information that is used by the model. We show that these are not the same. We
compare two recent ranking methods and a simple one we introduce, and evaluate
them with regard to both of these aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GNN-LM: Language Modeling based on Global Contexts via GNN. (arXiv:2110.08743v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08743">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion that ``{\it to copy is easier than to memorize}``, in
this work, we introduce GNN-LM, which extends the vanilla neural language model
(LM) by allowing to reference similar contexts in the entire training corpus.
We build a directed heterogeneous graph between an input context and its
semantically related neighbors selected from the training corpus, where nodes
are tokens in the input context and retrieved neighbor contexts, and edges
represent connections between nodes. Graph neural networks (GNNs) are
constructed upon the graph to aggregate information from similar contexts to
decode the token. This learning paradigm provides direct access to the
reference contexts and helps improve a model's generalization ability. We
conduct comprehensive experiments to validate the effectiveness of the GNN-LM:
GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a
3.9 point improvement over its counterpart of the vanilla LM model), and shows
substantial improvement on One Billion Word and Enwiki8 datasets against strong
baselines. In-depth ablation studies are performed to understand the mechanics
of GNN-LM. \footnote{The code can be found at
\url{https://github.com/ShannonAI/GNN-LM}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensembling Graph Predictions for AMR Parsing. (arXiv:2110.09131v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09131">
<div class="article-summary-box-inner">
<span><p>In many machine learning tasks, models are trained to predict structure data
such as graphs. For example, in natural language processing, it is very common
to parse texts into dependency trees or abstract meaning representation (AMR)
graphs. On the other hand, ensemble methods combine predictions from multiple
models to create a new one that is more robust and accurate than individual
predictions. In the literature, there are many ensembling techniques proposed
for classification or regression problems, however, ensemble graph prediction
has not been studied thoroughly. In this work, we formalize this problem as
mining the largest graph that is the most supported by a collection of graph
predictions. As the problem is NP-Hard, we propose an efficient heuristic
algorithm to approximate the optimal solution. To validate our approach, we
carried out experiments in AMR parsing problems. The experimental results
demonstrate that the proposed approach can combine the strength of
state-of-the-art AMR parsers to create new predictions that are more accurate
than any individual models in five standard benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Embedded Knowledge Graph Multi-hop Question Answering by introducing Relational Chain Reasoning. (arXiv:2110.12679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12679">
<div class="article-summary-box-inner">
<span><p>Knowledge Base Question Answering (KBQA) aims to answer userquestions from a
knowledge base (KB) by identifying the reasoningrelations between topic entity
and answer. As a complex branchtask of KBQA, multi-hop KGQA requires reasoning
over multi-hop relational chains preserved in KG to arrive at the right
answer.Despite the successes made in recent years, the existing works
onanswering multi-hop complex question face the following challenges: i)
suffering from poor performances due to the neglect of explicit relational
chain order and its relational types reflected inuser questions; ii) failing to
consider implicit relations between thetopic entity and the answer implied in
structured KG because oflimited neighborhood size constraints in subgraph
retrieval based algorithms. To address these issues in multi-hop KGQA, we
proposea novel model in this paper, namely Relational Chain-based Embed-ded
KGQA (Rce-KGQA), which simultaneously utilizes the explicitrelational chain
described in natural language questions and the implicit relational chain
stored in structured KG. Our extensiveempirical study on two open-domain
benchmarks proves that ourmethod significantly outperforms the state-of-the-art
counterpartslike GraftNet, PullNet and EmbedKGQA. Comprehensive ablation
experiments also verify the effectiveness of our method for multi-hop KGQA
tasks. We have made our model's source code availableat Github:
https://github.com/albert-jin/Rce-KGQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. (arXiv:2110.13900v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13900">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) achieves great success in speech recognition,
while limited exploration has been attempted for other speech processing tasks.
As speech signal contains multi-faceted information including speaker identity,
paralinguistics, spoken content, etc., learning universal representations for
all speech tasks is challenging. To tackle the problem, we propose a new
pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM
jointly learns masked speech prediction and denoising in pre-training. By this
means, WavLM does not only keep the speech content modeling capability by the
masked speech prediction, but also improves the potential to non-ASR tasks by
the speech denoising. In addition, WavLM employs gated relative position bias
for the Transformer structure to better capture sequence ordering of input
speech, and scale up the training dataset from 60k hours to 94k hours. WavLM
Large achieves state-of-the-art performance on the SUPERB benchmark, and brings
significant improvements for various speech processing tasks on their
representative benchmarks. The code and pre-trained models are available at
https://aka.ms/wavlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrast and Generation Make BART a Good Dialogue Emotion Recognizer. (arXiv:2112.11202v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11202">
<div class="article-summary-box-inner">
<span><p>In dialogue systems, utterances with similar semantics may have distinctive
emotions under different contexts. Therefore, modeling long-range contextual
emotional relationships with speaker dependency plays a crucial part in
dialogue emotion recognition. Meanwhile, distinguishing the different emotion
categories is non-trivial since they usually have semantically similar
sentiments. To this end, we adopt supervised contrastive learning to make
different emotions mutually exclusive to identify similar emotions better.
Meanwhile, we utilize an auxiliary response generation task to enhance the
model's ability of handling context information, thereby forcing the model to
recognize emotions with similar semantics in diverse contexts. To achieve these
objectives, we use the pre-trained encoder-decoder model BART as our backbone
model since it is very suitable for both understanding and generation tasks.
The experiments on four datasets demonstrate that our proposed model obtains
significantly more favorable results than the state-of-the-art model in
dialogue emotion recognition. The ablation study further demonstrates the
effectiveness of supervised contrastive loss and generative loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Sensitivity of Deep Learning Based Text Classification Algorithms to Practical Input Perturbations. (arXiv:2201.00318v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00318">
<div class="article-summary-box-inner">
<span><p>Text classification is a fundamental Natural Language Processing task that
has a wide variety of applications, where deep learning approaches have
produced state-of-the-art results. While these models have been heavily
criticized for their black-box nature, their robustness to slight perturbations
in input text has been a matter of concern. In this work, we carry out a
data-focused study evaluating the impact of systematic practical perturbations
on the performance of the deep learning based text classification models like
CNN, LSTM, and BERT-based algorithms. The perturbations are induced by the
addition and removal of unwanted tokens like punctuation and stop-words that
are minimally associated with the final performance of the model. We show that
these deep learning approaches including BERT are sensitive to such legitimate
input perturbations on four standard benchmark datasets SST2, TREC-6, BBC News,
and tweet_eval. We observe that BERT is more susceptible to the removal of
tokens as compared to the addition of tokens. Moreover, LSTM is slightly more
sensitive to input perturbations as compared to CNN based model. The work also
serves as a practical guide to assessing the impact of discrepancies in
train-test conditions on the final performance of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03335">
<div class="article-summary-box-inner">
<span><p>We present a new open-source and extensible knowledge extraction toolkit,
called DeepKE (Deep learning based Knowledge Extraction), supporting standard
fully supervised, low-resource few-shot and document-level scenarios. DeepKE
implements various information extraction tasks, including named entity
recognition, relation extraction and attribute extraction. With a unified
framework, DeepKE allows developers and researchers to customize datasets and
models to extract information from unstructured texts according to their
requirements. Specifically, DeepKE not only provides various functional modules
and model implementation for different tasks and scenarios but also organizes
all components by consistent frameworks to maintain sufficient modularity and
extensibility. Besides, we present an online platform in
<a href="http://deepke.zjukg.cn/">this http URL</a> for real-time extraction of various tasks. DeepKE has
been equipped with Google Colab tutorials and comprehensive documents for
beginners. We release the source code at https://github.com/zjunlp/DeepKE, with
a demo video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Lexical Simplification for Turkish. (arXiv:2201.05878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05878">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the first automatic lexical simplification system
for the Turkish language. Recent text simplification efforts rely on manually
crafted simplified corpora and comprehensive NLP tools that can analyse the
target text both in word and sentence levels. Turkish is a morphologically rich
agglutinative language that requires unique considerations such as the proper
handling of inflectional cases. Being a low-resource language in terms of
available resources and industrial-strength tools, it makes the text
simplification task harder to approach. We present a new text simplification
pipeline based on pretrained representation model BERT together with
morphological features to generate grammatically correct and semantically
appropriate word-level simplifications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMDA: Language Models for Dialog Applications. (arXiv:2201.08239v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08239">
<div class="article-summary-box-inner">
<span><p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">On the in vivo recognition of kidney stones using machine learning. (arXiv:2201.08865v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08865">
<div class="article-summary-box-inner">
<span><p>Determining the type of kidney stones allows urologists to prescribe a
treatment to avoid recurrence of renal lithiasis. An automated in-vivo
image-based classification method would be an important step towards an
immediate identification of the kidney stone type required as a first phase of
the diagnosis. In the literature it was shown on ex-vivo data (i.e., in very
controlled scene and image acquisition conditions) that an automated kidney
stone classification is indeed feasible. This pilot study compares the kidney
stone recognition performances of six shallow machine learning methods and
three deep-learning architectures which were tested with in-vivo images of the
four most frequent urinary calculi types acquired with an endoscope during
standard ureteroscopies. This contribution details the database construction
and the design of the tested kidney stones classifiers. Even if the best
results were obtained by the Inception v3 architecture (weighted precision,
recall and F1-score of 0.97, 0.98 and 0.97, respectively), it is also shown
that choosing an appropriate colour space and texture features allows a shallow
machine learning method to approach closely the performances of the most
promising deep-learning methods (the XGBoost classifier led to weighted
precision, recall and F1-score values of 0.96). This paper is the first one
that explores the most discriminant features to be extracted from images
acquired during ureteroscopies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-to-Video Re-Identification via Mutual Discriminative Knowledge Transfer. (arXiv:2201.08887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08887">
<div class="article-summary-box-inner">
<span><p>The gap in representations between image and video makes Image-to-Video
Re-identification (I2V Re-ID) challenging, and recent works formulate this
problem as a knowledge distillation (KD) process. In this paper, we propose a
mutual discriminative knowledge distillation framework to transfer a
video-based richer representation to an image based representation more
effectively. Specifically, we propose the triplet contrast loss (TCL), a novel
loss designed for KD. During the KD process, the TCL loss transfers the local
structure, exploits the higher order information, and mitigates the
misalignment of the heterogeneous output of teacher and student networks.
Compared with other losses for KD, the proposed TCL loss selectively transfers
the local discriminative features from teacher to student, making it effective
in the ReID. Besides the TCL loss, we adopt mutual learning to regularize both
the teacher and student networks training. Extensive experiments demonstrate
the effectiveness of our method on the MARS, DukeMTMC-VideoReID and VeRi-776
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signal Strength and Noise Drive Feature Preference in CNN Image Classifiers. (arXiv:2201.08893v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08893">
<div class="article-summary-box-inner">
<span><p>Feature preference in Convolutional Neural Network (CNN) image classifiers is
integral to their decision making process, and while the topic has been well
studied, it is still not understood at a fundamental level. We test a range of
task relevant feature attributes (including shape, texture, and color) with
varying degrees of signal and noise in highly controlled CNN image
classification experiments using synthetic datasets to determine feature
preferences. We find that CNNs will prefer features with stronger signal
strength and lower noise irrespective of whether the feature is texture, shape,
or color. This provides guidance for a predictive model for task relevant
feature preferences, demonstrates pathways for bias in machine models that can
be avoided with careful controls on experimental setup, and suggests that
comparisons between how humans and machines prefer task relevant features in
vision classification tasks should be revisited. Code to reproduce experiments
in this paper can be found at
\url{https://github.com/mwolff31/signal_preference}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ensemble Model for Face Liveness Detection. (arXiv:2201.08901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08901">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a passive method to detect face presentation attack
a.k.a face liveness detection using an ensemble deep learning technique. Face
liveness detection is one of the key steps involved in user identity
verification of customers during the online onboarding/transaction processes.
During identity verification, an unauthenticated user tries to bypass the
verification system by several means, for example, they can capture a user
photo from social media and do an imposter attack using printouts of users
faces or using a digital photo from a mobile device and even create a more
sophisticated attack like video replay attack. We have tried to understand the
different methods of attack and created an in-house large-scale dataset
covering all the kinds of attacks to train a robust deep learning model. We
propose an ensemble method where multiple features of the face and background
regions are learned to predict whether the user is a bonafide or an attacker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAR Image Change Detection Based on Multiscale Capsule Network. (arXiv:2201.08935v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08935">
<div class="article-summary-box-inner">
<span><p>Traditional synthetic aperture radar image change detection methods based on
convolutional neural networks (CNNs) face the challenges of speckle noise and
deformation sensitivity. To mitigate these issues, we proposed a Multiscale
Capsule Network (Ms-CapsNet) to extract the discriminative information between
the changed and unchanged pixels. On the one hand, the multiscale capsule
module is employed to exploit the spatial relationship of features. Therefore,
equivariant properties can be achieved by aggregating the features from
different positions. On the other hand, an adaptive fusion convolution (AFC)
module is designed for the proposed Ms-CapsNet. Higher semantic features can be
captured for the primary capsules. Feature extracted by the AFC module
significantly improves the robustness to speckle noise. The effectiveness of
the proposed Ms-CapsNet is verified on three real SAR datasets. The comparison
experiments with four state-of-the-art methods demonstrate the efficiency of
the proposed method. Our codes are available at
https://github.com/summitgao/SAR_CD_MS_CapsNet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive DropBlock Enhanced Generative Adversarial Networks for Hyperspectral Image Classification. (arXiv:2201.08938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08938">
<div class="article-summary-box-inner">
<span><p>In recent years, hyperspectral image (HSI) classification based on generative
adversarial networks (GAN) has achieved great progress. GAN-based
classification methods can mitigate the limited training sample dilemma to some
extent. However, several studies have pointed out that existing GAN-based HSI
classification methods are heavily affected by the imbalanced training data
problem. The discriminator in GAN always contradicts itself and tries to
associate fake labels to the minority-class samples, and thus impair the
classification performance. Another critical issue is the mode collapse in
GAN-based methods. The generator is only capable of producing samples within a
narrow scope of the data space, which severely hinders the advancement of
GAN-based HSI classification methods. In this paper, we proposed an Adaptive
DropBlock-enhanced Generative Adversarial Networks (ADGAN) for HSI
classification. First, to solve the imbalanced training data problem, we adjust
the discriminator to be a single classifier, and it will not contradict itself.
Second, an adaptive DropBlock (AdapDrop) is proposed as a regularization method
employed in the generator and discriminator to alleviate the mode collapse
issue. The AdapDrop generated drop masks with adaptive shapes instead of a
fixed size region, and it alleviates the limitations of DropBlock in dealing
with ground objects with various shapes. Experimental results on three HSI
datasets demonstrated that the proposed ADGAN achieved superior performance
over state-of-the-art GAN-based methods. Our codes are available at
https://github.com/summitgao/HC_ADGAN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCNGAN: A Deformable Convolutional-Based GAN with QP Adaptation for Perceptual Quality Enhancement of Compressed Video. (arXiv:2201.08944v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08944">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a deformable convolution-based generative
adversarial network (DCNGAN) for perceptual quality enhancement of compressed
videos. DCNGAN is also adaptive to the quantization parameters (QPs). Compared
with optical flows, deformable convolutions are more effective and efficient to
align frames. Deformable convolutions can operate on multiple frames, thus
leveraging more temporal information, which is beneficial for enhancing the
perceptual quality of compressed videos. Instead of aligning frames in a
pairwise manner, the deformable convolution can process multiple frames
simultaneously, which leads to lower computational complexity. Experimental
results demonstrate that the proposed DCNGAN outperforms other state-of-the-art
compressed video quality enhancement algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Aggregation for Adaptive RGBT Tracking. (arXiv:2201.08949v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08949">
<div class="article-summary-box-inner">
<span><p>Visual object tracking with RGB and thermal infrared (TIR) spectra available,
shorted in RGBT tracking, is a novel and challenging research topic which draws
increasing attention nowadays. In this paper, we propose an RGBT tracker which
takes spatio-temporal clues into account for robust appearance model learning,
and simultaneously, constructs an adaptive fusion sub-network for cross-modal
interactions. Unlike most existing RGBT trackers that implement object tracking
tasks with only spatial information included, temporal information is further
considered in this method. Specifically, different from traditional Siamese
trackers, which only obtain one search image during the process of picking up
template-search image pairs, an extra search sample adjacent to the original
one is selected to predict the temporal transformation, resulting in improved
robustness of tracking performance.As for multi-modal tracking, constrained to
the limited RGBT datasets, the adaptive fusion sub-network is appended to our
method at the decision level to reflect the complementary characteristics
contained in two modalities. To design a thermal infrared assisted RGB tracker,
the outputs of the classification head from the TIR modality are taken into
consideration before the residual connection from the RGB modality. Extensive
experimental results on three challenging datasets, i.e. VOT-RGBT2019, GTOT and
RGBT210, verify the effectiveness of our method. Code will be shared at
\textcolor{blue}{\emph{https://github.com/Zhangyong-Tang/TAAT}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Representation Learning with Self-Supervised Attention for Low-Label High-data Regime. (arXiv:2201.08951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08951">
<div class="article-summary-box-inner">
<span><p>Self-supervision has shown outstanding results for natural language
processing, and more recently, for image recognition. Simultaneously, vision
transformers and its variants have emerged as a promising and scalable
alternative to convolutions on various computer vision tasks. In this paper, we
are the first to question if self-supervised vision transformers (SSL-ViTs) can
be adapted to two important computer vision tasks in the low-label, high-data
regime: few-shot image classification and zero-shot image retrieval. The
motivation is to reduce the number of manual annotations required to train a
visual embedder, and to produce generalizable, semantically meaningful and
robust embeddings. For few-shot image classification we train SSL-ViTs without
any supervision, on external data, and use this trained embedder to adapt
quickly to novel classes with limited number of labels. For zero-shot image
retrieval, we use SSL-ViTs pre-trained on a large dataset without any labels
and fine-tune them with several metric learning objectives. Our self-supervised
attention representations outperforms the state-of-the-art on several public
benchmarks for both tasks, namely miniImageNet and CUB200 for few-shot image
classification by up-to 6%-10%, and Stanford Online Products, Cars196 and
CUB200 for zero-shot image retrieval by up-to 4%-11%. Code is available at
\url{https://github.com/AutoVision-cloud/SSL-ViT-lowlabel-highdata}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedMed-GAN: Federated Multi-Modal Unsupervised Brain Image Synthesis. (arXiv:2201.08953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08953">
<div class="article-summary-box-inner">
<span><p>Utilizing the paired multi-modal neuroimaging data has been proved to be
effective to investigate human cognitive activities and certain pathologies.
However, it is not practical to obtain the full set of paired neuroimaging data
centrally since the collection faces several constraints, e.g., high
examination costs, long acquisition time, and even image corruption. In
addition, most of the paired neuroimaging data are dispersed into different
medical institutions and cannot group together for centralized training
considering the privacy issues. Under the circumstance, there is a clear need
to launch federated learning and facilitate the integration of other unpaired
data from different hospitals or data owners. In this paper, we build up a new
benchmark for federated multi-modal unsupervised brain image synthesis (termed
as FedMed-GAN) to bridge the gap between federated learning and medical GAN.
Moreover, based on the similarity of edge information across multi-modal
neuroimaging data, we propose a novel edge loss to solve the generative mode
collapse issue of FedMed-GAN and mitigate the performance drop resulting from
differential privacy. Compared with the state-of-the-art method shown in our
built benchmark, our novel edge loss could significantly speed up the generator
convergence rate without sacrificing performance under different unpaired data
distribution settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Change Detection from Synthetic Aperture Radar Images via Graph-Based Knowledge Supplement Network. (arXiv:2201.08954v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08954">
<div class="article-summary-box-inner">
<span><p>Synthetic aperture radar (SAR) image change detection is a vital yet
challenging task in the field of remote sensing image analysis. Most previous
works adopt a self-supervised method which uses pseudo-labeled samples to guide
subsequent training and testing. However, deep networks commonly require many
high-quality samples for parameter optimization. The noise in pseudo-labels
inevitably affects the final change detection performance. To solve the
problem, we propose a Graph-based Knowledge Supplement Network (GKSNet). To be
more specific, we extract discriminative information from the existing labeled
dataset as additional knowledge, to suppress the adverse effects of noisy
samples to some extent. Afterwards, we design a graph transfer module to
distill contextual information attentively from the labeled dataset to the
target dataset, which bridges feature correlation between datasets. To validate
the proposed method, we conducted extensive experiments on four SAR datasets,
which demonstrated the superiority of the proposed GKSNet as compared to
several state-of-the-art baselines. Our codes are available at
https://github.com/summitgao/SAR_CD_GKSNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality Bank: Learn multi-modality images across data centers without sharing medical data. (arXiv:2201.08955v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08955">
<div class="article-summary-box-inner">
<span><p>Multi-modality images have been widely used and provide comprehensive
information for medical image analysis. However, acquiring all modalities among
all institutes is costly and often impossible in clinical settings. To leverage
more comprehensive multi-modality information, we propose a privacy secured
decentralized multi-modality adaptive learning architecture named ModalityBank.
Our method could learn a set of effective domain-specific modulation parameters
plugged into a common domain-agnostic network. We demonstrate by switching
different sets of configurations, the generator could output high-quality
images for a specific modality. Our method could also complete the missing
modalities across all data centers, thus could be used for modality completion
purposes. The downstream task trained from the synthesized multi-modality
samples could achieve higher performance than learning from one real data
center and achieve close-to-real performance compare with all real images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Efficient Representations for Enhanced Object Detection on Large-scene SAR Images. (arXiv:2201.08958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08958">
<div class="article-summary-box-inner">
<span><p>It is a challenging problem to detect and recognize targets on complex
large-scene Synthetic Aperture Radar (SAR) images. Recently developed deep
learning algorithms can automatically learn the intrinsic features of SAR
images, but still have much room for improvement on large-scene SAR images with
limited data. In this paper, based on learning representations and multi-scale
features of SAR images, we propose an efficient and robust deep learning based
target detection method. Especially, by leveraging the effectiveness of
adversarial autoencoder (AAE) which influences the distribution of the
investigated data explicitly, the raw SAR dataset is augmented into an enhanced
version with a large quantity and diversity. Besides, an auto-labeling scheme
is proposed to improve labeling efficiency. Finally, with jointly training
small target chips and large-scene images, an integrated YOLO network combining
non-maximum suppression on sub-images is used to realize multiple targets
detection of high resolution images. The numerical experimental results on the
MSTAR dataset show that our method can realize target detection and recognition
on large-scene images accurately and efficiently. The superior anti-noise
performance is also confirmed by experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Correlation-based Feature Refinement for Few-shot Counting. (arXiv:2201.08959v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08959">
<div class="article-summary-box-inner">
<span><p>Few-shot counting aims to count objects of any class in an image given only a
few exemplars of the same class. Existing correlation-based few-shot counting
approaches suffer from the coarseness and low semantic level of the
correlation. To solve these problems, we propose an iterative framework to
progressively refine the exemplar-related features based on the correlation
between the image and exemplars. Then the density map is predicted from the
final refined feature map. The iterative framework includes a Correlation
Distillation module and a Feature Refinement module. During the iterations, the
exemplar-related features are gradually refined, while the exemplar-unrelated
features are suppressed, benefiting few-shot counting where the
exemplar-related features are more important. Our approach surpasses all
baselines significantly on few-shot counting benchmark FSC-147. Surprisingly,
though designed for general class-agnostic counting, our approach still
achieves state-of-the-art performance on car counting benchmarks CARPK and
PUCPR+, and crowd counting benchmarks UCSD and Mall. We also achieve
competitive performance on crowd counting benchmark ShanghaiTech. The code will
be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Representation for SPD Matrices with Application to Image-Set Classification. (arXiv:2201.08962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08962">
<div class="article-summary-box-inner">
<span><p>Collaborative representation-based classification (CRC) has demonstrated
remarkable progress in the past few years because of its closed-form analytical
solutions. However, the existing CRC methods are incapable of processing the
nonlinear variational information directly. Recent advances illustrate that how
to effectively model these nonlinear variational information and learn
invariant representations is an open challenge in the community of computer
vision and pattern recognition To this end, we try to design a new algorithm to
handle this problem. Firstly, the second-order statistic, i.e., covariance
matrix is applied to model the original image sets. Due to the space formed by
a set of nonsingular covariance matrices is a well-known Symmetric Positive
Definite (SPD) manifold, generalising the Euclidean collaborative
representation to the SPD manifold is not an easy task. Then, we devise two
strategies to cope with this issue. One attempts to embed the SPD
manifold-valued data representations into an associated tangent space via the
matrix logarithm map. Another is to embed them into a Reproducing Kernel
Hilbert Space (RKHS) by utilizing the Riemannian kernel function. After these
two treatments, CRC is applicable to the SPD manifold-valued features. The
evaluations on four banchmarking datasets justify its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffractive all-optical computing for quantitative phase imaging. (arXiv:2201.08964v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08964">
<div class="article-summary-box-inner">
<span><p>Quantitative phase imaging (QPI) is a label-free computational imaging
technique that provides optical path length information of specimens. In modern
implementations, the quantitative phase image of an object is reconstructed
digitally through numerical methods running in a computer, often using
iterative algorithms. Here, we demonstrate a diffractive QPI network that can
synthesize the quantitative phase image of an object by converting the input
phase information of a scene into intensity variations at the output plane. A
diffractive QPI network is a specialized all-optical processor designed to
perform a quantitative phase-to-intensity transformation through passive
diffractive surfaces that are spatially engineered using deep learning and
image data. Forming a compact, all-optical network that axially extends only
~200-300 times the illumination wavelength, this framework can replace
traditional QPI systems and related digital computational burden with a set of
passive transmissive layers. All-optical diffractive QPI networks can
potentially enable power-efficient, high frame-rate and compact phase imaging
systems that might be useful for various applications, including, e.g., on-chip
microscopy and sensing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Rectangle Flip Attack: A Query-based Black-box Attack against Object Detection. (arXiv:2201.08970v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08970">
<div class="article-summary-box-inner">
<span><p>Object detection has been widely used in many safety-critical tasks, such as
autonomous driving. However, its vulnerability to adversarial examples has not
been sufficiently studied, especially under the practical scenario of black-box
attacks, where the attacker can only access the query feedback of predicted
bounding-boxes and top-1 scores returned by the attacked model. Compared with
black-box attack to image classification, there are two main challenges in
black-box attack to detection. Firstly, even if one bounding-box is
successfully attacked, another sub-optimal bounding-box may be detected near
the attacked bounding-box. Secondly, there are multiple bounding-boxes, leading
to very high attack cost. To address these challenges, we propose a Parallel
Rectangle Flip Attack (PRFA) via random search. We explain the difference
between our method with other attacks in Fig.~\ref{fig1}. Specifically, we
generate perturbations in each rectangle patch to avoid sub-optimal detection
near the attacked region. Besides, utilizing the observation that adversarial
perturbations mainly locate around objects' contours and critical points under
white-box attacks, the search space of attacked rectangles is reduced to
improve the attack efficiency. Moreover, we develop a parallel mechanism of
attacking multiple rectangles simultaneously to further accelerate the attack
process. Extensive experiments demonstrate that our method can effectively and
efficiently attack various popular object detectors, including anchor-based and
anchor-free, and generate transferable adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Adversarial Recognition of Refined Window Structures for Inverse Procedural Fa\c{c}ade Modeling. (arXiv:2201.08977v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08977">
<div class="article-summary-box-inner">
<span><p>Deep learning methods are notoriously data-hungry, which requires a large
number of labeled samples. Unfortunately, the large amount of interactive
sample labeling efforts has dramatically hindered the application of deep
learning methods, especially for 3D modeling tasks, which require heterogeneous
samples. To alleviate the work of data annotation for learned 3D modeling of
fa\c{c}ades, this paper proposed a semi-supervised adversarial recognition
strategy embedded in inverse procedural modeling. Beginning with textured LOD-2
(Level-of-Details) models, we use the classical convolutional neural networks
to recognize the types and estimate the parameters of windows from image
patches. The window types and parameters are then assembled into procedural
grammar. A simple procedural engine is built inside an existing 3D modeling
software, producing fine-grained window geometries. To obtain a useful model
from a few labeled samples, we leverage the generative adversarial network to
train the feature extractor in a semi-supervised manner. The adversarial
training strategy can also exploit unlabeled data to make the training phase
more stable. Experiments using publicly available fa\c{c}ade image datasets
reveal that the proposed training strategy can obtain about 10% improvement in
classification accuracy and 50% improvement in parameter estimation under the
same network structure. In addition, performance gains are more pronounced when
testing against unseen data featuring different fa\c{c}ade styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BBA-net: A bi-branch attention network for crowd counting. (arXiv:2201.08983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08983">
<div class="article-summary-box-inner">
<span><p>In the field of crowd counting, the current mainstream CNN-based regression
methods simply extract the density information of pedestrians without finding
the position of each person. This makes the output of the network often found
to contain incorrect responses, which may erroneously estimate the total number
and not conducive to the interpretation of the algorithm. To this end, we
propose a Bi-Branch Attention Network (BBA-NET) for crowd counting, which has
three innovation points. i) A two-branch architecture is used to estimate the
density information and location information separately. ii) Attention
mechanism is used to facilitate feature extraction, which can reduce false
responses. iii) A new density map generation method combining geometric
adaptation and Voronoi split is introduced. Our method can integrate the
pedestrian's head and body information to enhance the feature expression
ability of the density map. Extensive experiments performed on two public
datasets show that our method achieves a lower crowd counting error compared to
other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing and Dissecting Crowd Counting By Synthetic Data. (arXiv:2201.08992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08992">
<div class="article-summary-box-inner">
<span><p>In this article, we propose a simulated crowd counting dataset CrowdX, which
has a large scale, accurate labeling, parameterized realization, and high
fidelity. The experimental results of using this dataset as data enhancement
show that the performance of the proposed streamlined and efficient benchmark
network ESA-Net can be improved by 8.4\%. The other two classic heterogeneous
architectures MCNN and CSRNet pre-trained on CrowdX also show significant
performance improvements. Considering many influencing factors determine
performance, such as background, camera angle, human density, and resolution.
Although these factors are important, there is still a lack of research on how
they affect crowd counting. Thanks to the CrowdX dataset with rich annotation
information, we conduct a large number of data-driven comparative experiments
to analyze these factors. Our research provides a reference for a deeper
understanding of the crowd counting problem and puts forward some useful
suggestions in the actual deployment of the algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Array Network for Low-light Image Enhancement. (arXiv:2201.08996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08996">
<div class="article-summary-box-inner">
<span><p>Convolution neural networks (CNNs) based methods have dominated the low-light
image enhancement tasks due to their outstanding performance. However, the
convolution operation is based on a local sliding window mechanism, which is
difficult to construct the long-range dependencies of the feature maps.
Meanwhile, the self-attention based global relationship aggregation methods
have been widely used in computer vision, but these methods are difficult to
handle high-resolution images because of the high computational complexity. To
solve this problem, this paper proposes a Linear Array Self-attention (LASA)
mechanism, which uses only two 2-D feature encodings to construct 3-D global
weights and then refines feature maps generated by convolution layers. Based on
LASA, Linear Array Network (LAN) is proposed, which is superior to the existing
state-of-the-art (SOTA) methods in both RGB and RAW based low-light enhancement
tasks with a smaller amount of parameters. The code is released in
\url{https://github.com/cuiziteng/LASA_enhancement}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-aware Warping for View Synthesis. (arXiv:2201.09023v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09023">
<div class="article-summary-box-inner">
<span><p>Existing image-based rendering methods usually adopt depth-based image
warping operation to synthesize novel views. In this paper, we reason the
essential limitations of the traditional warping operation to be the limited
neighborhood and only distance-based interpolation weights. To this end, we
propose content-aware warping, which adaptively learns the interpolation
weights for pixels of a relatively large neighborhood from their contextual
information via a lightweight neural network. Based on this learnable warping
module, we propose a new end-to-end learning-based framework for novel view
synthesis from two input source views, in which two additional modules, namely
confidence-based blending and feature-assistant spatial refinement, are
naturally proposed to handle the occlusion issue and capture the spatial
correlation among pixels of the synthesized view, respectively. Besides, we
also propose a weight-smoothness loss term to regularize the network.
Experimental results on structured light field datasets with wide baselines and
unstructured multi-view datasets show that the proposed method significantly
outperforms state-of-the-art methods both quantitatively and visually. The
source code will be publicly available at https://github.com/MantangGuo/CW4VS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inter-Semantic Domain Adversarial in Histopathological Images. (arXiv:2201.09041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09041">
<div class="article-summary-box-inner">
<span><p>In computer vision, data shift has proven to be a major barrier for safe and
robust deep learning applications. In medical applications, histopathological
images are often associated with data shift and they are hardly available. It
is important to understand to what extent a model can be made robust against
data shift using all available data. Here, we first show that domain
adversarial methods can be very deleterious if they are wrongly used. We then
use domain adversarial methods to transfer data shift invariance from one
dataset to another dataset with different semantics and show that domain
adversarial methods are efficient inter-semantically with similar performance
than intra-semantical domain adversarial methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware deep learning methods for robust diabetic retinopathy classification. (arXiv:2201.09042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09042">
<div class="article-summary-box-inner">
<span><p>Automatic classification of diabetic retinopathy from retinal images has been
widely studied using deep neural networks with impressive results. However,
there is a clinical need for estimation of the uncertainty in the
classifications, a shortcoming of modern neural networks. Recently, approximate
Bayesian deep learning methods have been proposed for the task but the studies
have only considered the binary referable/non-referable diabetic retinopathy
classification applied to benchmark datasets. We present novel results by
systematically investigating a clinical dataset and a clinically relevant
5-class classification scheme, in addition to benchmark datasets and the binary
classification scheme. Moreover, we derive a connection between uncertainty
measures and classifier risk, from which we develop a new uncertainty measure.
We observe that the previously proposed entropy-based uncertainty measure
generalizes to the clinical dataset on the binary classification scheme but not
on the 5-class scheme, whereas our new uncertainty measure generalizes to the
latter case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phase-SLAM: Phase Based Simultaneous Localization and Mapping for Mobile Structured Light Illumination Systems. (arXiv:2201.09048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09048">
<div class="article-summary-box-inner">
<span><p>Structured Light Illumination (SLI) systems have been used for reliable
indoor dense 3D scanning via phase triangulation. However, mobile SLI systems
for 360 degree 3D reconstruction demand 3D point cloud registration, involving
high computational complexity. In this paper, we propose a phase based
Simultaneous Localization and Mapping (Phase-SLAM) framework for fast and
accurate SLI sensor pose estimation and 3D object reconstruction. The novelty
of this work is threefold: (1) developing a reprojection model from 3D points
to 2D phase data towards phase registration with low computational complexity;
(2) developing a local optimizer to achieve SLI sensor pose estimation
(odometry) using the derived Jacobian matrix for the 6 DoF variables; (3)
developing a compressive phase comparison method to achieve high-efficiency
loop closure detection. The whole Phase-SLAM pipeline is then exploited using
existing global pose graph optimization techniques. We build datasets from both
the unreal simulation platform and a robotic arm based SLI system in real-world
to verify the proposed approach. The experiment results demonstrate that the
proposed Phase-SLAM outperforms other state-of-the-art methods in terms of the
efficiency and accuracy of pose estimation and 3D reconstruction. The
open-source code is available at https://github.com/ZHENGXi-git/Phase-SLAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LTC-SUM: Lightweight Client-driven Personalized Video Summarization Framework Using 2D CNN. (arXiv:2201.09049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09049">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel lightweight thumbnail container-based
summarization (LTC-SUM) framework for full feature-length videos. This
framework generates a personalized keyshot summary for concurrent users by
using the computational resource of the end-user device. State-of-the-art
methods that acquire and process entire video data to generate video summaries
are highly computationally intensive. In this regard, the proposed LTC-SUM
method uses lightweight thumbnails to handle the complex process of detecting
events. This significantly reduces computational complexity and improves
communication and storage efficiency by resolving computational and privacy
bottlenecks in resource-constrained end-user devices. These improvements were
achieved by designing a lightweight 2D CNN model to extract features from
thumbnails, which helped select and retrieve only a handful of specific
segments. Extensive quantitative experiments on a set of full 18 feature-length
videos (approximately 32.9 h in duration) showed that the proposed method is
significantly computationally efficient than state-of-the-art methods on the
same end-user device configurations. Joint qualitative assessments of the
results of 56 participants showed that participants gave higher ratings to the
summaries generated using the proposed method. To the best of our knowledge,
this is the first attempt in designing a fully client-driven personalized
keyshot video summarization framework using thumbnail containers for
feature-length videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore the Expression: Facial Expression Generation using Auxiliary Classifier Generative Adversarial Network. (arXiv:2201.09061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09061">
<div class="article-summary-box-inner">
<span><p>Facial expressions are a form of non-verbal communication that humans perform
seamlessly for meaningful transfer of information. Most of the literature
addresses the facial expression recognition aspect however, with the advent of
Generative Models, it has become possible to explore the affect space in
addition to mere classification of a set of expressions. In this article, we
propose a generative model architecture which robustly generates a set of
facial expressions for multiple character identities and explores the
possibilities of generating complex expressions by combining the simple ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LTC-GIF: Attracting More Clicks on Feature-length Sports Videos. (arXiv:2201.09077v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09077">
<div class="article-summary-box-inner">
<span><p>This paper proposes a lightweight method to attract users and increase views
of the video by presenting personalized artistic media -- i.e, static
thumbnails and animated GIFs. This method analyzes lightweight thumbnail
containers (LTC) using computational resources of the client device to
recognize personalized events from full-length sports videos. In addition,
instead of processing the entire video, small video segments are processed to
generate artistic media. This makes the proposed approach more computationally
efficient compared to the baseline approaches that create artistic media using
the entire video. The proposed method retrieves and uses thumbnail containers
and video segments, which reduces the required transmission bandwidth as well
as the amount of locally stored data used during artistic media generation.
When extensive experiments were conducted on the Nvidia Jetson TX2, the
computational complexity of the proposed method was 3.57 times lower than that
of the SoA method. In the qualitative assessment, GIFs generated using the
proposed method received 1.02 higher overall ratings compared to the SoA
method. To the best of our knowledge, this is the first technique that uses LTC
to generate artistic media while providing lightweight and high-performance
services even on resource-constrained devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension. (arXiv:2201.09079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09079">
<div class="article-summary-box-inner">
<span><p>Robust subspace recovery (RSR) is a fundamental problem in robust
representation learning. Here we focus on a recently proposed RSR method termed
Dual Principal Component Pursuit (DPCP) approach, which aims to recover a basis
of the orthogonal complement of the subspace and is amenable to handling
subspaces of high relative dimension. Prior work has shown that DPCP can
provably recover the correct subspace in the presence of outliers, as long as
the true dimension of the subspace is known. We show that DPCP can provably
solve RSR problems in the {\it unknown} subspace dimension regime, as long as
orthogonality constraints -- adopted in previous DPCP formulations -- are
relaxed and random initialization is used instead of spectral one. Namely, we
propose a very simple algorithm based on running multiple instances of a
projected sub-gradient descent method (PSGM), with each problem instance
seeking to find one vector in the null space of the subspace. We theoretically
prove that under mild conditions this approach will succeed with high
probability. In particular, we show that 1) all of the problem instances will
converge to a vector in the nullspace of the subspace and 2) the ensemble of
problem instance solutions will be sufficiently diverse to fully span the
nullspace of the subspace thus also revealing its true unknown codimension. We
provide empirical results that corroborate our theoretical results and showcase
the remarkable implicit rank regularization behavior of PSGM algorithm that
allows us to perform RSR without being aware of the subspace dimension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Study on Occlusion Invariant Face Recognition under Face Mask Occlusion. (arXiv:2201.09089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09089">
<div class="article-summary-box-inner">
<span><p>The face mask is an essential sanitaryware in daily lives growing during the
pandemic period and is a big threat to current face recognition systems. The
masks destroy a lot of details in a large area of face, and it makes it
difficult to recognize them even for humans. The evaluation report shows the
difficulty well when recognizing masked faces. Rapid development and
breakthrough of deep learning in the recent past have witnessed most promising
results from face recognition algorithms. But they fail to perform far from
satisfactory levels in the unconstrained environment during the challenges such
as varying lighting conditions, low resolution, facial expressions, pose
variation and occlusions. Facial occlusions are considered one of the most
intractable problems. Especially when the occlusion occupies a large region of
the face because it destroys lots of official features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Unpaired Single Image Super-Resolution of Faces. (arXiv:2201.09109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09109">
<div class="article-summary-box-inner">
<span><p>We propose an adversarial attack for facial class-specific Single Image
Super-Resolution (SISR) methods. Existing attacks, such as the Fast Gradient
Sign Method (FGSM) or the Projected Gradient Descent (PGD) method, are either
fast but ineffective, or effective but prohibitively slow on these networks. By
closely inspecting the surface that the MSE loss, used to train such networks,
traces under varying degradations, we were able to identify its parameterizable
property. We leverage this property to propose an adverasrial attack that is
able to locate the optimum degradation (effective) without needing multiple
gradient-ascent steps (fast). Our experiments show that the proposed method is
able to achieve a better speed vs effectiveness trade-off than the
state-of-theart adversarial attacks, such as FGSM and PGD, for the task of
unpaired facial as well as class-specific SISR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Potential of Auxiliary-Classifier GANs for Image Classification in Low Data Regimes. (arXiv:2201.09120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09120">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have shown promise in augmenting
datasets and boosting convolutional neural networks' (CNN) performance on image
classification tasks. But they introduce more hyperparameters to tune as well
as the need for additional time and computational power to train supplementary
to the CNN. In this work, we examine the potential for Auxiliary-Classifier
GANs (AC-GANs) as a 'one-stop-shop' architecture for image classification,
particularly in low data regimes. Additionally, we explore modifications to the
typical AC-GAN framework, changing the generator's latent space sampling scheme
and employing a Wasserstein loss with gradient penalty to stabilize the
simultaneous training of image synthesis and classification. Through
experiments on images of varying resolutions and complexity, we demonstrate
that AC-GANs show promise in image classification, achieving competitive
performance with standard CNNs. These methods can be employed as an
'all-in-one' framework with particular utility in the absence of large amounts
of training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A Review. (arXiv:2201.09130v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09130">
<div class="article-summary-box-inner">
<span><p>Death by suicide is the seventh of the leading death cause worldwide. The
recent advancement in Artificial Intelligence (AI), specifically AI application
in image and voice processing, has created a promising opportunity to
revolutionize suicide risk assessment. Subsequently, we have witnessed
fast-growing literature of researches that applies AI to extract audiovisual
non-verbal cues for mental illness assessment. However, the majority of the
recent works focus on depression, despite the evident difference between
depression signs and suicidal behavior non-verbal cues. In this paper, we
review the recent works that study suicide ideation and suicide behavior
detection through audiovisual feature analysis, mainly suicidal voice/speech
acoustic features analysis and suicidal visual cues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIDAS: Deep learning human action intention prediction from natural eye movement patterns. (arXiv:2201.09135v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09135">
<div class="article-summary-box-inner">
<span><p>Eye movements have long been studied as a window into the attentional
mechanisms of the human brain and made accessible as novelty style
human-machine interfaces. However, not everything that we gaze upon, is
something we want to interact with; this is known as the Midas Touch problem
for gaze interfaces. To overcome the Midas Touch problem, present interfaces
tend not to rely on natural gaze cues, but rather use dwell time or gaze
gestures. Here we present an entirely data-driven approach to decode human
intention for object manipulation tasks based solely on natural gaze cues. We
run data collection experiments where 16 participants are given manipulation
and inspection tasks to be performed on various objects on a table in front of
them. The subjects' eye movements are recorded using wearable eye-trackers
allowing the participants to freely move their head and gaze upon the scene. We
use our Semantic Fovea, a convolutional neural network model to obtain the
objects in the scene and their relation to gaze traces at every frame. We then
evaluate the data and examine several ways to model the classification task for
intention prediction. Our evaluation shows that intention prediction is not a
naive result of the data, but rather relies on non-linear temporal processing
of gaze cues. We model the task as a time series classification problem and
design a bidirectional Long-Short-Term-Memory (LSTM) network architecture to
decode intentions. Our results show that we can decode human intention of
motion purely from natural gaze cues and object relative position, with
$91.9\%$ accuracy. Our work demonstrates the feasibility of natural gaze as a
Zero-UI interface for human-machine interaction, i.e., users will only need to
act naturally, and do not need to interact with the interface itself or deviate
from their natural eye movement patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation. (arXiv:2201.09139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09139">
<div class="article-summary-box-inner">
<span><p>It is critical to obtain high resolution features with long range dependency
for dense prediction tasks such as semantic segmentation. To generate
high-resolution output of size $H\times W$ from a low-resolution feature map of
size $h\times w$ ($hw\ll HW$), a naive dense transformer incurs an intractable
complexity of $\mathcal{O}(hwHW)$, limiting its application on high-resolution
dense prediction. We propose a Dual-Flattening Transformer (DFlatFormer) to
enable high-resolution output by reducing complexity to $\mathcal{O}(hw(H+W))$
that is multiple orders of magnitude smaller than the naive dense transformer.
Decomposed queries are presented to retrieve row and column attentions
tractably through separate transformers, and their outputs are combined to form
a dense feature map at high resolution. To this end, the input sequence fed
from an encoder is row-wise and column-wise flattened to align with decomposed
queries by preserving their row and column structures, respectively. Row and
column transformers also interact with each other to capture their mutual
attentions with the spatial crossings between rows and columns. We also propose
to perform attentions through efficient grouping and pooling to further reduce
the model complexity. Extensive experiments on ADE20K and Cityscapes datasets
demonstrate the superiority of the proposed dual-flattening transformer
architecture with higher mIoUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Background Invariant Classification on Infrared Imagery by Data Efficient Training and Reducing Bias in CNNs. (arXiv:2201.09144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09144">
<div class="article-summary-box-inner">
<span><p>Even though convolutional neural networks can classify objects in images very
accurately, it is well known that the attention of the network may not always
be on the semantically important regions of the scene. It has been observed
that networks often learn background textures which are not relevant to the
object of interest. In turn this makes the networks susceptible to variations
and changes in the background which negatively affect their performance. We
propose a new two-step training procedure called \textit{split training} to
reduce this bias in CNNs on both Infrared imagery and RGB data. Our split
training procedure has two steps: using MSE loss first train the layers of the
network on images with background to match the activations of the same network
when it is trained using images without background; then with these layers
frozen, train the rest of the network with cross-entropy loss to classify the
objects. Our training method outperforms the traditional training procedure in
both a simple CNN architecture, and deep CNNs like VGG and Densenet which use
lots of hardware resources, and learns to mimic human vision which focuses more
on shape and structure than background with higher accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Network Applications in Creating a Meta-Universe. (arXiv:2201.09152v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09152">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) are machine learning methods that are
used in many important and novel applications. For example, in imaging science,
GANs are effectively utilized in generating image datasets, photographs of
human faces, image and video captioning, image-to-image translation,
text-to-image translation, video prediction, and 3D object generation to name a
few. In this paper, we discuss how GANs can be used to create an artificial
world. More specifically, we discuss how GANs help to describe an image
utilizing image/video captioning methods and how to translate the image to a
new image using image-to-image translation frameworks in a theme we desire. We
articulate how GANs impact creating a customized world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Integrated Approach for Video Captioning and Applications. (arXiv:2201.09153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09153">
<div class="article-summary-box-inner">
<span><p>Physical computing infrastructure, data gathering, and algorithms have
recently had significant advances to extract information from images and
videos. The growth has been especially outstanding in image captioning and
video captioning. However, most of the advancements in video captioning still
take place in short videos. In this research, we caption longer videos only by
using the keyframes, which are a small subset of the total video frames.
Instead of processing thousands of frames, only a few frames are processed
depending on the number of keyframes. There is a trade-off between the
computation of many frames and the speed of the captioning process. The
approach in this research is to allow the user to specify the trade-off between
execution time and accuracy. In addition, we argue that linking images, videos,
and natural language offers many practical benefits and immediate practical
applications. From the modeling perspective, instead of designing and staging
explicit algorithms to process videos and generate captions in complex
processing pipelines, our contribution lies in designing hybrid deep learning
architectures to apply in long videos by captioning video keyframes. We
consider the technology and the methodology that we have developed as steps
toward the applications discussed in this research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSNet: Extremely Light-Weight Siamese Network For Change Detection in Remote Sensing Image. (arXiv:2201.09156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09156">
<div class="article-summary-box-inner">
<span><p>The Siamese network is becoming the mainstream in change detection of remote
sensing images (RSI). However, in recent years, the development of more
complicated structure, module and training processe has resulted in the
cumbersome model, which hampers their application in large-scale RSI
processing. To this end, this paper proposes an extremely lightweight Siamese
network (LSNet) for RSI change detection, which replaces standard convolution
with depthwise separable atrous convolution, and removes redundant dense
connections, retaining only valid feature flows while performing Siamese
feature fusion, greatly compressing parameters and computation amount. Compared
with the first-place model on the CCD dataset, the parameters and the
computation amount of LSNet is greatly reduced by 90.35\% and 91.34\%
respectively, with only a 1.5\% drops in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pulmonary Fissure Segmentation in CT Images Based on ODoS Filter and Shape Features. (arXiv:2201.09163v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09163">
<div class="article-summary-box-inner">
<span><p>Priori knowledge of pulmonary anatomy plays a vital role in diagnosis of lung
diseases. In CT images, pulmonary fissure segmentation is a formidable mission
due to various of factors. To address the challenge, an useful approach based
on ODoS filter and shape features is presented for pulmonary fissure
segmentation. Here, we adopt an ODoS filter by merging the orientation
information and magnitude information to highlight structure features for
fissure enhancement, which can effectively distinguish between pulmonary
fissures and clutters. Motivated by the fact that pulmonary fissures appear as
linear structures in 2D space and planar structures in 3D space in orientation
field, an orientation curvature criterion and an orientation partition scheme
are fused to separate fissure patches and other structures in different
orientation partition, which can suppress parts of clutters. Considering the
shape difference between pulmonary fissures and tubular structures in magnitude
field, a shape measure approach and a 3D skeletonization model are combined to
segment pulmonary fissures for clutters removal. When applying our scheme to 55
chest CT scans which acquired from a publicly available LOLA11 datasets, the
median F1-score, False Discovery Rate (FDR), and False Negative Rate (FNR)
respectively are 0.896, 0.109, and 0.100, which indicates that the presented
method has a satisfactory pulmonary fissure segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pre-trained Audio-Visual Transformer for Emotion Recognition. (arXiv:2201.09165v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09165">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a pretrained audio-visual Transformer trained on
more than 500k utterances from nearly 4000 celebrities from the VoxCeleb2
dataset for human behavior understanding. The model aims to capture and extract
useful information from the interactions between human facial and auditory
behaviors, with application in emotion recognition. We evaluate the model
performance on two datasets, namely CREMAD-D (emotion classification) and
MSP-IMPROV (continuous emotion regression). Experimental results show that
fine-tuning the pre-trained model helps improving emotion classification
accuracy by 5-7% and Concordance Correlation Coefficients (CCC) in continuous
emotion recognition by 0.03-0.09 compared to the same model trained from
scratch. We also demonstrate the robustness of finetuning the pre-trained model
in a low-resource setting. With only 10% of the original training set provided,
fine-tuning the pre-trained model can lead to at least 10% better emotion
recognition accuracy and a CCC score improvement by at least 0.1 for continuous
emotion recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed X-Ray Image Separation for Artworks with Concealed Designs. (arXiv:2201.09167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09167">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on X-ray images of paintings with concealed
sub-surface designs (e.g., deriving from reuse of the painting support or
revision of a composition by the artist), which include contributions from both
the surface painting and the concealed features. In particular, we propose a
self-supervised deep learning-based image separation approach that can be
applied to the X-ray images from such paintings to separate them into two
hypothetical X-ray images. One of these reconstructed images is related to the
X-ray image of the concealed painting, while the second one contains only
information related to the X-ray of the visible painting. The proposed
separation network consists of two components: the analysis and the synthesis
sub-networks. The analysis sub-network is based on learned coupled iterative
shrinkage thresholding algorithms (LCISTA) designed using algorithm unrolling
techniques, and the synthesis sub-network consists of several linear mappings.
The learning algorithm operates in a totally self-supervised fashion without
requiring a sample set that contains both the mixed X-ray images and the
separated ones. The proposed method is demonstrated on a real painting with
concealed content, Do\~na Isabel de Porcel by Francisco de Goya, to show its
effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval. (arXiv:2201.09168v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09168">
<div class="article-summary-box-inner">
<span><p>This paper aims for the task of text-to-video retrieval, where given a query
in the form of a natural-language sentence, it is asked to retrieve videos
which are semantically relevant to the given query, from a great number of
unlabeled videos. The success of this task depends on cross-modal
representation learning that projects both videos and sentences into common
spaces for semantic similarity computation. In this work, we concentrate on
video representation learning, an essential component for text-to-video
retrieval. Inspired by the reading strategy of humans, we propose a
Reading-strategy Inspired Visual Representation Learning (RIVRL) to represent
videos, which consists of two branches: a previewing branch and an
intensive-reading branch. The previewing branch is designed to briefly capture
the overview information of videos, while the intensive-reading branch is
designed to obtain more in-depth information. Moreover, the intensive-reading
branch is aware of the video overview captured by the previewing branch. Such
holistic information is found to be useful for the intensive-reading branch to
extract more fine-grained features. Extensive experiments on three datasets are
conducted, where our model RIVRL achieves a new state-of-the-art on TGIF and
VATEX. Moreover, on MSR-VTT, our model using two video features shows
comparable performance to the state-of-the-art using seven video features and
even outperforms models pre-trained on the large-scale HowTo100M dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASCNet: Action Semantic Consistent Learning of Arbitrary Progress Levels for Early Action Prediction. (arXiv:2201.09169v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09169">
<div class="article-summary-box-inner">
<span><p>Early action prediction aims to recognize human actions from only a part of
action execution, which is an important video analysis task for many practical
applications. Most prior works treat partial or full videos as a whole, which
neglects the semantic consistencies among partial videos of various progress
levels due to their large intra-class variances. In contrast, we partition
original partial or full videos to form a series of new partial videos and mine
the Action Semantic Consistent Knowledge (ASCK) among these new partial videos
evolving in arbitrary progress levels. Moreover, a novel Action Semantic
Consistent learning network (ASCNet) under the teacher-student framework is
proposed for early action prediction. Specifically, we treat partial videos as
nodes and their action semantic consistencies as edges. Then we build a
bi-directional fully connected graph for the teacher network and a
single-directional fully connected graph for the student network to model ASCK
among partial videos. The MSE and MMD losses are incorporated as our
distillation loss to further transfer the ASCK from the teacher to the student
network. Extensive experiments and ablative studies have been conducted,
demonstrating the effectiveness of modeling ASCK for early action prediction.
With the proposed ASCNet, we have achieved state-of-the-art performance on two
benchmarks. The code will be released if the paper is accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Minimize the Remainder in Supervised Learning. (arXiv:2201.09193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09193">
<div class="article-summary-box-inner">
<span><p>The learning process of deep learning methods usually updates the model's
parameters in multiple iterations. Each iteration can be viewed as the
first-order approximation of Taylor's series expansion. The remainder, which
consists of higher-order terms, is usually ignored in the learning process for
simplicity. This learning scheme empowers various multimedia based
applications, such as image retrieval, recommendation system, and video search.
Generally, multimedia data (e.g., images) are semantics-rich and
high-dimensional, hence the remainders of approximations are possibly non-zero.
In this work, we consider the remainder to be informative and study how it
affects the learning process. To this end, we propose a new learning approach,
namely gradient adjustment learning (GAL), to leverage the knowledge learned
from the past training iterations to adjust vanilla gradients, such that the
remainders are minimized and the approximations are improved. The proposed GAL
is model- and optimizer-agnostic, and is easy to adapt to the standard learning
framework. It is evaluated on three tasks, i.e., image classification, object
detection, and regression, with state-of-the-art models and optimizers. The
experiments show that the proposed GAL consistently enhances the evaluated
models, whereas the ablation studies validate various aspects of the proposed
GAL. The code is available at
\url{https://github.com/luoyan407/gradient_adjustment.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Predict Gradients for Semi-Supervised Continual Learning. (arXiv:2201.09196v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09196">
<div class="article-summary-box-inner">
<span><p>A key challenge for machine intelligence is to learn new visual concepts
without forgetting the previously acquired knowledge. Continual learning is
aimed towards addressing this challenge. However, there is a gap between
existing supervised continual learning and human-like intelligence, where human
is able to learn from both labeled and unlabeled data. How unlabeled data
affects learning and catastrophic forgetting in the continual learning process
remains unknown. To explore these issues, we formulate a new semi-supervised
continual learning method, which can be generically applied to existing
continual learning models. Specifically, a novel gradient learner learns from
labeled data to predict gradients on unlabeled data. Hence, the unlabeled data
could fit into the supervised continual learning method. Different from
conventional semi-supervised settings, we do not hypothesize that the
underlying classes, which are associated to the unlabeled data, are known to
the learning process. In other words, the unlabeled data could be very distinct
from the labeled data. We evaluate the proposed method on mainstream continual
learning, adversarial continual learning, and semi-supervised learning tasks.
The proposed method achieves state-of-the-art performance on classification
accuracy and backward transfer in the continual learning setting while
achieving desired performance on classification accuracy in the semi-supervised
learning setting. This implies that the unlabeled images can enhance the
generalizability of continual learning models on the predictive ability on
unseen data and significantly alleviate catastrophic forgetting. The code is
available at \url{https://github.com/luoyan407/grad_prediction.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Based UAV Localization System in Denial Environments. (arXiv:2201.09201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09201">
<div class="article-summary-box-inner">
<span><p>Unmanned Aerial Vehicle (UAV) localization capability is critical in a Global
Navigation Satellite System (GNSS) denial environment. The aim of this paper is
to investigate the problem of locating the UAV itself through a purely visual
approach. This task mainly refers to: matching the corresponding geo-tagged
satellite images through the images acquired by the camera when the UAV does
not acquire GNSS signals, where the satellite images are the bridge between the
UAV images and the location information. However, the sampling points of
previous cross-view datasets based on UAVs are discrete in spatial distribution
and the inter-class relationships are not established. In the actual process of
UAV-localization, the inter-class feature similarity of the proximity position
distribution should be small due to the continuity of UAV movement in space. In
view of this, this paper has reformulated an intensive dataset for UAV
positioning tasks, which is named DenseUAV, aiming to solve the problems caused
by spatial distance and scale transformation in practical application
scenarios, so as to achieve high-precision UAV-localization in GNSS denial
environment. In addition, a new continuum-type evaluation metric named SDM is
designed to evaluate the accuracy of model matching by exploiting the continuum
of UAVs in space. Specifically, with the ideas of siamese networks and metric
learning, a transformer-based baseline was constructed to enhance the capture
of spatially subtle features. Ultimately, a neighbor-search post-processing
strategy was proposed to solve the problem of large distance localisation bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deeply Explain CNN via Hierarchical Decomposition. (arXiv:2201.09205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09205">
<div class="article-summary-box-inner">
<span><p>In computer vision, some attribution methods for explaining CNNs attempt to
study how the intermediate features affect the network prediction. However,
they usually ignore the feature hierarchies among the intermediate features.
This paper introduces a hierarchical decomposition framework to explain CNN's
decision-making process in a top-down manner. Specifically, we propose a
gradient-based activation propagation (gAP) module that can decompose any
intermediate CNN decision to its lower layers and find the supporting features.
Then we utilize the gAP module to iteratively decompose the network decision to
the supporting evidence from different CNN layers. The proposed framework can
generate a deep hierarchy of strongly associated supporting evidence for the
network decision, which provides insight into the decision-making process.
Moreover, gAP is effort-free for understanding CNN-based models without network
architecture modification and extra training process. Experiments show the
effectiveness of the proposed method. The code and interactive demo website
will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization. (arXiv:2201.09206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09206">
<div class="article-summary-box-inner">
<span><p>Cross-view geo-localization is a task of matching the same geographic image
from different views, e.g., unmanned aerial vehicle (UAV) and satellite. The
most difficult challenges are the position shift and the uncertainty of
distance and scale. Existing methods are mainly aimed at digging for more
comprehensive fine-grained information. However, it underestimates the
importance of extracting robust feature representation and the impact of
feature alignment. The CNN-based methods have achieved great success in
cross-view geo-localization. However it still has some limitations, e.g., it
can only extract part of the information in the neighborhood and some scale
reduction operations will make some fine-grained information lost. In
particular, we introduce a simple and efficient transformer-based structure
called Feature Segmentation and Region Alignment (FSRA) to enhance the model's
ability to understand contextual information as well as to understand the
distribution of instances. Without using additional supervisory information,
FSRA divides regions based on the heat distribution of the transformer's
feature map, and then aligns multiple specific regions in different views one
on one. Finally, FSRA integrates each region into a set of feature
representations. The difference is that FSRA does not divide regions manually,
but automatically based on the heat distribution of the feature map. So that
specific instances can still be divided and aligned when there are significant
shifts and scale changes in the image. In addition, a multiple sampling
strategy is proposed to overcome the disparity in the number of satellite
images and that of images from other sources. Experiments show that the
proposed method has superior performance and achieves the state-of-the-art in
both tasks of drone view target localization and drone navigation. Code will be
released at https://github.com/Dmmm1997/FSRA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Object Tracking on Multi-modal RGB-D Videos: A Review. (arXiv:2201.09207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09207">
<div class="article-summary-box-inner">
<span><p>The development of visual object tracking has continued for decades. Recent
years, as the wide accessibility of the low-cost RGBD sensors, the task of
visual object tracking on RGB-D videos has drawn much attention. Compared to
conventional RGB-only tracking, the RGB-D videos can provide more information
that facilitates objecting tracking in some complicated scenarios. The goal of
this review is to summarize the relative knowledge of the research filed of
RGB-D tracking. To be specific, we will generalize the related RGB-D tracking
benchmarking datasets as well as the corresponding performance measurements.
Besides, the existing RGB-D tracking methods are summarized in the paper.
Moreover, we discuss the possible future direction in the field of RGB-D
tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design of Sensor Fusion Driver Assistance System for Active Pedestrian Safety. (arXiv:2201.09208v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09208">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a parallel architecture for a sensor fusion
detection system that combines a camera and 1D light detection and ranging
(lidar) sensor for object detection. The system contains two object detection
methods, one based on an optical flow, and the other using lidar. The two
sensors can effectively complement the defects of the other. The accurate
longitudinal accuracy of the object's location and its lateral movement
information can be achieved simultaneously. Using a spatio-temporal alignment
and a policy of sensor fusion, we completed the development of a fusion
detection system with high reliability at distances of up to 20 m. Test results
show that the proposed system achieves a high level of accuracy for pedestrian
or object detection in front of a vehicle, and has high robustness to special
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FN-Net:Remove the Outliers by Filtering the Noise. (arXiv:2201.09213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09213">
<div class="article-summary-box-inner">
<span><p>Establishing the correspondence between two images is an important research
direction of computer vision. When estimating the relationship between two
images, it is often disturbed by outliers. In this paper, we propose a
convolutional neural network that can filter the noise of outliers. It can
output the probability that the pair of feature points is an inlier and regress
the essential matrix representing the relative pose of the camera. The outliers
are mainly caused by the noise introduced by the previous processing. The
outliers rejection can be treated as a problem of noise elimination, and the
soft threshold function has a very good effect on noise reduction. Therefore,
we designed an adaptive denoising module based on soft threshold function to
remove noise components in the outliers, to reduce the probability that the
outlier is predicted to be an inlier. Experimental results on the YFCC100M
dataset show that our method exceeds the state-of-the-art in relative pose
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-Driven Lossy Image Compression; A Comprehensive Survey. (arXiv:2201.09240v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09240">
<div class="article-summary-box-inner">
<span><p>In the realm of image processing and computer vision (CV), machine learning
(ML) architectures are widely applied. Convolutional neural networks (CNNs)
solve a wide range of image processing issues and can solve image compression
problem. Compression of images is necessary due to bandwidth and memory
constraints. Helpful, redundant, and irrelevant information are three different
forms of information found in images. This paper aims to survey recent
techniques utilizing mostly lossy image compression using ML architectures
including different auto-encoders (AEs) such as convolutional auto-encoders
(CAEs), variational auto-encoders (VAEs), and AEs with hyper-prior models,
recurrent neural networks (RNNs), CNNs, generative adversarial networks (GANs),
principal component analysis (PCA) and fuzzy means clustering. We divide all of
the algorithms into several groups based on architecture. We cover still image
compression in this survey. Various discoveries for the researchers are
emphasized and possible future directions for researchers. The open research
problems such as out of memory (OOM), striped region distortion (SRD),
aliasing, and compatibility of the frameworks with central processing unit
(CPU) and graphics processing unit (GPU) simultaneously are explained. The
majority of the publications in the compression domain surveyed are from the
previous five years and use a variety of approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Increasing the Cost of Model Extraction with Calibrated Proof of Work. (arXiv:2201.09243v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09243">
<div class="article-summary-box-inner">
<span><p>In model extraction attacks, adversaries can steal a machine learning model
exposed via a public API by repeatedly querying it and adjusting their own
model based on obtained predictions. To prevent model stealing, existing
defenses focus on detecting malicious queries, truncating, or distorting
outputs, thus necessarily introducing a tradeoff between robustness and model
utility for legitimate users. Instead, we propose to impede model extraction by
requiring users to complete a proof-of-work before they can read the model's
predictions. This deters attackers by greatly increasing (even up to 100x) the
computational effort needed to leverage query access for model extraction.
Since we calibrate the effort required to complete the proof-of-work to each
query, this only introduces a slight overhead for regular users (up to 2x). To
achieve this, our calibration applies tools from differential privacy to
measure the information revealed by a query. Our method requires no
modification of the victim model and can be applied by machine learning
practitioners to guard their publicly exposed models against being easily
stolen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face recognition via compact second order image gradient orientations. (arXiv:2201.09246v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09246">
<div class="article-summary-box-inner">
<span><p>Conventional subspace learning approaches based on image gradient
orientations only employ the first-order gradient information. However, recent
researches on human vision system (HVS) uncover that the neural image is a
landscape or a surface whose geometric properties can be captured through the
second order gradient information. The second order image gradient orientations
(SOIGO) can mitigate the adverse effect of noises in face images. To reduce the
redundancy of SOIGO, we propose compact SOIGO (CSOIGO) by applying linear
complex principal component analysis (PCA) in SOIGO. Combined with
collaborative representation based classification (CRC) algorithm, the
classification performance of CSOIGO is further enhanced. CSOIGO is evaluated
under real-world disguise, synthesized occlusion and mixed variations.
Experimental results indicate that the proposed method is superior to its
competing approaches with few training samples, and even outperforms some
prevailing deep neural network based approaches. The source code of CSOIGO is
available at https://github.com/yinhefeng/SOIGO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral, Probabilistic, and Deep Metric Learning: Tutorial and Survey. (arXiv:2201.09267v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09267">
<div class="article-summary-box-inner">
<span><p>This is a tutorial and survey paper on metric learning. Algorithms are
divided into spectral, probabilistic, and deep metric learning. We first start
with the definition of distance metric, Mahalanobis distance, and generalized
Mahalanobis distance. In spectral methods, we start with methods using scatters
of data, including the first spectral metric learning, relevant methods to
Fisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant
Component Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric
learning, imbalanced metric learning, locally linear metric adaptation, and
adversarial metric learning are covered. We also explain several kernel
spectral methods for metric learning in the feature space. We also introduce
geometric metric learning methods on the Riemannian manifolds. In probabilistic
methods, we start with collapsing classes in both input and feature spaces and
then explain the neighborhood component analysis methods, Bayesian metric
learning, information theoretic methods, and empirical risk minimization in
metric learning. In deep learning methods, we first introduce reconstruction
autoencoders and supervised loss functions for metric learning. Then, Siamese
networks and its various loss functions, triplet mining, and triplet sampling
are explained. Deep discriminant analysis methods, based on Fisher discriminant
analysis, are also reviewed. Finally, we introduce multi-modal deep metric
learning, geometric metric learning by neural networks, and few-shot metric
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet-Attention CNN for Image Classification. (arXiv:2201.09271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09271">
<div class="article-summary-box-inner">
<span><p>The feature learning methods based on convolutional neural network (CNN) have
successfully produced tremendous achievements in image classification tasks.
However, the inherent noise and some other factors may weaken the effectiveness
of the convolutional feature statistics. In this paper, we investigate Discrete
Wavelet Transform (DWT) in the frequency domain and design a new
Wavelet-Attention (WA) block to only implement attention in the high-frequency
domain. Based on this, we propose a Wavelet-Attention convolutional neural
network (WA-CNN) for image classification. Specifically, WA-CNN decomposes the
feature maps into low-frequency and high-frequency components for storing the
structures of the basic objects, as well as the detailed information and noise,
respectively. Then, the WA block is leveraged to capture the detailed
information in the high-frequency domain with different attention factors but
reserves the basic object structures in the low-frequency domain. Experimental
results on CIFAR-10 and CIFAR-100 datasets show that our proposed WA-CNN
achieves significant improvements in classification accuracy compared to other
related networks. Specifically, based on MobileNetV2 backbones, WA-CNN achieves
1.26% Top-1 accuracy improvement on the CIFAR-10 benchmark and 1.54% Top-1
accuracy improvement on the CIFAR-100 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to scale hyperparameters for quickshift image segmentation. (arXiv:2201.09286v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09286">
<div class="article-summary-box-inner">
<span><p>Quickshift is a popular algorithm for image segmentation, used as a
preprocessing step in many applications. Unfortunately, it is quite challenging
to understand the hyperparameters' influence on the number and shape of
superpixels produced by the method. In this paper, we study theoretically a
slightly modified version of the quickshift algorithm, with a particular
emphasis on homogeneous image patches with i.i.d. pixel noise and sharp
boundaries between such patches. Leveraging this analysis, we derive a simple
heuristic to scale quickshift hyperparameters when dealing with real images,
which we check empirically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey for Deep RGBT Tracking. (arXiv:2201.09296v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09296">
<div class="article-summary-box-inner">
<span><p>Visual object tracking with the visible (RGB) and thermal infrared (TIR)
electromagnetic waves, shorted in RGBT tracking, recently draws increasing
attention in the tracking community. Considering the rapid development of deep
learning, a survey for the recent deep neural network based RGBT trackers is
presented in this paper. Firstly, we give brief introduction for the RGBT
trackers concluded into this category. Then, a comparison among the existing
RGBT trackers on several challenging benchmarks is given statistically.
Specifically, MDNet and Siamese architectures are the two mainstream frameworks
in the RGBT community, especially the former. Trackers based on MDNet achieve
higher performance while Siamese-based trackers satisfy the real-time
requirement. In summary, since the large-scale dataset LasHeR is published, the
integration of end-to-end framework, e.g., Siamese and Transformer, should be
further considered to fulfil the real-time as well as more robust performance.
Furthermore, the mathematical meaning should be more considered during
designing the network. This survey can be treated as a look-up-table for
researchers who are concerned about RGBT tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">1000x Faster Camera and Machine Vision with Ordinary Devices. (arXiv:2201.09302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09302">
<div class="article-summary-box-inner">
<span><p>In digital cameras, we find a major limitation: the image and video form
inherited from a film camera obstructs it from capturing the rapidly changing
photonic world. Here, we present vidar, a bit sequence array where each bit
represents whether the accumulation of photons has reached a threshold, to
record and reconstruct the scene radiance at any moment. By employing only
consumer-level CMOS sensors and integrated circuits, we have developed a vidar
camera that is 1,000x faster than conventional cameras. By treating vidar as
spike trains in biological vision, we have further developed a spiking neural
network-based machine vision system that combines the speed of the machine and
the mechanism of biological vision, achieving high-speed object detection and
tracking 1,000x faster than human vision. We demonstrate the utility of the
vidar camera and the super vision system in an assistant referee and target
pointing system. Our study is expected to fundamentally revolutionize the image
and video concepts and related industries, including photography, movies, and
visual media, and to unseal a new spiking neural network-enabled speed-free
machine vision era.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Basket-based Softmax. (arXiv:2201.09308v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09308">
<div class="article-summary-box-inner">
<span><p>Softmax-based losses have achieved state-of-the-art performances on various
tasks such as face recognition and re-identification. However, these methods
highly relied on clean datasets with global labels, which limits their usage in
many real-world applications. An important reason is that merging and
organizing datasets from various temporal and spatial scenarios is usually not
realistic, as noisy labels can be introduced and exponential-increasing
resources are required. To address this issue, we propose a novel
mining-during-training strategy called Basket-based Softmax (BBS) as well as
its parallel version to effectively train models on multiple datasets in an
end-to-end fashion. Specifically, for each training sample, we simultaneously
adopt similarity scores as the clue to mining negative classes from other
datasets, and dynamically add them to assist the learning of discriminative
features. Experimentally, we demonstrate the efficiency and superiority of the
BBS on the tasks of face recognition and re-identification, with both simulated
and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual cGAN for MRI Super-resolution. (arXiv:2201.09314v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09314">
<div class="article-summary-box-inner">
<span><p>Capturing high-resolution magnetic resonance (MR) images is a time consuming
process, which makes it unsuitable for medical emergencies and pediatric
patients. Low-resolution MR imaging, by contrast, is faster than its
high-resolution counterpart, but it compromises on fine details necessary for a
more precise diagnosis. Super-resolution (SR), when applied to low-resolution
MR images, can help increase their utility by synthetically generating
high-resolution images with little additional time. In this paper, we present a
SR technique for MR images that is based on generative adversarial networks
(GANs), which have proven to be quite useful in generating sharp-looking
details in SR. We introduce a conditional GAN with perceptual loss, which is
conditioned upon the input low-resolution image, which improves the performance
for isotropic and anisotropic MRI super-resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse-view Cone Beam CT Reconstruction using Data-consistent Supervised and Adversarial Learning from Scarce Training Data. (arXiv:2201.09318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09318">
<div class="article-summary-box-inner">
<span><p>Reconstruction of CT images from a limited set of projections through an
object is important in several applications ranging from medical imaging to
industrial settings. As the number of available projections decreases,
traditional reconstruction techniques such as the FDK algorithm and model-based
iterative reconstruction methods perform poorly. Recently, data-driven methods
such as deep learning-based reconstruction have garnered a lot of attention in
applications because they yield better performance when enough training data is
available. However, even these methods have their limitations when there is a
scarcity of available training data. This work focuses on image reconstruction
in such settings, i.e., when both the number of available CT projections and
the training data is extremely limited. We adopt a sequential reconstruction
approach over several stages using an adversarially trained shallow network for
'destreaking' followed by a data-consistency update in each stage. To deal with
the challenge of limited data, we use image subvolumes to train our method, and
patch aggregation during testing. To deal with the computational challenge of
learning on 3D datasets for 3D reconstruction, we use a hybrid 3D-to-2D mapping
network for the 'destreaking' part. Comparisons to other methods over several
test examples indicate that the proposed method has much potential, when both
the number of projections and available training data are highly limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out of Distribution Detection on ImageNet-O. (arXiv:2201.09352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09352">
<div class="article-summary-box-inner">
<span><p>Out of distribution (OOD) detection is a crucial part of making machine
learning systems robust. The ImageNet-O dataset is an important tool in testing
the robustness of ImageNet trained deep neural networks that are widely used
across a variety of systems and applications. We aim to perform a comparative
analysis of OOD detection methods on ImageNet-O, a first of its kind dataset
with a label distribution different than that of ImageNet, that has been
created to aid research in OOD detection for ImageNet models. As this dataset
is fairly new, we aim to provide a comprehensive benchmarking of some of the
current state of the art OOD detection methods on this novel dataset. This
benchmarking covers a variety of model architectures, settings where we haves
prior access to the OOD data versus when we don't, predictive score based
approaches, deep generative approaches to OOD detection, and more.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey and Systematization of 3D Object Detection Models and Methods. (arXiv:2201.09354v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09354">
<div class="article-summary-box-inner">
<span><p>This paper offers a comprehensive survey of recent developments in 3D object
detection covering the full pipeline from input data, over data representation
and feature extraction to the actual detection modules. We include basic
concepts, focus our survey on a broad spectrum of different approaches arising
in the last ten years and propose a systematization which offers a practical
framework to compare those approaches on the methods level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based SAR Image Despeckling. (arXiv:2201.09355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09355">
<div class="article-summary-box-inner">
<span><p>Synthetic Aperture Radar (SAR) images are usually degraded by a
multiplicative noise known as speckle which makes processing and interpretation
of SAR images difficult. In this paper, we introduce a transformer-based
network for SAR image despeckling. The proposed despeckling network comprises
of a transformer-based encoder which allows the network to learn global
dependencies between different image regions - aiding in better despeckling.
The network is trained end-to-end with synthetically generated speckled images
using a composite loss function. Experiments show that the proposed method
achieves significant improvements over traditional and convolutional neural
network-based despeckling methods on both synthetic and real SAR images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection. (arXiv:2201.09360v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09360">
<div class="article-summary-box-inner">
<span><p>A critical step in the fight against COVID-19, which continues to have a
catastrophic impact on peoples lives, is the effective screening of patients
presented in the clinics with severe COVID-19 symptoms. Chest radiography is
one of the promising screening approaches. Many studies reported detecting
COVID-19 in chest X-rays accurately using deep learning. A serious limitation
of many published approaches is insufficient attention paid to explaining
decisions made by deep learning models. Using explainable artificial
intelligence methods, we demonstrate that model decisions may rely on
confounding factors rather than medical pathology. After an analysis of
potential confounding factors found on chest X-ray images, we propose a novel
method to minimise their negative impact. We show that our proposed method is
more robust than previous attempts to counter confounding factors such as ECG
leads in chest X-rays that often influence model classification decisions. In
addition to being robust, our method achieves results comparable to the
state-of-the-art. The source code and pre-trained weights are publicly
available (https://github.com/tomek1911/POTHER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09367">
<div class="article-summary-box-inner">
<span><p>The freeform architectural modeling process often involves two important
stages: concept design and digital modeling. In the first stage, architects
usually sketch the overall 3D shape and the panel layout on a physical or
digital paper briefly. In the second stage, a digital 3D model is created using
the sketching as the reference. The digital model needs to incorporate
geometric requirements for its components, such as planarity of panels due to
consideration of construction costs, which can make the modeling process more
challenging. In this work, we present a novel sketch-based system to bridge the
concept design and digital modeling of freeform roof-like shapes represented as
planar quadrilateral (PQ) meshes. Our system allows the user to sketch the
surface boundary and contour lines under axonometric projection and supports
the sketching of occluded regions. In addition, the user can sketch feature
lines to provide directional guidance to the PQ mesh layout. Given the 2D
sketch input, we propose a deep neural network to infer in real-time the
underlying surface shape along with a dense conjugate direction field, both of
which are used to extract the final PQ mesh. To train and validate our network,
we generate a large synthetic dataset that mimics architect sketching of
freeform quadrilateral patches. The effectiveness and usability of our system
are demonstrated with quantitative and qualitative evaluation as well as user
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Severely Deformed Mesh Reconstruction (DMR) from a Single-View Image. (arXiv:2201.09373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09373">
<div class="article-summary-box-inner">
<span><p>Much progress has been made in the supervised learning of 3D reconstruction
of rigid objects from multi-view images or a video. However, it is more
challenging to reconstruct severely deformed objects from a single-view RGB
image in an unsupervised manner. Although training-based methods, such as
specific category-level training, have been shown to successfully reconstruct
rigid objects and slightly deformed objects like birds from a single-view
image, they cannot effectively handle severely deformed objects and neither can
be applied to some downstream tasks in the real world due to the inconsistent
semantic meaning of vertices, which are crucial in defining the adopted 3D
templates of objects to be reconstructed. In this work, we introduce a
template-based method to infer 3D shapes from a single-view image and apply the
reconstructed mesh to a downstream task, i.e., absolute length measurement.
Without using 3D ground truth, our method faithfully reconstructs 3D meshes and
achieves state-of-the-art accuracy in a length measurement task on a severely
deformed fish dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReconFormer: Accelerated MRI Reconstruction Using Recurrent Transformer. (arXiv:2201.09376v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09376">
<div class="article-summary-box-inner">
<span><p>Accelerating magnetic resonance image (MRI) reconstruction process is a
challenging ill-posed inverse problem due to the excessive under-sampling
operation in k-space. In this paper, we propose a recurrent transformer model,
namely \textbf{ReconFormer}, for MRI reconstruction which can iteratively
reconstruct high fertility magnetic resonance images from highly under-sampled
k-space data. In particular, the proposed architecture is built upon Recurrent
Pyramid Transformer Layers (RPTL), which jointly exploits intrinsic multi-scale
information at every architecture unit as well as the dependencies of the deep
feature correlation through recurrent states. Moreover, the proposed
ReconFormer is lightweight since it employs the recurrent structure for its
parameter efficiency. We validate the effectiveness of ReconFormer on multiple
datasets with different magnetic resonance sequences and show that it achieves
significant improvements over the state-of-the-art methods with better
parameter efficiency. Implementation code will be available in
https://github.com/guopengf/ReconFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">vCLIMB: A Novel Video Class Incremental Learning Benchmark. (arXiv:2201.09381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09381">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) is under-explored in the video domain. The few
existing works contain splits with imbalanced class distributions over the
tasks, or study the problem in unsuitable datasets. We introduce vCLIMB, a
novel video continual learning benchmark. vCLIMB is a standardized test-bed to
analyze catastrophic forgetting of deep models in video continual learning. In
contrast to previous work, we focus on class incremental continual learning
with models trained on a sequence of disjoint tasks, and distribute the number
of classes uniformly across the tasks. We perform in-depth evaluations of
existing CL methods in vCLIMB, and observe two unique challenges in video data.
The selection of instances to store in episodic memory is performed at the
frame level. Second, untrimmed training data influences the effectiveness of
frame sampling strategies. We address these two challenges by proposing a
temporal consistency regularization that can be applied on top of memory-based
continual learning methods. Our approach significantly improves the baseline,
by up to 24% on the untrimmed continual learning task. To streamline and foster
future research in video continual learning, we will publicly release the code
for our benchmark and method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey on Federated Learning: Concept and Applications. (arXiv:2201.09384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09384">
<div class="article-summary-box-inner">
<span><p>This paper provides a comprehensive study of Federated Learning (FL) with an
emphasis on components, challenges, applications and FL environment. FL can be
applicable in multiple fields and domains in real-life models. in the medical
system, the privacy of patients records and their medical condition is critical
data, therefore collaborative learning or federated learning comes into the
picture. On other hand build an intelligent system assist the medical staff
without sharing the data lead into the FL concept and one of the applications
that are used is a brain tumor diagnosis intelligent system based on AI methods
that can efficiently work in a collaborative environment.this paper will
introduce some of the applications and related work in the medical field and
work under the FL concept then summarize them to introduce the main limitations
of their work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Patients Privacy Protection with Stganography and Visual Encryption. (arXiv:2201.09388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09388">
<div class="article-summary-box-inner">
<span><p>In this survey, thirty models for steganography and visual encryption methods
have been discussed to provide patients privacy protection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks. (arXiv:2201.09390v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09390">
<div class="article-summary-box-inner">
<span><p>This work proposes an attention-based sequence-to-sequence model for
handwritten word recognition and explores transfer learning for data-efficient
training of HTR systems. To overcome training data scarcity, this work
leverages models pre-trained on scene text images as a starting point towards
tailoring the handwriting recognition models. ResNet feature extraction and
bidirectional LSTM-based sequence modeling stages together form an encoder. The
prediction stage consists of a decoder and a content-based attention mechanism.
The effectiveness of the proposed end-to-end HTR system has been empirically
evaluated on a novel multi-writer dataset Imgur5K and the IAM dataset. The
experimental results evaluate the performance of the HTR framework, further
supported by an in-depth analysis of the error cases. Source code and
pre-trained models are available at https://github.com/dmitrijsk/AttentionHTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISeval: a Metric Library for Medical Image Segmentation Evaluation. (arXiv:2201.09395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09395">
<div class="article-summary-box-inner">
<span><p>Correct performance assessment is crucial for evaluating modern artificial
intelligence algorithms in medicine like deep-learning based medical image
segmentation models. However, there is no universal metric library in Python
for standardized and reproducible evaluation. Thus, we propose our open-source
publicly available Python package MISeval: a metric library for Medical Image
Segmentation Evaluation. The implemented metrics can be intuitively used and
easily integrated into any performance assessment pipeline. The package
utilizes modern CI/CD strategies to ensure functionality and stability. MISeval
is available from PyPI (miseval) and GitHub:
https://github.com/frankkramer-lab/miseval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Label Assignment for Object Detection by Combining Predicted and Anchor IoUs. (arXiv:2201.09396v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09396">
<div class="article-summary-box-inner">
<span><p>Label assignment plays a significant role in modern object detection models.
Detection models may yield totally different performances with different label
assignment strategies. For anchor-based detection models, the IoU threshold
between the anchors and their corresponding ground truth bounding boxes is the
key element since the positive samples and negative samples are divided by the
IoU threshold. Early object detectors simply utilize a fixed threshold for all
training samples, while recent detection algorithms focus on adaptive
thresholds based on the distribution of the IoUs to the ground truth boxes. In
this paper, we introduce a simple and effective approach to perform label
assignment dynamically based on the training status with predictions. By
introducing the predictions in label assignment, more high-quality samples with
higher IoUs to the ground truth objects are selected as the positive samples,
which could reduce the discrepancy between the classification scores and the
IoU scores, and generate more high-quality boundary boxes. Our approach shows
improvements in the performance of the detection models with the adaptive label
assignment algorithm and lower bounding box losses for those positive samples,
indicating more samples with higher quality predicted boxes are selected as
positives. The source code will be available at
https://github.com/ZTX-100/DLA-Combined-IoUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09400">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) is a widely used non-radiative and
non-invasive method for clinical interrogation of organ structures and
metabolism, with an inherently long scanning time. Methods by k-space
undersampling and deep learning based reconstruction have been popularised to
accelerate the scanning process. This work focuses on investigating how
powerful transformers are for fast MRI by exploiting and comparing different
novel network architectures. In particular, a generative adversarial network
(GAN) based Swin transformer (ST-GAN) was introduced for the fast MRI
reconstruction. To further preserve the edge and texture information, edge
enhanced GAN based Swin transformer (EESGAN) and texture enhanced GAN based
Swin transformer (TES-GAN) were also developed, where a dual-discriminator GAN
structure was applied. We compared our proposed GAN based transformers,
standalone Swin transformer and other convolutional neural networks based based
GAN model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that
transformers work well for the MRI reconstruction from different undersampling
conditions. The utilisation of GAN's adversarial structure improves the quality
of images reconstructed when undersampled for 30% or higher.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Chest X-Ray Report Generation by Leveraging Warm-Starting. (arXiv:2201.09405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09405">
<div class="article-summary-box-inner">
<span><p>Automatically generating a report from a patient's Chest X-Rays (CXRs) is a
promising solution to reducing clinical workload and improving patient care.
However, current CXR report generators, which are predominantly
encoder-to-decoder models, lack the diagnostic accuracy to be deployed in a
clinical setting. To improve CXR report generation, we investigate
warm-starting the encoder and decoder with recent open-source computer vision
and natural language processing checkpoints, such as the Vision Transformer
(ViT) and PubMedBERT. To this end, each checkpoint is evaluated on the
MIMIC-CXR and IU X-Ray datasets using natural language generation and Clinical
Efficacy (CE) metrics. Our experimental investigation demonstrates that the
Convolutional vision Transformer (CvT) ImageNet-21K and the Distilled
Generative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for
warm-starting the encoder and decoder, respectively. Compared to the
state-of-the-art (M2 Transformer Progressive), CvT2DistilGPT2 attained an
improvement of 8.3% for CE F-1, 1.8% for BLEU-4, 1.6% for ROUGE-L, and 1.0% for
METEOR. The reports generated by CvT2DistilGPT2 are more diagnostically
accurate and have a higher similarity to radiologist reports than previous
approaches. By leveraging warm-starting, CvT2DistilGPT2 brings automatic CXR
report generation one step closer to the clinical setting. CvT2DistilGPT2 and
its MIMIC-CXR checkpoint are available at
https://github.com/aehrc/cvt2distilgpt2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Document Layout Analysis via Unsupervised Document Style Guide. (arXiv:2201.09407v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09407">
<div class="article-summary-box-inner">
<span><p>The document layout analysis (DLA) aims to decompose document images into
high-level semantic areas (i.e., figures, tables, texts, and background).
Creating a DLA framework with strong generalization capabilities is a challenge
due to document objects are diversity in layout, size, aspect ratio, texture,
etc. Many researchers devoted this challenge by synthesizing data to build
large training sets. However, the synthetic training data has different styles
and erratic quality. Besides, there is a large gap between the source data and
the target data. In this paper, we propose an unsupervised cross-domain DLA
framework based on document style guidance. We integrated the document quality
assessment and the document cross-domain analysis into a unified framework. Our
framework is composed of three components, Document Layout Generator (GLD),
Document Elements Decorator(GED), and Document Style Discriminator(DSD). The
GLD is used to document layout generates, the GED is used to document layout
elements fill, and the DSD is used to document quality assessment and
cross-domain guidance. First, we apply GLD to predict the positions of the
generated document. Then, we design a novel algorithm based on aesthetic
guidance to fill the document positions. Finally, we use contrastive learning
to evaluate the quality assessment of the document. Besides, we design a new
strategy to change the document quality assessment component into a document
cross-domain style guide component. Our framework is an unsupervised document
layout analysis framework. We have proved through numerous experiments that our
proposed method has achieved remarkable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Attention-based Hybrid Dimensional Network for Multimodal Imaging Computer-aided Diagnosis. (arXiv:2201.09421v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09421">
<div class="article-summary-box-inner">
<span><p>Recent works on Multimodal 3D Computer-aided diagnosis have demonstrated that
obtaining a competitive automatic diagnosis model when a 3D convolution neural
network (CNN) brings more parameters and medical images are scarce remains
nontrivial and challenging. Considering both consistencies of regions of
interest in multimodal images and diagnostic accuracy, we propose a novel
mutual attention-based hybrid dimensional network for MultiModal 3D medical
image classification (MMNet). The hybrid dimensional network integrates 2D CNN
with 3D convolution modules to generate deeper and more informative feature
maps, and reduce the training complexity of 3D fusion. Besides, the pre-trained
model of ImageNet can be used in 2D CNN, which improves the performance of the
model. The stereoscopic attention is focused on building rich contextual
interdependencies of the region in 3D medical images. To improve the regional
correlation of pathological tissues in multimodal medical images, we further
design a mutual attention framework in the network to build the region-wise
consistency in similar stereoscopic regions of different image modalities,
providing an implicit manner to instruct the network to focus on pathological
tissues. MMNet outperforms many previous solutions and achieves results
competitive to the state-of-the-art on three multimodal imaging datasets, i.e.,
Parotid Gland Tumor (PGT) dataset, the MRNet dataset, and the PROSTATEx
dataset, and its advantages are validated by extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniFormer: Unifying Convolution and Self-attention for Visual Recognition. (arXiv:2201.09450v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09450">
<div class="article-summary-box-inner">
<span><p>It is a challenging task to learn discriminative representation from images
and videos, due to large local redundancy and complex global dependency in
these visual data. Convolution neural networks (CNNs) and vision transformers
(ViTs) have been two dominant frameworks in the past few years. Though CNNs can
efficiently decrease local redundancy by convolution within a small
neighborhood, the limited receptive field makes it hard to capture global
dependency. Alternatively, ViTs can effectively capture long-range dependency
via self-attention, while blind similarity comparisons among all the tokens
lead to high redundancy. To resolve these problems, we propose a novel Unified
transFormer (UniFormer), which can seamlessly integrate the merits of
convolution and self-attention in a concise transformer format. Different from
the typical transformer blocks, the relation aggregators in our UniFormer block
are equipped with local and global token affinity respectively in shallow and
deep layers, allowing to tackle both redundancy and dependency for efficient
and effective representation learning. Finally, we flexibly stack our UniFormer
blocks into a new powerful backbone, and adopt it for various vision tasks from
image to video domain, from classification to dense prediction. Without any
extra training data, our UniFormer achieves 86.3 top-1 accuracy on ImageNet-1K
classification. With only ImageNet-1K pre-training, it can simply achieve
state-of-the-art performance in a broad range of downstream tasks, e.g., it
obtains 82.9/84.8 top-1 accuracy on Kinetics-400/600, 60.9/71.2 top-1 accuracy
on Something-Something V1/V2 video classification tasks, 53.8 box AP and 46.4
mask AP on COCO object detection task, 50.8 mIoU on ADE20K semantic
segmentation task, and 77.4 AP on COCO pose estimation task. Code is available
at https://github.com/Sense-X/UniFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyber Mobility Mirror for Enabling Cooperative Driving Automation: A Co-Simulation Platform. (arXiv:2201.09463v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09463">
<div class="article-summary-box-inner">
<span><p>Endowed with automation and connectivity, Connected and Automated Vehicles
(CAVs) are meant to be a revolutionary promoter for Cooperative Driving
Automation (CDA). Nevertheless, CAVs need high-fidelity perception information
on their surroundings, which is available but costly to collect from various
on-board sensors, such as radar, camera, and LiDAR, as well as
vehicle-to-everything (V2X) communications. Therefore, precisely simulating the
sensing process with high-fidelity sensor inputs and timely retrieving the
perception information via a cost-effective platform are of increasing
significance for enabling CDA-related research, e.g., development of
decision-making or control module. Most state-of-the-art traffic simulation
studies for CAVs rely on the situation-awareness information by directly
calling on intrinsic attributes of the objects, which impedes the reliability
and fidelity for testing and validation of CDA algorithms. In this study, a
co-simulation platform is developed, which can simulate both the real world
with a high-fidelity sensor perception system and the cyber world (or "mirror"
world) with a real-time 3D reconstruction system. Specifically, the real-world
simulator is mainly in charge of simulating the road-users (such as vehicles,
bicyclists, and pedestrians), infrastructure (e.g., traffic signals and
roadside sensors) as well as the object detection process. The mirror-world
simulator is responsible for reconstructing 3D objects and their trajectories
from the perceived information (provided by those roadside sensors in the
real-world simulator) to support the development and evaluation of CDA
algorithms. To illustrate the efficacy of this co-simulation platform, a
roadside LiDAR-based real-time vehicle detection and 3D reconstruction system
is prototyped as a study case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forgery Attack Detection in Surveillance Video Streams Using Wi-Fi Channel State Information. (arXiv:2201.09487v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09487">
<div class="article-summary-box-inner">
<span><p>The cybersecurity breaches expose surveillance video streams to forgery
attacks, under which authentic streams are falsified to hide unauthorized
activities. Traditional video forensics approaches can localize forgery traces
using spatial-temporal analysis on relatively long video clips, while falling
short in real-time forgery detection. The recent work correlates time-series
camera and wireless signals to detect looped videos but cannot realize
fine-grained forgery localization. To overcome these limitations, we propose
Secure-Pose, which exploits the pervasive coexistence of surveillance and Wi-Fi
infrastructures to defend against video forgery attacks in a real-time and
fine-grained manner. We observe that coexisting camera and Wi-Fi signals convey
common human semantic information and forgery attacks on video streams will
decouple such information correspondence. Particularly, retrievable human pose
features are first extracted from concurrent video and Wi-Fi channel state
information (CSI) streams. Then, a lightweight detection network is developed
to accurately discover forgery attacks and an efficient localization algorithm
is devised to seamlessly track forgery traces in video streams. We implement
Secure-Pose using one Logitech camera and two Intel 5300 NICs and evaluate it
in different environments. Secure-Pose achieves a high detection accuracy of
98.7% and localizes abnormal objects under playback and tampering attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerated Intravascular Ultrasound Imaging using Deep Reinforcement Learning. (arXiv:2201.09522v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09522">
<div class="article-summary-box-inner">
<span><p>Intravascular ultrasound (IVUS) offers a unique perspective in the treatment
of vascular diseases by creating a sequence of ultrasound-slices acquired from
within the vessel. However, unlike conventional hand-held ultrasound, the thin
catheter only provides room for a small number of physical channels for signal
transfer from a transducer-array at the tip. For continued improvement of image
quality and frame rate, we present the use of deep reinforcement learning to
deal with the current physical information bottleneck. Valuable inspiration has
come from the field of magnetic resonance imaging (MRI), where learned
acquisition schemes have brought significant acceleration in image acquisition
at competing image quality. To efficiently accelerate IVUS imaging, we propose
a framework that utilizes deep reinforcement learning for an optimal adaptive
acquisition policy on a per-frame basis enabled by actor-critic methods and
Gumbel top-$K$ sampling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent 3D Hand Reconstruction in Video via self-supervised Learning. (arXiv:2201.09548v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09548">
<div class="article-summary-box-inner">
<span><p>We present a method for reconstructing accurate and consistent 3D hands from
a monocular video. We observe that detected 2D hand keypoints and the image
texture provide important cues about the geometry and texture of the 3D hand,
which can reduce or even eliminate the requirement on 3D hand annotation. Thus
we propose ${\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model,
that can jointly estimate pose, shape, texture, and the camera viewpoint from a
single RGB input through the supervision of easily accessible 2D detected
keypoints. We leverage the continuous hand motion information contained in the
unlabeled video data and propose ${\rm {S}^{2}HAND(V)}$, which uses a set of
weights shared ${\rm {S}^{2}HAND}$ to process each frame and exploits
additional motion, texture, and shape consistency constrains to promote more
accurate hand poses and more consistent shapes and textures. Experiments on
benchmark datasets demonstrate that our self-supervised approach produces
comparable hand reconstruction performance compared with the recent
full-supervised methods in single-frame as input setup, and notably improves
the reconstruction accuracy and consistency when using video training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Average Biased ReLU Based CNN Descriptor for Improved Face Retrieval. (arXiv:1804.02051v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.02051">
<div class="article-summary-box-inner">
<span><p>The convolutional neural networks (CNN), including AlexNet, GoogleNet,
VGGNet, etc. extract features for many computer vision problems which are very
discriminative. The trained CNN model over one dataset performs reasonably well
whereas on another dataset of similar type the hand-designed feature descriptor
outperforms the same trained CNN model. The Rectified Linear Unit (ReLU) layer
discards some values in order to introduce the non-linearity. In this paper, it
is proposed that the discriminative ability of deep image representation using
trained model can be improved by Average Biased ReLU (AB-ReLU) at the last few
layers. Basically, AB-ReLU improves the discriminative ability in two ways: 1)
it exploits some of the discriminative and discarded negative information of
ReLU and 2) it also neglects the irrelevant and positive information used in
ReLU. The VGGFace model trained in MatConvNet over the VGG-Face dataset is used
as the feature descriptor for face retrieval over other face datasets. The
proposed approach is tested over six challenging, unconstrained and robust face
datasets (PubFig, LFW, PaSC, AR, FERET and ExtYale) and also on a large scale
face dataset (PolyUNIR) in retrieval framework. It is observed that the AB-ReLU
outperforms the ReLU when used with a pre-trained VGGFace model over the face
datasets. The validation error by training the network after replacing all
ReLUs with AB-ReLUs is also observed to be favorable over each dataset. The
AB-ReLU even outperforms the state-of-the-art activation functions, such as
Sigmoid, ReLU, Leaky ReLU and Flexible ReLU over all seven face datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grassmannian Discriminant Maps (GDM) for Manifold Dimensionality Reduction with Application to Image Set Classification. (arXiv:1806.10830v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1806.10830">
<div class="article-summary-box-inner">
<span><p>In image set classification, a considerable progress has been made by
representing original image sets on Grassmann manifolds. In order to extend the
advantages of the Euclidean based dimensionality reduction methods to the
Grassmann Manifold, several methods have been suggested recently which jointly
perform dimensionality reduction and metric learning on Grassmann manifold to
improve performance. Nevertheless, when applied to complex datasets, the
learned features do not exhibit enough discriminatory power. To overcome this
problem, we propose a new method named Grassmannian Discriminant Maps (GDM) for
manifold dimensionality reduction problems. The core of the method is a new
discriminant function for metric learning and dimensionality reduction. For
comparison and better understanding, we also study a simple variations to GDM.
The key difference between them is the discriminant function. We experiment on
data sets corresponding to three tasks: face recognition, object
categorization, and hand gesture recognition to evaluate the proposed method
and its simple extensions. Compared with the state of the art, the results
achieved show the effectiveness of the proposed algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Parsing in Videos: A Multi-modal Appraoch. (arXiv:1903.02252v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.02252">
<div class="article-summary-box-inner">
<span><p>Text-level discourse parsing aims to unmask how two sentences in the text are
related to each other. We propose the task of Visual Discourse Parsing, which
requires understanding discourse relations among scenes in a video. Here we use
the term scene to refer to a subset of video frames that can better summarize
the video. In order to collect a dataset for learning discourse cues from
videos, one needs to manually identify the scenes from a large pool of video
frames and then annotate the discourse relations between them. This is clearly
a time consuming, expensive and tedious task. In this work, we propose an
approach to identify discourse cues from the videos without the need to
explicitly identify and annotate the scenes. We also present a novel dataset
containing 310 videos and the corresponding discourse cues to evaluate our
approach. We believe that many of the multi-discipline AI problems such as
Visual Dialog and Visual Storytelling would greatly benefit from the use of
visual discourse cues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Localization of Mixed Image Tampering Techniques. (arXiv:1904.08484v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.08484">
<div class="article-summary-box-inner">
<span><p>With technological advances leading to an increase in mechanisms for image
tampering, fraud detection methods must continue to be upgraded to match their
sophistication. One problem with current methods is that they require prior
knowledge of the method of forgery in order to determine which features to
extract from the image to localize the region of interest. When a machine
learning algorithm is used to learn different types of tampering from a large
set of various image types, with a large enough database we can easily classify
which images are tampered. However, we still are left with the question of
which features to train on, and how to localize the manipulation. In this work,
deep learning for object detection is adapted to tampering detection to solve
these two problems, while fusing features from multiple classic techniques for
improved accuracy. A Multi-stream version of the Faster RCNN network will be
employed with the second stream having an input of the element-wise sum of the
ELA and BAG error maps to provide even higher accuracy than a single stream
alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey of Object Classification and Detection based on 2D/3D data. (arXiv:1905.12683v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.12683">
<div class="article-summary-box-inner">
<span><p>Recently, by using deep neural network based algorithms, object
classification, detection and semantic segmentation solutions are significantly
improved. However, one challenge for 2D image-based systems is that they cannot
provide accurate 3D location information. This is critical for location
sensitive applications such as autonomous driving and robot navigation. On the
other hand, 3D methods, such as RGB-D and RGB-LiDAR based systems, can provide
solutions that significantly improve the RGB only approaches. That is why this
is an interesting research area for both industry and academia. Compared with
2D image-based systems, 3D-based systems are more complicated due to the
following five reasons: 1) Data representation itself is more complicated. 3D
images can be represented by point clouds, meshes, volumes. 2D images have
pixel grid representations. 2) The computation and memory resource requirement
is higher as an extra dimension is added. 3) Different distribution of the
objects and difference in scene areas between indoor and outdoor make one
unified framework hard to achieve. 4) 3D data, especially for the outdoor
scenario, is sparse compared with the dense 2D images which makes the detection
task more challenging. Finally, large size labelled datasets, which are
extremely important for supervised based algorithms, are still under
construction compared with well-built 2D datasets such as ImageNet. Based on
challenges listed above, the described systems are organized by application
scenarios, data representation methods and main tasks addressed. At the same
time, critical 2D based systems which greatly influence the 3D ones are also
introduced to show the connection between them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Parametric Activation Functions. (arXiv:2006.03179v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03179">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that the choice of activation function can
significantly affect the performance of deep learning networks. However, the
benefits of novel activation functions have been inconsistent and task
dependent, and therefore the rectified linear unit (ReLU) is still the most
commonly used. This paper proposes a technique for customizing activation
functions automatically, resulting in reliable improvements in performance.
Evolutionary search is used to discover the general form of the function, and
gradient descent to optimize its parameters for different parts of the network
and over the learning process. Experiments with four different neural network
architectures on the CIFAR-10 and CIFAR-100 image classification datasets show
that this approach is effective. It discovers both general activation functions
and specialized functions for different architectures, consistently improving
accuracy over ReLU and other activation functions by significant margins. The
approach can therefore be used as an automated optimization step in applying
deep learning to new tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binary Neural Networks for Memory-Efficient and Effective Visual Place Recognition in Changing Environments. (arXiv:2010.00716v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00716">
<div class="article-summary-box-inner">
<span><p>Visual place recognition (VPR) is a robot's ability to determine whether a
place was visited before using visual data. While conventional hand-crafted
methods for VPR fail under extreme environmental appearance changes, those
based on convolutional neural networks (CNNs) achieve state-of-the-art
performance but result in heavy runtime processes and model sizes that demand a
large amount of memory. Hence, CNN-based approaches are unsuitable for
resource-constrained platforms, such as small robots and drones. In this paper,
we take a multi-step approach of decreasing the precision of model parameters,
combining it with network depth reduction and fewer neurons in the classifier
stage to propose a new class of highly compact models that drastically reduces
the memory requirements and computational effort while maintaining
state-of-the-art VPR performance. To the best of our knowledge, this is the
first attempt to propose binary neural networks for solving the visual place
recognition problem effectively under changing conditions and with
significantly reduced resource requirements. Our best-performing binary neural
network, dubbed FloppyNet, achieves comparable VPR performance when considered
against its full-precision and deeper counterparts while consuming 99% less
memory and increasing the inference speed seven times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Local Robust Quaternion Matrix Completion for Large-Scale Color Images and Videos Inpainting. (arXiv:2011.08675v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08675">
<div class="article-summary-box-inner">
<span><p>The image nonlocal self-similarity (NSS) prior refers to the fact that a
local patch often has many nonlocal similar patches to it across the image and
has been widely applied in many recently proposed machining learning algorithms
for image processing. However, there is no theoretical analysis on its working
principle in the literature. In this paper, we discover a potential causality
between NSS and low-rank property of color images, which is also available to
grey images. A new patch group based NSS prior learning scheme is proposed to
learn explicit NSS models of natural color images. The numerical low-rank
property of patched matrices is also rigorously proved. The NSS-based QMC
algorithm computes an optimal low-rank approximation to the high-rank color
image, resulting in high PSNR and SSIM measures and particularly the better
visual quality. A new tensor NSS-based QMC method is also presented to solve
the color video inpainting problem based on quaternion tensor representation.
The numerical experiments on large-scale color images and videos indicate the
advantages of NSS-based QMC over the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular 3D Object Detection with Sequential Feature Association and Depth Hint Augmentation. (arXiv:2011.14589v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14589">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection, with the aim of predicting the geometric
properties of on-road objects, is a promising research topic for the
intelligent perception systems of autonomous driving. Most state-of-the-art
methods follow a keypoint-based paradigm, where the keypoints of objects are
predicted and employed as the basis for regressing the other geometric
properties. In this work, a unified network named as FADNet is presented to
address the task of monocular 3D object detection. In contrast to previous
keypoint-based methods, we propose to divide the output modalities into
different groups according to the estimation difficulty of object properties.
Different groups are treated differently and sequentially associated by a
convolutional Gated Recurrent Unit. Another contribution of this work is the
strategy of depth hint augmentation. To provide characterized depth patterns as
hints for depth estimation, a dedicated depth hint module is designed to
generate row-wise features named as depth hints, which are explicitly
supervised in a bin-wise manner. The contributions of this work are validated
by conducting experiments and ablation study on the KITTI benchmark. Without
utilizing depth priors, post optimization, or other refinement modules, our
network performs competitively against state-of-the-art methods while
maintaining a decent running speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CM-Net: Concentric Mask based Arbitrary-Shaped Text Detection. (arXiv:2011.14714v9 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14714">
<div class="article-summary-box-inner">
<span><p>Recently fast arbitrary-shaped text detection has become an attractive
research topic. However, most existing methods are non-real-time, which may
fall short in intelligent systems. Although a few real-time text methods are
proposed, the detection accuracy is far behind non-real-time methods. To
improve the detection accuracy and speed simultaneously, we propose a novel
fast and accurate text detection framework, namely CM-Net, which is constructed
based on a new text representation method and a multi-perspective feature (MPF)
module. The former can fit arbitrary-shaped text contours by concentric mask
(CM) in an efficient and robust way. The latter encourages the network to learn
more CM-related discriminative features from multiple perspectives and brings
no extra computational cost. Benefiting the advantages of CM and MPF, the
proposed CM-Net only needs to predict one CM of the text instance to rebuild
the text contour and achieves the best balance between detection accuracy and
speed compared with previous works. Moreover, to ensure that multi-perspective
features are effectively learned, the multi-factor constraints loss is
proposed. Extensive experiments demonstrate the proposed CM is efficient and
robust to fit arbitrary-shaped text instances, and also validate the
effectiveness of MPF and constraints loss for discriminative text features
recognition. Furthermore, experimental results show that the proposed CM-Net is
superior to existing state-of-the-art (SOTA) real-time text detection methods
in both detection speed and accuracy on MSRA-TD500, CTW1500, Total-Text, and
ICDAR2015 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Iterative Neural Network for High-Quality Image-Domain Material Decomposition in Dual-Energy CT. (arXiv:2012.01986v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01986">
<div class="article-summary-box-inner">
<span><p>Dual-energy computed tomography (DECT) has been widely used in many
applications that need material decomposition. Image-domain methods directly
decompose material images from high- and low-energy attenuation images, and
thus, are susceptible to noise and artifacts on attenuation images. The purpose
of this study is to develop an improved iterative neural network (INN) for
high-quality image-domain material decomposition in DECT, and to study its
properties. We propose a new INN architecture for DECT material decomposition.
The proposed INN architecture uses distinct cross-material convolutional neural
network (CNN) in image refining modules, and uses image decomposition physics
in image reconstruction modules. The distinct cross-material CNN refiners
incorporate distinct encoding-decoding filters and cross-material model that
captures correlations between different materials. We study the distinct
cross-material CNN refiner with patch-based reformulation and tight-frame
condition. Numerical experiments with extended cardiactorso (XCAT) phantom and
clinical data show that the proposed INN significantly improves the image
quality over several image-domain material decomposition methods, including a
conventional model-based image decomposition (MBID) method using an
edge-preserving regularizer, a recent MBID method using pre-learned
material-wise sparsifying transforms, and a noniterative deep CNN method. Our
study with patch-based reformulations reveals that learned filters of distinct
cross-material CNN refiners can approximately satisfy the tight-frame
condition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MERANet: Facial Micro-Expression Recognition using 3D Residual Attention Network. (arXiv:2012.04581v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04581">
<div class="article-summary-box-inner">
<span><p>Micro-expression has emerged as a promising modality in affective computing
due to its high objectivity in emotion detection. Despite the higher
recognition accuracy provided by the deep learning models, there are still
significant scope for improvements in micro-expression recognition techniques.
The presence of micro-expressions in small-local regions of the face, as well
as the limited size of available databases, continue to limit the accuracy in
recognizing micro-expressions. In this work, we propose a facial
micro-expression recognition model using 3D residual attention network named
MERANet to tackle such challenges. The proposed model takes advantage of
spatial-temporal attention and channel attention together, to learn deeper
fine-grained subtle features for classification of emotions. Further, the
proposed model encompasses both spatial and temporal information simultaneously
using the 3D kernels and residual connections. Moreover, the channel features
and spatio-temporal features are re-calibrated using the channel and
spatio-temporal attentions, respectively in each residual module. Our attention
mechanism enables the model to learn to focus on different facial areas of
interest. The experiments are conducted on benchmark facial micro-expression
datasets. A superior performance is observed as compared to the
state-of-the-art for facial micro-expression recognition on benchmark data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-Based Human Pose Estimation: A Survey. (arXiv:2012.13392v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13392">
<div class="article-summary-box-inner">
<span><p>Human pose estimation aims to locate the human body parts and build human
body representation (e.g., body skeleton) from input data such as images and
videos. It has drawn increasing attention during the past decade and has been
utilized in a wide range of applications including human-computer interaction,
motion analysis, augmented reality, and virtual reality. Although the recently
developed deep learning-based solutions have achieved high performance in human
pose estimation, there still remain challenges due to insufficient training
data, depth ambiguities, and occlusion. The goal of this survey paper is to
provide a comprehensive review of recent deep learning-based solutions for both
2D and 3D pose estimation via a systematic analysis and comparison of these
solutions based on their input data and inference procedures. More than 250
research papers since 2014 are covered in this survey. Furthermore, 2D and 3D
human pose estimation datasets and evaluation metrics are included.
Quantitative performance comparisons of the reviewed methods on popular
datasets are summarized and discussed. Finally, the challenges involved,
applications, and future research directions are concluded. A regularly updated
project page is provided: \url{https://github.com/zczcwh/DL-HPE}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Image Super-Resolution via Neural Differential Equation. (arXiv:2101.08987v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08987">
<div class="article-summary-box-inner">
<span><p>We propose a new approach for the image super-resolution (SR) task that
progressively restores a high-resolution (HR) image from an input
low-resolution (LR) image on the basis of a neural ordinary differential
equation. In particular, we newly formulate the SR problem as an initial value
problem, where the initial value is the input LR image. Unlike conventional
progressive SR methods that perform gradual updates using straightforward
iterative mechanisms, our SR process is formulated in a concrete manner based
on explicit modeling with a much clearer understanding. Our method can be
easily implemented using conventional neural networks for image restoration.
Moreover, the proposed method can super-resolve an image with arbitrary scale
factors on continuous domain, and achieves superior SR performance over
state-of-the-art SR methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs. (arXiv:2102.02828v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02828">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) constructed natively on the sphere have
been developed recently and shown to be highly effective for the analysis of
spherical data. While an efficient framework has been formulated, spherical
CNNs are nevertheless highly computationally demanding; typically they cannot
scale beyond spherical signals of thousands of pixels. We develop scattering
networks constructed natively on the sphere that provide a powerful
representational space for spherical data. Spherical scattering networks are
computationally scalable and exhibit rotational equivariance, while their
representational space is invariant to isometries and provides efficient and
stable signal representations. By integrating scattering networks as an
additional type of layer in the generalized spherical CNN framework, we show
how they can be leveraged to scale spherical CNNs to the high-resolution data
typical of many practical applications, with spherical signals of many tens of
megapixels and beyond.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Multisensor Change Detection. (arXiv:2103.05102v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05102">
<div class="article-summary-box-inner">
<span><p>Most change detection methods assume that pre-change and post-change images
are acquired by the same sensor. However, in many real-life scenarios, e.g.,
natural disaster, it is more practical to use the latest available images
before and after the occurrence of incidence, which may be acquired using
different sensors. In particular, we are interested in the combination of the
images acquired by optical and Synthetic Aperture Radar (SAR) sensors. SAR
images appear vastly different from the optical images even when capturing the
same scene. Adding to this, change detection methods are often constrained to
use only target image-pair, no labeled data, and no additional unlabeled data.
Such constraints limit the scope of traditional supervised machine learning and
unsupervised generative approaches for multi-sensor change detection. Recent
rapid development of self-supervised learning methods has shown that some of
them can even work with only few images. Motivated by this, in this work we
propose a method for multi-sensor change detection using only the unlabeled
target bi-temporal images that are used for training a network in
self-supervised fashion by using deep clustering and contrastive learning. The
proposed method is evaluated on four multi-modal bi-temporal scenes showing
change and the benefits of our self-supervised approach are demonstrated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Contrastive Optimization of Siamese Networks for Place Recognition. (arXiv:2103.06638v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06638">
<div class="article-summary-box-inner">
<span><p>Visual place recognition is a challenging task in computer vision and a key
component of camera-based localization and navigation systems. Recently,
Convolutional Neural Networks (CNNs) achieved high results and good
generalization capabilities. They are usually trained using pairs or triplets
of images labeled as either similar or dissimilar, in a binary fashion. In
practice, the similarity between two images is not binary, but continuous.
Furthermore, training these CNNs is computationally complex and involves costly
pair and triplet mining strategies.
</p>
<p>We propose a Generalized Contrastive loss (GCL) function that relies on image
similarity as a continuous measure, and use it to train a siamese CNN.
Furthermore, we present three techniques for automatic annotation of image
pairs with labels indicating their degree of similarity, and deploy them to
re-annotate the MSLS, TB-Places, and 7Scenes datasets.
</p>
<p>We demonstrate that siamese CNNs trained using the GCL function and the
improved annotations consistently outperform their binary counterparts. Our
models trained on MSLS outperform the state-of-the-art methods, including
NetVLAD, NetVLAD-SARE, AP-GeM and Patch-NetVLAD, and generalize well on the
Pittsburgh30k, Tokyo 24/7, RobotCar Seasons v2 and Extended CMU Seasons
datasets. Furthermore, training a siamese network using the GCL function does
not require complex pair mining. We release the source code at
https://github.com/marialeyvallina/generalized_contrastive_loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Gaussian Noise Consistency Regularization for Robustness and Uncertainty Calibration under Noise Domain Shifts. (arXiv:2104.01231v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01231">
<div class="article-summary-box-inner">
<span><p>Deep neural networks achieve high prediction accuracy when the train and test
distributions coincide. In practice though, various types of corruptions occur
which deviate from this setup and cause severe performance degradations. Few
methods have been proposed to address generalization in the presence of
unforeseen domain shifts. In particular, digital noise corruptions arise
commonly in practice during the image acquisition stage and present a
significant challenge for current robustness approaches. In this paper, we
propose a diverse Gaussian noise consistency regularization method for
improving robustness of image classifiers under a variety of noise corruptions
while still maintaining high clean accuracy. We derive bounds to motivate our
Gaussian noise consistency regularization using a local loss landscape
analysis. We show that this simple approach improves robustness against various
unforeseen noise corruptions over standard and adversarial training and other
strong baselines. Furthermore, when combined with diverse data augmentation
techniques we empirically show this type of consistency regularization further
improves robustness and uncertainty calibration for common corruptions upon the
state-of-the-art for several image classification benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary. (arXiv:2104.14631v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14631">
<div class="article-summary-box-inner">
<span><p>With the advance of deep learning technology, automatic video generation from
audio or text has become an emerging and promising research topic. In this
paper, we present a novel approach to synthesize video from the text. The
method builds a phoneme-pose dictionary and trains a generative adversarial
network (GAN) to generate video from interpolated phoneme poses. Compared to
audio-driven video generation algorithms, our approach has a number of
advantages: 1) It only needs a fraction of the training data used by an
audio-driven approach; 2) It is more flexible and not subject to vulnerability
due to speaker variation; 3) It significantly reduces the preprocessing,
training and inference time. We perform extensive experiments to compare the
proposed method with state-of-the-art talking face generation methods on a
benchmark dataset and datasets of our own. The results demonstrate the
effectiveness and superiority of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Agent Semi-Siamese Training for Long-tail and Shallow Face Learning. (arXiv:2105.04113v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04113">
<div class="article-summary-box-inner">
<span><p>With the recent development of deep convolutional neural networks and
large-scale datasets, deep face recognition has made remarkable progress and
been widely used in various applications. However, unlike the existing public
face datasets, in many real-world scenarios of face recognition, the depth of
training dataset is shallow, which means only two face images are available for
each ID. With the non-uniform increase of samples, such issue is converted to a
more general case, a.k.a long-tail face learning, which suffers from data
imbalance and intra-class diversity dearth simultaneously. These adverse
conditions damage the training and result in the decline of model performance.
Based on the Semi-Siamese Training (SST), we introduce an advanced solution,
named Multi-Agent Semi-Siamese Training (MASST), to address these problems.
MASST includes a probe network and multiple gallery agents, the former aims to
encode the probe features, and the latter constitutes a stack of networks that
encode the prototypes (gallery features). For each training iteration, the
gallery network, which is sequentially rotated from the stack, and the probe
network form a pair of semi-siamese networks. We give the theoretical and
empirical analysis that, given the long-tail (or shallow) data and training
loss, MASST smooths the loss landscape and satisfies the Lipschitz continuity
with the help of multiple agents and the updating gallery queue. The proposed
method is out of extra-dependency, thus can be easily integrated with the
existing loss functions and network architectures. It is worth noting that,
although multiple gallery agents are employed for training, only the probe
network is needed for inference, without increasing the inference cost.
Extensive experiments and comparisons demonstrate the advantages of MASST for
long-tail and shallow face learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forensic Analysis of Video Files Using Metadata. (arXiv:2105.06361v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06361">
<div class="article-summary-box-inner">
<span><p>The unprecedented ease and ability to manipulate video content has led to a
rapid spread of manipulated media. The availability of video editing tools
greatly increased in recent years, allowing one to easily generate
photo-realistic alterations. Such manipulations can leave traces in the
metadata embedded in video files. This metadata information can be used to
determine video manipulations, brand of video recording device, the type of
video editing tool, and other important evidence. In this paper, we focus on
the metadata contained in the popular MP4 video wrapper/container. We describe
our method for metadata extractor that uses the MP4's tree structure. Our
approach for analyzing the video metadata produces a more compact
representation. We will describe how we construct features from the metadata
and then use dimensionality reduction and nearest neighbor classification for
forensic analysis of a video file. Our approach allows one to visually inspect
the distribution of metadata features and make decisions. The experimental
results confirm that the performance of our approach surpasses other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AC-CovidNet: Attention Guided Contrastive CNN for Recognition of Covid-19 in Chest X-Ray Images. (arXiv:2105.10239v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10239">
<div class="article-summary-box-inner">
<span><p>Covid-19 global pandemic continues to devastate health care systems across
the world. At present, the Covid-19 testing is costly and time-consuming. Chest
X-Ray (CXR) testing can be a fast, scalable, and non-invasive method. The
existing methods suffer due to the limited CXR samples available from Covid-19.
Thus, inspired by the limitations of the open-source work in this field, we
propose attention guided contrastive CNN architecture (AC-CovidNet) for
Covid-19 detection in CXR images. The proposed method learns the robust and
discriminative features with the help of contrastive loss. Moreover, the
proposed method gives more importance to the infected regions as guided by the
attention mechanism. We compute the sensitivity of the proposed method over the
publicly available Covid-19 dataset. It is observed that the proposed
AC-CovidNet exhibits very promising performance as compared to the existing
methods even with limited training data. It can tackle the bottleneck of CXR
Covid-19 datasets being faced by the researchers. The code used in this paper
is released publicly at \url{https://github.com/shivram1987/AC-CovidNet/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapted Human Pose: Monocular 3D Human Pose Estimation with Zero Real 3D Pose Data. (arXiv:2105.10837v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10837">
<div class="article-summary-box-inner">
<span><p>The ultimate goal for an inference model is to be robust and functional in
real life applications. However, training vs. test data domain gaps often
negatively affect model performance. This issue is especially critical for the
monocular 3D human pose estimation problem, in which 3D human data is often
collected in a controlled lab setting. In this paper, we focus on alleviating
the negative effect of domain shift in both appearance and pose space for 3D
human pose estimation by presenting our adapted human pose (AHuP) approach.
AHuP is built upon two key components: (1) semantically aware adaptation (SAA)
for the cross-domain feature space adaptation, and (2) skeletal pose adaptation
(SPA) for the pose space adaptation which takes only limited information from
the target domain. By using zero real 3D human pose data, one of our adapted
synthetic models shows comparable performance with the SOTA pose estimation
models trained with large scale real 3D human datasets. The proposed SPA can be
also employed independently as a light-weighted head to improve existing SOTA
models in a novel context. A new 3D scan-based synthetic human dataset called
ScanAva+ is also going to be publicly released with this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Contextual Deep Network Based Multimodal Pedestrian Detection For Autonomous Driving. (arXiv:2105.12713v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12713">
<div class="article-summary-box-inner">
<span><p>Pedestrian Detection is the most critical module of an Autonomous Driving
system. Although a camera is commonly used for this purpose, its quality
degrades severely in low-light night time driving scenarios. On the other hand,
the quality of a thermal camera image remains unaffected in similar conditions.
This paper proposes an end-to-end multimodal fusion model for pedestrian
detection using RGB and thermal images. Its novel spatio-contextual deep
network architecture is capable of exploiting the multimodal input efficiently.
It consists of two distinct deformable ResNeXt-50 encoders for feature
extraction from the two modalities. Fusion of these two encoded features takes
place inside a multimodal feature embedding module (MuFEm) consisting of
several groups of a pair of Graph Attention Network and a feature fusion unit.
The output of the last feature fusion unit of MuFEm is subsequently passed to
two CRFs for their spatial refinement. Further enhancement of the features is
achieved by applying channel-wise attention and extraction of contextual
information with the help of four RNNs traversing in four different directions.
Finally, these feature maps are used by a single-stage decoder to generate the
bounding box of each pedestrian and the score map. We have performed extensive
experiments of the proposed framework on three publicly available multimodal
pedestrian detection benchmark datasets, namely KAIST, CVC-14, and UTokyo. The
results on each of them improved the respective state-of-the-art performance. A
short video giving an overview of this work along with its qualitative results
can be seen at https://youtu.be/FDJdSifuuCs. Our source code will be released
upon publication of the paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Graph Reasoning for Neural Network Regularization. (arXiv:2106.01805v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01805">
<div class="article-summary-box-inner">
<span><p>Regularizers help deep neural networks prevent feature co-adaptations.
Dropout, as a commonly used regularization technique, stochastically disables
neuron activations during network optimization. However, such complete feature
disposal can affect the feature representation and network understanding.
Toward better descriptions of latent representations, we present DropGraph that
learns a regularization function by constructing a stand-alone graph from the
backbone features. DropGraph first samples stochastic spatial feature vectors
and then incorporates graph reasoning methods to generate feature map
distortions. This add-on graph regularizes the network during training and can
be completely skipped during inference. We provide intuitions on the linkage
between graph reasoning and Dropout with further discussions on how partial
graph reasoning method reduces feature correlations. To this end, we
extensively study the modeling of graph vertex dependencies and the utilization
of the graph for distorting backbone feature maps. DropGraph was validated on 4
tasks with a total of 8 different datasets. The experimental results show that
our method outperforms other state-of-the-art regularizers while leaving the
base model structure unmodified during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occlusion-aware Unsupervised Learning of Depth from 4-D Light Fields. (arXiv:2106.03043v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03043">
<div class="article-summary-box-inner">
<span><p>Depth estimation is a fundamental issue in 4-D light field processing and
analysis. Although recent supervised learning-based light field depth
estimation methods have significantly improved the accuracy and efficiency of
traditional optimization-based ones, these methods rely on the training over
light field data with ground-truth depth maps which are challenging to obtain
or even unavailable for real-world light field data. Besides, due to the
inevitable gap (or domain difference) between real-world and synthetic data,
they may suffer from serious performance degradation when generalizing the
models trained with synthetic data to real-world data. By contrast, we propose
an unsupervised learning-based method, which does not require ground-truth
depth as supervision during training. Specifically, based on the basic
knowledge of the unique geometry structure of light field data, we present an
occlusion-aware strategy to improve the accuracy on occlusion areas, in which
we explore the angular coherence among subsets of the light field views to
estimate initial depth maps, and utilize a constrained unsupervised loss to
learn their corresponding reliability for final depth prediction. Additionally,
we adopt a multi-scale network with a weighted smoothness loss to handle the
textureless areas. Experimental results on synthetic data show that our method
can significantly shrink the performance gap between the previous unsupervised
method and supervised ones, and produce depth maps with comparable accuracy to
traditional methods with obviously reduced computational cost. Moreover,
experiments on real-world datasets show that our method can avoid the domain
shift problem presented in supervised methods, demonstrating the great
potential of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13948">
<div class="article-summary-box-inner">
<span><p>Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Margin-Aware Intra-Class Novelty Identification for Medical Images. (arXiv:2108.00117v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00117">
<div class="article-summary-box-inner">
<span><p>Traditional anomaly detection methods focus on detecting inter-class
variations while medical image novelty identification is inherently an
intra-class detection problem. For example, a machine learning model trained
with normal chest X-ray and common lung abnormalities, is expected to discover
and flag idiopathic pulmonary fibrosis which a rare lung disease and unseen by
the model during training. The nuances from intra-class variations and lack of
relevant training data in medical image analysis pose great challenges for
existing anomaly detection methods. To tackle the challenges, we propose a
hybrid model - Transformation-based Embedding learning for Novelty Detection
(TEND) which without any out-of-distribution training data, performs novelty
identification by combining both autoencoder-based and classifier-based method.
With a pre-trained autoencoder as image feature extractor, TEND learns to
discriminate the feature embeddings of in-distribution data from the
transformed counterparts as fake out-of-distribution inputs. To enhance the
separation, a distance objective is optimized to enforce a margin between the
two classes. Extensive experimental results on both natural image datasets and
medical image datasets are presented and our method out-performs
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport for Unsupervised Denoising Learning. (arXiv:2108.02574v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02574">
<div class="article-summary-box-inner">
<span><p>Recently, much progress has been made in unsupervised denoising learning.
However, existing methods more or less rely on some assumptions on the signal
and/or degradation model, which limits their practical performance. How to
construct an optimal criterion for unsupervised denoising learning without any
prior knowledge on the degradation model is still an open question. Toward
answering this question, this work proposes a criterion for unsupervised
denoising learning based on the optimal transport theory. This criterion has
favorable properties, e.g., approximately maximal preservation of the
information of the signal, whilst achieving perceptual reconstruction.
Furthermore, though a relaxed unconstrained formulation is used in practical
implementation, we prove that the relaxed formulation in theory has the same
solution as the original constrained formulation. Experiments on synthetic and
real-world data, including realistic photographic, microscopy, depth, and raw
depth images, demonstrate that the proposed method even compares favorably with
supervised methods, e.g., approaching the PSNR of supervised methods while
having better perceptual quality. Particularly, for spatially correlated noise
and realistic microscopy images, the proposed method not only achieves better
perceptual quality but also has higher PSNR than supervised methods. Besides,
it shows remarkable superiority in harsh practical conditions with complex
noise, e.g., raw depth images. Code is available at
https://github.com/wangweiSJTU/OTUR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Field Image Super-Resolution with Transformers. (arXiv:2108.07597v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07597">
<div class="article-summary-box-inner">
<span><p>Light field (LF) image super-resolution (SR) aims at reconstructing
high-resolution LF images from their low-resolution counterparts. Although
CNN-based methods have achieved remarkable performance in LF image SR, these
methods cannot fully model the non-local properties of the 4D LF data. In this
paper, we propose a simple but effective Transformer-based method for LF image
SR. In our method, an angular Transformer is designed to incorporate
complementary information among different views, and a spatial Transformer is
developed to capture both local and long-range dependencies within each
sub-aperture image. With the proposed angular and spatial Transformers, the
beneficial information in an LF can be fully exploited and the SR performance
is boosted. We validate the effectiveness of our angular and spatial
Transformers through extensive ablation studies, and compare our method to
recent state-of-the-art methods on five public LF datasets. Our method achieves
superior SR performance with a small model size and low computational cost.
Code is available at https://github.com/ZhengyuLiang24/LFT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07845">
<div class="article-summary-box-inner">
<span><p>We present ARCH++, an image-based method to reconstruct 3D avatars with
arbitrary clothing styles. Our reconstructed avatars are animation-ready and
highly realistic, in both the visible regions from input views and the unseen
regions. While prior work shows great promise of reconstructing animatable
clothed humans with various topologies, we observe that there exist fundamental
limitations resulting in sub-optimal reconstruction quality. In this paper, we
revisit the major steps of image-based avatar reconstruction and address the
limitations with ARCH++. First, we introduce an end-to-end point based geometry
encoder to better describe the semantics of the underlying 3D human body, in
replacement of previous hand-crafted features. Second, in order to address the
occupancy ambiguity caused by topological changes of clothed humans in the
canonical pose, we propose a co-supervising framework with cross-space
consistency to jointly estimate the occupancy in both the posed and canonical
spaces. Last, we use image-to-image translation networks to further refine
detailed geometry and texture on the reconstructed surface, which improves the
fidelity and consistency across arbitrary viewpoints. In the experiments, we
demonstrate improvements over the state of the art on both public benchmarks
and user studies in reconstruction quality and realism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11845">
<div class="article-summary-box-inner">
<span><p>This letter is concerned with image classification with deep convolutional
neural networks (CNNs). The focus is on the following question: given a set of
candidate CNN models, how to select the right one with the best generalization
property for the current task? Present model selection methods require access
to a batch of labeled data for computing a pre-specified performance metric,
such as the cross-entropy loss, the classification error rate, the negative
log-likelihood. In many practical cases, labels are not available in time as
labeling itself is a time-consuming and expensive task. To this end, this
letter presents an approach to CNN model selection using only unlabeled data.
This method is developed based on a principle termed consistent relative
confidence. The effectiveness and efficiency of the proposed method are
demonstrated by experiments using benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Tensor Completion via Element-wise Weighted Low-rank Tensor Train with Overlapping Ket Augmentation. (arXiv:2109.05736v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05736">
<div class="article-summary-box-inner">
<span><p>In recent years, there have been an increasing number of applications of
tensor completion based on the tensor train (TT) format because of its
efficiency and effectiveness in dealing with higher-order tensor data. However,
existing tensor completion methods using TT decomposition have two obvious
drawbacks. One is that they only consider mode weights according to the degree
of mode balance, even though some elements are recovered better in an
unbalanced mode. The other is that serious blocking artifacts appear when the
missing element rate is relatively large. To remedy such two issues, in this
work, we propose a novel tensor completion approach via the element-wise
weighted technique. Accordingly, a novel formulation for tensor completion and
an effective optimization algorithm, called as tensor completion by parallel
weighted matrix factorization via tensor train (TWMac-TT), is proposed. In
addition, we specifically consider the recovery quality of edge elements from
adjacent blocks. Different from traditional reshaping and ket augmentation, we
utilize a new tensor augmentation technique called overlapping ket
augmentation, which can further avoid blocking artifacts. We then conduct
extensive performance evaluations on synthetic data and several real image data
sets. Our experimental results demonstrate that the proposed algorithm TWMac-TT
outperforms several other competing tensor completion methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Navigation-Oriented Scene Understanding for Robotic Autonomy: Learning to Segment Driveability in Egocentric Images. (arXiv:2109.07245v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07245">
<div class="article-summary-box-inner">
<span><p>This work tackles scene understanding for outdoor robotic navigation, solely
relying on images captured by an on-board camera. Conventional visual scene
understanding interprets the environment based on specific descriptive
categories. However, such a representation is not directly interpretable for
decision-making and constrains robot operation to a specific domain. Thus, we
propose to segment egocentric images directly in terms of how a robot can
navigate in them, and tailor the learning problem to an autonomous navigation
task. Building around an image segmentation network, we present a generic
affordance consisting of 3 driveability levels which can broadly apply to both
urban and off-road scenes. By encoding these levels with soft ordinal labels,
we incorporate inter-class distances during learning which improves
segmentation compared to standard "hard" one-hot labelling. In addition, we
propose a navigation-oriented pixel-wise loss weighting method which assigns
higher importance to safety-critical areas. We evaluate our approach on
large-scale public image segmentation datasets ranging from sunny city streets
to snowy forest trails. In a cross-dataset generalization experiment, we show
that our affordance learning scheme can be applied across a diverse mix of
datasets and improves driveability estimation in unseen environments compared
to general-purpose, single-dataset segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Optimal Control Framework for Joint-channel Parallel MRI Reconstruction without Coil Sensitivities. (arXiv:2109.09738v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09738">
<div class="article-summary-box-inner">
<span><p>Goal: This work aims at developing a novel calibration-free fast parallel MRI
(pMRI) reconstruction method incorporate with discrete-time optimal control
framework. The reconstruction model is designed to learn a regularization that
combines channels and extracts features by leveraging the information sharing
among channels of multi-coil images. We propose to recover both magnitude and
phase information by taking advantage of structured convolutional networks in
image and Fourier spaces. Methods: We develop a novel variational model with a
learnable objective function that integrates an adaptive multi-coil image
combination operator and effective image regularization in the image and
Fourier spaces. We cast the reconstruction network as a structured
discrete-time optimal control system, resulting in an optimal control
formulation of parameter training where the parameters of the objective
function play the role of control variables. We demonstrate that the Lagrangian
method for solving the control problem is equivalent to back-propagation,
ensuring the local convergence of the training algorithm. Results: We conduct a
large number of numerical experiments of the proposed method with comparisons
to several state-of-the-art pMRI reconstruction networks on real pMRI datasets.
The numerical results demonstrate the promising performance of the proposed
method evidently. Conclusion: We conduct a large number of numerical
experiments of the proposed method with comparisons to several state-of-the-art
pMRI reconstruction networks on real pMRI datasets. The numerical results
demonstrate the promising performance of the proposed method evidently.
Significance: By learning multi-coil image combination operator and performing
regularizations in both image domain and k-space domain, the proposed method
achieves a highly efficient image reconstruction network for pMRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editing. (arXiv:2109.10737v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10737">
<div class="article-summary-box-inner">
<span><p>The semantic controllability of StyleGAN is enhanced by unremitting research.
Although the existing weak supervision methods work well in manipulating the
style codes along one attribute, the accuracy of manipulating multiple
attributes is neglected. Sequential editing leads to error accumulation and
attribute entanglement. To address these limitations, we design a Dynamic Style
Manipulation Network (DyStyle) whose structure and parameters vary by input
samples, to perform nonlinear and adaptive manipulation of latent codes for
flexible and precise attribute control. In order to efficient and stable
optimization of the DyStyle network, we propose a Dynamic Multi-Attribute
Contrastive Learning (DmaCL) method: including dynamic multi-attribute
contrastor and dynamic multi-attribute contrastive loss, which simultaneously
disentangle a variety of attributes from the generative image and latent space
of model. As a result, our approach demonstrates fine-grained disentangled
edits along multiple numeric and binary attributes. Qualitative and
quantitative comparisons with existing style manipulation methods verify the
superiority of our method in terms of the multi-attribute control accuracy and
identity preservation without compromising photorealism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong 3D Object Recognition and Grasp Synthesis Using Dual Memory Recurrent Self-Organization Networks. (arXiv:2109.11544v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11544">
<div class="article-summary-box-inner">
<span><p>Humans learn to recognize and manipulate new objects in lifelong settings
without forgetting the previously gained knowledge under non-stationary and
sequential conditions. In autonomous systems, the agents also need to mitigate
similar behavior to continually learn the new object categories and adapt to
new environments. In most conventional deep neural networks, this is not
possible due to the problem of catastrophic forgetting, where the newly gained
knowledge overwrites existing representations. Furthermore, most
state-of-the-art models excel either in recognizing the objects or in grasp
prediction, while both tasks use visual input. The combined architecture to
tackle both tasks is very limited. In this paper, we proposed a hybrid model
architecture consists of a dynamically growing dual-memory recurrent neural
network (GDM) and an autoencoder to tackle object recognition and grasping
simultaneously. The autoencoder network is responsible to extract a compact
representation for a given object, which serves as input for the GDM learning,
and is responsible to predict pixel-wise antipodal grasp configurations. The
GDM part is designed to recognize the object in both instances and categories
levels. We address the problem of catastrophic forgetting using the intrinsic
memory replay, where the episodic memory periodically replays the neural
activation trajectories in the absence of external sensory information. To
extensively evaluate the proposed model in a lifelong setting, we generate a
synthetic dataset due to lack of sequential 3D objects dataset. Experiment
results demonstrated that the proposed model can learn both object
representation and grasping simultaneously in continual learning scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Affinity with Maximum Bipartite Matching in Few-Shot Learning. (arXiv:2110.02399v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02399">
<div class="article-summary-box-inner">
<span><p>We propose an asymmetric affinity score for representing the complexity of
utilizing the knowledge of one task for learning another one. Our method is
based on the maximum bipartite matching algorithm and utilizes the Fisher
Information matrix. We provide theoretical analyses demonstrating that the
proposed score is mathematically well-defined, and subsequently use the
affinity score to propose a novel algorithm for the few-shot learning problem.
In particular, using this score, we find relevant training data labels to the
test data and leverage the discovered relevant data for episodically
fine-tuning a few-shot model. Results on various few-shot benchmark datasets
demonstrate the efficacy of the proposed approach by improving the
classification accuracy over the state-of-the-art methods even when using
smaller models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Camera Calibration through Camera Projection Loss. (arXiv:2110.03479v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03479">
<div class="article-summary-box-inner">
<span><p>Camera calibration is a necessity in various tasks including 3D
reconstruction, hand-eye coordination for a robotic interaction, autonomous
driving, etc. In this work we propose a novel method to predict extrinsic
(baseline, pitch, and translation), intrinsic (focal length and principal point
offset) parameters using an image pair. Unlike existing methods, instead of
designing an end-to-end solution, we proposed a new representation that
incorporates camera model equations as a neural network in multi-task learning
framework. We estimate the desired parameters via novel camera projection loss
(CPL) that uses the camera model neural network to reconstruct the 3D points
and uses the reconstruction loss to estimate the camera parameters. To the best
of our knowledge, ours is the first method to jointly estimate both the
intrinsic and extrinsic parameters via a multi-task learning methodology that
combines analytical equations in learning framework for the estimation of
camera parameters. We also proposed a novel dataset using CARLA Simulator.
Empirically, we demonstrate that our proposed approach achieves better
performance with respect to both deep learning-based and traditional methods on
8 out of 10 parameters evaluated using both synthetic and real data. Our code
and generated dataset are available at
https://github.com/thanif/Camera-Calibration-through-Camera-Projection-Loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Unlearning of Backdoors via Implicit Hypergradient. (arXiv:2110.03735v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03735">
<div class="article-summary-box-inner">
<span><p>We propose a minimax formulation for removing backdoors from a given poisoned
model based on a small set of clean data. This formulation encompasses much of
prior work on backdoor removal. We propose the Implicit Bacdoor Adversarial
Unlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which
breaks down the minimax into separate inner and outer problems, our algorithm
utilizes the implicit hypergradient to account for the interdependence between
inner and outer optimization. We theoretically analyze its convergence and the
generalizability of the robustness gained by solving minimax on clean data to
unseen test data. In our evaluation, we compare I-BAU with six state-of-art
backdoor defenses on seven backdoor attacks over two datasets and various
attack settings, including the common setting where the attacker targets one
class as well as important but underexplored settings where multiple classes
are targeted. I-BAU's performance is comparable to and most often significantly
better than the best baseline. Particularly, its performance is more robust to
the variation on triggers, attack settings, poison ratio, and clean data size.
Moreover, I-BAU requires less computation to take effect; particularly, it is
more than $13\times$ faster than the most efficient baseline in the
single-target attack setting. Furthermore, it can remain effective in the
extreme case where the defender can only access 100 clean samples -- a setting
where all the baselines fail to produce acceptable results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03825">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are known to be vulnerable to adversarial
attacks. A range of defense methods have been proposed to train adversarially
robust DNNs, among which adversarial training has demonstrated promising
results. However, despite preliminary understandings developed for adversarial
training, it is still not clear, from the architectural perspective, what
configurations can lead to more robust DNNs. In this paper, we address this gap
via a comprehensive investigation on the impact of network width and depth on
the robustness of adversarially trained DNNs. Specifically, we make the
following key observations: 1) more parameters (higher model capacity) does not
necessarily help adversarial robustness; 2) reducing capacity at the last stage
(the last group of blocks) of the network can actually improve adversarial
robustness; and 3) under the same parameter budget, there exists an optimal
architectural configuration for adversarial robustness. We also provide a
theoretical analysis explaning why such network configuration can help
robustness. These architectural insights can help design adversarially robust
DNNs. Code is available at \url{https://github.com/HanxunH/RobustWRN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Is Graph: Structured Graph Module for Video Action Recognition. (arXiv:2110.05904v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05904">
<div class="article-summary-box-inner">
<span><p>In the field of action recognition, video clips are always treated as ordered
frames for subsequent processing. To achieve spatio-temporal perception,
existing approaches propose to embed adjacent temporal interaction in the
convolutional layer. The global semantic information can therefore be obtained
by stacking multiple local layers hierarchically. However, such global temporal
accumulation can only reflect the high-level semantics in deep layers,
neglecting the potential low-level holistic clues in shallow layers. In this
paper, we first propose to transform a video sequence into a graph to obtain
direct long-term dependencies among temporal frames. To preserve sequential
information during transformation, we devise a structured graph module (SGM),
achieving fine-grained temporal interactions throughout the entire network. In
particular, SGM divides the neighbors of each node into several temporal
regions so as to extract global structural information with diverse sequential
flows. Extensive experiments are performed on standard benchmark datasets,
i.e., Something-Something V1 &amp; V2, Diving48, Kinetics-400, UCF101, and HMDB51.
The reported performance and analysis demonstrate that SGM can achieve
outstanding precision with less computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEMO: Test Time Robustness via Adaptation and Augmentation. (arXiv:2110.09506v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09506">
<div class="article-summary-box-inner">
<span><p>While deep neural networks can attain good accuracy on in-distribution test
points, many applications require robustness even in the face of unexpected
perturbations in the input, changes in the domain, or other sources of
distribution shift. We study the problem of test time robustification, i.e.,
using the test input to improve model robustness. Recent prior works have
proposed methods for test time adaptation, however, they each introduce
additional assumptions, such as access to multiple test points, that prevent
widespread adoption. In this work, we aim to study and devise methods that make
no assumptions about the model training process and are broadly applicable at
test time. We propose a simple approach that can be used in any test setting
where the model is probabilistic and adaptable: when presented with a test
example, perform different data augmentations on the data point, and then adapt
(all of) the model parameters by minimizing the entropy of the model's average,
or marginal, output distribution across the augmentations. Intuitively, this
objective encourages the model to make the same prediction across different
augmentations, thus enforcing the invariances encoded in these augmentations,
while also maintaining confidence in its predictions. In our experiments, we
evaluate two baseline ResNet models, two robust ResNet-50 models, and a robust
vision transformer model, and we demonstrate that this approach achieves
accuracy gains of 1-8\% over standard model evaluation and also generally
outperforms prior augmentation and adaptation strategies. For the setting in
which only one test point is available, we achieve state-of-the-art results on
the ImageNet-C, ImageNet-R, and, among ResNet-50 models, ImageNet-A
distribution shift benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution. (arXiv:2110.09992v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09992">
<div class="article-summary-box-inner">
<span><p>Despite the growing popularity of video super-resolution (VSR), there is
still no good way to assess the quality of the restored details in upscaled
frames. Some SR methods may produce the wrong digit or an entirely different
face. Whether a method's results are trustworthy depends on how well it
restores truthful details. Image super-resolution can use natural distributions
to produce a high-resolution image that is only somewhat similar to the real
one. VSR enables exploration of additional information in neighboring frames to
restore details from the original scene. The ERQA metric, which we propose in
this paper, aims to estimate a model's ability to restore real details using
VSR. On the assumption that edges are significant for detail and character
recognition, we chose edge fidelity as the foundation for this metric.
Experimental validation of our work is based on the MSU Video Super-Resolution
Benchmark, which includes the most difficult patterns for detail restoration
and verifies the fidelity of details from the original frame. Code for the
proposed metric is publicly available at
https://github.com/msu-video-group/ERQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alpha-IoU: A Family of Power Intersection over Union Losses for Bounding Box Regression. (arXiv:2110.13675v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13675">
<div class="article-summary-box-inner">
<span><p>Bounding box (bbox) regression is a fundamental task in computer vision. So
far, the most commonly used loss functions for bbox regression are the
Intersection over Union (IoU) loss and its variants. In this paper, we
generalize existing IoU-based losses to a new family of power IoU losses that
have a power IoU term and an additional power regularization term with a single
power parameter $\alpha$. We call this new family of losses the $\alpha$-IoU
losses and analyze properties such as order preservingness and loss/gradient
reweighting. Experiments on multiple object detection benchmarks and models
demonstrate that $\alpha$-IoU losses, 1) can surpass existing IoU-based losses
by a noticeable performance margin; 2) offer detectors more flexibility in
achieving different levels of bbox regression accuracy by modulating $\alpha$;
and 3) are more robust to small datasets and noisy bboxes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiMoSeg: Real-time Bird's Eye View based LiDAR Motion Segmentation. (arXiv:2111.04875v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04875">
<div class="article-summary-box-inner">
<span><p>Moving object detection and segmentation is an essential task in the
Autonomous Driving pipeline. Detecting and isolating static and moving
components of a vehicle's surroundings are particularly crucial in path
planning and localization tasks. This paper proposes a novel real-time
architecture for motion segmentation of Light Detection and Ranging (LiDAR)
data. We use three successive scans of LiDAR data in 2D Bird's Eye View (BEV)
representation to perform pixel-wise classification as static or moving.
Furthermore, we propose a novel data augmentation technique to reduce the
significant class imbalance between static and moving objects. We achieve this
by artificially synthesizing moving objects by cutting and pasting static
vehicles. We demonstrate a low latency of 8 ms on a commonly used automotive
embedded platform, namely Nvidia Jetson Xavier. To the best of our knowledge,
this is the first work directly performing motion segmentation in LiDAR BEV
space. We provide quantitative results on the challenging SemanticKITTI
dataset, and qualitative results are provided in https://youtu.be/2aJ-cL8b0LI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Myocardial Infarction Detection over Multi-view Echocardiography. (arXiv:2111.05790v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05790">
<div class="article-summary-box-inner">
<span><p>Myocardial infarction (MI) is the leading cause of mortality in the world
that occurs due to a blockage of the coronary arteries feeding the myocardium.
An early diagnosis of MI and its localization can mitigate the extent of
myocardial damage by facilitating early therapeutic interventions. Following
the blockage of a coronary artery, the regional wall motion abnormality (RWMA)
of the ischemic myocardial segments is the earliest change to set in.
Echocardiography is the fundamental tool to assess any RWMA. Assessing the
motion of the left ventricle (LV) wall only from a single echocardiography view
may lead to missing the diagnosis of MI as the RWMA may not be visible on that
specific view. Therefore, in this study, we propose to fuse apical 4-chamber
(A4C) and apical 2-chamber (A2C) views in which a total of 12 myocardial
segments can be analyzed for MI detection. The proposed method first estimates
the motion of the LV wall by Active Polynomials (APs), which extract and track
the endocardial boundary to compute myocardial segment displacements. The
features are extracted from the A4C and A2C view displacements, which are
concatenated and fed into the classifiers to detect MI. The main contributions
of this study are 1) creation of a new benchmark dataset by including both A4C
and A2C views in a total of 260 echocardiography recordings, which is publicly
shared with the research community, 2) improving the performance of the prior
work of threshold-based APs by a Machine Learning based approach, and 3) a
pioneer MI detection approach via multi-view echocardiography by fusing the
information of A4C and A2C views. Experimental results show that the proposed
method achieves 90.91% sensitivity and 86.36% precision for MI detection over
multi-view echocardiography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pansharpening by convolutional neural networks in the full resolution framework. (arXiv:2111.08334v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08334">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been a growing interest in deep learning-based
pansharpening. Thus far, research has mainly focused on architectures.
Nonetheless, model training is an equally important issue. A first problem is
the absence of ground truths, unavoidable in pansharpening. This is often
addressed by training networks in a reduced resolution domain and using the
original data as ground truth, relying on an implicit scale invariance
assumption. However, on full resolution images results are often disappointing,
suggesting such invariance not to hold. A further problem is the scarcity of
training data, which causes a limited generalization ability and a poor
performance on off-training test images. In this paper, we propose a
full-resolution training framework for deep learning-based pansharpening. The
framework is fully general and can be used for any deep learning-based
pansharpening model. Training takes place in the high-resolution domain,
relying only on the original data, thus avoiding any loss of information. To
ensure spectral and spatial fidelity, a suitable two-component loss is defined.
The spectral component enforces consistency between the pansharpened output and
the low-resolution multispectral input. The spatial component, computed at
high-resolution, maximizes the local correlation between each pansharpened band
and the panchromatic input. At testing time, the target-adaptive operating
modality is adopted, achieving good generalization with a limited computational
overhead. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1
images show that methods trained with the proposed framework guarantee a pretty
good performance in terms of both full-resolution numerical indexes and visual
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransMorph: Transformer for unsupervised medical image registration. (arXiv:2111.10480v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10480">
<div class="article-summary-box-inner">
<span><p>In the last decade, convolutional neural networks (ConvNets) have been a
major focus of research in medical image analysis. However, the performances of
ConvNets may be limited by a lack of explicit consideration of the long-range
spatial relationships in an image. Recently Vision Transformer architectures
have been proposed to address the shortcomings of ConvNets and have produced
state-of-the-art performances in many medical imaging applications.
Transformers may be a strong candidate for image registration because their
unlimited receptive field enables a more precise comprehension of the spatial
correspondence between moving and fixed images. Here, we present TransMorph, a
hybrid Transformer-ConvNet model for volumetric medical image registration.
This paper also presents diffeomorphic and Bayesian variants of TransMorph: the
diffeomorphic variants ensure the topology-preserving deformations, and the
Bayesian variant produces a well-calibrated registration uncertainty estimate.
We extensively validated the proposed models using 3D medical images from three
applications: inter-patient and atlas-to-patient brain MRI registration and
phantom-to-CT registration. The proposed models are evaluated in comparison to
a variety of existing registration methods and Transformer architectures.
Qualitative and quantitative results demonstrate that the proposed
Transformer-based model leads to a substantial performance improvement over the
baseline methods, confirming the effectiveness of Transformers for medical
image registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-Bed Human Pose Estimation from Unseen and Privacy-Preserving Image Domains. (arXiv:2111.15124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15124">
<div class="article-summary-box-inner">
<span><p>Medical applications have benefited greatly from the rapid advancement in
computer vision. Considering patient monitoring in particular, in-bed human
posture estimation offers important health-related metrics with potential value
in medical condition assessments. Despite great progress in this domain, it
remains challenging due to substantial ambiguity during occlusions, and the
lack of large corpora of manually labeled data for model training, particularly
with domains such as thermal infrared imaging which are privacy-preserving, and
thus of great interest. Motivated by the effectiveness of self-supervised
methods in learning features directly from data, we propose a multi-modal
conditional variational autoencoder (MC-VAE) capable of reconstructing features
from missing modalities seen during training. This approach is used with HRNet
to enable single modality inference for in-bed pose estimation. Through
extensive evaluations, we demonstrate that body positions can be effectively
recognized from the available modality, achieving on par results with baseline
models that are highly dependent on having access to multiple modes at
inference time. The proposed framework supports future research towards
self-supervised learning that generates a robust model from a single source,
and expects it to generalize over many unknown distributions in clinical
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features. (arXiv:2111.15263v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15263">
<div class="article-summary-box-inner">
<span><p>Linguistic knowledge has brought great benefits to scene text recognition by
providing semantics to refine character sequences. However, since linguistic
knowledge has been applied individually on the output sequence, previous
methods have not fully utilized the semantics to understand visual clues for
text recognition. This paper introduces a novel method, called Multi-modAl Text
Recognition Network (MATRN), that enables interactions between visual and
semantic features for better recognition performances. Specifically, MATRN
identifies visual and semantic feature pairs and encodes spatial information
into semantic features. Based on the spatial encoding, visual and semantic
features are enhanced by referring to related features in the other modality.
Furthermore, MATRN stimulates combining semantic features into visual features
by hiding visual clues related to the character in the training phase. Our
experiments demonstrate that MATRN achieves state-of-the-art performances on
seven benchmarks with large margins, while naive combinations of two modalities
show marginal improvements. Further ablative studies prove the effectiveness of
our proposed components. Our implementation is publicly available at
https://github.com/wp03052/MATRN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Deep Learning for Low-Shot Object Detection. (arXiv:2112.02814v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02814">
<div class="article-summary-box-inner">
<span><p>Object detection has achieved a huge breakthrough with deep neural networks
and massive annotated data. However, current detection methods cannot be
directly transferred to the scenario where the annotated data is scarce due to
the severe overfitting problem. Although few-shot learning and zero-shot
learning have been extensively explored in the field of image classification,
it is indispensable to design new methods for object detection in the
data-scarce scenario since object detection has an additional challenging
localization task. Low-Shot Object Detection (LSOD) is an emerging research
topic of detecting objects from a few or even no annotated samples, consisting
of One-Shot Object Detection (OSOD), Few-Shot Object Detection (FSOD) and
Zero-Shot Object Detection (ZSD). This survey provides a comprehensive review
of LSOD methods. First, we propose a thorough taxonomy of LSOD methods and
analyze them systematically, comprising some extensional topics of LSOD
(semi-supervised LSOD, weakly-supervised LSOD and incremental LSOD). Then, we
indicate the pros and cons of current LSOD methods with a comparison of their
performance. Finally, we discuss the challenges and promising directions of
LSOD to provide guidance for future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symmetry Perception by Deep Networks: Inadequacy of Feed-Forward Architectures and Improvements with Recurrent Connections. (arXiv:2112.04162v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04162">
<div class="article-summary-box-inner">
<span><p>Symmetry is omnipresent in nature and perceived by the visual system of many
species, as it facilitates detecting ecologically important classes of objects
in our environment. Symmetry perception requires abstraction of long-range
spatial dependencies between image regions, and its underlying neural
mechanisms remain elusive. In this paper, we evaluate Deep Neural Network (DNN)
architectures on the task of learning symmetry perception from examples. We
demonstrate that feed-forward DNNs that excel at modelling human performance on
object recognition tasks, are unable to acquire a general notion of symmetry.
This is the case even when the DNNs are architected to capture long-range
spatial dependencies, such as through `dilated' convolutions and the recently
introduced `transformers' design. By contrast, we find that recurrent
architectures are capable of learning to perceive symmetry by decomposing the
long-range spatial dependencies into a sequence of local operations, that are
reusable for novel images. These results suggest that recurrent connections
likely play an important role in symmetry perception in artificial systems, and
possibly, biological ones too.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Robot Collaborative Perception with Graph Neural Networks. (arXiv:2201.01760v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01760">
<div class="article-summary-box-inner">
<span><p>Multi-robot systems such as swarms of aerial robots are naturally suited to
offer additional flexibility, resilience, and robustness in several tasks
compared to a single robot by enabling cooperation among the agents. To enhance
the autonomous robot decision-making process and situational awareness,
multi-robot systems have to coordinate their perception capabilities to
collect, share, and fuse environment information among the agents in an
efficient and meaningful way such to accurately obtain context-appropriate
information or gain resilience to sensor noise or failures. In this paper, we
propose a general-purpose Graph Neural Network (GNN) with the main goal to
increase, in multi-robot perception tasks, single robots' inference perception
accuracy as well as resilience to sensor failures and disturbances. We show
that the proposed framework can address multi-view visual perception problems
such as monocular depth estimation and semantic segmentation. Several
experiments both using photo-realistic and real data gathered from multiple
aerial robots' viewpoints show the effectiveness of the proposed approach in
challenging inference conditions including images corrupted by heavy noise and
camera occlusions or failures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning. (arXiv:2201.04676v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04676">
<div class="article-summary-box-inner">
<span><p>It is a challenging task to learn rich and multi-scale spatiotemporal
semantics from high-dimensional videos, due to large local redundancy and
complex global dependency between video frames. The recent advances in this
research have been mainly driven by 3D convolutional neural networks and vision
transformers. Although 3D convolution can efficiently aggregate local context
to suppress local redundancy from a small 3D neighborhood, it lacks the
capability to capture global dependency because of the limited receptive field.
Alternatively, vision transformers can effectively capture long-range
dependency by self-attention mechanism, while having the limitation on reducing
local redundancy with blind similarity comparison among all the tokens in each
layer. Based on these observations, we propose a novel Unified transFormer
(UniFormer) which seamlessly integrates merits of 3D convolution and
spatiotemporal self-attention in a concise transformer format, and achieves a
preferable balance between computation and accuracy. Different from traditional
transformers, our relation aggregator can tackle both spatiotemporal redundancy
and dependency, by learning local and global token affinity respectively in
shallow and deep layers. We conduct extensive experiments on the popular video
benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&amp;V2.
With only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1
accuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than
other state-of-the-art methods. For Something-Something V1 and V2, our
UniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1
accuracy respectively. Code is available at
https://github.com/Sense-X/UniFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking. (arXiv:2201.07425v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07425">
<div class="article-summary-box-inner">
<span><p>In this work, we contribute a new million-scale Unmanned Aerial Vehicle (UAV)
tracking benchmark, called WebUAV-3M. Firstly, we collect 4,485 videos with
more than 3M frames from the Internet. Then, an efficient and scalable
Semi-Automatic Target Annotation (SATA) pipeline is devised to label the
tremendous WebUAV-3M in every frame. To the best of our knowledge, the densely
bounding box annotated WebUAV-3M is by far the largest public UAV tracking
benchmark. We expect to pave the way for the follow-up study in the UAV
tracking by establishing a million-scale annotated benchmark covering a wide
range of target categories. Moreover, considering the close connections among
visual appearance, natural language and audio, we enrich WebUAV-3M by providing
natural language specification and audio description, encouraging the
exploration of natural language features and audio cues for UAV tracking.
Equipped with this benchmark, we delve into million-scale deep UAV tracking
problems, aiming to provide the community with a dedicated large-scale
benchmark for training deep UAV trackers and evaluating UAV tracking
approaches. Extensive experiments on WebUAV-3M demonstrate that there is still
a big room for robust deep UAV tracking improvements. The dataset, toolkits and
baseline results will be available at
\url{https://github.<a href="/abs/com/9836328">com/9836328</a>47/WebUAV-3M}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation. (arXiv:2201.07927v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07927">
<div class="article-summary-box-inner">
<span><p>Despite recent advances in appearance-based gaze estimation techniques, the
need for training data that covers the target head pose and gaze distribution
remains a crucial challenge for practical deployment. This work examines a
novel approach for synthesizing gaze estimation training data based on
monocular 3D face reconstruction. Unlike prior works using multi-view
reconstruction, photo-realistic CG models, or generative neural networks, our
approach can manipulate and extend the head pose range of existing training
data without any additional requirements. We introduce a projective matching
procedure to align the reconstructed 3D facial mesh to the camera coordinate
system and synthesize face images with accurate gaze labels. We also propose a
mask-guided gaze estimation model and data augmentation strategies to further
improve the estimation accuracy by taking advantage of the synthetic training
data. Experiments using multiple public datasets show that our approach can
significantly improve the estimation performance on challenging cross-dataset
settings with non-overlapping gaze distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDhuman: High-quality Human Performance Capture with Sparse Views. (arXiv:2201.08158v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08158">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce HDhuman, a method that addresses the challenge of
novel view rendering of human performers that wear clothes with complex texture
patterns using a sparse set of camera views. Although some recent works have
achieved remarkable rendering quality on humans with relatively uniform
textures using sparse views, the rendering quality remains limited when dealing
with complex texture patterns as they are unable to recover the high-frequency
geometry details that observed in the input views. To this end, the proposed
HDhuman uses a human reconstruction network with a pixel-aligned spatial
transformer and a rendering network that uses geometry-guided pixel-wise
feature integration to achieve high-quality human reconstruction and rendering.
The designed pixel-aligned spatial transformer calculates the correlations
between the input views, producing human reconstruction results with
high-frequency details. Based on the surface reconstruction results, the
geometry-guided pixel-wise visibility reasoning provides guidance for
multi-view feature integration, enabling the rendering network to render
high-quality images at 2k resolution on novel views. Unlike previous neural
rendering works that always need to train or fine-tune an independent network
for a different scene, our method is a general framework that is able to
generalize to novel subjects. Experiments show that our approach outperforms
all the prior generic or specific methods on both synthetic data and real-world
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization. (arXiv:2201.08613v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08613">
<div class="article-summary-box-inner">
<span><p>Localizing keypoints of an object is a basic visual problem. However,
supervised learning of a keypoint localization network often requires a large
amount of data, which is expensive and time-consuming to obtain. To remedy
this, there is an ever-growing interest in semi-supervised learning (SSL),
which leverages a small set of labeled data along with a large set of unlabeled
data. Among these SSL approaches, pseudo-labeling (PL) is one of the most
popular. PL approaches apply pseudo-labels to unlabeled data, and then train
the model with a combination of the labeled and pseudo-labeled data
iteratively. The key to the success of PL is the selection of high-quality
pseudo-labeled samples. Previous works mostly select training samples by
manually setting a single confidence threshold. We propose to automatically
select reliable pseudo-labeled samples with a series of dynamic thresholds,
which constitutes a learning curriculum. Extensive experiments on six keypoint
localization benchmark datasets demonstrate that the proposed approach
significantly outperforms the previous state-of-the-art SSL approaches.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-25 23:07:51.820361352 UTC">2022-01-25 23:07:51 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>