<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-23T01:30:00Z">05-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Local dynamic mode of Cognitive Behavioral Therapy. (arXiv:2205.09752v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09752">
<div class="article-summary-box-inner">
<span><p>In order to increase mental health equity among the most vulnerable and
marginalized communities, it is important to increase access to high-quality
therapists. One facet of addressing these needs, is to provide timely feedback
to clinicians as they interact with their clients, in a way that is also
contextualized to specific clients and interactions they have had. Dynamical
systems provide a framework through which to analyze interactions. The present
work applies these methods to the domain of automated psychotherapist
evaluation for Cognitive Behavioral Therapy (CBT). Our methods extract local
dynamic modes from short windows of conversation and learns to correlate the
observed dynamics to CBT competence. The results demonstrate the value of this
paradigm and outlines the way in which these methods can be used to study and
improve therapeutic strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Holistic View on Argument Quality Prediction. (arXiv:2205.09803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09803">
<div class="article-summary-box-inner">
<span><p>Argumentation is one of society's foundational pillars, and, sparked by
advances in NLP and the vast availability of text data, automated mining of
arguments receives increasing attention. A decisive property of arguments is
their strength or quality. While there are works on the automated estimation of
argument strength, their scope is narrow: they focus on isolated datasets and
neglect the interactions with related argument mining tasks, such as argument
identification, evidence detection, or emotional appeal. In this work, we close
this gap by approaching argument quality estimation from multiple different
angles: Grounded on rich results from thorough empirical evaluations, we assess
the generalization capabilities of argument quality estimation across diverse
domains, the interplay with related argument mining tasks, and the impact of
emotions on perceived argument strength. We find that generalization depends on
a sufficient representation of different domains in the training part. In
zero-shot transfer and multi-task experiments, we reveal that argument quality
is among the more challenging tasks but can improve others. Finally, we show
that emotions play a minor role in argument quality than is often assumed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MiDAS: Multi-integrated Domain Adaptive Supervision for Fake News Detection. (arXiv:2205.09817v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09817">
<div class="article-summary-box-inner">
<span><p>COVID-19 related misinformation and fake news, coined an 'infodemic', has
dramatically increased over the past few years. This misinformation exhibits
concept drift, where the distribution of fake news changes over time, reducing
effectiveness of previously trained models for fake news detection. Given a set
of fake news models trained on multiple domains, we propose an adaptive
decision module to select the best-fit model for a new sample. We propose
MiDAS, a multi-domain adaptative approach for fake news detection that ranks
relevancy of existing models to new samples. MiDAS contains 2 components: a
doman-invariant encoder, and an adaptive model selector. MiDAS integrates
multiple pre-trained and fine-tuned models with their training data to create a
domain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of
the invariant embedding space to estimate each model's relevance to a new
sample. Higher ranked models provide predictions, and lower ranked models
abstain. We evaluate MiDAS on generalization to drifted data with 9 fake news
datasets, each obtained from different domains and modalities. MiDAS achieves
new state-of-the-art performance on multi-domain adaptation for
out-of-distribution fake news classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation. (arXiv:2205.09830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09830">
<div class="article-summary-box-inner">
<span><p>Women are often perceived as junior to their male counterparts, even within
the same job titles. While there has been significant progress in the
evaluation of gender bias in natural language processing (NLP), existing
studies seldom investigate how biases toward gender groups change when
compounded with other societal biases. In this work, we investigate how
seniority impacts the degree of gender bias exhibited in pretrained neural
generation models by introducing a novel framework for probing compound bias.
We contribute a benchmark robustness-testing dataset spanning two domains, U.S.
senatorship and professorship, created using a distant-supervision method. Our
dataset includes human-written text with underlying ground truth and paired
counterfactuals. We then examine GPT-2 perplexity and the frequency of gendered
language in generated text. Our results show that GPT-2 amplifies bias by
considering women as junior and men as senior more often than the ground truth
in both domains. These results suggest that NLP applications built using GPT-2
may harm women in professional capacities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarization as Indirect Supervision for Relation Extraction. (arXiv:2205.09837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09837">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) models have been challenged by their reliance on
training data with expensive annotations. Considering that summarization tasks
aim at acquiring concise expressions of synoptical information from the longer
context, these tasks naturally align with the objective of RE, i.e., extracting
a kind of synoptical information that describes the relation of entity
mentions. We present SuRE, which converts RE into a summarization formulation.
SuRE leads to more precise and resource-efficient RE based on indirect
supervision from summarization tasks. To achieve this goal, we develop sentence
and relation conversion techniques that essentially bridge the formulation of
summarization and RE tasks. We also incorporate constraint decoding techniques
with Trie scoring to further enhance summarization-based RE with robust
inference. Experiments on three RE datasets demonstrate the effectiveness of
SuRE in both full-dataset and low-resource settings, showing that summarization
is a promising source of indirect supervision to improve RE models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A toolbox for idea generation and evaluation: Machine learning, data-driven, and contest-driven approaches to support idea generation. (arXiv:2205.09840v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09840">
<div class="article-summary-box-inner">
<span><p>The significance and abundance of data are increasing due to the growing
digital data generated from social media, sensors, scholarly literature,
patents, different forms of documents published online, databases, product
manuals, etc. Various data sources can be used to generate ideas, yet, in
addition to bias, the size of the available digital data is a major challenge
when it comes to manual analysis. Hence, human-machine interaction is essential
for generating valuable ideas where machine learning and data-driven techniques
generate patterns from data and serve human sense-making. However, the use of
machine learning and data-driven approaches to generate ideas is a relatively
new area. Moreover, it is also possible to stimulate innovation using
contest-driven idea generation and evaluation. The results and contributions of
this thesis can be viewed as a toolbox of idea-generation techniques, including
a list of data-driven and machine learning techniques with corresponding data
sources and models to support idea generation. In addition, the results include
two models, one method and one framework, to better support data-driven and
contest- driven idea generation. The beneficiaries of these artefacts are
practitioners in data and knowledge engineering, data mining project managers,
and innovation agents. Innovation agents include incubators, contest
organizers, consultants, innovation accelerators, and industries. Since the
proposed artefacts consist of process models augmented with AI techniques,
human-centred AI is a promising area of research that can contribute to the
artefacts' further development and promote creativity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Table Retrieval May Not Necessitate Table-specific Model Design. (arXiv:2205.09843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09843">
<div class="article-summary-box-inner">
<span><p>Tables are an important form of structured data for both human and machine
readers alike, providing answers to questions that cannot, or cannot easily, be
found in texts. Recent work has designed special models and training paradigms
for table-related tasks such as table-based question answering and table
retrieval. Though effective, they add complexity in both modeling and data
acquisition compared to generic text solutions and obscure which elements are
truly beneficial. In this work, we focus on the task of table retrieval, and
ask: "is table-specific model design necessary for table retrieval, or can a
simpler text-based model be effectively used to achieve a similar result?"
First, we perform an analysis on a table-based portion of the Natural Questions
dataset (NQ-table), and find that structure plays a negligible role in more
than 70% of the cases. Based on this, we experiment with a general Dense
Passage Retriever (DPR) based on text and a specialized Dense Table Retriever
(DTR) that uses table-specific model designs. We find that DPR performs well
without any table-specific design and training, and even achieves superior
results compared to DTR when fine-tuned on properly linearized tables. We then
experiment with three modules to explicitly encode table structures, namely
auxiliary row/column embeddings, hard attention masks, and soft relation-based
attention biases. However, none of these yielded significant improvements,
suggesting that table-specific model design may not be necessary for table
retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Bias in Meta-Embeddings. (arXiv:2205.09867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09867">
<div class="article-summary-box-inner">
<span><p>Combining multiple source embeddings to create meta-embeddings is considered
effective to obtain more accurate embeddings. Different methods have been
proposed to develop meta-embeddings from a given set of source embeddings.
However, the source embeddings can contain unfair gender bias, and the bias in
the combination of multiple embeddings and debiasing it effectively have not
been studied yet. In this paper, we investigate the bias in three types of
meta-embeddings: (1) Multi-Source No-Debiasing: meta-embedding from multiple
source embeddings without any debiasing. The experimental results show that
meta-embedding amplifies the gender bias compared to those of input source
embeddings; (2) Multi-Source Single-Debiasing: meta-embedding from multiple
source embeddings debiased by a single method and it can be created in three
ways: debiasing each source embedding, debiasing the learned meta-embeddings,
and debiasing both source embeddings and meta-embeddings. The results show that
debiasing both is the best in two out of three bias evaluation benchmarks; (3)
Single-Source Multi-Debiasing: meta-embedding from the same source embedding
debiased by different methods. It performed more effectively than its source
embeddings debiased with a single method in all three bias evaluation
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let the Model Decide its Curriculum for Multitask Learning. (arXiv:2205.09898v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09898">
<div class="article-summary-box-inner">
<span><p>Curriculum learning strategies in prior multi-task learning approaches
arrange datasets in a difficulty hierarchy either based on human perception or
by exhaustively searching the optimal arrangement. However, human perception of
difficulty may not always correlate well with machine interpretation leading to
poor performance and exhaustive search is computationally expensive. Addressing
these concerns, we propose two classes of techniques to arrange training
instances into a learning curriculum based on difficulty scores computed via
model-based approaches. The two classes i.e Dataset-level and Instance-level
differ in granularity of arrangement. Through comprehensive experiments with 12
datasets, we show that instance-level and dataset-level techniques result in
strong representations as they lead to an average performance improvement of
4.17% and 3.15% over their respective baselines. Furthermore, we find that most
of this improvement comes from correctly answering the difficult instances,
implying a greater efficacy of our techniques on difficult tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation. (arXiv:2205.09921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09921">
<div class="article-summary-box-inner">
<span><p>Relative positional embeddings (RPE) have received considerable attention
since RPEs effectively model the relative distance among tokens and enable
length extrapolation. We propose KERPLE, a framework that generalizes relative
position embedding for extrapolation by kernelizing positional differences. We
achieve this goal using conditionally positive definite (CPD) kernels, a class
of functions known for generalizing distance metrics. To maintain the inner
product interpretation of self-attention, we show that a CPD kernel can be
transformed into a PD kernel by adding a constant offset. This offset is
implicitly absorbed in the Softmax normalization during self-attention. The
diversity of CPD kernels allows us to derive various RPEs that enable length
extrapolation in a principled way. Experiments demonstrate that the logarithmic
variant achieves excellent extrapolation performance on three large language
modeling datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SALTED: A Framework for SAlient Long-Tail Translation Error Detection. (arXiv:2205.09988v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09988">
<div class="article-summary-box-inner">
<span><p>Traditional machine translation (MT) metrics provide an average measure of
translation quality that is insensitive to the long tail of behavioral problems
in MT. Examples include translation of numbers, physical units, dropped content
and hallucinations. These errors, which occur rarely and unpredictably in
Neural Machine Translation (NMT), greatly undermine the reliability of
state-of-the-art MT systems. Consequently, it is important to have visibility
into these problems during model development. Towards this direction, we
introduce SALTED, a specifications-based framework for behavioral testing of MT
models that provides fine-grained views of salient long-tail errors, permitting
trustworthy visibility into previously invisible problems. At the core of our
approach is the development of high-precision detectors that flag errors (or
alternatively, verify output correctness) between a source sentence and a
system output. We demonstrate that such detectors could be used not just to
identify salient long-tail errors in MT systems, but also for higher-recall
filtering of the training data, fixing targeted errors with model fine-tuning
in NMT and generating novel data for metamorphic testing to elicit further bugs
in models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Descartes: Generating Short Descriptions of Wikipedia Articles. (arXiv:2205.10012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10012">
<div class="article-summary-box-inner">
<span><p>We introduce and tackle the problem of automatically generating short
descriptions of Wikipedia articles (e.g., Belgium has a short description
Country in Western Europe). We introduce Descartes, a model that can generate
descriptions performing on par with human editors. Our human evaluation results
indicate that Descartes is preferred over editor-written descriptions about 50%
of time. Further manual analysis show that Descartes generates descriptions
considered as "valid" for 91.3% of articles, this is the as same editor-written
descriptions. Such performances are made possible by integrating other signals
naturally existing in Wikipedia: (i) articles about the same entity in
different languages, (ii) existing short descriptions in other languages, and
(iii) structural information from Wikidata. Our work has direct practical
applications in helping Wikipedia editors to provide short descriptions for the
more than 9 million articles still missing one. Finally, our proposed
architecture can easily be re-purposed to address other information gaps in
Wikipedia.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translating Hanja historical documents to understandable Korean and English. (arXiv:2205.10019v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10019">
<div class="article-summary-box-inner">
<span><p>The Annals of Joseon Dynasty (AJD) contain the daily records of the Kings of
Joseon, the 500-year kingdom preceding the modern nation of Korea. The Annals
were originally written in an archaic Korean writing system, `Hanja', and
translated into Korean from 1968 to 1993. However, this translation was literal
and contained many archaic Korean words; thus, a new expert translation effort
began in 2012, completing the records of only one king in a decade. Also,
expert translators are working on an English translation, of which only one
king's records are available because of the high cost and slow progress. Thus,
we propose H2KE, the neural machine translation model that translates Hanja
historical documents to understandable Korean and English. Based on the
multilingual neural machine translation approach, it translates the historical
document written in Hanja, using both the full dataset of outdated Korean
translation and a small dataset of recently translated Korean and English. We
compare our method with two baselines: one is a recent model that
simultaneously learns to restore and translate Hanja historical document and
the other is the transformer that trained on newly translated corpora only. The
results show that our method significantly outperforms the baselines in terms
of BLEU score in both modern Korean and English translations. We also conduct a
human evaluation that shows that our translation is preferred over the original
expert translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transition-based Semantic Role Labeling with Pointer Networks. (arXiv:2205.10023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10023">
<div class="article-summary-box-inner">
<span><p>Semantic role labeling (SRL) focuses on recognizing the predicate-argument
structure of a sentence and plays a critical role in many natural language
processing tasks such as machine translation and question answering.
Practically all available methods do not perform full SRL, since they rely on
pre-identified predicates, and most of them follow a pipeline strategy, using
specific models for undertaking one or several SRL subtasks. In addition,
previous approaches have a strong dependence on syntactic information to
achieve state-of-the-art performance, despite being syntactic trees equally
hard to produce. These simplifications and requirements make the majority of
SRL systems impractical for real-world applications. In this article, we
propose the first transition-based SRL approach that is capable of completely
processing an input sentence in a single left-to-right pass, with neither
leveraging syntactic information nor resorting to additional modules. Thanks to
our implementation based on Pointer Networks, full SRL can be accurately and
efficiently done in $O(n^2)$, achieving the best performance to date on the
majority of languages from the CoNLL-2009 shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Extreme Parameter Compression for Pre-trained Language Models. (arXiv:2205.10036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10036">
<div class="article-summary-box-inner">
<span><p>Recent work explored the potential of large-scale Transformer-based
pre-trained models, especially Pre-trained Language Models (PLMs) in natural
language processing. This raises many concerns from various perspectives, e.g.,
financial costs and carbon emissions. Compressing PLMs like BERT with
negligible performance loss for faster inference and cheaper deployment has
attracted much attention. In this work, we aim to explore larger compression
ratios for PLMs, among which tensor decomposition is a potential but
under-investigated one. Two decomposition and reconstruction protocols are
further proposed to improve the effectiveness and efficiency during
compression. Our compressed BERT with ${1}/{7}$ parameters in Transformer
layers performs on-par with, sometimes slightly better than the original BERT
in GLUE benchmark. A tiny version achieves $96.7\%$ performance of BERT-base
with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding
the embedding layer) and $2.7 \times$ faster on inference. To show that the
proposed method is orthogonal to existing compression methods like knowledge
distillation, we also explore the benefit of the proposed method on a distilled
BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking. (arXiv:2205.10059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10059">
<div class="article-summary-box-inner">
<span><p>In dialogue state tracking, dialogue history is a crucial material, and its
utilization varies between different models. However, no matter how the
dialogue history is used, each existing model uses its own consistent dialogue
history during the entire state tracking process, regardless of which slot is
updated. Apparently, it requires different dialogue history to update different
slots in different turns. Therefore, using consistent dialogue contents may
lead to insufficient or redundant information for different slots, which
affects the overall performance. To address this problem, we devise DiCoS-DST
to dynamically select the relevant dialogue contents corresponding to each slot
for state updating. Specifically, it first retrieves turn-level utterances of
dialogue history and evaluates their relevance to the slot from a combination
of three perspectives: (1) its explicit connection to the slot name; (2) its
relevance to the current turn dialogue; (3) Implicit Mention Oriented
Reasoning. Then these perspectives are combined to yield a decision, and only
the selected dialogue contents are fed into State Generator, which explicitly
minimizes the distracting information passed to the downstream state
prediction. Experimental results show that our approach achieves new
state-of-the-art performance on MultiWOZ 2.1 and MultiWOZ 2.2, and achieves
superior performance on multiple mainstream benchmark datasets (including
Sim-M, Sim-R, and DSTC2).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding and Mitigating the Uncertainty in Zero-Shot Translation. (arXiv:2205.10068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10068">
<div class="article-summary-box-inner">
<span><p>Zero-shot translation is a promising direction for building a comprehensive
multilingual neural machine translation (MNMT) system. However, its quality is
still not satisfactory due to off-target issues. In this paper, we aim to
understand and alleviate the off-target issues from the perspective of
uncertainty in zero-shot translation. By carefully examining the translation
output and model confidence, we identify two uncertainties that are responsible
for the off-target issues, namely, extrinsic data uncertainty and intrinsic
model uncertainty. Based on the observations, we propose two light-weight and
complementary approaches to denoise the training data for model training, and
mask out the vocabulary of the off-target languages in inference. Extensive
experiments on both balanced and unbalanced datasets show that our approaches
significantly improve the performance of zero-shot translation over strong MNMT
baselines. Qualitative analyses provide insights into where our approaches
reduce off-target translations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uzbek affix finite state machine for stemming. (arXiv:2205.10078v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10078">
<div class="article-summary-box-inner">
<span><p>This work presents a morphological analyzer for the Uzbek language using a
finite state machine. The proposed methodology is a morphologic analysis of
Uzbek words by using an affix striping to find a root and without including any
lexicon. This method helps to perform morphological analysis of words from a
large amount of text at high speed as well as it is not required using of
memory for keeping vocabulary. According to Uzbek, an agglutinative language
can be designed with finite state machines (FSMs). In contrast to the previous
works, this study modeled the completed FSMs for all word classes by using the
Uzbek language's morphotactic rules in right to left order. This paper shows
the stages of this methodology including the classification of the affixes, the
generation of the FSMs for each affix class, and the combination into a head
machine to make analysis a word.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-self-supervised Automated ICD Coding. (arXiv:2205.10088v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10088">
<div class="article-summary-box-inner">
<span><p>Clinical Text Notes (CTNs) contain physicians' reasoning process, written in
an unstructured free text format, as they examine and interview patients. In
recent years, several studies have been published that provide evidence for the
utility of machine learning for predicting doctors' diagnoses from CTNs, a task
known as ICD coding. Data annotation is time consuming, particularly when a
degree of specialization is needed, as is the case for medical data. This paper
presents a method of augmenting a sparsely annotated dataset of Icelandic CTNs
with a machine-learned imputation in a semi-self-supervised manner. We train a
neural network on a small set of annotated CTNs and use it to extract clinical
features from a set of un-annotated CTNs. These clinical features consist of
answers to about a thousand potential questions that a physician might find the
answers to during a consultation of a patient. The features are then used to
train a classifier for the diagnosis of certain types of diseases. We report
the results of an evaluation of this data augmentation method over three tiers
of data availability to the physician. Our data augmentation method shows a
significant positive effect which is diminished when clinical features from the
examination of the patient and diagnostics are made available. We recommend our
method for augmenting scarce datasets for systems that take decisions based on
clinical features that do not include examinations or tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing. (arXiv:2205.10095v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10095">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) models for natural language processing (NLP) tasks often
handle private data, demanding protection against breaches and disclosures.
Data protection laws, such as the European Union's General Data Protection
Regulation (GDPR), thereby enforce the need for privacy. Although many
privacy-preserving NLP methods have been proposed in recent years, no
categories to organize them have been introduced yet, making it hard to follow
the progress of the literature. To close this gap, this article systematically
reviews over sixty DL methods for privacy-preserving NLP published between 2016
and 2020, covering theoretical foundations, privacy-enhancing technologies, and
analysis of their suitability for real-world scenarios. First, we introduce a
novel taxonomy for classifying the existing methods into three categories: data
safeguarding methods, trusted methods, and verification methods. Second, we
present an extensive summary of privacy threats, datasets for applications, and
metrics for privacy evaluation. Third, throughout the review, we describe
privacy issues in the NLP pipeline in a holistic view. Further, we discuss open
challenges in privacy-preserving NLP regarding data traceability, computation
overhead, dataset size, the prevalence of human biases in embeddings, and the
privacy-utility tradeoff. Finally, this review presents future research
directions to guide successive research and development of privacy-preserving
NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually-Augmented Language Modeling. (arXiv:2205.10178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10178">
<div class="article-summary-box-inner">
<span><p>Human language is grounded on multimodal knowledge including visual knowledge
like colors, sizes, and shapes. However, current large-scale pre-trained
language models rely on the text-only self-supervised training with massive
text data, which precludes them from utilizing relevant visual information when
necessary. To address this, we propose a novel pre-training framework, named
VaLM, to Visually-augment text tokens with retrieved relevant images for
Language Modeling. Specifically, VaLM builds on a novel text-vision alignment
method via an image retrieval module to fetch corresponding images given a
textual context. With the visually-augmented context, VaLM uses a visual
knowledge fusion layer to enable multimodal grounded language modeling by
attending on both text context and visual knowledge in images. We evaluate the
proposed model on various multimodal commonsense reasoning tasks, which require
visual information to excel. VaLM outperforms the text-only baseline with
substantial gains of +8.66% and +37.81% accuracy on object color and size
reasoning, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototypical Calibration for Few-shot Learning of Language Models. (arXiv:2205.10183v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10183">
<div class="article-summary-box-inner">
<span><p>In-context learning of GPT-like models has been recognized as fragile across
different hand-crafted templates, and demonstration permutations. In this work,
we propose prototypical calibration to adaptively learn a more robust decision
boundary for zero- and few-shot classification, instead of greedy decoding.
Concretely, our method first adopts Gaussian mixture distribution to estimate
the prototypical clusters for all categories. Then we assign each cluster to
the corresponding label by solving a weighted bipartite matching problem. Given
an example, its prediction is calibrated by the likelihood of prototypical
clusters. Experimental results show that prototypical calibration yields a 15%
absolute improvement on a diverse set of tasks. Extensive analysis across
different scales also indicates that our method calibrates the decision
boundary as expected, greatly improving the robustness of GPT to templates,
permutations, and class imbalance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Class Semantic Matching for Semi-supervised Text Classification. (arXiv:2205.10189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10189">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning is a promising way to reduce the annotation cost for
text-classification. Combining with pre-trained language models (PLMs), e.g.,
BERT, recent semi-supervised learning methods achieved impressive performance.
In this work, we further investigate the marriage between semi-supervised
learning and a pre-trained language model. Unlike existing approaches that
utilize PLMs only for model parameter initialization, we explore the inherent
topic matching capability inside PLMs for building a more powerful
semi-supervised learning approach. Specifically, we propose a joint
semi-supervised learning process that can progressively build a standard
$K$-way classifier and a matching network for the input text and the Class
Semantic Representation (CSR). The CSR will be initialized from the given
labeled sentences and progressively updated through the training process. By
means of extensive experiments, we show that our method can not only bring
remarkable improvement to baselines, but also overall be more stable, and
achieves state-of-the-art performance in semi-supervised text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Trade-off between Redundancy and Local Coherence in Summarization. (arXiv:2205.10192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10192">
<div class="article-summary-box-inner">
<span><p>Extractive summarization systems are known to produce poorly coherent and, if
not accounted for, highly redundant text. In this work, we tackle the problem
of summary redundancy in unsupervised extractive summarization of long,
highly-redundant documents. For this, we leverage a psycholinguistic theory of
human reading comprehension which directly models local coherence and
redundancy. Implementing this theory, our system operates at the proposition
level and exploits properties of human memory representations to rank similarly
content units that are coherent and non-redundant, hence encouraging the
extraction of less redundant final summaries. Because of the impact of the
summary length on automatic measures, we control for it by formulating content
selection as an optimization problem with soft constraints in the budget of
information retrieved. Using summarization of scientific articles as a case
study, extensive experiments demonstrate that the proposed systems extract
consistently less redundant summaries across increasing levels of document
redundancy, whilst maintaining comparable performance (in terms of relevancy
and local coherence) against strong unsupervised baselines according to
automated evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?. (arXiv:2205.10226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10226">
<div class="article-summary-box-inner">
<span><p>Learned self-attention functions in state-of-the-art NLP models often
correlate with human attention. We investigate whether self-attention in
large-scale pre-trained language models is as predictive of human eye fixation
patterns during task-reading as classical cognitive models of human attention.
We compare attention functions across two task-specific reading datasets for
sentiment analysis and relation extraction. We find the predictiveness of
large-scale pre-trained self-attention for human attention depends on `what is
in the tail', e.g., the syntactic nature of rare contexts. Further, we observe
that task-specific fine-tuning does not increase the correlation with human
task-specific reading. Through an input reduction experiment we give
complementary insights on the sparsity and fidelity trade-off, showing that
lower-entropy attention vectors are more faithful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Anchored Contrastive Learning for Language Understanding. (arXiv:2205.10227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10227">
<div class="article-summary-box-inner">
<span><p>Contrastive learning (CL) has achieved astonishing progress in computer
vision, speech, and natural language processing fields recently with
self-supervised learning. However, CL approach to the supervised setting is not
fully explored, especially for the natural language understanding
classification task. Intuitively, the class label itself has the intrinsic
ability to perform hard positive/negative mining, which is crucial for CL.
Motivated by this, we propose a novel label anchored contrastive learning
approach (denoted as LaCon) for language understanding. Specifically, three
contrastive objectives are devised, including a multi-head instance-centered
contrastive loss (ICL), a label-centered contrastive loss (LCL), and a label
embedding regularizer (LER). Our approach does not require any specialized
network architecture or any extra data augmentation, thus it can be easily
plugged into existing powerful pre-trained language models. Compared to the
state-of-the-art baselines, LaCon obtains up to 4.1% improvement on the popular
datasets of GLUE and CLUE benchmarks. Besides, LaCon also demonstrates
significant advantages under the few-shot and data imbalance settings, which
obtains up to 9.4% improvement on the FewGLUE and FewCLUE benchmarking tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Don't Know My Favorite Color: Preventing Dialogue Representations from Revealing Speakers' Private Personas. (arXiv:2205.10228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10228">
<div class="article-summary-box-inner">
<span><p>Social chatbots, also known as chit-chat chatbots, evolve rapidly with large
pretrained language models. Despite the huge progress, privacy concerns have
arisen recently: training data of large language models can be extracted via
model inversion attacks. On the other hand, the datasets used for training
chatbots contain many private conversations between two individuals. In this
work, we further investigate the privacy leakage of the hidden states of
chatbots trained by language modeling which has not been well studied yet. We
show that speakers' personas can be inferred through a simple neural network
with high accuracy. To this end, we propose effective defense objectives to
protect persona leakage from hidden states. We conduct extensive experiments to
demonstrate that our proposed defense objectives can greatly reduce the attack
accuracy from 37.6% to 0.5%. Meanwhile, the proposed objectives preserve
language models' powerful generation ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RigoBERTa: A State-of-the-Art Language Model For Spanish. (arXiv:2205.10233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10233">
<div class="article-summary-box-inner">
<span><p>This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.
RigoBERTa is trained over RigoCorpus, a well-curated corpus formed up from
different subcorpora with key features. It follows the DeBERTa architecture,
which has several advantages over other architectures of similar size as BERT
or RoBERTa.
</p>
<p>RigoBERTa performance is assessed over 13 NLU tasks in comparison with other
available Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa
outperformed the three models in 10 out of the 13 tasks, achieving new
"State-of-the-Art" results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated learning for violence incident prediction in a simulated cross-institutional psychiatric setting. (arXiv:2205.10234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10234">
<div class="article-summary-box-inner">
<span><p>Inpatient violence is a common and severe problem within psychiatry. Knowing
who might become violent can influence staffing levels and mitigate severity.
Predictive machine learning models can assess each patient's likelihood of
becoming violent based on clinical notes. Yet, while machine learning models
benefit from having more data, data availability is limited as hospitals
typically do not share their data for privacy preservation. Federated Learning
(FL) can overcome the problem of data limitation by training models in a
decentralised manner, without disclosing data between collaborators. However,
although several FL approaches exist, none of these train Natural Language
Processing models on clinical notes. In this work, we investigate the
application of Federated Learning to clinical Natural Language Processing,
applied to the task of Violence Risk Assessment by simulating a
cross-institutional psychiatric setting. We train and compare four models: two
local models, a federated model and a data-centralised model. Our results
indicate that the federated model outperforms the local models and has similar
performance as the data-centralised model. These findings suggest that
Federated Learning can be used successfully in a cross-institutional setting
and is a step towards new applications of Federated Learning based on clinical
notes
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database. (arXiv:2205.10237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10237">
<div class="article-summary-box-inner">
<span><p>The emotional state of a speaker can be influenced by many different factors
in dialogues, such as dialogue scene, dialogue topic, and interlocutor
stimulus. The currently available data resources to support such multimodal
affective analysis in dialogues are however limited in scale and diversity. In
this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue
dataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different
TV series, a total of 9,082 turns and 24,449 utterances. M3 ED is annotated
with 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and
neutral) at utterance level, and encompasses acoustic, visual, and textual
modalities. To the best of our knowledge, M3ED is the first multimodal
emotional dialogue dataset in Chinese. It is valuable for cross-culture emotion
analysis and recognition. We apply several state-of-the-art methods on the M3ED
dataset to verify the validity and quality of the dataset. We also propose a
general Multimodal Dialogue-aware Interaction framework, MDI, to model the
dialogue context for emotion recognition, which achieves comparable performance
to the state-of-the-art methods on the M3ED. The full dataset and codes are
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing and Explaining Language Models. (arXiv:2205.10238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10238">
<div class="article-summary-box-inner">
<span><p>During the last decade, Natural Language Processing has become, after
Computer Vision, the second field of Artificial Intelligence that was massively
changed by the advent of Deep Learning. Regardless of the architecture, the
language models of the day need to be able to process or generate text, as well
as predict missing words, sentences or relations depending on the task. Due to
their black-box nature, such models are difficult to interpret and explain to
third parties. Visualization is often the bridge that language model designers
use to explain their work, as the coloring of the salient words and phrases,
clustering or neuron activations can be used to quickly understand the
underlying models. This paper showcases the techniques used in some of the most
popular Deep Learning for NLP visualizations, with a special focus on
interpretability and explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterformer: A Transformer Architecture for Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10282">
<div class="article-summary-box-inner">
<span><p>We study node representation learning on heterogeneous text-rich networks,
where nodes and edges are multi-typed and some types of nodes are associated
with text information. Although recent studies on graph neural networks (GNNs)
and pretrained language models (PLMs) have demonstrated their power in encoding
network and text signals, respectively, less focus has been given to delicately
coupling these two types of models on heterogeneous text-rich networks.
Specifically, existing GNNs rarely model text in each node in a contextualized
way; existing PLMs can hardly be applied to characterize graph structures due
to their sequence architecture. In this paper, we propose Heterformer, a
Heterogeneous GNN-nested transformer that blends GNNs and PLMs into a unified
model. Different from previous "cascaded architectures" that directly add GNN
layers upon a PLM, our Heterformer alternately stacks two modules - a
graph-attention-based neighbor aggregation module and a transformer-based text
and neighbor joint encoding module - to facilitate thorough mutual enhancement
between network and text signals. Meanwhile, Heterformer is capable of
characterizing network heterogeneity and nodes without text information.
Comprehensive experiments on three large-scale datasets from different domains
demonstrate the superiority of Heterformer over state-of-the-art baselines in
link prediction, transductive/inductive node classification, node clustering,
and semantics-based retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities. (arXiv:2205.10312v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10312">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA) aims at finding equivalent entities in different
knowledge graphs (KGs). Embedding-based approaches have dominated the EA task
in recent years. Those methods face problems that come from the geometric
properties of embedding vectors, including hubness and isolation. To solve
these geometric problems, many normalization approaches have been adopted to
EA. However, the increasing scale of KGs renders it is hard for EA models to
adopt the normalization processes, thus limiting their usage in real-world
applications. To tackle this challenge, we present ClusterEA, a general
framework that is capable of scaling up EA models and enhancing their results
by leveraging normalization methods on mini-batches with a high entity
equivalent rate. ClusterEA contains three components to align entities between
large-scale KGs, including stochastic training, ClusterSampler, and
SparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic
fashion to produce entity embeddings. Based on the embeddings, a novel
ClusterSampler strategy is proposed for sampling highly overlapped
mini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes
local and global similarity and then fuses all similarity matrices to obtain
the final similarity matrix. Extensive experiments with real-life datasets on
EA benchmarks offer insight into the proposed framework, and suggest that it is
capable of outperforming the state-of-the-art scalable EA framework by up to 8
times in terms of Hits@1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lossless Acceleration for Seq2seq Generation with Aggressive Decoding. (arXiv:2205.10350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10350">
<div class="article-summary-box-inner">
<span><p>We study lossless acceleration for seq2seq generation with a novel decoding
algorithm -- Aggressive Decoding. Unlike the previous efforts (e.g.,
non-autoregressive decoding) speeding up seq2seq generation at the cost of
quality loss, our approach aims to yield the identical (or better) generation
compared with autoregressive decoding but in a significant speedup, achieved by
innovative cooperation of aggressive decoding and verification that are both
efficient due to parallel computing.
</p>
<p>We propose two Aggressive Decoding paradigms for 2 kinds of seq2seq tasks: 1)
For the seq2seq tasks whose inputs and outputs are highly similar (e.g.,
Grammatical Error Correction), we propose Input-guided Aggressive Decoding
(IAD) that aggressively copies from the input sentence as drafted decoded
tokens to verify in parallel; 2) For other general seq2seq tasks (e.g., Machine
Translation), we propose Generalized Aggressive Decoding (GAD) that first
employs an additional non-autoregressive decoding model for aggressive decoding
and then verifies in parallel in the autoregressive manner.
</p>
<p>We test Aggressive Decoding on the most popular 6-layer Transformer model on
GPU in multiple seq2seq tasks: 1) For IAD, we show that it can introduce a
7x-9x speedup for the Transformer in Grammatical Error Correction and Text
Simplification tasks with the identical results as greedy decoding; 2) For GAD,
we observe a 3x-5x speedup with the identical or even better quality in two
important seq2seq tasks: Machine Translation and Abstractive Summarization.
Moreover, Aggressive Decoding can benefit even more from stronger computing
devices that are better at parallel computing. Given the lossless quality as
well as significant and promising speedup, we believe Aggressive Decoding may
potentially evolve into a de facto standard for efficient and lossless seq2seq
generation in the near future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selecting Informative Contexts Improves Language Model Finetuning. (arXiv:2005.00175v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00175">
<div class="article-summary-box-inner">
<span><p>Language model fine-tuning is essential for modern natural language
processing, but is computationally expensive and time-consuming. Further, the
effectiveness of fine-tuning is limited by the inclusion of training examples
that negatively affect performance. Here we present a general fine-tuning
method that we call information gain filtration for improving the overall
training efficiency and final performance of language model fine-tuning. We
define the information gain of an example as the improvement on a test metric
after training on that example. A secondary learner is then trained to
approximate this quantity. During fine-tuning, this learner selects informative
examples and skips uninformative ones. We show that our method has consistent
improvement across datasets, fine-tuning tasks, and language model
architectures. For example, we achieve a median perplexity of 54.0 on a books
dataset compared to 57.3 for standard fine-tuning. We present statistical
evidence that offers insight into the improvements of our method over standard
fine-tuning. The generality of our method leads us to propose a new paradigm
for language model fine-tuning -- we encourage researchers to release
pretrained secondary learners on common corpora to promote efficient and
effective fine-tuning, thereby improving the performance and reducing the
overall energy footprint of language model fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Text Simplification. (arXiv:2008.08612v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08612">
<div class="article-summary-box-inner">
<span><p>Text Simplification (TS) aims to reduce the linguistic complexity of content
to make it easier to understand. Research in TS has been of keen interest,
especially as approaches to TS have shifted from manual, hand-crafted rules to
automated simplification. This survey seeks to provide a comprehensive overview
of TS, including a brief description of earlier approaches used, discussion of
various aspects of simplification (lexical, semantic and syntactic), and latest
techniques being utilized in the field. We note that the research in the field
has clearly shifted towards utilizing deep learning techniques to perform TS,
with a specific focus on developing solutions to combat the lack of data
available for simplification. We also include a discussion of datasets and
evaluations metrics commonly used, along with discussion of related fields
within Natural Language Processing (NLP), like semantic similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models. (arXiv:2104.08666v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08666">
<div class="article-summary-box-inner">
<span><p>Numerous works have analyzed biases in vision and pre-trained language models
individually - however, less attention has been paid to how these biases
interact in multimodal settings. This work extends text-based bias analysis
methods to investigate multimodal language models, and analyzes intra- and
inter-modality associations and biases learned by these models. Specifically,
we demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often
preferring to reinforce a stereotype over faithfully describing the visual
scene. We demonstrate these findings on a controlled case-study and extend them
for a larger set of stereotypically gendered entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Out-of-Domain Detection via Pre-trained Transformers. (arXiv:2106.00948v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00948">
<div class="article-summary-box-inner">
<span><p>Deployed real-world machine learning applications are often subject to
uncontrolled and even potentially malicious inputs. Those out-of-domain inputs
can lead to unpredictable outputs and sometimes catastrophic safety issues.
Prior studies on out-of-domain detection require in-domain task labels and are
limited to supervised classification scenarios. Our work tackles the problem of
detecting out-of-domain samples with only unsupervised in-domain data. We
utilize the latent representations of pre-trained transformers and propose a
simple yet effective method to transform features across all layers to
construct out-of-domain detectors efficiently. Two domain-specific fine-tuning
approaches are further proposed to boost detection accuracy. Our empirical
evaluations of related methods on two datasets validate that our method greatly
improves out-of-domain detection ability in a more general scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WALNUT: A Benchmark on Weakly Supervised Learning for Natural Language Understanding. (arXiv:2108.12603v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12603">
<div class="article-summary-box-inner">
<span><p>Building machine learning models for natural language understanding (NLU)
tasks relies heavily on labeled data. Weak supervision has been proven valuable
when large amount of labeled data is unavailable or expensive to obtain.
Existing works studying weak supervision for NLU either mostly focus on a
specific task or simulate weak supervision signals from ground-truth labels. It
is thus hard to compare different approaches and evaluate the benefit of weak
supervision without access to a unified and systematic benchmark with diverse
tasks and real-world weak labeling rules. In this paper, we propose such a
benchmark, named WALNUT (semi-WeAkly supervised Learning for Natural language
Understanding Testbed), to advocate and facilitate research on weak supervision
for NLU. WALNUT consists of NLU tasks with different types, including
document-level and token-level prediction tasks. WALNUT is the first
semi-weakly supervised learning benchmark for NLU, where each task contains
weak labels generated by multiple real-world weak sources, together with a
small set of clean labels. We conduct baseline evaluations on WALNUT to
systematically evaluate the effectiveness of various weak supervision methods
and model architectures. Our results demonstrate the benefit of weak
supervision for low-resource NLU tasks and highlight interesting patterns
across tasks. We expect WALNUT to stimulate further research on methodologies
to leverage weak supervision more effectively. The benchmark and code for
baselines are available at \url{aka.ms/walnut_benchmark}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11797">
<div class="article-summary-box-inner">
<span><p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising
capabilities in grounding natural language in image data, facilitating a broad
variety of cross-modal tasks. However, we note that there exists a significant
gap between the objective forms of model pre-training and fine-tuning,
resulting in a need for large amounts of labeled data to stimulate the visual
grounding capability of VL-PTMs for downstream tasks. To address the challenge,
we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt
Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual
grounding into a fill-in-the-blank problem with color-based co-referential
markers in image and text, maximally mitigating the gap. In this way, CPT
enables strong few-shot and even zero-shot visual grounding capabilities of
VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs
outperform their fine-tuned counterparts by a large margin (e.g., 17.3%
absolute accuracy improvement, and 73.8% relative standard deviation reduction
on average with one shot in RefCOCO evaluation). We make the data and code for
this paper publicly available at https://github.com/thunlp/CPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization. (arXiv:2110.07166v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07166">
<div class="article-summary-box-inner">
<span><p>Hallucination is a known issue for neural abstractive summarization models.
Recent work suggests that the degree of hallucination may depend on errors in
the training data. In this work, we propose a new method called Contrastive
Parameter Ensembling (CaPE) to use training data more effectively, utilizing
variations in noise in training samples to reduce hallucination. We first
select clean and noisy subsets from the training data using different automatic
factual metrics. Then, we fine-tune a base summarization model, which is
trained on all training samples, on the clean (noisy) subset to obtain an
\textit{expert} (\textit{anti-expert}) model. Finally, we adjust the parameters
of base model by the difference between parameters of the \textit{expert} and
\textit{anti-expert} models, steering the base model towards the
\textit{expert} model and away from the \textit{anti-expert} model.
Experimental results show that CaPE improves performance across different
automatic factual metrics and human evaluation, with the maximum improvement of
16.69\% and 15.78\% on summary-level dependency-arc entailment accuracy for the
XSUM and CNN/DM datasets. The improvement in factual performance does not
degrade the performance on other metrics of informativeness such as ROUGE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. (arXiv:2110.08412v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08412">
<div class="article-summary-box-inner">
<span><p>To explain NLP models, importance measures such as attention inform which
inputs tokens are important for a prediction are popular. However, an open
question is how well these explanations accurately reflect a model's logic, a
property called faithfulness.
</p>
<p>To answer this question, we propose an new faithfulness benchmark called
Recursive ROAR. This works by recursively masking allegedly important tokens
and then retrain the model. The principle is, that this should result in worse
model performance compared to masking random tokens. The result is a
performance curve given a masking-ratio. Furthermore, we propose a summarizing
metric using the area-between-curves, which allows for easy comparison across
papers, models, and tasks.
</p>
<p>To provide a thorough review, we evaluate 4 different importance measures on
8 different datasets, using both LSTM-attention models and RoBERTa models.
</p>
<p>We find that the faithfulness of importance measures is both model-dependent
and task-dependent. This conclusion contradicts previous evaluations in both
computer vision and faithfulness of attention literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Character-level HyperNetworks for Hate Speech Detection. (arXiv:2111.06336v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06336">
<div class="article-summary-box-inner">
<span><p>The massive spread of hate speech, hateful content targeted at specific
subpopulations, is a problem of critical social importance. Automated methods
of hate speech detection typically employ state-of-the-art deep learning
(DL)-based text classifiers-large pretrained neural language models of over 100
million parameters, adapting these models to the task of hate speech detection
using relevant labeled datasets. Unfortunately, there are only a few public
labeled datasets of limited size that are available for this purpose. We make
several contributions with high potential for advancing this state of affairs.
We present HyperNetworks for hate speech detection, a special class of DL
networks whose weights are regulated by a small-scale auxiliary network. These
architectures operate at character-level, as opposed to word or subword-level,
and are several orders of magnitude smaller compared to the popular DL
classifiers. We further show that training hate detection classifiers using
additional large amounts of automatically generated examples is beneficial in
general, yet this practice especially boosts the performance of the proposed
HyperNetworks. We report the results of extensive experiments, assessing the
performance of multiple neural architectures on hate detection using five
public datasets. The assessed methods include the pretrained language models of
BERT, RoBERTa, ALBERT, MobileBERT and CharBERT, a variant of BERT that
incorporates character alongside subword embeddings. In addition to the
traditional setup of within-dataset evaluation, we perform cross-dataset
evaluation experiments, testing the generalization of the various models in
conditions of data shift. Our results show that the proposed HyperNetworks
achieve performance that is competitive, and better in some cases, than these
pretrained language models, while being smaller by orders of magnitude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proposition-Level Clustering for Multi-Document Summarization. (arXiv:2112.08770v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08770">
<div class="article-summary-box-inner">
<span><p>Text clustering methods were traditionally incorporated into multi-document
summarization (MDS) as a means for coping with considerable information
repetition. Particularly, clusters were leveraged to indicate information
saliency as well as to avoid redundancy. Such prior methods focused on
clustering sentences, even though closely related sentences usually contain
also non-aligned parts. In this work, we revisit the clustering approach,
grouping together sub-sentential propositions, aiming at more precise
information alignment. Specifically, our method detects salient propositions,
clusters them into paraphrastic clusters, and generates a representative
sentence for each cluster via text fusion. Our summarization method improves
over the previous state-of-the-art MDS method in the DUC 2004 and TAC 2011
datasets, both in automatic ROUGE scores and human preference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond. (arXiv:2201.11443v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11443">
<div class="article-summary-box-inner">
<span><p>The shift towards publicly available text sources has enabled language
processing at unprecedented scale, yet leaves under-serviced the domains where
public and openly licensed data is scarce. Proactively collecting text data for
research is a viable strategy to address this scarcity, but lacks systematic
methodology taking into account the many ethical, legal and
confidentiality-related aspects of data collection. Our work presents a case
study on proactive data collection in peer review -- a challenging and
under-resourced NLP domain. We outline ethical and legal desiderata for
proactive data collection and introduce "Yes-Yes-Yes", the first donation-based
peer reviewing data collection workflow that meets these requirements. We
report on the implementation of Yes-Yes-Yes at ACL Rolling Review and
empirically study the implications of proactive data collection for dataset
size and the biases induced by the donation behavior on the peer reviewing
platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptor: Objective-Centric Adaptation Framework for Language Models. (arXiv:2203.03989v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03989">
<div class="article-summary-box-inner">
<span><p>Progress in natural language processing research is catalyzed by the
possibilities given by the widespread software frameworks. This paper
introduces Adaptor library that transposes the traditional model-centric
approach composed of pre-training + fine-tuning steps to objective-centric
approach, composing the training process by applications of selected
objectives. We survey research directions that can benefit from enhanced
objective-centric experimentation in multitask training, custom objectives
development, dynamic training curricula, or domain adaptation. Adaptor aims to
ease reproducibility of these research directions in practice. Finally, we
demonstrate the practical applicability of Adaptor in selected unsupervised
domain adaptation scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09590">
<div class="article-summary-box-inner">
<span><p>Within the emerging research efforts to combine structured and unstructured
knowledge, many approaches incorporate factual knowledge, e.g., available in
form of structured knowledge graphs (KGs), into pre-trained language models
(PLMs) and then apply the knowledge-enhanced PLMs to downstream NLP tasks.
However, (1) they typically only consider \textit{static} factual knowledge,
whereas, e.g., knowledge graphs (KGs) also contain \textit{temporal facts} or
\textit{events} indicating evolutionary relationships among entities at
different timestamps. (2) PLMs cannot be directly applied to many KG tasks,
such as temporal KG completion. In this paper, we focus on \textbf{e}nhancing
temporal knowledge embeddings with \textbf{co}ntextualized \textbf{la}nguage
representations (ECOLA). We align structured knowledge, contained in temporal
knowledge graphs, with their textual descriptions extracted from news articles,
and propose a novel knowledge-text prediction task to inject the abundant
information from descriptions into temporal knowledge embeddings. ECOLA jointly
optimizes the knowledge-text prediction objective and the temporal knowledge
embeddings, which can simultaneously take full advantage of textual and
knowledge information. The proposed fusion method is model-agnostic and can be
combined with potentially any temporal KG model. For training ECOLA, we
introduce three temporal KG datasets with aligned textual descriptions.
Experimental results on the temporal knowledge graph completion task show that
ECOLA outperforms state-of-the-art temporal KG models by a large margin. The
proposed datasets can serve as new temporal KG benchmarks and facilitate future
research on structured and unstructured knowledge integration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linearizing Transformer with Key-Value Memory. (arXiv:2203.12644v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12644">
<div class="article-summary-box-inner">
<span><p>Efficient transformer variants with linear time complexity have been
developed to mitigate the quadratic computational overhead of the vanilla
transformer. Among them are low-rank projection methods such as Linformer and
kernel-based Transformers. Despite their unique merits, they usually suffer
from a performance drop comparing with the vanilla transformer on many sequence
generation tasks, and often fail to obtain computation gain when the generation
is short. We propose MemSizer, an approach towards closing the performance gap
while improving the efficiency even with short generation. It projects the
source sequences into lower dimension representations like Linformer, while
enjoying efficient recurrent-style incremental computation similar to
kernel-based transformers. This yields linear computation time and constant
memory complexity at inference time. MemSizer also employs a lightweight
multi-head mechanism which renders the computation as light as a single-head
model. We demonstrate that MemSizer provides an improved balance between
efficiency and accuracy over the vanilla transformer and other efficient
transformer variants in three typical sequence generation tasks, including
machine translation, abstractive text summarization, and language modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STaR: Bootstrapping Reasoning With Reasoning. (arXiv:2203.14465v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14465">
<div class="article-summary-box-inner">
<span><p>Generating step-by-step "chain-of-thought" rationales improves language model
performance on complex reasoning tasks like mathematics or commonsense
question-answering. However, inducing language model rationale generation
currently requires either constructing massive rationale datasets or
sacrificing accuracy by using only few-shot inference. We propose a technique
to iteratively leverage a small number of rationale examples and a large
dataset without rationales, to bootstrap the ability to perform successively
more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR),
relies on a simple loop: generate rationales to answer many questions, prompted
with a few rationale examples; if the generated answers are wrong, try again to
generate a rationale given the correct answer; fine-tune on all the rationales
that ultimately yielded correct answers; repeat. We show that STaR
significantly improves performance on multiple datasets compared to a model
fine-tuned to directly predict final answers, and performs comparably to
fine-tuning a 30$\times$ larger state-of-the-art language model on
CommensenseQA. Thus, STaR lets a model improve itself by learning from its own
generated reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16487">
<div class="article-summary-box-inner">
<span><p>Different from previous work accelerating translation at the cost of quality
loss, we propose Generalized Aggressive Decoding (GAD) -- a novel decoding
paradigm for lossless speedup of autoregressive translation, through the
collaboration of autoregressive and non-autoregressive translation (NAT) of the
Transformer. At each decoding iteration, GAD aggressively decodes a number of
tokens with NAT as a draft and then verifies them in the autoregressive manner,
where only the tokens that pass the verification are accepted as decoded
tokens. GAD can achieve the same results as autoregressive translation but much
more efficiently because both NAT drafting and autoregressive verification
compute in parallel.
</p>
<p>We conduct experiments in four standard WMT benchmarks and confirm that the
vanilla GAD yields exactly the same results as greedy decoding with an around
$3\times$ speedup, and that its variant (GAD++) with an advanced verification
strategy not only outperforms the greedy translation and even achieves the
comparable translation quality with the beam search result, but also further
improves the decoding speed, resulting in an around $5\times$ speedup over
autoregressive translation. Moreover, GAD can be easily generalized for
lossless speedup of other seq2seq tasks like Abstractive Summarization, and
benefit more from stronger computing devices, demonstrating its potential to
become a de facto decoding paradigm in the future. Our models and codes are
available at https://github.com/hemingkx/GAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Baseline Readability Model for Cebuano. (arXiv:2203.17225v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17225">
<div class="article-summary-box-inner">
<span><p>In this study, we developed the first baseline readability model for the
Cebuano language. Cebuano is the second most-used native language in the
Philippines with about 27.5 million speakers. As the baseline, we extracted
traditional or surface-based features, syllable patterns based from Cebuano's
documented orthography, and neural embeddings from the multilingual BERT model.
Results show that the use of the first two handcrafted linguistic features
obtained the best performance trained on an optimized Random Forest model with
approximately 87% across all metrics. The feature sets and algorithm used also
is similar to previous results in readability assessment for the Filipino
language showing potential of crosslingual application. To encourage more work
for readability assessment in Philippine languages such as Cebuano, we
open-sourced both code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can language models learn from explanations in context?. (arXiv:2204.02329v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02329">
<div class="article-summary-box-inner">
<span><p>Large language models can perform new tasks by adapting to a few in-context
examples. For humans, rapid learning from examples can benefit from
explanations that connect examples to task principles. We therefore investigate
whether explanations of few-shot examples can allow language models to adapt
more effectively. We annotate a set of 40 challenging tasks from BIG-Bench with
explanations of answers to a small subset of questions, as well as a variety of
matched control explanations. We evaluate the effects of various zero-shot and
few-shot prompts that include different types of explanations, instructions,
and controls on the performance of a range of large language models. We analyze
these results using statistical multilevel modeling techniques that account for
the nested dependencies among conditions, tasks, prompts, and models. We find
that explanations of examples can improve performance. Adding untuned
explanations to a few-shot prompt offers a modest improvement in performance;
about 1/3 the effect size of adding few-shot examples, but twice the effect
size of task instructions. We then show that explanations tuned for performance
on a small validation set offer substantially larger benefits; building a
prompt by selecting examples and explanations together substantially improves
performance over selecting examples alone. Hand-tuning explanations can
substantially improve performance on challenging tasks. Furthermore, even
untuned explanations outperform carefully matched controls, suggesting that the
benefits are due to the link between an example and its explanation, rather
than lower-level features of the language used. However, only large models can
benefit from explanations. In summary, explanations can support the in-context
learning abilities of large language models on challenging tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07441">
<div class="article-summary-box-inner">
<span><p>Large-scale single-stream pre-training has shown dramatic performance in
image-text retrieval. Regrettably, it faces low inference efficiency due to
heavy attention layers. Recently, two-stream methods like CLIP and ALIGN with
high inference efficiency have also shown promising performance, however, they
only consider instance-level alignment between the two streams (thus there is
still room for improvement). To overcome these limitations, we propose a novel
COllaborative Two-Stream vision-language pretraining model termed COTS for
image-text retrieval by enhancing cross-modal interaction. In addition to
instance level alignment via momentum contrastive learning, we leverage two
extra levels of cross-modal interactions in our COTS: (1) Token-level
interaction - a masked visionlanguage modeling (MVLM) learning objective is
devised without using a cross-stream network module, where variational
autoencoder is imposed on the visual encoder to generate visual tokens for each
image. (2) Task-level interaction - a KL-alignment learning objective is
devised between text-to-image and image-to-text retrieval tasks, where the
probability distribution per task is computed with the negative queues in
momentum contrastive learning. Under a fair comparison setting, our COTS
achieves the highest performance among all two-stream methods and comparable
performance (but with 10,800X faster in inference) w.r.t. the latest
single-stream methods. Importantly, our COTS is also applicable to
text-to-video retrieval, yielding new state-ofthe-art on the widely-used
MSR-VTT dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Representation Collapse of Sparse Mixture of Experts. (arXiv:2204.09179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09179">
<div class="article-summary-box-inner">
<span><p>Sparse mixture of experts provides larger model capacity while requiring a
constant computational overhead. It employs the routing mechanism to distribute
input tokens to the best-matched experts according to their hidden
representations. However, learning such a routing mechanism encourages token
clustering around expert centroids, implying a trend toward representation
collapse. In this work, we propose to estimate the routing scores between
tokens and experts on a low-dimensional hypersphere. We conduct extensive
experiments on cross-lingual language model pre-training and fine-tuning on
downstream tasks. Experimental results across seven multilingual benchmarks
show that our method achieves consistent gains. We also present a comprehensive
analysis on the representation and routing behaviors of our models. Our method
alleviates the representation collapse issue and achieves more consistent
routing than the baseline mixture-of-experts methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks. (arXiv:2204.10615v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10615">
<div class="article-summary-box-inner">
<span><p>Logical approaches to representing language have developed and evaluated
computational models of quantifier words since the 19th century, but today's
NLU models still struggle to capture their semantics. We rely on Generalized
Quantifier Theory for language-independent representations of the semantics of
quantifier words, to quantify their contribution to the errors of NLU models.
We find that quantifiers are pervasive in NLU benchmarks, and their occurrence
at test time is associated with performance drops. Multilingual models also
exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse
for non-English languages. To facilitate directly-targeted probing, we present
an adversarial generalized quantifier NLI task (GQNLI) and show that
pre-trained language models have a clear lack of robustness in generalized
quantifier reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptable Text Matching via Meta-Weight Regulator. (arXiv:2204.12668v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12668">
<div class="article-summary-box-inner">
<span><p>Neural text matching models have been used in a range of applications such as
question answering and natural language inference, and have yielded a good
performance. However, these neural models are of a limited adaptability,
resulting in a decline in performance when encountering test examples from a
different dataset or even a different task. The adaptability is particularly
important in the few-shot setting: in many cases, there is only a limited
amount of labeled data available for a target dataset or task, while we may
have access to a richly labeled source dataset or task. However, adapting a
model trained on the abundant source data to a few-shot target dataset or task
is challenging. To tackle this challenge, we propose a Meta-Weight Regulator
(MWR), which is a meta-learning approach that learns to assign weights to the
source examples based on their relevance to the target loss. Specifically, MWR
first trains the model on the uniformly weighted source examples, and measures
the efficacy of the model on the target examples via a loss function. By
iteratively performing a (meta) gradient descent, high-order gradients are
propagated to the source examples. These gradients are then used to update the
weights of source examples, in a way that is relevant to the target
performance. As MWR is model-agnostic, it can be applied to any backbone neural
model. Extensive experiments are conducted with various backbone text matching
models, on four widely used datasets and two tasks. The results demonstrate
that our proposed approach significantly outperforms a number of existing
adaptation methods and effectively improves the cross-dataset and cross-task
adaptability of the neural text matching models in the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Language Models with Language Feedback. (arXiv:2204.14146v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14146">
<div class="article-summary-box-inner">
<span><p>Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level
summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaVAE: Exploring Adaptive GPT-2s in Variational Auto-Encoders for Language Modeling. (arXiv:2205.05862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05862">
<div class="article-summary-box-inner">
<span><p>Variational Auto-Encoder (VAE) has become the de-facto learning paradigm in
achieving both representation learning and generation for natural language.
However, existing VAE-based language models either employ elementary RNNs,
which is not powerful to handle complex situations, or fine-tunes two
pre-trained language models (PLMs) for any downstream task, which is a huge
drain on resources. In this paper, we introduce the first VAE framework
empowered with adaptive GPT-2s (AdaVAE). Different from existing systems, we
unify both the encoder\&amp;decoder of VAE model using GPT-2s with adaptive
parameter-efficient components. Experiments from multiple dimensions validate
that AdaVAE is competent to better organize language in generation task and
representation modeling, even with less than $15\%$ activated parameters in
training. Our code is available at \url{https://github.com/ImKeTT/adavae}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rule Induction for Efficient Semi-Supervised Learning. (arXiv:2205.09067v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09067">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning has shown promise in allowing NLP models to
generalize from small amounts of labeled data. Meanwhile, pretrained
transformer models act as black-box correlation engines that are difficult to
explain and sometimes behave unreliably. In this paper, we propose tackling
both of these challenges via Automatic Rule Induction (ARI), a simple and
general-purpose framework for the automatic discovery and integration of
symbolic rules into pretrained transformer models. First, we extract weak
symbolic rules from low-capacity machine learning models trained on small
amounts of labeled data. Next, we use an attention mechanism to integrate these
rules into high-capacity pretrained transformer models. Last, the
rule-augmented system becomes part of a self-training framework to boost
supervision signal on unlabeled data. These steps can be layered beneath a
variety of existing weak supervision and semi-supervised NLP algorithms in
order to improve performance and interpretability. Experiments across nine
sequence classification and relation extraction tasks suggest that ARI can
improve state-of-the-art methods with no manual effort and minimal
computational overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Prompt-based Models Clueless?. (arXiv:2205.09295v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09295">
<div class="article-summary-box-inner">
<span><p>Finetuning large pre-trained language models with a task-specific head has
advanced the state-of-the-art on many natural language understanding
benchmarks. However, models with a task-specific head require a lot of training
data, making them susceptible to learning and exploiting dataset-specific
superficial cues that do not generalize to other datasets. Prompting has
reduced the data requirement by reusing the language model head and formatting
the task input to match the pre-training objective. Therefore, it is expected
that few-shot prompt-based models do not exploit superficial cues. This paper
presents an empirical examination of whether few-shot prompt-based models also
exploit superficial cues. Analyzing few-shot prompt-based models on MNLI, SNLI,
HANS, and COPA has revealed that prompt-based models also exploit superficial
cues. While the models perform well on instances with superficial cues, they
often underperform or only marginally outperform random accuracy on instances
without superficial cues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Inflection as a Data Augmentation Method for Parsing. (arXiv:2205.09350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09350">
<div class="article-summary-box-inner">
<span><p>We propose a morphology-based method for low-resource (LR) dependency
parsing. We train a morphological inflector for target LR languages, and apply
it to related rich-resource (RR) treebanks to create cross-lingual
(x-inflected) treebanks that resemble the target LR language. We use such
inflected treebanks to train parsers in zero- (training on x-inflected
treebanks) and few-shot (training on x-inflected and target language treebanks)
setups. The results show that the method sometimes improves the baselines, but
not consistently.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding. (arXiv:2205.09753v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09753">
<div class="article-summary-box-inner">
<span><p>One essential task for autonomous driving is to encode the information of a
driving scene into vector representations so that the downstream task such as
trajectory prediction could perform well. The driving scene is complicated, and
there exists heterogeneity within elements, where they own diverse types of
information i.e., agent dynamics, map routing, road lines, etc. Meanwhile,
there also exist relativity across elements - meaning they have spatial
relations with each other; such relations should be canonically represented
regarding the relative measurements since the absolute value of the coordinate
is meaningless. Taking these two observations into consideration, we propose a
novel backbone, namely Heterogeneous Driving Graph Transformer (HDGT), which
models the driving scene as a heterogeneous graph with different types of nodes
and edges. For graph construction, each node represents either an agent or a
road element and each edge represents their semantics relations such as
Pedestrian-To-Crosswalk, Lane-To-Left-Lane. As for spatial relation encoding,
instead of setting a fixed global reference, the coordinate information of the
node as well as its in-edges is transformed to the local node-centric
coordinate system. For the aggregation module in the graph neural network
(GNN), we adopt the transformer structure in a hierarchical way to fit the
heterogeneous nature of inputs. Experimental results show that the proposed
method achieves new state-of-the-art on INTERACTION Prediction Challenge and
Waymo Open Motion Challenge, in which we rank 1st and 2nd respectively
regarding the minADE/minFDE metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying outliers in astronomical images with unsupervised machine learning. (arXiv:2205.09760v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09760">
<div class="article-summary-box-inner">
<span><p>Astronomical outliers, such as unusual, rare or unknown types of astronomical
objects or phenomena, constantly lead to the discovery of genuinely unforeseen
knowledge in astronomy. More unpredictable outliers will be uncovered in
principle with the increment of the coverage and quality of upcoming survey
data. However, it is a severe challenge to mine rare and unexpected targets
from enormous data with human inspection due to a significant workload.
Supervised learning is also unsuitable for this purpose since designing proper
training sets for unanticipated signals is unworkable. Motivated by these
challenges, we adopt unsupervised machine learning approaches to identify
outliers in the data of galaxy images to explore the paths for detecting
astronomical outliers. For comparison, we construct three methods, which are
built upon the k-nearest neighbors (KNN), Convolutional Auto-Encoder (CAE)+
KNN, and CAE + KNN + Attention Mechanism (attCAE KNN) separately. Testing sets
are created based on the Galaxy Zoo image data published online to evaluate the
performance of the above methods. Results show that attCAE KNN achieves the
best recall (78%), which is 53% higher than the classical KNN method and 22%
higher than CAE+KNN. The efficiency of attCAE KNN (10 minutes) is also superior
to KNN (4 hours) and equal to CAE+KNN(10 minutes) for accomplishing the same
task. Thus, we believe it is feasible to detect astronomical outliers in the
data of galaxy images in an unsupervised manner. Next, we will apply attCAE KNN
to available survey datasets to assess its applicability and reliability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Peek at Peak Emotion Recognition. (arXiv:2205.09791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09791">
<div class="article-summary-box-inner">
<span><p>Despite much progress in the field of facial expression recognition, little
attention has been paid to the recognition of peak emotion. Aviezer et al. [1]
showed that humans have trouble discerning between positive and negative peak
emotions. In this work we analyze how deep learning fares on this challenge. We
find that (i) despite using very small datasets, features extracted from deep
learning models can achieve results significantly better than humans. (ii) We
find that deep learning models, even when trained only on datasets tagged by
humans, still outperform humans in this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-invariant Augmentation for Semi-Supervised Graph Classification. (arXiv:2205.09802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09802">
<div class="article-summary-box-inner">
<span><p>Recently, contrastiveness-based augmentation surges a new climax in the
computer vision domain, where some operations, including rotation, crop, and
flip, combined with dedicated algorithms, dramatically increase the model
generalization and robustness. Following this trend, some pioneering attempts
employ the similar idea to graph data. Nevertheless, unlike images, it is much
more difficult to design reasonable augmentations without changing the nature
of graphs. Although exciting, the current graph contrastive learning does not
achieve as promising performance as visual contrastive learning. We conjecture
the current performance of graph contrastive learning might be limited by the
violation of the label-invariant augmentation assumption. In light of this, we
propose a label-invariant augmentation for graph-structured data to address
this challenge. Different from the node/edge modification and subgraph
extraction, we conduct the augmentation in the representation space and
generate the augmented samples in the most difficult direction while keeping
the label of augmented data the same as the original samples. In the
semi-supervised scenario, we demonstrate our proposed method outperforms the
classical graph neural network based methods and recent graph contrastive
learning on eight benchmark graph-structured data, followed by several in-depth
experiments to further explore the label-invariant augmentation in several
aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of Depth, Camera Pose and Optical Flow from Monocular Video. (arXiv:2205.09821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09821">
<div class="article-summary-box-inner">
<span><p>We propose DFPNet -- an unsupervised, joint learning system for monocular
Depth, Optical Flow and egomotion (Camera Pose) estimation from monocular image
sequences. Due to the nature of 3D scene geometry these three components are
coupled. We leverage this fact to jointly train all the three components in an
end-to-end manner. A single composite loss function -- which involves image
reconstruction-based loss for depth &amp; optical flow, bidirectional consistency
checks and smoothness loss components -- is used to train the network. Using
hyperparameter tuning, we are able to reduce the model size to less than 5%
(8.4M parameters) of state-of-the-art DFP models. Evaluation on KITTI and
Cityscapes driving datasets reveals that our model achieves results comparable
to state-of-the-art in all of the three tasks, even with the significantly
smaller model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subcellular Protein Localisation in the Human Protein Atlas using Ensembles of Diverse Deep Architectures. (arXiv:2205.09841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09841">
<div class="article-summary-box-inner">
<span><p>Automated visual localisation of subcellular proteins can accelerate our
understanding of cell function in health and disease. Despite recent advances
in machine learning (ML), humans still attain superior accuracy by using
diverse clues. We show how this gap can be narrowed by addressing three key
aspects: (i) automated improvement of cell annotation quality, (ii) new
Convolutional Neural Network (CNN) architectures supporting unbalanced and
noisy data, and (iii) informed selection and fusion of multiple &amp; diverse
machine learning models. We introduce a new "AI-trains-AI" method for improving
the quality of weak labels and propose novel CNN architectures exploiting
wavelet filters and Weibull activations. We also explore key factors in the
multi-CNN ensembling process by analysing correlations between image-level and
cell-level predictions. Finally, in the context of the Human Protein Atlas, we
demonstrate that our system achieves state-of-the-art performance in the
multi-label single-cell classification of protein localisation patterns. It
also significantly improves generalisation ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation of Artificial CT Images using Patch-based Conditional Generative Adversarial Networks. (arXiv:2205.09842v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09842">
<div class="article-summary-box-inner">
<span><p>Deep learning has a great potential to alleviate diagnosis and prognosis for
various clinical procedures. However, the lack of a sufficient number of
medical images is the most common obstacle in conducting image-based analysis
using deep learning. Due to the annotations scarcity, semi-supervised
techniques in the automatic medical analysis are getting high attention.
Artificial data augmentation and generation techniques such as generative
adversarial networks (GANs) may help overcome this obstacle. In this work, we
present an image generation approach that uses generative adversarial networks
with a conditional discriminator where segmentation masks are used as
conditions for image generation. We validate the feasibility of GAN-enhanced
medical image generation on whole heart computed tomography (CT) images and its
seven substructures, namely: left ventricle, right ventricle, left atrium,
right atrium, myocardium, pulmonary arteries, and aorta. Obtained results
demonstrate the suitability of the proposed adversarial approach for the
accurate generation of high-quality CT images. The presented method shows great
potential to facilitate further research in the domain of artificial medical
image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Gender Prediction Based on Deep Transfer Learning from Panoramic Radiograph Images. (arXiv:2205.09850v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09850">
<div class="article-summary-box-inner">
<span><p>Panoramic Dental Radiography (PDR) image processing is one of the most
extensively used manual methods for gender determination in forensic medicine.
Manual approaches require a wide range of mandibular parameter measurements in
metric units. Besides being time-consuming, these methods also necessitate the
employment of experienced professionals. In this context, deep learning models
are widely utilized in the auto-analysis of radiological images nowadays, owing
to their high processing speed, accuracy, and stability. In our study, a data
set consisting of 24,000 dental panoramic images was prepared for binary
classification, and the transfer learning method was used to accelerate the
training and increase the performance of our proposed DenseNet121 deep learning
model. With the transfer learning method, instead of starting the learning
process from scratch, the existing patterns learned beforehand were used.
Extensive comparisons were made using deep transfer learning (DTL) models
VGG16, ResNet50, and EfficientNetB6 to assess the classification performance of
the proposed model in PDR images. According to the findings of the comparative
analysis, the proposed model outperformed the other approaches by achieving a
success rate of 97.25% in gender classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. (arXiv:2205.09853v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09853">
<div class="article-summary-box-inner">
<span><p>Video prediction is a challenging task. The quality of video frames from
current state-of-the-art (SOTA) generative models tends to be poor and
generalization beyond the training data is difficult. Furthermore, existing
prediction frameworks are typically not capable of simultaneously handling
other video-related tasks such as unconditional generation or interpolation. In
this work, we devise a general-purpose framework called Masked Conditional
Video Diffusion (MCVD) for all of these video synthesis tasks using a
probabilistic conditional score-based denoising diffusion model, conditioned on
past and/or future frames. We train the model in a manner where we randomly and
independently mask all the past frames or all the future frames. This novel but
straightforward setup allows us to train a single model that is capable of
executing a broad range of video tasks, specifically: future/past prediction --
when only future/past frames are masked; unconditional generation -- when both
past and future frames are masked; and interpolation -- when neither past nor
future frames are masked. Our experiments show that this approach can generate
high-quality frames for diverse types of videos. Our MCVD models are built from
simple non-recurrent 2D-convolutional architectures, conditioning on blocks of
frames and generating blocks of frames. We generate videos of arbitrary lengths
autoregressively in a block-wise manner. Our approach yields SOTA results
across standard video prediction and interpolation benchmarks, with computation
times for training models measured in 1-12 days using $\le$ 4 GPUs.
https://mask-cond-video-diffusion.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real Time Multi-Object Detection for Helmet Safety. (arXiv:2205.09878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09878">
<div class="article-summary-box-inner">
<span><p>The National Football League and Amazon Web Services teamed up to develop the
best sports injury surveillance and mitigation program via the Kaggle
competition. Through which the NFL wants to assign specific players to each
helmet, which would help accurately identify each player's "exposures"
throughout a football play. We are trying to implement a computer vision based
ML algorithms capable of assigning detected helmet impacts to correct players
via tracking information. Our paper will explain the approach to automatically
track player helmets and their collisions. This will also allow them to review
previous plays and explore the trends in exposure over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Labels: Visual Representations for Bone Marrow Cell Morphology Recognition. (arXiv:2205.09880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09880">
<div class="article-summary-box-inner">
<span><p>Analyzing and inspecting bone marrow cell cytomorphology is a critical but
highly complex and time-consuming component of hematopathology diagnosis.
Recent advancements in artificial intelligence have paved the way for the
application of deep learning algorithms to complex medical tasks. Nevertheless,
there are many challenges in applying effective learning algorithms to medical
image analysis, such as the lack of sufficient and reliably annotated training
datasets and the highly class-imbalanced nature of most medical data. Here, we
improve on the state-of-the-art methodologies of bone marrow cell recognition
by deviating from sole reliance on labeled data and leveraging self-supervision
in training our learning models. We investigate our approach's effectiveness in
identifying bone marrow cell types. Our experiments demonstrate significant
performance improvements in conducting different bone marrow cell recognition
tasks compared to the current state-of-the-art methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep transfer learning for image classification: a survey. (arXiv:2205.09904v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09904">
<div class="article-summary-box-inner">
<span><p>Deep neural networks such as convolutional neural networks (CNNs) and
transformers have achieved many successes in image classification in recent
years. It has been consistently demonstrated that best practice for image
classification is when large deep models can be trained on abundant labelled
data. However there are many real world scenarios where the requirement for
large amounts of training data to get the best performance cannot be met. In
these scenarios transfer learning can help improve performance. To date there
have been no surveys that comprehensively review deep transfer learning as it
relates to image classification overall. However, several recent general
surveys of deep transfer learning and ones that relate to particular
specialised target image classification tasks have been published. We believe
it is important for the future progress in the field that all current knowledge
is collated and the overarching patterns analysed and discussed. In this survey
we formally define deep transfer learning and the problem it attempts to solve
in relation to image classification. We survey the current state of the field
and identify where recent progress has been made. We show where the gaps in
current knowledge are and make suggestions for how to progress the field to
fill in these knowledge gaps. We present a new taxonomy of the applications of
transfer learning for image classification. This taxonomy makes it easier to
see overarching patterns of where transfer learning has been effective and,
where it has failed to fulfill its potential. This also allows us to suggest
where the problems lie and how it could be used more effectively. We show that
under this new taxonomy, many of the applications where transfer learning has
been shown to be ineffective or even hinder performance are to be expected when
taking into account the source and target datasets and the techniques used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperspectral Unmixing Based on Nonnegative Matrix Factorization: A Comprehensive Review. (arXiv:2205.09933v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09933">
<div class="article-summary-box-inner">
<span><p>Hyperspectral unmixing has been an important technique that estimates a set
of endmembers and their corresponding abundances from a hyperspectral image
(HSI). Nonnegative matrix factorization (NMF) plays an increasingly significant
role in solving this problem. In this article, we present a comprehensive
survey of the NMF-based methods proposed for hyperspectral unmixing. Taking the
NMF model as a baseline, we show how to improve NMF by utilizing the main
properties of HSIs (e.g., spectral, spatial, and structural information). We
categorize three important development directions including constrained NMF,
structured NMF, and generalized NMF. Furthermore, several experiments are
conducted to illustrate the effectiveness of associated algorithms. Finally, we
conclude the article with possible future directions with the purposes of
providing guidelines and inspiration to promote the development of
hyperspectral unmixing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PGDP5K: A Diagram Parsing Dataset for Plane Geometry Problems. (arXiv:2205.09947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09947">
<div class="article-summary-box-inner">
<span><p>Diagram parsing is an important foundation for geometry problem solving,
attracting increasing attention in the field of intelligent education and
document image understanding. Due to the complex layout and between-primitive
relationship, plane geometry diagram parsing (PGDP) is still a challenging task
deserving further research and exploration. An appropriate dataset is critical
for the research of PGDP. Although some datasets with rough annotations have
been proposed to solve geometric problems, they are either small in scale or
not publicly available. The rough annotations also make them not very useful.
Thus, we propose a new large-scale geometry diagram dataset named PGDP5K and a
novel annotation method. Our dataset consists of 5000 diagram samples composed
of 16 shapes, covering 5 positional relations, 22 symbol types and 6 text
types. Different from previous datasets, our PGDP5K dataset is labeled with
more fine-grained annotations at primitive level, including primitive classes,
locations and relationships. What is more, combined with above annotations and
geometric prior knowledge, it can generate intelligible geometric propositions
automatically and uniquely. We performed experiments on PGDP5K and
IMP-Geometry3K datasets reveal that the state-of-the-art (SOTA) method achieves
only 66.07% F1 value. This shows that PGDP5K presents a challenge for future
research. Our dataset is available at
<a href="http://www.nlpr.ia.ac.cn/databases/CASIA-PGDP5K/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering as Attention: Unified Image Segmentation with Hierarchical Clustering. (arXiv:2205.09949v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09949">
<div class="article-summary-box-inner">
<span><p>We propose a hierarchical clustering-based image segmentation scheme for deep
neural networks, called HCFormer. We interpret image segmentation, including
semantic, instance, and panoptic segmentation, as a pixel clustering problem,
and accomplish it by bottom-up, hierarchical clustering with deep neural
networks. Our hierarchical clustering removes the pixel decoder from
conventional segmentation models and simplifies the segmentation pipeline,
resulting in improved segmentation accuracies and interpretability. HCFormer
can address semantic, instance, and panoptic segmentation with the same
architecture because the pixel clustering is a common approach for various
image segmentation. In experiments, HCFormer achieves comparable or superior
segmentation accuracies compared to baseline methods on semantic segmentation
(55.5 mIoU on ADE20K), instance segmentation (47.1 AP on COCO), and panoptic
segmentation (55.7 PQ on COCO).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Attention Composition for Temporal Action Localization. (arXiv:2205.09956v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09956">
<div class="article-summary-box-inner">
<span><p>Temporal action localization aims at localizing action instances from
untrimmed videos. Existing works have designed various effective modules to
precisely localize action instances based on appearance and motion features.
However, by treating these two kinds of features with equal importance,
previous works cannot take full advantage of each modality feature, making the
learned model still sub-optimal. To tackle this issue, we make an early effort
to study temporal action localization from the perspective of multi-modality
feature learning, based on the observation that different actions exhibit
specific preferences to appearance or motion modality. Specifically, we build a
novel structured attention composition module. Unlike conventional attention,
the proposed module would not infer frame attention and modality attention
independently. Instead, by casting the relationship between the modality
attention and the frame attention as an attention assignment process, the
structured attention composition module learns to encode the frame-modality
structure and uses it to regularize the inferred frame attention and modality
attention, respectively, upon the optimal transport theory. The final
frame-modality attention is obtained by the composition of the two individual
attentions. The proposed structured attention composition module can be
deployed as a plug-and-play module into existing action localization
frameworks. Extensive experiments on two widely used benchmarks show that the
proposed structured attention composition consistently improves four
state-of-the-art temporal action localization methods and builds new
state-of-the-art performance on THUMOS14. Code is availabel at
https://github.com/VividLe/Online-Action-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Feature Learning on Point Clouds using Multi-resolution Features and Learnable Pooling. (arXiv:2205.09962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09962">
<div class="article-summary-box-inner">
<span><p>Existing point cloud feature learning networks often incorporate sequences of
sampling, neighborhood grouping, neighborhood-wise feature learning, and
feature aggregation to learn high-semantic point features that represent the
global context of a point cloud. Unfortunately, the compounded loss of
information concerning granularity and non-maximum point features due to
sampling and max pooling could adversely affect the high-semantic point
features from existing networks such that they are insufficient to represent
the local context of a point cloud, which in turn may hinder the network in
distinguishing fine shapes. To cope with this problem, we propose a novel point
cloud feature learning network, PointStack, using multi-resolution feature
learning and learnable pooling (LP). The multi-resolution feature learning is
realized by aggregating point features of various resolutions in the multiple
layers, so that the final point features contain both high-semantic and
high-resolution information. On the other hand, the LP is used as a generalized
pooling function that calculates the weighted sum of multi-resolution point
features through the attention mechanism with learnable queries, in order to
extract all possible information from all available point features.
Consequently, PointStack is capable of extracting high-semantic point features
with minimal loss of information concerning granularity and non-maximum point
features. Therefore, the final aggregated point features can effectively
represent both global and local contexts of a point cloud. In addition, both
the global structure and the local shape details of a point cloud can be well
comprehended by the network head, which enables PointStack to advance the
state-of-the-art of feature learning on point clouds. The codes are available
at https://github.com/kaist-avelab/PointStack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Font Generation by Learning Fine-Grained Local Styles. (arXiv:2205.09965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09965">
<div class="article-summary-box-inner">
<span><p>Few-shot font generation (FFG), which aims to generate a new font with a few
examples, is gaining increasing attention due to the significant reduction in
labor cost. A typical FFG pipeline considers characters in a standard font
library as content glyphs and transfers them to a new target font by extracting
style information from the reference glyphs. Most existing solutions explicitly
disentangle content and style of reference glyphs globally or component-wisely.
However, the style of glyphs mainly lies in the local details, i.e. the styles
of radicals, components, and strokes together depict the style of a glyph.
Therefore, even a single character can contain different styles distributed
over spatial locations. In this paper, we propose a new font generation
approach by learning 1) the fine-grained local styles from references, and 2)
the spatial correspondence between the content and reference glyphs. Therefore,
each spatial location in the content glyph can be assigned with the right
fine-grained style. To this end, we adopt cross-attention over the
representation of the content glyphs as the queries and the representations of
the reference glyphs as the keys and values. Instead of explicitly
disentangling global or component-wise modeling, the cross-attention mechanism
can attend to the right local styles in the reference glyphs and aggregate the
reference styles into a fine-grained style representation for the given content
glyphs. The experiments show that the proposed method outperforms the
state-of-the-art methods in FFG. In particular, the user studies also
demonstrate the style consistency of our approach significantly outperforms
previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning. (arXiv:2205.09995v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09995">
<div class="article-summary-box-inner">
<span><p>Learning with little data is challenging but often inevitable in various
application scenarios where the labeled data is limited and costly. Recently,
few-shot learning (FSL) gained increasing attention because of its
generalizability of prior knowledge to new tasks that contain only a few
samples. However, for data-intensive models such as vision transformer (ViT),
current fine-tuning based FSL approaches are inefficient in knowledge
generalization and thus degenerate the downstream task performances. In this
paper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an
effective and efficient FSL on ViT model. The key idea is to apply a mask on
image patches to screen out the task-irrelevant ones and to guide the ViT to
focus on task-relevant and discriminative patches during FSL. Particularly,
MG-ViT only introduces an additional mask operation and a residual connection,
enabling the inheritance of parameters from pre-trained ViT without any other
cost. To optimally select representative few-shot samples, we also include an
active learning based sample selection method to further improve the
generalizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both
Agri-ImageNet classification task and ACFR apple detection task with
gradient-weighted class activation mapping (Grad-CAM) as the mask. The
experimental results show that the MG-ViT model significantly improves the
performance when compared with general fine-tuning based ViT models, providing
novel insights and a concrete approach towards generalizing data-intensive and
large-scale deep learning models for FSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InDistill: Transferring Knowledge From Pruned Intermediate Layers. (arXiv:2205.10003v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10003">
<div class="article-summary-box-inner">
<span><p>Deploying deep neural networks on hardware with limited resources, such as
smartphones and drones, constitutes a great challenge due to their
computational complexity. Knowledge distillation approaches aim at transferring
knowledge from a large model to a lightweight one, also known as teacher and
student respectively, while distilling the knowledge from intermediate layers
provides an additional supervision to that task. The capacity gap between the
models, the information encoding that collapses its architectural alignment,
and the absence of appropriate learning schemes for transferring multiple
layers restrict the performance of existing methods. In this paper, we propose
a novel method, termed InDistill, that can drastically improve the performance
of existing single-layer knowledge distillation methods by leveraging the
properties of channel pruning to both reduce the capacity gap between the
models and retain the architectural alignment. Furthermore, we propose a
curriculum learning based scheme for enhancing the effectiveness of
transferring knowledge from multiple intermediate layers. The proposed method
surpasses state-of-the-art performance on three benchmark image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Depth Estimation with Isometric-Self-Sample-Based Learning. (arXiv:2205.10006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10006">
<div class="article-summary-box-inner">
<span><p>Managing the dynamic regions in the photometric loss formulation has been a
main issue for handling the self-supervised depth estimation problem. Most
previous methods have alleviated this issue by removing the dynamic regions in
the photometric loss formulation based on the masks estimated from another
module, making it difficult to fully utilize the training images. In this
paper, to handle this problem, we propose an isometric self-sample-based
learning (ISSL) method to fully utilize the training images in a simple yet
effective way. The proposed method provides additional supervision during
training using self-generated images that comply with pure static scene
assumption. Specifically, the isometric self-sample generator synthesizes
self-samples for each training image by applying random rigid transformations
on the estimated depth. Thus both the generated self-samples and the
corresponding training image always follow the static scene assumption. We show
that plugging our ISSL module into several existing models consistently
improves the performance by a large margin. In addition, it also boosts the
depth accuracy over different types of scene, i.e., outdoor scenes (KITTI and
Make3D) and indoor scene (NYUv2), validating its high effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action parsing using context features. (arXiv:2205.10008v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10008">
<div class="article-summary-box-inner">
<span><p>We propose an action parsing algorithm to parse a video sequence containing
an unknown number of actions into its action segments. We argue that context
information, particularly the temporal information about other actions in the
video sequence, is valuable for action segmentation. The proposed parsing
algorithm temporally segments the video sequence into action segments. The
optimal temporal segmentation is found using a dynamic programming search
algorithm that optimizes the overall classification confidence score. The
classification score of each segment is determined using local features
calculated from that segment as well as context features calculated from other
candidate action segments of the sequence. Experimental results on the
Breakfast activity data-set showed improved segmentation accuracy compared to
existing state-of-the-art parsing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructive Interpretability with CoLabel: Corroborative Integration, Complementary Features, and Collaborative Learning. (arXiv:2205.10011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10011">
<div class="article-summary-box-inner">
<span><p>Machine learning models with explainable predictions are increasingly sought
after, especially for real-world, mission-critical applications that require
bias detection and risk mitigation. Inherent interpretability, where a model is
designed from the ground-up for interpretability, provides intuitive insights
and transparent explanations on model prediction and performance. In this
paper, we present CoLabel, an approach to build interpretable models with
explanations rooted in the ground truth. We demonstrate CoLabel in a vehicle
feature extraction application in the context of vehicle make-model recognition
(VMMR). CoLabel performs VMMR with a composite of interpretable features such
as vehicle color, type, and make, all based on interpretable annotations of the
ground truth labels. First, CoLabel performs corroborative integration to join
multiple datasets that each have a subset of desired annotations of color,
type, and make. Then, CoLabel uses decomposable branches to extract
complementary features corresponding to desired annotations. Finally, CoLabel
fuses them together for final predictions. During feature fusion, CoLabel
harmonizes complementary branches so that VMMR features are compatible with
each other and can be projected to the same semantic space for classification.
With inherent interpretability, CoLabel achieves superior performance to the
state-of-the-art black-box models, with accuracy of 0.98, 0.95, and 0.94 on
CompCars, Cars196, and BoxCars116K, respectively. CoLabel provides intuitive
explanations due to constructive interpretability, and subsequently achieves
high accuracy and usability in mission-critical situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Demographic Bias Transfer from Dataset to Model: A Case Study in Facial Expression Recognition. (arXiv:2205.10049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10049">
<div class="article-summary-box-inner">
<span><p>The increasing amount of applications of Artificial Intelligence (AI) has led
researchers to study the social impact of these technologies and evaluate their
fairness. Unfortunately, current fairness metrics are hard to apply in
multi-class multi-demographic classification problems, such as Facial
Expression Recognition (FER). We propose a new set of metrics to approach these
problems. Of the three metrics proposed, two focus on the representational and
stereotypical bias of the dataset, and the third one on the residual bias of
the trained model. These metrics combined can potentially be used to study and
compare diverse bias mitigation methods. We demonstrate the usefulness of the
metrics by applying them to a FER problem based on the popular Affectnet
dataset. Like many other datasets for FER, Affectnet is a large
Internet-sourced dataset with 291,651 labeled images. Obtaining images from the
Internet raises some concerns over the fairness of any system trained on this
data and its ability to generalize properly to diverse populations. We first
analyze the dataset and some variants, finding substantial racial bias and
gender stereotypes. We then extract several subsets with different demographic
properties and train a model on each one, observing the amount of residual bias
in the different setups. We also provide a second analysis on a different
dataset, FER+.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality. (arXiv:2205.10063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10063">
<div class="article-summary-box-inner">
<span><p>Masked AutoEncoder (MAE) has recently led the trends of visual
self-supervision area by an elegant asymmetric encoder-decoder design, which
significantly optimizes both the pre-training efficiency and fine-tuning
accuracy. Notably, the success of the asymmetric structure relies on the
"global" property of Vanilla Vision Transformer (ViT), whose self-attention
mechanism reasons over arbitrary subset of discrete image patches. However, it
is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be
adopted in MAE pre-training as they commonly introduce operators within "local"
windows, making it difficult to handle the random sequence of partial vision
tokens. In this paper, we propose Uniform Masking (UM), successfully enabling
MAE pre-training for Pyramid-based ViTs with locality (termed "UM-MAE" for
short). Specifically, UM includes a Uniform Sampling (US) that strictly samples
$1$ random patch from each $2 \times 2$ grid, and a Secondary Masking (SM)
which randomly masks a portion of (usually $25\%$) the already sampled regions
as learnable tokens. US preserves equivalent elements across multiple
non-overlapped local windows, resulting in the smooth support for popular
Pyramid-based ViTs; whilst SM is designed for better transferable visual
representations since US reduces the difficulty of pixel recovery pre-task that
hinders the semantic learning. We demonstrate that UM-MAE significantly
improves the pre-training efficiency (e.g., it speeds up and reduces the GPU
memory by $\sim 2\times$) of Pyramid-based ViTs, but maintains the competitive
fine-tuning performance across downstream tasks. For example using HTC++
detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only
in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The
codes are available at https://github.com/implus/UM-MAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Cross-Modal Knowledge Mining for Multimodal Human Activity Recognition. (arXiv:2205.10071v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10071">
<div class="article-summary-box-inner">
<span><p>Human Activity Recognition is a field of research where input data can take
many forms. Each of the possible input modalities describes human behaviour in
a different way, and each has its own strengths and weaknesses. We explore the
hypothesis that leveraging multiple modalities can lead to better recognition.
Since manual annotation of input data is expensive and time-consuming, the
emphasis is made on self-supervised methods which can learn useful feature
representations without any ground truth labels. We extend a number of recent
contrastive self-supervised approaches for the task of Human Activity
Recognition, leveraging inertial and skeleton data. Furthermore, we propose a
flexible, general-purpose framework for performing multimodal self-supervised
learning, named Contrastive Multiview Coding with Cross-Modal Knowledge Mining
(CMC-CMKM). This framework exploits modality-specific knowledge in order to
mitigate the limitations of typical self-supervised frameworks. The extensive
experiments on two widely-used datasets demonstrate that the suggested
framework significantly outperforms contrastive unimodal and multimodal
baselines on different scenarios, including fully-supervised fine-tuning,
activity retrieval and semi-supervised learning. Furthermore, it shows
performance competitive even compared to supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unintended memorisation of unique features in neural networks. (arXiv:2205.10079v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10079">
<div class="article-summary-box-inner">
<span><p>Neural networks pose a privacy risk due to their propensity to memorise and
leak training data. We show that unique features occurring only once in
training data are memorised by discriminative multi-layer perceptrons and
convolutional neural networks trained on benchmark imaging datasets. We design
our method for settings where sensitive training data is not available, for
example medical imaging. Our setting knows the unique feature, but not the
training data, model weights or the unique feature's label. We develop a score
estimating a model's sensitivity to a unique feature by comparing the KL
divergences of the model's output distributions given modified
out-of-distribution images. We find that typical strategies to prevent
overfitting do not prevent unique feature memorisation. And that images
containing a unique feature are highly influential, regardless of the influence
the images's other features. We also find a significant variation in
memorisation with training seed. These results imply that neural networks pose
a privacy risk to rarely occurring private information. This risk is more
pronounced in healthcare applications since sensitive patient information can
be memorised when it remains in training data due to an imperfect data
sanitisation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergence of Double-slit Interference by Representing Visual Space in Artificial Neural Networks. (arXiv:2205.10081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10081">
<div class="article-summary-box-inner">
<span><p>Artificial neural networks have realized incredible successes at image
recognition, but the underlying mechanism of visual space representation
remains a huge mystery. Grid cells (2014 Nobel Prize) in the entorhinal cortex
support a periodic representation as a metric for coding space. Here, we
develop a self-supervised convolutional neural network to perform visual space
location, leading to the emergence of single-slit diffraction and double-slit
interference patterns of waves. Our discoveries reveal the nature of CNN
encoding visual space to a certain extent. CNN is no longer a black box in
terms of visual spatial encoding, it is interpretable. Our findings indicate
that the periodicity property of waves provides a space metric, suggesting a
general role of spatial coordinate frame in artificial neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">People Tracking and Re-Identifying in Distributed Contexts: Extension of PoseTReID. (arXiv:2205.10086v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10086">
<div class="article-summary-box-inner">
<span><p>In our previous paper, we introduced PoseTReID which is a generic framework
for real-time 2D multi-person tracking in distributed interaction spaces where
long-term people's identities are important for other studies such as behavior
analysis, etc. In this paper, we introduce a further study of PoseTReID
framework in order to give a more complete comprehension of the framework. We
use a well-known bounding box detector YOLO (v4) for the detection to compare
to OpenPose which was used in our last paper, and we use SORT and DeepSORT to
compare to centroid which was also used previously, and most importantly for
the re-identification, we use a bunch of deep leaning methods such as MLFN,
OSNet, and OSNet-AIN with our custom classification layer to compare to FaceNet
which was also used earlier in our last paper. By evaluating on our PoseTReID
datasets, even though those deep learning re-identification methods are
designed for only short-term re-identification across multiple cameras or
videos, it is worth showing that they give impressive results which boost the
overall tracking performance of PoseTReID framework regardless the type of
tracking method. At the same time, we also introduce our research-friendly and
open source Python toolbox pyppbox, which is pure written in Python and
contains all sub-modules which are used this study along with real-time online
and offline evaluations for our PoseTReID datasets. This pyppbox is available
on GitHub https://github.com/rathaumons/pyppbox .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernel Normalized Convolutional Networks. (arXiv:2205.10089v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10089">
<div class="article-summary-box-inner">
<span><p>Existing deep convolutional neural network (CNN) architectures frequently
rely upon batch normalization (BatchNorm) to effectively train the model.
BatchNorm significantly improves model performance, but performs poorly with
smaller batch sizes. To address this limitation, we propose kernel
normalization and kernel normalized convolutional layers, and incorporate them
into kernel normalized convolutional networks (KNConvNets) as the main building
blocks. We implement KNConvNets corresponding to the state-of-the-art CNNs such
as ResNet and DenseNet while forgoing BatchNorm layers. Through extensive
experiments, we illustrate that KNConvNets consistently outperform their batch,
group, and layer normalized counterparts in terms of both accuracy and
convergence rate while maintaining competitive computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Concepts Tokenization. (arXiv:2205.10093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10093">
<div class="article-summary-box-inner">
<span><p>Obtaining the human-like perception ability of abstracting visual concepts
from concrete pixels has always been a fundamental and important target in
machine learning research fields such as disentangled representation learning
and scene decomposition. Towards this goal, we propose an unsupervised
transformer-based Visual Concepts Tokenization framework, dubbed VCT, to
perceive an image into a set of disentangled visual concept tokens, with each
concept token responding to one type of independent visual concept.
Particularly, to obtain these concept tokens, we only use cross-attention to
extract visual information from the image tokens layer by layer without
self-attention between concept tokens, preventing information leakage across
concept tokens. We further propose a Concept Disentangling Loss to facilitate
that different concept tokens represent independent visual concepts. The
cross-attention and disentangling loss play the role of induction and mutual
exclusion for the concept tokens, respectively. Extensive experiments on
several popular datasets verify the effectiveness of VCT on the tasks of
disentangled representation learning and scene decomposition. VCT achieves the
state of the art results by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion. (arXiv:2205.10101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10101">
<div class="article-summary-box-inner">
<span><p>Measuring the perceptual quality of images automatically is an essential task
in the area of computer vision, as degradations on image quality can exist in
many processes from image acquisition, transmission to enhancing. Many Image
Quality Assessment(IQA) algorithms have been designed to tackle this problem.
However, it still remains un settled due to the various types of image
distortions and the lack of large-scale human-rated datasets. In this paper, we
propose a novel algorithm based on the Swin Transformer [31] with fused
features from multiple stages, which aggregates information from both local and
global features to better predict the quality. To address the issues of
small-scale datasets, relative rankings of images have been taken into account
together with regression loss to simultaneously optimize the model.
Furthermore, effective data augmentation strategies are also used to improve
the performance. In comparisons with previous works, experiments are carried
out on two standard IQA datasets and a challenge dataset. The results
demonstrate the effectiveness of our work. The proposed method outperforms
other methods on standard datasets and ranks 2nd in the no-reference track of
NTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that
our method is promising in solving diverse IQA problems and thus can be used to
real-word applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging. (arXiv:2205.10102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10102">
<div class="article-summary-box-inner">
<span><p>In coded aperture snapshot spectral compressive imaging (CASSI) systems,
hyperspectral image (HSI) reconstruction methods are employed to recover the
spatial-spectral signal from a compressed measurement. Among these algorithms,
deep unfolding methods demonstrate promising performance but suffer from two
issues. Firstly, they do not estimate the degradation patterns and
ill-posedness degree from the highly related CASSI to guide the iterative
learning. Secondly, they are mainly CNN-based, showing limitations in capturing
long-range dependencies. In this paper, we propose a principled
Degradation-Aware Unfolding Framework (DAUF) that estimates parameters from the
compressed image and physical mask, and then uses these parameters to control
each iteration. Moreover, we customize a novel Half-Shuffle Transformer (HST)
that simultaneously captures local contents and non-local dependencies. By
plugging HST into DAUF, we establish the first Transformer-based deep unfolding
method, Degradation-Aware Unfolding Half-Shuffle Transformer (DAUHST), for HSI
reconstruction. Experiments show that DAUHST significantly surpasses
state-of-the-art methods while requiring cheaper computational and memory
costs. Code and models will be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Preserving Image Registration. (arXiv:2205.10120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10120">
<div class="article-summary-box-inner">
<span><p>Image registration is a key task in medical imaging applications, allowing to
represent medical images in a common spatial reference frame. Current
literature on image registration is generally based on the assumption that
images are usually accessible to the researcher, from which the spatial
transformation is subsequently estimated. This common assumption may not be met
in current practical applications, since the sensitive nature of medical images
may ultimately require their analysis under privacy constraints, preventing to
share the image content in clear form. In this work, we formulate the problem
of image registration under a privacy preserving regime, where images are
assumed to be confidential and cannot be disclosed in clear. We derive our
privacy preserving image registration framework by extending classical
registration paradigms to account for advanced cryptographic tools, such as
secure multi-party computation and homomorphic encryption, that enable the
execution of operations without leaking the underlying data. To overcome the
problem of performance and scalability of cryptographic tools in high
dimensions, we first propose to optimize the underlying image registration
operations using gradient approximations. We further revisit the use of
homomorphic encryption and use a packing method to allow the encryption and
multiplication of large matrices more efficiently. We demonstrate our privacy
preserving framework in linear and non-linear registration problems, evaluating
its accuracy and scalability with respect to standard image registration. Our
results show that privacy preserving image registration is feasible and can be
adopted in sensitive medical imaging applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliability-based Mesh-to-Grid Image Reconstruction. (arXiv:2205.10138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10138">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel method for the reconstruction of images from
samples located at non-integer positions, called mesh. This is a common
scenario for many image processing applications, such as super-resolution,
warping or virtual view generation in multi-camera systems. The proposed method
relies on a set of initial estimates that are later refined by a new
reliability-based content-adaptive framework that employs denoising in order to
reduce the reconstruction error. The reliability of the initial estimate is
computed so stronger denoising is applied to less reliable estimates. The
proposed technique can improve the reconstruction quality by more than 2 dB (in
terms of PSNR) with respect to the initial estimate and it outperforms the
state-of-the-art denoising-based refinement by up to 0.7 dB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The developmental trajectory of object recognition robustness: children are like small adults but unlike big deep neural networks. (arXiv:2205.10144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10144">
<div class="article-summary-box-inner">
<span><p>In laboratory object recognition tasks based on undistorted photographs, both
adult humans and Deep Neural Networks (DNNs) perform close to ceiling. Unlike
adults', whose object recognition performance is robust against a wide range of
image distortions, DNNs trained on standard ImageNet (1.3M images) perform
poorly on distorted images. However, the last two years have seen impressive
gains in DNN distortion robustness, predominantly achieved through
ever-increasing large-scale datasets$\unicode{x2014}$orders of magnitude larger
than ImageNet. While this simple brute-force approach is very effective in
achieving human-level robustness in DNNs, it raises the question of whether
human robustness, too, is simply due to extensive experience with (distorted)
visual input during childhood and beyond. Here we investigate this question by
comparing the core object recognition performance of 146 children (aged
4$\unicode{x2013}$15) against adults and against DNNs. We find, first, that
already 4$\unicode{x2013}$6 year-olds showed remarkable robustness to image
distortions and outperform DNNs trained on ImageNet. Second, we estimated the
number of $\unicode{x201C}$images$\unicode{x201D}$ children have been exposed
to during their lifetime. Compared to various DNNs, children's high robustness
requires relatively little data. Third, when recognizing objects
children$\unicode{x2014}$like adults but unlike DNNs$\unicode{x2014}$rely
heavily on shape but not on texture cues. Together our results suggest that the
remarkable robustness to distortions emerges early in the developmental
trajectory of human object recognition and is unlikely the result of a mere
accumulation of experience with distorted visual input. Even though current
DNNs match human performance regarding robustness they seem to rely on
different and more data-hungry strategies to do so.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swapping Semantic Contents for Mixing Images. (arXiv:2205.10158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10158">
<div class="article-summary-box-inner">
<span><p>Deep architecture have proven capable of solving many tasks provided a
sufficient amount of labeled data. In fact, the amount of available labeled
data has become the principal bottleneck in low label settings such as
Semi-Supervised Learning. Mixing Data Augmentations do not typically yield new
labeled samples, as indiscriminately mixing contents creates between-class
samples. In this work, we introduce the SciMix framework that can learn to
generator to embed a semantic style code into image backgrounds, we obtain new
mixing scheme for data augmentation. We then demonstrate that SciMix yields
novel mixed samples that inherit many characteristics from their non-semantic
parents. Afterwards, we verify those samples can be used to improve the
performance semi-supervised frameworks like Mean Teacher or Fixmatch, and even
fully supervised learning on a small labeled dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Generation of Synthetic Images of Palm Vein Patterns: A Review. (arXiv:2205.10179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10179">
<div class="article-summary-box-inner">
<span><p>With the recent success of computer vision and deep learning, remarkable
progress has been achieved on automatic personal recognition using vein
biometrics. However, collecting large-scale real-world training data for palm
vein recognition has turned out to be challenging, mainly due to the noise and
irregular variations included at the time of acquisition. Meanwhile, existing
palm vein recognition datasets are usually collected under near-infrared light,
lacking detailed annotations on attributes (e.g., pose), so the influences of
different attributes on vein recognition have been poorly investigated.
Therefore, this paper examines the suitability of synthetic vein images
generated to compensate for the urgent lack of publicly available large-scale
datasets. Firstly, we present an overview of recent research progress on palm
vein recognition, from the basic background knowledge to vein anatomical
structure, data acquisition, public database, and quality assessment
procedures. Then, we focus on the state-of-the-art methods that have allowed
the generation of vascular structures for biometric purposes and the modeling
of biological networks with their respective application domains. In addition,
we review the existing research on the generation of style transfer and
biological nature-based synthetic palm vein image algorithms. Afterward, we
formalize a general flowchart for the creation of a synthetic database
comparing real palm vein images and generated synthetic samples to obtain some
understanding into the development of the realistic vein imaging system.
Ultimately, we conclude by discussing the challenges, insights, and future
perspectives in generating synthetic palm vein images for further works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-Scooter Rider Detection and Classification in Dense Urban Environments. (arXiv:2205.10184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10184">
<div class="article-summary-box-inner">
<span><p>Accurate detection and classification of vulnerable road users is a safety
critical requirement for the deployment of autonomous vehicles in heterogeneous
traffic. Although similar in physical appearance to pedestrians, e-scooter
riders follow distinctly different characteristics of movement and can reach
speeds of up to 45kmph. The challenge of detecting e-scooter riders is
exacerbated in urban environments where the frequency of partial occlusion is
increased as riders navigate between vehicles, traffic infrastructure and other
road users. This can lead to the non-detection or mis-classification of
e-scooter riders as pedestrians, providing inaccurate information for accident
mitigation and path planning in autonomous vehicle applications. This research
introduces a novel benchmark for partially occluded e-scooter rider detection
to facilitate the objective characterization of detection models. A novel,
occlusion-aware method of e-scooter rider detection is presented that achieves
a 15.93% improvement in detection performance over the current state of the
art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration. (arXiv:2205.10195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10195">
<div class="article-summary-box-inner">
<span><p>How to properly model the inter-frame relation within the video sequence is
an important but unsolved challenge for video restoration (VR). In this work,
we propose an unsupervised flow-aligned sequence-to-sequence model (S2SVR) to
address this problem. On the one hand, the sequence-to-sequence model, which
has proven capable of sequence modeling in the field of natural language
processing, is explored for the first time in VR. Optimized serialization
modeling shows potential in capturing long-range dependencies among frames. On
the other hand, we equip the sequence-to-sequence model with an unsupervised
optical flow estimator to maximize its potential. The flow estimator is trained
with our proposed unsupervised distillation loss, which can alleviate the data
discrepancy and inaccurate degraded optical flow issues of previous flow-based
methods. With reliable optical flow, we can establish accurate correspondence
among multiple frames, narrowing the domain difference between 1D language and
2D misaligned frames and improving the potential of the sequence-to-sequence
model. S2SVR shows superior performance in multiple VR tasks, including video
deblurring, video super-resolution, and compressed video quality enhancement.
Code and models are publicly available at
https://github.com/linjing7/VR-Baseline
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Underwater Image Enhancement and Improved Underwater Biological Detection Pipeline. (arXiv:2205.10199v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10199">
<div class="article-summary-box-inner">
<span><p>For aquaculture resource evaluation and ecological environment monitoring,
automatic detection and identification of marine organisms is critical.
However, due to the low quality of underwater images and the characteristics of
underwater biological, a lack of abundant features may impede traditional
hand-designed feature extraction approaches or CNN-based object detection
algorithms, particularly in complex underwater environment. Therefore, the goal
of this paper is to perform object detection in the underwater environment.
This paper proposed a novel method for capturing feature information, which
adds the convolutional block attention module (CBAM) to the YOLOv5 backbone.
The interference of underwater creature characteristics on object
characteristics is decreased, and the output of the backbone network to object
information is enhanced. In addition, the self-adaptive global histogram
stretching algorithm (SAGHS) is designed to eliminate the degradation problems
such as low contrast and color loss caused by underwater environmental
information to better restore image quality. Extensive experiments and
comprehensive evaluation on the URPC2021 benchmark dataset demonstrate the
effectiveness and adaptivity of our methods. Beyond that, this paper conducts
an exhaustive analysis of the role of training data on performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Guide Adaptive Depth Sampling?. (arXiv:2205.10202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10202">
<div class="article-summary-box-inner">
<span><p>Recent advances in depth sensing technologies allow fast electronic
maneuvering of the laser beam, as opposed to fixed mechanical rotations. This
will enable future sensors, in principle, to vary in real-time the sampling
pattern. We examine here the abstract problem of whether adapting the sampling
pattern for a given frame can reduce the reconstruction error or allow a
sparser pattern. We propose a constructive generic method to guide adaptive
depth sampling algorithms.
</p>
<p>Given a sampling budget B, a depth predictor P and a desired quality measure
M, we propose an Importance Map that highlights important sampling locations.
This map is defined for a given frame as the per-pixel expected value of M
produced by the predictor P, given a pattern of B random samples. This map can
be well estimated in a training phase. We show that a neural network can learn
to produce a highly faithful Importance Map, given an RGB image. We then
suggest an algorithm to produce a sampling pattern for the scene, which is
denser in regions that are harder to reconstruct. The sampling strategy of our
modular framework can be adjusted according to hardware limitations, type of
depth predictor, and any custom reconstruction error measure that should be
minimized. We validate through simulations that our approach outperforms grid
and random sampling patterns as well as recent state-of-the-art adaptive
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Count Anything: Reference-less Class-agnostic Counting with Weak Supervision. (arXiv:2205.10203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10203">
<div class="article-summary-box-inner">
<span><p>Object counting is a seemingly simple task with diverse real-world
applications. Most counting methods focus on counting instances of specific,
known classes. While there are class-agnostic counting methods that can
generalise to unseen classes, these methods require reference images to define
the type of object to be counted, as well as instance annotations during
training. We identify that counting is, at its core, a repetition-recognition
task and show that a general feature space, with global context, is sufficient
to enumerate instances in an image without a prior on the object type present.
Specifically, we demonstrate that self-supervised vision transformer features
combined with a lightweight count regression head achieve competitive results
when compared to other class-agnostic counting tasks without the need for
point-level supervision or reference images. Our method thus facilitates
counting on a constantly changing set composition. To the best of our
knowledge, we are both the first reference-less class-agnostic counting method
as well as the first weakly-supervised class-agnostic counting method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-time Batch Normalization. (arXiv:2205.10210v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10210">
<div class="article-summary-box-inner">
<span><p>Deep neural networks often suffer the data distribution shift between
training and testing, and the batch statistics are observed to reflect the
shift. In this paper, targeting of alleviating distribution shift in test time,
we revisit the batch normalization (BN) in the training process and reveals two
key insights benefiting test-time optimization: $(i)$ preserving the same
gradient backpropagation form as training, and $(ii)$ using dataset-level
statistics for robust optimization and inference. Based on the two insights, we
propose a novel test-time BN layer design, GpreBN, which is optimized during
testing by minimizing Entropy loss. We verify the effectiveness of our method
on two typical settings with distribution shift, i.e., domain generalization
and robustness tasks. Our GpreBN significantly improves the test-time
performance and achieves the state of the art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. (arXiv:2205.10218v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10218">
<div class="article-summary-box-inner">
<span><p>Generalization across different environments with the same tasks is critical
for successful applications of visual reinforcement learning (RL) in real
scenarios. However, visual distractions -- which are common in real scenes --
from high-dimensional observations can be hurtful to the learned
representations in visual RL, thus degrading the performance of generalization.
To tackle this problem, we propose a novel approach, namely Characteristic
Reward Sequence Prediction (CRESP), to extract the task-relevant information by
learning reward sequence distributions (RSDs), as the reward signals are
task-relevant in RL and invariant to visual distractions. Specifically, to
effectively capture the task-relevant information via RSDs, CRESP introduces an
auxiliary task -- that is, predicting the characteristic functions of RSDs --
to learn task-relevant representations, because we can well approximate the
high-dimensional distributions by leveraging the corresponding characteristic
functions. Experiments demonstrate that CRESP significantly improves the
performance of generalization on unseen environments, outperforming several
state-of-the-arts on DeepMind Control tasks with different visual distractions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mosaic Zonotope Shadow Matching for Risk-Aware Autonomous Localization in Harsh Urban Environments. (arXiv:2205.10223v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10223">
<div class="article-summary-box-inner">
<span><p>Risk-aware urban localization with the Global Navigation Satellite System
(GNSS) remains an unsolved problem with frequent misdetection of the user's
street or side of the street. Significant advances in 3D map-aided GNSS use
grid-based GNSS shadow matching alongside AI-driven line-of-sight (LOS)
classifiers and server-based processing to improve localization accuracy,
especially in the cross-street direction. Our prior work introduces a new
paradigm for shadow matching that proposes set-valued localization with
computationally efficient zonotope set representations. While existing
literature improved accuracy and efficiency, the current state of shadow
matching theory does not address the needs of risk-aware autonomous systems. We
extend our prior work to propose Mosaic Zonotope Shadow Matching (MZSM) that
employs a classifier-agnostic polytope mosaic architecture to provide
risk-awareness and certifiable guarantees on urban positioning. We formulate a
recursively expanding binary tree that refines an initial location estimate
with set operations into smaller polytopes. Together, the smaller polytopes
form a mosaic. We weight the tree branches with the probability that the user
is in line of sight of the satellite and expand the tree with each new
satellite observation. Our method yields an exact shadow matching distribution
from which we guarantee uncertainty bounds on the user localization. We perform
high-fidelity simulations using a 3D building map of San Francisco to validate
our algorithm's risk-aware improvements. We demonstrate that MZSM provides
certifiable guarantees across varied data-driven LOS classifier accuracies and
yields a more precise understanding of the uncertainty over existing methods.
We validate that our tree-based construction is efficient and tractable,
computing a mosaic from 14 satellites in 0.63 seconds and growing quadratically
in the satellite number.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Demographic Attribute Guided Approach to Age Estimation. (arXiv:2205.10254v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10254">
<div class="article-summary-box-inner">
<span><p>Face-based age estimation has attracted enormous attention due to wide
applications to public security surveillance, human-computer interaction, etc.
With vigorous development of deep learning, age estimation based on deep neural
network has become the mainstream practice. However, seeking a more suitable
problem paradigm for age change characteristics, designing the corresponding
loss function and designing a more effective feature extraction module still
needs to be studied. What is more, change of face age is also related to
demographic attributes such as ethnicity and gender, and the dynamics of
different age groups is also quite different. This problem has so far not been
paid enough attention to. How to use demographic attribute information to
improve the performance of age estimation remains to be further explored. In
light of these issues, this research makes full use of auxiliary information of
face attributes and proposes a new age estimation approach with an attribute
guidance module. We first design a multi-scale attention residual convolution
unit (MARCU) to extract robust facial features other than simply using other
standard feature modules such as VGG and ResNet. Then, after being especially
treated through full connection (FC) layers, the facial demographic attributes
are weight-summed by 1*1 convolutional layer and eventually merged with the age
features by a global FC layer. Lastly, we propose a new error compression
ranking (ECR) loss to better converge the age regression value. Experimental
results on three public datasets of UTKFace, LAP2016 and Morph show that our
proposed approach achieves superior performance compared to other
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Co-Laughter Gesture Relationship on RGB videos in Dyadic Conversation Contex. (arXiv:2205.10266v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10266">
<div class="article-summary-box-inner">
<span><p>The development of virtual agents has enabled human-avatar interactions to
become increasingly rich and varied. Moreover, an expressive virtual agent i.e.
that mimics the natural expression of emotions, enhances social interaction
between a user (human) and an agent (intelligent machine). The set of
non-verbal behaviors of a virtual character is, therefore, an important
component in the context of human-machine interaction. Laughter is not just an
audio signal, but an intrinsic relationship of multimodal non-verbal
communication, in addition to audio, it includes facial expressions and body
movements. Motion analysis often relies on a relevant motion capture dataset,
but the main issue is that the acquisition of such a dataset is expensive and
time-consuming. This work studies the relationship between laughter and body
movements in dyadic conversations. The body movements were extracted from
videos using deep learning based pose estimator model. We found that, in the
explored NDC-ME dataset, a single statistical feature (i.e, the maximum value,
or the maximum of Fourier transform) of a joint movement weakly correlates with
laughter intensity by 30%. However, we did not find a direct correlation
between audio features and body movements. We discuss about the challenges to
use such dataset for the audio-driven co-laughter motion synthesis task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">B-cos Networks: Alignment is All We Need for Interpretability. (arXiv:2205.10268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10268">
<div class="article-summary-box-inner">
<span><p>We present a new direction for increasing the interpretability of deep neural
networks (DNNs) by promoting weight-input alignment during training. For this,
we propose to replace the linear transforms in DNNs by our B-cos transform. As
we show, a sequence (network) of such transforms induces a single linear
transform that faithfully summarises the full model computations. Moreover, the
B-cos transform introduces alignment pressure on the weights during
optimisation. As a result, those induced linear transforms become highly
interpretable and align with task-relevant features. Importantly, the B-cos
transform is designed to be compatible with existing architectures and we show
that it can easily be integrated into common models such as VGGs, ResNets,
InceptionNets, and DenseNets, whilst maintaining similar performance on
ImageNet. The resulting explanations are of high visual quality and perform
well under quantitative metrics for interpretability. Code available at
https://www.github.com/moboehle/B-cos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression ensembles quantify aesthetic complexity and the evolution of visual art. (arXiv:2205.10271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10271">
<div class="article-summary-box-inner">
<span><p>The quantification of visual aesthetics and complexity have a long history,
the latter previously operationalized via the application of compression
algorithms. Here we generalize and extend the compression approach beyond
simple complexity measures to quantify algorithmic distance in historical and
contemporary visual media. The proposed "ensemble" approach works by
compressing a large number of transformed versions of a given input image,
resulting in a vector of associated compression ratios. This approach is more
efficient than other compression-based algorithmic distances, and is
particularly suited for the quantitative analysis of visual artifacts, because
human creative processes can be understood as algorithms in the broadest sense.
Unlike comparable image embedding methods using machine learning, our approach
is fully explainable through the transformations. We demonstrate that the
method is cognitively plausible and fit for purpose by evaluating it against
human complexity judgments, and on automated detection tasks of authorship and
style. We show how the approach can be used to reveal and quantify trends in
art historical data, both on the scale of centuries and in rapidly evolving
contemporary NFT art markets. We further quantify temporal resemblance to
disambiguate artists outside the documented mainstream from those who are
deeply embedded in Zeitgeist. Finally, we note that compression ensembles
constitute a quantitative representation of the concept of visual family
resemblance, as distinct sets of dimensions correspond to shared visual
characteristics otherwise hard to pin down. Our approach provides a new
perspective for the study of visual art, algorithmic image analysis, and
quantitative aesthetics more generally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Skin Lesion Segmentation via Dilated Scale-Wise Feature Fusion Network. (arXiv:2205.10272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10272">
<div class="article-summary-box-inner">
<span><p>Skin lesion detection in dermoscopic images is essential in the accurate and
early diagnosis of skin cancer by a computerized apparatus. Current skin lesion
segmentation approaches show poor performance in challenging circumstances such
as indistinct lesion boundaries, low contrast between the lesion and the
surrounding area, or heterogeneous background that causes over/under
segmentation of the skin lesion. To accurately recognize the lesion from the
neighboring regions, we propose a dilated scale-wise feature fusion network
based on convolution factorization. Our network is designed to simultaneously
extract features at different scales which are systematically fused for better
detection. The proposed model has satisfactory accuracy and efficiency. Various
experiments for lesion segmentation are performed along with comparisons with
the state-of-the-art models. Our proposed model consistently showcases
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors. (arXiv:2205.10279v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10279">
<div class="article-summary-box-inner">
<span><p>Deep learning is increasingly moving towards a transfer learning paradigm
whereby large foundation models are fine-tuned on downstream tasks, starting
from an initialization learned on the source task. But an initialization
contains relatively little information about the source task. Instead, we show
that we can learn highly informative posteriors from the source task, through
supervised or self-supervised approaches, which then serve as the basis for
priors that modify the whole loss surface on the downstream task. This simple
modular approach enables significant performance gains and more data-efficient
learning on a variety of downstream classification and segmentation tasks,
serving as a drop-in replacement for standard pre-training strategies. These
highly informative priors also can be saved for future use, similar to
pre-trained weights, and stand in contrast to the zero-mean isotropic
uninformative priors that are typically used in Bayesian deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User Localization using RF Sensing: A Performance comparison between LIS and mmWave Radars. (arXiv:2205.10321v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10321">
<div class="article-summary-box-inner">
<span><p>Since electromagnetic signals are omnipresent, Radio Frequency (RF)-sensing
has the potential to become a universal sensing mechanism with applications in
localization, smart-home, retail, gesture recognition, intrusion detection,
etc. Two emerging technologies in RF-sensing, namely sensing through Large
Intelligent Surfaces (LISs) and mmWave Frequency-Modulated Continuous-Wave
(FMCW) radars, have been successfully applied to a wide range of applications.
In this work, we compare LIS and mmWave radars for localization in real-world
and simulated environments. In our experiments, the mmWave radar achieves 0.71
Intersection Over Union (IOU) and 3cm error for bounding boxes, while LIS has
0.56 IOU and 10cm distance error. Although the radar outperforms the LIS in
terms of accuracy, LIS features additional applications in communication in
addition to sensing scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UCC: Uncertainty guided Cross-head Co-training for Semi-Supervised Semantic Segmentation. (arXiv:2205.10334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10334">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have witnessed great successes in semantic
segmentation, which requires a large number of labeled data for training. We
present a novel learning framework called Uncertainty guided Cross-head
Co-training (UCC) for semi-supervised semantic segmentation. Our framework
introduces weak and strong augmentations within a shared encoder to achieve
co-training, which naturally combines the benefits of consistency and
self-training. Every segmentation head interacts with its peers and, the weak
augmentation result is used for supervising the strong. The consistency
training samples' diversity can be boosted by Dynamic Cross-Set Copy-Paste
(DCSCP), which also alleviates the distribution mismatch and class imbalance
problems. Moreover, our proposed Uncertainty Guided Re-weight Module (UGRM)
enhances the self-training pseudo labels by suppressing the effect of the
low-quality pseudo labels from its peer via modeling uncertainty. Extensive
experiments on Cityscapes and PASCAL VOC 2012 demonstrate the effectiveness of
our UCC. Our approach significantly outperforms other state-of-the-art
semi-supervised semantic segmentation methods. It achieves 77.17$\%$, 76.49$\%$
mIoU on Cityscapes and PASCAL VOC 2012 datasets respectively under 1/16
protocols, which are +10.1$\%$, +7.91$\%$ better than the supervised baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes. (arXiv:2205.10337v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10337">
<div class="article-summary-box-inner">
<span><p>We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient visual object representation using a biologically plausible spike-latency code and winner-take-all inhibition. (arXiv:2205.10338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10338">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have surpassed human performance in key visual
challenges such as object recognition, but require a large amount of energy,
computation, and memory. In contrast, spiking neural networks (SNNs) have the
potential to improve both the efficiency and biological plausibility of object
recognition systems. Here we present a SNN model that uses spike-latency coding
and winner-take-all inhibition (WTA-I) to efficiently represent visual stimuli
from the Fashion MNIST dataset. Stimuli were preprocessed with center-surround
receptive fields and then fed to a layer of spiking neurons whose synaptic
weights were updated using spike-timing-dependent-plasticity (STDP). We
investigate how the quality of the represented objects changes under different
WTA-I schemes and demonstrate that a network of 150 spiking neurons can
efficiently represent objects with as little as 40 spikes. Studying how core
object recognition may be implemented using biologically plausible learning
rules in SNNs may not only further our understanding of the brain, but also
lead to novel and efficient artificial vision systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT). (arXiv:2205.10342v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10342">
<div class="article-summary-box-inner">
<span><p>Vision transformers, with their ability to more efficiently model long-range
context, have demonstrated impressive accuracy gains in several computer vision
and medical image analysis tasks including segmentation. However, such methods
need large labeled datasets for training, which is hard to obtain for medical
image analysis. Self-supervised learning (SSL) has demonstrated success in
medical image segmentation using convolutional networks. In this work, we
developed a \underline{s}elf-distillation learning with \underline{m}asked
\underline{i}mage modeling method to perform SSL for vision
\underline{t}ransformers (SMIT) applied to 3D multi-organ segmentation from CT
and MRI. Our contribution is a dense pixel-wise regression within masked
patches called masked image prediction, which we combined with masked patch
token distillation as pretext task to pre-train vision transformers. We show
our approach is more accurate and requires fewer fine tuning datasets than
other pretext tasks. Unlike prior medical image methods, which typically used
image sets arising from disease sites and imaging modalities corresponding to
the target tasks, we used 3,643 CT scans (602,708 images) arising from head and
neck, lung, and kidney cancers as well as COVID-19 for pre-training and applied
it to abdominal organs segmentation from MRI pancreatic cancer patients as well
as publicly available 13 different abdominal organs segmentation from CT. Our
method showed clear accuracy improvement (average DSC of 0.875 from MRI and
0.878 from CT) with reduced requirement for fine-tuning datasets over commonly
used pretext tasks. Extensive comparisons against multiple current SSL methods
were done. Code will be made available upon acceptance for publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse super-resolution with pretrained deep hiererarchical VAEs. (arXiv:2205.10347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10347">
<div class="article-summary-box-inner">
<span><p>Image super-resolution is a one-to-many problem, but most deep-learning based
methods only provide one single solution to this problem. In this work, we
tackle the problem of diverse super-resolution by reusing VD-VAE, a
state-of-the art variational autoencoder (VAE). We find that the hierarchical
latent representation learned by VD-VAE naturally separates the image
low-frequency information, encoded in the latent groups at the top of the
hierarchy, from the image high-frequency details, determined by the latent
groups at the bottom of the latent hierarchy. Starting from this observation,
we design a super-resolution model exploiting the specific structure of VD-VAE
latent space. Specifically, we train an encoder to encode low-resolution images
in the subset of VD-VAE latent space encoding the low-frequency information,
and we combine this encoder with VD-VAE generative model to sample diverse
super-resolved version of a low-resolution input. We demonstrate the ability of
our method to generate diverse solutions to the super-resolution problem on
face super-resolution with upsampling factors x4, x8, and x16.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching StyleGAN with Illumination Physics. (arXiv:2205.10351v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10351">
<div class="article-summary-box-inner">
<span><p>StyleGAN generates novel images of a scene from latent codes which are
impressively disentangled. But StyleGAN generates images that are "like" its
training set. This paper shows how to use simple physical properties of images
to enrich StyleGAN's generation capacity. We use an intrinsic image method to
decompose an image, then search the latent space of a pretrained StyleGAN to
find novel directions that fix one component (say, albedo) and vary another
(say, shading). Therefore, we can change the lighting of a complex scene
without changing the scene layout, object colors, and shapes. Or we can change
the colors of objects without changing shading intensity or their scene layout.
Our experiments suggest the proposed method, StyLitGAN, can add and remove
luminaires in the scene and generate images with realistic lighting effects --
cast shadows, soft shadows, inter-reflections, glossy effects -- requiring no
labeled paired relighting data or any other geometric supervision. Qualitative
evaluation confirms that our generated images are realistic and that we can
change or fix components at will. Quantitative evaluation shows that
pre-trained StyleGAN could not produce the images StyLitGAN produces; we can
automatically generate realistic out-of-distribution images, and so can
significantly enrich the range of images StyleGAN can produce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognizing License Plates in Real-Time. (arXiv:1906.04376v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.04376">
<div class="article-summary-box-inner">
<span><p>License plate detection and recognition (LPDR) is of growing importance for
enabling intelligent transportation and ensuring the security and safety of the
cities. However, LPDR faces a big challenge in a practical environment. The
license plates can have extremely diverse sizes, fonts and colors, and the
plate images are usually of poor quality caused by skewed capturing angles,
uneven lighting, occlusion, and blurring. In applications such as surveillance,
it often requires fast processing. To enable real-time and accurate license
plate recognition, in this work, we propose a set of techniques: 1) a contour
reconstruction method along with edge-detection to quickly detect the candidate
plates; 2) a simple zero-one-alternation scheme to effectively remove the fake
top and bottom borders around plates to facilitate more accurate segmentation
of characters on plates; 3) a set of techniques to augment the training data,
incorporate SIFT features into the CNN network, and exploit transfer learning
to obtain the initial parameters for more effective training; and 4) a
two-phase verification procedure to determine the correct plate at low cost, a
statistical filtering in the plate detection stage to quickly remove unwanted
candidates, and the accurate CR results after the CR process to perform further
plate verification without additional processing. We implement a complete LPDR
system based on our algorithms. The experimental results demonstrate that our
system can accurately recognize license plate in real-time. Additionally, it
works robustly under various levels of illumination and noise, and in the
presence of car movement. Compared to peer schemes, our system is not only
among the most accurate ones but is also the fastest, and can be easily applied
to other scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer-Wise Data-Free CNN Compression. (arXiv:2011.09058v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09058">
<div class="article-summary-box-inner">
<span><p>We present a computationally efficient method for compressing a trained
neural network without using real data. We break the problem of data-free
network compression into independent layer-wise compressions. We show how to
efficiently generate layer-wise training data using only a pretrained network.
We use this data to perform independent layer-wise compressions on the
pretrained network. We also show how to precondition the network to improve the
accuracy of our layer-wise compression method. We present results for
layer-wise compression using quantization and pruning. When quantizing, we
compress with higher accuracy than related works while using orders of
magnitude less compute. When compressing MobileNetV2 and evaluating on
ImageNet, our method outperforms existing methods for quantization at all
bit-widths, achieving a $+0.34\%$ improvement in $8$-bit quantization, and a
stronger improvement at lower bit-widths (up to a $+28.50\%$ improvement at $5$
bits). When pruning, we outperform baselines of a similar compute envelope,
achieving $1.5$ times the sparsity rate at the same accuracy. We also show how
to combine our efficient method with high-compute generative methods to improve
upon their results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flow-based Spatio-Temporal Structured Prediction of Dynamics. (arXiv:2104.04391v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04391">
<div class="article-summary-box-inner">
<span><p>Conditional Normalizing Flows (CNFs) are flexible generative models capable
of representing complicated distributions with high dimensionality and large
interdimensional correlations, making them appealing for structured output
learning. Their effectiveness in modelling multivariates spatio-temporal
structured data has yet to be completely investigated. We propose MotionFlow as
a novel normalizing flows approach that autoregressively conditions the output
distributions on the spatio-temporal input features. It combines deterministic
and stochastic representations with CNFs to create a probabilistic neural
generative approach that can model the variability seen in high-dimensional
structured spatio-temporal data. We specifically propose to use conditional
priors to factorize the latent space for the time dependent modeling. We also
exploit the use of masked convolutions as autoregressive conditionals in CNFs.
As a result, our method is able to define arbitrarily expressive output
probability distributions under temporal dynamics in multivariate prediction
tasks. We apply our method to different tasks, including trajectory prediction,
motion prediction, time series forecasting, and binary segmentation, and
demonstrate that our model is able to leverage normalizing flows to learn
complicated time dependent conditional distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement. (arXiv:2104.08223v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08223">
<div class="article-summary-box-inner">
<span><p>This paper presents a generic method for generating full facial 3D animation
from speech. Existing approaches to audio-driven facial animation exhibit
uncanny or static upper face animation, fail to produce accurate and plausible
co-articulation or rely on person-specific models that limit their scalability.
To improve upon existing models, we propose a generic audio-driven facial
animation approach that achieves highly realistic motion synthesis results for
the entire face. At the core of our approach is a categorical latent space for
facial animation that disentangles audio-correlated and audio-uncorrelated
information based on a novel cross-modality loss. Our approach ensures highly
accurate lip motion, while also synthesizing plausible animation of the parts
of the face that are uncorrelated to the audio signal, such as eye blinks and
eye brow motion. We demonstrate that our approach outperforms several baselines
and obtains state-of-the-art quality both qualitatively and quantitatively. A
perceptual user study demonstrates that our approach is deemed more realistic
than the current state-of-the-art in over 75% of cases. We recommend watching
the supplemental video before reading the paper:
https://github.com/facebookresearch/meshtalk
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Algorithmic Stability in Unsupervised Representation Learning. (arXiv:2106.05238v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05238">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the algorithmic stability of unsupervised
representation learning with deep generative models, as a function of repeated
re-training on the same input data. Algorithms for learning low dimensional
linear representations -- for example principal components analysis (PCA), or
linear independent components analysis (ICA) -- come with guarantees that they
will always reveal the same latent representations (perhaps up to an arbitrary
rotation or permutation). Unfortunately, for non-linear representation
learning, such as in a variational auto-encoder (VAE) model trained by
stochastic gradient descent, we have no such guarantees. Recent work on
identifiability in non-linear ICA have introduced a family of deep generative
models that have identifiable latent representations, achieved by conditioning
on side information (e.g. informative labels). We empirically evaluate the
stability of these models under repeated re-estimation of parameters, and
compare them to both standard VAEs and deep generative models which learn to
cluster in their latent space. Surprisingly, we discover side information is
not necessary for algorithmic stability: using standard quantitative measures
of identifiability, we find deep generative models with latent clusterings are
empirically identifiable to the same degree as models which rely on auxiliary
labels. We relate these results to the possibility of identifiable non-linear
ICA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesis in Style: Semantic Segmentation of Historical Documents using Synthetic Data. (arXiv:2107.06777v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06777">
<div class="article-summary-box-inner">
<span><p>One of the most pressing problems in the automated analysis of historical
documents is the availability of annotated training data. The problem is that
labeling samples is a time-consuming task because it requires human expertise
and thus, cannot be automated well. In this work, we propose a novel method to
construct synthetic labeled datasets for historical documents where no
annotations are available. We train a StyleGAN model to synthesize document
images that capture the core features of the original documents. While
originally, the StyleGAN architecture was not intended to produce labels, it
indirectly learns the underlying semantics to generate realistic images. Using
our approach, we can extract the semantic information from the intermediate
feature maps and use it to generate ground truth labels. To investigate if our
synthetic dataset can be used to segment the text in historical documents, we
use it to train multiple supervised segmentation models and evaluate their
performance. We also train these models on another dataset created by a
state-of-the-art synthesis approach to show that the models trained on our
dataset achieve better results while requiring even less human annotation
effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Dependency Guided Contrastive Learning for Multiple Labeling in Prenatal Ultrasound. (arXiv:2108.05055v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05055">
<div class="article-summary-box-inner">
<span><p>Standard plane recognition plays an important role in prenatal ultrasound
(US) screening. Automatically recognizing the standard plane along with the
corresponding anatomical structures in US image can not only facilitate US
image interpretation but also improve diagnostic efficiency. In this study, we
build a novel multi-label learning (MLL) scheme to identify multiple standard
planes and corresponding anatomical structures of fetus simultaneously. Our
contribution is three-fold. First, we represent the class correlation by word
embeddings to capture the fine-grained semantic and latent statistical
concurrency. Second, we equip the MLL with a graph convolutional network to
explore the inner and outer relationship among categories. Third, we propose a
novel cluster relabel-based contrastive learning algorithm to encourage the
divergence among ambiguous classes. Extensive validation was performed on our
large in-house dataset. Our approach reports the highest accuracy as 90.25% for
standard planes labeling, 85.59% for planes and structures labeling and mAP as
94.63%. The proposed MLL scheme provides a novel perspective for standard plane
recognition and can be easily extended to other medical image classification
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11797">
<div class="article-summary-box-inner">
<span><p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising
capabilities in grounding natural language in image data, facilitating a broad
variety of cross-modal tasks. However, we note that there exists a significant
gap between the objective forms of model pre-training and fine-tuning,
resulting in a need for large amounts of labeled data to stimulate the visual
grounding capability of VL-PTMs for downstream tasks. To address the challenge,
we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt
Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual
grounding into a fill-in-the-blank problem with color-based co-referential
markers in image and text, maximally mitigating the gap. In this way, CPT
enables strong few-shot and even zero-shot visual grounding capabilities of
VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs
outperform their fine-tuned counterparts by a large margin (e.g., 17.3%
absolute accuracy improvement, and 73.8% relative standard deviation reduction
on average with one shot in RefCOCO evaluation). We make the data and code for
this paper publicly available at https://github.com/thunlp/CPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Track Boosting and Synthetic Data Aided Drone Detection. (arXiv:2111.12389v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12389">
<div class="article-summary-box-inner">
<span><p>This is the paper for the first place winning solution of the Drone vs. Bird
Challenge, organized by AVSS 2021. As the usage of drones increases with
lowered costs and improved drone technology, drone detection emerges as a vital
object detection task. However, detecting distant drones under unfavorable
conditions, namely weak contrast, long-range, low visibility, requires
effective algorithms. Our method approaches the drone detection problem by
fine-tuning a YOLOv5 model with real and synthetically generated data using a
Kalman-based object tracker to boost detection confidence. Our results indicate
that augmenting the real data with an optimal subset of synthetic data can
increase the performance. Moreover, temporal information gathered by object
tracking methods can increase performance further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Federated Learning with Adaptive Batchnorm for Healthcare. (arXiv:2112.00734v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00734">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in applying machine learning techniques to
healthcare. Recently, federated learning (FL) is gaining popularity since it
allows researchers to train powerful models without compromising data privacy
and security. However, the performance of existing FL approaches often
deteriorates when encountering non-iid situations where there exist
distribution gaps among clients, and few previous efforts focus on
personalization in healthcare. In this article, we propose FedAP to tackle
domain shifts and then obtain personalized models for local clients. FedAP
learns the similarity between clients based on the statistics of the batch
normalization layers while preserving the specificity of each client with
different local batch normalization. Comprehensive experiments on five
healthcare benchmarks demonstrate that FedAP achieves better accuracy compared
to state-of-the-art methods (e.g., 10% accuracy improvement for PAMAP2) with
faster convergence speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalisation effects of predictive uncertainty estimation in deep learning for digital pathology. (arXiv:2112.09693v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09693">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has shown great potential in digital pathology
applications. The robustness of a diagnostic DL-based solution is essential for
safe clinical deployment. In this work we evaluate if adding uncertainty
estimates for DL predictions in digital pathology could result in increased
value for the clinical applications, by boosting the general predictive
performance or by detecting mispredictions. We compare the effectiveness of
model-integrated methods (MC dropout and Deep ensembles) with a model-agnostic
approach (Test time augmentation, TTA). Moreover, four uncertainty metrics are
compared. Our experiments focus on two domain shift scenarios: a shift to a
different medical center and to an underrepresented subtype of cancer. Our
results show that uncertainty estimates increase reliability by reducing a
model's sensitivity to classification threshold selection as well as by
detecting between 70\% and 90\% of the mispredictions done by the model.
Overall, the deep ensembles method achieved the best performance closely
followed by TTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flow-Guided Sparse Transformer for Video Deblurring. (arXiv:2201.01893v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01893">
<div class="article-summary-box-inner">
<span><p>Exploiting similar and sharper scene patches in spatio-temporal neighborhoods
is critical for video deblurring. However, CNN-based methods show limitations
in capturing long-range dependencies and modeling non-local self-similarity. In
this paper, we propose a novel framework, Flow-Guided Sparse Transformer
(FGST), for video deblurring. In FGST, we customize a self-attention module,
Flow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each
$query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of
the estimated optical flow to globally sample spatially sparse yet highly
related $key$ elements corresponding to the same scene patch in neighboring
frames. Besides, we present a Recurrent Embedding (RE) mechanism to transfer
information from past frames and strengthen long-range temporal dependencies.
Comprehensive experiments demonstrate that our proposed FGST outperforms
state-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields
more visually pleasing results in real video deblurring. Code and pre-trained
models are publicly available at https://github.com/linjing7/VR-Baseline
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09048">
<div class="article-summary-box-inner">
<span><p>Various models have been proposed to solve the object detection problem.
However, most of them require many hand-designed components to demonstrate good
performance. To mitigate these issues, Transformer based DETR and its variant
Deformable DETR were suggested. They solved much of the complex issue of
designing a head of object detection model but it has not been generally clear
that the Transformer-based models could be considered as the state-of-the-art
method in object detection without doubt. Furthermore, as DETR adapted
Transformer method only for the detection head, but still with including CNN
for the backbone body, it has not been certain that it would be possible to
build the competent end-to-end pipeline with the combination of attention
modules. In this paper, we propose that combining several attention modules
with our new Task Specific Split Transformer(TSST) is a fairly good enough
method to produce the best COCO results without traditionally hand-designed
components. By splitting generally purposed attention module into two separated
mission specific attention module, the proposed method addresses the way to
design simpler object detection models than before. Extensive experiments on
the COCO benchmark demonstrate the effectiveness of our approach. Code is
released at https://github.com/navervision/tsst
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HMD-EgoPose: Head-Mounted Display-Based Egocentric Marker-Less Tool and Hand Pose Estimation for Augmented Surgical Guidance. (arXiv:2202.11891v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11891">
<div class="article-summary-box-inner">
<span><p>The success or failure of modern computer-assisted surgery procedures hinges
on the precise six-degree-of-freedom (6DoF) position and orientation (pose)
estimation of tracked instruments and tissue. In this paper, we present
HMD-EgoPose, a single-shot learning-based approach to hand and object pose
estimation and demonstrate state-of-the-art performance on a benchmark dataset
for monocular red-green-blue (RGB) 6DoF marker-less hand and surgical
instrument pose tracking. Further, we reveal the capacity of our HMD-EgoPose
framework for performant 6DoF pose estimation on a commercially available
optical see-through head-mounted display (OST-HMD) through a low-latency
streaming approach. Our framework utilized an efficient convolutional neural
network (CNN) backbone for multi-scale feature extraction and a set of
subnetworks to jointly learn the 6DoF pose representation of the rigid surgical
drill instrument and the grasping orientation of the hand of a user. To make
our approach accessible to a commercially available OST-HMD, the Microsoft
HoloLens 2, we created a pipeline for low-latency video and data communication
with a high-performance computing workstation capable of optimized network
inference. HMD-EgoPose outperformed current state-of-the-art approaches on a
benchmark dataset for surgical tool pose estimation, achieving an average tool
3D vertex error of 11.0 mm on real data and furthering the progress towards a
clinically viable marker-free tracking strategy. Through our low-latency
streaming approach, we achieved a round trip latency of 199.1 ms for pose
estimation and augmented visualization of the tracked model when integrated
with the OST-HMD. Our single-shot learned approach was robust to occlusion and
complex surfaces and improved on current state-of-the-art approaches to
marker-less tool and hand pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TwistSLAM: Constrained SLAM in Dynamic Environment. (arXiv:2202.12384v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12384">
<div class="article-summary-box-inner">
<span><p>Classical visual simultaneous localization and mapping (SLAM) algorithms
usually assume the environment to be rigid. This assumption limits the
applicability of those algorithms as they are unable to accurately estimate the
camera poses and world structure in real life scenes containing moving objects
(e.g. cars, bikes, pedestrians, etc.). To tackle this issue, we propose
TwistSLAM: a semantic, dynamic and stereo SLAM system that can track dynamic
objects in the environment. Our algorithm creates clusters of points according
to their semantic class. Thanks to the definition of inter-cluster constraints
modeled by mechanical joints (function of the semantic class), a novel
constrained bundle adjustment is then able to jointly estimate both poses and
velocities of moving objects along with the classical world structure and
camera trajectory. We evaluate our approach on several sequences from the
public KITTI dataset and demonstrate quantitatively that it improves camera and
object tracking compared to state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04275">
<div class="article-summary-box-inner">
<span><p>This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural
Network (CNN) for pose estimation of noncooperative spacecraft across domain
gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared
multi-scale feature encoder and multiple prediction heads that perform
different tasks on a shared feature output. These tasks are all related to
detection and pose estimation of a target spacecraft from an image, such as
prediction of pre-defined satellite keypoints, direct pose regression, and
binary segmentation of the satellite foreground. It is shown that by jointly
training on different yet related tasks with extensive data augmentations on
synthetic images only, the shared encoder learns features that are common
across image domains that have fundamentally different visual characteristics
compared to synthetic images. This work also introduces Online Domain
Refinement (ODR) which refines the parameters of the normalization layers of
SPNv2 on the target domain images online at deployment. Specifically, ODR
performs self-supervised entropy minimization of the predicted satellite
foreground, thereby improving the CNN's performance on the target domain images
without their pose labels and with minimal computational efforts. The GitHub
repository for SPNv2 is available at \url{https://github.com/tpark94/spnv2}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Autofocusing using Tiny Transformer Networks for Digital Holographic Microscopy. (arXiv:2203.07772v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07772">
<div class="article-summary-box-inner">
<span><p>The numerical wavefront backpropagation principle of digital holography
confers unique extended focus capabilities, without mechanical displacements
along z-axis. However, the determination of the correct focusing distance is a
non-trivial and time consuming issue. A deep learning (DL) solution is proposed
to cast the autofocusing as a regression problem and tested over both
experimental and simulated holograms. Single wavelength digital holograms were
recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$
microscope objective from a patterned target moving in 3D over an axial range
of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision
Transformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The
proposed tiny networks are compared with their original versions (ViT/B16,
VGG16 and Swin-Transformer Tiny) and the main neural networks used in digital
holography such as LeNet and AlexNet. The experiments show that the predicted
focusing distance $Z_R^{\mathrm{Pred}}$ is accurately inferred with an accuracy
of 1.2 $\mu$m in average in comparison with the DHM depth of field of 15
$\mu$m. Numerical simulations show that all tiny models give the
$Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such a prospect would
significantly improve the current capabilities of computer vision position
sensing in applications such as 3D microscopy for life sciences or
micro-robotics. Moreover, all models reach an inference time on CPU, inferior
to 25 ms per inference. In terms of occlusions, TViT based on its Transformer
architecture is the most robust.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation. (arXiv:2203.14098v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14098">
<div class="article-summary-box-inner">
<span><p>A fundamental and challenging problem in deep learning is catastrophic
forgetting, i.e. the tendency of neural networks to fail to preserve the
knowledge acquired from old tasks when learning new tasks. This problem has
been widely investigated in the research community and several Incremental
Learning (IL) approaches have been proposed in the past years. While earlier
works in computer vision have mostly focused on image classification and object
detection, more recently some IL approaches for semantic segmentation have been
introduced. These previous works showed that, despite its simplicity, knowledge
distillation can be effectively employed to alleviate catastrophic forgetting.
In this paper, we follow this research direction and, inspired by recent
literature on contrastive learning, we propose a novel distillation framework,
Uncertainty-aware Contrastive Distillation (\method). In a nutshell, \method~is
operated by introducing a novel distillation loss that takes into account all
the images in a mini-batch, enforcing similarity between features associated to
all the pixels from the same classes, and pulling apart those corresponding to
pixels from different classes. In order to mitigate catastrophic forgetting, we
contrast features of the new model with features extracted by a frozen model
learned at the previous incremental step. Our experimental results demonstrate
the advantage of the proposed distillation technique, which can be used in
synergy with previous IL approaches, and leads to state-of-art performance on
three commonly adopted benchmarks for incremental semantic segmentation. The
code is available at \url{https://github.com/ygjwd12345/UCD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval. (arXiv:2204.07441v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07441">
<div class="article-summary-box-inner">
<span><p>Large-scale single-stream pre-training has shown dramatic performance in
image-text retrieval. Regrettably, it faces low inference efficiency due to
heavy attention layers. Recently, two-stream methods like CLIP and ALIGN with
high inference efficiency have also shown promising performance, however, they
only consider instance-level alignment between the two streams (thus there is
still room for improvement). To overcome these limitations, we propose a novel
COllaborative Two-Stream vision-language pretraining model termed COTS for
image-text retrieval by enhancing cross-modal interaction. In addition to
instance level alignment via momentum contrastive learning, we leverage two
extra levels of cross-modal interactions in our COTS: (1) Token-level
interaction - a masked visionlanguage modeling (MVLM) learning objective is
devised without using a cross-stream network module, where variational
autoencoder is imposed on the visual encoder to generate visual tokens for each
image. (2) Task-level interaction - a KL-alignment learning objective is
devised between text-to-image and image-to-text retrieval tasks, where the
probability distribution per task is computed with the negative queues in
momentum contrastive learning. Under a fair comparison setting, our COTS
achieves the highest performance among all two-stream methods and comparable
performance (but with 10,800X faster in inference) w.r.t. the latest
single-stream methods. Importantly, our COTS is also applicable to
text-to-video retrieval, yielding new state-ofthe-art on the widely-used
MSR-VTT dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01271">
<div class="article-summary-box-inner">
<span><p>Pose estimation plays a critical role in human-centered vision applications.
However, it is difficult to deploy state-of-the-art HRNet-based pose estimation
models on resource-constrained edge devices due to the high computational cost
(more than 150 GMACs per frame). In this paper, we study efficient architecture
design for real-time multi-person pose estimation on edge. We reveal that
HRNet's high-resolution branches are redundant for models at the
low-computation region via our gradual shrinking experiments. Removing them
improves both efficiency and performance. Inspired by this finding, we design
LitePose, an efficient single-branch architecture for pose estimation, and
introduce two simple approaches to enhance the capacity of LitePose, including
Fusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the
redundancy in high-resolution branches, allowing scale-aware feature fusion
with low overhead. Large Kernel Convs significantly improve the model's
capacity and receptive field while maintaining a low computational cost. With
only 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3
kernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the
latency by up to 5.0x without sacrificing performance, compared with prior
state-of-the-art efficient pose estimation models, pushing the frontier of
real-time multi-person pose estimation on edge. Our code and pre-trained models
are released at https://github.com/mit-han-lab/litepose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPortraitDrawing: Generating Human Body Images from Freehand Sketches. (arXiv:2205.02070v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02070">
<div class="article-summary-box-inner">
<span><p>Researchers have explored various ways to generate realistic images from
freehand sketches, e.g., for objects and human faces. However, how to generate
realistic human body images from sketches is still a challenging problem. It
is, first because of the sensitivity to human shapes, second because of the
complexity of human images caused by body shape and pose changes, and third
because of the domain gap between realistic images and freehand sketches. In
this work, we present DeepPortraitDrawing, a deep generative framework for
converting roughly drawn sketches to realistic human body images. To encode
complicated body shapes under various poses, we take a local-to-global
approach. Locally, we employ semantic part auto-encoders to construct
part-level shape spaces, which are useful for refining the geometry of an input
pre-segmented hand-drawn sketch. Globally, we employ a cascaded spatial
transformer network to refine the structure of body parts by adjusting their
spatial locations and relative proportions. Finally, we use a global synthesis
network for the sketch-to-image translation task, and a face refinement network
to enhance facial details. Extensive experiments have shown that given roughly
sketched human portraits, our method produces more realistic images than the
state-of-the-art sketch-to-image synthesis techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2 segmentation models. (arXiv:2205.02152v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02152">
<div class="article-summary-box-inner">
<span><p>Recent studies indicate that detecting radiographic patterns on CT scans can
yield high sensitivity and specificity for Covid-19 localization. In this
paper, we investigate the appropriateness of deep learning models
transferability, for semantic segmentation of pneumonia-infected areas in CT
images. Transfer learning allows for the fast initialization/reutilization of
detection models, given that large volumes of training data are not available.
Our work explores the efficacy of using pre-trained U-Net architectures, on a
specific CT data set, for identifying Covid-19 side-effects over images from
different datasets. Experimental results indicate improvement in the
segmentation accuracy of identifying Covid-19 infected regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise-Tolerant Learning for Audio-Visual Action Recognition. (arXiv:2205.07611v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07611">
<div class="article-summary-box-inner">
<span><p>Recently, video recognition is emerging with the help of multi-modal
learning, which focuses on integrating multiple modalities to improve the
performance or robustness of a model. Although various multi-modal learning
methods have been proposed and offer remarkable recognition results, almost all
of these methods rely on high-quality manual annotations and assume that
modalities among multi-modal data provide relevant semantic information.
Unfortunately, most widely used video datasets are collected from the Internet
and inevitably contain noisy labels and noisy correspondence. To solve this
problem, we use the audio-visual action recognition task as a proxy and propose
a noise-tolerant learning framework to find anti-interference model parameters
to both noisy labels and noisy correspondence. Our method consists of two
phases and aims to rectify noise by the inherent correlation between
modalities. A noise-tolerant contrastive training phase is performed first to
learn robust model parameters unaffected by the noisy labels. To reduce the
influence of noisy correspondence, we propose a cross-modal noise estimation
component to adjust the consistency between different modalities. Since the
noisy correspondence existed at the instance level, a category-level
contrastive loss is proposed to further alleviate the interference of noisy
correspondence. Then in the hybrid supervised training phase, we calculate the
distance metric among features to obtain corrected labels, which are used as
complementary supervision. In addition, we investigate the noisy correspondence
in real-world datasets and conduct comprehensive experiments with synthetic and
real noise data. The results verify the advantageous performance of our method
compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning on 3D point clouds with random compressed rehearsal. (arXiv:2205.08013v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08013">
<div class="article-summary-box-inner">
<span><p>Contemporary deep neural networks offer state-of-the-art results when applied
to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds
are important datatype for precise modeling of three-dimensional environments,
but effective processing of this type of data proves to be challenging. In the
world of large, heavily-parameterized network architectures and
continuously-streamed data, there is an increasing need for machine learning
models that can be trained on additional data. Unfortunately, currently
available models cannot fully leverage training on additional data without
losing their past knowledge. Combating this phenomenon, called catastrophic
forgetting, is one of the main objectives of continual learning. Continual
learning for deep neural networks has been an active field of research,
primarily in 2D computer vision, natural language processing, reinforcement
learning, and robotics. However, in 3D computer vision, there are hardly any
continual learning solutions specifically designed to take advantage of point
cloud structure. This work proposes a novel neural network architecture capable
of continual learning on 3D point cloud data. We utilize point cloud structure
properties for preserving a heavily compressed set of past data. By using
rehearsal and reconstruction as regularization methods of the learning process,
our approach achieves a significant decrease of catastrophic forgetting
compared to the existing solutions on several most popular point cloud datasets
considering two continual learning settings: when a task is known beforehand,
and in the challenging scenario of when task information is unknown to the
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers. (arXiv:2205.08078v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08078">
<div class="article-summary-box-inner">
<span><p>Vision transformers using self-attention or its proposed alternatives have
demonstrated promising results in many image related tasks. However, the
underpinning inductive bias of attention is not well understood. To address
this issue, this paper analyzes attention through the lens of convex duality.
For the non-linear dot-product self-attention, and alternative mechanisms such
as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent
finite-dimensional convex problems that are interpretable and solvable to
global optimality. The convex programs lead to {\it block nuclear-norm
regularization} that promotes low rank in the latent feature and token
dimensions. In particular, we show how self-attention networks implicitly
clusters the tokens, based on their latent similarity. We conduct experiments
for transferring a pre-trained transformer backbone for CIFAR-100
classification by fine-tuning a variety of convex attention heads. The results
indicate the merits of the bias induced by attention compared with the existing
MLP or linear heads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning. (arXiv:2205.09542v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09542">
<div class="article-summary-box-inner">
<span><p>In this work, we tackle the challenging problem of arbitrary image style
transfer using a novel style feature representation learning method. A suitable
style representation, as a key component in image stylization tasks, is
essential to achieve satisfactory results. Existing deep neural network based
approaches achieve reasonable results with the guidance from second-order
statistics such as Gram matrix of content features. However, they do not
leverage sufficient style information, which results in artifacts such as local
distortions and style inconsistency. To address these issues, we propose to
learn style representation directly from image features instead of their
second-order statistics, by analyzing the similarities and differences between
multiple styles and considering the style distribution. Specifically, we
present Contrastive Arbitrary Style Transfer (CAST), which is a new style
representation learning and style transfer method via contrastive learning. Our
framework consists of three key components, i.e., a multi-layer style projector
for style code encoding, a domain enhancement module for effective learning of
style distribution, and a generative network for image style transfer. We
conduct qualitative and quantitative evaluations comprehensively to demonstrate
that our approach achieves significantly better results compared to those
obtained via state-of-the-art methods. Code and models are available at
https://github.com/zyxElsa/CAST_pytorch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLCNet: Rethinking of Ensemble Modeling with Classification Confidence Network. (arXiv:2205.09612v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09612">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a Classification Confidence Network (CLCNet) that
can determine whether the classification model classifies input samples
correctly. It can take a classification result in the form of vector in any
dimension, and return a confidence score as output, which represents the
probability of an instance being classified correctly. We can utilize CLCNet in
a simple cascade structure system consisting of several SOTA (state-of-the-art)
classification models, and our experiments show that the system can achieve the
following advantages: 1. The system can customize the average computation
requirement (FLOPs) per image while inference. 2. Under the same computation
requirement, the performance of the system can exceed any model that has
identical structure with the model in the system, but different in size. In
fact, this is a new type of ensemble modeling. Like general ensemble modeling,
it can achieve higher performance than single classification model, yet our
system requires much less computation than general ensemble modeling. We have
uploaded our code to a github repository:
https://github.com/yaoching0/CLCNet-Rethinking-of-Ensemble-Modeling.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-23 23:08:52.868690174 UTC">2022-05-23 23:08:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>