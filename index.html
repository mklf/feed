<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-30T01:30:00Z">08-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.AI updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascading Neural Network Methodology for Artificial Intelligence-Assisted Radiographic Detection and Classification of Lead-Less Implanted Electronic Devices within the Chest. (arXiv:2108.11954v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11954">
<div class="article-summary-box-inner">
<span><p>Background &amp; Purpose: Chest X-Ray (CXR) use in pre-MRI safety screening for
Lead-Less Implanted Electronic Devices (LLIEDs), easily overlooked or
misidentified on a frontal view (often only acquired), is common. Although most
LLIED types are "MRI conditional": 1. Some are stringently conditional; 2.
Different conditional types have specific patient- or device- management
requirements; and 3. Particular types are "MRI unsafe". This work focused on
developing CXR interpretation-assisting Artificial Intelligence (AI)
methodology with: 1. 100% detection for LLIED presence/location; and 2. High
classification in LLIED typing. Materials &amp; Methods: Data-mining
(03/1993-02/2021) produced an AI Model Development Population (1,100
patients/4,871 images) creating 4,924 LLIED Region-Of-Interests (ROIs) (with
image-quality grading) used in Training, Validation, and Testing. For
developing the cascading neural network (detection via Faster R-CNN and
classification via Inception V3), "ground-truth" CXR annotation (ROI labeling
per LLIED), as well as inference display (as Generated Bounding Boxes (GBBs)),
relied on a GPU-based graphical user interface. Results: To achieve 100% LLIED
detection, probability threshold reduction to 0.00002 was required by Model 1,
resulting in increasing GBBs per LLIED-related ROI. Targeting LLIED-type
classification following detection of all LLIEDs, Model 2 multi-classified to
reach high-performance while decreasing falsely positive GBBs. Despite 24%
suboptimal ROI image quality, classification was correct in 98.9% and AUCs for
the 9 LLIED-types were 1.00 for 8 and 0.92 for 1. For all misclassification
cases: 1. None involved stringently conditional or unsafe LLIEDs; and 2. Most
were attributable to suboptimal images. Conclusion: This project successfully
developed a LLIED-related AI methodology supporting: 1. 100% detection; and 2.
Typically 100% type classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Sentence Ordering Method Using BERT Pretrained Model. (arXiv:2108.11994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11994">
<div class="article-summary-box-inner">
<span><p>Building systems with capability of natural language understanding (NLU) has
been one of the oldest areas of AI. An essential component of NLU is to detect
logical succession of events contained in a text. The task of sentence ordering
is proposed to learn succession of events with applications in AI tasks. The
performance of previous works employing statistical methods is poor, while the
neural networks-based approaches are in serious need of large corpora for model
learning. In this paper, we propose a method for sentence ordering which does
not need a training phase and consequently a large corpus for learning. To this
end, we generate sentence embedding using BERT pre-trained model and measure
sentence similarity using cosine similarity score. We suggest this score as an
indicator of sequential events' level of coherence. We finally sort the
sentences through brute-force search to maximize overall similarities of the
sequenced sentences. Our proposed method outperformed other baselines on
ROCStories, a corpus of 5-sentence human-made stories. The method is
specifically more efficient than neural network-based methods when no huge
corpus is available. Among other advantages of this method are its
interpretability and needlessness to linguistic knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding the Logit Distributions of Adversarially-Trained Deep Neural Networks. (arXiv:2108.12001v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12001">
<div class="article-summary-box-inner">
<span><p>Adversarial defenses train deep neural networks to be invariant to the input
perturbations from adversarial attacks. Almost all defense strategies achieve
this invariance through adversarial training i.e. training on inputs with
adversarial perturbations. Although adversarial training is successful at
mitigating adversarial attacks, the behavioral differences between
adversarially-trained (AT) models and standard models are still poorly
understood. Motivated by a recent study on learning robustness without input
perturbations by distilling an AT model, we explore what is learned during
adversarial training by analyzing the distribution of logits in AT models. We
identify three logit characteristics essential to learning adversarial
robustness. First, we provide a theoretical justification for the finding that
adversarial training shrinks two important characteristics of the logit
distribution: the max logit values and the "logit gaps" (difference between the
logit max and next largest values) are on average lower for AT models. Second,
we show that AT and standard models differ significantly on which samples are
high or low confidence, then illustrate clear qualitative differences by
visualizing samples with the largest confidence difference. Finally, we find
learning information about incorrect classes to be essential to learning
robustness by manipulating the non-max logit information during distillation
and measuring the impact on the student's robustness. Our results indicate that
learning some adversarial robustness without input perturbations requires a
model to learn specific sample-wise confidences and incorrect class orderings
that follow complex distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-based Self-Critical Training For Question Generation. (arXiv:2108.12026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12026">
<div class="article-summary-box-inner">
<span><p>We present in this work a fully Transformer-based reinforcement learning
generator-evaluator architecture for neural question generation. Question
generation is a task that consists in generating questions given a context and
answer. To improve the quality of the generated question, we came up with a
semantic-based self-critical training layout in generator-evaluator
architecture, which goes beyond typical maximum likelihood training. Evaluation
metrics for language modeling only based on n-gram overlapping do not consider
semantic relations between reference and candidate strings. To improve the
evaluation step, we assess our model for both n-gram overlap using BLEU and
semantically using BERTScore and NUBIA, a novel state-of-the-art evaluation
metric for text generation. Question generation could be used in many
downstream applications, including in extending question answering datasets,
conversational systems, and educational assessment systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12056">
<div class="article-summary-box-inner">
<span><p>Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow's machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of learning, and retaining, new information without repeated exposure to it.
Past efforts to engineer such systems have sought to build or regulate
artificial neural networks using task-specific modules with constrained
circumstances of application. This has not yet enabled continual learning over
long sequences of previously unseen data without corrupting existing knowledge:
a problem known as catastrophic forgetting. In this paper, we introduce a
system that can learn sequentially over previously unseen datasets (ImageNet,
CIFAR-100) with little forgetting over time. This is accomplished by regulating
the activity of weights in a convolutional neural network on the basis of
inputs using top-down modulation generated by a second feed-forward neural
network. We find that our method learns continually under domain transfer with
sparse bursts of activity in weights that are recycled across tasks, rather
than by maintaining task-specific modules. Sparse synaptic bursting is found to
balance enhanced and diminished activity in a way that facilitates adaptation
to new inputs without corrupting previously acquired functions. This behavior
emerges during a prior meta-learning phase in which regulated synapses are
selectively disinhibited, or grown, from an initial state of uniform
suppression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
<div class="article-summary-box-inner">
<span><p>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Give Checkable Answers with Prover-Verifier Games. (arXiv:2108.12099v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12099">
<div class="article-summary-box-inner">
<span><p>Our ability to know when to trust the decisions made by machine learning
systems has not kept up with the staggering improvements in their performance,
limiting their applicability in high-stakes domains. We introduce
Prover-Verifier Games (PVGs), a game-theoretic framework to encourage learning
agents to solve decision problems in a verifiable manner. The PVG consists of
two learners with competing objectives: a trusted verifier network tries to
choose the correct answer, and a more powerful but untrusted prover network
attempts to persuade the verifier of a particular answer, regardless of its
correctness. The goal is for a reliable justification protocol to emerge from
this game. We analyze variants of the framework, including simultaneous and
sequential games, and narrow the space down to a subset of games which provably
have the desired equilibria. We develop instantiations of the PVG for two
algorithmic tasks, and show that in practice, the verifier learns a robust
decision rule that is able to receive useful and reliable information from an
untrusted prover. Importantly, the protocol still works even when the verifier
is frozen and the prover's messages are directly optimized to convince the
verifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-aware Warping Factors in Mask-based Speech Enhancement. (arXiv:2108.12128v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12128">
<div class="article-summary-box-inner">
<span><p>This paper proposes the use of two task-aware warping factors in mask-based
speech enhancement (SE). One controls the balance between speech-maintenance
and noise-removal in training phases, while the other controls SE power applied
to specific downstream tasks in testing phases. Our intention is to alleviate
the problem that SE systems trained to improve speech quality often fail to
improve other downstream tasks, such as automatic speaker verification (ASV)
and automatic speech recognition (ASR), because they do not share the same
objects. It is easy to apply the proposed dual-warping factors approach to any
mask-based SE method, and it allows a single SE system to handle multiple tasks
without task-dependent training. The effectiveness of our proposed approach has
been confirmed on the SITW dataset for ASV evaluation and the LibriSpeech
dataset for ASR and speech quality evaluations of 0-20dB. We show that
different warping values are necessary for a single SE to achieve optimal
performance w.r.t. the three tasks. With the use of task-dependent warping
factors, speech quality was improved by an 84.7% PESQ increase, ASV had a 22.4%
EER reduction, and ASR had a 52.2% WER reduction, on 0dB speech. The
effectiveness of the task-dependent warping factors were also cross-validated
on VoxCeleb-1 test set for ASV and LibriSpeech dev-clean set for ASV and
quality evaluations. The proposed method is highly effective and easy to apply
in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WAD: A Deep Reinforcement Learning Agent for Urban Autonomous Driving. (arXiv:2108.12134v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12134">
<div class="article-summary-box-inner">
<span><p>Urban autonomous driving is an open and challenging problem to solve as the
decision-making system has to account for several dynamic factors like
multi-agent interactions, diverse scene perceptions, complex road geometries,
and other rarely occurring real-world events. On the other side, with deep
reinforcement learning (DRL) techniques, agents have learned many complex
policies. They have even achieved super-human-level performances in various
Atari Games and Deepmind's AlphaGo. However, current DRL techniques do not
generalize well on complex urban driving scenarios. This paper introduces the
DRL driven Watch and Drive (WAD) agent for end-to-end urban autonomous driving.
Motivated by recent advancements, the study aims to detect important
objects/states in high dimensional spaces of CARLA and extract the latent state
from them. Further, passing on the latent state information to WAD agents based
on TD3 and SAC methods to learn the optimal driving policy. Our novel approach
utilizing fewer resources, step-by-step learning of different driving tasks,
hard episode termination policy, and reward mechanism has led our agents to
achieve a 100% success rate on all driving tasks in the original CARLA
benchmark and set a new record of 82% on further complex NoCrash benchmark,
outperforming the state-of-the-art model by more than +30% on NoCrash
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lyra: A Benchmark for Turducken-Style Code Generation. (arXiv:2108.12144v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12144">
<div class="article-summary-box-inner">
<span><p>Code generation is crucial to reduce manual software development efforts.
Recently, neural techniques have been used to generate source code
automatically. While promising, these approaches are evaluated on tasks for
generating code in single programming languages. However, in actual
development, one programming language is often embedded in another. For
example, SQL statements are often embedded as strings in base programming
languages such as Python and Java, and JavaScript programs are often embedded
in sever-side programming languages, such as PHP, Java, and Python. We call
this a turducken-style programming. In this paper, we define a new code
generation task: given a natural language comment, this task aims to generate a
program in a base language with an embedded language. To our knowledge, this is
the first turducken-style code generation task. For this task, we present Lyra:
a dataset in Python with embedded SQL. This dataset contains 2,000 carefully
annotated database manipulation programs from real usage projects. Each program
is paired with both a Chinese comment and an English comment. In our
experiment, we adopted Transformer, a state-of-the-art technique, as the
baseline. In the best setting, Transformer achieves 0.5% and 1.5% AST exact
matching accuracy using Chinese and English comments, respectively. Therefore,
we believe that Lyra provides a new challenge for code generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cleaning Inconsistent Data in Temporal DL-Lite Under Best Repair Semantics. (arXiv:2108.12149v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12149">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of handling inconsistent data in
Temporal Description Logic (TDL) knowledge bases. Considering the data part of
the Knowledge Base as the source of inconsistency over time, we propose an ABox
repair approach. This is the first work handling the repair in TDL Knowledge
bases. To do so, our goal is twofold: 1) detect temporal inconsistencies and 2)
propose a data temporal reparation. For the inconsistency detection, we propose
a reduction approach from TDL to DL which allows to provide a tight NP-complete
upper bound for TDL concept satisfiability and to use highly optimised DL
reasoners that can bring precise explanation (the set of inconsistent data
assertions). Thereafter, from the obtained explanation, we propose a method for
automatically computing the best repair in the temporal setting based on the
allowed rigid predicates and the time order of assertions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLocal-K: Global and Local Kernels for Recommender Systems. (arXiv:2108.12184v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12184">
<div class="article-summary-box-inner">
<span><p>Recommender systems typically operate on high-dimensional sparse user-item
matrices. Matrix completion is a very challenging task to predict one's
interest based on millions of other users having each seen a small subset of
thousands of items. We propose a Global-Local Kernel-based matrix completion
framework, named GLocal-K, that aims to generalise and represent a
high-dimensional sparse user-item matrix entry into a low dimensional space
with a small number of important features. Our GLocal-K can be divided into two
major stages. First, we pre-train an auto encoder with the local kernelised
weight matrix, which transforms the data from one space into the feature space
by using a 2d-RBF kernel. Then, the pre-trained auto encoder is fine-tuned with
the rating matrix, produced by a convolution-based global kernel, which
captures the characteristics of each item. We apply our GLocal-K model under
the extreme low-resource setting, which includes only a user-item rating
matrix, with no side information. Our model outperforms the state-of-the-art
baselines on three collaborative filtering benchmarks: ML-100K, ML-1M, and
Douban.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Robustness of Neural Language Models to Input Perturbations. (arXiv:2108.12237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12237">
<div class="article-summary-box-inner">
<span><p>High-performance neural language models have obtained state-of-the-art
results on a wide range of Natural Language Processing (NLP) tasks. However,
results for common benchmark datasets often do not reflect model reliability
and robustness when applied to noisy, real-world data. In this study, we design
and implement various types of character-level and word-level perturbation
methods to simulate realistic scenarios in which input texts may be slightly
noisy or different from the data distribution on which NLP systems were
trained. Conducting comprehensive experiments on different NLP tasks, we
investigate the ability of high-performance language models such as BERT,
XLNet, RoBERTa, and ELMo in handling different types of input perturbations.
The results suggest that language models are sensitive to input perturbations
and their performance can decrease even when small changes are introduced. We
highlight that models need to be further improved and that current benchmarks
are not reflecting model robustness well. We argue that evaluations on
perturbed inputs should routinely complement widely-used benchmarks in order to
yield a more realistic understanding of NLP systems robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Models for (Temporally) Attributed Description Logics. (arXiv:2108.12239v1 [cs.LO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12239">
<div class="article-summary-box-inner">
<span><p>In the search for knowledge graph embeddings that could capture ontological
knowledge, geometric models of existential rules have been recently introduced.
It has been shown that convex geometric regions capture the so-called
quasi-chained rules. Attributed description logics (DL) have been defined to
bridge the gap between DL languages and knowledge graphs, whose facts often
come with various kinds of annotations that may need to be taken into account
for reasoning. In particular, temporally attributed DLs are enriched by
specific attributes whose semantics allows for some temporal reasoning.
Considering that geometric models and (temporally) attributed DLs are promising
tools designed for knowledge graphs, this paper investigates their
compatibility, focusing on the attributed version of a Horn dialect of the
DL-Lite family. We first adapt the definition of geometric models to attributed
DLs and show that every satisfiable ontology has a convex geometric model. Our
second contribution is a study of the impact of temporal attributes. We show
that a temporally attributed DL may not have a convex geometric model in
general but we can recover geometric satisfiability by imposing some
restrictions on the use of the temporal attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning models are not robust against noise in clinical text. (arXiv:2108.12242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12242">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) systems are attracting increasing interest in
the medical domain due to their ability to learn complicated tasks that require
human intelligence and expert knowledge. AI systems that utilize
high-performance Natural Language Processing (NLP) models have achieved
state-of-the-art results on a wide variety of clinical text processing
benchmarks. They have even outperformed human accuracy on some tasks. However,
performance evaluation of such AI systems have been limited to accuracy
measures on curated and clean benchmark datasets that may not properly reflect
how robustly these systems can operate in real-world situations. In order to
address this challenge, we introduce and implement a wide variety of
perturbation methods that simulate different types of noise and variability in
clinical text data. While noisy samples produced by these perturbation methods
can often be understood by humans, they may cause AI systems to make erroneous
decisions. Conducting extensive experiments on several clinical text processing
tasks, we evaluated the robustness of high-performance NLP models against
various types of character-level and word-level noise. The results revealed
that the NLP models performance degrades when the input contains small amounts
of noise. This study is a significant step towards exposing vulnerabilities of
AI models utilized in clinical text processing systems. The proposed
perturbation methods can be used in performance evaluation tests to assess how
robustly clinical NLP models can operate on noisy data, in real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-To-End Anomaly Detection for Identifying Malicious Cyber Behavior through NLP-Based Log Embeddings. (arXiv:2108.12276v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12276">
<div class="article-summary-box-inner">
<span><p>Rule-based IDS (intrusion detection systems) are being replaced by more
robust neural IDS, which demonstrate great potential in the field of
Cybersecurity. However, these ML approaches continue to rely on ad-hoc feature
engineering techniques, which lack the capacity to vectorize inputs in ways
that are fully relevant to the discovery of anomalous cyber activity. We
propose a deep end-to-end framework with NLP-inspired components for
identifying potentially malicious behaviors on enterprise computer networks. We
also demonstrate the efficacy of this technique on the recently released DARPA
OpTC data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process. (arXiv:2108.12278v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12278">
<div class="article-summary-box-inner">
<span><p>Recent research efforts in lifelong learning propose to grow a mixture of
models to adapt to an increasing number of tasks. The proposed methodology
shows promising results in overcoming catastrophic forgetting. However, the
theory behind these successful models is still not well understood. In this
paper, we perform the theoretical analysis for lifelong learning models by
deriving the risk bounds based on the discrepancy distance between the
probabilistic representation of data generated by the model and that
corresponding to the target dataset. Inspired by the theoretical analysis, we
introduce a new lifelong learning approach, namely the Lifelong Infinite
Mixture (LIMix) model, which can automatically expand its network architectures
or choose an appropriate component to adapt its parameters for learning a new
task, while preserving its previously learnt information. We propose to
incorporate the knowledge by means of Dirichlet processes by using a gating
mechanism which computes the dependence between the knowledge learnt previously
and stored in each component, and a new set of data. Besides, we train a
compact Student model which can accumulate cross-domain representations over
time and make quick inferences. The code is available at
https://github.com/dtuzi123/Lifelong-infinite-mixture-model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers. (arXiv:2108.12284v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12284">
<div class="article-summary-box-inner">
<span><p>Recently, many datasets have been proposed to test the systematic
generalization ability of neural networks. The companion baseline Transformers,
typically trained with default hyper-parameters from standard tasks, are shown
to fail dramatically. Here we demonstrate that by revisiting model
configurations as basic as scaling of embeddings, early stopping, relative
positional embedding, and Universal Transformer variants, we can drastically
improve the performance of Transformers on systematic generalization. We report
improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics
dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity
split, and from 35% to 81% on COGS. On SCAN, relative positional embedding
largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100%
accuracy on the length split with a cutoff at 26. Importantly, performance
differences between these models are typically invisible on the IID data split.
This calls for proper generalization validation sets for developing neural
networks that generalize systematically. We publicly release the code to
reproduce our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Music Composition with Deep Learning: A Review. (arXiv:2108.12290v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12290">
<div class="article-summary-box-inner">
<span><p>Generating a complex work of art such as a musical composition requires
exhibiting true creativity that depends on a variety of factors that are
related to the hierarchy of musical language. Music generation have been faced
with Algorithmic methods and recently, with Deep Learning models that are being
used in other fields such as Computer Vision. In this paper we want to put into
context the existing relationships between AI-based music composition models
and human musical composition and creativity processes. We give an overview of
the recent Deep Learning models for music composition and we compare these
models to the music composition process from a theoretical point of view. We
have tried to answer some of the most relevant open questions for this task by
analyzing the ability of current Deep Learning models to generate music with
creativity or the similarity between AI and human composition processes, among
others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Supervised Heterogeneous Transfer Learning using Dynamic Distribution Adaptation and Manifold Regularization. (arXiv:2108.12293v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12293">
<div class="article-summary-box-inner">
<span><p>Transfer learning aims to learn classifiers for a target domain by
transferring knowledge from a source domain. However, due to two main issues:
feature discrepancy and distribution divergence, transfer learning can be a
very difficult problem in practice. In this paper, we present a framework
called TLF that builds a classifier for the target domain having only few
labeled training records by transferring knowledge from the source domain
having many labeled records. While existing methods often focus on one issue
and leave the other one for the further work, TLF is capable of handling both
issues simultaneously. In TLF, we alleviate feature discrepancy by identifying
shared label distributions that act as the pivots to bridge the domains. We
handle distribution divergence by simultaneously optimizing the structural risk
functional, joint distributions between domains, and the manifold consistency
underlying marginal distributions. Moreover, for the manifold consistency we
exploit its intrinsic properties by identifying k nearest neighbors of a
record, where the value of k is determined automatically in TLF. Furthermore,
since negative transfer is not desired, we consider only the source records
that are belonging to the source pivots during the knowledge transfer. We
evaluate TLF on seven publicly available natural datasets and compare the
performance of TLF against the performance of eleven state-of-the-art
techniques. We also evaluate the effectiveness of TLF in some challenging
situations. Our experimental results, including statistical sign test and
Nemenyi test analyses, indicate a clear superiority of the proposed framework
over the state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TE-YOLOF: Tiny and efficient YOLOF for blood cell detection. (arXiv:2108.12313v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12313">
<div class="article-summary-box-inner">
<span><p>Blood cell detection in microscopic images is an essential branch of medical
image processing research. Since disease detection based on manual checking of
blood cells is time-consuming and full of errors, testing of blood cells using
object detectors with Deep Convolutional Neural Network can be regarded as a
feasible solution. In this work, an object detector based on YOLOF has been
proposed to detect blood cell objects such as red blood cells, white blood
cells and platelets. This object detector is called TE-YOLOF, Tiny and
Efficient YOLOF, and it is a One-Stage detector using dilated encoder to
extract information from single-level feature maps. For increasing efficiency
and flexibility, the EfficientNet Convolutional Neural Network is utilized as
the backbone for the proposed object detector. Furthermore, the Depthwise
Separable Convolution is applied to enhance the performance and minimize the
parameters of the network. In addition, the Mish activation function is
employed to increase the precision. Extensive experiments on the BCCD dataset
prove the effectiveness of the proposed model, which is more efficient than
other existing studies for blood cell detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMT-Based Safety Verification of Data-Aware Processes under Ontologies (Extended Version). (arXiv:2108.12330v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12330">
<div class="article-summary-box-inner">
<span><p>In the context of verification of data-aware processes (DAPs), a formal
approach based on satisfiability modulo theories (SMT) has been considered to
verify parameterised safety properties of so-called artifact-centric systems.
This approach requires a combination of model-theoretic notions and algorithmic
techniques based on backward reachability. We introduce here a variant of one
of the most investigated models in this spectrum, namely simple artifact
systems (SASs), where, instead of managing a database, we operate over a
description logic (DL) ontology expressed in (a slight extension of) RDFS. This
DL, enjoying suitable model-theoretic properties, allows us to define DL-based
SASs to which backward reachability can still be applied, leading to
decidability in PSPACE of the corresponding safety problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Heuristics and Learning in a Computational Architecture for Cognitive Trading. (arXiv:2108.12333v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12333">
<div class="article-summary-box-inner">
<span><p>The successes of Artificial Intelligence in recent years in areas such as
image analysis, natural language understanding and strategy games have sparked
interest from the world of finance. Specifically, there are high expectations,
and ongoing engineering projects, regarding the creation of artificial agents,
known as robotic traders, capable of juggling the financial markets with the
skill of experienced human traders. Obvious economic implications aside, this
is certainly an area of great scientific interest, due to the challenges that
such a real context poses to the use of AI techniques. Precisely for this
reason, we must be aware that artificial agents capable of operating at such
levels are not just round the corner, and that there will be no simple answers,
but rather a concurrence of various technologies and methods to the success of
the effort. In the course of this article, we review the issues inherent in the
design of effective robotic traders as well as the consequently applicable
solutions, having in view the general objective of bringing the current state
of the art of robo-trading up to the next level of intelligence, which we refer
to as Cognitive Trading. Key to our approach is the joining of two
methodological and technological directions which, although both deeply rooted
in the disciplinary field of artificial intelligence, have so far gone their
separate ways: heuristics and learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning. (arXiv:2108.12370v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12370">
<div class="article-summary-box-inner">
<span><p>We demonstrate a library for the integration of domain knowledge in deep
learning architectures. Using this library, the structure of the data is
expressed symbolically via graph declarations and the logical constraints over
outputs or latent variables can be seamlessly added to the deep models. The
domain knowledge can be defined explicitly, which improves the models'
explainability in addition to the performance and generalizability in the
low-data regime. Several approaches for such an integration of symbolic and
sub-symbolic models have been introduced; however, there is no library to
facilitate the programming for such an integration in a generic way while
various underlying algorithms can be used. Our library aims to simplify
programming for such an integration in both training and inference phases while
separating the knowledge representation from learning algorithms. We showcase
various NLP benchmark tasks and beyond. The framework is publicly available at
Github(https://github.com/HLR/DomiKnowS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pedestrian Detection and Tracking Framework for Autonomous Cars: Efficient Fusion of Camera and LiDAR Data. (arXiv:2108.12375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12375">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel method for pedestrian detection and tracking by
fusing camera and LiDAR sensor data. To deal with the challenges associated
with the autonomous driving scenarios, an integrated tracking and detection
framework is proposed. The detection phase is performed by converting LiDAR
streams to computationally tractable depth images, and then, a deep neural
network is developed to identify pedestrian candidates both in RGB and depth
images. To provide accurate information, the detection phase is further
enhanced by fusing multi-modal sensor information using the Kalman filter. The
tracking phase is a combination of the Kalman filter prediction and an optical
flow algorithm to track multiple pedestrians in a scene. We evaluate our
framework on a real public driving dataset. Experimental results demonstrate
that the proposed method achieves significant performance improvement over a
baseline method that solely uses image-based pedestrian detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Effect Identification from Multiple Incomplete Data Sources: A General Search-based Approach. (arXiv:1902.01073v5 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.01073">
<div class="article-summary-box-inner">
<span><p>Causal effect identification considers whether an interventional probability
distribution can be uniquely determined without parametric assumptions from
measured source distributions and structural knowledge on the generating
system. While complete graphical criteria and procedures exist for many
identification problems, there are still challenging but important extensions
that have not been considered in the literature. To tackle these new settings,
we present a search algorithm directly over the rules of do-calculus. Due to
generality of do-calculus, the search is capable of taking more advanced
data-generating mechanisms into account along with an arbitrary type of both
observational and experimental source distributions. The search is enhanced via
a heuristic and search space reduction techniques. The approach, called
do-search, is provably sound, and it is complete with respect to
identifiability problems that have been shown to be completely characterized by
do-calculus. When extended with additional rules, the search is capable of
handling missing data problems as well. With the versatile search, we are able
to approach new problems such as combined transportability and selection bias,
or multiple sources of selection bias. We perform a systematic analysis of
bivariate missing data problems and study causal inference under case-control
design. We also present the R package dosearch that provides an interface for a
C++ implementation of the search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces. (arXiv:2012.08859v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08859">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art Neural Architecture Search (NAS) methods neither
efficiently scale to multiple hardware platforms, nor handle diverse
architectural search-spaces. To remedy this, we present DONNA (Distilling
Optimal Neural Network Architectures), a novel pipeline for rapid, scalable and
diverse NAS, that scales to many user scenarios. DONNA consists of three
phases. First, an accuracy predictor is built using blockwise knowledge
distillation from a reference model. This predictor enables searching across
diverse networks with varying macro-architectural parameters such as layer
types and attention mechanisms, as well as across micro-architectural
parameters such as block repeats and expansion rates. Second, a rapid
evolutionary search finds a set of pareto-optimal architectures for any
scenario using the accuracy predictor and on-device measurements. Third,
optimal models are quickly finetuned to training-from-scratch accuracy. DONNA
is up to 100x faster than MNasNet in finding state-of-the-art architectures
on-device. Classifying ImageNet, DONNA architectures are 20% faster than
EfficientNet-B0 and MobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5%
higher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition
to NAS, DONNA is used for search-space extension and exploration, as well as
hardware-aware model compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00433">
<div class="article-summary-box-inner">
<span><p>Broader disclosive transparency$-$truth and clarity in communication
regarding the function of AI systems$-$is widely considered desirable.
Unfortunately, it is a nebulous concept, difficult to both define and quantify.
This is problematic, as previous work has demonstrated possible trade-offs and
negative consequences to disclosive transparency, such as a confusion effect,
where 'too much information' clouds a reader's understanding of what a system
description means. Disclosive transparency's subjective nature has rendered
deep study into these problems and their remedies difficult. To improve this
state of affairs, We introduce neural language model-based probabilistic
metrics to directly model disclosive transparency, and demonstrate that they
correlate with user and expert opinions of system transparency, making them a
valid objective proxy. Finally, we demonstrate the use of these metrics in a
pilot study quantifying the relationships between transparency, confusion, and
user perceptions in a corpus of real NLP system descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Right Kind of Fairness in AI. (arXiv:2102.08453v6 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08453">
<div class="article-summary-box-inner">
<span><p>Fairness is a concept of justice. Various definitions exist, some of them
conflicting with each other. In the absence of an uniformly accepted notion of
fairness, choosing the right kind for a specific situation has always been a
central issue in human history. When it comes to implementing sustainable
fairness in artificial intelligence systems, this old question plays a key role
once again: How to identify the most appropriate fairness metric for a
particular application? The answer is often a matter of context, and the best
choice depends on ethical standards and legal requirements. Since ethics
guidelines on this topic are kept rather general for now, we aim to provide
more hands-on guidance with this document. Therefore, we first structure the
complex landscape of existing fairness metrics and explain the different
options by example. Furthermore, we propose the "Fairness Compass", a tool
which formalises the selection process and makes identifying the most
appropriate fairness definition for a given system a simple, straightforward
procedure. Because this process also allows to document the reasoning behind
the respective decisions, we argue that this approach can help to build trust
from the user through explaining and justifying the implemented fairness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Navigation of an Ultrasound Probe Towards Standard Scan Planes with Deep Reinforcement Learning. (arXiv:2103.00718v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00718">
<div class="article-summary-box-inner">
<span><p>Autonomous ultrasound (US) acquisition is an important yet challenging task,
as it involves interpretation of the highly complex and variable images and
their spatial relationships. In this work, we propose a deep reinforcement
learning framework to autonomously control the 6-D pose of a virtual US probe
based on real-time image feedback to navigate towards the standard scan planes
under the restrictions in real-world US scans. Furthermore, we propose a
confidence-based approach to encode the optimization of image quality in the
learning process. We validate our method in a simulation environment built with
real-world data collected in the US imaging of the spine. Experimental results
demonstrate that our method can perform reproducible US probe navigation
towards the standard scan plane with an accuracy of $4.91mm/4.65^\circ$ in the
intra-patient setting, and accomplish the task in the intra- and inter-patient
settings with a success rate of $92\%$ and $46\%$, respectively. The results
also show that the introduction of image quality optimization in our method can
effectively improve the navigation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transform Network Architectures for Deep Learning based End-to-End Image/Video Coding in Subsampled Color Spaces. (arXiv:2103.01760v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01760">
<div class="article-summary-box-inner">
<span><p>Most of the existing deep learning based end-to-end image/video coding (DLEC)
architectures are designed for non-subsampled RGB color format. However, in
order to achieve a superior coding performance, many state-of-the-art
block-based compression standards such as High Efficiency Video Coding
(HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for
YUV 4:2:0 format, where U and V components are subsampled by considering the
human visual system. This paper investigates various DLEC designs to support
YUV 4:2:0 format by comparing their performance against the main profiles of
HEVC and VVC standards under a common evaluation framework. Moreover, a new
transform network architecture is proposed to improve the efficiency of coding
YUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the
proposed architecture significantly outperforms naive extensions of existing
architectures designed for RGB format and achieves about 10% average BD-rate
improvement over the intra-frame coding in HEVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation. (arXiv:2103.16219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16219">
<div class="article-summary-box-inner">
<span><p>For unsupervised image-to-image translation, we propose a discriminator
architecture which focuses on the statistical features instead of individual
patches. The network is stabilized by distribution matching of key statistical
features at multiple scales. Unlike the existing methods which impose more and
more constraints on the generator, our method facilitates the shape deformation
and enhances the fine details with a greatly simplified framework. We show that
the proposed method outperforms the existing state-of-the-art models in various
challenging applications including selfie-to-anime, male-to-female and glasses
removal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximate Computing for Robotic path planning -- Experimentation, Case Study and Practical Implications. (arXiv:2104.05773v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05773">
<div class="article-summary-box-inner">
<span><p>Approximate computing is a computation domain which can be used to trade time
and energy with quality and therefore is useful in embedded systems. Energy is
the prime resource in battery-driven embedded systems, like robots. Approximate
computing can be used as a technique to generate approximate version of the
control functionalities of a robot, enabling it to ration energy for
computation at the cost of degraded quality. Usually, the programmer of the
function specifies the extent of degradation that is safe for the overall
safety of the system. However, in a collaborative environment, where several
sub-systems co-exist and some of the functionality of each of them have been
approximated, the safety of the overall system may be compromised. In this
paper, we consider multiple identical robots operate in a warehouse, and the
path planning function of the robot is approximated. Although the planned paths
are safe for individual robots (i.e. they do not collide with the racks), we
show that this leads to a collision among the robots. So, a controlled
approximation needs to be carried out in such situations to harness the full
power of this new paradigm if it needs to be a mainstream paradigm in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06669">
<div class="article-summary-box-inner">
<span><p>We propose the task of Narrative Reordering (NAREOR) which involves rewriting
a given story in a different narrative order while preserving its plot. We
present a dataset, NAREORC, with human rewritings of stories within ROCStories
in non-linear orders, and conduct a detailed analysis of it. Further, we
propose novel task-specific training methods with suitable evaluation metrics.
We perform experiments on NAREORC using state-of-the-art models such as BART
and T5 and conduct extensive automatic and human evaluations. We demonstrate
that NAREOR is a challenging task with potential for further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Guided Curriculum Learning for Neural Machine Translation. (arXiv:2105.04475v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04475">
<div class="article-summary-box-inner">
<span><p>In the field of machine learning, the well-trained model is assumed to be
able to recover the training labels, i.e. the synthetic labels predicted by the
model should be as close to the ground-truth labels as possible. Inspired by
this, we propose a self-guided curriculum strategy to encourage the learning of
neural machine translation (NMT) models to follow the above recovery criterion,
where we cast the recovery degree of each training example as its learning
difficulty. Specifically, we adopt the sentence level BLEU score as the proxy
of recovery degree. Different from existing curricula relying on linguistic
prior knowledge or third-party language models, our chosen learning difficulty
is more suitable to measure the degree of knowledge mastery of the NMT models.
Experiments on translation benchmarks, including WMT14
English$\Rightarrow$German and WMT17 Chinese$\Rightarrow$English, demonstrate
that our approach can consistently improve translation performance against
strong baseline Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmenter: Transformer for Semantic Segmentation. (arXiv:2105.05633v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05633">
<div class="article-summary-box-inner">
<span><p>Image segmentation is often ambiguous at the level of individual image
patches and requires contextual information to reach label consensus. In this
paper we introduce Segmenter, a transformer model for semantic segmentation. In
contrast to convolution-based methods, our approach allows to model global
context already at the first layer and throughout the network. We build on the
recent Vision Transformer (ViT) and extend it to semantic segmentation. To do
so, we rely on the output embeddings corresponding to image patches and obtain
class labels from these embeddings with a point-wise linear decoder or a mask
transformer decoder. We leverage models pre-trained for image classification
and show that we can fine-tune them on moderate sized datasets available for
semantic segmentation. The linear decoder allows to obtain excellent results
already, but the performance can be further improved by a mask transformer
generating class masks. We conduct an extensive ablation study to show the
impact of the different parameters, in particular the performance is better for
large models and small patch sizes. Segmenter attains excellent results for
semantic segmentation. It outperforms the state of the art on both ADE20K and
Pascal Context datasets and is competitive on Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Precision Training (AdaPT): A dynamic fixed point quantized training approach for DNNs. (arXiv:2107.13490v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13490">
<div class="article-summary-box-inner">
<span><p>Quantization is a technique for reducing deep neural networks (DNNs) training
and inference times, which is crucial for training in resource constrained
environments or applications where inference is time critical. State-of-the-art
(SOTA) quantization approaches focus on post-training quantization, i.e.,
quantization of pre-trained DNNs for speeding up inference. While work on
quantized training exists, most approaches require refinement in full precision
(usually single precision) in the final training phase or enforce a global word
length across the entire DNN. This leads to suboptimal assignments of
bit-widths to layers and, consequently, suboptimal resource usage. In an
attempt to overcome such limitations, we introduce AdaPT, a new fixed-point
quantized sparsifying training strategy. AdaPT decides about precision switches
between training epochs based on information theoretic conditions. The goal is
to determine on a per-layer basis the lowest precision that causes no
quantization-induced information loss while keeping the precision high enough
such that future learning steps do not suffer from vanishing gradients. The
benefits of the resulting fully quantized DNN are evaluated based on an
analytical performance model which we develop. We illustrate that an average
speedup of 1.27 compared to standard training in float32 with an average
accuracy increase of 0.98% can be achieved for AlexNet/ResNet on CIFAR10/100
and we further demonstrate these AdaPT trained models achieve an average
inference speedup of 2.33 with a model size reduction of 0.52.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04409">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which
would inveigle neural networks to make prediction errors with small
perturbations on the input images. Researchers have been devoted to promoting
the research on the universal adversarial perturbations (UAPs) which are
gradient-free and have little prior knowledge on data distributions. Procedural
adversarial noise attack is a data-free universal perturbation generation
method. In this paper, we propose two universal adversarial perturbation (UAP)
generation methods based on procedural noise functions: Simplex noise and
Worley noise. In our framework, the shading which disturbs visual
classification is generated with rendering technology. Without changing the
semantic representations, the adversarial examples generated via our methods
show superior performance on the attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08988">
<div class="article-summary-box-inner">
<span><p>Social media platforms provide convenient means for users to participate in
multiple online activities on various contents and create fast widespread
interactions. However, this rapidly growing access has also increased the
diverse information, and characterizing user types to understand people's
lifestyle decisions shared in social media is challenging. In this paper, we
propose a weakly supervised graph embedding based framework for understanding
user types. We evaluate the user embedding learned using weak supervision over
well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.
Experiments on real-world datasets demonstrate that the proposed framework
outperforms the baselines for detecting user types. Finally, we illustrate data
analysis on different types of users (e.g., practitioner vs. promotional) from
our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our
method for constructing user representation readily generalizes to other
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation. (arXiv:2108.10511v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10511">
<div class="article-summary-box-inner">
<span><p>Practical recommender systems experience a cold-start problem when observed
user-item interactions in the history are insufficient. Meta learning,
especially gradient based one, can be adopted to tackle this problem by
learning initial parameters of the model and thus allowing fast adaptation to a
specific task from limited data examples. Though with significant performance
improvement, it commonly suffers from two critical issues: the
non-compatibility with mainstream industrial deployment and the heavy
computational burdens, both due to the inner-loop gradient operation. These two
issues make them hard to be applied in practical recommender systems. To enjoy
the benefits of meta learning framework and mitigate these problems, we propose
a recommendation framework called Contextual Modulation Meta Learning (CMML).
CMML is composed of fully feed-forward operations so it is computationally
efficient and completely compatible with the mainstream industrial deployment.
CMML consists of three components, including a context encoder that can
generate context embedding to represent a specific task, a hybrid context
generator that aggregates specific user-item features with task-level context,
and a contextual modulation network, which can modulate the recommendation
model to adapt effectively. We validate our approach on both scenario-specific
and user-specific cold-start setting on various real-world datasets, showing
CMML can achieve comparable or even better performance with gradient based
methods yet with much higher computational efficiency and better
interpretability.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Seq2Seq Autoencoder via Contrastive Learning for Abstractive Text Summarization. (arXiv:2108.11992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11992">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a denoising sequence-to-sequence (seq2seq)
autoencoder via contrastive learning for abstractive text summarization. Our
model adopts a standard Transformer-based architecture with a multi-layer
bi-directional encoder and an auto-regressive decoder. To enhance its denoising
ability, we incorporate self-supervised contrastive learning along with various
sentence-level document augmentation. These two components, seq2seq autoencoder
and contrastive learning, are jointly trained through fine-tuning, which
improves the performance of text summarization with regard to ROUGE scores and
human evaluation. We conduct experiments on two datasets and demonstrate that
our model outperforms many existing benchmarks and even achieves comparable
performance to the state-of-the-art abstractive systems trained with more
complex architecture and extensive computation resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Sentence Ordering Method Using BERT Pretrained Model. (arXiv:2108.11994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11994">
<div class="article-summary-box-inner">
<span><p>Building systems with capability of natural language understanding (NLU) has
been one of the oldest areas of AI. An essential component of NLU is to detect
logical succession of events contained in a text. The task of sentence ordering
is proposed to learn succession of events with applications in AI tasks. The
performance of previous works employing statistical methods is poor, while the
neural networks-based approaches are in serious need of large corpora for model
learning. In this paper, we propose a method for sentence ordering which does
not need a training phase and consequently a large corpus for learning. To this
end, we generate sentence embedding using BERT pre-trained model and measure
sentence similarity using cosine similarity score. We suggest this score as an
indicator of sequential events' level of coherence. We finally sort the
sentences through brute-force search to maximize overall similarities of the
sequenced sentences. Our proposed method outperformed other baselines on
ROCStories, a corpus of 5-sentence human-made stories. The method is
specifically more efficient than neural network-based methods when no huge
corpus is available. Among other advantages of this method are its
interpretability and needlessness to linguistic knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa. (arXiv:2108.12009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12009">
<div class="article-summary-box-inner">
<span><p>We present EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with
RoBERTa, a simple yet expressive scheme of solving the ERC (emotion recognition
in conversation) task. By simply prepending speaker names to utterances and
inserting separation tokens between the utterances in a dialogue, EmoBERTa can
learn intra- and inter- speaker states and context to predict the emotion of a
current speaker, in an end-to-end manner. Our experiments show that we reach a
new state of the art on the two popular ERC datasets using a basic and
straight-forward approach. We've open sourced our code and models at
https://github.com/tae898/erc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-based Self-Critical Training For Question Generation. (arXiv:2108.12026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12026">
<div class="article-summary-box-inner">
<span><p>We present in this work a fully Transformer-based reinforcement learning
generator-evaluator architecture for neural question generation. Question
generation is a task that consists in generating questions given a context and
answer. To improve the quality of the generated question, we came up with a
semantic-based self-critical training layout in generator-evaluator
architecture, which goes beyond typical maximum likelihood training. Evaluation
metrics for language modeling only based on n-gram overlapping do not consider
semantic relations between reference and candidate strings. To improve the
evaluation step, we assess our model for both n-gram overlap using BLEU and
semantically using BERTScore and NUBIA, a novel state-of-the-art evaluation
metric for text generation. Question generation could be used in many
downstream applications, including in extending question answering datasets,
conversational systems, and educational assessment systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using GAN-based models to sentimental analysis on imbalanced datasets in education domain. (arXiv:2108.12061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12061">
<div class="article-summary-box-inner">
<span><p>While the whole world is still struggling with the COVID-19 pandemic, online
learning and home office become more common. Many schools transfer their
courses teaching to the online classroom. Therefore, it is significant to mine
the students' feedback and opinions from their reviews towards studies so that
both schools and teachers can know where they need to improve. This paper
trains machine learning and deep learning models using both balanced and
imbalanced datasets for sentiment classification. Two SOTA category-aware text
generation GAN models: CatGAN and SentiGAN, are utilized to synthesize text
used to balance the highly imbalanced dataset. Results on three datasets with
different imbalance degree from distinct domains show that when using generated
text to balance the dataset, the F1-score of machine learning and deep learning
model on sentiment classification increases 2.79% ~ 9.28%. Also, the results
indicate that the average growth degree for CR100k is higher than CR23k, the
average growth degree for deep learning is more increased than machine learning
algorithms, and the average growth degree for more complex deep learning models
is more increased than simpler deep learning models in experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4-bit Quantization of LSTM-based Speech Recognition Models. (arXiv:2108.12074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12074">
<div class="article-summary-box-inner">
<span><p>We investigate the impact of aggressive low-precision representations of
weights and activations in two families of large LSTM-based architectures for
Automatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden
Markov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers
(RNN-Ts). Using a 4-bit integer representation, a na\"ive quantization approach
applied to the LSTM portion of these models results in significant Word Error
Rate (WER) degradation. On the other hand, we show that minimal accuracy loss
is achievable with an appropriate choice of quantizers and initializations. In
particular, we customize quantization schemes depending on the local properties
of the network, improving recognition performance while limiting computational
time. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH)
test sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or
2000 hours of SWB data achieves $&lt;$0.5% and $&lt;$1% average WER degradation,
respectively. On the more challenging RNN-T models, our quantization strategy
limits degradation in 4-bit inference to 1.3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
<div class="article-summary-box-inner">
<span><p>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lingxi: A Diversity-aware Chinese Modern Poetry Generation System. (arXiv:2108.12108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12108">
<div class="article-summary-box-inner">
<span><p>Poetry generation has been a difficult task in natural language processing.
Unlike plain neural text generation tasks, poetry has a high requirement for
novelty, since an easily-understood sentence with too many high frequency words
might not be considered as poetic, while adequately ambiguous sentences with
low frequency words can possibly be novel and creative. Inspired by this, we
present Lingxi, a diversity-aware Chinese modern poetry generation system. We
propose nucleus sampling with randomized head (NS-RH) algorithm, which
randomizes the high frequency part ("head") of the predicted distribution, in
order to emphasize on the "comparatively low frequency" words. The proposed
algorithm can significantly increase the novelty of generated poetry compared
with traditional sampling methods. The permutation of distribution is
controllable by tuning the filtering parameter that determines the "head" to
permutate, achieving diversity-aware sampling. We find that even when a large
portion of filtered vocabulary is randomized, it can actually generate fluent
poetry but with notably higher novelty. We also propose a
semantic-similarity-based rejection sampling algorithm, which creates longer
and more informative context on the basis of the short input poetry title while
maintaining high semantic similarity to the title, alleviating the off-topic
issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Generation of Accurate \& Fluent Medical X-ray Reports. (arXiv:2108.12126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12126">
<div class="article-summary-box-inner">
<span><p>Our paper focuses on automating the generation of medical reports from chest
X-ray image inputs, a critical yet time-consuming task for radiologists. Unlike
existing medical re-port generation efforts that tend to produce human-readable
reports, we aim to generate medical reports that are both fluent and clinically
accurate. This is achieved by our fully differentiable and end-to-end paradigm
containing three complementary modules: taking the chest X-ray images and
clinical his-tory document of patients as inputs, our classification module
produces an internal check-list of disease-related topics, referred to as
enriched disease embedding; the embedding representation is then passed to our
transformer-based generator, giving rise to the medical reports; meanwhile, our
generator also pro-duces the weighted embedding representation, which is fed to
our interpreter to ensure consistency with respect to disease-related
topics.Our approach achieved promising results on commonly-used metrics
concerning language fluency and clinical accuracy. Moreover, noticeable
performance gains are consistently ob-served when additional input information
is available, such as the clinical document and extra scans of different views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Secoco: Self-Correcting Encoding for Neural Machine Translation. (arXiv:2108.12137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12137">
<div class="article-summary-box-inner">
<span><p>This paper presents Self-correcting Encoding (Secoco), a framework that
effectively deals with input noise for robust neural machine translation by
introducing self-correcting predictors. Different from previous robust
approaches, Secoco enables NMT to explicitly correct noisy inputs and delete
specific errors simultaneously with the translation decoding process. Secoco is
able to achieve significant improvements over strong baselines on two
real-world test sets and a benchmark WMT dataset with good interpretability. We
will make our code and dataset publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving callsign recognition with air-surveillance data in air-traffic communication. (arXiv:2108.12156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12156">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) can be used as the assistance of speech
communication between pilots and air-traffic controllers. Its application can
significantly reduce the complexity of the task and increase the reliability of
transmitted information. Evidently, high accuracy predictions are needed to
minimize the risk of errors. Especially, high accuracy is required in
recognition of key information, such as commands and callsigns, used to
navigate pilots. Our results prove that the surveillance data containing
callsigns can help to considerably improve the recognition of a callsign in an
utterance when the weights of probable callsign n-grams are reduced per
utterance. In this paper, we investigate two approaches: (1) G-boosting, when
callsigns weights are adjusted at language model level (G) and followed by the
dynamic decoder with an on-the-fly composition, and (2) lattice rescoring when
callsign information is introduced on top of lattices generated using a
conventional decoder. Boosting callsign n-grams with the combination of two
methods allowed us to gain 28.4% of absolute improvement in callsign
recognition accuracy and up to 74.2% of relative improvement in WER of callsign
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar Based Identification Of Speaker Role For Improving ATCO And Pilot ASR. (arXiv:2108.12175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12175">
<div class="article-summary-box-inner">
<span><p>Assistant Based Speech Recognition (ABSR) for air traffic control is
generally trained by pooling both Air Traffic Controller (ATCO) and pilot data.
In practice, this is motivated by the fact that the proportion of pilot data is
lesser compared to ATCO while their standard language of communication is
similar. However, due to data imbalance of ATCO and pilot and their varying
acoustic conditions, the ASR performance is usually significantly better for
ATCOs than pilots. In this paper, we propose to (1) split the ATCO and pilot
data using an automatic approach exploiting ASR transcripts, and (2) consider
ATCO and pilot ASR as two separate tasks for Acoustic Model (AM) training. For
speaker role classification of ATCO and pilot data, a hypothesized ASR
transcript is generated with a seed model, subsequently used to classify the
speaker role based on the knowledge extracted from grammar defined by
International Civil Aviation Organization (ICAO). This approach provides an
average speaker role identification accuracy of 83% for ATCO and pilot.
Finally, we show that training AMs separately for each task, or using a
multitask approach is well suited for this data compared to AM trained by
pooling all data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling. (arXiv:2108.12177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12177">
<div class="article-summary-box-inner">
<span><p>Social media has effectively become the prime hub of communication and
digital marketing. As these platforms enable the free manifestation of thoughts
and facts in text, images and video, there is an extensive need to screen them
to protect individuals and groups from offensive content targeted at them. Our
work intends to classify codemixed social media comments/posts in the Dravidian
languages of Tamil, Kannada, and Malayalam. We intend to improve offensive
language identification by generating pseudo-labels on the dataset. A custom
dataset is constructed by transliterating all the code-mixed texts into the
respective Dravidian language, either Kannada, Malayalam, or Tamil and then
generating pseudo-labels for the transliterated dataset. The two datasets are
combined using the generated pseudo-labels to create a custom dataset called
CMTRA. As Dravidian languages are under-resourced, our approach increases the
amount of training data for the language models. We fine-tune several recent
pretrained language models on the newly constructed dataset. We extract the
pretrained language embeddings and pass them onto recurrent neural networks. We
observe that fine-tuning ULMFiT on the custom dataset yields the best results
on the code-mixed test sets of all three languages. Our approach yields the
best results among the benchmarked models on Tamil-English, achieving a
weighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of
0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and
Kannada-English, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12189">
<div class="article-summary-box-inner">
<span><p>This paper presents Macquarie University's participation to the BioASQ
Synergy Task, and BioASQ9b Phase B. In each of these tasks, our participation
focused on the use of query-focused extractive summarisation to obtain the
ideal answers to medical questions. The Synergy Task is an end-to-end question
answering task on COVID-19 where systems are required to return relevant
documents, snippets, and answers to a given question. Given the absence of
training data, we used a query-focused summarisation system that was trained
with the BioASQ8b training data set and we experimented with methods to
retrieve the documents and snippets. Considering the poor quality of the
documents and snippets retrieved by our system, we observed reasonably good
quality in the answers returned. For phase B of the BioASQ9b task, the relevant
documents and snippets were already included in the test data. Our system split
the snippets into candidate sentences and used BERT variants under a sentence
classification setup. The system used the question and candidate sentence as
input and was trained to predict the likelihood of the candidate sentence being
part of the ideal answer. The runs obtained either the best or second best
ROUGE-F1 results of all participants to all batches of BioASQ9b. This shows
that using BERT in a classification setup is a very strong baseline for the
identification of ideal answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translation Error Detection as Rationale Extraction. (arXiv:2108.12197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12197">
<div class="article-summary-box-inner">
<span><p>Recent Quality Estimation (QE) models based on multilingual pre-trained
representations have achieved very competitive results when predicting the
overall quality of translated sentences. Predicting translation errors, i.e.
detecting specifically which words are incorrect, is a more challenging task,
especially with limited amounts of training data. We hypothesize that, not
unlike humans, successful QE models rely on translation errors to predict
overall sentence quality. By exploring a set of feature attribution methods
that assign relevance scores to the inputs to explain model predictions, we
study the behaviour of state-of-the-art sentence-level QE models and show that
explanations (i.e. rationales) extracted from these models can indeed be used
to detect translation errors. We therefore (i) introduce a novel
semi-supervised method for word-level QE and (ii) propose to use the QE task as
a new benchmark for evaluating the plausibility of feature attribution, i.e.
how interpretable model explanations are to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features are dependent
upon each other. Experiment results on five public datasets show that our model
performs significantly better than previous approaches. The source code can be
found in https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors. (arXiv:2108.12216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12216">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the capacity of a language model-based method for
grammatical error detection in detail. We first show that 5 to 10% of training
data are enough for a BERT-based error detection method to achieve performance
equivalent to a non-language model-based method can achieve with the full
training data; recall improves much faster with respect to training data size
in the BERT-based method than in the non-language model method while precision
behaves similarly. These suggest that (i) the BERT-based method should have a
good knowledge of grammar required to recognize certain types of error and that
(ii) it can transform the knowledge into error detection rules by fine-tuning
with a few training samples, which explains its high generalization ability in
grammatical error detection. We further show with pseudo error data that it
actually exhibits such nice properties in learning rules for recognizing
various types of error. Finally, based on these findings, we explore a
cost-effective method for detecting grammatical errors with feedback comments
explaining relevant grammatical rules to learners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Text in Self-Supervised Speech Pretraining. (arXiv:2108.12226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12226">
<div class="article-summary-box-inner">
<span><p>Self-supervised pretraining for Automated Speech Recognition (ASR) has shown
varied degrees of success. In this paper, we propose to jointly learn
representations during pretraining from two different modalities: speech and
text. The proposed method, tts4pretrain complements the power of contrastive
learning in self-supervision with linguistic/lexical representations derived
from synthesized speech, effectively learning from untranscribed speech and
unspoken text. Lexical learning in the speech encoder is enforced through an
additional sequence loss term that is coupled with contrastive loss during
pretraining. We demonstrate that this novel pretraining method yields Word
Error Rate (WER) reductions of 10% relative on the well-benchmarked,
Librispeech task over a state-of-the-art baseline pretrained with wav2vec2.0
only. The proposed method also serves as an effective strategy to compensate
for the lack of transcribed speech, effectively matching the performance of
5000 hours of transcribed speech with just 100 hours of transcribed speech on
the AMI meeting transcription task. Finally, we demonstrate WER reductions of
up to 15% on an in-house Voice Search task over traditional pretraining.
Incorporating text into encoder pretraining is complimentary to rescoring with
a larger or in-domain language model, resulting in additional 6% relative
reduction in WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Robustness of Neural Language Models to Input Perturbations. (arXiv:2108.12237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12237">
<div class="article-summary-box-inner">
<span><p>High-performance neural language models have obtained state-of-the-art
results on a wide range of Natural Language Processing (NLP) tasks. However,
results for common benchmark datasets often do not reflect model reliability
and robustness when applied to noisy, real-world data. In this study, we design
and implement various types of character-level and word-level perturbation
methods to simulate realistic scenarios in which input texts may be slightly
noisy or different from the data distribution on which NLP systems were
trained. Conducting comprehensive experiments on different NLP tasks, we
investigate the ability of high-performance language models such as BERT,
XLNet, RoBERTa, and ELMo in handling different types of input perturbations.
The results suggest that language models are sensitive to input perturbations
and their performance can decrease even when small changes are introduced. We
highlight that models need to be further improved and that current benchmarks
are not reflecting model robustness well. We argue that evaluations on
perturbed inputs should routinely complement widely-used benchmarks in order to
yield a more realistic understanding of NLP systems robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning models are not robust against noise in clinical text. (arXiv:2108.12242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12242">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) systems are attracting increasing interest in
the medical domain due to their ability to learn complicated tasks that require
human intelligence and expert knowledge. AI systems that utilize
high-performance Natural Language Processing (NLP) models have achieved
state-of-the-art results on a wide variety of clinical text processing
benchmarks. They have even outperformed human accuracy on some tasks. However,
performance evaluation of such AI systems have been limited to accuracy
measures on curated and clean benchmark datasets that may not properly reflect
how robustly these systems can operate in real-world situations. In order to
address this challenge, we introduce and implement a wide variety of
perturbation methods that simulate different types of noise and variability in
clinical text data. While noisy samples produced by these perturbation methods
can often be understood by humans, they may cause AI systems to make erroneous
decisions. Conducting extensive experiments on several clinical text processing
tasks, we evaluated the robustness of high-performance NLP models against
various types of character-level and word-level noise. The results revealed
that the NLP models performance degrades when the input contains small amounts
of noise. This study is a significant step towards exposing vulnerabilities of
AI models utilized in clinical text processing systems. The proposed
perturbation methods can be used in performance evaluation tests to assess how
robustly clinical NLP models can operate on noisy data, in real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propaganda on the Sentence Level during the COVID-19 Pandemic. (arXiv:2108.12269v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12269">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation, conspiracy, and questionable content and
information manipulation by foreign adversaries on social media has surged
along with the COVID-19 pandemic. Such malicious cyber-enabled actions may
cause increasing social polarization, health crises, and property loss. In this
paper, using fine-tuned contextualized embedding trained on Reddit, we tackle
the detection of the propaganda of such user accounts and their targeted issues
on Twitter during March 2020 when the COVID-19 epidemic became recognized as a
pandemic. Our result shows that the pro-China group appeared to be tweeting 35
to 115 times more than the neutral group. At the same time, neutral groups were
tweeting more positive-attitude content and voicing alarm for the COVID-19
situation. The pro-China group was also using more call-for-action words on
political issues not necessarily China-related.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?. (arXiv:2108.12275v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12275">
<div class="article-summary-box-inner">
<span><p>In this paper we address the problem of fine-tuned text generation with a
limited computational budget. For that, we use a well-performing text
generative adversarial network (GAN) architecture - Diversity-Promoting GAN
(DPGAN), and attempted a drop-in replacement of the LSTM layer with a
self-attention-based Transformer layer in order to leverage their efficiency.
The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,
quality and diversity of generated text and stability. Computational
experiments suggested that a transformer architecture is unable to drop-in
replace the LSTM layer, under-performing during the pre-training phase and
undergoing a complete mode collapse during the GAN tuning phase. Our results
suggest that the transformer architecture need to be adapted before it can be
used as a replacement for RNNs in text-generating GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree Decomposition Attention for AMR-to-Text Generation. (arXiv:2108.12300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12300">
<div class="article-summary-box-inner">
<span><p>Text generation from AMR requires mapping a semantic graph to a string that
it annotates. Transformer-based graph encoders, however, poorly capture vertex
dependencies that may benefit sequence prediction. To impose order on an
encoder, we locally constrain vertex self-attention using a graph's tree
decomposition. Instead of forming a full query-key bipartite graph, we restrict
attention to vertices in parent, subtree, and same-depth bags of a vertex. This
hierarchical context lends both sparsity and structure to vertex state updates.
We apply dynamic programming to derive a forest of tree decompositions,
choosing the most structurally similar tree to the AMR. Our system outperforms
a self-attentive baseline by 1.6 BLEU and 1.8 chrF++.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Tree Decomposition Parsers for AMR-to-Text Generation. (arXiv:2108.12304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12304">
<div class="article-summary-box-inner">
<span><p>Graph encoders in AMR-to-text generation models often rely on neighborhood
convolutions or global vertex attention. While these approaches apply to
general graphs, AMRs may be amenable to encoders that target their tree-like
structure. By clustering edges into a hierarchy, a tree decomposition
summarizes graph structure. Our model encodes a derivation forest of tree
decompositions and extracts an expected tree. From tree node embeddings, it
builds graph edge features used in vertex attention of the graph encoder.
Encoding TD forests instead of shortest-pairwise paths in a self-attentive
baseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also
surpasses a convolutional baseline for molecular property prediction by 1.92%
ROC-AUC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAPE: Context-Aware Private Embeddings for Private Language Learning. (arXiv:2108.12318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12318">
<div class="article-summary-box-inner">
<span><p>Deep learning-based language models have achieved state-of-the-art results in
a number of applications including sentiment analysis, topic labelling, intent
classification and others. Obtaining text representations or embeddings using
these models presents the possibility of encoding personally identifiable
information learned from language and context cues that may present a risk to
reputation or privacy. To ameliorate these issues, we propose Context-Aware
Private Embeddings (CAPE), a novel approach which preserves privacy during
training of embeddings. To maintain the privacy of text representations, CAPE
applies calibrated noise through differential privacy, preserving the encoded
semantic links while obscuring sensitive information. In addition, CAPE employs
an adversarial training regime that obscures identified private variables.
Experimental results demonstrate that the proposed approach reduces private
information leakage better than either single intervention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning. (arXiv:2108.12370v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12370">
<div class="article-summary-box-inner">
<span><p>We demonstrate a library for the integration of domain knowledge in deep
learning architectures. Using this library, the structure of the data is
expressed symbolically via graph declarations and the logical constraints over
outputs or latent variables can be seamlessly added to the deep models. The
domain knowledge can be defined explicitly, which improves the models'
explainability in addition to the performance and generalizability in the
low-data regime. Several approaches for such an integration of symbolic and
sub-symbolic models have been introduced; however, there is no library to
facilitate the programming for such an integration in a generic way while
various underlying algorithms can be used. Our library aims to simplify
programming for such an integration in both training and inference phases while
separating the knowledge representation from learning algorithms. We showcase
various NLP benchmark tasks and beyond. The framework is publicly available at
Github(https://github.com/HLR/DomiKnowS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. (arXiv:2108.12409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12409">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the transformer model by Vaswani et al. (2017), a
fundamental question remains open: how to achieve extrapolation at inference
time to longer sequences than seen during training? We first show that
extrapolation can be improved by changing the position representation method,
though we find that existing proposals do not allow efficient extrapolation. We
introduce a simple and efficient method, Attention with Linear Biases (ALiBi),
that allows for extrapolation. ALiBi does not add positional embeddings to the
word embeddings; instead, it biases the query-key attention scores with a term
that is proportional to their distance. We show that this method allows
training a 1.3 billion parameter model on input sequences of length 1024 that
extrapolates to input sequences of length 2048, achieving the same perplexity
as a sinusoidal position embedding model trained on inputs of length 2048, 11%
faster and using 11% less memory. ALiBi's inductive bias towards recency allows
it to outperform multiple strong position methods on the WikiText-103
benchmark. Finally, we provide analysis of ALiBi to understand why it leads to
better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach. (arXiv:2003.10715v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.10715">
<div class="article-summary-box-inner">
<span><p>Knowledge about the software used in scientific investigations is necessary
for different reasons, including provenance of the results, measuring software
impact to attribute developers, and bibliometric software citation analysis in
general. Additionally, providing information about whether and how the software
and the source code are available allows an assessment about the state and role
of open source software in science in general. While such analyses can be done
manually, large scale analyses require the application of automated methods of
information extraction and linking. In this paper, we present SoftwareKG - a
knowledge graph that contains information about software mentions from more
than 51,000 scientific articles from the social sciences. A silver standard
corpus, created by a distant and weak supervision approach, and a gold standard
corpus, created by manual annotation, were used to train an LSTM based neural
network to identify software mentions in scientific articles. The model
achieves a recognition rate of .82 F-score in exact matches. As a result, we
identified more than 133,000 software mentions. For entity disambiguation, we
used the public domain knowledge base DBpedia. Furthermore, we linked the
entities of the knowledge graph to other knowledge bases such as the Microsoft
Academic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we
illustrate, how SoftwareKG can be used to assess the role of software in the
social sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of on-device fully neural end-to-end automatic speech recognition algorithms. (arXiv:2012.07974v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07974">
<div class="article-summary-box-inner">
<span><p>In this paper, we review various end-to-end automatic speech recognition
algorithms and their optimization techniques for on-device applications.
Conventional speech recognition systems comprise a large number of discrete
components such as an acoustic model, a language model, a pronunciation model,
a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted
Finite State Transducer (WFST), and so on. To obtain sufficiently high speech
recognition accuracy with such conventional speech recognition systems, a very
large language model (up to 100 GB) is usually needed. Hence, the corresponding
WFST size becomes enormous, which prohibits their on-device implementation.
Recently, fully neural network end-to-end speech recognition algorithms have
been proposed. Examples include speech recognition systems based on
Connectionist Temporal Classification (CTC), Recurrent Neural Network
Transducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic
Chunk-wise Attention (MoChA), transformer-based speech recognition systems, and
so on. These fully neural network-based systems require much smaller memory
footprints compared to conventional algorithms, therefore their on-device
implementation has become feasible. In this paper, we review such end-to-end
speech recognition models. We extensively discuss their structures,
performance, and advantages compared to conventional algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00433">
<div class="article-summary-box-inner">
<span><p>Broader disclosive transparency$-$truth and clarity in communication
regarding the function of AI systems$-$is widely considered desirable.
Unfortunately, it is a nebulous concept, difficult to both define and quantify.
This is problematic, as previous work has demonstrated possible trade-offs and
negative consequences to disclosive transparency, such as a confusion effect,
where 'too much information' clouds a reader's understanding of what a system
description means. Disclosive transparency's subjective nature has rendered
deep study into these problems and their remedies difficult. To improve this
state of affairs, We introduce neural language model-based probabilistic
metrics to directly model disclosive transparency, and demonstrate that they
correlate with user and expert opinions of system transparency, making them a
valid objective proxy. Finally, we demonstrate the use of these metrics in a
pilot study quantifying the relationships between transparency, confusion, and
user perceptions in a corpus of real NLP system descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Semi-Supervised Learning: An Approach To Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems. (arXiv:2104.03643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03643">
<div class="article-summary-box-inner">
<span><p>Air traffic management and specifically air-traffic control (ATC) rely mostly
on voice communications between Air Traffic Controllers (ATCos) and pilots. In
most cases, these voice communications follow a well-defined grammar that could
be leveraged in Automatic Speech Recognition (ASR) technologies. The callsign
used to address an airplane is an essential part of all ATCo-pilot
communications. We propose a two-steps approach to add contextual knowledge
during semi-supervised training to reduce the ASR system error rates at
recognizing the part of the utterance that contains the callsign. Initially, we
represent in a WFST the contextual knowledge (i.e. air-surveillance data) of an
ATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the
contextual knowledge is added by second-pass decoding (i.e. lattice
re-scoring). Results show that `unseen domains' (e.g. data from airports not
present in the supervised training data) are further aided by contextual SSL
when compared to standalone SSL. For this task, we introduce the Callsign Word
Error Rate (CA-WER) as an evaluation metric, which only assesses ASR
performance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER
relative improvement applying SSL with an additional 17.5% CA-WER improvement
by adding contextual knowledge during SSL on a challenging ATC-based test set
gathered from LiveATC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Remove: Towards Isotropic Pre-trained BERT Embedding. (arXiv:2104.05274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05274">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models such as BERT have become a more common choice of
natural language processing (NLP) tasks. Research in word representation shows
that isotropic embeddings can significantly improve performance on downstream
tasks. However, we measure and analyze the geometry of pre-trained BERT
embedding and find that it is far from isotropic. We find that the word vectors
are not centered around the origin, and the average cosine similarity between
two random words is much higher than zero, which indicates that the word
vectors are distributed in a narrow cone and deteriorate the representation
capacity of word embedding. We propose a simple, and yet effective method to
fix this problem: remove several dominant directions of BERT embedding with a
set of learnable weights. We train the weights on word similarity tasks and
show that processed embedding is more isotropic. Our method is evaluated on
three standardized tasks: word similarity, word analogy, and semantic textual
similarity. In all tasks, the word embedding processed by our method
consistently outperforms the original embedding (with average improvement of
13% on word analogy and 16% on semantic textual similarity) and two baseline
methods. Our method is also proven to be more robust to changes of
hyperparameter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06669">
<div class="article-summary-box-inner">
<span><p>We propose the task of Narrative Reordering (NAREOR) which involves rewriting
a given story in a different narrative order while preserving its plot. We
present a dataset, NAREORC, with human rewritings of stories within ROCStories
in non-linear orders, and conduct a detailed analysis of it. Further, we
propose novel task-specific training methods with suitable evaluation metrics.
We perform experiments on NAREORC using state-of-the-art models such as BART
and T5 and conduct extensive automatic and human evaluations. We demonstrate
that NAREOR is a challenging task with potential for further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08808">
<div class="article-summary-box-inner">
<span><p>The ability to continuously expand knowledge over time and utilize it to
rapidly generalize to new tasks is a key feature of human linguistic
intelligence. Existing models that pursue rapid generalization to new tasks
(e.g., few-shot learning methods), however, are mostly trained in a single shot
on fixed datasets, unable to dynamically expand their knowledge; while
continual learning algorithms are not specifically designed for rapid
generalization. We present a new learning setup, Continual Learning of Few-Shot
Learners (CLIF), to address the challenges of both learning settings in a
unified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks
arriving sequentially, accumulating knowledge for improved generalization to
new tasks, while also retaining performance on the tasks learned earlier. We
examine how the generalization ability is affected in the continual learning
setup, evaluate a number of continual learning algorithms, and propose a novel
regularized adapter generation approach. We find that catastrophic forgetting
affects generalization ability to a less degree than performance on seen tasks;
while continual learning algorithms can still bring considerable benefit to the
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08809">
<div class="article-summary-box-inner">
<span><p>Determining coreference of concept mentions across multiple documents is a
fundamental task in natural language understanding. Work on cross-document
coreference resolution (CDCR) typically considers mentions of events in the
news, which seldom involve abstract technical concepts that are prevalent in
science and technology. These complex concepts take diverse or ambiguous forms
and have many hierarchical levels of granularity (e.g., tasks and subtasks),
posing challenges for CDCR. We present a new task of Hierarchical CDCR (H-CDCR)
with the goal of jointly inferring coreference clusters and hierarchy between
them. We create SciCo, an expert-annotated dataset for H-CDCR in scientific
papers, 3X larger than the prominent ECB+ resource. We study strong baseline
models that we customize for H-CDCR, and highlight challenges for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08836">
<div class="article-summary-box-inner">
<span><p>Multimodal pre-training with text, layout, and image has achieved SOTA
performance for visually-rich document understanding tasks recently, which
demonstrates the great potential for joint learning across different
modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model
for multilingual document understanding, which aims to bridge the language
barriers for visually-rich document understanding. To accurately evaluate
LayoutXLM, we also introduce a multilingual form understanding benchmark
dataset named XFUND, which includes form understanding samples in 7 languages
(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and
key-value pairs are manually labeled for each language. Experiment results show
that the LayoutXLM model has significantly outperformed the existing SOTA
cross-lingual pre-trained models on the XFUND dataset. The pre-trained
LayoutXLM model and the XFUND dataset are publicly available at
https://aka.ms/layoutxlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09106">
<div class="article-summary-box-inner">
<span><p>Subword units are commonly used for end-to-end automatic speech recognition
(ASR), while a fully acoustic-oriented subword modeling approach is somewhat
missing. We propose an acoustic data-driven subword modeling (ADSM) approach
that adapts the advantages of several text-based and acoustic-based subword
methods into one pipeline. With a fully acoustic-oriented label design and
learning process, ADSM produces acoustic-structured subword units and
acoustic-matched target sequence for further ASR training. The obtained ADSM
labels are evaluated with different end-to-end ASR approaches including CTC,
RNN-Transducer and attention models. Experiments on the LibriSpeech corpus show
that ADSM clearly outperforms both byte pair encoding (BPE) and
pronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis
shows that ADSM achieves acoustically more logical word segmentation and more
balanced sequence length, and thus, is suitable for both time-synchronous and
label-synchronous models. We also briefly describe how to apply acoustic-based
subword regularization and unseen text segmentation using ADSM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Guided Curriculum Learning for Neural Machine Translation. (arXiv:2105.04475v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04475">
<div class="article-summary-box-inner">
<span><p>In the field of machine learning, the well-trained model is assumed to be
able to recover the training labels, i.e. the synthetic labels predicted by the
model should be as close to the ground-truth labels as possible. Inspired by
this, we propose a self-guided curriculum strategy to encourage the learning of
neural machine translation (NMT) models to follow the above recovery criterion,
where we cast the recovery degree of each training example as its learning
difficulty. Specifically, we adopt the sentence level BLEU score as the proxy
of recovery degree. Different from existing curricula relying on linguistic
prior knowledge or third-party language models, our chosen learning difficulty
is more suitable to measure the degree of knowledge mastery of the NMT models.
Experiments on translation benchmarks, including WMT14
English$\Rightarrow$German and WMT17 Chinese$\Rightarrow$English, demonstrate
that our approach can consistently improve translation performance against
strong baseline Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08988">
<div class="article-summary-box-inner">
<span><p>Social media platforms provide convenient means for users to participate in
multiple online activities on various contents and create fast widespread
interactions. However, this rapidly growing access has also increased the
diverse information, and characterizing user types to understand people's
lifestyle decisions shared in social media is challenging. In this paper, we
propose a weakly supervised graph embedding based framework for understanding
user types. We evaluate the user embedding learned using weak supervision over
well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.
Experiments on real-world datasets demonstrate that the proposed framework
outperforms the baselines for detecting user types. Finally, we illustrate data
analysis on different types of users (e.g., practitioner vs. promotional) from
our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our
method for constructing user representation readily generalizes to other
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09119">
<div class="article-summary-box-inner">
<span><p>With the development of deep learning (DL), natural language processing (NLP)
makes it possible for us to analyze and understand a large amount of language
texts. Accordingly, we can achieve a semantic communication in terms of joint
semantic source and channel coding over a noisy channel with the help of NLP.
However, the existing method to realize this goal is to use a fixed transformer
of NLP while ignoring the difference of semantic information contained in each
sentence. To solve this problem, we propose a new semantic communication system
based on Universal Transformer. Compared with the traditional transformer, an
adaptive circulation mechanism is introduced in the Universal Transformer.
Through the introduction of the circulation mechanism, the new semantic
communication system can be more flexible to transmit sentences with different
semantic information, and achieve better end-to-end performance under various
channel conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns of a table. Generally, RE models for this task require
labelled tables for training. These labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, unlike real tables,
these synthetic tables lack associated metadata, such as, column-headers,
captions, etc; this is because synthetic tables are created out of KGs that do
not store such metadata. Meanwhile, previous works have shown that metadata is
important for accurate RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables.
Afterward, we experiment with a BERT-based model, in line with recently
published works, that takes as input a combination of proposed artificial
metadata and table content. Our empirical results show that this leads to an
improvement of 9\%-45\% in F1 score, in absolute terms, over 2 tabular
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10939">
<div class="article-summary-box-inner">
<span><p>Offensive Language detection in social media platforms has been an active
field of research over the past years. In non-native English spoken countries,
social media users mostly use a code-mixed form of text in their
posts/comments. This poses several challenges in the offensive content
identification tasks, and considering the low resources available for Tamil,
the task becomes much harder. The current study presents extensive experiments
using multiple deep learning, and transfer learning models to detect offensive
content on YouTube. We propose a novel and flexible approach of selective
translation and transliteration techniques to reap better results from
fine-tuning and ensembling multilingual transformer networks like BERT, Distil-
BERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best
model for this task. The best performing models were ULMFiT and mBERTBiLSTM for
this Tamil code-mix dataset instead of more popular transfer learning models
such as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The
proposed model ULMFiT and mBERTBiLSTM yielded good results and are promising
for effective offensive speech identification in low-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutReader: Pre-training of Text and Layout for Reading Order Detection. (arXiv:2108.11591v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11591">
<div class="article-summary-box-inner">
<span><p>Reading order detection is the cornerstone to understanding visually-rich
documents (e.g., receipts and forms). Unfortunately, no existing work took
advantage of advanced deep learning models because it is too laborious to
annotate a large enough dataset. We observe that the reading order of WORD
documents is embedded in their XML metadata; meanwhile, it is easy to convert
WORD documents to PDFs or images. Therefore, in an automated manner, we
construct ReadingBank, a benchmark dataset that contains reading order, text,
and layout information for 500,000 document images covering a wide spectrum of
document types. This first-ever large-scale dataset unleashes the power of deep
neural networks for reading order detection. Specifically, our proposed
LayoutReader captures the text and layout information for reading order
prediction using the seq2seq model. It performs almost perfectly in reading
order detection and significantly improves both open-source and commercial OCR
engines in ordering text lines in their results in our experiments. We will
release the dataset and model at \url{https://aka.ms/layoutreader}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Negative Sampling for Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2108.11607v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11607">
<div class="article-summary-box-inner">
<span><p>In many situations (e.g., distant supervision), unlabeled entity problem
seriously degrades the performances of named entity recognition (NER) models.
Recently, this issue has been well addressed by a notable approach based on
negative sampling. In this work, we perform two studies along this direction.
Firstly, we analyze why negative sampling succeeds both theoretically and
empirically. Based on the observation that named entities are highly sparse in
datasets, we show a theoretical guarantee that, for a long sentence, the
probability of containing no unlabeled entities in sampled negatives is high.
Missampling tests on synthetic datasets have verified our guarantee in
practice. Secondly, to mine hard negatives and further reduce missampling
rates, we propose a weighted and adaptive sampling distribution for negative
sampling. Experiments on synthetic datasets and well-annotated datasets show
that our method significantly improves negative sampling in robustness and
effectiveness. We also have achieved new state-of-the-art results on real-world
datasets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Cross-modal Contrastive Features for Video Domain Adaptation. (arXiv:2108.11974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11974">
<div class="article-summary-box-inner">
<span><p>Learning transferable and domain adaptive feature representations from videos
is important for video-relevant tasks such as action recognition. Existing
video domain adaptation methods mainly rely on adversarial feature alignment,
which has been derived from the RGB image space. However, video data is usually
associated with multi-modal information, e.g., RGB and optical flow, and thus
it remains a challenge to design a better method that considers the cross-modal
inputs under the cross-domain adaptation setting. To this end, we propose a
unified framework for video domain adaptation, which simultaneously regularizes
cross-modal and cross-domain feature representations. Specifically, we treat
each modality in a domain as a view and leverage the contrastive learning
technique with properly designed sampling strategies. As a result, our
objectives regularize feature spaces, which originally lack the connection
across modalities or have less alignment across domains. We conduct experiments
on domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and
EPIC-Kitchens, and demonstrate the effectiveness of our components against
state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection in Medical Imaging -- A Mini Review. (arXiv:2108.11986v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11986">
<div class="article-summary-box-inner">
<span><p>The increasing digitization of medical imaging enables machine learning based
improvements in detecting, visualizing and segmenting lesions, easing the
workload for medical experts. However, supervised machine learning requires
reliable labelled data, which is is often difficult or impossible to collect or
at least time consuming and thereby costly. Therefore methods requiring only
partly labeled data (semi-supervised) or no labeling at all (unsupervised
methods) have been applied more regularly. Anomaly detection is one possible
methodology that is able to leverage semi-supervised and unsupervised methods
to handle medical imaging tasks like classification and segmentation. This
paper uses a semi-exhaustive literature review of relevant anomaly detection
papers in medical imaging to cluster into applications, highlight important
results, establish lessons learned and give further advice on how to approach
anomaly detection in medical imaging. The qualitative analysis is based on
google scholar and 4 different search terms, resulting in 120 different
analysed papers. The main results showed that the current research is mostly
motivated by reducing the need for labelled data. Also, the successful and
substantial amount of research in the brain MRI domain shows the potential for
applications in further domains like OCT and chest X-ray.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transformer based Semantic Segmentation Networks for Pathological Image Segmentation. (arXiv:2108.11993v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11993">
<div class="article-summary-box-inner">
<span><p>Histopathology has played an essential role in cancer diagnosis. With the
rapid advances in convolutional neural networks (CNN). Various CNN-based
automated pathological image segmentation approaches have been developed in
computer-assisted pathological image analysis. In the past few years,
Transformer neural networks (Transformer) have shown the unique merit of
capturing the global long distance dependencies across the entire image as a
new deep learning paradigm. Such merit is appealing for exploring spatially
heterogeneous pathological images. However, there have been very few, if any,
studies that have systematically evaluated the current Transformer based
approaches in pathological image segmentation. To assess the performance of
Transformer segmentation models on whole slide images (WSI), we quantitatively
evaluated six prevalent transformer-based models on tumor segmentation, using
the widely used PAIP liver histopathological dataset. For a more comprehensive
analysis, we also compare the transformer-based models with six major
traditional CNN-based models. The results show that the Transformer-based
models exhibit a general superior performance over the CNN-based models. In
particular, Segmenter, Swin-Transformer and TransUNet, all transformer-based,
came out as the best performers among the twelve evaluated models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drop-DTW: Aligning Common Signal Between Sequences While Dropping Outliers. (arXiv:2108.11996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11996">
<div class="article-summary-box-inner">
<span><p>In this work, we consider the problem of sequence-to-sequence alignment for
signals containing outliers. Assuming the absence of outliers, the standard
Dynamic Time Warping (DTW) algorithm efficiently computes the optimal alignment
between two (generally) variable-length sequences. While DTW is robust to
temporal shifts and dilations of the signal, it fails to align sequences in a
meaningful way in the presence of outliers that can be arbitrarily interspersed
in the sequences. To address this problem, we introduce Drop-DTW, a novel
algorithm that aligns the common signal between the sequences while
automatically dropping the outlier elements from the matching. The entire
procedure is implemented as a single dynamic program that is efficient and
fully differentiable. In our experiments, we show that Drop-DTW is a robust
similarity measure for sequence retrieval and demonstrate its effectiveness as
a training loss on diverse applications. With Drop-DTW, we address temporal
step localization on instructional videos, representation learning from noisy
videos, and cross-modal representation learning for audio-visual retrieval and
localization. In all applications, we take a weakly- or unsupervised approach
and demonstrate state-of-the-art results under these settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Tutorial on Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12043">
<div class="article-summary-box-inner">
<span><p>Disentangled representation learning has been proposed as an approach to
learning general representations. This can be done in the absence of, or with
limited, annotations. A good general representation can be readily fine-tuned
for new target tasks using modest amounts of data, or even be used directly in
unseen domains achieving remarkable performance in the corresponding task. This
alleviation of the data and annotation requirements offers tantalising
prospects for tractable and affordable applications in computer vision and
healthcare. Finally, disentangled representations can offer model
explainability and can help us understand the underlying causal relations of
the factors of variation, increasing their suitability for real-world
deployment. In this tutorial paper, we will offer an overview of the
disentangled representation learning, its building blocks and criteria, and
discuss applications in computer vision and medical imaging. We conclude our
tutorial by presenting the identified opportunities for the integration of
recent machine learning advances into disentanglement, as well as the remaining
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultrafast Focus Detection for Automated Microscopy. (arXiv:2108.12050v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12050">
<div class="article-summary-box-inner">
<span><p>Recent advances in scientific instruments have resulted in dramatic increase
in the volumes and velocities of data being generated in every-day
laboratories. Scanning electron microscopy is one such example where
technological advancements are now overwhelming scientists with critical data
for montaging, alignment, and image segmentation -- key practices for many
scientific domains, including, for example, neuroscience, where they are used
to derive the anatomical relationships of the brain. These instruments now
necessitate equally advanced computing resources and techniques to realize
their full potential. Here we present a fast out-of-focus detection algorithm
for electron microscopy images collected serially and demonstrate that it can
be used to provide near-real time quality control for neurology research. Our
technique, Multi-scale Histologic Feature Detection, adapts classical computer
vision techniques and is based on detecting various fine-grained histologic
features. We further exploit the inherent parallelism in the technique by
employing GPGPU primitives in order to accelerate characterization. Tests are
performed that demonstrate near-real-time detection of out-of-focus conditions.
We deploy these capabilities as a funcX function and show that it can be
applied as data are collected using an automated pipeline . We discuss
extensions that enable scaling out to support multi-beam microscopes and
integration with existing focus systems for purposes of implementing
auto-focus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12056">
<div class="article-summary-box-inner">
<span><p>Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow's machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of learning, and retaining, new information without repeated exposure to it.
Past efforts to engineer such systems have sought to build or regulate
artificial neural networks using task-specific modules with constrained
circumstances of application. This has not yet enabled continual learning over
long sequences of previously unseen data without corrupting existing knowledge:
a problem known as catastrophic forgetting. In this paper, we introduce a
system that can learn sequentially over previously unseen datasets (ImageNet,
CIFAR-100) with little forgetting over time. This is accomplished by regulating
the activity of weights in a convolutional neural network on the basis of
inputs using top-down modulation generated by a second feed-forward neural
network. We find that our method learns continually under domain transfer with
sparse bursts of activity in weights that are recycled across tasks, rather
than by maintaining task-specific modules. Sparse synaptic bursting is found to
balance enhanced and diminished activity in a way that facilitates adaptation
to new inputs without corrupting previously acquired functions. This behavior
emerges during a prior meta-learning phase in which regulated synapses are
selectively disinhibited, or grown, from an initial state of uniform
suppression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Stable Configurations for Semantic Placement of Novel Objects. (arXiv:2108.12062v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12062">
<div class="article-summary-box-inner">
<span><p>Human environments contain numerous objects configured in a variety of
arrangements. Our goal is to enable robots to repose previously unseen objects
according to learned semantic relationships in novel environments. We break
this problem down into two parts: (1) finding physically valid locations for
the objects and (2) determining if those poses satisfy learned, high-level
semantic relationships. We build our models and training from the ground up to
be tightly integrated with our proposed planning algorithm for semantic
placement of unknown objects. We train our models purely in simulation, with no
fine-tuning needed for use in the real world. Our approach enables motion
planning for semantic rearrangement of unknown objects in scenes with varying
geometry from only RGB-D sensing. Our experiments through a set of simulated
ablations demonstrate that using a relational classifier alone is not
sufficient for reliable planning. We further demonstrate the ability of our
planner to generate and execute diverse manipulation plans through a set of
real-world experiments with a variety of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automatic Image Content Retrieval Method for better Mobile Device Display User Experiences. (arXiv:2108.12068v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12068">
<div class="article-summary-box-inner">
<span><p>A growing number of commercially available mobile phones come with integrated
high-resolution digital cameras. That enables a new class of dedicated
applications to image analysis such as mobile visual search, image cropping,
object detection, content-based image retrieval, image classification. In this
paper, a new mobile application for image content retrieval and classification
for mobile device display is proposed to enrich the visual experience of users.
The mobile application can extract a certain number of images based on the
content of an image with visual saliency methods aiming at detecting the most
critical regions in a given image from a perceptual viewpoint. First, the most
critical areas from a perceptual perspective are extracted using the local
maxima of a 2D saliency function. Next, a salient region is cropped using the
bounding box centred on the local maxima of the thresholded Saliency Map of the
image. Then, each image crop feds into an Image Classification system based on
SVM and SIFT descriptors to detect the class of object present in the image.
ImageNet repository was used as the reference for semantic category
classification. Android platform was used to implement the mobile application
on a client-server architecture. A mobile client sends the photo taken by the
camera to the server, which processes the image and returns the results (image
contents such as image crops and related target classes) to the mobile client.
The application was run on thousands of pictures and showed encouraging results
towards a better user visual experience with mobile displays.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Underwater Sonar Images by the Learned Descriptor Based on Style Transfer Method. (arXiv:2108.12072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12072">
<div class="article-summary-box-inner">
<span><p>This paper proposes a method that combines the style transfer technique and
the learned descriptor to enhance the matching performances of underwater sonar
images. In the field of underwater vision, sonar is currently the most
effective long-distance detection sensor, it has excellent performances in map
building and target search tasks. However, the traditional image matching
algorithms are all developed based on optical images. In order to solve this
contradiction, the style transfer method is used to convert the sonar images
into optical styles, and at the same time, the learned descriptor with
excellent expressiveness for sonar images matching is introduced. Experiments
show that this method significantly enhances the matching quality of sonar
images. In addition, it also provides new ideas for the preprocessing of
underwater sonar images by using the style transfer approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection and Continual Learning of Novel Face Presentation Attacks. (arXiv:2108.12081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12081">
<div class="article-summary-box-inner">
<span><p>Advances in deep learning, combined with availability of large datasets, have
led to impressive improvements in face presentation attack detection research.
However, state-of-the-art face antispoofing systems are still vulnerable to
novel types of attacks that are never seen during training. Moreover, even if
such attacks are correctly detected, these systems lack the ability to adapt to
newly encountered attacks. The post-training ability of continually detecting
new types of attacks and self-adaptation to identify these attack types, after
the initial detection phase, is highly appealing. In this paper, we enable a
deep neural network to detect anomalies in the observed input data points as
potential new types of attacks by suppressing the confidence-level of the
network outside the training samples' distribution. We then use experience
replay to update the model to incorporate knowledge about new types of attacks
without forgetting the past learned attack types. Experimental results are
provided to demonstrate the effectiveness of the proposed method on two
benchmark datasets as well as a newly introduced dataset which exhibits a large
variety of attack types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Denoising Method for Side Scan Sonar Images without High-quality Reference Data. (arXiv:2108.12083v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12083">
<div class="article-summary-box-inner">
<span><p>Subsea images measured by the side scan sonars (SSSs) are necessary visual
data in the process of deep-sea exploration by using the autonomous underwater
vehicles (AUVs). They could vividly reflect the topography of the seabed, but
usually accompanied by complex and severe noise. This paper proposes a deep
denoising method for SSS images without high-quality reference data, which uses
one single noise SSS image to perform self-supervised denoising. Compared with
the classical artificially designed filters, the deep denoising method shows
obvious advantages. The denoising experiments are performed on the real seabed
SSS images, and the results demonstrate that our proposed method could
effectively reduce the noise on the SSS image while minimizing the image
quality and detail loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOVEA: Foveated Image Magnification for Autonomous Navigation. (arXiv:2108.12102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12102">
<div class="article-summary-box-inner">
<span><p>Efficient processing of high-resolution video streams is safety-critical for
many robotics applications such as autonomous driving. Image downsampling is a
commonly adopted technique to ensure the latency constraint is met. However,
this naive approach greatly restricts an object detector's capability to
identify small objects. In this paper, we propose an attentional approach that
elastically magnifies certain regions while maintaining a small input canvas.
The magnified regions are those that are believed to have a high probability of
containing an object, whose signal can come from a dataset-wide prior or
frame-level prior computed from recent object predictions. The magnification is
implemented by a KDE-based mapping to transform the bounding boxes into warping
parameters, which are then fed into an image sampler with anti-cropping
regularization. The detector is then fed with the warped image and we apply a
differentiable backward mapping to get bounding box outputs in the original
space. Our regional magnification allows algorithms to make better use of
high-resolution input without incurring the cost of high-resolution processing.
On the autonomous driving datasets Argoverse-HD and BDD100K, we show our
proposed method boosts the detection AP over standard Faster R-CNN, with and
without finetuning. Additionally, building on top of the previous
state-of-the-art in streaming detection, our method sets a new record for
streaming AP on Argoverse-HD (from 17.8 to 23.0 on a GTX 1080 Ti GPU),
suggesting that it has achieved a superior accuracy-latency tradeoff.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binocular Mutual Learning for Improving Few-shot Classification. (arXiv:2108.12104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12104">
<div class="article-summary-box-inner">
<span><p>Most of the few-shot learning methods learn to transfer knowledge from
datasets with abundant labeled data (i.e., the base set). From the perspective
of class space on base set, existing methods either focus on utilizing all
classes under a global view by normal pretraining, or pay more attention to
adopt an episodic manner to train meta-tasks within few classes in a local
view. However, the interaction of the two views is rarely explored. As the two
views capture complementary information, we naturally think of the
compatibility of them for achieving further performance gains. Inspired by the
mutual learning paradigm and binocular parallax, we propose a unified
framework, namely Binocular Mutual Learning (BML), which achieves the
compatibility of the global view and the local view through both intra-view and
cross-view modeling. Concretely, the global view learns in the whole class
space to capture rich inter-class relationships. Meanwhile, the local view
learns in the local class space within each episode, focusing on matching
positive pairs correctly. In addition, cross-view mutual interaction further
promotes the collaborative learning and the implicit exploration of useful
knowledge from each other. During meta-test, binocular embeddings are
aggregated together to support decision-making, which greatly improve the
accuracy of classification. Extensive experiments conducted on multiple
benchmarks including cross-domain validation confirm the effectiveness of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognition Awareness: An Application of Latent Cognizance to Open-Set Recognition. (arXiv:2108.12115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12115">
<div class="article-summary-box-inner">
<span><p>This study investigates an application of a new probabilistic interpretation
of a softmax output to Open-Set Recognition (OSR). Softmax is a mechanism
wildly used in classification and object recognition.
</p>
<p>However, a softmax mechanism forces a model to operate under a closed-set
paradigm, i.e., to predict an object class out of a set of pre-defined labels.
</p>
<p>This characteristic contributes to efficacy in classification, but poses a
risk of non-sense prediction in object recognition.
</p>
<p>Object recognition is often operated under a dynamic and diverse condition.
</p>
<p>A foreign object -- an object of any unprepared class -- can be encountered
at any time.
</p>
<p>OSR is intended to address an issue of identifying a foreign object in object
recognition.
</p>
<p>Based on Bayes theorem and the emphasis of conditioning on the context,
softmax inference has been re-interpreted.
</p>
<p>This re-interpretation has led to a new approach to OSR, called Latent
Cognizance (LC). Our investigation employs various scenarios, using Imagenet
2012 dataset as well as fooling and open-set images. The findings support LC
hypothesis and show its effectiveness on OSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Densely-Populated Traffic Detection using YOLOv5 and Non-Maximum Suppression Ensembling. (arXiv:2108.12118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12118">
<div class="article-summary-box-inner">
<span><p>Vehicular object detection is the heart of any intelligent traffic system. It
is essential for urban traffic management. R-CNN, Fast R-CNN, Faster R-CNN and
YOLO were some of the earlier state-of-the-art models. Region based CNN methods
have the problem of higher inference time which makes it unrealistic to use the
model in real-time. YOLO on the other hand struggles to detect small objects
that appear in groups. In this paper, we propose a method that can locate and
classify vehicular objects from a given densely crowded image using YOLOv5. The
shortcoming of YOLO was solved my ensembling 4 different models. Our proposed
model performs well on images taken from both top view and side view of the
street in both day and night. The performance of our proposed model was
measured on Dhaka AI dataset which contains densely crowded vehicular images.
Our experiment shows that our model achieved mAP@0.5 of 0.458 with inference
time of 0.75 sec which outperforms other state-of-the-art models on
performance. Hence, the model can be implemented in the street for real-time
traffic detection which can be used for traffic control and data collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAE-GAN: Dynamic Aspect-aware GAN for Text-to-Image Synthesis. (arXiv:2108.12141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12141">
<div class="article-summary-box-inner">
<span><p>Text-to-image synthesis refers to generating an image from a given text
description, the key goal of which lies in photo realism and semantic
consistency. Previous methods usually generate an initial image with sentence
embedding and then refine it with fine-grained word embedding. Despite the
significant progress, the 'aspect' information (e.g., red eyes) contained in
the text, referring to several words rather than a word that depicts 'a
particular part or feature of something', is often ignored, which is highly
helpful for synthesizing image details. How to make better utilization of
aspect information in text-to-image synthesis still remains an unresolved
challenge. To address this problem, in this paper, we propose a Dynamic
Aspect-awarE GAN (DAE-GAN) that represents text information comprehensively
from multiple granularities, including sentence-level, word-level, and
aspect-level. Moreover, inspired by human learning behaviors, we develop a
novel Aspect-aware Dynamic Re-drawer (ADR) for image refinement, in which an
Attended Global Refinement (AGR) module and an Aspect-aware Local Refinement
(ALR) module are alternately employed. AGR utilizes word-level embedding to
globally enhance the previously generated image, while ALR dynamically employs
aspect-level embedding to refine image details from a local perspective.
Finally, a corresponding matching loss function is designed to ensure the
text-image semantic consistency at different levels. Extensive experiments on
two well-studied and publicly available datasets (i.e., CUB-200 and COCO)
demonstrate the superiority and rationality of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Matching Algorithm based on Image Attribute Transfer and Local Features for Underwater Acoustic and Optical Images. (arXiv:2108.12151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12151">
<div class="article-summary-box-inner">
<span><p>In the field of underwater vision research, image matching between the sonar
sensors and optical cameras has always been a challenging problem. Due to the
difference in the imaging mechanism between them, which are the gray value,
texture, contrast, etc. of the acoustic images and the optical images are also
variant in local locations, which makes the traditional matching method based
on the optical image invalid. Coupled with the difficulties and high costs of
underwater data acquisition, it further affects the research process of
acousto-optic data fusion technology. In order to maximize the use of
underwater sensor data and promote the development of multi-sensor information
fusion (MSIF), this study applies the image attribute transfer method based on
deep learning approach to solve the problem of acousto-optic image matching,
the core of which is to eliminate the imaging differences between them as much
as possible. At the same time, the advanced local feature descriptor is
introduced to solve the challenging acousto-optic matching problem.
Experimental results show that our proposed method could preprocess
acousto-optic images effectively and obtain accurate matching results.
Additionally, the method is based on the combination of image depth semantic
layer, and it could indirectly display the local feature matching relationship
between original image pair, which provides a new solution to the underwater
multi-sensor image matching problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection of Defect using Energy of Point Pattern Features within Random Finite Set Framework. (arXiv:2108.12159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12159">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an efficient approach for industrial defect
detection that is modeled based on anomaly detection using point pattern data.
Most recent works use \textit{global features} for feature extraction to
summarize image content. However, global features are not robust against
lighting and viewpoint changes and do not describe the image's geometrical
information to be fully utilized in the manufacturing industry. To the best of
our knowledge, we are the first to propose using transfer learning of
local/point pattern features to overcome these limitations and capture
geometrical information of the image regions. We model these local/point
pattern features as a random finite set (RFS). In addition we propose RFS
energy, in contrast to RFS likelihood as anomaly score. The similarity
distribution of point pattern features of the normal sample has been modeled as
a multivariate Gaussian. Parameters learning of the proposed RFS energy does
not require any heavy computation. We evaluate the proposed approach on the
MVTec AD dataset, a multi-object defect detection dataset. Experimental results
show the outstanding performance of our proposed approach compared to the
state-of-the-art methods, and the proposed RFS energy outperforms the
state-of-the-art in the few shot learning settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LassoLayer: Nonlinear Feature Selection by Switching One-to-one Links. (arXiv:2108.12165v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12165">
<div class="article-summary-box-inner">
<span><p>Along with the desire to address more complex problems, feature selection
methods have gained in importance. Feature selection methods can be classified
into wrapper method, filter method, and embedded method. Being a powerful
embedded feature selection method, Lasso has attracted the attention of many
researchers. However, as a linear approach, the applicability of Lasso has been
limited. In this work, we propose LassoLayer that is one-to-one connected and
trained by L1 optimization, which work to drop out unnecessary units for
prediction. For nonlinear feature selections, we build LassoMLP: the network
equipped with LassoLayer as its first layer. Because we can insert LassoLayer
in any network structure, it can harness the strength of neural network
suitable for tasks where feature selection is needed. We evaluate LassoMLP in
feature selection with regression and classification tasks. LassoMLP receives
features including considerable numbers of noisy factors that is harmful for
overfitting. In the experiments using MNIST dataset, we confirm that LassoMLP
outperforms the state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCo DistillNet: a Cross-layer Correlation Distillation Network for Pathological Gastric Cancer Segmentation. (arXiv:2108.12173v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12173">
<div class="article-summary-box-inner">
<span><p>In recent years, deep convolutional neural networks have made significant
advances in pathology image segmentation. However, pathology image segmentation
encounters with a dilemma in which the higher-performance networks generally
require more computational resources and storage. This phenomenon limits the
employment of high-accuracy networks in real scenes due to the inherent
high-resolution of pathological images. To tackle this problem, we propose CoCo
DistillNet, a novel Cross-layer Correlation (CoCo) knowledge distillation
network for pathological gastric cancer segmentation. Knowledge distillation, a
general technique which aims at improving the performance of a compact network
through knowledge transfer from a cumbersome network. Concretely, our CoCo
DistillNet models the correlations of channel-mixed spatial similarity between
different layers and then transfers this knowledge from a pre-trained
cumbersome teacher network to a non-trained compact student network. In
addition, we also utilize the adversarial learning strategy to further prompt
the distilling procedure which is called Adversarial Distillation (AD).
Furthermore, to stabilize our training procedure, we make the use of the
unsupervised Paraphraser Module (PM) to boost the knowledge paraphrase in the
teacher network. As a result, extensive experiments conducted on the Gastric
Cancer Segmentation Dataset demonstrate the prominent ability of CoCo
DistillNet which achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Aligned and Misaligned Features in One-stage Object Detection. (arXiv:2108.12176v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12176">
<div class="article-summary-box-inner">
<span><p>One-stage object detectors rely on the point feature to predict the detection
results. However, the point feature may lack the information of the whole
object and lead to a misalignment between the object and the point feature.
Meanwhile, the classification and regression tasks are sensitive to different
object regions, but their features are spatially aligned. In this paper, we
propose a simple and plug-in operator that could generate aligned and
disentangled features for each task, respectively, without breaking the fully
convolutional manner. By predicting two task-aware point sets that are located
in each sensitive region, this operator could disentangle the two tasks from
the spatial dimension, as well as align the point feature with the object. We
also reveal an interesting finding of the opposite effect of the long-range
skip-connection for classification and regression, respectively. Based on the
object-aligned and task-disentangled operator (OAT), we propose OAT-Net, which
explicitly exploits point-set features for more accurate detection results.
Extensive experiments on the MS-COCO dataset show that OAT can consistently
boost different one-stage detectors by $\sim$2 AP. Notably, OAT-Net achieves
53.7 AP with Res2Net-101-DCN backbone and shows promising performance gain for
small objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiSiam: Self-supervised Multi-instance Siamese Representation Learning for Autonomous Driving. (arXiv:2108.12178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12178">
<div class="article-summary-box-inner">
<span><p>Autonomous driving has attracted much attention over the years but turns out
to be harder than expected, probably due to the difficulty of labeled data
collection for model training. Self-supervised learning (SSL), which leverages
unlabeled data only for representation learning, might be a promising way to
improve model performance. Existing SSL methods, however, usually rely on the
single-centric-object guarantee, which may not be applicable for multi-instance
datasets such as street scenes. To alleviate this limitation, we raise two
issues to solve: (1) how to define positive samples for cross-view consistency
and (2) how to measure similarity in multi-instance circumstances. We first
adopt an IoU threshold during random cropping to transfer global-inconsistency
to local-consistency. Then, we propose two feature alignment methods to enable
2D feature maps for multi-instance similarity measurement. Additionally, we
adopt intra-image clustering with self-attention for further mining intra-image
similarity and translation-invariance. Experiments show that, when pre-trained
on Waymo dataset, our method called Multi-instance Siamese Network (MultiSiam)
remarkably improves generalization ability and achieves state-of-the-art
transfer performance on autonomous driving benchmarks, including Cityscapes and
BDD100K, while existing SSL counterparts like MoCo, MoCo-v2, and BYOL show
significant performance drop. By pre-training on SODA10M, a large-scale
autonomous driving dataset, MultiSiam exceeds the ImageNet pre-trained MoCo-v2,
demonstrating the potential of domain-specific pre-training. Code will be
available at https://github.com/KaiChen1998/MultiSiam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIMo -- A Dataset for Indoor Building Monitoring with a Time-of-Flight Camera. (arXiv:2108.12196v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12196">
<div class="article-summary-box-inner">
<span><p>We present TIMo (Time-of-flight Indoor Monitoring), a dataset for video-based
monitoring of indoor spaces captured using a time-of-flight (ToF) camera. The
resulting depth videos feature people performing a set of different predefined
actions, for which we provide detailed annotations. Person detection for people
counting and anomaly detection are the two targeted applications. Most existing
surveillance video datasets provide either grayscale or RGB videos. Depth
information, on the other hand, is still a rarity in this class of datasets in
spite of being popular and much more common in other research fields within
computer vision. Our dataset addresses this gap in the landscape of
surveillance video datasets. The recordings took place at two different
locations with the ToF camera set up either in a top-down or a tilted
perspective on the scene. The dataset is publicly available at
https://vizta-tof.kl.dfki.de/timo-dataset-overview/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Rule-Based Clutter Detection in Automotive Radar Data. (arXiv:2108.12224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12224">
<div class="article-summary-box-inner">
<span><p>Automotive radar sensors output a lot of unwanted clutter or ghost
detections, whose position and velocity do not correspond to any real object in
the sensor's field of view. This poses a substantial challenge for environment
perception methods like object detection or tracking. Especially problematic
are clutter detections that occur in groups or at similar locations in multiple
consecutive measurements. In this paper, a new algorithm for identifying such
erroneous detections is presented. It is mainly based on the modeling of
specific commonly occurring wave propagation paths that lead to clutter. In
particular, the three effects explicitly covered are reflections at the
underbody of a car or truck, signals traveling back and forth between the
vehicle on which the sensor is mounted and another object, and multipath
propagation via specular reflection. The latter often occurs near guardrails,
concrete walls or similar reflective surfaces. Each of these effects is
described both theoretically and regarding a method for identifying the
corresponding clutter detections. Identification is done by analyzing
detections generated from a single sensor measurement only. The final algorithm
is evaluated on recordings of real extra-urban traffic. For labeling, a
semi-automatic process is employed. The results are promising, both in terms of
performance and regarding the very low execution time. Typically, a large part
of clutter is found, while only a small ratio of detections corresponding to
real objects are falsely classified by the algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process. (arXiv:2108.12278v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12278">
<div class="article-summary-box-inner">
<span><p>Recent research efforts in lifelong learning propose to grow a mixture of
models to adapt to an increasing number of tasks. The proposed methodology
shows promising results in overcoming catastrophic forgetting. However, the
theory behind these successful models is still not well understood. In this
paper, we perform the theoretical analysis for lifelong learning models by
deriving the risk bounds based on the discrepancy distance between the
probabilistic representation of data generated by the model and that
corresponding to the target dataset. Inspired by the theoretical analysis, we
introduce a new lifelong learning approach, namely the Lifelong Infinite
Mixture (LIMix) model, which can automatically expand its network architectures
or choose an appropriate component to adapt its parameters for learning a new
task, while preserving its previously learnt information. We propose to
incorporate the knowledge by means of Dirichlet processes by using a gating
mechanism which computes the dependence between the knowledge learnt previously
and stored in each component, and a new set of data. Besides, we train a
compact Student model which can accumulate cross-domain representations over
time and make quick inferences. The code is available at
https://github.com/dtuzi123/Lifelong-infinite-mixture-model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stop Throwing Away Discriminators! Re-using Adversaries for Test-Time Training. (arXiv:2108.12280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12280">
<div class="article-summary-box-inner">
<span><p>Thanks to their ability to learn data distributions without requiring paired
data, Generative Adversarial Networks (GANs) have become an integral part of
many computer vision methods, including those developed for medical image
segmentation. These methods jointly train a segmentor and an adversarial mask
discriminator, which provides a data-driven shape prior. At inference, the
discriminator is discarded, and only the segmentor is used to predict label
maps on test images. But should we discard the discriminator? Here, we argue
that the life cycle of adversarial discriminators should not end after
training. On the contrary, training stable GANs produces powerful shape priors
that we can use to correct segmentor mistakes at inference. To achieve this, we
develop stable mask discriminators that do not overfit or catastrophically
forget. At test time, we fine-tune the segmentor on each individual test
instance until it satisfies the learned shape prior. Our method is simple to
implement and increases model performance. Moreover, it opens new directions
for re-using mask discriminators at inference. We release the code used for the
experiments at https://vios-s.github.io/adversarial-test-time-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TE-YOLOF: Tiny and efficient YOLOF for blood cell detection. (arXiv:2108.12313v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12313">
<div class="article-summary-box-inner">
<span><p>Blood cell detection in microscopic images is an essential branch of medical
image processing research. Since disease detection based on manual checking of
blood cells is time-consuming and full of errors, testing of blood cells using
object detectors with Deep Convolutional Neural Network can be regarded as a
feasible solution. In this work, an object detector based on YOLOF has been
proposed to detect blood cell objects such as red blood cells, white blood
cells and platelets. This object detector is called TE-YOLOF, Tiny and
Efficient YOLOF, and it is a One-Stage detector using dilated encoder to
extract information from single-level feature maps. For increasing efficiency
and flexibility, the EfficientNet Convolutional Neural Network is utilized as
the backbone for the proposed object detector. Furthermore, the Depthwise
Separable Convolution is applied to enhance the performance and minimize the
parameters of the network. In addition, the Mish activation function is
employed to increase the precision. Extensive experiments on the BCCD dataset
prove the effectiveness of the proposed model, which is more efficient than
other existing studies for blood cell detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pedestrian Detection and Tracking Framework for Autonomous Cars: Efficient Fusion of Camera and LiDAR Data. (arXiv:2108.12375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12375">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel method for pedestrian detection and tracking by
fusing camera and LiDAR sensor data. To deal with the challenges associated
with the autonomous driving scenarios, an integrated tracking and detection
framework is proposed. The detection phase is performed by converting LiDAR
streams to computationally tractable depth images, and then, a deep neural
network is developed to identify pedestrian candidates both in RGB and depth
images. To provide accurate information, the detection phase is further
enhanced by fusing multi-modal sensor information using the Kalman filter. The
tracking phase is a combination of the Kalman filter prediction and an optical
flow algorithm to track multiple pedestrians in a scene. We evaluate our
framework on a real public driving dataset. Experimental results demonstrate
that the proposed method achieves significant performance improvement over a
baseline method that solely uses image-based pedestrian detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISNet: Integrate Image-Level and Semantic-Level Context for Semantic Segmentation. (arXiv:2108.12382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12382">
<div class="article-summary-box-inner">
<span><p>Co-occurrent visual pattern makes aggregating contextual information a common
paradigm to enhance the pixel representation for semantic image segmentation.
The existing approaches focus on modeling the context from the perspective of
the whole image, i.e., aggregating the image-level contextual information.
Despite impressive, these methods weaken the significance of the pixel
representations of the same category, i.e., the semantic-level contextual
information. To address this, this paper proposes to augment the pixel
representations by aggregating the image-level and semantic-level contextual
information, respectively. First, an image-level context module is designed to
capture the contextual information for each pixel in the whole image. Second,
we aggregate the representations of the same category for each pixel where the
category regions are learned under the supervision of the ground-truth
segmentation. Third, we compute the similarities between each pixel
representation and the image-level contextual information, the semantic-level
contextual information, respectively. At last, a pixel representation is
augmented by weighted aggregating both the image-level contextual information
and the semantic-level contextual information with the similarities as the
weights. Integrating the image-level and semantic-level context allows this
paper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K,
LIP, COCOStuff and Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3D Human Shape Reconstruction. (arXiv:2108.12384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12384">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to reconstruct a full 3D human shape from a single
image. Previous vertex-level and parameter regression approaches reconstruct 3D
human shape based on a pre-defined adjacency matrix to encode positive
relations between nodes. The deep topological relations for the surface of the
3D human body are not carefully exploited. Moreover, the performance of most
existing approaches often suffer from domain gap when handling more occlusion
cases in real-world scenes.
</p>
<p>In this work, we propose a Deep Mesh Relation Capturing Graph Convolution
Network, DC-GNet, with a shape completion task for 3D human shape
reconstruction. Firstly, we propose to capture deep relations within mesh
vertices, where an adaptive matrix encoding both positive and negative
relations is introduced. Secondly, we propose a shape completion task to learn
prior about various kinds of occlusion cases. Our approach encodes mesh
structure from more subtle relations between nodes in a more distant region.
Furthermore, our shape completion module alleviates the performance degradation
issue in the outdoor scene. Extensive experiments on several benchmarks show
that our approach outperforms the previous 3D human pose and shape estimation
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Hierarchical Light Field Coding Scheme Based on Hybrid Stacked Multiplicative Layers and Fourier Disparity Layers for Glasses-Free 3D Displays. (arXiv:2108.12399v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12399">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel hierarchical coding scheme for light fields based
on transmittance patterns of low-rank multiplicative layers and Fourier
disparity layers. The proposed scheme identifies multiplicative layers of light
field view subsets optimized using a convolutional neural network for different
scanning orders. Our approach exploits the hidden low-rank structure in the
multiplicative layers obtained from the subsets of different scanning patterns.
The spatial redundancies in the multiplicative layers can be efficiently
removed by performing low-rank approximation at different ranks on the Krylov
subspace. The intra-view and inter-view redundancies between approximated
layers are further removed by HEVC encoding. Next, a Fourier disparity layer
representation is constructed from the first subset of the approximated light
field based on the chosen hierarchical order. Subsequent view subsets are
synthesized by modeling the Fourier disparity layers that iteratively refine
the representation with improved accuracy. The critical advantage of the
proposed hybrid layered representation and coding scheme is that it utilizes
not just spatial and temporal redundancies in light fields but efficiently
exploits intrinsic similarities among neighboring sub-aperture images in both
horizontal and vertical directions as specified by different predication
orders. In addition, the scheme is flexible to realize a range of multiple
bitrates at the decoder within a single integrated system. The compression
performance of the proposed scheme is analyzed on real light fields. We
achieved substantial bitrate savings and maintained good light field
reconstruction quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynthIA: A Synthetic Inversion Approximation for the Stokes Vector Fusing SDO and Hinode into a Virtual Observatory. (arXiv:2108.12421v1 [astro-ph.IM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12421">
<div class="article-summary-box-inner">
<span><p>Both NASA's Solar Dynamics Observatory (SDO) and the JAXA/NASA Hinode mission
include spectropolarimetric instruments designed to measure the photospheric
magnetic field. SDO's Helioseismic and Magnetic Imager (HMI) emphasizes
full-disk high-cadence and good spatial resolution data acquisition while
Hinode's Solar Optical Telescope Spectro-Polarimeter (SOT-SP) focuses on high
spatial resolution and spectral sampling at the cost of a limited field of view
and slower temporal cadence. This work introduces a deep-learning system named
SynthIA (Synthetic Inversion Approximation), that can enhance both missions by
capturing the best of each instrument's characteristics. We use SynthIA to
produce a new magnetogram data product, SynodeP (Synthetic Hinode Pipeline),
that mimics magnetograms from the higher spectral resolution Hinode/SOT-SP
pipeline, but is derived from full-disk, high-cadence, and lower
spectral-resolution SDO/HMI Stokes observations. Results on held-out data show
that SynodeP has good agreement with the Hinode/SOT-SP pipeline inversions,
including magnetic fill fraction, which is not provided by the current SDO/HMI
pipeline. SynodeP further shows a reduction in the magnitude of the 24-hour
oscillations present in the SDO/HMI data. To demonstrate SynthIA's generality,
we show the use of SDO/AIA data and subsets of the HMI data as inputs, which
enables trade-offs between fidelity to the Hinode/SOT-SP inversions, number of
observations used, and temporal artifacts. We discuss possible generalizations
of SynthIA and its implications for space weather modeling. This work is part
of the NASA Heliophysics DRIVE Science Center (SOLSTICE) at the University of
Michigan under grant NASA 80NSSC20K0600E, and will be open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competing Ratio Loss for Discriminative Multi-class Image Classification. (arXiv:1912.11642v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.11642">
<div class="article-summary-box-inner">
<span><p>The development of deep convolutional neural network architecture is critical
to the improvement of image classification task performance. Many image
classification studies use deep convolutional neural network and focus on
modifying the network structure to improve image classification performance.
Conversely, our study focuses on loss function design. Cross-entropy Loss (CEL)
has been widely used for training deep convolutional neural network for the
task of multi-class classification. Although CEL has been successfully
implemented in several image classification tasks, it only focuses on the
posterior probability of the correct class. For this reason, a negative log
likelihood ratio loss (NLLR) was proposed to better differentiate between the
correct class and the competing incorrect ones. However, during the training of
the deep convolutional neural network, the value of NLLR is not always positive
or negative, which severely affects the convergence of NLLR. Our proposed
competing ratio loss (CRL) calculates the posterior probability ratio between
the correct class and the competing incorrect classes to further enlarge the
probability difference between the correct and incorrect classes. We added
hyperparameters to CRL, thereby ensuring its value to be positive and that the
update size of backpropagation is suitable for the CRL's fast convergence. To
demonstrate the performance of CRL, we conducted experiments on general image
classification tasks (CIFAR10/100, SVHN, ImageNet), the fine-grained image
classification tasks (CUB200-2011 and Stanford Car), and the challenging face
age estimation task (using Adience). Experimental results show the
effectiveness and robustness of the proposed loss function on different deep
convolutional neural network architectures and different image classification
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge Computing for Real-Time Near-Crash Detection for Smart Transportation Applications. (arXiv:2008.00549v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.00549">
<div class="article-summary-box-inner">
<span><p>Traffic near-crash events serve as critical data sources for various smart
transportation applications, such as being surrogate safety measures for
traffic safety research and corner case data for automated vehicle testing.
However, there are several key challenges for near-crash detection. First,
extracting near-crashes from original data sources requires significant
computing, communication, and storage resources. Also, existing methods lack
efficiency and transferability, which bottlenecks prospective large-scale
applications. To this end, this paper leverages the power of edge computing to
address these challenges by processing the video streams from existing dashcams
onboard in a real-time manner. We design a multi-thread system architecture
that operates on edge devices and model the bounding boxes generated by object
detection and tracking in linear complexity. The method is insensitive to
camera parameters and backward compatible with different vehicles. The edge
computing system has been evaluated with recorded videos and real-world tests
on two cars and four buses for over ten thousand hours. It filters out
irrelevant videos in real-time thereby saving labor cost, processing time,
network bandwidth, and data storage. It collects not only event videos but also
other valuable data such as road user type, event location, time to collision,
vehicle trajectory, vehicle speed, brake switch, and throttle. The experiments
demonstrate the promising performance of the system regarding efficiency,
accuracy, reliability, and transferability. It is among the first efforts in
applying edge computing for real-time traffic video analytics and is expected
to benefit multiple sub-fields in smart transportation research and
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channel-wise Knowledge Distillation for Dense Prediction. (arXiv:2011.13256v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13256">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has been proven to be a simple and effective tool
for training compact models. Almost all KD variants for dense prediction tasks
align the student and teacher networks' feature maps in the spatial domain,
typically by minimizing point-wise and/or pair-wise discrepancy. Observing that
in semantic segmentation, some layers' feature activations of each channel tend
to encode saliency of scene categories (analogue to class activation mapping),
we propose to align features channel-wise between the student and teacher
networks. To this end, we first transform the feature map of each channel into
a probabilty map using softmax normalization, and then minimize the
Kullback-Leibler (KL) divergence of the corresponding channels of the two
networks. By doing so, our method focuses on mimicking the soft distributions
of channels between networks. In particular, the KL divergence enables learning
to pay more attention to the most salient regions of the channel-wise maps,
presumably corresponding to the most useful signals for semantic segmentation.
Experiments demonstrate that our channel-wise distillation outperforms almost
all existing spatial distillation methods for semantic segmentation
considerably, and requires less computational cost during training. We
consistently achieve superior performance on three benchmarks with various
network structures. Code is available at: https://git.io/Distiller
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting 32 Pedestrian Attributes for Autonomous Vehicles. (arXiv:2012.02647v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02647">
<div class="article-summary-box-inner">
<span><p>Pedestrians are arguably one of the most safety-critical road users to
consider for autonomous vehicles in urban areas. In this paper, we address the
problem of jointly detecting pedestrians and recognizing 32 pedestrian
attributes from a single image. These encompass visual appearance and behavior,
and also include the forecasting of road crossing, which is a main safety
concern. For this, we introduce a Multi-Task Learning (MTL) model relying on a
composite field framework, which achieves both goals in an efficient way. Each
field spatially locates pedestrian instances and aggregates attribute
predictions over them. This formulation naturally leverages spatial context,
making it well suited to low resolution scenarios such as autonomous driving.
By increasing the number of attributes jointly learned, we highlight an issue
related to the scales of gradients, which arises in MTL with numerous tasks. We
solve it by normalizing the gradients coming from different objective functions
when they join at the fork in the network architecture during the backward
pass, referred to as fork-normalization. Experimental validation is performed
on JAAD, a dataset providing numerous attributes for pedestrian analysis from
autonomous vehicles, and shows competitive detection and attribute recognition
results, as well as a more stable MTL training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Optimal Neural Networks: Rapid Search in Diverse Spaces. (arXiv:2012.08859v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08859">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art Neural Architecture Search (NAS) methods neither
efficiently scale to multiple hardware platforms, nor handle diverse
architectural search-spaces. To remedy this, we present DONNA (Distilling
Optimal Neural Network Architectures), a novel pipeline for rapid, scalable and
diverse NAS, that scales to many user scenarios. DONNA consists of three
phases. First, an accuracy predictor is built using blockwise knowledge
distillation from a reference model. This predictor enables searching across
diverse networks with varying macro-architectural parameters such as layer
types and attention mechanisms, as well as across micro-architectural
parameters such as block repeats and expansion rates. Second, a rapid
evolutionary search finds a set of pareto-optimal architectures for any
scenario using the accuracy predictor and on-device measurements. Third,
optimal models are quickly finetuned to training-from-scratch accuracy. DONNA
is up to 100x faster than MNasNet in finding state-of-the-art architectures
on-device. Classifying ImageNet, DONNA architectures are 20% faster than
EfficientNet-B0 and MobileNetV2 on a Nvidia V100 GPU and 10% faster with 0.5%
higher accuracy than MobileNetV2-1.4x on a Samsung S20 smartphone. In addition
to NAS, DONNA is used for search-space extension and exploration, as well as
hardware-aware model compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantics Disentangling for Generalized Zero-Shot Learning. (arXiv:2101.07978v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07978">
<div class="article-summary-box-inner">
<span><p>Generalized zero-shot learning (GZSL) aims to classify samples under the
assumption that some classes are not observable during training. To bridge the
gap between the seen and unseen classes, most GZSL methods attempt to associate
the visual features of seen classes with attributes or to generate unseen
samples directly. Nevertheless, the visual features used in the prior
approaches do not necessarily encode semantically related information that the
shared attributes refer to, which degrades the model generalization to unseen
classes. To address this issue, in this paper, we propose a novel semantics
disentangling framework for the generalized zero-shot learning task (SDGZSL),
where the visual features of unseen classes are firstly estimated by a
conditional VAE and then factorized into semantic-consistent and
semantic-unrelated latent vectors. In particular, a total correlation penalty
is applied to guarantee the independence between the two factorized
representations, and the semantic consistency of which is measured by the
derived relation network. Extensive experiments conducted on four GZSL
benchmark datasets have evidenced that the semantic-consistent features
disentangled by the proposed SDGZSL are more generalizable in tasks of
canonical and generalized zero-shot learning. Our source code is available at
https://github.com/uqzhichen/SDGZSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transform Network Architectures for Deep Learning based End-to-End Image/Video Coding in Subsampled Color Spaces. (arXiv:2103.01760v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01760">
<div class="article-summary-box-inner">
<span><p>Most of the existing deep learning based end-to-end image/video coding (DLEC)
architectures are designed for non-subsampled RGB color format. However, in
order to achieve a superior coding performance, many state-of-the-art
block-based compression standards such as High Efficiency Video Coding
(HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for
YUV 4:2:0 format, where U and V components are subsampled by considering the
human visual system. This paper investigates various DLEC designs to support
YUV 4:2:0 format by comparing their performance against the main profiles of
HEVC and VVC standards under a common evaluation framework. Moreover, a new
transform network architecture is proposed to improve the efficiency of coding
YUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the
proposed architecture significantly outperforms naive extensions of existing
architectures designed for RGB format and achieves about 10% average BD-rate
improvement over the intra-frame coding in HEVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation. (arXiv:2103.16219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16219">
<div class="article-summary-box-inner">
<span><p>For unsupervised image-to-image translation, we propose a discriminator
architecture which focuses on the statistical features instead of individual
patches. The network is stabilized by distribution matching of key statistical
features at multiple scales. Unlike the existing methods which impose more and
more constraints on the generator, our method facilitates the shape deformation
and enhances the fine details with a greatly simplified framework. We show that
the proposed method outperforms the existing state-of-the-art models in various
challenging applications including selfie-to-anime, male-to-female and glasses
removal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Accurate Emulation of the SDO/HMI Stokes Inversion with Uncertainty Quantification. (arXiv:2103.17273v2 [astro-ph.SR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17273">
<div class="article-summary-box-inner">
<span><p>The Helioseismic and Magnetic Imager (HMI) onboard NASA's Solar Dynamics
Observatory (SDO) produces estimates of the photospheric magnetic field which
are a critical input to many space weather modelling and forecasting systems.
The magnetogram products produced by HMI and its analysis pipeline are the
result of a per-pixel optimization that estimates solar atmospheric parameters
and minimizes disagreement between a synthesized and observed Stokes vector. In
this paper, we introduce a deep learning-based approach that can emulate the
existing HMI pipeline results two orders of magnitude faster than the current
pipeline algorithms. Our system is a U-Net trained on input Stokes vectors and
their accompanying optimization-based VFISV inversions. We demonstrate that our
system, once trained, can produce high-fidelity estimates of the magnetic field
and kinematic and thermodynamic parameters while also producing meaningful
confidence intervals. We additionally show that despite penalizing only
per-pixel loss terms, our system is able to faithfully reproduce known
systematic oscillations in full-disk statistics produced by the pipeline. This
emulation system could serve as an initialization for the full Stokes inversion
or as an ultra-fast proxy inversion. This work is part of the NASA Heliophysics
DRIVE Science Center (SOLSTICE) at the University of Michigan, under grant NASA
80NSSC20K0600E, and has been open sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Semi-Supervised Learning: An Approach To Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems. (arXiv:2104.03643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03643">
<div class="article-summary-box-inner">
<span><p>Air traffic management and specifically air-traffic control (ATC) rely mostly
on voice communications between Air Traffic Controllers (ATCos) and pilots. In
most cases, these voice communications follow a well-defined grammar that could
be leveraged in Automatic Speech Recognition (ASR) technologies. The callsign
used to address an airplane is an essential part of all ATCo-pilot
communications. We propose a two-steps approach to add contextual knowledge
during semi-supervised training to reduce the ASR system error rates at
recognizing the part of the utterance that contains the callsign. Initially, we
represent in a WFST the contextual knowledge (i.e. air-surveillance data) of an
ATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the
contextual knowledge is added by second-pass decoding (i.e. lattice
re-scoring). Results show that `unseen domains' (e.g. data from airports not
present in the supervised training data) are further aided by contextual SSL
when compared to standalone SSL. For this task, we introduce the Callsign Word
Error Rate (CA-WER) as an evaluation metric, which only assesses ASR
performance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER
relative improvement applying SSL with an additional 17.5% CA-WER improvement
by adding contextual knowledge during SSL on a challenging ATC-based test set
gathered from LiveATC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRiPOD: Human Trajectory and Pose Dynamics Forecasting in the Wild. (arXiv:2104.04029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04029">
<div class="article-summary-box-inner">
<span><p>Joint forecasting of human trajectory and pose dynamics is a fundamental
building block of various applications ranging from robotics and autonomous
driving to surveillance systems. Predicting body dynamics requires capturing
subtle information embedded in the humans' interactions with each other and
with the objects present in the scene. In this paper, we propose a novel
TRajectory and POse Dynamics (nicknamed TRiPOD) method based on graph
attentional networks to model the human-human and human-object interactions
both in the input space and the output space (decoded future output). The model
is supplemented by a message passing interface over the graphs to fuse these
different levels of interactions efficiently. Furthermore, to incorporate a
real-world challenge, we propound to learn an indicator representing whether an
estimated body joint is visible/invisible at each frame, e.g. due to occlusion
or being outside the sensor field of view. Finally, we introduce a new
benchmark for this joint task based on two challenging datasets (PoseTrack and
3DPW) and propose evaluation metrics to measure the effectiveness of
predictions in the global space, even when there are invisible cases of joints.
Our evaluation shows that TRiPOD outperforms all prior work and
state-of-the-art specifically designed for each of the trajectory and pose
forecasting tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SA-ConvONet: Sign-Agnostic Optimization of Convolutional Occupancy Networks. (arXiv:2105.03582v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03582">
<div class="article-summary-box-inner">
<span><p>Surface reconstruction from point clouds is a fundamental problem in the
computer vision and graphics community. Recent state-of-the-arts solve this
problem by individually optimizing each local implicit field during inference.
Without considering the geometric relationships between local fields, they
typically require accurate normals to avoid the sign conflict problem in
overlapped regions of local fields, which severely limits their applicability
to raw scans where surface normals could be unavailable. Although SAL breaks
this limitation via sign-agnostic learning, further works still need to explore
how to extend this technique for local shape modeling. To this end, we propose
to learn implicit surface reconstruction by sign-agnostic optimization of
convolutional occupancy networks, to simultaneously achieve advanced
scalability to large-scale scenes, generality to novel shapes, and
applicability to raw scans in a unified framework. Concretely, we achieve this
goal by a simple yet effective design, which further optimizes the pre-trained
occupancy prediction networks with an unsigned cross-entropy loss during
inference. The learning of occupancy fields is conditioned on convolutional
features from an hourglass network architecture. Extensive experimental
comparisons with previous state-of-the-arts on both object-level and
scene-level datasets demonstrate the superior accuracy of our approach for
surface reconstruction from un-orientated point clouds. The code is available
at https://github.com/tangjiapeng/SA-ConvONet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Explainable, Privacy-Preserved Human-Motion Affect Recognition. (arXiv:2105.03958v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03958">
<div class="article-summary-box-inner">
<span><p>Human motion characteristics are used to monitor the progression of
neurological diseases and mood disorders. Since perceptions of emotions are
also interleaved with body posture and movements, emotion recognition from
human gait can be used to quantitatively monitor mood changes. Many existing
solutions often use shallow machine learning models with raw positional data or
manually extracted features to achieve this. However, gait is composed of many
highly expressive characteristics that can be used to identify human subjects,
and most solutions fail to address this, disregarding the subject's privacy.
This work introduces a novel deep neural network architecture to disentangle
human emotions and biometrics. In particular, we propose a cross-subject
transfer learning technique for training a multi-encoder autoencoder deep
neural network to learn disentangled latent representations of human motion
features. By disentangling subject biometrics from the gait data, we show that
the subject's privacy is preserved while the affect recognition performance
outperforms traditional methods. Furthermore, we exploit Guided Grad-CAM to
provide global explanations of the model's decision across gait cycles. We
evaluate the effectiveness of our method to existing methods at recognizing
emotions using both 3D temporal joint signals and manually extracted features.
We also show that this data can easily be exploited to expose a subject's
identity. Our method shows up to 7% improvement and highlights the joints with
the most significant influence across the average gait cycle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmenter: Transformer for Semantic Segmentation. (arXiv:2105.05633v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05633">
<div class="article-summary-box-inner">
<span><p>Image segmentation is often ambiguous at the level of individual image
patches and requires contextual information to reach label consensus. In this
paper we introduce Segmenter, a transformer model for semantic segmentation. In
contrast to convolution-based methods, our approach allows to model global
context already at the first layer and throughout the network. We build on the
recent Vision Transformer (ViT) and extend it to semantic segmentation. To do
so, we rely on the output embeddings corresponding to image patches and obtain
class labels from these embeddings with a point-wise linear decoder or a mask
transformer decoder. We leverage models pre-trained for image classification
and show that we can fine-tune them on moderate sized datasets available for
semantic segmentation. The linear decoder allows to obtain excellent results
already, but the performance can be further improved by a mask transformer
generating class masks. We conduct an extensive ablation study to show the
impact of the different parameters, in particular the performance is better for
large models and small patch sizes. Segmenter attains excellent results for
semantic segmentation. It outperforms the state of the art on both ADE20K and
Pascal Context datasets and is competitive on Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Emotional Dependencies with Graph Convolutional Networks for Facial Expression Recognition. (arXiv:2106.03487v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03487">
<div class="article-summary-box-inner">
<span><p>Over the past few years, deep learning methods have shown remarkable results
in many face-related tasks including automatic facial expression recognition
(FER) in-the-wild. Meanwhile, numerous models describing the human emotional
states have been proposed by the psychology community. However, we have no
clear evidence as to which representation is more appropriate and the majority
of FER systems use either the categorical or the dimensional model of affect.
Inspired by recent work in multi-label classification, this paper proposes a
novel multi-task learning (MTL) framework that exploits the dependencies
between these two models using a Graph Convolutional Network (GCN) to recognize
facial expressions in-the-wild. Specifically, a shared feature representation
is learned for both discrete and continuous recognition in a MTL setting.
Moreover, the facial expression classifiers and the valence-arousal regressors
are learned through a GCN that explicitly captures the dependencies between
them. To evaluate the performance of our method under real-world conditions we
perform extensive experiments on the AffectNet and Aff-Wild2 datasets. The
results of our experiments show that our method is capable of improving the
performance across different datasets and backbone architectures. Finally, we
also surpass the previous state-of-the-art methods on the categorical model of
AffectNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifold Matching via Deep Metric Learning for Generative Modeling. (arXiv:2106.10777v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10777">
<div class="article-summary-box-inner">
<span><p>We propose a manifold matching approach to generative models which includes a
distribution generator (or data generator) and a metric generator. In our
framework, we view the real data set as some manifold embedded in a
high-dimensional Euclidean space. The distribution generator aims at generating
samples that follow some distribution condensed around the real data manifold.
It is achieved by matching two sets of points using their geometric shape
descriptors, such as centroid and $p$-diameter, with learned distance metric;
the metric generator utilizes both real data and generated samples to learn a
distance metric which is close to some intrinsic geodesic distance on the real
data manifold. The produced distance metric is further used for manifold
matching. The two networks are learned simultaneously during the training
process. We apply the approach on both unsupervised and supervised learning
tasks: in unconditional image generation task, the proposed method obtains
competitive results compared with existing generative models; in
super-resolution task, we incorporate the framework in perception-based models
and improve visual qualities by producing samples with more natural textures.
Experiments and analysis demonstrate the feasibility and effectiveness of the
proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Geometric Distillation Network for Compressive Sensing MRI. (arXiv:2107.04943v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04943">
<div class="article-summary-box-inner">
<span><p>Compressed sensing (CS) is an efficient method to reconstruct MR image from
small sampled data in $k$-space and accelerate the acquisition of MRI. In this
work, we propose a novel deep geometric distillation network which combines the
merits of model-based and deep learning-based CS-MRI methods, it can be
theoretically guaranteed to improve geometric texture details of a linear
reconstruction. Firstly, we unfold the model-based CS-MRI optimization problem
into two sub-problems that consist of image linear approximation and image
geometric compensation. Secondly, geometric compensation sub-problem for
distilling lost texture details in approximation stage can be expanded by
Taylor expansion to design a geometric distillation module fusing features of
different geometric characteristic domains. Additionally, we use a learnable
version with adaptive initialization of the step-length parameter, which allows
model more flexibility that can lead to convergent smoothly. Numerical
experiments verify its superiority over other state-of-the-art CS-MRI
reconstruction approaches. The source code will be available at
\url{https://github.com/fanxiaohong/Deep-Geometric-Distillation-Network-for-CS-MRI}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Constrained Data Representation Learning for Human Motion Segmentation. (arXiv:2107.13362v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13362">
<div class="article-summary-box-inner">
<span><p>Recently, transfer subspace learning based approaches have shown to be a
valid alternative to unsupervised subspace clustering and temporal data
clustering for human motion segmentation (HMS). These approaches leverage prior
knowledge from a source domain to improve clustering performance on a target
domain, and currently they represent the state of the art in HMS. Bucking this
trend, in this paper, we propose a novel unsupervised model that learns a
representation of the data and digs clustering information from the data
itself. Our model is reminiscent of temporal subspace clustering, but presents
two critical differences. First, we learn an auxiliary data matrix that can
deviate from the initial data, hence confer more degrees of freedom to the
coding matrix. Second, we introduce a regularization term for this auxiliary
data matrix that preserves the local geometrical structure present in the
high-dimensional space. The proposed model is efficiently optimized by using an
original Alternating Direction Method of Multipliers (ADMM) formulation
allowing to learn jointly the auxiliary data representation, a nonnegative
dictionary and a coding matrix. Experimental results on four benchmark datasets
for HMS demonstrate that our approach achieves significantly better clustering
performance then state-of-the-art methods, including both unsupervised and more
recent semi-supervised transfer learning approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Matching Networks for Semantic Correspondence. (arXiv:2108.00211v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00211">
<div class="article-summary-box-inner">
<span><p>Deep features have been proven powerful in building accurate dense semantic
correspondences in various previous works. However, the multi-scale and
pyramidal hierarchy of convolutional neural networks has not been well studied
to learn discriminative pixel-level features for semantic correspondence. In
this paper, we propose a multi-scale matching network that is sensitive to tiny
semantic differences between neighboring pixels. We follow the coarse-to-fine
matching strategy and build a top-down feature and matching enhancement scheme
that is coupled with the multi-scale hierarchy of deep convolutional neural
networks. During feature enhancement, intra-scale enhancement fuses
same-resolution feature maps from multiple layers together via local
self-attention and cross-scale enhancement hallucinates higher-resolution
feature maps along the top-down hierarchy. Besides, we learn complementary
matching details at different scales thus the overall matching score is refined
by features of different semantic levels gradually. Our multi-scale matching
network can be trained end-to-end easily with few additional learnable
parameters. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance on three popular benchmarks with high
computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to generate shape from global-local spectra. (arXiv:2108.02161v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02161">
<div class="article-summary-box-inner">
<span><p>In this work, we present a new learning-based pipeline for the generation of
3D shapes. We build our method on top of recent advances on the so called
shape-from-spectrum paradigm, which aims at recovering the full 3D geometric
structure of an object only from the eigenvalues of its Laplacian operator. In
designing our learning strategy, we consider the spectrum as a natural and
ready to use representation to encode variability of the shapes. Therefore, we
propose a simple decoder-only architecture that directly maps spectra to 3D
embeddings; in particular, we combine information from global and local
spectra, the latter being obtained from localized variants of the manifold
Laplacian. This combination captures the relations between the full shape and
its local parts, leading to more accurate generation of geometric details and
an improved semantic control in shape synthesis and novel editing applications.
Our results confirm the improvement of the proposed approach in comparison to
existing and alternative methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable Image Registration using Neural ODEs. (arXiv:2108.03443v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03443">
<div class="article-summary-box-inner">
<span><p>Deformable image registration, aiming to find spatial correspondence between
a given image pair, is one of the most critical problems in the domain of
medical image analysis. In this paper, we present a generic, fast, and accurate
diffeomorphic image registration framework that leverages neural ordinary
differential equations (NODEs). We model each voxel as a moving particle and
consider the set of all voxels in a 3D image as a high-dimensional dynamical
system whose trajectory determines the targeted deformation field. Compared
with traditional optimization-based methods, our framework reduces the running
time from tens of minutes to tens of seconds. Compared with recent data-driven
deep learning methods, our framework is more accessible since it does not
require large amounts of training data. Our experiments show that the
registration results of our method outperform state-of-the-arts under various
metrics, indicating that our modeling approach is well fitted for the task of
deformable image registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04409">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which
would inveigle neural networks to make prediction errors with small
perturbations on the input images. Researchers have been devoted to promoting
the research on the universal adversarial perturbations (UAPs) which are
gradient-free and have little prior knowledge on data distributions. Procedural
adversarial noise attack is a data-free universal perturbation generation
method. In this paper, we propose two universal adversarial perturbation (UAP)
generation methods based on procedural noise functions: Simplex noise and
Worley noise. In our framework, the shading which disturbs visual
classification is generated with rendering technology. Without changing the
semantic representations, the adversarial examples generated via our methods
show superior performance on the attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DexMV: Imitation Learning for Dexterous Manipulation from Human Videos. (arXiv:2108.05877v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05877">
<div class="article-summary-box-inner">
<span><p>While we have made significant progress on understanding hand-object
interactions in computer vision, it is still very challenging for robots to
perform complex dexterous manipulation. In this paper, we propose a new
platform and pipeline, DexMV (Dexterous Manipulation from Videos), for
imitation learning to bridge the gap between computer vision and robot
learning. We design a platform with: (i) a simulation system for complex
dexterous manipulation tasks with a multi-finger robot hand and (ii) a computer
vision system to record large-scale demonstrations of a human hand conducting
the same tasks. In our new pipeline, we extract 3D hand and object poses from
the videos, and convert them to robot demonstrations via motion retargeting. We
then apply and compare multiple imitation learning algorithms with the
demonstrations. We show that the demonstrations can indeed improve robot
learning by a large margin and solve the complex tasks which reinforcement
learning alone cannot solve. Project page with video:
https://yzqin.github.io/dexmv
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconcile Prediction Consistency for Balanced Object Detection. (arXiv:2108.10809v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10809">
<div class="article-summary-box-inner">
<span><p>Classification and regression are two pillars of object detectors. In most
CNN-based detectors, these two pillars are optimized independently. Without
direct interactions between them, the classification loss and the regression
loss can not be optimized synchronously toward the optimal direction in the
training phase. This clearly leads to lots of inconsistent predictions with
high classification score but low localization accuracy or low classification
score but high localization accuracy in the inference phase, especially for the
objects of irregular shape and occlusion, which severely hurts the detection
performance of existing detectors after NMS. To reconcile prediction
consistency for balanced object detection, we propose a Harmonic loss to
harmonize the optimization of classification branch and localization branch.
The Harmonic loss enables these two branches to supervise and promote each
other during training, thereby producing consistent predictions with high
co-occurrence of top classification and localization in the inference phase.
Furthermore, in order to prevent the localization loss from being dominated by
outliers during training phase, a Harmonic IoU loss is proposed to harmonize
the weight of the localization loss of different IoU-level samples.
Comprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the
generality and effectiveness of our model for facilitating existing object
detectors to state-of-the-art accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOP: You Only Look Once for Panoptic Driving Perception. (arXiv:2108.11250v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11250">
<div class="article-summary-box-inner">
<span><p>A panoptic driving perception system is an essential part of autonomous
driving. A high-precision and real-time perception system can assist the
vehicle in making the reasonable decision while driving. We present a panoptic
driving perception network (YOLOP) to perform traffic object detection,
drivable area segmentation and lane detection simultaneously. It is composed of
one encoder for feature extraction and three decoders to handle the specific
tasks. Our model performs extremely well on the challenging BDD100K dataset,
achieving state-of-the-art on all three tasks in terms of accuracy and speed.
Besides, we verify the effectiveness of our multi-task learning model for joint
training via ablative studies. To our best knowledge, this is the first work
that can process these three visual perception tasks simultaneously in
real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent
accuracy. To facilitate further research, the source codes and pre-trained
models will be released at https://github.com/hustvl/YOLOP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shifted Chunk Transformer for Spatio-Temporal Representational Learning. (arXiv:2108.11575v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11575">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal representational learning has been widely adopted in various
fields such as action recognition, video object segmentation, and action
anticipation. Previous spatio-temporal representational learning approaches
primarily employ ConvNets or sequential models,e.g., LSTM, to learn the
intra-frame and inter-frame features. Recently, Transformer models have
successfully dominated the study of natural language processing (NLP), image
classification, etc. However, the pure-Transformer based spatio-temporal
learning can be prohibitively costly on memory and computation to extract
fine-grained features from a tiny patch. To tackle the training difficulty and
enhance the spatio-temporal learning, we construct a shifted chunk Transformer
with pure self-attention blocks. Leveraging the recent efficient Transformer
design in NLP, this shifted chunk Transformer can learn hierarchical
spatio-temporal features from a local tiny patch to a global video clip. Our
shifted self-attention can also effectively model complicated inter-frame
variances. Furthermore, we build a clip encoder based on Transformer to model
long-term temporal dependencies. We conduct thorough ablation studies to
validate each component and hyper-parameters in our shifted chunk Transformer,
and it outperforms previous state-of-the-art approaches on Kinetics-400,
Kinetics-600, UCF101, and HMDB51. Code and trained models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Diversify for Single Domain Generalization. (arXiv:2108.11726v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11726">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims to generalize a model trained on multiple
source (i.e., training) domains to a distributionally different target (i.e.,
test) domain. In contrast to the conventional DG that strictly requires the
availability of multiple source domains, this paper considers a more realistic
yet challenging scenario, namely Single Domain Generalization (Single-DG),
where only one source domain is available for training. In this scenario, the
limited diversity may jeopardize the model generalization on unseen target
domains. To tackle this problem, we propose a style-complement module to
enhance the generalization power of the model by synthesizing images from
diverse distributions that are complementary to the source ones. More
specifically, we adopt a tractable upper bound of mutual information (MI)
between the generated and source samples and perform a two-step optimization
iteratively: (1) by minimizing the MI upper bound approximation for each sample
pair, the generated images are forced to be diversified from the source
samples; (2) subsequently, we maximize the MI between the samples from the same
semantic category, which assists the network to learn discriminative features
from diverse-styled images. Extensive experiments on three benchmark datasets
demonstrate the superiority of our approach, which surpasses the
state-of-the-art single-DG methods by up to 25.14%.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-30 23:02:35.148328507 UTC">2021-08-30 23:02:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>