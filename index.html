<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-26T01:30:00Z">04-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ChapterBreak: A Challenge Dataset for Long-Range Language Models. (arXiv:2204.10878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10878">
<div class="article-summary-box-inner">
<span><p>While numerous architectures for long-range language models (LRLMs) have
recently been proposed, a meaningful evaluation of their discourse-level
language understanding capabilities has not yet followed. To this end, we
introduce ChapterBreak, a challenge dataset that provides an LRLM with a long
segment from a narrative that ends at a chapter boundary and asks it to
distinguish the beginning of the ground-truth next chapter from a set of
negative segments sampled from the same narrative. A fine-grained human
annotation reveals that our dataset contains many complex types of chapter
transitions (e.g., parallel narratives, cliffhanger endings) that require
processing global context to comprehend. Experiments on ChapterBreak show that
existing LRLMs fail to effectively leverage long-range context, substantially
underperforming a segment-level model trained directly for this task. We
publicly release our ChapterBreak dataset to spur more principled future
research into LRLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locally Aggregated Feature Attribution on Natural Language Model Understanding. (arXiv:2204.10893v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10893">
<div class="article-summary-box-inner">
<span><p>With the growing popularity of deep-learning models, model understanding
becomes more important. Much effort has been devoted to demystify deep neural
networks for better interpretability. Some feature attribution methods have
shown promising results in computer vision, especially the gradient-based
methods where effectively smoothing the gradients with reference data is key to
a robust and faithful result. However, direct application of these
gradient-based methods to NLP tasks is not trivial due to the fact that the
input consists of discrete tokens and the "reference" tokens are not explicitly
defined. In this work, we propose Locally Aggregated Feature Attribution
(LAFA), a novel gradient-based feature attribution method for NLP models.
Instead of relying on obscure reference tokens, it smooths gradients by
aggregating similar reference texts derived from language model embeddings. For
evaluation purpose, we also design experiments on different NLP tasks including
Entity Recognition and Sentiment Analysis on public datasets as well as key
feature detection on a constructed Amazon catalogue dataset. The superior
performance of the proposed method is demonstrated through experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCSE: Multimodal Contrastive Learning of Sentence Embeddings. (arXiv:2204.10931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10931">
<div class="article-summary-box-inner">
<span><p>Learning semantically meaningful sentence embeddings is an open problem in
natural language processing. In this work, we propose a sentence embedding
learning approach that exploits both visual and textual information via a
multimodal contrastive objective. Through experiments on a variety of semantic
textual similarity tasks, we demonstrate that our approach consistently
improves the performance across various datasets and pre-trained encoders. In
particular, combining a small amount of multimodal data with a large text-only
corpus, we improve the state-of-the-art average Spearman's correlation by 1.7%.
By analyzing the properties of the textual embedding space, we show that our
model excels in aligning semantically similar sentences, providing an
explanation for its improved performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-level Alignment Training Scheme for Video-and-Language Grounding. (arXiv:2204.10938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10938">
<div class="article-summary-box-inner">
<span><p>To solve video-and-language grounding tasks, the key is for the network to
understand the connection between the two modalities. For a pair of video and
language description, their semantic relation is reflected by their encodings'
similarity. A good multi-modality encoder should be able to well capture both
inputs' semantics and encode them in the shared feature space where embedding
distance gets properly translated into their semantic similarity. In this work,
we focused on this semantic connection between video and language, and
developed a multi-level alignment training scheme to directly shape the
encoding process. Global and segment levels of video-language alignment pairs
were designed, based on the information similarity ranging from high-level
context to fine-grained semantics. The contrastive loss was used to contrast
the encodings' similarities between the positive and negative alignment pairs,
and to ensure the network is trained in such a way that similar information is
encoded closely in the shared feature space while information of different
semantics is kept apart. Our multi-level alignment training can be applied to
various video-and-language grounding tasks. Together with the task-specific
training loss, our framework achieved comparable performance to previous
state-of-the-arts on multiple video QA and retrieval datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10939">
<div class="article-summary-box-inner">
<span><p>Document intelligence automates the extraction of information from documents
and supports many business applications. Recent self-supervised learning
methods on large-scale unlabeled document datasets have opened up promising
directions towards reducing annotation efforts by training models with
self-supervised objectives. However, most of the existing document pretraining
methods are still language-dominated. We present UDoc, a new unified
pretraining framework for document understanding. UDoc is designed to support
most document understanding tasks, extending the Transformer to take multimodal
embeddings as input. Each input element is composed of words and visual
features from a semantic region of the input document image. An important
feature of UDoc is that it learns a generic representation by making use of
three self-supervised losses, encouraging the representation to model
sentences, learn similarities, and align modalities. Extensive empirical
analysis demonstrates that the pretraining procedure learns better joint
representations and leads to improvements in downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Meaning Representation for Task-Oriented Dialogue Systems. (arXiv:2204.10989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10989">
<div class="article-summary-box-inner">
<span><p>Dialogue meaning representation formulates natural language utterance
semantics in their conversational context in an explicit and machine-readable
form. Previous work typically follows the intent-slot framework, which is easy
for annotation yet limited on scalability for complex linguistic expressions. A
line of works alleviates the representation issue by introducing hierarchical
structures but challenging to express complex compositional semantics, such as
negation and coreference. We propose Dialogue Meaning Representation (DMR), a
flexible and easily extendable representation for task-oriented dialogue. Our
representation contains a set of nodes and edges with inheritance hierarchy to
represent rich semantics for compositional semantics and task-specific
concepts. We annotated DMR-FastFood, a multi-turn dialogue dataset with more
than 70k utterances, with DMR. We propose two evaluation tasks to evaluate
different machine learning based dialogue models, and further propose a novel
coreference resolution model GNNCoref for the graph-based coreference
resolution task. Experiments show that DMR can be parsed well with pretrained
Seq2Seq model, and GNNCoref outperforms the baseline models by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction. (arXiv:2204.10994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10994">
<div class="article-summary-box-inner">
<span><p>This paper presents MuCGEC, a multi-reference multi-source evaluation dataset
for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences
collected from three different Chinese-as-a-Second-Language (CSL) learner
sources. Each sentence has been corrected by three annotators, and their
corrections are meticulously reviewed by an expert, resulting in 2.3 references
per sentence. We conduct experiments with two mainstream CGEC models, i.e., the
sequence-to-sequence (Seq2Seq) model and the sequence-to-edit (Seq2Edit) model,
both enhanced with large pretrained language models (PLMs), achieving
competitive benchmark performance on previous and our datasets. We also discuss
CGEC evaluation methodologies, including the effect of multiple references and
using a char-based metric. Our annotation guidelines, data, and code are
available at \url{https://github.com/HillZhang1999/MuCGEC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LitMind Dictionary: An Open-Source Online Dictionary. (arXiv:2204.11087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11087">
<div class="article-summary-box-inner">
<span><p>Dictionaries can help language learners to learn vocabulary by providing
definitions of words. Since traditional dictionaries present word senses as
discrete items in predefined inventories, they fall short of flexibility, which
is required in providing specific meanings of words in particular contexts. In
this paper, we introduce the LitMind Dictionary
(https://dictionary.litmind.ink), an open-source online generative dictionary
that takes a word and context containing the word as input and automatically
generates a definition as output. Incorporating state-of-the-art definition
generation models, it supports not only Chinese and English, but also
Chinese-English cross-lingual queries. Moreover, it has a user-friendly
front-end design that can help users understand the query words quickly and
easily. All the code and data are available at
https://github.com/blcuicall/litmind-dictionary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiMulti: a Corpus for Cross-Lingual Summarization. (arXiv:2204.11104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11104">
<div class="article-summary-box-inner">
<span><p>Cross-lingual summarization (CLS) is the task to produce a summary in one
particular language for a source document in a different language. We introduce
WikiMulti - a new dataset for cross-lingual summarization based on Wikipedia
articles in 15 languages. As a set of baselines for further studies, we
evaluate the performance of existing cross-lingual abstractive summarization
methods on our dataset. We make our dataset publicly available here:
https://github.com/tikhonovpavel/wikimulti
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning. (arXiv:2204.11117v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11117">
<div class="article-summary-box-inner">
<span><p>Recent work has found that multi-task training with a large number of diverse
tasks can uniformly improve downstream performance on unseen target tasks. In
contrast, literature on task transferability has established that the choice of
intermediate tasks can heavily affect downstream task performance. In this
work, we aim to disentangle the effect of scale and relatedness of tasks in
multi-task representation learning. We find that, on average, increasing the
scale of multi-task learning, in terms of the number of tasks, indeed results
in better learned representations than smaller multi-task setups. However, if
the target tasks are known ahead of time, then training on a smaller set of
related tasks is competitive to the large-scale multi-task training at a
reduced computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding Knowledge for Document Summarization: A Survey. (arXiv:2204.11190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11190">
<div class="article-summary-box-inner">
<span><p>Knowledge-aware methods have boosted a range of Natural Language Processing
applications over the last decades. With the gathered momentum, knowledge
recently has been pumped into enormous attention in document summarization
research. Previous works proved that knowledge-embedded document summarizers
excel at generating superior digests, especially in terms of informativeness,
coherence, and fact consistency. This paper pursues to present the first
systematic survey for the state-of-the-art methodologies that embed knowledge
into document summarizers. Particularly, we propose novel taxonomies to
recapitulate knowledge and knowledge embeddings under the document
summarization view. We further explore how embeddings are generated in learning
architectures of document summarization models, especially in deep learning
models. At last, we discuss the challenges of this topic and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPiDA: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification. (arXiv:2204.11205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11205">
<div class="article-summary-box-inner">
<span><p>Recent works have empirically shown the effectiveness of data augmentation
(DA) in NLP tasks, especially for those suffering from data scarcity.
Intuitively, given the size of generated data, their diversity and quality are
crucial to the performance of targeted tasks. However, to the best of our
knowledge, most existing methods consider only either the diversity or the
quality of augmented data, thus cannot fully mine the potential of DA for NLP.
In this paper, we present an easy and plug-in data augmentation framework EPiDA
to support effective text classification. EPiDA employs two mechanisms:
relative entropy maximization (REM) and conditional entropy minimization (CEM)
to control data generation, where REM is designed to enhance the diversity of
augmented data while CEM is exploited to ensure their semantic consistency.
EPiDA can support efficient and continuous data generation for effective
classifier training. Extensive experiments show that EPiDA outperforms existing
SOTA methods in most cases, though not using any agent networks or pre-trained
generation networks, and it works well with various DA algorithms and
classification models. Code is available at
https://github.com/zhaominyiz/EPiDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training. (arXiv:2204.11218v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11218">
<div class="article-summary-box-inner">
<span><p>Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained
language models (PLMs) like BERT contain matching subnetworks that have similar
transfer learning performance as the original PLM. These subnetworks are found
using magnitude-based pruning. In this paper, we find that the BERT subnetworks
have even more potential than these studies have shown. Firstly, we discover
that the success of magnitude pruning can be attributed to the preserved
pre-training performance, which correlates with the downstream transferability.
Inspired by this, we propose to directly optimize the subnetwork structure
towards the pre-training objectives, which can better preserve the pre-training
performance. Specifically, we train binary masks over model weights on the
pre-training tasks, with the aim of preserving the universal transferability of
the subnetwork, which is agnostic to any specific downstream tasks. We then
fine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The
results show that, compared with magnitude pruning, mask training can
effectively find BERT subnetworks with improved overall performance on
downstream tasks. Moreover, our method is also more efficient in searching
subnetworks and more advantageous when fine-tuning within a certain range of
data scarcity. Our code is available at https://github.com/llyx97/TAMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Naturalness of Simulated Conversations for End-to-End Neural Diarization. (arXiv:2204.11232v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11232">
<div class="article-summary-box-inner">
<span><p>This paper investigates a method for simulating natural conversation in the
model training of end-to-end neural diarization (EEND). Due to the lack of any
annotated real conversational dataset, EEND is usually pretrained on a
large-scale simulated conversational dataset first and then adapted to the
target real dataset. Simulated datasets play an essential role in the training
of EEND, but as yet there has been insufficient investigation into an optimal
simulation method. We thus propose a method to simulate natural conversational
speech. In contrast to conventional methods, which simply combine the speech of
multiple speakers, our method takes turn-taking into account. We define four
types of speaker transition and sequentially arrange them to simulate natural
conversations. The dataset simulated using our method was found to be
statistically similar to the real dataset in terms of the silence and overlap
ratios. The experimental results on two-speaker diarization using the CALLHOME
and CSJ datasets showed that the simulated dataset contributes to improving the
performance of EEND.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-domain Dialogue Generation Grounded with Dynamic Multi-form Knowledge Fusion. (arXiv:2204.11239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11239">
<div class="article-summary-box-inner">
<span><p>Open-domain multi-turn conversations normally face the challenges of how to
enrich and expand the content of the conversation. Recently, many approaches
based on external knowledge are proposed to generate rich semantic and
information conversation. Two types of knowledge have been studied for
knowledge-aware open-domain dialogue generation: structured triples from
knowledge graphs and unstructured texts from documents. To take both advantages
of abundant unstructured latent knowledge in the documents and the information
expansion capabilities of the structured knowledge graph, this paper presents a
new dialogue generation model, Dynamic Multi-form Knowledge Fusion based
Open-domain Chatt-ing Machine (DMKCM).In particular, DMKCM applies an indexed
text (a virtual Knowledge Base) to locate relevant documents as 1st hop and
then expands the content of the dialogue and its 1st hop using a commonsense
knowledge graph to get apposite triples as 2nd hop. To merge these two forms of
knowledge into the dialogue effectively, we design a dynamic virtual knowledge
selector and a controller that help to enrich and expand knowledge space.
Moreover, DMKCM adopts a novel dynamic knowledge memory module that effectively
uses historical reasoning knowledge to generate better responses. Experimental
results indicate the effectiveness of our method in terms of dialogue coherence
and informativeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved far-field speech recognition using Joint Variational Autoencoder. (arXiv:2204.11286v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11286">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems suffer considerably when source
speech is corrupted with noise or room impulse responses (RIR). Typically,
speech enhancement is applied in both mismatched and matched scenario training
and testing. In matched setting, acoustic model (AM) is trained on
dereverberated far-field features while in mismatched setting, AM is fixed. In
recent past, mapping speech features from far-field to close-talk using
denoising autoencoder (DA) has been explored. In this paper, we focus on
matched scenario training and show that the proposed joint VAE based mapping
achieves a significant improvement over DA. Specifically, we observe an
absolute improvement of 2.5% in word error rate (WER) compared to DA based
enhancement and 3.96% compared to AM trained directly on far-field filterbank
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complexity and Avoidance. (arXiv:2204.11289v1 [math.LO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11289">
<div class="article-summary-box-inner">
<span><p>In this dissertation we examine the relationships between the several
hierarchies, including the complexity, $\mathrm{LUA}$ (Linearly Universal
Avoidance), and shift complexity hierarchies, with an eye towards quantitative
bounds on growth rates therein. We show that for suitable $f$ and $p$, there
are $q$ and $g$ such that $\mathrm{LUA}(q) \leq_\mathrm{s} \mathrm{COMPLEX}(f)$
and $\mathrm{COMPLEX}(g) \leq_\mathrm{s} \mathrm{LUA}(p)$, as well as quantify
the growth rates of $q$ and $g$. In the opposite direction, we show that for
certain sub-identical $f$ satisfying $\lim_{n \to \infty}{f(n)/n}=1$ there is a
$q$ such that $\mathrm{COMPLEX}(f) \leq_\mathrm{w} \mathrm{LUA}(q)$, and for
certain fast-growing $p$ there is a $g$ such that $\mathrm{LUA}(p)
\leq_\mathrm{s} \mathrm{COMPLEX}(g)$, as well as quantify the growth rates of
$q$ and $g$.
</p>
<p>Concerning shift complexity, explicit bounds are given on how slow-growing
$q$ must be for any member of $\rm{LUA}(q)$ to compute $\delta$-shift complex
sequences. Motivated by the complexity hierarchy, we generalize the notion of
shift complexity to consider sequences $X$ satisfying $\operatorname{KP}(\tau)
\geq f(|\tau|) - O(1)$ for all substrings $\tau$ of $X$ where $f$ is any order
function. We show that for sufficiently slow-growing $f$, $f$-shift complex
sequences can be uniformly computed by $g$-complex sequences, where $g$ grows
slightly faster than $f$.
</p>
<p>The structure of the $\mathrm{LUA}$ hierarchy is examined using bushy tree
forcing, with the main result being that for any order function $p$, there is a
slow-growing order function $q$ such that $\mathrm{LUA}(p)$ and
$\mathrm{LUA}(q)$ are weakly incomparable. Using this, we prove new results
about the filter of the weak degrees of deep nonempty $\Pi^0_1$ classes and the
connection between the shift complexity and $\mathrm{LUA}$ hierarchies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion-Aware Transformer Encoder for Empathetic Dialogue Generation. (arXiv:2204.11320v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11320">
<div class="article-summary-box-inner">
<span><p>Modern day conversational agents are trained to emulate the manner in which
humans communicate. To emotionally bond with the user, these virtual agents
need to be aware of the affective state of the user. Transformers are the
recent state of the art in sequence-to-sequence learning that involves training
an encoder-decoder model with word embeddings from utterance-response pairs. We
propose an emotion-aware transformer encoder for capturing the emotional
quotient in the user utterance in order to generate human-like empathetic
responses. The contributions of our paper are as follows: 1) An emotion
detector module trained on the input utterances determines the affective state
of the user in the initial phase 2) A novel transformer encoder is proposed
that adds and normalizes the word embedding with emotion embedding thereby
integrating the semantic and affective aspects of the input utterance 3) The
encoder and decoder stacks belong to the Transformer-XL architecture which is
the recent state of the art in language modeling. Experimentation on the
benchmark Facebook AI empathetic dialogue dataset confirms the efficacy of our
model from the higher BLEU-4 scores achieved for the generated responses as
compared to existing methods. Emotionally intelligent virtual agents are now a
reality and inclusion of affect as a modality in all human-machine interfaces
is foreseen in the immediate future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Item Response Theory Framework for Persuasion. (arXiv:2204.11337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11337">
<div class="article-summary-box-inner">
<span><p>In this paper, we apply Item Response Theory, popular in education and
political science research, to the analysis of argument persuasiveness in
language. We empirically evaluate the model's performance on three datasets,
including a novel dataset in the area of political advocacy. We show the
advantages of separating these components under several style and content
representations, including evaluating the ability of the speaker embeddings
generated by the model to parallel real-world observations about
persuadability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate Me Not: Detecting Hate Inducing Memes in Code Switched Languages. (arXiv:2204.11356v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11356">
<div class="article-summary-box-inner">
<span><p>The rise in the number of social media users has led to an increase in the
hateful content posted online. In countries like India, where multiple
languages are spoken, these abhorrent posts are from an unusual blend of
code-switched languages. This hate speech is depicted with the help of images
to form "Memes" which create a long-lasting impact on the human mind. In this
paper, we take up the task of hate and offense detection from multimodal data,
i.e. images (Memes) that contain text in code-switched languages. We firstly
present a novel triply annotated Indian political Memes (IPM) dataset, which
comprises memes from various Indian political events that have taken place
post-independence and are classified into three distinct categories. We also
propose a binary-channelled CNN cum LSTM based model to process the images
using the CNN model and text using the LSTM model to get state-of-the-art
results for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Conditioned Question Generation for Robust Attention Distribution in Neural Information Retrieval. (arXiv:2204.11373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11373">
<div class="article-summary-box-inner">
<span><p>We show that supervised neural information retrieval (IR) models are prone to
learning sparse attention patterns over passage tokens, which can result in key
phrases including named entities receiving low attention weights, eventually
leading to model under-performance. Using a novel targeted synthetic data
generation method that identifies poorly attended entities and conditions the
generation episodes on those, we teach neural IR to attend more uniformly and
robustly to all entities in a given passage. On two public IR benchmarks, we
empirically show that the proposed method helps improve both the model's
attention patterns and retrieval performance, including in zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Financial data analysis application via multi-strategy text processing. (arXiv:2204.11394v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11394">
<div class="article-summary-box-inner">
<span><p>Maintaining financial system stability is critical to economic development,
and early identification of risks and opportunities is essential. The financial
industry contains a wide variety of data, such as financial statements,
customer information, stock trading data, news, etc. Massive heterogeneous data
calls for intelligent algorithms for machines to process and understand. This
paper mainly focuses on the stock trading data and news about China A-share
companies. We present a financial data analysis application, Financial Quotient
Porter, designed to combine textual and numerical data by using a
multi-strategy data mining approach. Additionally, we present our efforts and
plans in deep learning financial text processing application scenarios using
natural language processing (NLP) and knowledge graph (KG) technologies. Based
on KG technology, risks and opportunities can be identified from heterogeneous
data. NLP technology can be used to extract entities, relations, and events
from unstructured text, and analyze market sentiment. Experimental results show
market sentiments towards a company and an industry, as well as news-level
associations between companies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11406">
<div class="article-summary-box-inner">
<span><p>Self-augmentation has been received increasing research interest recently to
improve named entity recognition (NER) performance in low-resource scenarios.
Token substitution and mixup are two feasible heterogeneous self-augmentation
techniques for NER that can achieve effective performance with certain
specialized efforts. Noticeably, self-augmentation may introduce potentially
noisy augmented data. Prior research has mainly resorted to heuristic rule
based constraints to reduce the noise for specific self-augmentation
individually. In this paper, we revisit the two self-augmentation methods for
NER, and propose a unified meta-reweighting strategy for these heterogeneous
methods to achieve a natural integration. Our method is easily extensible,
imposing little effort on a specific self-augmentation method. Experiments on
different Chinese and English NER benchmarks demonstrate that our token
substitution and mixup method, as well as their integration, can obtain
effective performance improvement. Based on the meta-reweighting mechanism, we
can enhance the advantages of the self-augmentation techniques without extra
efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Headline Diagnosis: Manipulation of Content Farm Headlines. (arXiv:2204.11408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11408">
<div class="article-summary-box-inner">
<span><p>As technology grows faster, the news spreads through social media. In order
to attract more readers and acquire additional profit, some news agencies
reproduce massive news in a more appealing manner. Therefore, it is essential
to accurately predict whether a news article is from official news agencies.
This work develops a headline classification based on Convoluted Neural Network
to determine credibility of a news article. The model primarily focuses on
investigating key factors from headlines. These factors include word
segmentation, part-of-speech tags, and sentiment features. With integrating
these features into the proposed classification model, the demonstrated
evaluation achieves 93.99% for accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It Takes Two Flints to Make a Fire: Multitask Learning of Neural Relation and Explanation Classifiers. (arXiv:2204.11424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11424">
<div class="article-summary-box-inner">
<span><p>We propose an explainable approach for relation extraction that mitigates the
tension between generalization and explainability by jointly training for the
two goals. Our approach uses a multi-task learning architecture, which jointly
trains a classifier for relation extraction, and a sequence model that labels
words in the context of the relation that explain the decisions of the relation
classifier. We also convert the model outputs to rules to bring global
explanations to this approach. This sequence model is trained using a hybrid
strategy: supervised, when supervision from pre-existing patterns is available,
and semi-supervised otherwise. In the latter situation, we treat the sequence
model's labels as latent variables, and learn the best assignment that
maximizes the performance of the relation classifier. We evaluate the proposed
approach on the two datasets and show that the sequence model provides labels
that serve as accurate explanations for the relation classifier's decisions,
and, importantly, that the joint training generally improves the performance of
the relation classifier. We also evaluate the performance of the generated
rules and show that the new rules are great add-on to the manual rules and
bring the rule-based system much closer to the neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Star-Transformer. (arXiv:1902.09113v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.09113">
<div class="article-summary-box-inner">
<span><p>Although Transformer has achieved great successes on many NLP tasks, its
heavy structure with fully-connected attention connections leads to
dependencies on large training data. In this paper, we present
Star-Transformer, a lightweight alternative by careful sparsification. To
reduce model complexity, we replace the fully-connected structure with a
star-shaped topology, in which every two non-adjacent nodes are connected
through a shared relay node. Thus, complexity is reduced from quadratic to
linear, while preserving capacity to capture both local composition and
long-range dependency. The experiments on four tasks (22 datasets) show that
Star-Transformer achieved significant improvements against the standard
Transformer for the modestly sized datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drink Bleach or Do What Now? Covid-HeRA: A Study of Risk-Informed Health Decision Making in the Presence of COVID-19 Misinformation. (arXiv:2010.08743v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08743">
<div class="article-summary-box-inner">
<span><p>Given the widespread dissemination of inaccurate medical advice related to
the 2019 coronavirus pandemic (COVID-19), such as fake remedies, treatments and
prevention suggestions, misinformation detection has emerged as an open problem
of high importance and interest for the research community. Several works study
health misinformation detection, yet little attention has been given to the
perceived severity of misinformation posts. In this work, we frame health
misinformation as a risk assessment task. More specifically, we study the
severity of each misinformation story and how readers perceive this severity,
i.e., how harmful a message believed by the audience can be and what type of
signals can be used to recognize potentially malicious fake news and detect
refuted claims. To address our research questions, we introduce a new benchmark
dataset, accompanied by detailed data analysis. We evaluate several traditional
and state-of-the-art models and show there is a significant gap in performance
when applying traditional misinformation classification models to this task. We
conclude with open challenges and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Potential Idiomatic Expression (PIE)-English: Corpus for Classes of Idioms. (arXiv:2105.03280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03280">
<div class="article-summary-box-inner">
<span><p>We present a fairly large, Potential Idiomatic Expression (PIE) dataset for
Natural Language Processing (NLP) in English. The challenges with NLP systems
with regards to tasks such as Machine Translation (MT), word sense
disambiguation (WSD) and information retrieval make it imperative to have a
labelled idioms dataset with classes such as it is in this work. To the best of
the authors' knowledge, this is the first idioms corpus with classes of idioms
beyond the literal and the general idioms classification. In particular, the
following classes are labelled in the dataset: metaphor, simile, euphemism,
parallelism, personification, oxymoron, paradox, hyperbole, irony and literal.
We obtain an overall inter-annotator agreement (IAA) score, between two
independent annotators, of 88.89%. Many past efforts have been limited in the
corpus size and classes of samples but this dataset contains over 20,100
samples with almost 1,200 cases of idioms (with their meanings) from 10 classes
(or senses). The corpus may also be extended by researchers to meet specific
needs. The corpus has part of speech (PoS) tagging from the NLTK library.
Classification experiments performed on the corpus to obtain a baseline and
comparison among three common models, including the BERT model, give good
results. We also make publicly available the corpus and the relevant codes for
working with it for NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06232">
<div class="article-summary-box-inner">
<span><p>To diversify and enrich generated dialogue responses, knowledge-grounded
dialogue has been investigated in recent years. The existing methods tackle the
knowledge grounding challenge by retrieving the relevant sentences over a large
corpus and augmenting the dialogues with explicit extra information. Despite
their success, however, the existing works have drawbacks in inference
efficiency. This paper proposes KnowExpert, a framework to bypass the explicit
retrieval process and inject knowledge into the pre-trained language models
with lightweight adapters and adapt to the knowledge-grounded dialogue task. To
the best of our knowledge, this is the first attempt to tackle this challenge
without retrieval in this task under an open-domain chit-chat scenario. The
experimental results show that Knowexpert performs comparably with some
retrieval-based baselines while being time-efficient in inference,
demonstrating the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distantly Supervised Relation Extraction via Recursive Hierarchy-Interactive Attention and Entity-Order Perception. (arXiv:2105.08213v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08213">
<div class="article-summary-box-inner">
<span><p>Wrong-labeling problem and long-tail relations severely affect the
performance of distantly supervised relation extraction task. Many studies
mitigate the effect of wrong-labeling through selective attention mechanism and
handle long-tail relations by introducing relation hierarchies to share
knowledge. However, almost all existing studies ignore the fact that, in a
sentence, the appearance order of two entities contributes to the understanding
of its semantics. Furthermore, they only utilize each relation level of
relation hierarchies separately, but do not exploit the heuristic effect
between relation levels, i.e., higher-level relations can give useful
information to the lower ones. Based on the above, in this paper, we design a
novel Recursive Hierarchy-Interactive Attention network (RHIA) to further
handle long-tail relations, which models the heuristic effect between relation
levels. From the top down, it passes relation-related information layer by
layer, which is the most significant difference from existing models, and
generates relation-augmented sentence representations for each relation level
in a recursive structure. Besides, we introduce a newfangled training
objective, called Entity-Order Perception (EOP), to make the sentence encoder
retain more entity appearance information. Substantial experiments on the
popular (NYT) dataset are conducted. Compared to prior baselines, our RHIA-EOP
achieves state-of-the-art performance in terms of precision-recall (P-R)
curves, AUC, Top-N precision and other evaluation metrics. Insightful analysis
also demonstrates the necessity and effectiveness of each component of
RHIA-EOP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-Aware Language Models as Temporal Knowledge Bases. (arXiv:2106.15110v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15110">
<div class="article-summary-box-inner">
<span><p>Many facts come with an expiration date, from the name of the President to
the basketball team Lebron James plays for. But language models (LMs) are
trained on snapshots of data collected at a specific moment in time, and this
can limit their utility, especially in the closed-book setting where the
pretraining corpus must contain the facts the model should memorize. We
introduce a diagnostic dataset aimed at probing LMs for factual knowledge that
changes over time and highlight problems with LMs at either end of the spectrum
-- those trained on specific slices of temporal data, as well as those trained
on a wide range of temporal data. To mitigate these problems, we propose a
simple technique for jointly modeling text with its timestamp. This improves
memorization of seen facts from the training time period, as well as
calibration on predictions about unseen facts from future time periods. We also
show that models trained with temporal context can be efficiently "refreshed"
as new data arrives, without the need for retraining from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. (arXiv:2108.12409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12409">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the transformer model by Vaswani et al. (2017), a
fundamental question has yet to be answered: how does a model achieve
extrapolation at inference time for sequences that are longer than it saw
during training? We first show that extrapolation can be enabled by simply
changing the position representation method, though we find that current
methods do not allow for efficient extrapolation. We therefore introduce a
simpler and more efficient position method, Attention with Linear Biases
(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,
it biases query-key attention scores with a penalty that is proportional to
their distance. We show that this method trains a 1.3 billion parameter model
on input sequences of length 1024 that extrapolates to input sequences of
length 2048, achieving the same perplexity as a sinusoidal position embedding
model trained on inputs of length 2048 but training 11% faster and using 11%
less memory. ALiBi's inductive bias towards recency also leads it to outperform
multiple strong position methods on the WikiText-103 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedKD: Communication Efficient Federated Learning via Knowledge Distillation. (arXiv:2108.13323v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13323">
<div class="article-summary-box-inner">
<span><p>Federated learning is widely used to learn intelligent models from
decentralized data. In federated learning, clients need to communicate their
local model updates in each iteration of model learning. However, model updates
are large in size if the model contains numerous parameters, and there usually
needs many rounds of communication until model converges. Thus, the
communication cost in federated learning can be quite heavy. In this paper, we
propose a communication efficient federated learning method based on knowledge
distillation. Instead of directly communicating the large models between
clients and server, we propose an adaptive mutual distillation framework to
reciprocally learn a student and a teacher model on each client, where only the
student model is shared by different clients and updated collaboratively to
reduce the communication cost. Both the teacher and student on each client are
learned on its local data and the knowledge distilled from each other, where
their distillation intensities are controlled by their prediction quality. To
further reduce the communication cost, we propose a dynamic gradient
approximation method based on singular value decomposition to approximate the
exchanged gradients with dynamic precision. Extensive experiments on benchmark
datasets in different tasks show that our approach can effectively reduce the
communication cost and achieve competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13679">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01982">
<div class="article-summary-box-inner">
<span><p>Learning hierarchical structures in sequential data -- from simple
algorithmic patterns to natural language -- in a reliable, generalizable way
remains a challenging problem for neural language models. Past work has shown
that recurrent neural networks (RNNs) struggle to generalize on held-out
algorithmic or syntactic patterns without supervision or some inductive bias.
To remedy this, many papers have explored augmenting RNNs with various
differentiable stacks, by analogy with finite automata and pushdown automata
(PDAs). In this paper, we improve the performance of our recently proposed
Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure
that simulates a nondeterministic PDA, with two important changes. First, the
model now assigns unnormalized positive weights instead of probabilities to
stack actions, and we provide an analysis of why this improves training.
Second, the model can directly observe the state of the underlying PDA. Our
model achieves lower cross-entropy than all previous stack RNNs on five
context-free language modeling tasks (within 0.05 nats of the
information-theoretic lower bound), including a task on which the NS-RNN
previously failed to outperform a deterministic stack RNN baseline. Finally, we
propose a restricted version of the NS-RNN that incrementally processes
infinitely long sequences, and we present language modeling results on the Penn
Treebank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLM-K: Improving Cross-Lingual Language Model Pre-training with Multilingual Knowledge. (arXiv:2109.12573v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12573">
<div class="article-summary-box-inner">
<span><p>Cross-lingual pre-training has achieved great successes using monolingual and
bilingual plain text corpora. However, most pre-trained models neglect
multilingual knowledge, which is language agnostic but comprises abundant
cross-lingual structure alignment. In this paper, we propose XLM-K, a
cross-lingual language model incorporating multilingual knowledge in
pre-training. XLM-K augments existing multilingual pre-training with two
knowledge tasks, namely Masked Entity Prediction Task and Object Entailment
Task. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly
demonstrate significant improvements over existing multilingual language
models. The results on MLQA and NER exhibit the superiority of XLM-K in
knowledge related tasks. The success in XNLI shows a better cross-lingual
transferability obtained in XLM-K. What is more, we provide a detailed probing
analysis to confirm the desired knowledge captured in our pre-training regimen.
The code is available at
https://github.com/microsoft/Unicoder/tree/master/pretraining/xlmk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Enhanced Span-based Decomposition Method for Few-Shot Sequence Labeling. (arXiv:2109.13023v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13023">
<div class="article-summary-box-inner">
<span><p>Few-Shot Sequence Labeling (FSSL) is a canonical paradigm for the tagging
models, e.g., named entity recognition and slot filling, to generalize on an
emerging, resource-scarce domain. Recently, the metric-based meta-learning
framework has been recognized as a promising approach for FSSL. However, most
prior works assign a label to each token based on the token-level similarities,
which ignores the integrality of named entities or slots. To this end, in this
paper, we propose ESD, an Enhanced Span-based Decomposition method for FSSL.
ESD formulates FSSL as a span-level matching problem between test query and
supporting instances. Specifically, ESD decomposes the span matching problem
into a series of span-level procedures, mainly including enhanced span
representation, class prototype aggregation and span conflicts resolution.
Extensive experiments show that ESD achieves the new state-of-the-art results
on two popular FSSL benchmarks, FewNERD and SNIPS, and is proven to be more
robust in the nested and noisy tagging scenarios. Our code is available at
https://github.com/Wangpeiyi9979/ESD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Dense Retrieval for Dialogue Response Selection. (arXiv:2110.06612v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06612">
<div class="article-summary-box-inner">
<span><p>Recent progress in deep learning has continuously improved the accuracy of
dialogue response selection. In particular, sophisticated neural network
architectures are leveraged to capture the rich interactions between dialogue
context and response candidates. While remarkably effective, these models also
bring in a steep increase in computational cost. Consequently, such models can
only be used as a re-rank module in practice. In this study, we present a
solution to directly select proper responses from a large corpus or even a
nonparallel corpus that only consists of unpaired sentences, using a dense
retrieval model. To push the limits of dense retrieval, we design an
interaction layer upon the dense retrieval models and apply a set of
tailor-designed learning strategies. Our model shows superiority over strong
baselines on the conventional re-rank evaluation setting, which is remarkable
given its efficiency. To verify the effectiveness of our approach in realistic
scenarios, we also conduct full-rank evaluation, where the target is to select
proper responses from a full candidate pool that may contain millions of
candidates and evaluate them fairly through human annotations. Our proposed
model notably outperforms pipeline baselines that integrate fast recall and
expressive re-rank modules. Human evaluation results show that enlarging the
candidate pool with nonparallel corpora improves response quality further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Cross-lingual Summarization and Machine Translation with Compression Rate. (arXiv:2110.07936v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07936">
<div class="article-summary-box-inner">
<span><p>Cross-Lingual Summarization (CLS) is a task that extracts important
information from a source document and summarizes it into a summary in another
language. It is a challenging task that requires a system to understand,
summarize, and translate at the same time, making it highly related to
Monolingual Summarization (MS) and Machine Translation (MT). In practice, the
training resources for Machine Translation are far more than that for
cross-lingual and monolingual summarization. Thus incorporating the Machine
Translation corpus into CLS would be beneficial for its performance. However,
the present work only leverages a simple multi-task framework to bring Machine
Translation in, lacking deeper exploration. In this paper, we propose a novel
task, Cross-lingual Summarization with Compression rate (CSC), to benefit
Cross-Lingual Summarization by large-scale Machine Translation corpus. Through
introducing compression rate, the information ratio between the source and the
target text, we regard the MT task as a special CLS task with a compression
rate of 100%. Hence they can be trained as a unified task, sharing knowledge
more effectively. However, a huge gap exists between the MT task and the CLS
task, where samples with compression rates between 30% and 90% are extremely
rare. Hence, to bridge these two tasks smoothly, we propose an effective data
augmentation method to produce document-summary pairs with different
compression rates. The proposed method not only improves the performance of the
CLS task, but also provides controllability to generate summaries in desired
lengths. Experiments demonstrate that our method outperforms various strong
baselines in three cross-lingual summarization datasets. We released our code
and data at https://github.com/ybai-nlp/CLS_CR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">milIE: Modular & Iterative Multilingual Open Information Extraction. (arXiv:2110.08144v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08144">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OpenIE) is the task of extracting (subject,
predicate, object) triples from natural language sentences. Current OpenIE
systems extract all triple slots independently. In contrast, we explore the
hypothesis that it may be beneficial to extract triple slots iteratively: first
extract easy slots, followed by the difficult ones by conditioning on the easy
slots, and therefore achieve a better overall extraction. Based on this
hypothesis, we propose a neural OpenIE system, milIE, that operates in an
iterative fashion. Due to the iterative nature, the system is also modular --
it is possible to seamlessly integrate rule based extraction systems with a
neural end-to-end system, thereby allowing rule based systems to supply
extraction slots which milIE can leverage for extracting the remaining slots.
We confirm our hypothesis empirically: milIE outperforms SOTA systems on
multiple languages ranging from Chinese to Arabic. Additionally, we are the
first to provide an OpenIE test dataset for Arabic and Galician.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization. (arXiv:2110.08168v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08168">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have achieved state-of-the-art performance on
short-input summarization. However, they still struggle with summarizing longer
text. In this paper, we present DYLE, a novel dynamic latent extraction
approach for abstractive long-input summarization. DYLE jointly trains an
extractor and a generator and treats the extracted text snippets as the latent
variable, allowing dynamic snippet-level attention weights during decoding. To
provide adequate supervision, we propose simple yet effective heuristics for
oracle extraction as well as a consistency loss term, which encourages the
extractor to approximate the averaged dynamic weights predicted by the
generator. We evaluate our method on different long-document and long-dialogue
summarization tasks: GovReport, QMSum, and arXiv. Experiment results show that
DYLE outperforms all existing methods on GovReport and QMSum, with gains up to
6.1 ROUGE, while yielding strong results on arXiv. Further analysis shows that
the proposed dynamic weights provide interpretability of our generation
process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributionally Robust Recurrent Decoders with Random Network Distillation. (arXiv:2110.13229v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13229">
<div class="article-summary-box-inner">
<span><p>Neural machine learning models can successfully model language that is
similar to their training distribution, but they are highly susceptible to
degradation under distribution shift, which occurs in many practical
applications when processing out-of-domain (OOD) text. This has been attributed
to "shortcut learning": relying on weak correlations over arbitrary large
contexts.
</p>
<p>We propose a method based on OOD detection with Random Network Distillation
to allow an autoregressive language model to automatically disregard OOD
context during inference, smoothly transitioning towards a less expressive but
more robust model as the data becomes more OOD while retaining its full context
capability when operating in-distribution. We apply our method to a GRU
architecture, demonstrating improvements on multiple language modeling (LM)
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Explanations of Recommendations. (arXiv:2111.00670v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00670">
<div class="article-summary-box-inner">
<span><p>As recommendation is essentially a comparative (or ranking) process, a good
explanation should illustrate to users why an item is believed to be better
than another, i.e., comparative explanations about the recommended items.
Ideally, after reading the explanations, a user should reach the same ranking
of items as the system's. Unfortunately, little research attention has yet been
paid on such comparative explanations.
</p>
<p>In this work, we develop an extract-and-refine architecture to explain the
relative comparisons among a set of ranked items from a recommender system. For
each recommended item, we first extract one sentence from its associated
reviews that best suits the desired comparison against a set of reference
items. Then this extracted sentence is further articulated with respect to the
target user through a generative model to better explain why the item is
recommended. We design a new explanation quality metric based on BLEU to guide
the end-to-end training of the extraction and refinement components, which
avoids generation of generic content. Extensive offline evaluations on two
large recommendation benchmark datasets and serious user studies against an
array of state-of-the-art explainable recommendation algorithms demonstrate the
necessity of comparative explanations and the effectiveness of our solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Enactivist account of Mind Reading in Natural Language Understanding. (arXiv:2111.06179v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06179">
<div class="article-summary-box-inner">
<span><p>In this paper we apply our understanding of the radical enactivist agenda to
the classic AI-hard problem of Natural Language Understanding. When Turing
devised his famous test the assumption was that a computer could use language
and the challenge would be to mimic human intelligence. It turned out playing
chess and formal logic were easy compared to understanding what people say. The
techniques of good old-fashioned AI (GOFAI) assume symbolic representation is
the core of reasoning and by that paradigm human communication consists of
transferring representations from one mind to another. However, one finds that
representations appear in another's mind, without appearing in the intermediary
language. People communicate by mind reading it seems. Systems with speech
interfaces such as Alexa and Siri are of course common, but they are limited.
Rather than adding mind reading skills, we introduced a "cheat" that enabled
our systems to fake it. The cheat is simple and only slightly interesting to
computer scientists and not at all interesting to philosophers. However,
reading about the enactivist idea that we "directly perceive" the intentions of
others, our cheat took on a new light and in this paper look again at how
natural language understanding might actually work between humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Contextual Toxicity Detection in Conversations. (arXiv:2111.12447v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12447">
<div class="article-summary-box-inner">
<span><p>Understanding toxicity in user conversations is undoubtedly an important
problem. Addressing "covert" or implicit cases of toxicity is particularly hard
and requires context. Very few previous studies have analysed the influence of
conversational context in human perception or in automated detection models. We
dive deeper into both these directions. We start by analysing existing
contextual datasets and come to the conclusion that toxicity labelling by
humans is in general influenced by the conversational structure, polarity and
topic of the context. We then propose to bring these findings into
computational detection models by introducing and evaluating (a) neural
architectures for contextual toxicity detection that are aware of the
conversational structure, and (b) data augmentation strategies that can help
model contextual toxicity detection. Our results have shown the encouraging
potential of neural architectures that are aware of the conversation structure.
We have also demonstrated that such models can benefit from synthetic data,
especially in the social media domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Text-to-SQL Parsing through Question Decomposition. (arXiv:2112.06311v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06311">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query
relational data. Training such parsers, by contrast, generally requires
expertise in annotating natural language (NL) utterances with corresponding SQL
queries. In this work, we propose a weak supervision approach for training
text-to-SQL parsers. We take advantage of the recently proposed question
meaning representation called QDMR, an intermediate between NL and formal query
languages. Given questions, their QDMR structures (annotated by non-experts or
automatically predicted), and the answers, we are able to automatically
synthesize SQL queries that are used to train text-to-SQL models. We test our
approach by experimenting on five benchmark datasets. Our results show that the
weakly supervised models perform competitively with those trained on annotated
NL-SQL data. Overall, we effectively train text-to-SQL parsers, while using
zero SQL annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. (arXiv:2112.07577v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07577">
<div class="article-summary-box-inner">
<span><p>Dense retrieval approaches can overcome the lexical gap and lead to
significantly improved search results. However, they require large amounts of
training data which is not available for most domains. As shown in previous
work (Thakur et al., 2021b), the performance of dense retrievers severely
degrades under a domain shift. This limits the usage of dense retrieval
approaches to only a few domains with large training datasets.
</p>
<p>In this paper, we propose the novel unsupervised domain adaptation method
Generative Pseudo Labeling (GPL), which combines a query generator with pseudo
labeling from a cross-encoder. On six representative domain-specialized
datasets, we find the proposed GPL can outperform an out-of-the-box
state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL
requires less (unlabeled) data from the target domain and is more robust in its
training than previous methods.
</p>
<p>We further investigate the role of six recent pre-training methods in the
scenario of domain adaptation for retrieval tasks, where only three could yield
improved results. The best approach, TSDAE (Wang et al., 2021) can be combined
with GPL, yielding another average improvement of 1.4 points nDCG@10 across the
six tasks. The code and the models are available at
https://github.com/UKPLab/gpl.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Knowledge Graph Embeddings based Approach for Author Name Disambiguation using Literals. (arXiv:2201.09555v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09555">
<div class="article-summary-box-inner">
<span><p>Scholarly data is growing continuously containing information about the
articles from plethora of venues including conferences, journals, etc. Many
initiatives have been taken to make scholarly data available in the for of
Knowledge Graphs (KGs). These efforts to standardize these data and make them
accessible have also lead to many challenges such as exploration of scholarly
articles, ambiguous authors, etc. This study more specifically targets the
problem of Author Name Disambiguation (AND) on Scholarly KGs and presents a
novel framework, Literally Author Name Disambiguation (LAND), which utilizes
Knowledge Graph Embeddings (KGEs) using multimodal literal information
generated from these KGs. This framework is based on three components: 1)
Multimodal KGEs, 2) A blocking procedure, and finally, 3) Hierarchical
Agglomerative Clustering. Extensive experiments have been conducted on two
newly created KGs: (i) KG containing information from Scientometrics Journal
from 1978 onwards (OC-782K), and (ii) a KG extracted from a well-known
benchmark for AND provided by AMiner (AMiner-534K). The results show that our
proposed architecture outperforms our baselines of 8-14% in terms of F$_1$
score and shows competitive performances on a challenging benchmark such as
AMiner. The code and the datasets are publicly available through Github and
Zenodo respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XAlign: Cross-lingual Fact-to-Text Alignment and Generation for Low-Resource Languages. (arXiv:2202.00291v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00291">
<div class="article-summary-box-inner">
<span><p>Multiple critical scenarios (like Wikipedia text generation given English
Infoboxes) need automated generation of descriptive text in low resource (LR)
languages from English fact triples. Previous work has focused on English
fact-to-text (F2T) generation. To the best of our knowledge, there has been no
previous attempt on cross-lingual alignment or generation for LR languages.
Building an effective cross-lingual F2T (XF2T) system requires alignment
between English structured facts and LR sentences. We propose two unsupervised
methods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with
0.45M pairs across 8 languages, of which 5402 pairs have been manually
annotated. We also train strong baseline XF2T generation models on the XAlign
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Tracking Dialogue State by Inheriting Slot Values in Mentioned Slot Pools. (arXiv:2202.07156v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07156">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) is a component of the task-oriented dialogue
system. It is responsible for extracting and managing slot values according to
dialogue utterances, where each slot represents an essential part of the
information to accomplish a task, and slot value is updated recurrently in each
dialogue turn. However, many DST models cannot update slot values
appropriately. These models may repeatedly inherit wrong slot values extracted
in previous turns, resulting in the fail of the entire DST task. They cannot
update indirectly mentioned slots well, either. This study designed a model
with a mentioned slot pool (MSP) to tackle the update problem. The MSP is a
slot-specific memory that records all mentioned slot values that may be
inherited, and our model updates slot values according to the MSP and the
dialogue context. Our model rejects inheriting the previous slot value when it
predicates the value is wrong. Then, it re-extracts the slot value from the
current dialogue context. As the contextual information accumulates with the
dialogue progress, the new value is more likely to be correct. It also can
track the indirectly mentioned slot by picking a value from the MSP.
Experimental results showed our model reached state-of-the-art DST performance
on MultiWOZ 2.1 and 2.2 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval 2022 Task 12: Symlink- Linking Mathematical Symbols to their Descriptions. (arXiv:2202.09695v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09695">
<div class="article-summary-box-inner">
<span><p>Given the increasing number of livestreaming videos, automatic speech
recognition and post-processing for livestreaming video transcripts are crucial
for efficient data management as well as knowledge mining. A key step in this
process is punctuation restoration which restores fundamental text structures
such as phrase and sentence boundaries from the video transcripts. This work
presents a new human-annotated corpus, called BehancePR, for punctuation
restoration in livestreaming video transcripts. Our experiments on BehancePR
demonstrate the challenges of punctuation restoration for this domain.
Furthermore, we show that popular natural language processing toolkits are
incapable of detecting sentence boundary on non-punctuated transcripts of
livestreaming videos, calling for more research effort to develop robust models
for this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval. (arXiv:2203.03367v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03367">
<div class="article-summary-box-inner">
<span><p>Passage retrieval is a fundamental task in information retrieval (IR)
research, which has drawn much attention recently. In the English field, the
availability of large-scale annotated dataset (e.g, MS MARCO) and the emergence
of deep pre-trained language models (e.g, BERT) has resulted in a substantial
improvement of existing passage retrieval systems. However, in the Chinese
field, especially for specific domains, passage retrieval systems are still
immature due to quality-annotated dataset being limited by scale. Therefore, in
this paper, we present a novel multi-domain Chinese dataset for passage
retrieval (Multi-CPR). The dataset is collected from three different domains,
including E-commerce, Entertainment video and Medical. Each dataset contains
millions of passages and a certain amount of human annotated query-passage
related pairs. We implement various representative passage retrieval methods as
baselines. We find that the performance of retrieval models trained on dataset
from general domain will inevitably decrease on specific domain. Nevertheless,
a passage retrieval system built on in-domain annotated dataset can achieve
significant improvement, which indeed demonstrates the necessity of domain
labeled data for further optimization. We hope the release of the Multi-CPR
dataset could benchmark Chinese passage retrieval task in specific domain and
also make advances for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Does the Performance Improvement Come From? -- A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03853">
<div class="article-summary-box-inner">
<span><p>This article aims to provide the information retrieval community with some
reflections on recent advances in retrieval learning by analyzing the
reproducibility of image-text retrieval models. Due to the increase of
multimodal data over the last decade, image-text retrieval has steadily become
a major research direction in the field of information retrieval. Numerous
researchers train and evaluate image-text retrieval algorithms using benchmark
datasets such as MS-COCO and Flickr30k. Research in the past has mostly focused
on performance, with multiple state-of-the-art methodologies being suggested in
a variety of ways. According to their assertions, these techniques provide
improved modality interactions and hence more precise multimodal
representations. In contrast to previous works, we focus on the reproducibility
of the approaches and the examination of the elements that lead to improved
performance by pretrained and nonpretrained models in retrieving images and
text. To be more specific, we first examine the related reproducibility
concerns and explain why our focus is on image-text retrieval tasks. Second, we
systematically summarize the current paradigm of image-text retrieval models
and the stated contributions of those approaches. Third, we analyze various
aspects of the reproduction of pretrained and nonpretrained retrieval models.
To complete this, we conducted ablation experiments and obtained some
influencing factors that affect retrieval recall more than the improvement
claimed in the original paper. Finally, we present some reflections and
challenges that the retrieval community should consider in the future. Our
source code is publicly available at
https://github.com/WangFei-2019/Image-text-Retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Dependency Tree Into Self-attention for Sentence Representation. (arXiv:2203.05918v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05918">
<div class="article-summary-box-inner">
<span><p>Recent progress on parse tree encoder for sentence representation learning is
notable. However, these works mainly encode tree structures recursively, which
is not conducive to parallelization. On the other hand, these works rarely take
into account the labels of arcs in dependency trees. To address both issues, we
propose Dependency-Transformer, which applies a relation-attention mechanism
that works in concert with the self-attention mechanism. This mechanism aims to
encode the dependency and the spatial positional relations between nodes in the
dependency tree of sentences. By a score-based method, we successfully inject
the syntax information without affecting Transformer's parallelizability. Our
model outperforms or is comparable to the state-of-the-art methods on four
tasks for sentence representation and has obvious advantages in computational
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIOS: An Algorithmically Generated Biomedical Knowledge Graph. (arXiv:2203.09975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09975">
<div class="article-summary-box-inner">
<span><p>Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for
biomedical and healthcare big data and artificial intelligence (AI),
facilitating natural language processing, model development, and data exchange.
For decades, these knowledge graphs have been developed via expert curation;
however, this method can no longer keep up with today's AI development, and a
transition to algorithmically generated BioMedKGs is necessary. In this work,
we introduce the Biomedical Informatics Ontology System (BIOS), the first
large-scale publicly available BioMedKG generated completely by machine
learning algorithms. BIOS currently contains 4.1 million concepts, 7.4 million
terms in two languages, and 7.3 million relation triplets. We present the
methodology for developing BIOS, including the curation of raw biomedical
terms, computational identification of synonymous terms and aggregation of
these terms to create concept nodes, semantic type classification of the
concepts, relation identification, and biomedical machine translation. We
provide statistics on the current BIOS content and perform preliminary
assessments of term quality, synonym grouping, and relation extraction. The
results suggest that machine learning-based BioMedKG development is a viable
alternative to traditional expert curation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. (arXiv:2203.10316v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10316">
<div class="article-summary-box-inner">
<span><p>Solving math word problems requires deductive reasoning over the quantities
in the text. Various recent research efforts mostly relied on
sequence-to-sequence or sequence-to-tree models to generate mathematical
expressions without explicitly performing relational reasoning between
quantities in the given context. While empirically effective, such approaches
typically do not provide explanations for the generated expressions. In this
work, we view the task as a complex relation extraction problem, proposing a
novel approach that presents explainable deductive reasoning steps to
iteratively construct target expressions, where each step involves a primitive
operation over two quantities defining their relation. Through extensive
experiments on four benchmark datasets, we show that the proposed model
significantly outperforms existing strong baselines. We further demonstrate
that the deductive procedure not only presents more explainable steps but also
enables us to make more accurate predictions on questions that require more
complex reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16776">
<div class="article-summary-box-inner">
<span><p>Utilizing text-only data with an external language model (LM) in end-to-end
RNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class
of methods such as density ratio (DR) and ILM estimation (ILME) have been
developed, outperforming the classic shallow fusion (SF) method. The basic idea
behind these methods is that RNN-T posterior should first subtract the
implicitly learned ILM prior, in order to integrate the external LM. While
recent studies suggest that RNN-T only learns some low-order language model
information, the DR method uses a well-trained ILM. We hypothesize that this
setting is appropriate and may deteriorate the performance of the DR method,
and propose a low-order density ratio method (LODR) by training a low-order
weak ILM for DR. Extensive empirical experiments are conducted on both
in-domain and cross-domain scenarios on English LibriSpeech &amp; Tedlium-2 and
Chinese WenetSpeech &amp; AISHELL-1 datasets. It is shown that LODR consistently
outperforms SF in all tasks, while performing generally close to ILME and
better than DR in most tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03162">
<div class="article-summary-box-inner">
<span><p>We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated speech tools for helping communities process restricted-access corpora for language revival efforts. (arXiv:2204.07272v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07272">
<div class="article-summary-box-inner">
<span><p>Many archival recordings of speech from endangered languages remain
unannotated and inaccessible to community members and language learning
programs. One bottleneck is the time-intensive nature of annotation. An even
narrower bottleneck occurs for recordings with access constraints, such as
language that must be vetted or filtered by authorised community members before
annotation can begin. We propose a privacy-preserving workflow to widen both
bottlenecks for recordings where speech in the endangered language is
intermixed with a more widely-used language such as English for meta-linguistic
commentary and questions (e.g. What is the word for 'tree'?). We integrate
voice activity detection (VAD), spoken language identification (SLI), and
automatic speech recognition (ASR) to transcribe the metalinguistic content,
which an authorised person can quickly scan to triage recordings that can be
annotated by people with lower levels of access. We report work-in-progress
processing 136 hours archival audio containing a mix of English and Muruwari.
Our collaborative work with the Muruwari custodian of the archival materials
show that this workflow reduces metalanguage transcription time by 20% even
given only minimal amounts of annotated training data: 10 utterances per
language for SLI and for ASR at most 39 minutes, and possibly as little as 39
seconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarization with Graphical Elements. (arXiv:2204.07551v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07551">
<div class="article-summary-box-inner">
<span><p>Automatic text summarization has experienced substantial progress in recent
years. With this progress, the question has arisen whether the types of
summaries that are typically generated by automatic summarization models align
with users' needs. Ter Hoeve et al (2020) answer this question negatively.
Amongst others, they recommend focusing on generating summaries with more
graphical elements. This is in line with what we know from the
psycholinguistics literature about how humans process text. Motivated from
these two angles, we propose a new task: summarization with graphical elements,
and we verify that these summaries are helpful for a critical mass of people.
We collect a high quality human labeled dataset to support research into the
task. We present a number of baseline methods that show that the task is
interesting and challenging. Hence, with this work we hope to inspire a new
line of research within the automatic summarization community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Hard Negative Entities for Entity Set Expansion. (arXiv:2204.07789v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07789">
<div class="article-summary-box-inner">
<span><p>Entity Set Expansion (ESE) is a promising task which aims to expand entities
of the target semantic class described by a small seed entity set. Various NLP
and IR applications will benefit from ESE due to its ability to discover
knowledge. Although previous ESE methods have achieved great progress, most of
them still lack the ability to handle hard negative entities (i.e., entities
that are difficult to distinguish from the target entities), since two entities
may or may not belong to the same semantic class based on different granularity
levels we analyze on. To address this challenge, we devise an entity-level
masked language model with contrastive learning to refine the representation of
entities. In addition, we propose the ProbExpan, a novel probabilistic ESE
framework utilizing the entity representation obtained by the aforementioned
language model to expand entities. Extensive experiments and detailed analyses
on three datasets show that our method outperforms previous state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logical Inference for Counting on Semi-structured Tables. (arXiv:2204.07803v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07803">
<div class="article-summary-box-inner">
<span><p>Recently, the Natural Language Inference (NLI) task has been studied for
semi-structured tables that do not have a strict format. Although neural
approaches have achieved high performance in various types of NLI, including
NLI between semi-structured tables and texts, they still have difficulty in
performing a numerical type of inference, such as counting. To handle a
numerical type of inference, we propose a logical inference system for
reasoning between semi-structured tables and texts. We use logical
representations as meaning representations for tables and texts and use model
checking to handle a numerical type of inference between texts and tables. To
evaluate the extent to which our system can perform inference with numerical
comparatives, we make an evaluation protocol that focuses on numerical
understanding between semi-structured tables and texts in English. We show that
our system can more robustly perform inference between tables and texts that
requires numerical understanding compared with current neural approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLISS: Robust Sequence-to-Sequence Learning via Self-Supervised Input Representation. (arXiv:2204.07837v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07837">
<div class="article-summary-box-inner">
<span><p>Data augmentations (DA) are the cores to achieving robust
sequence-to-sequence learning on various natural language processing (NLP)
tasks. However, most of the DA approaches force the decoder to make predictions
conditioned on the perturbed input representation, underutilizing supervised
information provided by perturbed input. In this work, we propose a
framework-level robust sequence-to-sequence learning approach, named BLISS, via
self-supervised input representation, which has the great potential to
complement the data-level augmentation approaches. The key idea is to supervise
the sequence-to-sequence framework with both the \textit{supervised}
("input$\rightarrow$output") and \textit{self-supervised} ("perturbed
input$\rightarrow$input") information. We conduct comprehensive experiments to
validate the effectiveness of BLISS on various tasks, including machine
translation, grammatical error correction, and text summarization. The results
show that BLISS outperforms significantly the vanilla Transformer and
consistently works well across tasks than the other five contrastive baselines.
Extensive analyses reveal that BLISS learns robust representations and rich
linguistic knowledge, confirming our claim. Source code will be released upon
publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09667">
<div class="article-summary-box-inner">
<span><p>Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Summary of the ALQAC 2021 Competition. (arXiv:2204.10717v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10717">
<div class="article-summary-box-inner">
<span><p>We summarize the evaluation of the first Automated Legal Question Answering
Competition (ALQAC 2021). The competition this year contains three tasks, which
aims at processing the statute law document, which are Legal Text Information
Retrieval (Task 1), Legal Text Entailment Prediction (Task 2), and Legal Text
Question Answering (Task 3). The final goal of these tasks is to build a system
that can automatically determine whether a particular statement is lawful.
There is no limit to the approaches of the participating teams. This year,
there are 5 teams participating in Task 1, 6 teams participating in Task 2, and
5 teams participating in Task 3. There are in total 36 runs submitted to the
organizer. In this paper, we summarize each team's approaches, official
results, and some discussion about the competition. Only results of the teams
who successfully submit their approach description paper are reported in this
paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TOUR: Dynamic Topic and Sentiment Analysis of User Reviews for Assisting App Release. (arXiv:2103.15774v2 [cs.SE] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15774">
<div class="article-summary-box-inner">
<span><p>App reviews deliver user opinions and emerging issues (e.g., new bugs) about
the app releases. Due to the dynamic nature of app reviews, topics and
sentiment of the reviews would change along with app release versions. Although
several studies have focused on summarizing user opinions by analyzing user
sentiment towards app features, no practical tool is released. The large
quantity of reviews and noise words also necessitates an automated tool for
monitoring user reviews. In this paper, we introduce TOUR for dynamic TOpic and
sentiment analysis of User Reviews. TOUR is able to (i) detect and summarize
emerging app issues over app versions, (ii) identify user sentiment towards app
features, and (iii) prioritize important user reviews for facilitating
developers' examination. The core techniques of TOUR include the online topic
modeling approach and sentiment prediction strategy. TOUR provides entries for
developers to customize the hyper-parameters and the results are presented in
an interactive way. We evaluate TOUR by conducting a developer survey that
involves 15 developers, and all of them confirm the practical usefulness of the
recommended feature changes by TOUR.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Identity Preserving Loss for Learned Image Compression. (arXiv:2204.10869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10869">
<div class="article-summary-box-inner">
<span><p>Deep learning model inference on embedded devices is challenging due to the
limited availability of computation resources. A popular alternative is to
perform model inference on the cloud, which requires transmitting images from
the embedded device to the cloud. Image compression techniques are commonly
employed in such cloud-based architectures to reduce transmission latency over
low bandwidth networks. This work proposes an end-to-end image compression
framework that learns domain-specific features to achieve higher compression
ratios than standard HEVC/JPEG compression techniques while maintaining
accuracy on downstream tasks (e.g., recognition). Our framework does not
require fine-tuning of the downstream task, which allows us to drop-in any
off-the-shelf downstream task model without retraining. We choose faces as an
application domain due to the ready availability of datasets and off-the-shelf
recognition models as representative downstream tasks. We present a novel
Identity Preserving Reconstruction (IPR) loss function which achieves
Bits-Per-Pixel (BPP) values that are ~38% and ~42% of CRF-23 HEVC compression
for LFW (low-resolution) and CelebA-HQ (high-resolution) datasets,
respectively, while maintaining parity in recognition accuracy. The superior
compression ratio is achieved as the model learns to retain the domain-specific
features (e.g., facial features) while sacrificing details in the background.
Furthermore, images reconstructed by our proposed compression model are robust
to changes in downstream model architectures. We show at-par recognition
performance on the LFW dataset with an unseen recognition model while retaining
a lower BPP value of ~38% of CRF-23 HEVC compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative sampling in tractography using autoencoders (GESTA). (arXiv:2204.10891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10891">
<div class="article-summary-box-inner">
<span><p>Current tractography methods use the local orientation information to
propagate streamlines from seed locations. Many such seeds provide streamlines
that stop prematurely or fail to map the true pathways because some white
matter bundles are "harder-to-track" than others. This results in tractography
reconstructions with poor white and gray matter spatial coverage. In this work,
we propose a generative, autoencoder-based method, named GESTA (Generative
Sampling in Tractography using Autoencoders), that produces streamlines with
better spatial coverage. Compared to other deep learning methods, our
autoencoder-based framework is not constrained by any prior or a fixed set of
bundles. GESTA produces new and complete streamlines for any white matter
bundle. GESTA is shown to be effective on both synthetic and human brain in
vivo data. Our streamline evaluation framework ensures that the streamlines
produced by GESTA are anatomically plausible and fit well to the local
diffusion signal. The streamline evaluation criteria assess anatomy (white
matter coverage), local orientation alignment (direction), geometry features of
streamlines, and optionally, gray matter connectivity. The GESTA framework
offers considerable gains in bundle coverage using a reduced set of seeding
streamlines with a 1.5x improvement for the "Fiber Cup", and 6x for the ISMRM
2015 Tractography Challenge datasets. Similarly, it provides a 4x white matter
volume increase on the BIL&amp;GIN callosal homotopic dataset. It also successfully
generates new streamlines in poorly populated bundles, such as the fornix and
other hard-to-track bundles, on in vivo data. GESTA is thus the first deep
tractography generative method that can improve white matter reconstruction of
hard-to-track bundles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning from Synthetic In-vitro Soybean Pods Dataset for In-situ Segmentation of On-branch Soybean Pod. (arXiv:2204.10902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10902">
<div class="article-summary-box-inner">
<span><p>The mature soybean plants are of complex architecture with pods frequently
touching each other, posing a challenge for in-situ segmentation of on-branch
soybean pods. Deep learning-based methods can achieve accurate training and
strong generalization capabilities, but it demands massive labeled data, which
is often a limitation, especially for agricultural applications. As lacking the
labeled data to train an in-situ segmentation model for on-branch soybean pods,
we propose a transfer learning from synthetic in-vitro soybean pods. First, we
present a novel automated image generation method to rapidly generate a
synthetic in-vitro soybean pods dataset with plenty of annotated samples. The
in-vitro soybean pods samples are overlapped to simulate the frequently
physically touching of on-branch soybean pods. Then, we design a two-step
transfer learning. In the first step, we finetune an instance segmentation
network pretrained by a source domain (MS COCO dataset) with a synthetic target
domain (in-vitro soybean pods dataset). In the second step, transferring from
simulation to reality is performed by finetuning on a few real-world mature
soybean plant samples. The experimental results show the effectiveness of the
proposed two-step transfer learning method, such that AP$_{50}$ was 0.80 for
the real-world mature soybean plant test dataset, which is higher than that of
direct adaptation and its AP$_{50}$ was 0.77. Furthermore, the visualizations
of in-situ segmentation results of on-branch soybean pods show that our method
performs better than other methods, especially when soybean pods overlap
densely.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label a Herd in Minutes: Individual Holstein-Friesian Cattle Identification. (arXiv:2204.10905v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10905">
<div class="article-summary-box-inner">
<span><p>We describe a practically evaluated approach for training visual cattle ID
systems for a whole farm requiring only ten minutes of labelling effort. In
particular, for the task of automatic identification of individual
Holstein-Friesians in real-world farm CCTV, we show that self-supervision,
metric learning, cluster analysis, and active learning can complement each
other to significantly reduce the annotation requirements usually needed to
train cattle identification frameworks. Evaluating the approach on the test
portion of the publicly available Cows2021 dataset, for training we use 23,350
frames across 435 single individual tracklets generated by automated oriented
cattle detection and tracking in operational farm footage. Self-supervised
metric learning is first employed to initialise a candidate identity space
where each tracklet is considered a distinct entity. Grouping entities into
equivalence classes representing cattle identities is then performed by
automated merging via cluster analysis and active learning. Critically, we
identify the inflection point at which automated choices cannot replicate
improvements based on human intervention to reduce annotation to a minimum.
Experimental results show that cluster analysis and a few minutes of labelling
after automated self-supervision can improve the test identification accuracy
of 153 identities to 92.44% (ARI=0.93) from the 74.9% (ARI=0.754) obtained by
self-supervision only. These promising results indicate that a tailored
combination of human and machine reasoning in visual cattle ID pipelines can be
highly effective whilst requiring only minimal labelling effort. We provide all
key source code and network weights with this paper for easy result
reproduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing Occlusions with 4D Neural Fields. (arXiv:2204.10916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10916">
<div class="article-summary-box-inner">
<span><p>For computer vision systems to operate in dynamic situations, they need to be
able to represent and reason about object permanence. We introduce a framework
for learning to estimate 4D visual representations from monocular RGB-D, which
is able to persist objects, even once they become obstructed by occlusions.
Unlike traditional video representations, we encode point clouds into a
continuous representation, which permits the model to attend across the
spatiotemporal context to resolve occlusions. On two large video datasets that
we release along with this paper, our experiments show that the representation
is able to successfully reveal occlusions for several tasks, without any
architectural changes. Visualizations show that the attention mechanism
automatically learns to follow occluded objects. Since our approach can be
trained end-to-end and is easily adaptable, we believe it will be useful for
handling occlusions in many video understanding tasks. Data, code, and models
are available at https://occlusions.cs.columbia.edu/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegDiscover: Visual Concept Discovery via Unsupervised Semantic Segmentation. (arXiv:2204.10926v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10926">
<div class="article-summary-box-inner">
<span><p>Visual concept discovery has long been deemed important to improve
interpretability of neural networks, because a bank of semantically meaningful
concepts would provide us with a starting point for building machine learning
models that exhibit intelligible reasoning process. Previous methods have
disadvantages: either they rely on labelled support sets that incorporate human
biases for objects that are "useful," or they fail to identify multiple
concepts that occur within a single image. We reframe the concept discovery
task as an unsupervised semantic segmentation problem, and present SegDiscover,
a novel framework that discovers semantically meaningful visual concepts from
imagery datasets with complex scenes without supervision. Our method contains
three important pieces: generating concept primitives from raw images,
discovering concepts by clustering in the latent space of a self-supervised
pretrained encoder, and concept refinement via neural network smoothing.
Experimental results provide evidence that our method can discover multiple
concepts within a single image and outperforms state-of-the-art unsupervised
methods on complex datasets such as Cityscapes and COCO-Stuff. Our method can
be further used as a neural network explanation tool by comparing results
obtained by different encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-level Alignment Training Scheme for Video-and-Language Grounding. (arXiv:2204.10938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10938">
<div class="article-summary-box-inner">
<span><p>To solve video-and-language grounding tasks, the key is for the network to
understand the connection between the two modalities. For a pair of video and
language description, their semantic relation is reflected by their encodings'
similarity. A good multi-modality encoder should be able to well capture both
inputs' semantics and encode them in the shared feature space where embedding
distance gets properly translated into their semantic similarity. In this work,
we focused on this semantic connection between video and language, and
developed a multi-level alignment training scheme to directly shape the
encoding process. Global and segment levels of video-language alignment pairs
were designed, based on the information similarity ranging from high-level
context to fine-grained semantics. The contrastive loss was used to contrast
the encodings' similarities between the positive and negative alignment pairs,
and to ensure the network is trained in such a way that similar information is
encoded closely in the shared feature space while information of different
semantics is kept apart. Our multi-level alignment training can be applied to
various video-and-language grounding tasks. Together with the task-specific
training loss, our framework achieved comparable performance to previous
state-of-the-arts on multiple video QA and retrieval datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10939">
<div class="article-summary-box-inner">
<span><p>Document intelligence automates the extraction of information from documents
and supports many business applications. Recent self-supervised learning
methods on large-scale unlabeled document datasets have opened up promising
directions towards reducing annotation efforts by training models with
self-supervised objectives. However, most of the existing document pretraining
methods are still language-dominated. We present UDoc, a new unified
pretraining framework for document understanding. UDoc is designed to support
most document understanding tasks, extending the Transformer to take multimodal
embeddings as input. Each input element is composed of words and visual
features from a semantic region of the input document image. An important
feature of UDoc is that it learns a generic representation by making use of
three self-supervised losses, encouraging the representation to model
sentences, learn similarities, and align modalities. Extensive empirical
analysis demonstrates that the pretraining procedure learns better joint
representations and leads to improvements in downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid Cancer Classification. (arXiv:2204.10942v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10942">
<div class="article-summary-box-inner">
<span><p>Thyroid cancer is currently the fifth most common malignancy diagnosed in
women. Since differentiation of cancer sub-types is important for treatment and
current, manual methods are time consuming and subjective, automatic
computer-aided differentiation of cancer types is crucial. Manual
differentiation of thyroid cancer is based on tissue sections, analysed by
pathologists using histological features. Due to the enormous size of gigapixel
whole slide images, holistic classification using deep learning methods is not
feasible. Patch based multiple instance learning approaches, combined with
aggregations such as bag-of-words, is a common approach. This work's
contribution is to extend a patch based state-of-the-art method by generating
and combining feature vectors of three different patch resolutions and
analysing three distinct ways of combining them. The results showed
improvements in one of the three multi-scale approaches, while the others led
to decreased scores. This provides motivation for analysis and discussion of
the individual approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRPlanes: High Resolution Airplane Dataset for Deep Learning. (arXiv:2204.10959v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10959">
<div class="article-summary-box-inner">
<span><p>Airplane detection from satellite imagery is a challenging task due to the
complex backgrounds in the images and differences in data acquisition
conditions caused by the sensor geometry and atmospheric effects. Deep learning
methods provide reliable and accurate solutions for automatic detection of
airplanes; however, huge amount of training data is required to obtain
promising results. In this study, we create a novel airplane detection dataset
called High Resolution Planes (HRPlanes) by using images from Google Earth (GE)
and labeling the bounding box of each plane on the images. HRPlanes include GE
images of several different airports across the world to represent a variety of
landscape, seasonal and satellite geometry conditions obtained from different
satellites. We evaluated our dataset with two widely used object detection
methods namely YOLOv4 and Faster R-CNN. Our preliminary results show that the
proposed dataset can be a valuable data source and benchmark data set for
future applications. Moreover, proposed architectures and results of this study
could be used for transfer learning of different datasets and models for
airplane detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10962">
<div class="article-summary-box-inner">
<span><p>Visual attention helps achieve robust perception under noise, corruption, and
distribution shifts in human vision, which are areas where modern neural
networks still fall short. We present VARS, Visual Attention from Recurrent
Sparse reconstruction, a new attention formulation built on two prominent
features of the human visual attention mechanism: recurrency and sparsity.
Related features are grouped together via recurrent connections between
neurons, with salient objects emerging via sparse regularization. VARS adopts
an attractor network with recurrent connections that converges toward a stable
pattern over time. Network layers are represented as ordinary differential
equations (ODEs), formulating attention as a recurrent attractor network that
equivalently optimizes the sparse reconstruction of input using a dictionary of
"templates" encoding underlying patterns of data. We show that self-attention
is a special case of VARS with a single-step optimization and no sparsity
constraint. VARS can be readily used as a replacement for self-attention in
popular vision transformers, consistently improving their robustness across
various benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks. (arXiv:2204.10965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10965">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose CLIP-Dissect, a new technique to automatically
describe the function of individual hidden neurons inside vision networks.
CLIP-Dissect leverages recent advances in multimodal vision/language models to
label internal neurons with open-ended concepts without the need for any
labeled data or human examples, which are required for existing tools to
succeed. We show that CLIP-Dissect provides more accurate descriptions than
existing methods for neurons where the ground-truth is available as well as
qualitatively good descriptions for hidden layer neurons. In addition, our
method is very flexible: it is model agnostic, can easily handle new concepts
and can be extended to take advantage of better multimodal models in the
future. Finally CLIP-Dissect is computationally efficient and labels all
neurons of a layer in a large vision model in tens of minutes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Restoration of Weather-affected Images using Deep Gaussian Process-based CycleGAN. (arXiv:2204.10970v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10970">
<div class="article-summary-box-inner">
<span><p>Existing approaches for restoring weather-degraded images follow a
fully-supervised paradigm and they require paired data for training. However,
collecting paired data for weather degradations is extremely challenging, and
existing methods end up training on synthetic data. To overcome this issue, we
describe an approach for supervising deep networks that are based on CycleGAN,
thereby enabling the use of unlabeled real-world data for training.
Specifically, we introduce new losses for training CycleGAN that lead to more
effective training, resulting in high-quality reconstructions. These new losses
are obtained by jointly modeling the latent space embeddings of predicted clean
images and original clean images through Deep Gaussian Processes. This enables
the CycleGAN architecture to transfer the knowledge from one domain
(weather-degraded) to another (clean) more effectively. We demonstrate that the
proposed method can be effectively applied to different restoration tasks like
de-raining, de-hazing and de-snowing and it outperforms other unsupervised
techniques (that leverage weather-based characteristics) by a considerable
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRM: Gradient Rectification Module for Visual Place Retrieval. (arXiv:2204.10972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10972">
<div class="article-summary-box-inner">
<span><p>Visual place retrieval aims to search images in the database that depict
similar places as the query image. However, global descriptors encoded by the
network usually fall into a low dimensional principal space, which is harmful
to the retrieval performance. We first analyze the cause of this phenomenon,
pointing out that it is due to degraded distribution of the gradients of
descriptors. Then, a new module called Gradient Rectification Module(GRM) is
proposed to alleviate this issue. It can be appended after the final pooling
layer. This module can rectify the gradients to the complement space of the
principal space. Therefore, the network is encouraged to generate descriptors
more uniformly in the whole space. At last, we conduct experiments on multiple
datasets and generalize our method to classification task under prototype
learning framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Recolored Image by Spatial Correlation. (arXiv:2204.10973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10973">
<div class="article-summary-box-inner">
<span><p>Image forensics, aiming to ensure the authenticity of the image, has made
great progress in dealing with common image manipulation such as copy-move,
splicing, and inpainting in the past decades. However, only a few researchers
pay attention to an emerging editing technique called image recoloring, which
can manipulate the color values of an image to give it a new style. To prevent
it from being used maliciously, the previous approaches address the
conventional recoloring from the perspective of inter-channel correlation and
illumination consistency. In this paper, we try to explore a solution from the
perspective of the spatial correlation, which exhibits the generic detection
capability for both conventional and deep learning-based recoloring. Through
theoretical and numerical analysis, we find that the recoloring operation will
inevitably destroy the spatial correlation between pixels, implying a new prior
of statistical discriminability. Based on such fact, we generate a set of
spatial correlation features and learn the informative representation from the
set via a convolutional neural network. To train our network, we use three
recoloring methods to generate a large-scale and high-quality data set.
Extensive experimental results in two recoloring scenes demonstrate that the
spatial correlation features are highly discriminative. Our method achieves the
state-of-the-art detection accuracy on multiple benchmark datasets and exhibits
well generalization for unknown types of recoloring methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Contrastive Learning for Volumetric Medical Image Segmentation. (arXiv:2204.10983v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10983">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning needs a large amount of labeled data to achieve high
performance. However, in medical imaging analysis, each site may only have a
limited amount of data and labels, which makes learning ineffective. Federated
learning (FL) can help in this regard by learning a shared model while keeping
training data local for privacy. Traditional FL requires fully-labeled data for
training, which is inconvenient or sometimes infeasible to obtain due to high
labeling cost and the requirement of expertise. Contrastive learning (CL), as a
self-supervised learning approach, can effectively learn from unlabeled data to
pre-train a neural network encoder, followed by fine-tuning for downstream
tasks with limited annotations. However, when adopting CL in FL, the limited
data diversity on each client makes federated contrastive learning (FCL)
ineffective. In this work, we propose an FCL framework for volumetric medical
image segmentation with limited annotations. More specifically, we exchange the
features in the FCL pre-training process such that diverse contrastive data are
provided to each site for effective local CL while keeping raw data private.
Based on the exchanged features, global structural matching further leverages
the structural similarity to align local features to the remote ones such that
a unified feature space can be learned among different sites. Experiments on a
cardiac MRI dataset show the proposed framework substantially improves the
segmentation performance compared with state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TerrainMesh: Metric-Semantic Terrain Reconstruction from Aerial Images Using Joint 2D-3D Learning. (arXiv:2204.10993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10993">
<div class="article-summary-box-inner">
<span><p>This paper considers outdoor terrain mapping using RGB images obtained from
an aerial vehicle. While feature-based localization and mapping techniques
deliver real-time vehicle odometry and sparse keypoint depth reconstruction, a
dense model of the environment geometry and semantics (vegetation, buildings,
etc.) is usually recovered offline with significant computation and storage.
This paper develops a joint 2D-3D learning approach to reconstruct a local
metric-semantic mesh at each camera keyframe maintained by a visual odometry
algorithm. Given the estimated camera trajectory, the local meshes can be
assembled into a global environment model to capture the terrain topology and
semantics during online operation. A local mesh is reconstructed using an
initialization and refinement stage. In the initialization stage, we estimate
the mesh vertex elevation by solving a least squares problem relating the
vertex barycentric coordinates to the sparse keypoint depth measurements. In
the refinement stage, we associate 2D image and semantic features with the 3D
mesh vertices using camera projection and apply graph convolution to refine the
mesh vertex spatial coordinates and semantic features based on joint 2D and 3D
supervision. Quantitative and qualitative evaluation using real aerial images
show the potential of our method to support environmental monitoring and
surveillance applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks. (arXiv:2204.10997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10997">
<div class="article-summary-box-inner">
<span><p>Early diagnosis and intervention are clinically considered the paramount part
of treating cerebral palsy (CP), so it is essential to design an efficient and
interpretable automatic prediction system for CP. We highlight a significant
difference between CP infants' frequency of human movement and that of the
healthy group, which improves prediction performance. However, the existing
deep learning-based methods did not use the frequency information of infants'
movement for CP prediction. This paper proposes a frequency attention informed
graph convolutional network and validates it on two consumer-grade RGB video
datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency
attention module aids in improving both classification performance and system
interpretability. In addition, we design a frequency-binning method that
retains the critical frequency of the human joint position data while filtering
the noise. Our prediction performance achieves state-of-the-art research on
both datasets. Our work demonstrates the effectiveness of frequency information
in supporting the prediction of CP non-intrusively and provides a way for
supporting the early diagnosis of CP in the resource-limited regions where the
clinical resources are not abundant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training and challenging models for text-guided fashion image retrieval. (arXiv:2204.11004v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11004">
<div class="article-summary-box-inner">
<span><p>Retrieving relevant images from a catalog based on a query image together
with a modifying caption is a challenging multimodal task that can particularly
benefit domains like apparel shopping, where fine details and subtle variations
may be best expressed through natural language. We introduce a new evaluation
dataset, Challenging Fashion Queries (CFQ), as well as a modeling approach that
achieves state-of-the-art performance on the existing Fashion IQ (FIQ) dataset.
CFQ complements existing benchmarks by including relative captions with
positive and negative labels of caption accuracy and conditional image
similarity, where others provided only positive labels with a combined meaning.
We demonstrate the importance of multimodal pretraining for the task and show
that domain-specific weak supervision based on attribute labels can augment
generic large-scale pretraining. While previous modality fusion mechanisms lose
the benefits of multimodal pretraining, we introduce a residual attention
fusion mechanism that improves performance. We release CFQ and our code to the
research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Reconstruction from Point Clouds by Learning Predictive Context Priors. (arXiv:2204.11015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11015">
<div class="article-summary-box-inner">
<span><p>Surface reconstruction from point clouds is vital for 3D computer vision.
State-of-the-art methods leverage large datasets to first learn local context
priors that are represented as neural network-based signed distance functions
(SDFs) with some parameters encoding the local contexts. To reconstruct a
surface at a specific query location at inference time, these methods then
match the local reconstruction target by searching for the best match in the
local prior space (by optimizing the parameters encoding the local context) at
the given query location. However, this requires the local context prior to
generalize to a wide variety of unseen target regions, which is hard to
achieve. To resolve this issue, we introduce Predictive Context Priors by
learning Predictive Queries for each specific point cloud at inference time.
Specifically, we first train a local context prior using a large point cloud
dataset similar to previous techniques. For surface reconstruction at inference
time, however, we specialize the local context prior into our Predictive
Context Prior by learning Predictive Queries, which predict adjusted spatial
query locations as displacements of the original locations. This leads to a
global SDF that fits the specific point cloud the best. Intuitively, the query
prediction enables us to flexibly search the learned local context prior over
the entire prior space, rather than being restricted to the fixed query
locations, and this improves the generalizability. Our method does not require
ground truth signed distances, normals, or any additional procedure of signed
distance fusion across overlapping regions. Our experimental results in surface
reconstruction for single shapes or complex scenes show significant
improvements over the state-of-the-art under widely used benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Negatives in Contrastive Learning for Unpaired Image-to-Image Translation. (arXiv:2204.11018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11018">
<div class="article-summary-box-inner">
<span><p>Unpaired image-to-image translation aims to find a mapping between the source
domain and the target domain. To alleviate the problem of the lack of
supervised labels for the source images, cycle-consistency based methods have
been proposed for image structure preservation by assuming a reversible
relationship between unpaired images. However, this assumption only uses
limited correspondence between image pairs. Recently, contrastive learning (CL)
has been used to further investigate the image correspondence in unpaired image
translation by using patch-based positive/negative learning. Patch-based
contrastive routines obtain the positives by self-similarity computation and
recognize the rest patches as negatives. This flexible learning paradigm
obtains auxiliary contextualized information at a low cost. As the negatives
own an impressive sample number, with curiosity, we make an investigation based
on a question: are all negatives necessary for feature contrastive learning?
Unlike previous CL approaches that use negatives as much as possible, in this
paper, we study the negatives from an information-theoretic perspective and
introduce a new negative Pruning technology for Unpaired image-to-image
Translation (PUT) by sparsifying and ranking the patches. The proposed
algorithm is efficient, flexible and enables the model to learn essential
information between corresponding patches stably. By putting quality over
quantity, only a few negative patches are required to achieve better results.
Lastly, we validate the superiority, stability, and versatility of our model
through comparative experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indoor simultaneous localization and mapping based on fringe projection profilometry. (arXiv:2204.11020v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11020">
<div class="article-summary-box-inner">
<span><p>Simultaneous Localization and Mapping (SLAM) plays an important role in
outdoor and indoor applications ranging from autonomous driving to indoor
robotics. Outdoor SLAM has been widely used with the assistance of LiDAR or
GPS. For indoor applications, the LiDAR technique does not satisfy the accuracy
requirement and the GPS signals will be lost. An accurate and efficient scene
sensing technique is required for indoor SLAM. As the most promising 3D sensing
technique, the opportunities for indoor SLAM with fringe projection
profilometry (FPP) systems are obvious, but methods to date have not fully
leveraged the accuracy and speed of sensing that such systems offer. In this
paper, we propose a novel FPP-based indoor SLAM method based on the coordinate
transformation relationship of FPP, where the 2D-to-3D descriptor-assisted is
used for mapping and localization. The correspondences generated by matching
descriptors are used for fast and accurate mapping, and the transform
estimation between the 2D and 3D descriptors is used to localize the sensor.
The provided experimental results demonstrate that the proposed indoor SLAM can
achieve the localization and mapping accuracy around one millimeter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Data-Free Model Stealing in a Hard Label Setting. (arXiv:2204.11022v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11022">
<div class="article-summary-box-inner">
<span><p>Machine learning models deployed as a service (MLaaS) are susceptible to
model stealing attacks, where an adversary attempts to steal the model within a
restricted access framework. While existing attacks demonstrate near-perfect
clone-model performance using softmax predictions of the classification
network, most of the APIs allow access to only the top-1 labels. In this work,
we show that it is indeed possible to steal Machine Learning models by
accessing only top-1 predictions (Hard Label setting) as well, without access
to model gradients (Black-Box setting) or even the training dataset (Data-Free
setting) within a low query budget. We propose a novel GAN-based framework that
trains the student and generator in tandem to steal the model effectively while
overcoming the challenge of the hard label setting by utilizing gradients of
the clone network as a proxy to the victim's gradients. We propose to overcome
the large query costs associated with a typical Data-Free setting by utilizing
publicly available (potentially unrelated) datasets as a weak image prior. We
additionally show that even in the absence of such data, it is possible to
achieve state-of-the-art results within a low query budget using synthetically
crafted samples. We are the first to demonstrate the scalability of Model
Stealing in a restricted access setting on a 100 class dataset as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VISTA: Vision Transformer enhanced by U-Net and Image Colorfulness Frame Filtration for Automatic Retail Checkout. (arXiv:2204.11024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11024">
<div class="article-summary-box-inner">
<span><p>Multi-class product counting and recognition identifies product items from
images or videos for automated retail checkout. The task is challenging due to
the real-world scenario of occlusions where product items overlap, fast
movement in the conveyor belt, large similarity in overall appearance of the
items being scanned, novel products, and the negative impact of misidentifying
items. Further, there is a domain bias between training and test sets,
specifically, the provided training dataset consists of synthetic images and
the test set videos consist of foreign objects such as hands and tray. To
address these aforementioned issues, we propose to segment and classify
individual frames from a video sequence. The segmentation method consists of a
unified single product item- and hand-segmentation followed by entropy masking
to address the domain bias problem. The multi-class classification method is
based on Vision Transformers (ViT). To identify the frames with target objects,
we utilize several image processing methods and propose a custom metric to
discard frames not having any product items. Combining all these mechanisms,
our best system achieves 3rd place in the AI City Challenge 2022 Track 4 with
an F1 score of 0.4545. Code will be available at
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning by Erasing: Conditional Entropy based Transferable Out-Of-Distribution Detection. (arXiv:2204.11041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11041">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection is essential to handle the distribution
shifts between training and test scenarios. For a new in-distribution (ID)
dataset, existing methods require retraining to capture the dataset-specific
feature representation or data distribution. In this paper, we propose a deep
generative models (DGM) based transferable OOD detection method, which is
unnecessary to retrain on a new ID dataset. We design an image erasing strategy
to equip exclusive conditional entropy distribution for each ID dataset, which
determines the discrepancy of DGM's posteriori ucertainty distribution on
different ID datasets. Owing to the powerful representation capacity of
convolutional neural networks, the proposed model trained on complex dataset
can capture the above discrepancy between ID datasets without retraining and
thus achieve transferable OOD detection. We validate the proposed method on
five datasets and verity that ours achieves comparable performance to the
state-of-the-art group based OOD detection methods that need to be retrained to
deploy on new ID datasets. Our code is available at
https://github.com/oOHCIOo/CETOOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Neural Architectures by Synthetic Dataset Design. (arXiv:2204.11045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11045">
<div class="article-summary-box-inner">
<span><p>Recent years have seen the emergence of many new neural network structures
(architectures and layers). To solve a given task, a network requires a certain
set of abilities reflected in its structure. The required abilities depend on
each task. There is so far no systematic study of the real capacities of the
proposed neural structures. The question of what each structure can and cannot
achieve is only partially answered by its performance on common benchmarks.
Indeed, natural data contain complex unknown statistical cues. It is therefore
impossible to know what cues a given neural structure is taking advantage of in
such data. In this work, we sketch a methodology to measure the effect of each
structure on a network's ability, by designing ad hoc synthetic datasets. Each
dataset is tailored to assess a given ability and is reduced to its simplest
form: each input contains exactly the amount of information needed to solve the
task. We illustrate our methodology by building three datasets to evaluate each
of the three following network properties: a) the ability to link local cues to
distant inferences, b) the translation covariance and c) the ability to group
pixels with the same characteristics and share information among them. Using a
first simplified depth estimation dataset, we pinpoint a serious nonlocal
deficit of the U-Net. We then evaluate how to resolve this limitation by
embedding its structure with nonlocal layers, which allow computing complex
features with long-range dependencies. Using a second dataset, we compare
different positional encoding methods and use the results to further improve
the U-Net on the depth estimation task. The third introduced dataset serves to
demonstrate the need for self-attention-like mechanisms for resolving more
realistic depth estimation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class Balanced PixelNet for Neurological Image Segmentation. (arXiv:2204.11048v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11048">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an automatic brain tumor segmentation approach
(e.g., PixelNet) using a pixel-level convolutional neural network (CNN). The
model extracts feature from multiple convolutional layers and concatenate them
to form a hyper-column where samples a modest number of pixels for
optimization. Hyper-column ensures both local and global contextual information
for pixel-wise predictors. The model confirms the statistical efficiency by
sampling a few pixels in the training phase where spatial redundancy limits the
information learning among the neighboring pixels in conventional pixel-level
semantic segmentation approaches. Besides, label skewness in training data
leads the convolutional model often converge to certain classes which is a
common problem in the medical dataset. We deal with this problem by selecting
an equal number of pixels for all the classes in sampling time. The proposed
model has achieved promising results in brain tumor and ischemic stroke lesion
segmentation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial Expression Recognition. (arXiv:2204.11053v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11053">
<div class="article-summary-box-inner">
<span><p>High-quality annotated images are significant to deep facial expression
recognition (FER) methods. However, uncertain labels, mostly existing in
large-scale public datasets, often mislead the training process. In this paper,
we achieve uncertain label correction of facial expressions using auxiliary
action unit (AU) graphs, called ULC-AG. Specifically, a weighted regularization
module is introduced to highlight valid samples and suppress category imbalance
in every batch. Based on the latent dependency between emotions and AUs, an
auxiliary branch using graph convolutional layers is added to extract the
semantic information from graph topologies. Finally, a re-labeling strategy
corrects the ambiguous annotations by comparing their feature similarities with
semantic templates. Experiments show that our ULC-AG achieves 89.31% and 61.57%
accuracy on RAF-DB and AffectNet datasets, respectively, outperforming the
baseline and state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLP-Hash: Protecting Face Templates via Hashing of Randomized Multi-Layer Perceptron. (arXiv:2204.11054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11054">
<div class="article-summary-box-inner">
<span><p>Applications of face recognition systems for authentication purposes are
growing rapidly. Although state-of-the-art (SOTA) face recognition systems have
high recognition performance, the features which are extracted for each user
and are stored in the system's database contain privacy-sensitive information.
Accordingly, compromising this data would jeopardize users' privacy. In this
paper, we propose a new cancelable template protection method, dubbed MLP-hash,
which generates protected templates by passing the extracted features through a
user-specific randomly-weighted multi-layer perceptron (MLP) and binarizing the
MLP output. We evaluated the unlinkability, irreversibility, and recognition
performance of our proposed biometric template protection method to fulfill the
ISO/IEC 30136 standard requirements. Our experiments with SOTA face recognition
systems on the MOBIO and LFW datasets show that our method has competitive
performance with the BioHashing and IoM Hashing (IoM-GRP and IoM-URP) template
protection algorithms. We provide an open-source implementation of all the
experiments presented in this paper so that other researchers can verify our
findings and build upon our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformation Invariant Cancerous Tissue Classification Using Spatially Transformed DenseNet. (arXiv:2204.11066v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11066">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce a spatially transformed DenseNet architecture for
transformation invariant classification of cancer tissue. Our architecture
increases the accuracy of the base DenseNet architecture while adding the
ability to operate in a transformation invariant way while simultaneously being
simpler than other models that try to provide some form of invariance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Shape Priors by Pairwise Comparison for Robust Semantic Segmentation. (arXiv:2204.11090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11090">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is important in medical image analysis. Inspired by the
strong ability of traditional image analysis techniques in capturing shape
priors and inter-subject similarity, many deep learning (DL) models have been
recently proposed to exploit such prior information and achieved robust
performance. However, these two types of important prior information are
usually studied separately in existing models. In this paper, we propose a
novel DL model to model both type of priors within a single framework.
Specifically, we introduce an extra encoder into the classic encoder-decoder
structure to form a Siamese structure for the encoders, where one of them takes
a target image as input (the image-encoder), and the other concatenates a
template image and its foreground regions as input (the template-encoder). The
template-encoder encodes the shape priors and appearance characteristics of
each foreground class in the template image. A cosine similarity based
attention module is proposed to fuse the information from both encoders, to
utilize both types of prior information encoded by the template-encoder and
model the inter-subject similarity for each foreground class. Extensive
experiments on two public datasets demonstrate that our proposed method can
produce superior performance to competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can domain adaptation make object recognition work for everyone?. (arXiv:2204.11122v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11122">
<div class="article-summary-box-inner">
<span><p>Despite the rapid progress in deep visual recognition, modern computer vision
datasets significantly overrepresent the developed world and models trained on
such datasets underperform on images from unseen geographies. We investigate
the effectiveness of unsupervised domain adaptation (UDA) of such models across
geographies at closing this performance gap. To do so, we first curate two
shifts from existing datasets to study the Geographical DA problem, and
discover new challenges beyond data distribution shift: context shift, wherein
object surroundings may change significantly across geographies, and
subpopulation shift, wherein the intra-category distributions may shift. We
demonstrate the inefficacy of standard DA methods at Geographical DA,
highlighting the need for specialized geographical adaptation solutions to
address the challenge of making object recognition work for everyone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Bundle Adjustment for Satellite Imaging via Quantum Machine Learning. (arXiv:2204.11133v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11133">
<div class="article-summary-box-inner">
<span><p>Given is a set of images, where all images show views of the same area at
different points in time and from different viewpoints. The task is the
alignment of all images such that relevant information, e.g., poses, changes,
and terrain, can be extracted from the fused image. In this work, we focus on
quantum methods for keypoint extraction and feature matching, due to the
demanding computational complexity of these sub-tasks. To this end, k-medoids
clustering, kernel density clustering, nearest neighbor search, and kernel
methods are investigated and it is explained how these methods can be
re-formulated for quantum annealers and gate-based quantum computers.
Experimental results obtained on digital quantum emulation hardware, quantum
annealers, and quantum gate computers show that classical systems still deliver
superior results. However, the proposed methods are ready for the current and
upcoming generations of quantum computing devices which have the potential to
outperform classical systems in the near future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supplementing Missing Visions via Dialog for Scene Graph Generations. (arXiv:2204.11143v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11143">
<div class="article-summary-box-inner">
<span><p>Most current AI systems rely on the premise that the input visual data are
sufficient to achieve competitive performance in various computer vision tasks.
However, the classic task setup rarely considers the challenging, yet common
practical situations where the complete visual data may be inaccessible due to
various reasons (e.g., restricted view range and occlusions). To this end, we
investigate a computer vision task setting with incomplete visual input data.
Specifically, we exploit the Scene Graph Generation (SGG) task with various
levels of visual data missingness as input. While insufficient visual input
intuitively leads to performance drop, we propose to supplement the missing
visions via the natural language dialog interactions to better accomplish the
task objective. We design a model-agnostic Supplementary Interactive Dialog
(SI-Dial) framework that can be jointly learned with most existing models,
endowing the current AI systems with the ability of question-answer
interactions in natural language. We demonstrate the feasibility of such a task
setting with missing visual input and the effectiveness of our proposed dialog
module as the supplementary information source through extensive experiments
and analysis, by achieving promising performance improvement over multiple
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gabor is Enough: Interpretable Deep Denoising with a Gabor Synthesis Dictionary Prior. (arXiv:2204.11146v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11146">
<div class="article-summary-box-inner">
<span><p>Image processing neural networks, natural and artificial, have a long history
with orientation-selectivity, often described mathematically as Gabor filters.
Gabor-like filters have been observed in the early layers of CNN classifiers
and even throughout low-level image processing networks. In this work, we take
this observation to the extreme and explicitly constrain the filters of a
natural-image denoising CNN to be learned 2D real Gabor filters. Surprisingly,
we find that the proposed network (GDLNet) can achieve near state-of-the-art
denoising performance amongst popular fully convolutional neural networks, with
only a fraction of the learned parameters. We further verify that this
parameterization maintains the noise-level generalization (training vs.
inference mismatch) characteristics of the base network, and investigate the
contribution of individual Gabor filter parameters to the performance of the
denoiser. We present positive findings for the interpretation of dictionary
learning networks as performing accelerated sparse-coding via the importance of
untied learned scale parameters between network layers. Our network's success
suggests that representations used by low-level image processing CNNs can be as
simple and interpretable as Gabor filterbanks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Unsupervised Industrial Anomaly Detection Algorithms. (arXiv:2204.11161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11161">
<div class="article-summary-box-inner">
<span><p>Anomaly defect detection has become an indispensable part of industrial
production process. In previous study, a large part of the traditional anomaly
detection algorithms belong to the category of supervised learning, while the
unsupervised situation is more common for most practical application scenarios.
Hence gradually unsupervised anomaly detection has been the subject of much
research over the last few years. In this survey, we provide a comprehensive
introduction to newly proposed approaches for visual anomaly detection. We hope
that it can help the research community as well as the industry field to build
a broader and cross-domain perspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning. (arXiv:2204.11167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11167">
<div class="article-summary-box-inner">
<span><p>Reasoning about visual relationships is central to how humans interpret the
visual world. This task remains challenging for current deep learning
algorithms since it requires addressing three key technical problems jointly:
1) identifying object entities and their properties, 2) inferring semantic
relations between pairs of entities, and 3) generalizing to novel
object-relation combinations, i.e., systematic generalization. In this work, we
use vision transformers (ViTs) as our base model for visual reasoning and make
better use of concepts defined as object entities and their relations to
improve the reasoning ability of ViTs. Specifically, we introduce a novel
concept-feature dictionary to allow flexible image feature retrieval at
training time with concept keys. This dictionary enables two new concept-guided
auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a
local task for facilitating semantic object-centric correspondence learning. To
examine the systematic generalization of visual reasoning models, we introduce
systematic splits for the standard HICO and GQA benchmarks. We show the
resulting model, Concept-guided Vision Transformer (or RelViT for short)
significantly outperforms prior approaches on HICO and GQA by 16% and 13% in
the original split, and by 43% and 18% in the systematic split. Our ablation
analyses also reveal our model's compatibility with multiple ViT variants and
robustness to hyper-parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic Evaluation of Transductive Few-Shot Learning. (arXiv:2204.11181v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11181">
<div class="article-summary-box-inner">
<span><p>Transductive inference is widely used in few-shot learning, as it leverages
the statistics of the unlabeled query set of a few-shot task, typically
yielding substantially better performances than its inductive counterpart. The
current few-shot benchmarks use perfectly class-balanced tasks at inference. We
argue that such an artificial regularity is unrealistic, as it assumes that the
marginal label probability of the testing samples is known and fixed to the
uniform distribution. In fact, in realistic scenarios, the unlabeled query sets
come with arbitrary and unknown label marginals. We introduce and study the
effect of arbitrary class distributions within the query sets of few-shot tasks
at inference, removing the class-balance artefact. Specifically, we model the
marginal probabilities of the classes as Dirichlet-distributed random
variables, which yields a principled and realistic sampling within the simplex.
This leverages the current few-shot benchmarks, building testing tasks with
arbitrary class distributions. We evaluate experimentally state-of-the-art
transductive methods over 3 widely used data sets, and observe, surprisingly,
substantial performance drops, even below inductive methods in some cases.
Furthermore, we propose a generalization of the mutual-information loss, based
on $\alpha$-divergences, which can handle effectively class-distribution
variations. Empirically, we show that our transductive $\alpha$-divergence
optimization outperforms state-of-the-art methods across several data sets,
models and few-shot settings. Our code is publicly available at
https://github.com/oveilleux/Realistic_Transductive_Few_Shot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames. (arXiv:2204.11184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11184">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider a novel problem of reconstructing a 3D human
avatar from multiple unconstrained frames, independent of assumptions on camera
calibration, capture space, and constrained actions. The problem should be
addressed by a framework that takes multiple unconstrained images as inputs,
and generates a shape-with-skinning avatar in the canonical space, finished in
one feed-forward pass. To this end, we present 3D Avatar Reconstruction in the
wild (ARwild), which first reconstructs the implicit skinning fields in a
multi-level manner, by which the image features from multiple images are
aligned and integrated to estimate a pixel-aligned implicit function that
represents the clothed shape. To enable the training and testing of the new
framework, we contribute a large-scale dataset, MVP-Human (Multi-View and
multi-Pose 3D Human), which contains 400 subjects, each of which has 15 scans
in different poses and 8-view images for each pose, providing 6,000 3D scans
and 48,000 images in total. Overall, benefits from the specific network
architecture and the diverse data, the trained model enables 3D avatar
reconstruction from unconstrained frames and achieves state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PUERT: Probabilistic Under-sampling and Explicable Reconstruction Network for CS-MRI. (arXiv:2204.11189v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11189">
<div class="article-summary-box-inner">
<span><p>Compressed Sensing MRI (CS-MRI) aims at reconstructing de-aliased images from
sub-Nyquist sampling k-space data to accelerate MR Imaging, thus presenting two
basic issues, i.e., where to sample and how to reconstruct. To deal with both
problems simultaneously, we propose a novel end-to-end Probabilistic
Under-sampling and Explicable Reconstruction neTwork, dubbed PUERT, to jointly
optimize the sampling pattern and the reconstruction network. Instead of
learning a deterministic mask, the proposed sampling subnet explores an optimal
probabilistic sub-sampling pattern, which describes independent Bernoulli
random variables at each possible sampling point, thus retaining robustness and
stochastics for a more reliable CS reconstruction. A dynamic gradient
estimation strategy is further introduced to gradually approximate the
binarization function in backward propagation, which efficiently preserves the
gradient information and further improves the reconstruction quality. Moreover,
in our reconstruction subnet, we adopt a model-based network design scheme with
high efficiency and interpretability, which is shown to assist in further
exploitation for the sampling subnet. Extensive experiments on two widely used
MRI datasets demonstrate that our proposed PUERT not only achieves
state-of-the-art results in terms of both quantitative metrics and visual
quality but also yields a sub-sampling pattern and a reconstruction model that
are both customized to training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2D LiDAR and Camera Fusion Using Motion Cues for Indoor Layout Estimation. (arXiv:2204.11202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11202">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel indoor layout estimation system based on the
fusion of 2D LiDAR and intensity camera data. A ground robot explores an indoor
space with a single floor and vertical walls, and collects a sequence of
intensity images and 2D LiDAR datasets. The LiDAR provides accurate depth
information, while the camera captures high-resolution data for semantic
interpretation. The alignment of sensor outputs and image segmentation are
computed jointly by aligning LiDAR points, as samples of the room contour, to
ground-wall boundaries in the images. The alignment problem is decoupled into a
top-down view projection and a 2D similarity transformation estimation, which
can be solved according to the vertical vanishing point and motion of two
sensors. The recursive random sample consensus algorithm is implemented to
generate, evaluate and optimize multiple hypotheses with the sequential
measurements. The system allows jointly analyzing the geometric interpretation
from different sensors without offline calibration. The ambiguity in images for
ground-wall boundary extraction is removed with the assistance of LiDAR
observations, which improves the accuracy of semantic segmentation. The
localization and mapping is refined using the fused data, which enables the
system to work reliably in scenes with low texture or low geometric features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Learning for Image Retrieval with Hybrid-Modality Queries. (arXiv:2204.11212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11212">
<div class="article-summary-box-inner">
<span><p>Image retrieval with hybrid-modality queries, also known as composing text
and image for image retrieval (CTI-IR), is a retrieval task where the search
intention is expressed in a more complex query format, involving both vision
and text modalities. For example, a target product image is searched using a
reference product image along with text about changing certain attributes of
the reference image as the query. It is a more challenging image retrieval task
that requires both semantic space learning and cross-modal fusion. Previous
approaches that attempt to deal with both aspects achieve unsatisfactory
performance. In this paper, we decompose the CTI-IR task into a three-stage
learning problem to progressively learn the complex knowledge for image
retrieval with hybrid-modality queries. We first leverage the semantic
embedding space for open-domain image-text retrieval, and then transfer the
learned knowledge to the fashion-domain with fashion-related pre-training
tasks. Finally, we enhance the pre-trained model from single-query to
hybrid-modality query for the CTI-IR task. Furthermore, as the contribution of
individual modality in the hybrid-modality query varies for different retrieval
scenarios, we propose a self-supervised adaptive weighting strategy to
dynamically determine the importance of image and text in the hybrid-modality
query for better retrieval. Extensive experiments show that our proposed model
significantly outperforms state-of-the-art methods in the mean of Recall@K by
24.9% and 9.5% on the Fashion-IQ and Shoes benchmark datasets respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RealNet: Combining Optimized Object Detection with Information Fusion Depth Estimation Co-Design Method on IoT. (arXiv:2204.11216v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11216">
<div class="article-summary-box-inner">
<span><p>Depth Estimation and Object Detection Recognition play an important role in
autonomous driving technology under the guidance of deep learning artificial
intelligence. We propose a hybrid structure called RealNet: a co-design method
combining the model-streamlined recognition algorithm, the depth estimation
algorithm with information fusion, and deploying them on the Jetson-Nano for
unmanned vehicles with monocular vision sensors. We use ROS for experiment. The
method proposed in this paper is suitable for mobile platforms with high
real-time request. Innovation of our method is using information fusion to
compensate the problem of insufficient frame rate of output image, and improve
the robustness of target detection and depth estimation under monocular
vision.Object Detection is based on YOLO-v5. We have simplified the network
structure of its DarkNet53 and realized a prediction speed up to 0.01s. Depth
Estimation is based on the VNL Depth Estimation, which considers multiple
geometric constraints in 3D global space. It calculates the loss function by
calculating the deviation of the virtual normal vector VN and the label, which
can obtain deeper depth information. We use PnP fusion algorithm to solve the
problem of insufficient frame rate of depth map output. It solves the motion
estimation depth from three-dimensional target to two-dimensional point based
on corner feature matching, which is faster than VNL calculation. We
interpolate VNL output and PnP output to achieve information fusion.
Experiments show that this can effectively eliminate the jitter of depth
information and improve robustness. At the control end, this method combines
the results of target detection and depth estimation to calculate the target
position, and uses a pure tracking control algorithm to track it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lesion Localization in OCT by Semi-Supervised Object Detection. (arXiv:2204.11227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11227">
<div class="article-summary-box-inner">
<span><p>Over 300 million people worldwide are affected by various retinal diseases.
By noninvasive Optical Coherence Tomography (OCT) scans, a number of abnormal
structural changes in the retina, namely retinal lesions, can be identified.
Automated lesion localization in OCT is thus important for detecting retinal
diseases at their early stage. To conquer the lack of manual annotation for
deep supervised learning, this paper presents a first study on utilizing
semi-supervised object detection (SSOD) for lesion localization in OCT images.
To that end, we develop a taxonomy to provide a unified and structured
viewpoint of the current SSOD methods, and consequently identify key modules in
these methods. To evaluate the influence of these modules in the new task, we
build OCT-SS, a new dataset consisting of over 1k expert-labeled OCT B-scan
images and over 13k unlabeled B-scans. Extensive experiments on OCT-SS identify
Unbiased Teacher (UnT) as the best current SSOD method for lesion localization.
Moreover, we improve over this strong baseline, with mAP increased from 49.34
to 50.86.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-Free Domain Adaptation via Distribution Estimation. (arXiv:2204.11257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11257">
<div class="article-summary-box-inner">
<span><p>Domain Adaptation aims to transfer the knowledge learned from a labeled
source domain to an unlabeled target domain whose data distributions are
different. However, the training data in source domain required by most of the
existing methods is usually unavailable in real-world applications due to
privacy preserving policies. Recently, Source-Free Domain Adaptation (SFDA) has
drawn much attention, which tries to tackle domain adaptation problem without
using source data. In this work, we propose a novel framework called SFDA-DE to
address SFDA task via source Distribution Estimation. Firstly, we produce
robust pseudo-labels for target data with spherical k-means clustering, whose
initial class centers are the weight vectors (anchors) learned by the
classifier of pretrained model. Furthermore, we propose to estimate the
class-conditioned feature distribution of source domain by exploiting target
data and corresponding anchors. Finally, we sample surrogate features from the
estimated distribution, which are then utilized to align two domains by
minimizing a contrastive adaptation loss function. Extensive experiments show
that the proposed method achieves state-of-the-art performance on multiple DA
benchmarks, and even outperforms traditional DA methods which require plenty of
source data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RMGN: A Regional Mask Guided Network for Parser-free Virtual Try-on. (arXiv:2204.11258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11258">
<div class="article-summary-box-inner">
<span><p>Virtual try-on(VTON) aims at fitting target clothes to reference person
images, which is widely adopted in e-commerce.Existing VTON approaches can be
narrowly categorized into Parser-Based(PB) and Parser-Free(PF) by whether
relying on the parser information to mask the persons' clothes and synthesize
try-on images. Although abandoning parser information has improved the
applicability of PF methods, the ability of detail synthesizing has also been
sacrificed. As a result, the distraction from original cloth may persistin
synthesized images, especially in complicated postures and high resolution
applications. To address the aforementioned issue, we propose a novel PF method
named Regional Mask Guided Network(RMGN). More specifically, a regional mask is
proposed to explicitly fuse the features of target clothes and reference
persons so that the persisted distraction can be eliminated. A posture
awareness loss and a multi-level feature extractor are further proposed to
handle the complicated postures and synthesize high resolution images.
Extensive experiments demonstrate that our proposed RMGN outperforms both
state-of-the-art PB and PF methods.Ablation studies further verify the
effectiveness ofmodules in RMGN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Semantic Weak Generalization Problem in Generative Zero-Shot Learning: Ante-hoc and Post-hoc. (arXiv:2204.11280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11280">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a simple and effective strategy lowering the
previously unexplored factors that limit the performance ceiling of generative
Zero-Shot Learning (ZSL). We begin by formally defining semantic
generalization, then look into approaches for reducing the semantic weak
generalization problem and minimizing its negative influence on classifier
training. In the ante-hoc phase, we augment the generator's semantic input, as
well as relax the fitting target of the generator. In the post-hoc phase (after
generating simulated unseen samples), we derive from the gradient of the loss
function to minimize the gradient increment on seen classifier weights carried
by biased unseen distribution, which tends to cause misleading on intra-seen
class decision boundaries. Without complicated designs, our approach hit the
essential problem and significantly outperform the state-of-the-art on four
widely used ZSL datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Scale Time-Series Representation Learning via Simultaneous Low and High Frequency Feature Bootstrapping. (arXiv:2204.11291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11291">
<div class="article-summary-box-inner">
<span><p>Learning representation from unlabeled time series data is a challenging
problem. Most existing self-supervised and unsupervised approaches in the
time-series domain do not capture low and high-frequency features at the same
time. Further, some of these methods employ large scale models like
transformers or rely on computationally expensive techniques such as
contrastive learning. To tackle these problems, we propose a non-contrastive
self-supervised learning approach efficiently captures low and high-frequency
time-varying features in a cost-effective manner. Our method takes raw time
series data as input and creates two different augmented views for two branches
of the model, by randomly sampling the augmentations from same family.
Following the terminology of BYOL, the two branches are called online and
target network which allows bootstrapping of the latent representation. In
contrast to BYOL, where a backbone encoder is followed by multilayer perceptron
(MLP) heads, the proposed model contains additional temporal convolutional
network (TCN) heads. As the augmented views are passed through large kernel
convolution blocks of the encoder, the subsequent combination of MLP and TCN
enables an effective representation of low as well as high-frequency
time-varying features due to the varying receptive fields. The two modules (MLP
and TCN) act in a complementary manner. We train an online network where each
module learns to predict the outcome of the respective module of target network
branch. To demonstrate the robustness of our model we performed extensive
experiments and ablation studies on five real-world time-series datasets. Our
method achieved state-of-art performance on all five real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colorectal cancer survival prediction using deep distribution based multiple-instance learning. (arXiv:2204.11294v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11294">
<div class="article-summary-box-inner">
<span><p>Several deep learning algorithms have been developed to predict survival of
cancer patients using whole slide images (WSIs).However, identification of
image phenotypes within the WSIs that are relevant to patient survival and
disease progression is difficult for both clinicians, and deep learning
algorithms. Most deep learning based Multiple Instance Learning (MIL)
algorithms for survival prediction use either top instances (e.g., maxpooling)
or top/bottom instances (e.g., MesoNet) to identify image phenotypes. In this
study, we hypothesize that wholistic information of the distribution of the
patch scores within a WSI can predict the cancer survival better. We developed
a distribution based multiple-instance survival learning algorithm
(DeepDisMISL) to validate this hypothesis. We designed and executed experiments
using two large international colorectal cancer WSIs datasets - MCO CRC and
TCGA COAD-READ. Our results suggest that the more information about the
distribution of the patch scores for a WSI, the better is the prediction
performance. Including multiple neighborhood instances around each selected
distribution location (e.g., percentiles) could further improve the prediction.
DeepDisMISL demonstrated superior predictive ability compared to other recently
published, state-of-the-art algorithms. Furthermore, our algorithm is
interpretable and could assist in understanding the relationship between cancer
morphological phenotypes and patients cancer survival risk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dictionary Attacks on Speaker Verification. (arXiv:2204.11304v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11304">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose dictionary attacks against speaker verification - a
novel attack vector that aims to match a large fraction of speaker population
by chance. We introduce a generic formulation of the attack that can be used
with various speech representations and threat models. The attacker uses
adversarial optimization to maximize raw similarity of speaker embeddings
between a seed speech sample and a proxy population. The resulting master voice
successfully matches a non-trivial fraction of people in an unknown population.
Adversarial waveforms obtained with our approach can match on average 69% of
females and 38% of males enrolled in the target system at a strict decision
threshold calibrated to yield false alarm rate of 1%. By using the attack with
a black-box voice cloning system, we obtain master voices that are effective in
the most challenging conditions and transferable between speaker encoders. We
also show that, combined with multiple attempts, this attack opens even more to
serious issues on the security of these systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMOCA: Emotion Driven Monocular Face Capture and Animation. (arXiv:2204.11312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11312">
<div class="article-summary-box-inner">
<span><p>As 3D facial avatars become more widely used for communication, it is
critical that they faithfully convey emotion. Unfortunately, the best recent
methods that regress parametric 3D face models from monocular images are unable
to capture the full spectrum of facial expression, such as subtle or extreme
emotions. We find the standard reconstruction metrics used for training
(landmark reprojection error, photometric error, and face recognition loss) are
insufficient to capture high-fidelity expressions. The result is facial
geometries that do not match the emotional content of the input image. We
address this with EMOCA (EMOtion Capture and Animation), by introducing a novel
deep perceptual emotion consistency loss during training, which helps ensure
that the reconstructed 3D expression matches the expression depicted in the
input image. While EMOCA achieves 3D reconstruction errors that are on par with
the current best methods, it significantly outperforms them in terms of the
quality of the reconstructed expression and the perceived emotional content. We
also directly regress levels of valence and arousal and classify basic
expressions from the estimated 3D face parameters. On the task of in-the-wild
emotion recognition, our purely geometric approach is on par with the best
image-based methods, highlighting the value of 3D geometry in analyzing human
behavior. The model and code are publicly available at
https://emoca.is.tue.mpg.de.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulating Fluids in Real-World Still Images. (arXiv:2204.11335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11335">
<div class="article-summary-box-inner">
<span><p>In this work, we tackle the problem of real-world fluid animation from a
still image. The key of our system is a surface-based layered representation
deriving from video decomposition, where the scene is decoupled into a surface
fluid layer and an impervious background layer with corresponding
transparencies to characterize the composition of the two layers. The animated
video can be produced by warping only the surface fluid layer according to the
estimation of fluid motions and recombining it with the background. In
addition, we introduce surface-only fluid simulation, a $2.5D$ fluid
calculation version, as a replacement for motion estimation. Specifically, we
leverage the triangular mesh based on a monocular depth estimator to represent
the fluid surface layer and simulate the motion in the physics-based framework
with the inspiration of the classic theory of the hybrid Lagrangian-Eulerian
method, along with a learnable network so as to adapt to complex real-world
image textures. We demonstrate the effectiveness of the proposed system through
comparison with existing methods in both standard objective metrics and
subjective ranking scores. Extensive experiments not only indicate our method's
competitive performance for common fluid scenes but also better robustness and
reasonability under complex transparent fluid scenarios. Moreover, as the
proposed surface-based layer representation and surface-only fluid simulation
naturally disentangle the scene, interactive editing such as adding objects to
the river and texture replacing could be easily achieved with realistic
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Medical Image Registration: A Comprehensive Review. (arXiv:2204.11341v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11341">
<div class="article-summary-box-inner">
<span><p>Image registration is a critical component in the applications of various
medical image analyses. In recent years, there has been a tremendous surge in
the development of deep learning (DL)-based medical image registration models.
This paper provides a comprehensive review of medical image registration.
Firstly, a discussion is provided for supervised registration categories, for
example, fully supervised, dual supervised, and weakly supervised registration.
Next, similarity-based as well as generative adversarial network (GAN)-based
registration are presented as part of unsupervised registration. Deep iterative
registration is then described with emphasis on deep similarity-based and
reinforcement learning-based registration. Moreover, the application areas of
medical image registration are reviewed. This review focuses on monomodal and
multimodal registration and associated imaging, for instance, X-ray, CT scan,
ultrasound, and MRI. The existing challenges are highlighted in this review,
where it is shown that a major challenge is the absence of a training dataset
with known transformations. Finally, a discussion is provided on the promising
future research areas in the field of DL-based medical image registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Reinforcement Learning Using a Low-Dimensional Observation Filter for Visual Complex Video Game Playing. (arXiv:2204.11370v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11370">
<div class="article-summary-box-inner">
<span><p>Deep Reinforcement Learning (DRL) has produced great achievements since it
was proposed, including the possibility of processing raw vision input data.
However, training an agent to perform tasks based on image feedback remains a
challenge. It requires the processing of large amounts of data from
high-dimensional observation spaces, frame by frame, and the agent's actions
are computed according to deep neural network policies, end-to-end. Image
pre-processing is an effective way of reducing these high dimensional spaces,
eliminating unnecessary information present in the scene, supporting the
extraction of features and their representations in the agent's neural network.
Modern video-games are examples of this type of challenge for DRL algorithms
because of their visual complexity. In this paper, we propose a low-dimensional
observation filter that allows a deep Q-network agent to successfully play in a
visually complex and modern video-game, called Neon Drive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRT: A Lightweight Single Image Deraining Recursive Transformer. (arXiv:2204.11385v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11385">
<div class="article-summary-box-inner">
<span><p>Over parameterization is a common technique in deep learning to help models
learn and generalize sufficiently to the given task; nonetheless, this often
leads to enormous network structures and consumes considerable computing
resources during training. Recent powerful transformer-based deep learning
models on vision tasks usually have heavy parameters and bear training
difficulty. However, many dense-prediction low-level computer vision tasks,
such as rain streak removing, often need to be executed on devices with limited
computing power and memory in practice. Hence, we introduce a recursive local
window-based self-attention structure with residual connections and propose
deraining a recursive transformer (DRT), which enjoys the superiority of the
transformer but requires a small amount of computing resources. In particular,
through recursive architecture, our proposed model uses only 1.3% of the number
of parameters of the current best performing model in deraining while exceeding
the state-of-the-art methods on the Rain100L benchmark by at least 0.33 dB.
Ablation studies also investigate the impact of recursions on derain outcomes.
Moreover, since the model contains no deliberate design for deraining, it can
also be applied to other image restoration tasks. Our experiment shows that it
can achieve competitive results on desnowing. The source code and pretrained
model can be found at https://github.com/YC-Liang/DRT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Frame Interpolation Based on Deformable Kernel Region. (arXiv:2204.11396v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11396">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation task has recently become more and more prevalent in
the computer vision field. At present, a number of researches based on deep
learning have achieved great success. Most of them are either based on optical
flow information, or interpolation kernel, or a combination of these two
methods. However, these methods have ignored that there are grid restrictions
on the position of kernel region during synthesizing each target pixel. These
limitations result in that they cannot well adapt to the irregularity of object
shape and uncertainty of motion, which may lead to irrelevant reference pixels
used for interpolation. In order to solve this problem, we revisit the
deformable convolution for video interpolation, which can break the fixed grid
restrictions on the kernel region, making the distribution of reference points
more suitable for the shape of the object, and thus warp a more accurate
interpolation frame. Experiments are conducted on four datasets to demonstrate
the superior performance of the proposed model in comparison to the
state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensorial tomographic differential phase-contrast microscopy. (arXiv:2204.11397v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11397">
<div class="article-summary-box-inner">
<span><p>We report Tensorial Tomographic Differential Phase-Contrast microscopy
(T2DPC), a quantitative label-free tomographic imaging method for simultaneous
measurement of phase and anisotropy. T2DPC extends differential phase-contrast
microscopy, a quantitative phase imaging technique, to highlight the vectorial
nature of light. The method solves for permittivity tensor of anisotropic
samples from intensity measurements acquired with a standard microscope
equipped with an LED matrix, a circular polarizer, and a polarization-sensitive
camera. We demonstrate accurate volumetric reconstructions of refractive index,
birefringence, and orientation for various validation samples, and show that
the reconstructed polarization structures of a biological specimen are
predictive of pathology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointInst3D: Segmenting 3D Instances by Points. (arXiv:2204.11402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11402">
<div class="article-summary-box-inner">
<span><p>The current state-of-the-art methods in 3D instance segmentation typically
involve a clustering step, despite the tendency towards heuristics, greedy
algorithms, and a lack of robustness to the changes in data statistics. In
contrast, we propose a fully-convolutional 3D point cloud instance segmentation
method that works in a per-point prediction fashion. In doing so it avoids the
challenges that clustering-based methods face: introducing dependencies among
different tasks of the model. We find the key to its success is assigning a
suitable target to each sampled point. Instead of the commonly used static or
distance-based assignment strategies, we propose to use an Optimal Transport
approach to optimally assign target masks to the sampled points according to
the dynamic matching costs. Our approach achieves promising results on both
ScanNet and S3DIS benchmarks. The proposed approach removes intertask
dependencies and thus represents a simpler and more flexible 3D instance
segmentation framework than other competing methods, while achieving improved
segmentation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Point Cloud Compression with Cross-Sectional Approach. (arXiv:2204.11409v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11409">
<div class="article-summary-box-inner">
<span><p>The recent development of dynamic point clouds has introduced the possibility
of mimicking natural reality, and greatly assisting quality of life. However,
to broadcast successfully, the dynamic point clouds require higher compression
due to their huge volume of data compared to the traditional video. Recently,
MPEG finalized a Video-based Point Cloud Compression standard known as V-PCC.
However, V-PCC requires huge computational time due to expensive normal
calculation and segmentation, sacrifices some points to limit the number of 2D
patches, and cannot occupy all spaces in the 2D frame. The proposed method
addresses these limitations by using a novel cross-sectional approach. This
approach reduces expensive normal estimation and segmentation, retains more
points, and utilizes more spaces for 2D frame generation compared to the VPCC.
The experimental results using standard video sequences show that the proposed
technique can achieve better compression in both geometric and texture data
compared to the V-PCC standard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Object Tracking Research: A Survey. (arXiv:2204.11410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11410">
<div class="article-summary-box-inner">
<span><p>Visual object tracking is an important task in computer vision, which has
many real-world applications, e.g., video surveillance, visual navigation.
Visual object tracking also has many challenges, e.g., object occlusion and
deformation. To solve above problems and track the target accurately and
efficiently, many tracking algorithms have emerged in recent years. This paper
presents the rationale and representative works of two most popular tracking
frameworks in past ten years, i.e., the corelation filter and Siamese network
for object tracking. Then we present some deep learning based tracking methods
categorized by different network structures. We also introduce some classical
strategies for handling the challenges in tracking problem. Further, this paper
detailedly present and compare the benchmarks and challenges for tracking, from
which we summarize the development history and development trend of visual
tracking. Focusing on the future development of object tracking, which we think
would be applied in real-world scenes before some problems to be addressed,
such as the problems in long-term tracking, low-power high-speed tracking and
attack-robust tracking. In the future, the integration of multimodal data,
e.g., the depth image, thermal image with traditional color image, will provide
more solutions for visual tracking. Moreover, tracking task will go together
with some other tasks, e.g., video object detection and segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Scene Classification Using A Transfer Learning Based Joint Optimization Strategy. (arXiv:2204.11420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11420">
<div class="article-summary-box-inner">
<span><p>Recently, audio-visual scene classification (AVSC) has attracted increasing
attention from multidisciplinary communities. Previous studies tended to adopt
a pipeline training strategy, which uses well-trained visual and acoustic
encoders to extract high-level representations (embeddings) first, then
utilizes them to train the audio-visual classifier. In this way, the extracted
embeddings are well suited for uni-modal classifiers, but not necessarily
suited for multi-modal ones. In this paper, we propose a joint training
framework, using the acoustic features and raw images directly as inputs for
the AVSC task. Specifically, we retrieve the bottom layers of pre-trained image
models as visual encoder, and jointly optimize the scene classifier and 1D-CNN
based acoustic encoder during training. We evaluate the approach on the
development dataset of TAU Urban Audio-Visual Scenes 2021. The experimental
results show that our proposed approach achieves significant improvement over
the conventional pipeline training strategy. Moreover, our best single system
outperforms previous state-of-the-art methods, yielding a log loss of 0.1517
and accuracy of 94.59% on the official test fold.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix. (arXiv:2204.11425v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11425">
<div class="article-summary-box-inner">
<span><p>The evaluation of human epidermal growth factor receptor 2 (HER2) expression
is essential to formulate a precise treatment for breast cancer. The routine
evaluation of HER2 is conducted with immunohistochemical techniques (IHC),
which is very expensive. Therefore, for the first time, we propose a breast
cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data
directly with the paired hematoxylin and eosin (HE) stained images. The dataset
contains 4870 registered image pairs, covering a variety of HER2 expression
levels. Based on BCI, as a minor contribution, we further build a pyramid
pix2pix image generation method, which achieves better HE to IHC translation
results than the other current popular algorithms. Extensive experiments
demonstrate that BCI poses new challenges to the existing image translation
research. Besides, BCI also opens the door for future pathology studies in HER2
expression evaluation based on the synthesized IHC images. BCI dataset can be
downloaded from https://bupt-ai-cz.github.io/BCI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers. (arXiv:2204.11432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11432">
<div class="article-summary-box-inner">
<span><p>Unsupervised semantic segmentation aims to discover groupings within and
across images that capture object and view-invariance of a category without
external supervision. Grouping naturally has levels of granularity, creating
ambiguity in unsupervised segmentation. Existing methods avoid this ambiguity
and treat it as a factor outside modeling, whereas we embrace it and desire
hierarchical grouping consistency for unsupervised segmentation.
</p>
<p>We approach unsupervised segmentation as a pixel-wise feature learning
problem. Our idea is that a good representation shall reveal not just a
particular level of grouping, but any level of grouping in a consistent and
predictable manner. We enforce spatial consistency of grouping and bootstrap
feature learning with co-segmentation among multiple views of the same image,
and enforce semantic consistency across the grouping hierarchy with clustering
transformers between coarse- and fine-grained features.
</p>
<p>We deliver the first data-driven unsupervised hierarchical semantic
segmentation method called Hierarchical Segment Grouping (HSG). Capturing
visual similarity and statistical co-occurrences, HSG also outperforms existing
unsupervised segmentation methods by a large margin on five major object- and
scene-centric benchmarks. Our code is publicly available at
https://github.com/twke18/HSG .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surpassing the Human Accuracy: Detecting Gallbladder Cancer from USG Images with Curriculum Learning. (arXiv:2204.11433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11433">
<div class="article-summary-box-inner">
<span><p>We explore the potential of CNN-based models for gallbladder cancer (GBC)
detection from ultrasound (USG) images as no prior study is known. USG is the
most common diagnostic modality for GB diseases due to its low cost and
accessibility. However, USG images are challenging to analyze due to low image
quality, noise, and varying viewpoints due to the handheld nature of the
sensor. Our exhaustive study of state-of-the-art (SOTA) image classification
techniques for the problem reveals that they often fail to learn the salient GB
region due to the presence of shadows in the USG images. SOTA object detection
techniques also achieve low accuracy because of spurious textures due to noise
or adjacent organs. We propose GBCNet to tackle the challenges in our problem.
GBCNet first extracts the regions of interest (ROIs) by detecting the GB (and
not the cancer), and then uses a new multi-scale, second-order pooling
architecture specializing in classifying GBC. To effectively handle spurious
textures, we propose a curriculum inspired by human visual acuity, which
reduces the texture biases in GBCNet. Experimental results demonstrate that
GBCNet significantly outperforms SOTA CNN models, as well as the expert
radiologists. Our technical innovations are generic to other USG image analysis
tasks as well. Hence, as a validation, we also show the efficacy of GBCNet in
detecting breast cancer from USG images. Project page with source code, trained
models, and data is available at https://gbc-iitd.github.io/gbcnet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Feature Distribution Alignment Learning for NIR-VIS and VIS-VIS Face Recognition. (arXiv:2204.11434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11434">
<div class="article-summary-box-inner">
<span><p>Face recognition for visible light (VIS) images achieve high accuracy thanks
to the recent development of deep learning. However, heterogeneous face
recognition (HFR), which is a face matching in different domains, is still a
difficult task due to the domain discrepancy and lack of large HFR dataset.
Several methods have attempted to reduce the domain discrepancy by means of
fine-tuning, which causes significant degradation of the performance in the VIS
domain because it loses the highly discriminative VIS representation. To
overcome this problem, we propose joint feature distribution alignment learning
(JFDAL) which is a joint learning approach utilizing knowledge distillation. It
enables us to achieve high HFR performance with retaining the original
performance for the VIS domain. Extensive experiments demonstrate that our
proposed method delivers statistically significantly better performances
compared with the conventional fine-tuning approach on a public HFR dataset
Oulu-CASIA NIR&amp;VIS and popular verification datasets in VIS domain such as FLW,
CFP, AgeDB. Furthermore, comparative experiments with existing state-of-the-art
HFR methods show that our method achieves a comparable HFR performance on the
Oulu-CASIA NIR&amp;VIS dataset with less degradation of VIS performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition. (arXiv:1706.00931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1706.00931">
<div class="article-summary-box-inner">
<span><p>Recently, Long Short-Term Memory (LSTM) has become a popular choice to model
individual dynamics for single-person action recognition due to its ability of
modeling the temporal information in various ranges of dynamic contexts.
However, existing RNN models only focus on capturing the temporal dynamics of
the person-person interactions by naively combining the activity dynamics of
individuals or modeling them as a whole. This neglects the inter-related
dynamics of how person-person interactions change over time. To this end, we
propose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) to
model the long-term inter-related dynamics between two interacting people on
the bounding boxes covering people. Specifically, for each frame, two
sub-memory units store individual motion information, while a concurrent LSTM
unit selectively integrates and stores inter-related motion information between
interacting people from these two sub-memory units via a new co-memory cell.
Experimental results on the BIT and UT datasets show the superiority of
Co-LSTSM compared with the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Hashing Methods. (arXiv:2003.03369v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.03369">
<div class="article-summary-box-inner">
<span><p>Nearest neighbor search aims to obtain the samples in the database with the
smallest distances from them to the queries, which is a basic task in a range
of fields, including computer vision and data mining. Hashing is one of the
most widely used methods for its computational and storage efficiency. With the
development of deep learning, deep hashing methods show more advantages than
traditional methods. In this survey, we detailedly investigate current deep
hashing algorithms including deep supervised hashing and deep unsupervised
hashing. Specifically, we categorize deep supervised hashing methods into
pairwise methods, ranking-based methods, pointwise methods as well as
quantization according to how measuring the similarities of the learned hash
codes. Moreover, deep unsupervised hashing is categorized into similarity
reconstruction-based methods, pseudo-label-based methods and prediction-free
self-supervised learning-based methods based on their semantic learning
manners. We also introduce three related important topics including
semi-supervised deep hashing, domain adaption deep hashing and multi-modal deep
hashing. Meanwhile, we present some commonly used public datasets and the
scheme to measure the performance of deep hashing algorithms. Finally, we
discuss some potential research directions in conclusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyclic Differentiable Architecture Search. (arXiv:2006.10724v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10724">
<div class="article-summary-box-inner">
<span><p>Differentiable ARchiTecture Search, i.e., DARTS, has drawn great attention in
neural architecture search. It tries to find the optimal architecture in a
shallow search network and then measures its performance in a deep evaluation
network. The independent optimization of the search and evaluation networks,
however, leaves room for potential improvement by allowing interaction between
the two networks. To address the problematic optimization issue, we propose new
joint optimization objectives and a novel Cyclic Differentiable ARchiTecture
Search framework, dubbed CDARTS. Considering the structure difference, CDARTS
builds a cyclic feedback mechanism between the search and evaluation networks
with introspective distillation. First, the search network generates an initial
architecture for evaluation, and the weights of the evaluation network are
optimized. Second, the architecture weights in the search network are further
optimized by the label supervision in classification, as well as the
regularization from the evaluation network through feature distillation.
Repeating the above cycle results in joint optimization of the search and
evaluation networks and thus enables the evolution of the architecture to fit
the final evaluation network. The experiments and analysis on CIFAR, ImageNet
and NAS-Bench-201 demonstrate the effectiveness of the proposed approach over
the state-of-the-art ones. Specifically, in the DARTS search space, we achieve
97.52% top-1 accuracy on CIFAR10 and 76.3% top-1 accuracy on ImageNet. In the
chain-structured search space, we achieve 78.2% top-1 accuracy on ImageNet,
which is 1.1% higher than EfficientNet-B0. Our code and models are publicly
available at https://github.com/microsoft/Cream.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards. (arXiv:2008.02693v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.02693">
<div class="article-summary-box-inner">
<span><p>Generating accurate descriptions for online fashion items is important not
only for enhancing customers' shopping experiences, but also for the increase
of online sales. Besides the need of correctly presenting the attributes of
items, the expressions in an enchanting style could better attract customer
interests. The goal of this work is to develop a novel learning framework for
accurate and expressive fashion captioning. Different from popular work on
image captioning, it is hard to identify and describe the rich attributes of
fashion items. We seed the description of an item by first identifying its
attributes, and introduce attribute-level semantic (ALS) reward and
sentence-level semantic (SLS) reward as metrics to improve the quality of text
descriptions. We further integrate the training of our model with maximum
likelihood estimation (MLE), attribute embedding, and Reinforcement Learning
(RL). To facilitate the learning, we build a new FAshion CAptioning Dataset
(FACAD), which contains 993K images and 130K corresponding enchanting and
diverse descriptions. Experiments on FACAD demonstrate the effectiveness of our
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Facial Landmark Detection and Applications: A Survey. (arXiv:2101.10808v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10808">
<div class="article-summary-box-inner">
<span><p>Dense facial landmark detection is one of the key elements of face processing
pipeline. It is used in virtual face reenactment, emotion recognition, driver
status tracking, etc. Early approaches were suitable for facial landmark
detection in controlled environments only, which is clearly insufficient.
Neural networks have shown an astonishing qualitative improvement for
in-the-wild face landmark detection problem, and are now being studied by many
researchers in the field. Numerous bright ideas are proposed, often
complimentary to each other. However, exploration of the whole volume of novel
approaches is quite challenging. Therefore, we present this survey, where we
summarize state-of-the-art algorithms into categories, provide a comparison of
recently introduced in-the-wild datasets (e.g., 300W, AFLW, COFW, WFLW) that
contain images with large pose, face occlusion, taken in unconstrained
conditions. In addition to quality, applications require fast inference, and
preferably on mobile devices. Hence, we include information about algorithm
inference speed both on desktop and mobile hardware, which is rarely studied.
Importantly, we highlight problems of algorithms, their applications,
vulnerabilities, and briefly touch on established methods. We hope that the
reader will find many novel ideas, will see how the algorithms are used in
applications, which will enable further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Face Recognition: Human vs. Machine. (arXiv:2103.01924v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01924">
<div class="article-summary-box-inner">
<span><p>The recent COVID-19 pandemic has increased the focus on hygienic and
contactless identity verification methods. However, the pandemic led to the
wide use of face masks, essential to keep the pandemic under control. The
effect of wearing a mask on face recognition (FR) in a collaborative
environment is a currently sensitive yet understudied issue. Recent reports
have tackled this by evaluating the masked probe effect on the performance of
automatic FR solutions. However, such solutions can fail in certain processes,
leading to performing the verification task by a human expert. This work
provides a joint evaluation and in-depth analyses of the face verification
performance of human experts in comparison to state-of-the-art automatic FR
solutions. This involves an extensive evaluation by human experts and 4
automatic recognition solutions. The study concludes with a set of take-home
messages on different aspects of the correlation between the verification
behavior of humans and machines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anytime Dense Prediction with Confidence Adaptivity. (arXiv:2104.00749v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00749">
<div class="article-summary-box-inner">
<span><p>Anytime inference requires a model to make a progression of predictions which
might be halted at any time. Prior research on anytime visual recognition has
mostly focused on image classification. We propose the first unified and
end-to-end approach for anytime dense prediction. A cascade of "exits" is
attached to the model to make multiple predictions. We redesign the exits to
account for the depth and spatial resolution of the features for each exit. To
reduce total computation, and make full use of prior predictions, we develop a
novel spatially adaptive approach to avoid further computation on regions where
early predictions are already sufficiently confident. Our full method, named
anytime dense prediction with confidence (ADP-C), achieves the same level of
final accuracy as the base model, and meanwhile significantly reduces total
computation. We evaluate our method on Cityscapes semantic segmentation and
MPII human pose estimation: ADP-C enables anytime inference without sacrificing
accuracy while also reducing the total FLOPs of its base models by 44.4% and
59.1%. We compare with anytime inference by deep equilibrium networks and
feature-based stochastic sampling, showing that ADP-C dominates both across the
accuracy-computation curve. Our code is available at
https://github.com/liuzhuang13/anytime .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in Prosthetic Hand Control. (arXiv:2104.03893v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03893">
<div class="article-summary-box-inner">
<span><p>For lower arm amputees, robotic prosthetic hands offer the promise to regain
the capability to perform fine object manipulation in activities of daily
living. Current control methods based on physiological signals such as EEG and
EMG are prone to poor inference outcomes due to motion artifacts, variability
of skin electrode junction impedance over time, muscle fatigue, and other
factors. Visual evidence is also susceptible to its own artifacts, most often
due to object occlusion, lighting changes, variable shapes of objects depending
on view-angle, among other factors. Multimodal evidence fusion using
physiological and vision sensor measurements is a natural approach due to the
complementary strengths of these modalities.
</p>
<p>In this paper, we present a Bayesian evidence fusion framework for grasp
intent inference using eye-view video, gaze, and EMG from the forearm processed
by neural network models. We analyze individual and fused performance as a
function of time as the hand approaches the object to grasp it. For this
purpose, we have also developed novel data processing and augmentation
techniques to train neural network components. Our experimental data analyses
demonstrate that EMG and visual evidence show complementary strengths, and as a
consequence, fusion of multimodal evidence can outperform each individual
evidence modality at any given time. Specifically, results indicate that, on
average, fusion improves the instantaneous upcoming grasp type classification
accuracy while in the reaching phase by 13.66% and 14.8%, relative to EMG and
visual evidence individually, resulting in an overall fusion accuracy of 95.3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Rainfall Estimation from Automotive Lidar. (arXiv:2104.11467v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11467">
<div class="article-summary-box-inner">
<span><p>Robust sensing and perception in adverse weather conditions remain one of the
biggest challenges for realizing reliable autonomous vehicle mobility services.
Prior work has established that rainfall rate is a useful measure for the
adversity of atmospheric weather conditions. This work presents a probabilistic
hierarchical Bayesian model that infers rainfall rate from automotive lidar
point cloud sequences with high accuracy and reliability. The model is a
hierarchical mixture of experts model, or a probabilistic decision tree, with
gating and expert nodes consisting of variational logistic and linear
regression models. Experimental data used to train and evaluate the model is
collected in a large-scale rainfall experiment facility from both stationary
and moving vehicle platforms. The results show prediction accuracy comparable
to the measurement resolution of a disdrometer, and the soundness and
usefulness of the uncertainty estimation. The model achieves RMSE 2.42\,mm/h
after filtering out uncertain predictions. The error is comparable to the mean
rainfall rate change of 3.5\,mm/h between measurements. Model parameter studies
show how predictive performance changes with tree depth, sampling duration, and
crop box dimension. A second experiment demonstrates the predictability of
higher rainfall above 300\,mm/h using a different lidar sensor, demonstrating
sensor independence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention and Prediction Guided Motion Detection for Low-Contrast Small Moving Targets. (arXiv:2104.13018v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13018">
<div class="article-summary-box-inner">
<span><p>Small target motion detection within complex natural environments is an
extremely challenging task for autonomous robots. Surprisingly, the visual
systems of insects have evolved to be highly efficient in detecting mates and
tracking prey, even though targets occupy as small as a few degrees of their
visual fields. The excellent sensitivity to small target motion relies on a
class of specialized neurons called small target motion detectors (STMDs).
However, existing STMD-based models are heavily dependent on visual contrast
and perform poorly in complex natural environments where small targets
generally exhibit extremely low contrast against neighbouring backgrounds. In
this paper, we develop an attention and prediction guided visual system to
overcome this limitation. The developed visual system comprises three main
subsystems, namely, an attention module, an STMD-based neural network, and a
prediction module. The attention module searches for potential small targets in
the predicted areas of the input image and enhances their contrast against
complex background. The STMD-based neural network receives the
contrast-enhanced image and discriminates small moving targets from background
false positives. The prediction module foresees future positions of the
detected targets and generates a prediction map for the attention module. The
three subsystems are connected in a recurrent architecture allowing information
to be processed sequentially to activate specific areas for small target
detection. Extensive experiments on synthetic and real-world datasets
demonstrate the effectiveness and superiority of the proposed visual system for
detecting small, low-contrast moving targets against complex natural
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Clustering via Building Consensus. (arXiv:2105.01289v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01289">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on unsupervised representation learning for
clustering of images. Recent advances in deep clustering and unsupervised
representation learning are based on the idea that different views of an input
image (generated through data augmentation techniques) must be close in the
representation space (exemplar consistency), and/or similar images must have
similar cluster assignments (population consistency). We define an additional
notion of consistency, consensus consistency, which ensures that
representations are learned to induce similar partitions for variations in the
representation space, different clustering algorithms or different
initializations of a single clustering algorithm. We define a clustering loss
by executing variations in the representation space and seamlessly integrate
all three consistencies (consensus, exemplar and population) into an end-to-end
learning framework. The proposed algorithm, consensus clustering using
unsupervised representation learning (ConCURL), improves upon the clustering
performance of state-of-the-art methods on four out of five image datasets.
Furthermore, we extend the evaluation procedure for clustering to reflect the
challenges encountered in real-world clustering tasks, such as maintaining
clustering performance in cases with distribution shifts. We also perform a
detailed ablation study for a deeper understanding of the proposed algorithm.
The code and the trained models are available at
https://github.com/JayanthRR/ConCURL_NCE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The DEVIL is in the Details: A Diagnostic Evaluation Benchmark for Video Inpainting. (arXiv:2105.05332v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05332">
<div class="article-summary-box-inner">
<span><p>Quantitative evaluation has increased dramatically among recent video
inpainting work, but the video and mask content used to gauge performance has
received relatively little attention. Although attributes such as camera and
background scene motion inherently change the difficulty of the task and affect
methods differently, existing evaluation schemes fail to control for them,
thereby providing minimal insight into inpainting failure modes. To address
this gap, we propose the Diagnostic Evaluation of Video Inpainting on
Landscapes (DEVIL) benchmark, which consists of two contributions: (i) a novel
dataset of videos and masks labeled according to several key inpainting failure
modes, and (ii) an evaluation scheme that samples slices of the dataset
characterized by a fixed content attribute, and scores performance on each
slice according to reconstruction, realism, and temporal consistency quality.
By revealing systematic changes in performance induced by particular
characteristics of the input content, our challenging benchmark enables more
insightful analysis into video inpainting methods and serves as an invaluable
diagnostic tool for the field. Our code and data are available at
https://github.com/MichiganCOG/devil .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alpha Matte Generation from Single Input for Portrait Matting. (arXiv:2106.03210v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03210">
<div class="article-summary-box-inner">
<span><p>In the portrait matting, the goal is to predict an alpha matte that
identifies the effect of each pixel on the foreground subject. Traditional
approaches and most of the existing works utilized an additional input, e.g.,
trimap, background image, to predict alpha matte. However, (1) providing
additional input is not always practical, and (2) models are too sensitive to
these additional inputs. To address these points, in this paper, we introduce
an additional input-free approach to perform portrait matting. We divide the
task into two subtasks, segmentation and alpha matte prediction. We first
generate a coarse segmentation map from the input image and then predict the
alpha matte by utilizing the image and segmentation map. Besides, we present a
segmentation encoding block to downsample the coarse segmentation map and
provide useful feature representation to the residual block, since using a
single encoder causes the vanishing of the segmentation information. We tested
our model on four different benchmark datasets. The proposed method
outperformed the MODNet and MGMatting methods that also take a single input.
Besides, we obtained comparable results with BGM-V2 and FBA methods that
require additional input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image2Point: 3D Point-Cloud Understanding with 2D Image Pretrained Models. (arXiv:2106.04180v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04180">
<div class="article-summary-box-inner">
<span><p>3D point-clouds and 2D images are different visual representations of the
physical world. While human vision can understand both representations,
computer vision models designed for 2D image and 3D point-cloud understanding
are quite different. Our paper explores the potential of transferring 2D model
architectures and weights to understand 3D point-clouds, by empirically
investigating the feasibility of the transfer, the benefits of the transfer,
and shedding light on why the transfer works. We discover that we can indeed
use the same architecture and pretrained weights of a neural net model to
understand both images and point-clouds. Specifically, we transfer the
image-pretrained model to a point-cloud model by copying or inflating the
weights. We find that finetuning the transformed image-pretrained models (FIP)
with minimal efforts -- only on input, output, and normalization layers -- can
achieve competitive performance on 3D point-cloud classification, beating a
wide range of point-cloud models that adopt task-specific architectures and use
a variety of tricks. When finetuning the whole model, the performance improves
even further. Meanwhile, FIP improves data efficiency, reaching up to 10.0
top-1 accuracy percent on few-shot classification. It also speeds up the
training of point-cloud models by up to 11.1x for a target accuracy (e.g., 90 %
accuracy). Lastly, we provide an explanation of the image to point-cloud
transfer from the aspect of neural collapse. The code is available at:
\url{https://github.com/chenfengxu714/image2point}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steerable Partial Differential Operators for Equivariant Neural Networks. (arXiv:2106.10163v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10163">
<div class="article-summary-box-inner">
<span><p>Recent work in equivariant deep learning bears strong similarities to
physics. Fields over a base space are fundamental entities in both subjects, as
are equivariant maps between these fields. In deep learning, however, these
maps are usually defined by convolutions with a kernel, whereas they are
partial differential operators (PDOs) in physics. Developing the theory of
equivariant PDOs in the context of deep learning could bring these subjects
even closer together and lead to a stronger flow of ideas. In this work, we
derive a $G$-steerability constraint that completely characterizes when a PDO
between feature vector fields is equivariant, for arbitrary symmetry groups
$G$. We then fully solve this constraint for several important groups. We use
our solutions as equivariant drop-in replacements for convolutional layers and
benchmark them in that role. Finally, we develop a framework for equivariant
maps based on Schwartz distributions that unifies classical convolutions and
differential operators and gives insight about the relation between the two.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Deep Neural Network based Photometry and Astrometry Framework for Wide Field Small Aperture Telescopes. (arXiv:2106.14349v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14349">
<div class="article-summary-box-inner">
<span><p>Wide field small aperture telescopes (WFSATs) are preferable observation
instruments for time domain astronomy, because they could obtain images of
celestial objects with high cadence in a cost-effective way. An automatic data
processing algorithm which could detect celestial objects and obtain their
positions and magnitudes from observed images is important for further
scientific research. In this paper, we extend the ability of a deep neural
network based astronomical target detection algorithm to make it suitable for
photometry and astrometry, by adding two new branches. Because the photometry
and astrometry neural network are data-driven regression algorithms, limited
training data with limited diversity would introduce the epistemic uncertainty
to final regression results. Therefore, we further investigate the epistemic
uncertainty of our algorithm and have found that differences of background
noises and differences of point spread functions between the training data and
the real would introduce uncertainties to final measurements. To reduce this
effect, we propose to use transfer learning strategy to train the neural
network with real data. The algorithm proposed in this paper could obtain
types, positions and magnitudes of celestial objects with high accuracy and
cost around 0.125 second to process an image, regardless of its size. The
algorithm could be integrated into data processing pipelines of WFSATs to
increase their response speed and detection ability to time-domain astronomical
events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SinGAN-Seg: Synthetic training data generation for medical image segmentation. (arXiv:2107.00471v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00471">
<div class="article-summary-box-inner">
<span><p>Analyzing medical data to find abnormalities is a time-consuming and costly
task, particularly for rare abnormalities, requiring tremendous efforts from
medical experts. Artificial intelligence has become a popular tool for the
automatic processing of medical data, acting as a supportive tool for doctors.
However, the machine learning models used to build these tools are highly
dependent on the data used to train them. Large amounts of data can be
difficult to obtain in medicine due to privacy, expensive and time-consuming
annotations, and a general lack of data samples for infrequent lesions. Here,
we present a novel synthetic data generation pipeline, called SinGAN-Seg, to
produce synthetic medical images with corresponding masks using a single
training image. Our method is different from the traditional GANs because our
model needs only a single image and the corresponding ground truth to train.
Our method produces alternative artificial segmentation datasets with ground
truth masks when real datasets are not allowed to share. The pipeline is
evaluated using qualitative and quantitative comparisons between real and
synthetic data to show that the style transfer technique used in our pipeline
significantly improves the quality of the generated data and our method is
better than other state-of-the-art GANs to prepare synthetic images when the
size of training datasets are limited. By training UNet++ using both real and
the synthetic data generated from the SinGAN-Seg pipeline, we show that models
trained with synthetic data have very close performances to those trained on
real data when the datasets have a considerable amount of data. In contrast,
Synthetic data generated from the SinGAN-Seg pipeline can improve the
performance of segmentation models when training datasets do not have a
considerable amount of data. The code is available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03429">
<div class="article-summary-box-inner">
<span><p>The success of neural networks on medical image segmentation tasks typically
relies on large labeled datasets for model training. However, acquiring and
manually labeling a large medical image set is resource-intensive, expensive,
and sometimes impractical due to data sharing and privacy issues. To address
this challenge, we propose AdvChain, a generic adversarial data augmentation
framework, aiming at improving both the diversity and effectiveness of training
data for medical image segmentation tasks. AdvChain augments data with dynamic
data augmentation, generating randomly chained photo-metric and geometric
transformations to resemble realistic yet challenging imaging variations to
expand training data. By jointly optimizing the data augmentation model and a
segmentation network during training, challenging examples are generated to
enhance network generalizability for the downstream task. The proposed
adversarial data augmentation does not rely on generative networks and can be
used as a plug-in module in general segmentation networks. It is
computationally efficient and applicable for both low-shot supervised and
semi-supervised learning. We analyze and evaluate the method on two MR image
segmentation tasks: cardiac segmentation and prostate segmentation with limited
labeled data. Results show that the proposed approach can alleviate the need
for labeled data while improving model generalization ability, indicating its
practical value in medical imaging applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LGD: Label-guided Self-distillation for Object Detection. (arXiv:2109.11496v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11496">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose the first self-distillation framework for general
object detection, termed LGD (Label-Guided self-Distillation). Previous studies
rely on a strong pretrained teacher to provide instructive knowledge that could
be unavailable in real-world scenarios. Instead, we generate an instructive
knowledge based only on student representations and regular labels. Our
framework includes sparse label-appearance encoder, inter-object relation
adapter and intra-object knowledge mapper that jointly form an implicit teacher
at training phase, dynamically dependent on labels and evolving student
representations. They are trained end-to-end with detector and discarded in
inference. Experimentally, LGD obtains decent results on various detectors,
datasets, and extensive tasks like instance segmentation. For example in
MS-COCO dataset, LGD improves RetinaNet with ResNet-50 under 2x single-scale
training from 36.2% to 39.0% mAP (+ 2.8%). It boosts much stronger detectors
like FCOS with ResNeXt-101 DCN v2 under 2x multi-scale training from 46.1% to
47.9% (+ 1.8%). Compared with a classical teacher-based method FGFI, LGD not
only performs better without requiring pretrained teacher but also reduces 51%
training cost beyond inherent student learning. Codes are available at
https://github.com/megvii-research/LGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Study on Transfer Learning Capabilities for Pneumonia Classification in Chest-X-Rays Image. (arXiv:2110.02780v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02780">
<div class="article-summary-box-inner">
<span><p>Over the last year, the severe acute respiratory syndrome coronavirus-2
(SARS-CoV-2) and its variants have highlighted the importance of screening
tools with high diagnostic accuracy for new illnesses such as COVID-19. To that
regard, deep learning approaches have proven as effective solutions for
pneumonia classification, especially when considering chest-x-rays images.
However, this lung infection can also be caused by other viral, bacterial or
fungi pathogens. Consequently, efforts are being poured toward distinguishing
the infection source to help clinicians to diagnose the correct disease origin.
Following this tendency, this study further explores the effectiveness of
established neural network architectures on the pneumonia classification task
through the transfer learning paradigm. To present a comprehensive comparison,
12 well-known ImageNet pre-trained models were fine-tuned and used to
discriminate among chest-x-rays of healthy people, and those showing pneumonia
symptoms derived from either a viral (i.e., generic or SARS-CoV-2) or bacterial
source. Furthermore, since a common public collection distinguishing between
such categories is currently not available, two distinct datasets of
chest-x-rays images, describing the aforementioned sources, were combined and
employed to evaluate the various architectures. The experiments were performed
using a total of 6330 images split between train, validation and test sets. For
all models, common classification metrics were computed (e.g., precision,
f1-score) and most architectures obtained significant performances, reaching,
among the others, up to 84.46% average f1-score when discriminating the 4
identified classes. Moreover, confusion matrices and activation maps computed
via the Grad-CAM algorithm were also reported to present an informed discussion
on the networks classifications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network Augmentation for Tiny Deep Learning. (arXiv:2110.08890v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08890">
<div class="article-summary-box-inner">
<span><p>We introduce Network Augmentation (NetAug), a new training method for
improving the performance of tiny neural networks. Existing regularization
techniques (e.g., data augmentation, dropout) have shown much success on large
neural networks by adding noise to overcome over-fitting. However, we found
these techniques hurt the performance of tiny neural networks. We argue that
training tiny models are different from large models: rather than augmenting
the data, we should augment the model, since tiny models tend to suffer from
under-fitting rather than over-fitting due to limited capacity. To alleviate
this issue, NetAug augments the network (reverse dropout) instead of inserting
noise into the dataset or the network. It puts the tiny model into larger
models and encourages it to work as a sub-model of larger models to get extra
supervision, in addition to functioning as an independent model. At test time,
only the tiny model is used for inference, incurring zero inference overhead.
We demonstrate the effectiveness of NetAug on image classification and object
detection. NetAug consistently improves the performance of tiny models,
achieving up to 2.2% accuracy improvement on ImageNet. On object detection,
achieving the same level of performance, NetAug requires 41% fewer MACs on
Pascal VOC and 38% fewer MACs on COCO than the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Dimensional Collapse in Contrastive Self-supervised Learning. (arXiv:2110.09348v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09348">
<div class="article-summary-box-inner">
<span><p>Self-supervised visual representation learning aims to learn useful
representations without relying on human annotations. Joint embedding approach
bases on maximizing the agreement between embedding vectors from different
views of the same image. Various methods have been proposed to solve the
collapsing problem where all embedding vectors collapse to a trivial constant
solution. Among these methods, contrastive learning prevents collapse via
negative sample pairs. It has been shown that non-contrastive methods suffer
from a lesser collapse problem of a different nature: dimensional collapse,
whereby the embedding vectors end up spanning a lower-dimensional subspace
instead of the entire available embedding space. Here, we show that dimensional
collapse also happens in contrastive learning. In this paper, we shed light on
the dynamics at play in contrastive learning that leads to dimensional
collapse. Inspired by our theory, we propose a novel contrastive learning
method, called DirectCLR, which directly optimizes the representation space
without relying on an explicit trainable projector. Experiments show that
DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DBSegment: Fast and robust segmentation of deep brain structures -- Evaluation of transportability across acquisition domains. (arXiv:2110.09473v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09473">
<div class="article-summary-box-inner">
<span><p>Segmenting deep brain structures from magnetic resonance images is important
for patient diagnosis, surgical planning, and research. Most current
state-of-the-art solutions follow a segmentation-by-registration approach,
where subject MRIs are mapped to a template with well-defined segmentations.
However, registration-based pipelines are time-consuming, thus, limiting their
clinical use. This paper uses deep learning to provide a robust and efficient
deep brain segmentation solution. The method consists of a pre-processing step
to conform all MRI images to the same orientation, followed by a convolutional
neural network using the nnU-Net framework. We use a total of 14 datasets from
both research and clinical collections. Of these, seven were used for training
and validation and seven were retained for independent testing. We trained the
network to segment 30 deep brain structures, as well as a brain mask, using
labels generated from a registration-based approach. We evaluated the
generalizability of the network by performing a leave-one-dataset-out
cross-validation, and extensive testing on external datasets. Furthermore, we
assessed cross-domain transportability by evaluating the results separately on
different domains. We achieved an average DSC of 0.89 $\pm$ 0.04 on the
independent testing datasets when compared to the registration-based gold
standard. On our test system, the computation time decreased from 42 minutes
for a reference registration-based pipeline to 1 minute. Our proposed method is
fast, robust, and generalizes with high reliability. It can be extended to the
segmentation of other brain structures. The method is publicly available on
GitHub, as well as a pip package for convenient usage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarks for Corruption Invariant Person Re-identification. (arXiv:2111.00880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00880">
<div class="article-summary-box-inner">
<span><p>When deploying person re-identification (ReID) model in safety-critical
applications, it is pivotal to understanding the robustness of the model
against a diverse array of image corruptions. However, current evaluations of
person ReID only consider the performance on clean datasets and ignore images
in various corrupted scenarios. In this work, we comprehensively establish six
ReID benchmarks for learning corruption invariant representation. In the field
of ReID, we are the first to conduct an exhaustive study on corruption
invariant learning in single- and cross-modality datasets, including
Market-1501, CUHK03, MSMT17, RegDB, SYSU-MM01. After reproducing and examining
the robustness performance of 21 recent ReID methods, we have some
observations: 1) transformer-based models are more robust towards corrupted
images, compared with CNN-based models, 2) increasing the probability of random
erasing (a commonly used augmentation method) hurts model corruption
robustness, 3) cross-dataset generalization improves with corruption robustness
increases. By analyzing the above observations, we propose a strong baseline on
both single- and cross-modality ReID datasets which achieves improved
robustness against diverse corruptions. Our codes are available on
https://github.com/MinghuiChen43/CIL-ReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are we ready for a new paradigm shift? A Survey on Visual Deep MLP. (arXiv:2111.04060v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04060">
<div class="article-summary-box-inner">
<span><p>Recently, the proposed deep MLP models have stirred up a lot of interest in
the vision community. Historically, the availability of larger datasets
combined with increased computing capacity leads to paradigm shifts. This
review paper provides detailed discussions on whether MLP can be a new paradigm
for computer vision. We compare the intrinsic connections and differences
between convolution, self-attention mechanism, and Token-mixing MLP in detail.
Advantages and limitations of Token-mixing MLP are provided, followed by
careful analysis of recent MLP-like variants, from module design to network
architecture, and their applications. In the GPU era, the locally and globally
weighted summations are the current mainstreams, represented by the convolution
and self-attention mechanism, as well as MLP. We suggest the further
development of paradigm to be considered alongside the next-generation
computing devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability. (arXiv:2111.10752v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10752">
<div class="article-summary-box-inner">
<span><p>The black-box adversarial attack has attracted impressive attention for its
practical use in the field of deep learning security. Meanwhile, it is very
challenging as there is no access to the network architecture or internal
weights of the target model. Based on the hypothesis that if an example remains
adversarial for multiple models, then it is more likely to transfer the attack
capability to other models, the ensemble-based adversarial attack methods are
efficient and widely used for black-box attacks. However, ways of ensemble
attack are rather less investigated, and existing ensemble attacks simply fuse
the outputs of all the models evenly. In this work, we treat the iterative
ensemble attack as a stochastic gradient descent optimization process, in which
the variance of the gradients on different models may lead to poor local
optima. To this end, we propose a novel attack method called the stochastic
variance reduced ensemble (SVRE) attack, which could reduce the gradient
variance of the ensemble models and take full advantage of the ensemble attack.
Empirical results on the standard ImageNet dataset demonstrate that the
proposed method could boost the adversarial transferability and outperforms
existing ensemble attacks significantly. Code is available at
https://github.com/JHL-HUST/SVRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decentralized Unsupervised Learning of Visual Representations. (arXiv:2111.10763v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10763">
<div class="article-summary-box-inner">
<span><p>Collaborative learning enables distributed clients to learn a shared model
for prediction while keeping the training data local on each client. However,
existing collaborative learning methods require fully-labeled data for
training, which is inconvenient or sometimes infeasible to obtain due to the
high labeling cost and the requirement of expertise. The lack of labels makes
collaborative learning impractical in many realistic settings. Self-supervised
learning can address this challenge by learning from unlabeled data.
Contrastive learning (CL), a self-supervised learning approach, can effectively
learn visual representations from unlabeled image data. However, the
distributed data collected on clients are usually not independent and
identically distributed (non-IID) among clients, and each client may only have
few classes of data, which degrades the performance of CL and learned
representations. To tackle this problem, we propose a collaborative contrastive
learning framework consisting of two approaches: feature fusion and
neighborhood matching, by which a unified feature space among clients is
learned for better data representations. Feature fusion provides remote
features as accurate contrastive information to each client for better local
learning. Neighborhood matching further aligns each client's local features to
the remote features such that well-clustered features among clients can be
learned. Extensive experiments show the effectiveness of the proposed
framework. It outperforms other methods by 11% on IID data and matches the
performance of centralized learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JoinABLe: Learning Bottom-up Assembly of Parametric CAD Joints. (arXiv:2111.12772v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12772">
<div class="article-summary-box-inner">
<span><p>Physical products are often complex assemblies combining a multitude of 3D
parts modeled in computer-aided design (CAD) software. CAD designers build up
these assemblies by aligning individual parts to one another using constraints
called joints. In this paper we introduce JoinABLe, a learning-based method
that assembles parts together to form joints. JoinABLe uses the weak
supervision available in standard parametric CAD files without the help of
object class labels or human guidance. Our results show that by making network
predictions over a graph representation of solid models we can outperform
multiple baseline methods with an accuracy (79.53%) that approaches human
performance (80%). Finally, to support future research we release the Fusion
360 Gallery assembly dataset, containing assemblies with rich information on
joints, contact surfaces, holes, and the underlying assembly graph structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Temporal Gradient for Semi-supervised Action Recognition. (arXiv:2111.13241v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13241">
<div class="article-summary-box-inner">
<span><p>Semi-supervised video action recognition tends to enable deep neural networks
to achieve remarkable performance even with very limited labeled data. However,
existing methods are mainly transferred from current image-based methods (e.g.,
FixMatch). Without specifically utilizing the temporal dynamics and inherent
multimodal attributes, their results could be suboptimal. To better leverage
the encoded temporal information in videos, we introduce temporal gradient as
an additional modality for more attentive feature extraction in this paper. To
be specific, our method explicitly distills the fine-grained motion
representations from temporal gradient (TG) and imposes consistency across
different modalities (i.e., RGB and TG). The performance of semi-supervised
action recognition is significantly improved without additional computation or
parameters during inference. Our method achieves the state-of-the-art
performance on three video action recognition benchmarks (i.e., Kinetics-400,
UCF-101, and HMDB-51) under several typical semi-supervised settings (i.e.,
different ratios of labeled data).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Batch Normalization Tells You Which Filter is Important. (arXiv:2112.01155v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01155">
<div class="article-summary-box-inner">
<span><p>The goal of filter pruning is to search for unimportant filters to remove in
order to make convolutional neural networks (CNNs) efficient without
sacrificing the performance in the process. The challenge lies in finding
information that can help determine how important or relevant each filter is
with respect to the final output of neural networks. In this work, we share our
observation that the batch normalization (BN) parameters of pre-trained CNNs
can be used to estimate the feature distribution of activation outputs, without
processing of training data. Upon observation, we propose a simple yet
effective filter pruning method by evaluating the importance of each filter
based on the BN parameters of pre-trained CNNs. The experimental results on
CIFAR-10 and ImageNet demonstrate that the proposed method can achieve
outstanding performance with and without fine-tuning in terms of the trade-off
between the accuracy drop and the reduction in computational complexity and
number of parameters of pruned networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Unconditional to Conditional GANs with Hyper-Modulation. (arXiv:2112.02219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02219">
<div class="article-summary-box-inner">
<span><p>GANs have matured in recent years and are able to generate high-resolution,
realistic images. However, the computational resources and the data required
for the training of high-quality GANs are enormous, and the study of transfer
learning of these models is therefore an urgent topic. Many of the available
high-quality pretrained GANs are unconditional (like StyleGAN). For many
applications, however, conditional GANs are preferable, because they provide
more control over the generation process, despite often suffering more training
difficulties. Therefore, in this paper, we focus on transferring from
high-quality pretrained unconditional GANs to conditional GANs. This requires
architectural adaptation of the pretrained GAN to perform the conditioning. To
this end, we propose hyper-modulated generative networks that allow for shared
and complementary supervision. To prevent the additional weights of the
hypernetwork to overfit, with subsequent mode collapse on small target domains,
we introduce a self-initialization procedure that does not require any real
data to initialize the hypernetwork parameters. To further improve the sample
efficiency of the transfer, we apply contrastive learning in the discriminator,
which effectively works on very limited batch sizes. In extensive experiments,
we validate the efficiency of the hypernetworks, self-initialization and
contrastive loss for knowledge transfer on standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Adaptive Projection with Pretrained Features for Anomaly Detection. (arXiv:2112.02597v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02597">
<div class="article-summary-box-inner">
<span><p>Anomaly detection aims to separate anomalies from normal samples, and the
pretrained network is promising for anomaly detection. However, adapting the
pretrained features would be confronted with the risk of pattern collapse when
finetuning on one-class training data. In this paper, we propose an anomaly
detection framework called constrained adaptive projection with pretrained
features (CAP). Combined with pretrained features, a simple linear projection
head applied on a specific input and its k most similar pretrained normal
representations is designed for feature adaptation, and a reformed
self-attention is leveraged to mine the inner-relationship among one-class
semantic features. A loss function is proposed to avoid potential pattern
collapse. Concretely, it considers the similarity between a specific data and
its corresponding adaptive normal representation, and incorporates a constraint
term slightly aligning pretrained and adaptive spaces. Our method achieves
state-ofthe-art anomaly detection performance on semantic anomaly detection and
sensory anomaly detection benchmarks including 96.5% AUROC on CIFAR- 100
dataset, 97.0% AUROC on CIFAR-10 dataset and 89.9% AUROC on MvTec dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auxiliary Learning for Self-Supervised Video Representation via Similarity-based Knowledge Distillation. (arXiv:2112.04011v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04011">
<div class="article-summary-box-inner">
<span><p>Despite the outstanding success of self-supervised pretraining methods for
video representation learning, they generalise poorly when the unlabeled
dataset for pretraining is small or the domain difference between unlabelled
data in source task (pretraining) and labeled data in target task (finetuning)
is significant. To mitigate these issues, we propose a novel approach to
complement self-supervised pretraining via an auxiliary pretraining phase,
based on knowledge similarity distillation, auxSKD, for better generalisation
with a significantly smaller amount of video data, e.g. Kinetics-100 rather
than Kinetics-400. Our method deploys a teacher network that iteratively
distills its knowledge to the student model by capturing the similarity
information between segments of unlabelled video data. The student model
meanwhile solves a pretext task by exploiting this prior knowledge. We also
introduce a novel pretext task, Video Segment Pace Prediction or VSPP, which
requires our model to predict the playback speed of a randomly selected segment
of the input video to provide more reliable self-supervised representations.
Our experimental results show superior results to the state of the art on both
UCF101 and HMDB51 datasets when pretraining on K100 in apple-to-apple
comparisons. Additionally, we show that our auxiliary pretraining, auxSKD, when
added as an extra pretraining phase to recent state of the art self-supervised
methods (i.e. VCOP, VideoPace, and RSPNet), improves their results on UCF101
and HMDB51. Our code is available at https://github.com/Plrbear/auxSKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending the WILDS Benchmark for Unsupervised Adaptation. (arXiv:2112.05090v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05090">
<div class="article-summary-box-inner">
<span><p>Machine learning systems deployed in the wild are often trained on a source
distribution but deployed on a different target distribution. Unlabeled data
can be a powerful point of leverage for mitigating these distribution shifts,
as it is frequently much more available than labeled data and can often be
obtained from distributions beyond the source distribution as well. However,
existing distribution shift benchmarks with unlabeled data do not reflect the
breadth of scenarios that arise in real-world applications. In this work, we
present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS
benchmark of distribution shifts to include curated unlabeled data that would
be realistically obtainable in deployment. These datasets span a wide range of
applications (from histology to wildlife conservation), tasks (classification,
regression, and detection), and modalities (photos, satellite images,
microscope slides, text, molecular graphs). The update maintains consistency
with the original WILDS benchmark by using identical labeled training,
validation, and test sets, as well as the evaluation metrics. On these
datasets, we systematically benchmark state-of-the-art methods that leverage
unlabeled data, including domain-invariant, self-training, and self-supervised
methods, and show that their success on WILDS is limited. To facilitate method
development and evaluation, we provide an open-source package that automates
data loading and contains all of the model architectures and methods used in
this paper. Code and leaderboards are available at https://wilds.stanford.edu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMDS-6: Environmental Microorganism Image Dataset Sixth Version for Image Denoising, Segmentation, Feature Extraction, Classification and Detection Methods Evaluation. (arXiv:2112.07111v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07111">
<div class="article-summary-box-inner">
<span><p>Environmental microorganisms (EMs) are ubiquitous around us and have an
important impact on the survival and development of human society. However, the
high standards and strict requirements for the preparation of environmental
microorganism (EM) data have led to the insufficient of existing related
databases, not to mention the databases with GT images. This problem seriously
affects the progress of related experiments. Therefore, This study develops the
Environmental Microorganism Dataset Sixth Version (EMDS-6), which contains 21
types of EMs. Each type of EM contains 40 original and 40 GT images, in total
1680 EM images. In this study, in order to test the effectiveness of EMDS-6. We
choose the classic algorithms of image processing methods such as image
denoising, image segmentation and target detection. The experimental result
shows that EMDS-6 can be used to evaluate the performance of image denoising,
image segmentation, image feature extraction, image classification, and object
detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images. (arXiv:2112.10775v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10775">
<div class="article-summary-box-inner">
<span><p>Multiple medical institutions collaboratively training a model using
federated learning (FL) has become a promising solution for maximizing the
potential of data-driven models, yet the non-independent and identically
distributed (non-iid) data in medical images is still an outstanding challenge
in real-world practice. The feature heterogeneity caused by diverse scanners or
protocols introduces a drift in the learning process, in both local (client)
and global (server) optimizations, which harms the convergence as well as model
performance. Many previous works have attempted to address the non-iid issue by
tackling the drift locally or globally, but how to jointly solve the two
essentially coupled drifts is still unclear. In this work, we concentrate on
handling both local and global drifts and introduce a new harmonizing framework
called HarmoFL. First, we propose to mitigate the local update drift by
normalizing amplitudes of images transformed into the frequency domain to mimic
a unified imaging setting, in order to generate a harmonized feature space
across local clients. Second, based on harmonized features, we design a client
weight perturbation guiding each local model to reach a flat optimum, where a
neighborhood area of the local optimal solution has a uniformly low loss.
Without any extra communication cost, the perturbation assists the global model
to optimize towards a converged optimal solution by aggregating several local
flat optima. We have theoretically analyzed the proposed method and empirically
conducted extensive experiments on three medical image classification and
segmentation tasks, showing that HarmoFL outperforms a set of recent
state-of-the-art methods with promising convergence behavior. Code is available
at https://github.com/med-air/HarmoFL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expansion-Squeeze-Excitation Fusion Network for Elderly Activity Recognition. (arXiv:2112.10992v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10992">
<div class="article-summary-box-inner">
<span><p>This work focuses on the task of elderly activity recognition, which is a
challenging task due to the existence of individual actions and human-object
interactions in elderly activities. Thus, we attempt to effectively aggregate
the discriminative information of actions and interactions from both RGB videos
and skeleton sequences by attentively fusing multi-modal features. Recently,
some nonlinear multi-modal fusion approaches are proposed by utilizing
nonlinear attention mechanism that is extended from Squeeze-and-Excitation
Networks (SENet). Inspired by this, we propose a novel
Expansion-Squeeze-Excitation Fusion Network (ESE-FN) to effectively address the
problem of elderly activity recognition, which learns modal and channel-wise
Expansion-Squeeze-Excitation (ESE) attentions for attentively fusing the
multi-modal features in the modal and channel-wise ways. Furthermore, we design
a new Multi-modal Loss (ML) to keep the consistency between the single-modal
features and the fused multi-modal features by adding the penalty of difference
between the minimum prediction losses on single modalities and the prediction
loss on the fused modality. Finally, we conduct experiments on a largest-scale
elderly activity dataset, i.e., ETRI-Activity3D (including 110,000+ videos, and
50+ categories), to demonstrate that the proposed ESE-FN achieves the best
accuracy compared with the state-of-the-art methods. In addition, more
extensive experimental results show that the proposed ESE-FN is also comparable
to the other methods in terms of normal action recognition task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space. (arXiv:2201.00814v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00814">
<div class="article-summary-box-inner">
<span><p>This paper explores the feasibility of finding an optimal sub-model from a
vision transformer and introduces a pure vision transformer slimming (ViT-Slim)
framework. It can search a sub-structure from the original model end-to-end
across multiple dimensions, including the input tokens, MHSA and MLP modules
with state-of-the-art performance. Our method is based on a learnable and
unified $\ell_1$ sparsity constraint with pre-defined factors to reflect the
global importance in the continuous searching space of different dimensions.
The searching process is highly efficient through a single-shot training
scheme. For instance, on DeiT-S, ViT-Slim only takes ~43 GPU hours for the
searching process, and the searched structure is flexible with diverse
dimensionalities in different modules. Then, a budget threshold is employed
according to the requirements of accuracy-FLOPs trade-off on running devices,
and a re-training process is performed to obtain the final model. The extensive
experiments show that our ViT-Slim can compress up to 40% of parameters and 40%
FLOPs on various vision transformers while increasing the accuracy by ~0.6% on
ImageNet. We also demonstrate the advantage of our searched models on several
downstream datasets. Our code is available at
https://github.com/Arnav0400/ViT-Slim.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Transformers for Video Recognition. (arXiv:2201.04288v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04288">
<div class="article-summary-box-inner">
<span><p>Video understanding requires reasoning at multiple spatiotemporal resolutions
-- from short fine-grained motions to events taking place over longer
durations. Although transformer architectures have recently advanced the
state-of-the-art, they have not explicitly modelled different spatiotemporal
resolutions. To this end, we present Multiview Transformers for Video
Recognition (MTV). Our model consists of separate encoders to represent
different views of the input video with lateral connections to fuse information
across views. We present thorough ablation studies of our model and show that
MTV consistently performs better than single-view counterparts in terms of
accuracy and computational cost across a range of model sizes. Furthermore, we
achieve state-of-the-art results on six standard datasets, and improve even
further with large-scale pretraining. Code and checkpoints are available at:
\href{https://github.com/google-research/scenic/tree/main/scenic/projects/mtv}{https://github.com/google-research/scenic}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images. (arXiv:2201.12785v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12785">
<div class="article-summary-box-inner">
<span><p>Transformer, benefiting from global (long-range) information modeling using
self-attention mechanism, has been successful in natural language processing
and computer vision recently. Convolutional Neural Networks, capable of
capturing local features, are difficult to model explicit long-distance
dependencies from global feature space. However, both local and global features
are crucial for dense prediction tasks, especially for 3D medical image
segmentation. In this paper, we present the further attempt to exploit
Transformer in 3D CNN for 3D medical image volumetric segmentation and propose
a novel network named TransBTSV2 based on the encoder-decoder structure.
Different from TransBTS, the proposed TransBTSV2 is not limited to brain tumor
segmentation (BTS) but focuses on general medical image segmentation, providing
a stronger and more efficient 3D baseline for volumetric segmentation of
medical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can
achieve accurate segmentation of medical images without any pre-training,
possessing the strong inductive bias as CNNs and powerful global context
modeling ability as Transformer. With the proposed insight to redesign the
internal structure of Transformer block and the introduced Deformable
Bottleneck Module to capture shape-aware local details, a highly efficient
architecture is achieved with superior performance. Extensive experimental
results on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and
KiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results
compared to the state-of-the-art methods for the segmentation of brain tumor,
liver tumor as well as kidney tumor. Code will be publicly available at
https://github.com/Wenxuan-1119/TransBTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Pedestrian-Vehicle Interaction for Pedestrian Trajectory Prediction. (arXiv:2202.05334v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05334">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the interaction between pedestrians and vehicles and
propose a novel neural network structure called the Pedestrian-Vehicle
Interaction (PVI) extractor for learning the pedestrian-vehicle interaction. We
implement the proposed PVI extractor on both sequential approaches (long
short-term memory (LSTM) models) and non-sequential approaches (convolutional
models). We use the Waymo Open Dataset that contains real-world urban traffic
scenes with both pedestrian and vehicle annotations. For the LSTM-based models,
our proposed model is compared with Social-LSTM and Social-GAN, and using our
proposed PVI extractor reduces the average displacement error (ADE) and the
final displacement error (FDE) by 7.46% and 5.24%, respectively. For the
convolutional-based models, our proposed model is compared with Social-STGCNN
and Social-IWSTCNN, and using our proposed PVI extractor reduces the ADE and
FDE by 2.10% and 1.27%, respectively. The results show that the
pedestrian-vehicle interaction influences pedestrian behavior, and the models
using the proposed PVI extractor can capture the interaction between
pedestrians and vehicles, and thereby outperform the compared methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A State-of-the-art Survey of U-Net in Microscopic Image Analysis: from Simple Usage to Structure Mortification. (arXiv:2202.06465v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06465">
<div class="article-summary-box-inner">
<span><p>Image analysis technology is used to solve the inadvertences of artificial
traditional methods in disease, wastewater treatment, environmental change
monitoring analysis and convolutional neural networks (CNN) play an important
role in microscopic image analysis. An important step in detection, tracking,
monitoring, feature extraction, modeling and analysis is image segmentation, in
which U-Net has increasingly applied in microscopic image segmentation. This
paper comprehensively reviews the development history of U-Net, and analyzes
various research results of various segmentation methods since the emergence of
U-Net and conducts a comprehensive review of related papers. First, this paper
has summarized the improved methods of U-Net and then listed the existing
significance of image segmentation techniques and their improvements that has
introduced over the years. Finally, focusing on the different improvement
strategies of U-Net in different papers, the related work of each application
target is reviewed according to detailed technical categories to facilitate
future research. Researchers can clearly see the dynamics of transmission of
technological development and keep up with future trends in this
interdisciplinary field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval 2022 Task 12: Symlink- Linking Mathematical Symbols to their Descriptions. (arXiv:2202.09695v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09695">
<div class="article-summary-box-inner">
<span><p>Given the increasing number of livestreaming videos, automatic speech
recognition and post-processing for livestreaming video transcripts are crucial
for efficient data management as well as knowledge mining. A key step in this
process is punctuation restoration which restores fundamental text structures
such as phrase and sentence boundaries from the video transcripts. This work
presents a new human-annotated corpus, called BehancePR, for punctuation
restoration in livestreaming video transcripts. Our experiments on BehancePR
demonstrate the challenges of punctuation restoration for this domain.
Furthermore, we show that popular natural language processing toolkits are
incapable of detecting sentence boundary on non-punctuated transcripts of
livestreaming videos, calling for more research effort to develop robust models
for this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FreeSOLO: Learning to Segment Objects without Annotations. (arXiv:2202.12181v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12181">
<div class="article-summary-box-inner">
<span><p>Instance segmentation is a fundamental vision task that aims to recognize and
segment each object in an image. However, it requires costly annotations such
as bounding boxes and segmentation masks for learning. In this work, we propose
a fully unsupervised learning method that learns class-agnostic instance
segmentation without any annotations. We present FreeSOLO, a self-supervised
instance segmentation framework built on top of the simple instance
segmentation method SOLO. Our method also presents a novel localization-aware
pre-training framework, where objects can be discovered from complicated scenes
in an unsupervised manner. FreeSOLO achieves 9.8% AP_{50} on the challenging
COCO dataset, which even outperforms several segmentation proposal methods that
use manual annotations. For the first time, we demonstrate unsupervised
class-agnostic instance segmentation successfully. FreeSOLO's box localization
significantly outperforms state-of-the-art unsupervised object
detection/discovery methods, with about 100% relative improvements in COCO AP.
FreeSOLO further demonstrates superiority as a strong pre-training method,
outperforming state-of-the-art self-supervised pre-training methods by +9.8% AP
when fine-tuning instance segmentation with only 5% COCO masks. Code is
available at: github.com/NVlabs/FreeSOLO
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QOC: Quantum On-Chip Training with Parameter Shift and Gradient Pruning. (arXiv:2202.13239v2 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13239">
<div class="article-summary-box-inner">
<span><p>Parameterized Quantum Circuits (PQC) are drawing increasing research interest
thanks to its potential to achieve quantum advantages on near-term Noisy
Intermediate Scale Quantum (NISQ) hardware. In order to achieve scalable PQC
learning, the training process needs to be offloaded to real quantum machines
instead of using exponential-cost classical simulators. One common approach to
obtain PQC gradients is parameter shift whose cost scales linearly with the
number of qubits. We present QOC, the first experimental demonstration of
practical on-chip PQC training with parameter shift. Nevertheless, we find that
due to the significant quantum errors (noises) on real machines, gradients
obtained from naive parameter shift have low fidelity and thus degrading the
training accuracy. To this end, we further propose probabilistic gradient
pruning to firstly identify gradients with potentially large errors and then
remove them. Specifically, small gradients have larger relative errors than
large ones, thus having a higher probability to be pruned. We perform extensive
experiments with the Quantum Neural Network (QNN) benchmarks on 5
classification tasks using 5 real quantum machines. The results demonstrate
that our on-chip training achieves over 90% and 60% accuracy for 2-class and
4-class image classification tasks. The probabilistic gradient pruning brings
up to 7% PQC accuracy improvements over no pruning. Overall, we successfully
obtain similar on-chip training accuracy compared with noise-free simulation
but have much better training scalability. The QOC code is available in the
TorchQuantum library.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00859">
<div class="article-summary-box-inner">
<span><p>Recent transformer-based solutions have been introduced to estimate 3D human
pose from 2D keypoint sequence by considering body joints among all frames
globally to learn spatio-temporal correlation. We observe that the motions of
different joints differ significantly. However, the previous methods cannot
efficiently model the solid inter-frame correspondence of each joint, leading
to insufficient learning of spatial-temporal correlation. We propose MixSTE
(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to
separately model the temporal motion of each joint and a spatial transformer
block to learn inter-joint spatial correlation. These two blocks are utilized
alternately to obtain better spatio-temporal feature encoding. In addition, the
network output is extended from the central frame to entire frames of the input
video, thereby improving the coherence between the input and output sequences.
Extensive experiments are conducted on three benchmarks (Human3.6M,
MPI-INF-3DHP, and HumanEva). The results show that our model outperforms the
state-of-the-art approach by 10.9% P-MPJPE and 7.6% MPJPE. The code is
available at https://github.com/JinluZhang1126/MixSTE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Does the Performance Improvement Come From? -- A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03853">
<div class="article-summary-box-inner">
<span><p>This article aims to provide the information retrieval community with some
reflections on recent advances in retrieval learning by analyzing the
reproducibility of image-text retrieval models. Due to the increase of
multimodal data over the last decade, image-text retrieval has steadily become
a major research direction in the field of information retrieval. Numerous
researchers train and evaluate image-text retrieval algorithms using benchmark
datasets such as MS-COCO and Flickr30k. Research in the past has mostly focused
on performance, with multiple state-of-the-art methodologies being suggested in
a variety of ways. According to their assertions, these techniques provide
improved modality interactions and hence more precise multimodal
representations. In contrast to previous works, we focus on the reproducibility
of the approaches and the examination of the elements that lead to improved
performance by pretrained and nonpretrained models in retrieving images and
text. To be more specific, we first examine the related reproducibility
concerns and explain why our focus is on image-text retrieval tasks. Second, we
systematically summarize the current paradigm of image-text retrieval models
and the stated contributions of those approaches. Third, we analyze various
aspects of the reproduction of pretrained and nonpretrained retrieval models.
To complete this, we conducted ablation experiments and obtained some
influencing factors that affect retrieval recall more than the improvement
claimed in the original paper. Finally, we present some reflections and
challenges that the retrieval community should consider in the future. Our
source code is publicly available at
https://github.com/WangFei-2019/Image-text-Retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04275">
<div class="article-summary-box-inner">
<span><p>This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural
Network (CNN) for pose estimation of noncooperative spacecraft across domain
gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared
multi-scale feature encoder and multiple prediction heads that perform
different tasks on a shared feature output. These tasks are all related to
detection and pose estimation of a target spacecraft from an image, such as
prediction of pre-defined satellite keypoints, direct pose regression, and
binary segmentation of the satellite foreground. It is shown that by jointly
training on different yet related tasks with extensive data augmentations on
synthetic images only, the shared encoder learns features that are common
across image domains that have fundamentally different visual characteristics
compared to synthetic images. This work also introduces Online Domain
Refinement (ODR) which refines the parameters of the normalization layers of
SPNv2 on the target domain images online at deployment. Specifically, ODR
performs self-supervised entropy minimization of the predicted satellite
foreground, thereby improving the CNN's performance on the target domain images
without their pose labels and with minimal computational efforts. The GitHub
repository for SPNv2 is available at \url{https://github.com/tpark94/spnv2}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intention-aware Feature Propagation Network for Interactive Segmentation. (arXiv:2203.05145v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05145">
<div class="article-summary-box-inner">
<span><p>We aim to tackle the problem of point-based interactive segmentation, in
which two key challenges are to infer user's intention correctly and to
propagate the user-provided annotations to unlabeled regions efficiently. To
address those challenges, we propose a novel intention-aware feature
propagation strategy that performs explicit user intention estimation and
learns an efficient click-augmented feature representation for high-resolution
foreground segmentation. Specifically, we develop a coarse-to-fine sparse
propagation network for each interactive segmentation step, which consists of a
coarse-level network for more effective tracking of user's interest, and a
fine-level network for zooming to the target object and performing fine-level
segmentation. Moreover, we design a new sparse graph network module for both
levels to enable efficient long-range propagation of click information.
Extensive experiments show that our method surpasses the previous
state-of-the-art methods on all popular benchmarks, demonstrating its efficacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss. (arXiv:2203.10882v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10882">
<div class="article-summary-box-inner">
<span><p>We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the facial video PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Underwater Light Field Retention : Neural Rendering for Underwater Imaging. (arXiv:2203.11006v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11006">
<div class="article-summary-box-inner">
<span><p>Underwater Image Rendering aims to generate a true-tolife underwater image
from a given clean one, which could be applied to various practical
applications such as underwater image enhancement, camera filter, and virtual
gaming. We explore two less-touched but challenging problems in underwater
image rendering, namely, i) how to render diverse underwater scenes by a single
neural network? ii) how to adaptively learn the underwater light fields from
natural exemplars, i,e., realistic underwater images? To this end, we propose a
neural rendering method for underwater imaging, dubbed UWNR (Underwater Neural
Rendering). Specifically, UWNR is a data-driven neural network that implicitly
learns the natural degenerated model from authentic underwater images, avoiding
introducing erroneous biases by hand-craft imaging models. Compared with
existing underwater image generation methods, UWNR utilizes the natural light
field to simulate the main characteristics ofthe underwater scene. Thus, it is
able to synthesize a wide variety ofunderwater images from one clean image with
various realistic underwater images. Extensive experiments demonstrate that our
approach achieves better visual effects and quantitative metrics over previous
methods. Moreover, we adopt UWNR to build an open Large Neural Rendering
Underwater Dataset containing various types of water quality, dubbed LNRUD. The
source code and LNRUD are available at https: //github.com/Ephemeral182/UWNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural Representations. (arXiv:2203.14510v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14510">
<div class="article-summary-box-inner">
<span><p>Precise representations of 3D faces are beneficial to various computer vision
and graphics applications. Due to the data discretization and model linearity,
however, it remains challenging to capture accurate identity and expression
clues in current studies. This paper presents a novel 3D morphable face model,
namely ImFace, to learn a nonlinear and continuous space with implicit neural
representations. It builds two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
and designs an improved learning strategy to extend embeddings of expressions
to allow more diverse changes. We further introduce a Neural Blend-Field to
learn sophisticated details by adaptively blending a series of local fields. In
addition to ImFace, an effective preprocessing pipeline is proposed to address
the issue of watertight input requirement in implicit representations, enabling
them to work with common facial surfaces for the first time. Extensive
experiments are performed to demonstrate the superiority of ImFace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Invariant Siamese Attention Mask for Small Object Change Detection via Everyday Indoor Robot Navigation. (arXiv:2203.15362v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15362">
<div class="article-summary-box-inner">
<span><p>The problem of image change detection via everyday indoor robot navigation is
explored from a novel perspective of the self-attention technique. Detecting
semantically non-distinctive and visually small changes remains a key challenge
in the robotics community. Intuitively, these small non-distinctive changes may
be better handled by the recent paradigm of the attention mechanism, which is
the basic idea of this work. However, existing self-attention models require
significant retraining cost per domain, so it is not directly applicable to
robotics applications. We propose a new self-attention technique with an
ability of unsupervised on-the-fly domain adaptation, which introduces an
attention mask into the intermediate layer of an image change detection model,
without modifying the input and output layers of the model. Experiments, in
which an indoor robot aims to detect visually small changes in everyday
navigation, demonstrate that our attention technique significantly boosts the
state-of-the-art image change detection model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection. (arXiv:2203.17054v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17054">
<div class="article-summary-box-inner">
<span><p>Single frame data contains finite information which limits the performance of
the existing vision-based multi-camera 3D object detection paradigms. For
fundamentally pushing the performance boundary in this area, a novel paradigm
dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the
spatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive
BEVDet framework with a few modifications just for fusing the feature from the
previous frame with the corresponding one in the current frame. In this way,
with negligible additional computing budget, we enable BEVDet4D to access the
temporal cues by querying and comparing the two candidate features. Beyond
this, we simplify the task of velocity prediction by removing the factors of
ego-motion and time in the learning target. As a result, BEVDet4D with robust
generalization performance reduces the velocity error by up to -62.9%. This
makes the vision-based methods, for the first time, become comparable with
those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes,
we report a new record of 54.5% NDS with the high-performance configuration
dubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base
by +7.3% NDS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion. (arXiv:2203.17191v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17191">
<div class="article-summary-box-inner">
<span><p>Recently, video frame interpolation using a combination of frame- and
event-based cameras has surpassed traditional image-based methods both in terms
of performance and memory efficiency. However, current methods still suffer
from (i) brittle image-level fusion of complementary interpolation results,
that fails in the presence of artifacts in the fused image, (ii) potentially
temporally inconsistent and inefficient motion estimation procedures, that run
for every inserted frame and (iii) low contrast regions that do not trigger
events, and thus cause events-only motion estimation to generate artifacts.
Moreover, previous methods were only tested on datasets consisting of planar
and faraway scenes, which do not capture the full complexity of the real world.
In this work, we address the above problems by introducing multi-scale
feature-level fusion and computing one-shot non-linear inter-frame motion from
events and images, which can be efficiently sampled for image warping. We also
collect the first large-scale events and frames dataset consisting of more than
100 challenging scenes with depth variations, captured with a new experimental
setup based on a beamsplitter. We show that our method improves the
reconstruction quality by up to 0.2 dB in terms of PSNR and up to 15% in LPIPS
score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2204.00570v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00570">
<div class="article-summary-box-inner">
<span><p>We consider unsupervised domain adaptation (UDA), where labeled data from a
source domain (e.g., photographs) and unlabeled data from a target domain
(e.g., sketches) are used to learn a classifier for the target domain.
Conventional UDA methods (e.g., domain adversarial training) learn
domain-invariant features to improve generalization to the target domain. In
this paper, we show that contrastive pre-training, which learns features on
unlabeled source and target data and then fine-tunes on labeled source data, is
competitive with strong UDA methods. However, we find that contrastive
pre-training does not learn domain-invariant features, diverging from
conventional UDA intuitions. We show theoretically that contrastive
pre-training can learn features that vary subtantially across domains but still
generalize to the target domain, by disentangling domain and class information.
Our results suggest that domain invariance is not necessary for UDA. We
empirically validate our theory on benchmark vision datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimize Deep Learning Models for Prediction of Gene Mutations Using Unsupervised Clustering. (arXiv:2204.01593v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01593">
<div class="article-summary-box-inner">
<span><p>Deep learning has become the mainstream methodological choice for analyzing
and interpreting whole-slide digital pathology images (WSIs). It is commonly
assumed that tumor regions carry most predictive information. In this paper, we
proposed an unsupervised clustering-based multiple-instance learning, and apply
our method to develop deep-learning models for prediction of gene mutations
using WSIs from three cancer types in The Cancer Genome Atlas (TCGA) studies
(CRC, LUAD, and HNSCC). We showed that unsupervised clustering of image patches
could help identify predictive patches, exclude patches lack of predictive
information, and therefore improve prediction on gene mutations in all three
different cancer types, compared with the WSI based method without selection of
image patches and models based on only tumor regions. Additionally, our
proposed algorithm outperformed two recently published baseline algorithms
leveraging unsupervised clustering to assist model prediction. The
unsupervised-clustering-based approach for mutation prediction allows
identification of the spatial regions related to mutation of a specific gene
via the resolved probability scores, highlighting the heterogeneity of a
predicted genotype in the tumor microenvironment. Finally, our study also
demonstrated that selection of tumor regions of WSIs is not always the best way
to identify patches for prediction of gene mutations, and other tissue types in
the tumor micro-environment may provide better prediction ability for gene
mutations than tumor tissues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex-Valued Autoencoders for Object Discovery. (arXiv:2204.02075v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02075">
<div class="article-summary-box-inner">
<span><p>Object-centric representations form the basis of human perception and enable
us to reason about the world and to systematically generalize to new settings.
Currently, most machine learning work on unsupervised object discovery focuses
on slot-based approaches, which explicitly separate the latent representations
of individual objects. While the result is easily interpretable, it usually
requires the design of involved architectures. In contrast to this, we propose
a distributed approach to object-centric representations: the Complex
AutoEncoder. Following a coding scheme theorized to underlie object
representations in biological neurons, its complex-valued activations represent
two messages: their magnitudes express the presence of a feature, while the
relative phase differences between neurons express which features should be
bound together to create joint object representations. We show that this simple
and efficient approach achieves better reconstruction performance than an
equivalent real-valued autoencoder on simple multi-object datasets.
Additionally, we show that it achieves competitive unsupervised object
discovery performance to a SlotAttention model on two datasets, and manages to
disentangle objects in a third dataset where SlotAttention fails - all while
being 7-70 times faster to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks. (arXiv:2204.02824v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02824">
<div class="article-summary-box-inner">
<span><p>Face inpainting aims to complete the corrupted regions of the face images,
which requires coordination between the completed areas and the non-corrupted
areas. Recently, memory-oriented methods illustrate great prospects in the
generation related tasks by introducing an external memory module to improve
image coordination. However, such methods still have limitations in restoring
the consistency and continuity for specificfacial semantic parts. In this
paper, we propose the coarse-to-fine Memory-Disentangled Refinement Networks
(MDRNets) for coordinated face inpainting, in which two collaborative modules
are integrated, Disentangled Memory Module (DMM) and Mask-Region Enhanced
Module (MREM). Specifically, the DMM establishes a group of disentangled memory
blocks to store the semantic-decoupled face representations, which could
provide the most relevant information to refine the semantic-level
coordination. The MREM involves a masked correlation mining mechanism to
enhance the feature relationships into the corrupted regions, which could also
make up for the correlation loss caused by memory disentanglement. Furthermore,
to better improve the inter-coordination between the corrupted and
non-corrupted regions and enhance the intra-coordination in corrupted regions,
we design InCo2 Loss, a pair of similarity based losses to constrain the
feature consistency. Eventually, extensive experiments conducted on CelebA-HQ
and FFHQ datasets demonstrate the superiority of our MDRNets compared with
previous State-Of-The-Art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2204.02887v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02887">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have shown to be very vulnerable to adversarial examples
crafted by adding human-imperceptible perturbations to benign inputs. After
achieving impressive attack success rates in the white-box setting, more focus
is shifted to black-box attacks. In either case, the common gradient-based
approaches generally use the $sign$ function to generate perturbations at the
end of the process. However, only a few works pay attention to the limitation
of the $sign$ function. Deviation between the original gradient and the
generated noises may lead to inaccurate gradient update estimation and
suboptimal solutions for adversarial transferability, which is crucial for
black-box attacks. To address this issue, we propose a Sampling-based Fast
Gradient Rescaling Method (S-FGRM) to improve the transferability of the
crafted adversarial examples. Specifically, we use data rescaling to substitute
the inefficient $sign$ function in gradient-based attacks without extra
computational cost. We also propose a Depth First Sampling method to eliminate
the fluctuation of rescaling and stabilize the gradient update. Our method can
be used in any gradient-based optimizations and is extensible to be integrated
with various input transformation or ensemble methods for further improving the
adversarial transferability. Extensive experiments on the standard ImageNet
dataset show that our S-FGRM could significantly boost the transferability of
gradient-based attacks and outperform the state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. (arXiv:2204.03162v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03162">
<div class="article-summary-box-inner">
<span><p>We present a novel task and dataset for evaluating the ability of vision and
language models to conduct visio-linguistic compositional reasoning, which we
call Winoground. Given two images and two captions, the goal is to match them
correctly - but crucially, both captions contain a completely identical set of
words, only in a different order. The dataset was carefully hand-curated by
expert annotators and is labeled with a rich set of fine-grained tags to assist
in analyzing model performance. We probe a diverse range of state-of-the-art
vision and language models and find that, surprisingly, none of them do much
better than chance. Evidently, these models are not as skilled at
visio-linguistic compositional reasoning as we might have hoped. We perform an
extensive analysis to obtain insights into how future work might try to
mitigate these models' shortcomings. We aim for Winoground to serve as a useful
evaluation set for advancing the state of the art and driving further progress
in the field. The dataset is available at
https://huggingface.co/datasets/facebook/winoground.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HunYuan_tvr for Text-Video Retrieval. (arXiv:2204.03382v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03382">
<div class="article-summary-box-inner">
<span><p>Text-Video Retrieval plays an important role in multi-modal understanding and
has attracted increasing attention in recent years. Most existing methods focus
on constructing contrastive pairs between whole videos and complete caption
sentences, while ignoring fine-grained cross-modal relationships, e.g., short
clips and phrases or single frame and word. In this paper, we propose a novel
method, named HunYuan\_tvr, to explore hierarchical cross-modal interactions by
simultaneously exploring video-sentence, clip-phrase, and frame-word
relationships. Considering intrinsic semantic relations between frames,
HunYuan\_tvr first performs self-attention to explore frame-wise correlations
and adaptively clusters correlated frames into clip-level representations.
Then, the clip-wise correlation is explored to aggregate clip representations
into a compact one to describe the video globally. In this way, we can
construct hierarchical video representations for frame-clip-video
granularities, and also explore word-wise correlations to form
word-phrase-sentence embeddings for the text modality. Finally, hierarchical
contrastive learning is designed to explore cross-modal
relationships,~\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which
enables HunYuan\_tvr to achieve a comprehensive multi-modal understanding.
Further boosted by adaptive label denoising and marginal sample enhancement,
HunYuan\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,
Rank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,
DiDemo, and ActivityNet respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Prototype Prompt-tuning with Pre-trained Representation for Class Incremental Learning. (arXiv:2204.03410v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03410">
<div class="article-summary-box-inner">
<span><p>Class incremental learning has attracted much attention, but most existing
works still continually fine-tune the entire representation model, inevitably
resulting in much catastrophic forgetting. Instead of struggling to fight
against such forgetting by replaying or distillation like most of the existing
methods, we take a novel pre-train-and-prompt-tuning paradigm to sequentially
learn new visual concepts based on a fixed semantic-rich pre-trained
representation model. In detail, we incrementally prompt-tune category
prototypes for classification and example prototypes to compensate for semantic
drift, the problem caused by learning bias at different phases. Extensive
experiments conducted on the mainstream incremental learning benchmarks
demonstrate that our method outperforms other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-aided Direct Sparse Odometry. (arXiv:2204.07640v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07640">
<div class="article-summary-box-inner">
<span><p>We introduce EDS, a direct monocular visual odometry using events and frames.
Our algorithm leverages the event generation model to track the camera motion
in the blind time between frames. The method formulates a direct probabilistic
approach of observed brightness increments. Per-pixel brightness increments are
predicted using a sparse number of selected 3D points and are compared to the
events via the brightness increment error to estimate camera motion. The method
recovers a semi-dense 3D map using photometric bundle adjustment. EDS is the
first method to perform 6-DOF VO using events and frames with a direct
approach. By design, it overcomes the problem of changing appearance in
indirect methods. We also show that, for a target error performance, EDS can
work at lower frame rates than state-of-the-art frame-based VO solutions. This
opens the door to low-power motion-tracking applications where frames are
sparingly triggered "on demand" and our method tracks the motion in between. We
release code and datasets to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Signatures. (arXiv:2204.07953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07953">
<div class="article-summary-box-inner">
<span><p>In this work we investigate the use of the Signature Transform in the context
of Learning. Under this assumption, we advance a supervised framework that
provides state-of-the-art classification accuracy with the use of very few
labels without the need of credit assignment and with minimal or no
overfitting. We leverage tools from harmonic analysis by the use of the
signature and log-signature, and use as a score function RMSE and MAE Signature
and log-signature. We develop a closed-form equation to compute probably good
optimal scale factors. Classification is performed at the CPU level orders of
magnitude faster than other methods. We report results on AFHQ, MNIST and
CIFAR10 achieving 100% accuracy on all tasks assuming we can determine at test
time which probably good optimal scale factor to use for each category.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography. (arXiv:2204.08466v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08466">
<div class="article-summary-box-inner">
<span><p>Although robust PCA has been increasingly adopted to extract vessels from
X-ray coronary angiography (XCA) images, challenging problems such as
inefficient vessel-sparsity modelling, noisy and dynamic background artefacts,
and high computational cost still remain unsolved. Therefore, we propose a
novel robust PCA unrolling network with sparse feature selection for
super-resolution XCA vessel imaging. Being embedded within a patch-wise
spatiotemporal super-resolution framework that is built upon a pooling layer
and a convolutional long short-term memory network, the proposed network can
not only gradually prune complex vessel-like artefacts and noisy backgrounds in
XCA during network training but also iteratively learn and select the
high-level spatiotemporal semantic information of moving contrast agents
flowing in the XCA-imaged vessels. The experimental results show that the
proposed method significantly outperforms state-of-the-art methods, especially
in the imaging of the vessel network and its distal vessels, by restoring the
intensity and geometry profiles of heterogeneous vessels against complex and
dynamic backgrounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-Aware Monocular 3D Object Detection. (arXiv:2204.08717v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08717">
<div class="article-summary-box-inner">
<span><p>The detection of 3D objects through a single perspective camera is a
challenging issue. The anchor-free and keypoint-based models receive increasing
attention recently due to their effectiveness and simplicity. However, most of
these methods are vulnerable to occluded and truncated objects. In this paper,
a single-stage monocular 3D object detection model is proposed. An
instance-segmentation head is integrated into the model training, which allows
the model to be aware of the visible shape of a target object. The detection
largely avoids interference from irrelevant regions surrounding the target
objects. In addition, we also reveal that the popular IoU-based evaluation
metrics, which were originally designed for evaluating stereo or LiDAR-based
detection methods, are insensitive to the improvement of monocular 3D object
detection algorithms. A novel evaluation metric, namely average depth
similarity (ADS) is proposed for the monocular 3D object detection models. Our
method outperforms the baseline on both the popular and the proposed evaluation
metrics while maintaining real-time efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results. (arXiv:2204.09314v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09314">
<div class="article-summary-box-inner">
<span><p>This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality
Enhancement of Compressed Video. In this challenge, we proposed the LDV 2.0
dataset, which includes the LDV dataset (240 videos) and 95 additional videos.
This challenge includes three tracks. Track 1 aims at enhancing the videos
compressed by HEVC at a fixed QP. Track 2 and Track 3 target both the
super-resolution and quality enhancement of HEVC compressed video. They require
x2 and x4 super-resolution, respectively. The three tracks totally attract more
than 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams
submitted the final results to Tracks 1, 2 and 3, respectively. The proposed
methods and solutions gauge the state-of-the-art of super-resolution and
quality enhancement of compressed video. The proposed LDV 2.0 dataset is
available at https://github.com/RenYang-home/LDV_dataset. The homepage of this
challenge (including open-sourced codes) is at
https://github.com/RenYang-home/NTIRE22_VEnh_SR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments. (arXiv:2204.09667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09667">
<div class="article-summary-box-inner">
<span><p>Recent work in Vision-and-Language Navigation (VLN) has presented two
environmental paradigms with differing realism -- the standard VLN setting
built on topological environments where navigation is abstracted away, and the
VLN-CE setting where agents must navigate continuous 3D environments using
low-level actions. Despite sharing the high-level task and even the underlying
instruction-path data, performance on VLN-CE lags behind VLN significantly. In
this work, we explore this gap by transferring an agent from the abstract
environment of VLN to the continuous environment of VLN-CE. We find that this
sim-2-sim transfer is highly effective, improving over the prior state of the
art in VLN-CE by +12% success rate. While this demonstrates the potential for
this direction, the transfer does not fully retain the original performance of
the agent in the abstract setting. We present a sequence of experiments to
identify what differences result in performance degradation, providing clear
directions for further improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Invariant Skin Segmentation. (arXiv:2204.09882v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09882">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of automatically detecting human skin in
images without reliance on color information. A primary motivation of the work
has been to achieve results that are consistent across the full range of skin
tones, even while using a training dataset that is significantly biased toward
lighter skin tones. Previous skin-detection methods have used color cues almost
exclusively, and we present a new approach that performs well in the absence of
such information. A key aspect of the work is dataset repair through
augmentation that is applied strategically during training, with the goal of
color invariant feature learning to enhance generalization. We have
demonstrated the concept using two architectures, and experimental results show
improvements in both precision and recall for most Fitzpatrick skin tones in
the benchmark ECU dataset. We further tested the system with the RFW dataset to
show that the proposed method performs much more consistently across different
ethnicities, thereby reducing the chance of bias based on skin color. To
demonstrate the effectiveness of our work, extensive experiments were performed
on grayscale images as well as images obtained under unconstrained illumination
and with artificial filters. Source code:
https://github.com/HanXuMartin/Color-Invariant-Skin-Segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Working memory inspired hierarchical video decomposition with transformative representations. (arXiv:2204.10105v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10105">
<div class="article-summary-box-inner">
<span><p>Video decomposition is very important to extract moving foreground objects
from complex backgrounds in computer vision, machine learning, and medical
imaging, e.g., extracting moving contrast-filled vessels from the complex and
noisy backgrounds of X-ray coronary angiography (XCA). However, the challenges
caused by dynamic backgrounds, overlapping heterogeneous environments and
complex noises still exist in video decomposition. To solve these problems,
this study is the first to introduce a flexible visual working memory model in
video decomposition tasks to provide interpretable and high-performance
hierarchical deep architecture, integrating the transformative representations
between sensory and control layers from the perspective of visual and cognitive
neuroscience. Specifically, robust PCA unrolling networks acting as a
structure-regularized sensor layer decompose XCA into sparse/low-rank
structured representations to separate moving contrast-filled vessels from
noisy and complex backgrounds. Then, patch recurrent convolutional LSTM
networks with a backprojection module embody unstructured random
representations of the control layer in working memory, recurrently projecting
spatiotemporally decomposed nonlocal patches into orthogonal subspaces for
heterogeneous vessel retrieval and interference suppression. This video
decomposition deep architecture effectively restores the heterogeneous profiles
of intensity and the geometries of moving objects against the complex
background interferences. Experiments show that the proposed method
significantly outperforms state-of-the-art methods in accurate moving
contrast-filled vessel extraction with excellent flexibility and computational
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmbedTrack -- Simultaneous Cell Segmentation and Tracking Through Learning Offsets and Clustering Bandwidths. (arXiv:2204.10713v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10713">
<div class="article-summary-box-inner">
<span><p>A systematic analysis of the cell behavior requires automated approaches for
cell segmentation and tracking. While deep learning has been successfully
applied for the task of cell segmentation, there are few approaches for
simultaneous cell segmentation and tracking using deep learning. Here, we
present EmbedTrack, a single convolutional neural network for simultaneous cell
segmentation and tracking which predicts easy to interpret embeddings. As
embeddings, offsets of cell pixels to their cell center and bandwidths are
learned. We benchmark our approach on nine 2D data sets from the Cell Tracking
Challenge, where our approach performs on seven out of nine data sets within
the top 3 contestants including three top 1 performances. The source code is
publicly available at https://git.scc.kit.edu/kit-loe-ge/embedtrack.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-26 23:08:44.611219503 UTC">2022-04-26 23:08:44 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>