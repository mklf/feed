<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-05T01:30:00Z">04-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A single speaker is almost all you need for automatic speech recognition. (arXiv:2204.00618v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00618">
<div class="article-summary-box-inner">
<span><p>We explore the use of speech synthesis and voice conversion applied to
augment datasets for automatic speech recognition (ASR) systems, in scenarios
with only one speaker available for the target language. Through extensive
experiments, we show that our approach achieves results compared to the
state-of-the-art (SOTA) and requires only one speaker in the target language
during speech synthesis/voice conversion model training. Finally, we show that
it is possible to obtain promising results in the training of an ASR model with
our data augmentation method and only a single real speaker in different target
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end multi-talker audio-visual ASR using an active speaker attention module. (arXiv:2204.00652v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00652">
<div class="article-summary-box-inner">
<span><p>This paper presents a new approach for end-to-end audio-visual multi-talker
speech recognition. The approach, referred to here as the visual context
attention model (VCAM), is important because it uses the available video
information to assign decoded text to one of multiple visible faces. This
essentially resolves the label ambiguity issue associated with most
multi-talker modeling approaches which can decode multiple label strings but
cannot assign the label strings to the correct speakers. This is implemented as
a transformer-transducer based end-to-end model and evaluated using a two
speaker audio-visual overlapping speech dataset created from YouTube videos. It
is shown in the paper that the VCAM model improves performance with respect to
previously reported audio-only and audio-visual multi-talker ASR systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CipherDAug: Ciphertext based Data Augmentation for Neural Machine Translation. (arXiv:2204.00665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00665">
<div class="article-summary-box-inner">
<span><p>We propose a novel data-augmentation technique for neural machine translation
based on ROT-$k$ ciphertexts. ROT-$k$ is a simple letter substitution cipher
that replaces a letter in the plaintext with the $k$th letter after it in the
alphabet. We first generate multiple ROT-$k$ ciphertexts using different values
of $k$ for the plaintext which is the source side of the parallel data. We then
leverage this enciphered training data along with the original parallel data
via multi-source training to improve neural machine translation. Our method,
CipherDAug, uses a co-regularization-inspired training procedure, requires no
external data sources other than the original training data, and uses a
standard Transformer to outperform strong data augmentation techniques on
several datasets by a significant margin. This technique combines easily with
existing approaches to data augmentation, and yields particularly strong
results in low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Argument Structure Extraction with Transfer Learning and Active Learning. (arXiv:2204.00707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00707">
<div class="article-summary-box-inner">
<span><p>The automation of extracting argument structures faces a pair of challenges
on (1) encoding long-term contexts to facilitate comprehensive understanding,
and (2) improving data efficiency since constructing high-quality argument
structures is time-consuming. In this work, we propose a novel context-aware
Transformer-based argument structure prediction model which, on five different
domains, significantly outperforms models that rely on features or only encode
limited contexts. To tackle the difficulty of data annotation, we examine two
complementary methods: (i) transfer learning to leverage existing annotated
data to boost model performance in a new target domain, and (ii) active
learning to strategically identify a small amount of samples for annotation. We
further propose model-independent sample acquisition strategies, which can be
generalized to diverse domains. With extensive experiments, we show that our
simple-yet-effective acquisition strategies yield competitive results against
three strong comparisons. Combined with transfer learning, substantial F1 score
boost (5-25) can be further achieved during the early iterations of active
learning across domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos. (arXiv:2204.00716v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00716">
<div class="article-summary-box-inner">
<span><p>Previous work has shown that dense retrievers are not robust to out-of-domain
and outlier queries, i.e. their effectiveness on these queries is much poorer
than what expected. In this paper, we consider a specific instance of such
queries: queries that contain typos. We show that a small character level
perturbation in queries (as caused by typos) highly impacts the effectiveness
of dense retrievers. We then demonstrate that the root cause of this resides in
the input tokenization strategy employed by BERT. In BERT, tokenization is
performed using the BERT's WordPiece tokenizer and we show that a token with a
typo will significantly change the token distributions obtained after
tokenization. This distribution change translates to changes in the input
embeddings passed to the BERT-based query encoder of dense retrievers. We then
turn our attention to devising dense retriever methods that are robust to such
typo queries, while still being as performant as previous methods on queries
without typos. For this, we use CharacterBERT as the backbone encoder and an
efficient yet effective training method, called Self-Teaching (ST), that
distills knowledge from queries without typos into the queries with typos.
Experimental results show that CharacterBERT in combination with ST achieves
significantly higher effectiveness on queries with typos compared to previous
methods. Along with these results and the open-sourced implementation of the
methods, we also provide a new passage retrieval dataset consisting of
real-world queries with typos and associated relevance assessments on the MS
MARCO corpus, thus supporting the research community in the investigation of
effective and robust dense retrievers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Simplify with Data Hopelessly Out of Alignment. (arXiv:2204.00741v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00741">
<div class="article-summary-box-inner">
<span><p>We consider whether it is possible to do text simplification without relying
on a "parallel" corpus, one that is made up of sentence-by-sentence alignments
of complex and ground truth simple sentences. To this end, we introduce a
number of concepts, some new and some not, including what we call Conjoined
Twin Networks, Flip-Flop Auto-Encoders (FFA) and Adversarial Networks (GAN). A
comparison is made between Jensen-Shannon (JS-GAN) and Wasserstein GAN, to see
how they impact performance, with stronger results for the former. An
experiment we conducted with a large dataset derived from Wikipedia found the
solid superiority of Twin Networks equipped with FFA and JS-GAN, over the
current best performing system. Furthermore, we discuss where we stand in a
relation to fully supervised methods in the past literature, and highlight with
examples qualitative differences that exist among simplified sentences
generated by supervision-free systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating recommendations for entity-oriented exploratory search. (arXiv:2204.00743v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00743">
<div class="article-summary-box-inner">
<span><p>We introduce the task of recommendation set generation for entity-oriented
exploratory search. Given an input search query which is open-ended or
under-specified, the task is to present the user with an easily-understandable
collection of query recommendations, with the goal of facilitating domain
exploration or clarifying user intent. Traditional query recommendation systems
select recommendations by identifying salient keywords in retrieved documents,
or by querying an existing taxonomy or knowledge base for related concepts. In
this work, we build a text-to-text model capable of generating a collection of
recommendations directly, using the language model as a "soft" knowledge base
capable of proposing new concepts not found in an existing taxonomy or set of
retrieved documents. We train the model to generate recommendation sets which
optimize a cost function designed to encourage comprehensiveness,
interestingness, and non-redundancy. In thorough evaluations performed by crowd
workers, we confirm the generalizability of our approach and the high quality
of the generated recommendations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00763">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline
setting or through human evaluation. The evaluation is often limited to
single-turn or very time-intensive. As an alternative, user simulators that
mimic user behavior allow us to consider a broad set of user goals to generate
human-like conversations for simulated evaluation. Employing existing user
simulators to evaluate TDSs is challenging as user simulators are primarily
designed to optimize dialogue policies for TDSs and have limited evaluation
capability. Moreover, the evaluation of user simulators is an open challenge.
In this work, we proposes a metaphorical user simulator for endto-end TDS
evaluation. We also propose a tester-based evaluation framework to generate
variants, i.e., dialogue systems with different capabilities. Our user
simulator constructs a metaphorical user model that assists the simulator in
reasoning by referring to prior knowledge when encountering new items. We
estimate the quality of simulators by checking the simulated interactions
between simulators and variants. Our experiments are conducted using three TDS
datasets. The metaphorical user simulator demonstrates better consistency with
manual evaluation than Agenda-based simulator and Seq2seq model on three
datasets; our tester framework demonstrates efficiency, and our approach
demonstrates better generalization and scalability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CL-XABSA: Contrastive Learning for Cross-lingual Aspect-based Sentiment Analysis. (arXiv:2204.00791v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00791">
<div class="article-summary-box-inner">
<span><p>As an extensive research in the field of Natural language processing (NLP),
aspect-based sentiment analysis (ABSA) is the task of predicting the sentiment
expressed in a text relative to the corresponding aspect. Unfortunately, most
languages lack of sufficient annotation resources, thus more and more recent
researchers focus on cross-lingual aspect-based sentiment analysis (XABSA).
However, most recent researches only concentrate on cross-lingual data
alignment instead of model alignment. To this end, we propose a novel
framework, CL-XABSA: Contrastive Learning for Cross-lingual Aspect-Based
Sentiment Analysis. Specifically, we design two contrastive strategies, token
level contrastive learning of token embeddings (TL-CTE) and sentiment level
contrastive learning of token embeddings (SL-CTE), to regularize the semantic
space of source and target language to be more uniform. Since our framework can
receive datasets in multiple languages during training, our framework can be
adapted not only for XABSA task, but also for multilingual aspect-based
sentiment analysis (MABSA). To further improve the performance of our model, we
perform knowledge distillation technology leveraging data from unlabeled target
language. In the distillation XABSA task, we further explore the comparative
effectiveness of different data (source dataset, translated dataset, and
code-switched dataset). The results demonstrate that the proposed method has a
certain improvement in the three tasks of XABSA, distillation XABSA and MABSA.
For reproducibility, our code for this paper is available at
https://github.com/GKLMIP/CL-XABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Contrastive Framework for Low-Resource Cross-Lingual Named Entity Recognition. (arXiv:2204.00796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00796">
<div class="article-summary-box-inner">
<span><p>Cross-lingual Named Entity Recognition (NER) has recently become a research
hotspot because it can alleviate the data-hungry problem for low-resource
languages. However, few researches have focused on the scenario where the
source-language labeled data is also limited in some specific domains. A common
approach for this scenario is to generate more training data through
translation or generation-based data augmentation method. Unfortunately, we
find that simply combining source-language data and the corresponding
translation cannot fully exploit the translated data and the improvements
obtained are somewhat limited. In this paper, we describe our novel
dual-contrastive framework ConCNER for cross-lingual NER under the scenario of
limited source-language labeled data. Specifically, based on the
source-language samples and their translations, we design two contrastive
objectives for cross-language NER at different grammatical levels, namely
Translation Contrastive Learning (TCL) to close sentence representations
between translated sentence pairs and Label Contrastive Learning (LCL) to close
token representations within the same labels. Furthermore, we utilize knowledge
distillation method where the NER model trained above is used as the teacher to
train a student model on unlabeled target-language data to better fit the
target language. We conduct extensive experiments on a wide variety of target
languages, and the results demonstrate that ConCNER tends to outperform
multiple baseline methods. For reproducibility, our code for this paper is
available at https://github.com/GKLMIP/ConCNER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Factual Accuracy of Abstractive Clinical Text Summarization using Multi-Objective Optimization. (arXiv:2204.00797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00797">
<div class="article-summary-box-inner">
<span><p>While there has been recent progress in abstractive summarization as applied
to different domains including news articles, scientific articles, and blog
posts, the application of these techniques to clinical text summarization has
been limited. This is primarily due to the lack of large-scale training data
and the messy/unstructured nature of clinical notes as opposed to other domains
where massive training data come in structured or semi-structured form.
Further, one of the least explored and critical components of clinical text
summarization is factual accuracy of clinical summaries. This is specifically
crucial in the healthcare domain, cardiology in particular, where an accurate
summary generation that preserves the facts in the source notes is critical to
the well-being of a patient. In this study, we propose a framework for
improving the factual accuracy of abstractive summarization of clinical text
using knowledge-guided multi-objective optimization. We propose to jointly
optimize three cost functions in our proposed architecture during training:
generative loss, entity loss and knowledge loss and evaluate the proposed
architecture on 1) clinical notes of patients with heart failure (HF), which we
collect for this study; and 2) two benchmark datasets, Indiana University Chest
X-ray collection (IU X-Ray), and MIMIC-CXR, that are publicly available. We
experiment with three transformer encoder-decoder architectures and demonstrate
that optimizing different loss functions leads to improved performance in terms
of entity-level factual accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end model for named entity recognition from speech without paired training data. (arXiv:2204.00803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00803">
<div class="article-summary-box-inner">
<span><p>Recent works showed that end-to-end neural approaches tend to become very
popular for spoken language understanding (SLU). Through the term end-to-end,
one considers the use of a single model optimized to extract semantic
information directly from the speech signal. A major issue for such models is
the lack of paired audio and textual data with semantic annotation. In this
paper, we propose an approach to build an end-to-end neural model to extract
semantic information in a scenario in which zero paired audio data is
available. Our approach is based on the use of an external model trained to
generate a sequence of vectorial representations from text. These
representations mimic the hidden representations that could be generated inside
an end-to-end automatic speech recognition (ASR) model by processing a speech
signal. An SLU neural module is then trained using these representations as
input and the annotated text as output. Last, the SLU module replaces the top
layers of the ASR model to achieve the construction of the end-to-end model.
Our experiments on named entity recognition, carried out on the QUAERO corpus,
show that this approach is very promising, getting better results than a
comparable cascade approach or than the use of synthetic voices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HLDC: Hindi Legal Documents Corpus. (arXiv:2204.00806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00806">
<div class="article-summary-box-inner">
<span><p>Many populous countries including India are burdened with a considerable
backlog of legal cases. Development of automated systems that could process
legal documents and augment legal practitioners can mitigate this. However,
there is a dearth of high-quality corpora that is needed to develop such
data-driven systems. The problem gets even more pronounced in the case of low
resource languages such as Hindi. In this resource paper, we introduce the
Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents
in Hindi. Documents are cleaned and structured to enable the development of
downstream applications. Further, as a use-case for the corpus, we introduce
the task of bail prediction. We experiment with a battery of models and propose
a Multi-Task Learning (MTL) based model for the same. MTL models use
summarization as an auxiliary task along with bail prediction as the main task.
Experiments with different models are indicative of the need for further
research in this area. We release the corpus and model implementation code with
this paper: https://github.com/Exploration-Lab/HLDC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Sequence-to-Tree Generation for Hierarchical Text Classification. (arXiv:2204.00811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00811">
<div class="article-summary-box-inner">
<span><p>Hierarchical Text Classification (HTC) is a challenging task where a document
can be assigned to multiple hierarchically structured categories within a
taxonomy. The majority of prior studies consider HTC as a flat multi-label
classification problem, which inevitably leads to "label inconsistency"
problem. In this paper, we formulate HTC as a sequence generation task and
introduce a sequence-to-tree framework (Seq2Tree) for modeling the hierarchical
label structure. Moreover, we design a constrained decoding strategy with
dynamic vocabulary to secure the label consistency of the results. Compared
with previous works, the proposed approach achieves significant and consistent
improvements on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient comparison of sentence embeddings. (arXiv:2204.00820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00820">
<div class="article-summary-box-inner">
<span><p>The domain of natural language processing (NLP), which has greatly evolved
over the last years, has highly benefited from the recent developments in word
and sentence embeddings. Such embeddings enable the transformation of complex
NLP tasks, like semantic similarity or Question and Answering (Q\&amp;A), into much
simpler to perform vector comparisons. However, such a problem transformation
raises new challenges like the efficient comparison of embeddings and their
manipulation. In this work, we will discuss about various word and sentence
embeddings algorithms, we will select a sentence embedding algorithm, BERT, as
our algorithm of choice and we will evaluate the performance of two vector
comparison approaches, FAISS and Elasticsearch, in the specific problem of
sentence embeddings. According to the results, FAISS outperforms Elasticsearch
when used in a centralized environment with only one node, especially when big
datasets are included.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation. (arXiv:2204.00862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00862">
<div class="article-summary-box-inner">
<span><p>Existing reference-free metrics have obvious limitations for evaluating
controlled text generation models. Unsupervised metrics can only provide a
task-agnostic evaluation result which correlates weakly with human judgments,
whereas supervised ones may overfit task-specific data with poor generalization
ability to other datasets. In this paper, we propose an unsupervised
reference-free metric called CTRLEval, which evaluates controlled text
generation from different aspects by formulating each aspect into multiple text
infilling tasks. On top of these tasks, the metric assembles the generation
probabilities from a pre-trained language model without any model training.
Experimental results show that our metric has higher correlations with human
judgments than other baselines, while obtaining better generalization of
evaluating generated texts from different models and with different qualities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding. (arXiv:2204.00871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00871">
<div class="article-summary-box-inner">
<span><p>Online alignment in machine translation refers to the task of aligning a
target word to a source word when the target sequence has only been partially
decoded. Good online alignments facilitate important applications such as
lexically constrained translation where user-defined dictionaries are used to
inject lexical constraints into the translation model. We propose a novel
posterior alignment technique that is truly online in its execution and
superior in terms of alignment error rates compared to existing methods. Our
proposed inference technique jointly considers alignment and token
probabilities in a principled manner and can be seamlessly integrated within
existing constrained beam-search decoding algorithms. On five language pairs,
including two distant language pairs, we achieve consistent drop in alignment
error rates. When deployed on seven lexically constrained translation tasks, we
achieve significant improvements in BLEU specifically around the constrained
positions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-VQA : Answering by Interactive Sub Question Sequence. (arXiv:2204.00879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00879">
<div class="article-summary-box-inner">
<span><p>Most existing approaches to Visual Question Answering (VQA) answer questions
directly, however, people usually decompose a complex question into a sequence
of simple sub questions and finally obtain the answer to the original question
after answering the sub question sequence(SQS). By simulating the process, this
paper proposes a conversation-based VQA (Co-VQA) framework, which consists of
three components: Questioner, Oracle, and Answerer. Questioner raises the sub
questions using an extending HRED model, and Oracle answers them one-by-one. An
Adaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed,
where the question-answer pair is used to update the visual representation
sequentially. To perform supervised learning for each model, we introduce a
well-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2
datasets. Experimental results show that our method achieves state-of-the-art
on VQA-CP v2. Further analyses show that SQSs help build direct semantic
connections between questions and images, provide question-adaptive
variable-length reasoning chains, and with explicit interpretability as well as
error traceability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging. (arXiv:2204.00885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00885">
<div class="article-summary-box-inner">
<span><p>Prompting methods recently achieve impressive success in few-shot learning.
These methods modify input samples with prompt sentence pieces, and decode
label tokens to map samples to corresponding labels. However, such a paradigm
is very inefficient for the task of slot tagging. Since slot tagging samples
are multiple consecutive words in a sentence, the prompting methods have to
enumerate all n-grams token spans to find all the possible slots, which greatly
slows down the prediction. To tackle this, we introduce an inverse paradigm for
prompting. Different from the classic prompts mapping tokens to labels, we
reversely predict slot values given slot types. Such inverse prompting only
requires a one-turn prediction for each slot type and greatly speeds up the
prediction. Besides, we propose a novel Iterative Prediction Strategy, from
which the model learns to refine predictions by considering the relations
between different slot types. We find, somewhat surprisingly, the proposed
method not only predicts faster but also significantly improves the effect
(improve over 6.1 F1-scores on 10-shot setting) and achieves new
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moment-based Adversarial Training for Embodied Language Comprehension. (arXiv:2204.00889v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00889">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on a vision-and-language task in which a robot is
instructed to execute household tasks. Given an instruction such as "Rinse off
a mug and place it in the coffee maker," the robot is required to locate the
mug, wash it, and put it in the coffee maker. This is challenging because the
robot needs to break down the instruction sentences into subgoals and execute
them in the correct order. On the ALFRED benchmark, the performance of
state-of-the-art methods is still far lower than that of humans. This is
partially because existing methods sometimes fail to infer subgoals that are
not explicitly specified in the instruction sentences. We propose Moment-based
Adversarial Training (MAT), which uses two types of moments for perturbation
updates in adversarial training. We introduce MAT to the embedding spaces of
the instruction, subgoals, and state representations to handle their varieties.
We validated our method on the ALFRED benchmark, and the results demonstrated
that our method outperformed the baseline method for all the metrics on the
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-Assisted Semantic Annotation Correction for Emotion-Related Questions. (arXiv:2204.00916v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00916">
<div class="article-summary-box-inner">
<span><p>Annotated data have traditionally been used to provide the input for training
a supervised machine learning (ML) model. However, current pre-trained ML
models for natural language processing (NLP) contain embedded linguistic
information that can be used to inform the annotation process. We use the BERT
neural language model to feed information back into an annotation task that
involves semantic labelling of dialog behavior in a question-asking game called
Emotion Twenty Questions (EMO20Q). First we describe the background of BERT,
the EMO20Q data, and assisted annotation tasks. Then we describe the methods
for fine-tuning BERT for the purpose of checking the annotated labels. To do
this, we use the paraphrase task as a way to check that all utterances with the
same annotation label are classified as paraphrases of each other. We show this
method to be an effective way to assess and revise annotations of textual user
data with complex, utterance-level semantic labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Local and Global Features in Transformer-based Extreme Multi-label Text Classification. (arXiv:2204.00933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00933">
<div class="article-summary-box-inner">
<span><p>Extreme multi-label text classification (XMTC) is the task of tagging each
document with the relevant labels from a very large space of predefined
categories. Recently, large pre-trained Transformer models have made
significant performance improvements in XMTC, which typically use the embedding
of the special CLS token to represent the entire document semantics as a global
feature vector, and match it against candidate labels. However, we argue that
such a global feature vector may not be sufficient to represent different
granularity levels of semantics in the document, and that complementing it with
the local word-level features could bring additional gains. Based on this
insight, we propose an approach that combines both the local and global
features produced by Transformer models to improve the prediction power of the
classifier. Our experiments show that the proposed model either outperforms or
is comparable to the state-of-the-art methods on benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Dialect Density Estimation for African American English. (arXiv:2204.00967v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00967">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore automatic prediction of dialect density of the
African American English (AAE) dialect, where dialect density is defined as the
percentage of words in an utterance that contain characteristics of the
non-standard dialect. We investigate several acoustic and language modeling
features, including the commonly used X-vector representation and ComParE
feature set, in addition to information extracted from ASR transcripts of the
audio files and prosodic information. To address issues of limited labeled
data, we use a weakly supervised model to project prosodic and X-vector
features into low-dimensional task-relevant representations. An XGBoost model
is then used to predict the speaker's dialect density from these features and
show which are most significant during inference. We evaluate the utility of
these features both alone and in combination for the given task. This work,
which does not rely on hand-labeled transcripts, is performed on audio segments
from the CORAAL database. We show a significant correlation between our
predicted and ground truth dialect density measures for AAE speech in this
database and propose this work as a tool for explaining and mitigating bias in
speech technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question-Driven Graph Fusion Network For Visual Question Answering. (arXiv:2204.00975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00975">
<div class="article-summary-box-inner">
<span><p>Existing Visual Question Answering (VQA) models have explored various visual
relationships between objects in the image to answer complex questions, which
inevitably introduces irrelevant information brought by inaccurate object
detection and text grounding. To address the problem, we propose a
Question-Driven Graph Fusion Network (QD-GFN). It first models semantic,
spatial, and implicit visual relations in images by three graph attention
networks, then question information is utilized to guide the aggregation
process of the three graphs, further, our QD-GFN adopts an object filtering
mechanism to remove question-irrelevant objects contained in the image.
Experiment results demonstrate that our QD-GFN outperforms the prior
state-of-the-art on both VQA 2.0 and VQA-CP v2 datasets. Further analysis shows
that both the novel graph aggregation method and object filtering mechanism
play a significant role in improving the performance of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Speech Based End-to-End Automated Speech Recognition (ASR) for Indian-English Accents. (arXiv:2204.00977v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00977">
<div class="article-summary-box-inner">
<span><p>Automated Speech Recognition (ASR) is an interdisciplinary application of
computer science and linguistics that enable us to derive the transcription
from the uttered speech waveform. It finds several applications in Military
like High-performance fighter aircraft, helicopters, air-traffic controller.
Other than military speech recognition is used in healthcare, persons with
disabilities and many more. ASR has been an active research area. Several
models and algorithms for speech to text (STT) have been proposed. One of the
most recent is Mozilla Deep Speech, it is based on the Deep Speech research
paper by Baidu. Deep Speech is a state-of-art speech recognition system is
developed using end-to-end deep learning, it is trained using well-optimized
Recurrent Neural Network (RNN) training system utilizing multiple Graphical
Processing Units (GPUs). This training is mostly done using American-English
accent datasets, which results in poor generalizability to other English
accents. India is a land of vast diversity. This can even be seen in the
speech, there are several English accents which vary from state to state. In
this work, we have used transfer learning approach using most recent Deep
Speech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition
system for Indian-English accents. This work utilizes fine-tuning and data
argumentation to further optimize and improve the Deep Speech ASR system. Indic
TTS data of Indian-English accents is used for transfer learning and
fine-tuning the pre-trained Deep Speech model. A general comparison is made
among the untrained model, our trained model and other available speech
recognition services for Indian-English Accents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension. (arXiv:2204.00996v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00996">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained models are able to zero-shot transfer knowledge from
rich-resource to low-resource languages in machine reading comprehension (MRC).
However, inherent linguistic discrepancies in different languages could make
answer spans predicted by zero-shot transfer violate syntactic constraints of
the target language. In this paper, we propose a novel multilingual MRC
framework equipped with a Siamese Semantic Disentanglement Model (SSDM) to
disassociate semantics from syntax in representations learned by multilingual
pre-trained models. To explicitly transfer only semantic knowledge to the
target language, we propose two groups of losses tailored for semantic and
syntactic encoding and disentanglement. Experimental results on three
multilingual MRC datasets (i.e., XQuAD, MLQA, and TyDi QA) demonstrate the
effectiveness of our proposed approach over models based on mBERT and XLM-100.
Code is available at:https://github.com/wulinjuan/SSDM_MRC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Efficiently Acquiring Annotations for Multilingual Models. (arXiv:2204.01016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01016">
<div class="article-summary-box-inner">
<span><p>When tasked with supporting multiple languages for a given problem, two
approaches have arisen: training a model for each language with the annotation
budget divided equally among them, and training on a high-resource language
followed by zero-shot transfer to the remaining languages. In this work, we
show that the strategy of joint learning across multiple languages using a
single model performs substantially better than the aforementioned
alternatives. We also demonstrate that active learning provides additional,
complementary benefits. We show that this simple approach enables the model to
be data efficient by allowing it to arbitrate its annotation budget to query
languages it is less certain on. We illustrate the effectiveness of our
proposed method on a diverse set of tasks: a classification task with 4
languages, a sequence tagging task with 4 languages and a dependency parsing
task with 5 languages. Our proposed method, whilst simple, substantially
outperforms the other viable alternatives for building a model in a
multilingual setting under constrained budgets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task2Dial: A Novel Task and Dataset for Commonsense enhanced Task-based Dialogue Grounded in Documents. (arXiv:2204.01061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01061">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel task on commonsense-enhanced task-based dialogue
grounded in documents and describes the Task2Dial dataset, a novel dataset of
document-grounded task-based dialogues, where an Information Giver (IG)
provides instructions (by consulting a document) to an Information Follower
(IF), so that the latter can successfully complete the task. In this unique
setting, the IF can ask clarification questions which may not be grounded in
the underlying document and require commonsense knowledge to be answered. The
Task2Dial dataset poses new challenges: (1) its human reference texts show more
lexical richness and variation than other document-grounded dialogue datasets;
(2) generating from this set requires paraphrasing as instructional responses
might have been modified from the underlying document; (3) requires commonsense
knowledge, since questions might not necessarily be grounded in the document;
(4) generating requires planning based on context, as task steps need to be
provided in order. The Task2Dial dataset contains dialogues with an average
$18.15$ number of turns and 19.79 tokens per turn, as compared to 12.94 and 12
respectively in existing datasets. As such, learning from this dataset promises
more natural, varied and less template-like system utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A sequence-to-sequence approach for document-level relation extraction. (arXiv:2204.01098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01098">
<div class="article-summary-box-inner">
<span><p>Motivated by the fact that many relations cross the sentence boundary, there
has been increasing interest in document-level relation extraction (DocRE).
DocRE requires integrating information within and across sentences, capturing
complex interactions between mentions of entities. Most existing methods are
pipeline-based, requiring entities as input. However, jointly learning to
extract entities and relations can improve performance and be more efficient
due to shared parameters and training steps. In this paper, we develop a
sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE
(entity extraction, coreference resolution and relation extraction) end-to-end,
replacing a pipeline of task-specific components. Using a simple strategy we
call entity hinting, we compare our approach to existing pipeline-based methods
on several popular biomedical datasets, in some cases exceeding their
performance. We also report the first end-to-end results on these datasets for
future comparison. Finally, we demonstrate that, under our model, an end-to-end
approach outperforms a pipeline-based approach. Our code, data and trained
models are available at {\small{\url{https://github.com/johngiorgi/seq2rel}}}.
An online demo is available at
{\small{\url{https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py}}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pragmatic constraints and pronoun reference disambiguation: the possible and the impossible. (arXiv:2204.01166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01166">
<div class="article-summary-box-inner">
<span><p>Pronoun disambiguation in understanding text and discourse often requires the
application of both general pragmatic knowledge and context-specific
information. In AI and linguistics research, this has mostly been studied in
cases where the referent is explicitly stated in the preceding text nearby.
However, pronouns in natural text often refer to entities, collections, or
events that are only implicit mentioned previously; in those cases the need to
use pragmatic knowledge to disambiguate becomes much more acute and the
characterization of the knowledge becomes much more difficult. Extended
literary texts at times employ both extremely complex patterns of reference and
extremely rich and subtle forms of knowledge. Indeed, it is occasionally
possible to have a pronoun that is far separated from its referent in a text.
In the opposite direction, pronoun use is affected by considerations of focus
of attention and by formal constraints such as a preference for parallel
syntactic structures; these can be so strong that no pragmatic knowledge
suffices to overrule them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation. (arXiv:2204.01171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01171">
<div class="article-summary-box-inner">
<span><p>Current language generation models suffer from issues such as repetition,
incoherence, and hallucinations. An often-repeated hypothesis is that this
brittleness of generation models is caused by the training and the generation
procedure mismatch, also referred to as exposure bias. In this paper, we verify
this hypothesis by analyzing exposure bias from an imitation learning
perspective. We show that exposure bias leads to an accumulation of errors,
analyze why perplexity fails to capture this accumulation, and empirically show
that this accumulation results in poor generation quality. Source code to
reproduce these experiments is available at
https://github.com/kushalarora/quantifying_exposure_bias
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PERFECT: Prompt-free and Efficient Few-shot Learning with Language Models. (arXiv:2204.01172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01172">
<div class="article-summary-box-inner">
<span><p>Current methods for few-shot fine-tuning of pretrained masked language models
(PLMs) require carefully engineered prompts and verbalizers for each new task
to convert examples into a cloze-format that the PLM can score. In this work,
we propose PERFECT, a simple and efficient method for few-shot fine-tuning of
PLMs without relying on any such handcrafting, which is highly effective given
as few as 32 data points. PERFECT makes two key design choices: First, we show
that manually engineered task prompts can be replaced with task-specific
adapters that enable sample-efficient fine-tuning and reduce memory and storage
costs by roughly factors of 5 and 100, respectively. Second, instead of using
handcrafted verbalizers, we learn new multi-token label embeddings during
fine-tuning, which are not tied to the model vocabulary and which allow us to
avoid complex auto-regressive decoding. These embeddings are not only learnable
from limited data but also enable nearly 100x faster training and inference.
Experiments on a wide range of few-shot NLP tasks demonstrate that PERFECT,
while being simple and efficient, also outperforms existing state-of-the-art
few-shot learning methods. Our code is publicly available at
https://github.com/rabeehk/perfect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Part-of-Speech Tagger for Yiddish: First Steps in Tagging the Yiddish Book Center Corpus. (arXiv:2204.01175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01175">
<div class="article-summary-box-inner">
<span><p>We describe the construction and evaluation of a part-of-speech tagger for
Yiddish (the first one, to the best of our knowledge). This is the first step
in a larger project of automatically assigning part-of-speech tags and
syntactic structure to Yiddish text for purposes of linguistic research. We
combine two resources for the current work - an 80K word subset of the Penn
Parsed Corpus of Historical Yiddish (PPCHY) (Santorini, 2021) and 650 million
words of OCR'd Yiddish text from the Yiddish Book Center (YBC). We compute word
embeddings on the YBC corpus, and these embeddings are used with a tagger model
trained and evaluated on the PPCHY. Yiddish orthography in the YBC corpus has
many spelling inconsistencies, and we present some evidence that even simple
non-contextualized embeddings are able to capture the relationships among
spelling variants without the need to first "standardize" the corpus. We
evaluate the tagger performance on a 10-fold cross-validation split, with and
without the embeddings, showing that the embeddings improve tagger performance.
However, a great deal of work remains to be done, and we conclude by discussing
some next steps, including the need for additional annotated training and test
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Text Generation via Variational Encoder-Decoder Models with Gaussian Process Priors. (arXiv:2204.01227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01227">
<div class="article-summary-box-inner">
<span><p>Generating high quality texts with high diversity is important for many NLG
applications, but current methods mostly focus on building deterministic models
to generate higher quality texts and do not provide many options for promoting
diversity. In this work, we present a novel latent structured variable model to
generate high quality texts by enriching contextual representation learning of
encoder-decoder models. Specifically, we introduce a stochastic function to map
deterministic encoder hidden states into random context variables. The proposed
stochastic function is sampled from a Gaussian process prior to (1) provide
infinite number of joint Gaussian distributions of random context variables
(diversity-promoting) and (2) explicitly model dependency between context
variables (accurate-encoding). To address the learning challenge of Gaussian
processes, we propose an efficient variational inference approach to
approximate the posterior distribution of random context variables. We evaluate
our method in two typical text generation tasks: paraphrase generation and text
style transfer. Experimental results on benchmark datasets demonstrate that our
method improves the generation quality and diversity compared with other
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Joint Speech-Text Embeddings for Semantic Matching. (arXiv:2204.01235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01235">
<div class="article-summary-box-inner">
<span><p>Embeddings play an important role in many recent end-to-end solutions for
language processing problems involving more than one data modality. Although
there has been some effort to understand the properties of single-modality
embedding spaces, particularly that of text, their cross-modal counterparts are
less understood. In this work, we study a joint speech-text embedding space
trained for semantic matching by minimizing the distance between paired
utterance and transcription inputs. This was done through dual encoders in a
teacher-student model setup, with a pretrained language model acting as the
teacher and a transformer-based speech encoder as the student. We extend our
method to incorporate automatic speech recognition through both pretraining and
multitask scenarios and found that both approaches improve semantic matching.
Multiple techniques were utilized to analyze and evaluate cross-modal semantic
alignment of the embeddings: a quantitative retrieval accuracy metric,
zero-shot classification to investigate generalizability, and probing of the
encoders to observe the extent of knowledge transfer from one modality to
another.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pragmatics-Centered Evaluation Framework for Natural Language Understanding. (arXiv:1907.08672v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.08672">
<div class="article-summary-box-inner">
<span><p>New models for natural language understanding have recently made an
unparalleled amount of progress, which has led some researchers to suggest that
the models induce universal text representations. However, current benchmarks
are predominantly targeting semantic phenomena; we make the case that
pragmatics needs to take center stage in the evaluation of natural language
understanding. We introduce PragmEval, a new benchmark for the evaluation of
natural language understanding, that unites 11 pragmatics-focused evaluation
datasets for English. PragmEval can be used as supplementary training data in a
multi-task learning setup, and is publicly available, alongside the code for
gathering and preprocessing the datasets. Using our evaluation suite, we show
that natural language inference, a widely used pretraining task, does not
result in genuinely universal representations, which presents a new challenge
for multi-task learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective-corrected Spatial Referring Expression Generation for Human-Robot Interaction. (arXiv:2104.01558v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01558">
<div class="article-summary-box-inner">
<span><p>Intelligent robots designed to interact with humans in real scenarios need to
be able to refer to entities actively by natural language. In spatial referring
expression generation, the ambiguity is unavoidable due to the diversity of
reference frames, which will lead to an understanding gap between humans and
robots. To narrow this gap, in this paper, we propose a novel
perspective-corrected spatial referring expression generation (PcSREG) approach
for human-robot interaction by considering the selection of reference frames.
The task of referring expression generation is simplified into the process of
generating diverse spatial relation units. First, we pick out all landmarks in
these spatial relation units according to the entropy of preference and allow
its updating through a stack model. Then all possible referring expressions are
generated according to different reference frame strategies. Finally, we
evaluate every expression using a probabilistic referring expression resolution
model and find the best expression that satisfies both of the appropriateness
and effectiveness. We implement the proposed approach on a robot system and
empirical experiments show that our approach can generate more effective
spatial referring expressions for practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis. (arXiv:2104.06835v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06835">
<div class="article-summary-box-inner">
<span><p>Exploiting rich linguistic information in raw text is crucial for expressive
text-to-speech (TTS). As large scale pre-trained text representation develops,
bidirectional encoder representations from Transformers (BERT) has been proven
to embody semantic information and employed to TTS recently. However, original
or simply fine-tuned BERT embeddings still cannot provide sufficient semantic
knowledge that expressive TTS models should take into account. In this paper,
we propose a word-level semantic representation enhancing method based on
dependency structure and pre-trained BERT embedding. The BERT embedding of each
word is reprocessed considering its specific dependencies and related words in
the sentence, to generate more effective semantic representation for TTS. To
better utilize the dependency structure, relational gated graph network (RGGN)
is introduced to make semantic information flow and aggregate through the
dependency structure. The experimental results show that the proposed method
can further improve the naturalness and expressiveness of synthesized speeches
on both Mandarin and English datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation. (arXiv:2104.08704v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08704">
<div class="article-summary-box-inner">
<span><p>Large pretrained generative models like GPT-3 often suffer from hallucinating
non-existent or incorrect content, which undermines their potential merits in
real applications. Existing work usually attempts to detect these
hallucinations based on a corresponding oracle reference at a sentence or
document level. However ground-truth references may not be readily available
for many free-form text generation applications, and sentence- or
document-level detection may fail to provide the fine-grained signals that
would prevent fallacious content in real time. As a first step to addressing
these issues, we propose a novel token-level, reference-free hallucination
detection task and an associated annotated dataset named HaDes (HAllucination
DEtection dataSet). To create this dataset, we first perturb a large number of
text segments extracted from English language Wikipedia, and then verify these
with crowd-sourced annotations. To mitigate label imbalance during annotation,
we utilize an iterative model-in-loop strategy. We conduct comprehensive data
analyses and create multiple baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OntoED: Low-resource Event Detection with Ontology Embedding. (arXiv:2105.10922v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10922">
<div class="article-summary-box-inner">
<span><p>Event Detection (ED) aims to identify event trigger words from a given text
and classify it into an event type. Most of current methods to ED rely heavily
on training instances, and almost ignore the correlation of event types. Hence,
they tend to suffer from data scarcity and fail to handle new unseen event
types. To address these problems, we formulate ED as a process of event
ontology population: linking event instances to pre-defined event types in
event ontology, and propose a novel ED framework entitled OntoED with ontology
embedding. We enrich event ontology with linkages among event types, and
further induce more event-event correlations. Based on the event ontology,
OntoED can leverage and propagate correlation knowledge, particularly from
data-rich to data-poor event types. Furthermore, OntoED can be applied to new
unseen event types, by establishing linkages to existing ones. Experiments
indicate that OntoED is more predominant and robust than previous approaches to
ED, especially in data-scarce scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04570">
<div class="article-summary-box-inner">
<span><p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarIA: Spanish Language Models. (arXiv:2107.07253v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07253">
<div class="article-summary-box-inner">
<span><p>This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as
well as the corresponding performance evaluations. Both models were pre-trained
using the largest Spanish corpus known to date, with a total of 570GB of clean
and deduplicated text processed for this work, compiled from the web crawlings
performed by the National Library of Spain from 2009 to 2019. We extended the
current evaluation datasets with an extractive Question Answering dataset and
our models outperform the existing Spanish models across tasks and settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v9 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08597">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique for doing this is to apply named entity
disambiguation (NED) systems to the question, and retrieve KB facts for the
disambiguated entities. This work presents CLOCQ, an efficient method that
prunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses
a top-k query processor over score-ordered lists of KB items that combine
signals about lexical matching, relevance to the question, coherence among
candidate items, and connectivity in the KB graph. Experiments with two recent
QA benchmarks for complex questions demonstrate the superiority of CLOCQ over
state-of-the-art baselines with respect to answer presence, size of the search
space, and runtimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12584">
<div class="article-summary-box-inner">
<span><p>In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, it is imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Input Length Matters: Improving RNN-T and MWER Training for Long-form Telephony Speech Recognition. (arXiv:2110.03841v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03841">
<div class="article-summary-box-inner">
<span><p>End-to-end models have achieved state-of-the-art results on several automatic
speech recognition tasks. However, they perform poorly when evaluated on
long-form data, e.g., minutes long conversational telephony audio. One reason
the model fails on long-form speech is that it has only seen short utterances
during training. In this paper we study the effect of training utterance length
on the word error rate (WER) for RNN-transducer (RNN-T) model. We compare two
widely used training objectives, log loss (or RNN-T loss) and minimum word
error rate (MWER) loss. We conduct experiments on telephony datasets in four
languages. Our experiments show that for both losses, the WER on long-form
speech reduces substantially as the training utterance length increases. The
average relative WER gain is 15.7% for log loss and 8.8% for MWER loss. When
training on short utterances, MWER loss leads to a lower WER than the log loss.
Such difference between the two losses diminishes when the input length
increases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MReD: A Meta-Review Dataset for Structure-Controllable Text Generation. (arXiv:2110.07474v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07474">
<div class="article-summary-box-inner">
<span><p>When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited. A typical example is
when using CNN/Daily Mail dataset for controllable text summarization, there is
no guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and the control signal to guide
the generation, which can only be built with a deep understanding of the domain
knowledge. Motivated by this vision, our paper introduces a new text generation
dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its
45k meta-review sentences are manually annotated with one of the 9 carefully
defined categories, including abstract, strength, decision, etc. We present
experimental results on start-of-the-art summarization models, and propose
methods for structure-controlled generation with both extractive and
abstractive models using our annotated data. By exploring various settings and
analyzing the model behavior with respect to the control signal, we demonstrate
the challenges of our proposed task and the values of our dataset MReD.
Meanwhile, MReD also allows us to have a better understanding of the
meta-review domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07480">
<div class="article-summary-box-inner">
<span><p>Nested entities are observed in many domains due to their compositionality,
which cannot be easily recognized by the widely-used sequence labeling
framework. A natural solution is to treat the task as a span classification
problem. To learn better span representation and increase classification
performance, it is crucial to effectively integrate heterogeneous factors
including inside tokens, boundaries, labels, and related spans which could be
contributing to nested entities recognition. To fuse these heterogeneous
factors, we propose a novel triaffine mechanism including triaffine attention
and scoring. Triaffine attention uses boundaries and labels as queries and uses
inside tokens and related spans as keys and values for span representations.
Triaffine scoring interacts with boundaries and span representations for
classification. Experiments show that our proposed method outperforms previous
span-based methods, achieves the state-of-the-art $F_1$ scores on nested NER
datasets GENIA and KBP2017, and shows comparable results on ACE2004 and
ACE2005.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances. (arXiv:2110.07592v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07592">
<div class="article-summary-box-inner">
<span><p>Toxic speech, also known as hate speech, is regarded as one of the crucial
issues plaguing online social media today. Most recent work on toxic speech
detection is constrained to the modality of text and written conversations with
very limited work on toxicity detection from spoken utterances or using the
modality of speech. In this paper, we introduce a new dataset DeToxy, the first
publicly available toxicity annotated dataset for the English language. DeToxy
is sourced from various openly available speech databases and consists of over
2 million utterances. We believe that our dataset would act as a benchmark for
the relatively new and un-explored Spoken Language Processing task of detecting
toxicity from spoken utterances and boost further research in this space.
Finally, we also provide strong unimodal baselines for our dataset and compare
traditional two-step and E2E approaches. Our experiments show that in the case
of spoken utterances, text-based approaches are largely dependent on gold
human-annotated transcripts for their performance and also suffer from the
problem of keyword bias. However, the presence of speech files in DeToxy helps
facilitates the development of E2E speech models which alleviate both the
above-stated problems by better capturing speech clues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07855">
<div class="article-summary-box-inner">
<span><p>Abstract Meaning Representation (AMR) parsing aims to translate sentences to
semantic representation with a hierarchical structure, and is recently
empowered by pretrained sequence-to-sequence models. However, there exists a
gap between their flat training objective (i.e., equally treats all output
tokens) and the hierarchical AMR structure, which limits the model
generalization. To bridge this gap, we propose a Hierarchical Curriculum
Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula
(IC). SC switches progressively from core to detail AMR semantic elements while
IC transits from structure-simple to -complex AMR instances during training.
Through these two warming-up processes, HCL reduces the difficulty of learning
complex structures, thus the flat model can better adapt to the AMR hierarchy.
Extensive experiments on AMR2.0, AMR3.0, structure-complex and
out-of-distribution situations verify the effectiveness of HCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark. (arXiv:2110.08466v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08466">
<div class="article-summary-box-inner">
<span><p>Dialogue safety problems severely limit the real-world deployment of neural
conversational models and have attracted great research interests recently.
However, dialogue safety problems remain under-defined and the corresponding
dataset is scarce. We propose a taxonomy for dialogue safety specifically
designed to capture unsafe behaviors in human-bot dialogue settings, with
focuses on context-sensitive unsafety, which is under-explored in prior works.
To spur research in this direction, we compile DiaSafety, a dataset with rich
context-sensitive unsafe examples. Experiments show that existing safety
guarding tools fail severely on our dataset. As a remedy, we train a dialogue
safety classifier to provide a strong baseline for context-sensitive dialogue
unsafety detection. With our classifier, we perform safety evaluations on
popular conversational models and show that existing dialogue systems still
exhibit concerning context-sensitive safety problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. (arXiv:2110.08527v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08527">
<div class="article-summary-box-inner">
<span><p>Recent work has shown pre-trained language models capture social biases from
the large amounts of text they are trained on. This has attracted attention to
developing techniques that mitigate such biases. In this work, we perform an
empirical survey of five recently proposed bias mitigation techniques:
Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace
Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of
each technique using three intrinsic bias benchmarks while also measuring the
impact of these techniques on a model's language modeling ability, as well as
its performance on downstream NLU tasks. We experimentally find that: (1)
Self-Debias is the strongest debiasing technique, obtaining improved scores on
all bias benchmarks; (2) Current debiasing techniques perform less consistently
when mitigating non-gender biases; And (3) improvements on bias benchmarks such
as StereoSet and CrowS-Pairs by using debiasing strategies are often
accompanied by a decrease in language modeling ability, making it difficult to
determine whether the bias mitigation was effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DECAR: Deep Clustering for learning general-purpose Audio Representations. (arXiv:2110.08895v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08895">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce DECAR (DEep Clustering for learning
general-purpose Audio Representations), a self-supervised pre-training approach
for learning general-purpose audio representations. Our system is based on
clustering: it utilizes an offline clustering step to produce pseudo-labels and
trains the network with a classification loss supervised by these
pseudo-labels. We develop on top of recent advances in self-supervised learning
for computer vision and design a lightweight, easy-to-use, self-supervised
pre-training scheme for learning audio representations. We pre-train DECAR
embeddings on a balanced subset of the large-scale AudioSet dataset and FSD50K
and evaluate our representations on the LAPE Benchmark consisting of 11
downstream classification tasks, including speech, music, animal sounds, and
acoustic scenes. Experimental results show that DECAR achieves results
competitive to the state-of-the-art on both linear evaluation and transfer
learning evaluation paradigms across all the downstream tasks in LAPE and
performs better than other prior-art in literature with just 15% of the total
amount of data available for pre-training. Furthermore, we conduct ablation
studies identifying key design choices and also make all our code and
pre-trained models publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Referring Video Object Segmentation with Multimodal Transformers. (arXiv:2111.14821v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14821">
<div class="article-summary-box-inner">
<span><p>The referring video object segmentation task (RVOS) involves segmentation of
a text-referred object instance in the frames of a given video. Due to the
complex nature of this multimodal task, which combines text reasoning, video
understanding, instance segmentation and tracking, existing approaches
typically rely on sophisticated pipelines in order to tackle it. In this paper,
we propose a simple Transformer-based approach to RVOS. Our framework, termed
Multimodal Tracking Transformer (MTTR), models the RVOS task as a sequence
prediction problem. Following recent advancements in computer vision and
natural language processing, MTTR is based on the realization that video and
text can be processed together effectively and elegantly by a single multimodal
Transformer model. MTTR is end-to-end trainable, free of text-related inductive
bias components and requires no additional mask-refinement post-processing
steps. As such, it simplifies the RVOS pipeline considerably compared to
existing methods. Evaluation on standard benchmarks reveals that MTTR
significantly outperforms previous art across multiple metrics. In particular,
MTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and
JHMDB-Sentences datasets respectively, while processing 76 frames per second.
In addition, we report strong results on the public validation set of
Refer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the
attention of researchers. The code to reproduce our experiments is available at
https://github.com/mttr2021/MTTR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge. (arXiv:2112.08619v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08619">
<div class="article-summary-box-inner">
<span><p>Humans usually have conversations by making use of prior knowledge about a
topic and background information of the people whom they are talking to.
However, existing conversational agents and datasets do not consider such
comprehensive information, and thus they have a limitation in generating the
utterances where the knowledge and persona are fused properly. To address this
issue, we introduce a call For Customized conversation (FoCus) dataset where
the customized answers are built with the user's persona and Wikipedia
knowledge. To evaluate the abilities to make informative and customized
utterances of pre-trained language models, we utilize BART and GPT-2 as well as
transformer-based models. We assess their generation abilities with automatic
scores and conduct human evaluations for qualitative results. We examine
whether the model reflects adequate persona and knowledge with our proposed two
sub-tasks, persona grounding (PG) and knowledge grounding (KG). Moreover, we
show that the utterances of our data are constructed with the proper knowledge
and persona through grounding quality assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-driven Semantic Segmentation. (arXiv:2201.03546v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03546">
<div class="article-summary-box-inner">
<span><p>We present LSeg, a novel model for language-driven semantic image
segmentation. LSeg uses a text encoder to compute embeddings of descriptive
input labels (e.g., "grass" or "building") together with a transformer-based
image encoder that computes dense per-pixel embeddings of the input image. The
image encoder is trained with a contrastive objective to align pixel embeddings
to the text embedding of the corresponding semantic class. The text embeddings
provide a flexible label representation in which semantically similar labels
map to similar regions in the embedding space (e.g., "cat" and "furry"). This
allows LSeg to generalize to previously unseen categories at test time, without
retraining or even requiring a single additional training sample. We
demonstrate that our approach achieves highly competitive zero-shot performance
compared to existing zero- and few-shot semantic segmentation methods, and even
matches the accuracy of traditional segmentation algorithms when a fixed label
set is provided. Code and demo are available at
https://github.com/isl-org/lang-seg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Stop Sets on Stopping Active Learning for Text Classification. (arXiv:2201.05460v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05460">
<div class="article-summary-box-inner">
<span><p>Active learning is an increasingly important branch of machine learning and a
powerful technique for natural language processing. The main advantage of
active learning is its potential to reduce the amount of labeled data needed to
learn high-performing models. A vital aspect of an effective active learning
algorithm is the determination of when to stop obtaining additional labeled
data. Several leading state-of-the-art stopping methods use a stop set to help
make this decision. However, there has been relatively less attention given to
the choice of stop set than to the stopping algorithms that are applied on the
stop set. Different choices of stop sets can lead to significant differences in
stopping method performance. We investigate the impact of different stop set
choices on different stopping methods. This paper shows the choice of the stop
set can have a significant impact on the performance of stopping methods and
the impact is different for stability-based methods from that on
confidence-based methods. Furthermore, the unbiased representative stop sets
suggested by original authors of methods work better than the systematically
biased stop sets used in recently published work, and stopping methods based on
stabilizing predictions have stronger performance than confidence-based
stopping methods when unbiased representative stop sets are used. We provide
the largest quantity of experimental results on the impact of stop sets to
date. The findings are important for helping to illuminate the impact of this
important aspect of stopping methods that has been under-considered in recently
published work and that can have a large practical impact on the performance of
stopping methods for important semantic computing applications such as
technology assisted review and text classification more broadly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer. (arXiv:2202.07543v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07543">
<div class="article-summary-box-inner">
<span><p>Memes are prevalent on the internet and continue to grow and evolve alongside
our culture. An automatic understanding of memes propagating on the internet
can shed light on the general sentiment and cultural attitudes of people. In
this work, we present team BLUE's solution for the second edition of the
MEMOTION shared task. We showcase two approaches for meme classification (i.e.
sentiment, humour, offensive, sarcasm and motivation levels) using a text-only
method using BERT, and a Multi-Modal-Multi-Task transformer network that
operates on both the meme image and its caption to output the final scores. In
both approaches, we leverage state-of-the-art pretrained models for text (BERT,
Sentence Transformer) and image processing (EfficientNetV4, CLIP). Through our
efforts, we obtain first place in task A, second place in task B and third
place in task C. In addition, our team obtained the highest average score for
all three tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09662">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models are able to generate fluent text and be
efficiently adapted across various natural language generation tasks. However,
language models that are pretrained on large unlabeled web text corpora have
been shown to suffer from degenerating toxic content and social bias behaviors,
consequently hindering their safe deployment. Various detoxification methods
were proposed to mitigate the language model's toxicity; however, these methods
struggled to detoxify language models when conditioned on prompts that contain
specific social identities related to gender, race, or religion. In this study,
we propose Reinforce-Detoxify; A reinforcement learning-based method for
mitigating toxicity in language models. We address the challenge of safety in
language models and propose a new reward model that is able to detect toxic
content and mitigate unintended bias towards social identities in toxicity
prediction. The experiments demonstrate that the Reinforce-Detoxify method for
language model detoxification outperforms existing detoxification approaches in
automatic evaluation metrics, indicating the ability of our approach in
language model detoxification and less prone to unintended bias toward social
identities in generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking and Refining the Distinct Metric. (arXiv:2202.13587v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13587">
<div class="article-summary-box-inner">
<span><p>Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for
evaluating diversity in language generation tasks. However, we observed that
the original approach for calculating distinct scores has evident biases that
tend to assign higher penalties to longer sequences. We refine the calculation
of distinct scores by scaling the number of distinct tokens based on their
expectations. We provide both empirical and theoretical evidence to show that
our method effectively removes the biases existing in the original distinct
score. Our experiments show that our proposed metric,
\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human
judgment in evaluating response diversity. To foster future research, we
provide an example implementation at
\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language. (arXiv:2203.03598v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03598">
<div class="article-summary-box-inner">
<span><p>Learning to classify video data from classes not included in the training
data, i.e. video-based zero-shot learning, is challenging. We conjecture that
the natural alignment between the audio and visual modalities in video data
provides a rich training signal for learning discriminative multi-modal
representations. Focusing on the relatively underexplored task of audio-visual
zero-shot learning, we propose to learn multi-modal representations from
audio-visual data using cross-modal attention and exploit textual label
embeddings for transferring knowledge from seen classes to unseen classes.
Taking this one step further, in our generalised audio-visual zero-shot
learning setting, we include all the training classes in the test-time search
space which act as distractors and increase the difficulty while making the
setting more realistic. Due to the lack of a unified benchmark in this domain,
we introduce a (generalised) zero-shot learning benchmark on three audio-visual
datasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet,
ensuring that the unseen test classes do not appear in the dataset used for
supervised training of the backbone deep models. Comparing multiple relevant
and recent methods, we demonstrate that our proposed AVCA model achieves
state-of-the-art performance on all three datasets. Code and data are available
at \url{https://github.com/ExplainableML/AVCA-GZSL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2203.07376v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07376">
<div class="article-summary-box-inner">
<span><p>Recently, context-dependent text-to-SQL semantic parsing which translates
natural language into SQL in an interaction process has attracted a lot of
attention. Previous works leverage context-dependence information either from
interaction history utterances or the previous predicted SQL queries but fail
in taking advantage of both since of the mismatch between natural language and
logic-form SQL. In this work, we propose a History Information Enhanced
text-to-SQL model (HIE-SQL) to exploit context-dependence information from both
history utterances and the last predicted SQL query. In view of the mismatch,
we treat natural language and SQL as two modalities and propose a bimodal
pre-trained model to bridge the gap between them. Besides, we design a
schema-linking graph to enhance connections from utterances and the SQL query
to the database schema. We show our history information enhanced methods
improve the performance of HIE-SQL by a significant margin, which achieves new
state-of-the-art results on the two context-dependent text-to-SQL benchmarks,
the SparC and CoSQL datasets, at the writing time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension. (arXiv:2203.11400v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11400">
<div class="article-summary-box-inner">
<span><p>One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis. (arXiv:2203.11562v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11562">
<div class="article-summary-box-inner">
<span><p>Speech synthesis has come a long way as current text-to-speech (TTS) models
can now generate natural human-sounding speech. However, most of the TTS
research focuses on using adult speech data and there has been very limited
work done on child speech synthesis. This study developed and validated a
training pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models
using child speech datasets. This approach adopts a multi-speaker TTS retuning
workflow to provide a transfer-learning pipeline. A publicly available child
speech dataset was cleaned to provide a smaller subset of approximately 19
hours, which formed the basis of our fine-tuning experiments. Both subjective
and objective evaluations were performed using a pretrained MOSNet for
objective evaluation and a novel subjective framework for mean opinion score
(MOS) evaluations. Subjective evaluations achieved the MOS of 3.95 for speech
intelligibility, 3.89 for voice naturalness, and 3.96 for voice consistency.
Objective evaluation using a pretrained MOSNet showed a strong correlation
between real and synthetic child voices. Speaker similarity was also verified
by calculating the cosine similarity between the embeddings of utterances. An
automatic speech recognition (ASR) model is also used to provide a word error
rate (WER) comparison between the real and synthetic child voices. The final
trained TTS model was able to synthesize child-like speech from reference audio
samples as short as 5 seconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11933">
<div class="article-summary-box-inner">
<span><p>Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mix and Match: Learning-free Controllable Text Generation using Energy Language Models. (arXiv:2203.13299v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13299">
<div class="article-summary-box-inner">
<span><p>Recent work on controlled text generation has either required attribute-based
fine-tuning of the base language model (LM), or has restricted the
parameterization of the attribute discriminator to be compatible with the base
autoregressive LM. In this work, we propose Mix and Match LM, a global
score-based alternative for controllable text generation that combines
arbitrary pre-trained black-box models for achieving the desired attributes in
the generated text without involving any fine-tuning or structural assumptions
about the black-box models. We interpret the task of controllable generation as
drawing samples from an energy-based model whose energy values are a linear
combination of scores from black-box models that are separately responsible for
fluency, the control attribute, and faithfulness to any conditioning context.
We use a Metropolis-Hastings sampling scheme to sample from this energy-based
model using bidirectional context and global attribute features. We validate
the effectiveness of our approach on various controlled generation and
style-based text revision tasks by outperforming recently proposed methods that
involve extra training, fine-tuning, or restrictive assumptions over the form
of models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Roadmap for Big Model. (arXiv:2203.14101v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14101">
<div class="article-summary-box-inner">
<span><p>With the rapid development of deep learning, training Big Models (BMs) for
multiple downstream tasks becomes a popular paradigm. Researchers have achieved
various outcomes in the construction of BMs and the BM application in many
fields. At present, there is a lack of research work that sorts out the overall
progress of BMs and guides the follow-up research. In this paper, we cover not
only the BM technologies themselves but also the prerequisites for BM training
and applications with BMs, dividing the BM review into four parts: Resource,
Models, Key Technologies and Application. We introduce 16 specific BM-related
topics in those four parts, they are Data, Knowledge, Computing System,
Parallel Training System, Language Model, Vision Model, Multi-modal Model,
Theory&amp;Interpretability, Commonsense Reasoning, Reliability&amp;Security,
Governance, Evaluation, Machine Translation, Text Generation, Dialogue and
Protein Research. In each topic, we summarize clearly the current studies and
propose some future research directions. At the end of this paper, we conclude
the further development of BMs in a more general view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14276">
<div class="article-summary-box-inner">
<span><p>While Natural Language Processing (NLP) algorithms keep reaching
unprecedented milestones, out-of-distribution generalization is still
challenging. In this paper we address the problem of multi-source adaptation to
unknown domains: Given labeled data from multiple source domains, we aim to
generalize to data drawn from target domains that are unknown to the algorithm
at training time. We present an algorithmic framework based on example-based
Hypernetwork adaptation: Given an input example, a T5 encoder-decoder first
generates a unique signature which embeds this example in the semantic space of
the source domains, and this signature is then fed into a Hypernetwork which
generates the weights of the task classifier. In an advanced version of our
model, the learned signature also serves for improving the representation of
the input example. In experiments with two tasks, sentiment classification and
natural language inference, across 29 adaptation settings, our algorithms
substantially outperform existing algorithms for this adaptation setup. To the
best of our knowledge, this is the first time Hypernetworks are applied to
domain adaptation or in example-based manner in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANNA: Enhanced Language Representation for Question Answering. (arXiv:2203.14507v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14507">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have brought significant improvements in
performance in a variety of natural language processing tasks. Most existing
models performing state-of-the-art results have shown their approaches in the
separate perspectives of data processing, pre-training tasks, neural network
modeling, or fine-tuning. In this paper, we demonstrate how the approaches
affect performance individually, and that the language model performs the best
results on a specific question answering task when those approaches are jointly
considered in pre-training models. In particular, we propose an extended
pre-training task, and a new neighbor-aware mechanism that attends neighboring
tokens more to capture the richness of context for pre-training language
modeling. Our best model achieves new state-of-the-art results of 95.7\% F1 and
90.6\% EM on SQuAD 1.1 and also outperforms existing pre-trained language
models such as RoBERTa, ALBERT, ELECTRA, and XLNet on the SQuAD 2.0 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrate Lattice-Free MMI into End-to-End Speech Recognition. (arXiv:2203.15614v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15614">
<div class="article-summary-box-inner">
<span><p>In automatic speech recognition (ASR) research, discriminative criteria have
achieved superior performance in DNN-HMM systems. Given this success, the
adoption of discriminative criteria is promising to boost the performance of
end-to-end (E2E) ASR systems. With this motivation, previous works have
introduced the minimum Bayesian risk (MBR, one of the discriminative criteria)
into E2E ASR systems. However, the effectiveness and efficiency of the
MBR-based methods are compromised: the MBR criterion is only used in system
training, which creates a mismatch between training and decoding; the
on-the-fly decoding process in MBR-based methods results in the need for
pre-trained models and slow training speeds. To this end, novel algorithms are
proposed in this work to integrate another widely used discriminative
criterion, lattice-free maximum mutual information (LF-MMI), into E2E ASR
systems not only in the training stage but also in the decoding process. The
proposed LF-MMI training and decoding methods show their effectiveness on two
widely used E2E frameworks: Attention-Based Encoder-Decoders (AEDs) and Neural
Transducers (NTs). Compared with MBR-based methods, the proposed LF-MMI method:
maintains the consistency between training and decoding; eschews the on-the-fly
decoding process; trains from randomly initialized models with superior
training efficiency. Experiments suggest that the LF-MMI method outperforms its
MBR counterparts and consistently leads to statistically significant
performance improvements on various frameworks and datasets from 30 hours to
14.3k hours. The proposed method achieves state-of-the-art (SOTA) results on
Aishell-1 (CER 4.10%) and Aishell-2 (CER 5.02%) datasets. Code is released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16487">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel
decoding paradigm for speeding up autoregressive translation without quality
loss, through the collaboration of autoregressive and non-autoregressive
translation (NAT) of the Transformer. At each decoding iteration, GAD
aggressively decodes a number of tokens in parallel as a draft with NAT and
then verifies them in the autoregressive manner, where only the tokens that
pass the verification are kept as decoded tokens. GAD can achieve the same
performance as autoregressive translation but much more efficiently because
both NAT drafting and autoregressive verification are fast due to parallel
computing. We conduct experiments in the WMT14 English-German translation task
and confirm that the vanilla GAD yields exactly the same results as greedy
decoding with an around 3x speedup, and that its variant (GAD++) with an
advanced verification strategy not only outperforms the greedy translation and
even achieves the comparable translation quality with the beam search result,
but also further improves the decoding speed, resulting in an around 5x speedup
over autoregressive translation. Our models and codes are available at
https://github.com/hemingkx/Generalized-Aggressive-Decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16973">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) to learn high-level speech representations has
been a popular approach to building Automatic Speech Recognition (ASR) systems
in low-resource settings. However, the common assumption made in literature is
that a considerable amount of unlabeled data is available for the same domain
or language that can be leveraged for SSL pre-training, which we acknowledge is
not feasible in a real-world setting. In this paper, as part of the Interspeech
Gram Vaani ASR challenge, we try to study the effect of domain, language,
dataset size, and other aspects of our upstream pre-training SSL data on the
final performance low-resource downstream ASR task. We also build on the
continued pre-training paradigm to study the effect of prior knowledge
possessed by models trained using SSL. Extensive experiments and studies reveal
that the performance of ASR systems is susceptible to the data used for SSL
pre-training. Their performance improves with an increase in similarity and
volume of pre-training data. We believe our work will be helpful to the speech
community in building better ASR systems in low-resource settings and steer
research towards improving generalization in SSL-based pre-training for speech
systems.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual explanations for polyp detection: How medical doctors assess intrinsic versus extrinsic explanations. (arXiv:2204.00617v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00617">
<div class="article-summary-box-inner">
<span><p>Deep learning has in recent years achieved immense success in all areas of
computer vision and has the potential of assisting medical doctors in analyzing
visual content for disease and other abnormalities. However, the current state
of deep learning is very much a black box, making medical professionals highly
skeptical about integrating these methods into clinical practice. Several
methods have been proposed in order to shine some light onto these black boxes,
but there is no consensus on the opinion of the medical doctors that will
consume these explanations. This paper presents a study asking medical doctors
about their opinion of current state-of-the-art explainable artificial
intelligence methods when applied to a gastrointestinal disease detection use
case. We compare two different categories of explanation methods, intrinsic and
extrinsic, and gauge their opinion of the current value of these explanations.
The results indicate that intrinsic explanations are preferred and that
explanation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Lymph Node Detection in T2 MRI using Neural Networks. (arXiv:2204.00622v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00622">
<div class="article-summary-box-inner">
<span><p>Purpose: Identification of abdominal Lymph Nodes (LN) that are suspicious for
metastasis in T2 Magnetic Resonance Imaging (MRI) scans is critical for staging
of lymphoproliferative diseases. Prior work on LN detection has been limited to
specific anatomical regions of the body (pelvis, rectum) in single MR slices.
Therefore, the development of a universal approach to detect LN in full T2 MRI
volumes is highly desirable.
</p>
<p>Methods: In this study, a Computer Aided Detection (CAD) pipeline to
universally identify abdominal LN in volumetric T2 MRI using neural networks is
proposed. First, we trained various neural network models for detecting LN:
Faster RCNN with and without Hard Negative Example Mining (HNEM), FCOS,
FoveaBox, VFNet, and Detection Transformer (DETR). Next, we show that the
state-of-the-art (SOTA) VFNet model with Adaptive Training Sample Selection
(ATSS) outperforms Faster RCNN with HNEM. Finally, we ensembled models that
surpassed a 45% mAP threshold. We found that the VFNet model and one-stage
model ensemble can be interchangeably used in the CAD pipeline.
</p>
<p>Results: Experiments on 122 test T2 MRI volumes revealed that VFNet achieved
a 51.1% mAP and 78.7% recall at 4 false positives (FP) per volume, while the
one-stage model ensemble achieved a mAP of 52.3% and sensitivity of 78.7% at
4FP.
</p>
<p>Conclusion: Our contribution is a CAD pipeline that detects LN in T2 MRI
volumes, resulting in a sensitivity improvement of $\sim$14 points over the
current SOTA method for LN detection (sensitivity of 78.7% at 4 FP vs. 64.6% at
5 FP per volume).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Image Super-Resolution with Deep Modeling of Image Statistics. (arXiv:2204.00623v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00623">
<div class="article-summary-box-inner">
<span><p>Modeling statistics of image priors is useful for image super-resolution, but
little attention has been paid from the massive works of deep learning-based
methods. In this work, we propose a Bayesian image restoration framework, where
natural image statistics are modeled with the combination of smoothness and
sparsity priors. Concretely, firstly we consider an ideal image as the sum of a
smoothness component and a sparsity residual, and model real image degradation
including blurring, downscaling, and noise corruption. Then, we develop a
variational Bayesian approach to infer their posteriors. Finally, we implement
the variational approach for single image super-resolution (SISR) using deep
neural networks, and propose an unsupervised training strategy. The experiments
on three image restoration tasks, \textit{i.e.,} ideal SISR, realistic SISR,
and real-world SISR, demonstrate that our method has superior model
generalizability against varying noise levels and degradation kernels and is
effective in unsupervised SISR. The code and resulting models are released via
\url{https://zmiclab.github.io/projects.html}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable and Interpretable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning. (arXiv:2204.00624v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00624">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an explainable and interpretable diabetic
retinopathy (ExplainDR) classification model based on neural-symbolic learning.
To gain explainability, a highlevel symbolic representation should be
considered in decision making. Specifically, we introduce a human-readable
symbolic representation, which follows a taxonomy style of diabetic retinopathy
characteristics related to eye health conditions to achieve explainability. We
then include humanreadable features obtained from the symbolic representation
in the disease prediction. Experimental results on a diabetic retinopathy
classification dataset show that our proposed ExplainDR method exhibits
promising performance when compared to that from state-of-the-art methods
applied to the IDRiD dataset, while also providing interpretability and
explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Neural Acoustic Fields. (arXiv:2204.00628v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00628">
<div class="article-summary-box-inner">
<span><p>Our environment is filled with rich and dynamic acoustic information. When we
walk into a cathedral, the reverberations as much as appearance inform us of
the sanctuary's wide open space. Similarly, as an object moves around us, we
expect the sound emitted to also exhibit this movement. While recent advances
in learned implicit functions have led to increasingly higher quality
representations of the visual world, there have not been commensurate advances
in learning spatial auditory representations. To address this gap, we introduce
Neural Acoustic Fields (NAFs), an implicit representation that captures how
sounds propagate in a physical scene. By modeling acoustic propagation in a
scene as a linear time-invariant system, NAFs learn to continuously map all
emitter and listener location pairs to a neural impulse response function that
can then be applied to arbitrary sounds. We demonstrate that the continuous
nature of NAFs enables us to render spatial acoustics for a listener at an
arbitrary location, and can predict sound propagation at novel locations. We
further show that the representation learned by NAFs can help improve visual
learning with sparse views. Finally, we show that a representation informative
of scene structure emerges during the learning of NAFs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopTemp: Parsing Precipitate Structure from Temper Topology. (arXiv:2204.00629v1 [cond-mat.mtrl-sci])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00629">
<div class="article-summary-box-inner">
<span><p>Technological advances are in part enabled by the development of novel
manufacturing processes that give rise to new materials or material property
improvements. Development and evaluation of new manufacturing methodologies is
labor-, time-, and resource-intensive expensive due to complex, poorly defined
relationships between advanced manufacturing process parameters and the
resulting microstructures. In this work, we present a topological
representation of temper (heat-treatment) dependent material micro-structure,
as captured by scanning electron microscopy, called TopTemp. We show that this
topological representation is able to support temper classification of
microstructures in a data limited setting, generalizes well to previously
unseen samples, is robust to image perturbations, and captures domain
interpretable features. The presented work outperforms conventional deep
learning baselines and is a first step towards improving understanding of
process parameters and resulting material properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extremely Low-light Image Enhancement with Scene Text Restoration. (arXiv:2204.00630v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00630">
<div class="article-summary-box-inner">
<span><p>Deep learning-based methods have made impressive progress in enhancing
extremely low-light images - the image quality of the reconstructed images has
generally improved. However, we found out that most of these methods could not
sufficiently recover the image details, for instance, the texts in the scene.
In this paper, a novel image enhancement framework is proposed to precisely
restore the scene texts, as well as the overall quality of the image
simultaneously under extremely low-light images conditions. Mainly, we employed
a self-regularised attention map, an edge map, and a novel text detection loss.
In addition, leveraging synthetic low-light images is beneficial for image
enhancement on the genuine ones in terms of text detection. The quantitative
and qualitative experimental results have shown that the proposed model
outperforms state-of-the-art methods in image restoration, text detection, and
text spotting on See In the Dark and ICDAR15 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00631">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT)s have recently become popular due to their
outstanding modeling capabilities, in particular for capturing long-range
information, and scalability to dataset and model sizes which has led to
state-of-the-art performance in various computer vision and medical image
analysis tasks. In this work, we introduce a unified framework consisting of
two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder
and Convolutional Neural Network (CNN) and transformer-based decoders. In the
proposed model, the encoder is linked to the decoder via skip connections at
five different resolutions with deep supervision. The design of proposed
architecture allows for meeting a wide range of trade-off requirements between
accuracy and computational cost. In addition, we present a methodology for
self-supervised pre-training of the encoder backbone via learning to predict
randomly masked volumetric tokens using contextual information of visible
tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered
from publicly available CT datasets, and present a systematic investigation of
various components such as masking ratio and patch size that affect the
representation learning capability and performance of downstream tasks. We
validate the effectiveness of our pre-training approach by fine-tuning and
testing our model on liver and liver tumor segmentation task using the Medical
Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance
in terms of various segmentation metrics. To demonstrate its generalizability,
we train and test the model on BraTS 21 dataset for brain tumor segmentation
using MRI images and outperform other methods in terms of Dice score. Code:
https://github.com/Project-MONAI/research-contributions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIMBAR: Single Image-Based Scene Relighting For Effective Data Augmentation For Automated Driving Vision Tasks. (arXiv:2204.00644v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00644">
<div class="article-summary-box-inner">
<span><p>Real-world autonomous driving datasets comprise of images aggregated from
different drives on the road. The ability to relight captured scenes to unseen
lighting conditions, in a controllable manner, presents an opportunity to
augment datasets with a richer variety of lighting conditions, similar to what
would be encountered in the real-world. This paper presents a novel image-based
relighting pipeline, SIMBAR, that can work with a single image as input. To the
best of our knowledge, there is no prior work on scene relighting leveraging
explicit geometric representations from a single image. We present qualitative
comparisons with prior multi-view scene relighting baselines. To further
validate and effectively quantify the benefit of leveraging SIMBAR for data
augmentation for automated driving vision tasks, object detection and tracking
experiments are conducted with a state-of-the-art method, a Multiple Object
Tracking Accuracy (MOTA) of 93.3% is achieved with CenterTrack on
SIMBAR-augmented KITTI - an impressive 9.0% relative improvement over the
baseline MOTA of 85.6% with CenterTrack on original KITTI, both models trained
from scratch and tested on Virtual KITTI. For more details and SIMBAR relit
datasets, please visit our project website (https://simbarv1.github.io/).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Neonatal Face Detection in Real-world Clinical Settings. (arXiv:2204.00655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00655">
<div class="article-summary-box-inner">
<span><p>Current face detection algorithms are extremely generalized and can obtain
decent accuracy when detecting the adult faces. These approaches are
insufficient when handling outlier cases, for example when trying to detect the
face of a neonate infant whose face composition and expressions are relatively
different than that of the adult. It is furthermore difficult when applied to
detect faces in a complicated setting such as the Neonate Intensive Care Unit.
By training a state-of-the-art face detection model, You-Only-Look-Once, on a
proprietary dataset containing labelled neonate faces in a clinical setting,
this work achieves near real time neonate face detection. Our preliminary
findings show an accuracy of 68.7%, compared to the off the shelf solution
which detected neonate faces with an accuracy of 7.37%. Although further
experiments are needed to validate our model, our results are promising and
prove the feasibility of detecting neonatal faces in challenging real-world
settings. The robust and real-time detection of neonatal faces would benefit
wide range of automated systems (e.g., pain recognition and surveillance) who
currently suffer from the time and effort due to the necessity of manual
annotations. To benefit the research community, we make our trained weights
publicly available at github(https://github.com/ja05haus/trained_neonate_face).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes. (arXiv:2204.00656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00656">
<div class="article-summary-box-inner">
<span><p>Most hard attention models initially observe a complete scene to locate and
sense informative glimpses, and predict class-label of a scene based on
glimpses. However, in many applications (e.g., aerial imaging), observing an
entire scene is not always feasible due to the limited time and resources
available for acquisition. In this paper, we develop a Sequential Transformers
Attention Model (STAM) that only partially observes a complete image and
predicts informative glimpse locations solely based on past glimpses. We design
our agent using DeiT-distilled and train it with a one-step actor-critic
algorithm. Furthermore, to improve classification performance, we introduce a
novel training objective, which enforces consistency between the class
distribution predicted by a teacher model from a complete image and the class
distribution predicted by our agent using glimpses. When the agent senses only
4% of the total image area, the inclusion of the proposed consistency loss in
our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW
datasets, respectively. Moreover, our agent outperforms previous
state-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on
ImageNet and fMoW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hazard Detection And Avoidance For The Nova-C Lander. (arXiv:2204.00660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00660">
<div class="article-summary-box-inner">
<span><p>In early 2022, Intuitive Machines' NOVA-C Lander will touch down on the lunar
surface becoming the first commercial endeavor to visit a celestial body.
NOVA-C will deliver six payloads to the lunar surface with various scientific
and engineering objectives, ushering in a new era of commercial space
exploration and utilization. However, to safely accomplish the mission, the
NOVA-C lander must ensure its landing site is free of hazards larger than 30 cm
and the slope of local terrain at touchdown is less than 10 degrees off
vertical. To accomplish this, NOVA-C utilizes Intuitive Machines' precision
navigation system, coupled with machine vision algorithms for scene reduction
and landing site characterization. A unique aspect to the NOVA-C approach is
the real-time nature of the hazard detection and avoidance algorithms--which
are performed 400 meters above and down range of the intended landing site and
completed within 15 seconds. In this paper, we review the theoretical
foundations for the hazard detection and avoidance algorithms, describe the
practical challenges of implementation on the NOVA-C flight computer, and
present test and analysis results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Audio-Video Modalities from Image Captions. (arXiv:2204.00679v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00679">
<div class="article-summary-box-inner">
<span><p>A major challenge in text-video and text-audio retrieval is the lack of
large-scale training data. This is unlike image-captioning, where datasets are
in the order of millions of samples. To close this gap we propose a new video
mining pipeline which involves transferring captions from image captioning
datasets to video clips with no additional manual effort. Using this pipeline,
we create a new large-scale, weakly labelled audio-video captioning dataset
consisting of millions of paired clips and captions. We show that training a
multimodal transformed based model on this data achieves competitive
performance on video retrieval and video captioning, matching or even
outperforming HowTo100M pretraining with 20x fewer clips. We also show that our
mined clips are suitable for text-audio pretraining, and achieve state of the
art results for the task of audio retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkeleVision: Towards Adversarial Resiliency of Person Tracking with Multi-Task Learning. (arXiv:2204.00734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00734">
<div class="article-summary-box-inner">
<span><p>Person tracking using computer vision techniques has wide ranging
applications such as autonomous driving, home security and sports analytics.
However, the growing threat of adversarial attacks raises serious concerns
regarding the security and reliability of such techniques. In this work, we
study the impact of multi-task learning (MTL) on the adversarial robustness of
the widely used SiamRPN tracker, in the context of person tracking.
Specifically, we investigate the effect of jointly learning with semantically
analogous tasks of person tracking and human keypoint detection. We conduct
extensive experiments with more powerful adversarial attacks that can be
physically realizable, demonstrating the practical value of our approach. Our
empirical study with simulated as well as real-world datasets reveals that
training with MTL consistently makes it harder to attack the SiamRPN tracker,
compared to typically training only on the single task of person tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00746">
<div class="article-summary-box-inner">
<span><p>We propose a novel one-stage Transformer-based semantic and spatial refined
transformer (SSRT) to solve the Human-Object Interaction detection task, which
requires to localize humans and objects, and predicts their interactions.
Differently from previous Transformer-based HOI approaches, which mostly focus
at improving the design of the decoder outputs for the final detection, SSRT
introduces two new modules to help select the most relevant object-action pairs
within an image and refine the queries' representation using rich semantic and
spatial features. These enhancements lead to state-of-the-art results on the
two most popular HOI benchmarks: V-COCO and HICO-DET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homography Loss for Monocular 3D Object Detection. (arXiv:2204.00754v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00754">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is an essential task in autonomous driving.
However, most current methods consider each 3D object in the scene as an
independent training sample, while ignoring their inherent geometric relations,
thus inevitably resulting in a lack of leveraging spatial constraints. In this
paper, we propose a novel method that takes all the objects into consideration
and explores their mutual relationships to help better estimate the 3D boxes.
Moreover, since 2D detection is more reliable currently, we also investigate
how to use the detected 2D boxes as guidance to globally constrain the
optimization of the corresponding predicted 3D boxes. To this end, a
differentiable loss function, termed as Homography Loss, is proposed to achieve
the goal, which exploits both 2D and 3D information, aiming at balancing the
positional relationships between different objects by global constraints, so as
to obtain more accurately predicted 3D boxes. Thanks to the concise design, our
loss function is universal and can be plugged into any mature monocular 3D
detector, while significantly boosting the performance over their baseline.
Experiments demonstrate that our method yields the best performance (Nov. 2021)
compared with the other state-of-the-arts by a large margin on KITTI 3D
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do learned representations respect causal relationships?. (arXiv:2204.00762v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00762">
<div class="article-summary-box-inner">
<span><p>Data often has many semantic attributes that are causally associated with
each other. But do attribute-specific learned representations of data also
respect the same causal relations? We answer this question in three steps.
First, we introduce NCINet, an approach for observational causal discovery from
high-dimensional data. It is trained purely on synthetically generated
representations and can be applied to real representations, and is specifically
designed to mitigate the domain gap between the two. Second, we apply NCINet to
identify the causal relations between image representations of different pairs
of attributes with known and unknown causal relations between the labels. For
this purpose, we consider image representations learned for predicting
attributes on the 3D Shapes, CelebA, and the CASIA-WebFace datasets, which we
annotate with multiple multi-class attributes. Third, we analyze the effect on
the underlying causal relation between learned representations induced by
various design choices in representation learning. Our experiments indicate
that (1) NCINet significantly outperforms existing observational causal
discovery approaches for estimating the causal relation between pairs of random
samples, both in the presence and absence of an unobserved confounder, (2)
under controlled scenarios, learned representations can indeed satisfy the
underlying causal relations between their respective labels, and (3) the causal
relations are positively correlated with the predictive capability of the
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images. (arXiv:2204.00790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00790">
<div class="article-summary-box-inner">
<span><p>Airports have an important role in both military and civilian domains. The
synthetic aperture radar (SAR) based airport detection has received increasing
attention in recent years. However, due to the high cost of SAR imaging and
annotation process, there is no publicly available SAR dataset for airport
detection. As a result, deep learning methods have not been fully used in
airport detection tasks. To provide a benchmark for airport detection research
in SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).
In order to adequately reflect the demands of real world applications, it
contains 624 SAR images from Sentinel 1B and covers 104 airfield instances with
different scales, orientations and shapes. The experiments of multiple deep
learning approach on this dataset proves its effectiveness. It developing
state-of-the-art airport area detection algorithms or other relevant tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IR-GAN: Image Manipulation with Linguistic Instruction by Increment Reasoning. (arXiv:2204.00792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00792">
<div class="article-summary-box-inner">
<span><p>Conditional image generation is an active research topic including text2image
and image translation.
</p>
<p>Recently image manipulation with linguistic instruction brings new challenges
of multimodal conditional generation.
</p>
<p>However, traditional conditional image generation models mainly focus on
generating high-quality and visually realistic images, and lack resolving the
partial consistency between image and instruction.
</p>
<p>To address this issue, we propose an Increment Reasoning Generative
Adversarial Network (IR-GAN), which aims to reason the consistency between
visual increment in images and semantic increment in instructions.
</p>
<p>First, we introduce the word-level and instruction-level instruction encoders
to learn user's intention from history-correlated instructions as semantic
increment.
</p>
<p>Second, we embed the representation of semantic increment into that of source
image for generating target image, where source image plays the role of
referring auxiliary.
</p>
<p>Finally, we propose a reasoning discriminator to measure the consistency
between visual increment and semantic increment, which purifies user's
intention and guarantees the good logic of generated target image.
</p>
<p>Extensive experiments and visualization conducted on two datasets show the
effectiveness of IR-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R(Det)^2: Randomized Decision Routing for Object Detection. (arXiv:2204.00794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00794">
<div class="article-summary-box-inner">
<span><p>In the paradigm of object detection, the decision head is an important part,
which affects detection performance significantly. Yet how to design a
high-performance decision head remains to be an open issue. In this paper, we
propose a novel approach to combine decision trees and deep neural networks in
an end-to-end learning manner for object detection. First, we disentangle the
decision choices and prediction values by plugging soft decision trees into
neural networks. To facilitate effective learning, we propose randomized
decision routing with node selective and associative losses, which can boost
the feature representative learning and network decision simultaneously.
Second, we develop the decision head for object detection with narrow branches
to generate the routing probabilities and masks, for the purpose of obtaining
divergent decisions from different nodes. We name this approach as the
randomized decision routing for object detection, abbreviated as R(Det)$^2$.
Experiments on MS-COCO dataset demonstrate that R(Det)$^2$ is effective to
improve the detection performance. Equipped with existing detectors, it
achieves $1.4\sim 3.6$\% AP improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Coherent Video Cartoonization with Perceptual Motion Consistency. (arXiv:2204.00795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00795">
<div class="article-summary-box-inner">
<span><p>In recent years, creative content generations like style transfer and neural
photo editing have attracted more and more attention. Among these,
cartoonization of real-world scenes has promising applications in entertainment
and industry. Different from image translations focusing on improving the style
effect of generated images, video cartoonization has additional requirements on
the temporal consistency. In this paper, we propose a spatially-adaptive
semantic alignment framework with perceptual motion consistency for coherent
video cartoonization in an unsupervised manner. The semantic alignment module
is designed to restore deformation of semantic structure caused by spatial
information lost in the encoder-decoder architecture. Furthermore, we devise
the spatio-temporal correlative map as a style-independent, global-aware
regularization on the perceptual motion consistency. Deriving from similarity
measurement of high-level features in photo and cartoon frames, it captures
global semantic information beyond raw pixel-value in optical flow. Besides,
the similarity measurement disentangles temporal relationships from
domain-specific style properties, which helps regularize the temporal
consistency without hurting style effects of cartoon images. Qualitative and
quantitative experiments demonstrate our method is able to generate highly
stylistic and temporal consistent cartoon videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RFVTM: A Recovery and Filtering Vertex Trichotomy Matching for Remote Sensing Image Registration. (arXiv:2204.00818v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00818">
<div class="article-summary-box-inner">
<span><p>Reliable feature point matching is a vital yet challenging process in
feature-based image registration. In this paper,a robust feature point matching
algorithm called Recovery and Filtering Vertex Trichotomy Matching (RFVTM) is
proposed to remove outliers and retain sufficient inliers for remote sensing
images. A novel affine invariant descriptor called vertex trichotomy descriptor
is proposed on the basis of that geometrical relations between any of vertices
and lines are preserved after affine transformations, which is constructed by
mapping each vertex into trichotomy sets. The outlier removals in Vertex
Trichotomy Matching (VTM) are implemented by iteratively comparing the
disparity of corresponding vertex trichotomy descriptors. Some inliers
mistakenly validated by a large amount of outliers are removed in VTM
iterations, and several residual outliers close to correct locations cannot be
excluded with the same graph structures. Therefore, a recovery and filtering
strategy is designed to recover some inliers based on identical vertex
trichotomy descriptors and restricted transformation errors. Assisted with the
additional recovered inliers, residual outliers can also be filtered out during
the process of reaching identical graph for the expanded vertex sets.
Experimental results demonstrate the superior performance on precision and
stability of this algorithm under various conditions, such as remote sensing
images with large transformations, duplicated patterns, or inconsistent
spectral content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Domain Generalized Segmentation. (arXiv:2204.00822v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00822">
<div class="article-summary-box-inner">
<span><p>Deep models trained on source domain lack generalization when evaluated on
unseen target domains with different data distributions. The problem becomes
even more pronounced when we have no access to target domain samples for
adaptation. In this paper, we address domain generalized semantic segmentation,
where a segmentation model is trained to be domain-invariant without using any
target domain data. Existing approaches to tackle this problem standardize data
into a unified distribution. We argue that while such a standardization
promotes global normalization, the resulting features are not discriminative
enough to get clear segmentation boundaries. To enhance separation between
categories while simultaneously promoting domain invariance, we propose a
framework including two novel modules: Semantic-Aware Normalization (SAN) and
Semantic-Aware Whitening (SAW). Specifically, SAN focuses on category-level
center alignment between features from different image styles, while SAW
enforces distributed alignment for the already center-aligned features. With
the help of SAN and SAW, we encourage both intra-category compactness and
inter-category separability. We validate our approach through extensive
experiments on widely-used datasets (i.e. GTAV, SYNTHIA, Cityscapes, Mapillary
and BDDS). Our approach shows significant improvements over existing
state-of-the-art on various backbone networks. Code is available at
https://github.com/leolyj/SAN-SAW
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Convolutional Re-parameterization. (arXiv:2204.00826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00826">
<div class="article-summary-box-inner">
<span><p>Structural re-parameterization has drawn increasing attention in various
computer vision tasks. It aims at improving the performance of deep models
without introducing any inference-time cost. Though efficient during inference,
such models rely heavily on the complicated training-time blocks to achieve
high accuracy, leading to large extra training cost. In this paper, we present
online convolutional re-parameterization (OREPA), a two-stage pipeline, aiming
to reduce the huge training overhead by squeezing the complex training-time
block into a single convolution. To achieve this goal, we introduce a linear
scaling layer for better optimizing the online blocks. Assisted with the
reduced training cost, we also explore some more effective re-param components.
Compared with the state-of-the-art re-param models, OREPA is able to save the
training-time memory cost by about 70% and accelerate the training speed by
around 2x. Meanwhile, equipped with OREPA, the models outperform previous
methods on ImageNet by up to +0.6%.We also conduct experiments on object
detection and semantic segmentation and show consistent improvements on the
downstream tasks. Codes are available at
https://github.com/JUGGHM/OREPA_CVPR2022 .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Registration of Images with Inconsistent Content Through Line-Support Region Segmentation and Geometrical Outlier Removal. (arXiv:2204.00832v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00832">
<div class="article-summary-box-inner">
<span><p>The implementation of automatic image registration is still difficult in
various applications. In this paper, an automatic image registration approach
through line-support region segmentation and geometrical outlier removal
(ALRS-GOR) is proposed. This new approach is designed to address the problems
associated with the registration of images with affine deformations and
inconsistent content, such as remote sensing images with different spectral
content or noise interference, or map images with inconsistent annotations. To
begin with, line-support regions, namely a straight region whose points share
roughly the same image gradient angle, are extracted to address the issues of
inconsistent content existing in images. To alleviate the incompleteness of
line segments, an iterative strategy with multi-resolution is employed to
preserve global structures that are masked at full resolution by image details
or noise. Then, Geometrical Outlier Removal (GOR) is developed to provide
reliable feature point matching, which is based on affineinvariant geometrical
classifications for corresponding matches initialized by SIFT. The candidate
outliers are selected by comparing the disparity of accumulated classifications
among all matches, instead of conventional methods which only rely on local
geometrical relations. Various image sets have been considered in this paper
for the evaluation of the proposed approach, including aerial images with
simulated affine deformations, remote sensing optical and synthetic aperture
radar images taken at different situations (multispectral, multisensor, and
multitemporal), and map images with inconsistent annotations. Experimental
results demonstrate the superior performance of the proposed method over the
existing approaches for the whole data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00833">
<div class="article-summary-box-inner">
<span><p>Pixel synthesis is a promising research paradigm for image generation, which
can well exploit pixel-wise prior knowledge for generation. However, existing
methods still suffer from excessive memory footprint and computation overhead.
In this paper, we propose a progressive pixel synthesis network towards
efficient image generation, coined as PixelFolder. Specifically, PixelFolder
formulates image generation as a progressive pixel regression problem and
synthesizes images by a multi-stage paradigm, which can greatly reduce the
overhead caused by large tensor transformations. In addition, we introduce
novel pixel folding operations to further improve model efficiency while
maintaining pixel-wise prior knowledge for end-to-end regression. With these
innovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,
reducing 90% computation and 57% parameters compared to the latest pixel
synthesis method called CIPS. To validate our approach, we conduct extensive
experiments on two benchmark datasets, namely FFHQ and LSUN Church. The
experimental results show that with much less expenditure, PixelFolder obtains
new state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77
FID and 2.45 FID on FFHQ and LSUN Church, respectively. Meanwhile, PixelFolder
is also more efficient than the SOTA methods like StyleGAN2, reducing about 74%
computation and 36% parameters, respectively. These results greatly validate
the effectiveness of the proposed PixelFolder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotated Object Detection via Scale-invariant Mahalanobis Distance in Aerial Images. (arXiv:2204.00840v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00840">
<div class="article-summary-box-inner">
<span><p>Rotated object detection in aerial images is a meaningful yet challenging
task as objects are densely arranged and have arbitrary orientations. The
eight-parameter (coordinates of box vectors) methods in rotated object
detection usually use ln-norm losses (L1 loss, L2 loss, and smooth L1 loss) as
loss functions. As ln-norm losses are mainly based on non-scale-invariant
Minkowski distance, using ln-norm losses will lead to inconsistency with the
detection metric rotational Intersection-over-Union (IoU) and training
instability. To address the problems, we use Mahalanobis distance to calculate
loss between the predicted and the target box vertices' vectors, proposing a
new loss function called Mahalanobis Distance Loss (MDL) for eight-parameter
rotated object detection. As Mahalanobis distance is scale-invariant, MDL is
more consistent with detection metric than ln-norm losses and more stable
during training. To alleviate the problem of boundary discontinuity like all
other eight-parameter methods, we further take the minimum loss value to make
MDL continuous at boundary cases. We achieve state-of-art performance on
DOTA-v1.0 with the proposed method MDL. Furthermore, with the comparative
experiment of smooth L1 loss under the same condi-tion, we find that MDL
performs better in rotated object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Neon Beam: Robust Physical-World Adversarial Attack to DNNs. (arXiv:2204.00853v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00853">
<div class="article-summary-box-inner">
<span><p>In the physical world, light affects the performance of deep neural networks.
Nowadays, many products based on deep neural network have been put into daily
life. There are few researches on the effect of light on the performance of
deep neural network models. However, the adversarial perturbations generated by
light may have extremely dangerous effects on these systems. In this work, we
propose an attack method called adversarial neon beam (AdvNB), which can
execute the physical attack by obtaining the physical parameters of adversarial
neon beams with very few queries. Experiments show that our algorithm can
achieve advanced attack effect in both digital test and physical test. In the
digital environment, 99.3% attack success rate was achieved, and in the
physical environment, 100% attack success rate was achieved. Compared with the
most advanced physical attack methods, our method can achieve better physical
perturbation concealment. In addition, by analyzing the experimental data, we
reveal some new phenomena brought about by the adversarial neon beam attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acoustic-to-articulatory Inversion based on Speech Decomposition and Auxiliary Feature. (arXiv:2204.00873v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00873">
<div class="article-summary-box-inner">
<span><p>Acoustic-to-articulatory inversion (AAI) is to obtain the movement of
articulators from speech signals. Until now, achieving a speaker-independent
AAI remains a challenge given the limited data. Besides, most current works
only use audio speech as input, causing an inevitable performance bottleneck.
To solve these problems, firstly, we pre-train a speech decomposition network
to decompose audio speech into speaker embedding and content embedding as the
new personalized speech features to adapt to the speaker-independent case.
Secondly, to further improve the AAI, we propose a novel auxiliary feature
network to estimate the lip auxiliary features from the above personalized
speech features. Experimental results on three public datasets show that,
compared with the state-of-the-art only using the audio speech feature, the
proposed method reduces the average RMSE by 0.25 and increases the average
correlation coefficient by 2.0% in the speaker-dependent case. More
importantly, the average RMSE decreases by 0.29 and the average correlation
coefficient increases by 5.0% in the speaker-independent case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moment-based Adversarial Training for Embodied Language Comprehension. (arXiv:2204.00889v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00889">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on a vision-and-language task in which a robot is
instructed to execute household tasks. Given an instruction such as "Rinse off
a mug and place it in the coffee maker," the robot is required to locate the
mug, wash it, and put it in the coffee maker. This is challenging because the
robot needs to break down the instruction sentences into subgoals and execute
them in the correct order. On the ALFRED benchmark, the performance of
state-of-the-art methods is still far lower than that of humans. This is
partially because existing methods sometimes fail to infer subgoals that are
not explicitly specified in the instruction sentences. We propose Moment-based
Adversarial Training (MAT), which uses two types of moments for perturbation
updates in adversarial training. We introduce MAT to the embedding spaces of
the instruction, subgoals, and state representations to handle their varieties.
We validated our method on the ALFRED benchmark, and the results demonstrated
that our method outperformed the baseline method for all the metrics on the
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Free Lunch to Person Re-identification: Learning from Automatically Generated Noisy Tracklets. (arXiv:2204.00891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00891">
<div class="article-summary-box-inner">
<span><p>A series of unsupervised video-based re-identification (re-ID) methods have
been proposed to solve the problem of high labor cost required to annotate
re-ID datasets. But their performance is still far lower than the supervised
counterparts. In the mean time, clean datasets without noise are used in these
methods, which is not realistic. In this paper, we propose to tackle this
problem by learning re-ID models from automatically generated person tracklets
by multiple objects tracking (MOT) algorithm. To this end, we design a
tracklet-based multi-level clustering (TMC) framework to effectively learn the
re-ID model from the noisy person tracklets. First, intra-tracklet isolation to
reduce ID switch noise within tracklets; second, alternates between using
inter-tracklet association to eliminate ID fragmentation noise and network
training using the pseudo label. Extensive experiments on MARS with various
manually generated noises show the effectiveness of the proposed framework.
Specifically, the proposed framework achieved mAP 53.4% and rank-1 63.7% on the
simulated tracklets with strongest noise, even outperforming the best existing
method on clean tracklets. Based on the results, we believe that building re-ID
models from automatically generated noisy tracklets is a reasonable approach
and will also be an important way to make re-ID models feasible in real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mix-up Self-Supervised Learning for Contrast-agnostic Applications. (arXiv:2204.00901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00901">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning has attracted significant research
attention recently. It learns effective visual representations from unlabeled
data by embedding augmented views of the same image close to each other while
pushing away embeddings of different images. Despite its great success on
ImageNet classification, COCO object detection, etc., its performance degrades
on contrast-agnostic applications, e.g., medical image classification, where
all images are visually similar to each other. This creates difficulties in
optimizing the embedding space as the distance between images is rather small.
To solve this issue, we present the first mix-up self-supervised learning
framework for contrast-agnostic applications. We address the low variance
across images based on cross-domain mix-up and build the pretext task based on
two synergistic objectives: image reconstruction and transparency prediction.
Experimental results on two benchmark datasets validate the effectiveness of
our method, where an improvement of 2.5% ~ 7.4% in top-1 accuracy was obtained
compared to existing self-supervised learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Algebraic Fitting for Multiple Circle Primitives Extraction from Raw Point Clouds. (arXiv:2204.00920v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00920">
<div class="article-summary-box-inner">
<span><p>The shape of circle is one of fundamental geometric primitives of man-made
engineering objects. Thus, extraction of circles from scanned point clouds is a
quite important task in 3D geometry data processing. However, existing circle
extraction methods either are sensitive to the quality of raw point clouds when
classifying circle-boundary points, or require well-designed fitting functions
when regressing circle parameters. To relieve the challenges, we propose an
end-to-end Point Cloud Circle Algebraic Fitting Network (Circle-Net) based on a
synergy of deep circle-boundary point feature learning and weighted algebraic
fitting. First, we design a circle-boundary learning module, which considers
local and global neighboring contexts of each point, to detect all potential
circle-boundary points. Second, we develop a deep feature based circle
parameter learning module for weighted algebraic fitting, without designing any
weight metric, to avoid the influence of outliers during fitting. Unlike most
of the cutting-edge circle extraction wisdoms, the proposed
classification-and-fitting modules are originally co-trained with a
comprehensive loss to enhance the quality of extracted circles.Comparisons on
the established dataset and real-scanned point clouds exhibit clear
improvements of Circle-Net over SOTAs in terms of both noise-robustness and
extraction accuracy. We will release our code, model, and data for both
training and evaluation on GitHub upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word separation in continuous sign language using isolated signs and post-processing. (arXiv:2204.00923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00923">
<div class="article-summary-box-inner">
<span><p>Continuous Sign Language Recognition (CSLR) is a long challenging task in
Computer Vision due to the difficulties in detecting the explicit boundaries
between the words in a sign sentence. To deal with this challenge, we propose a
two-stage model. In the first stage, the predictor model, which includes a
combination of CNN, SVD, and LSTM, is trained with the isolated signs. In the
second stage, we apply a post-processing algorithm to the Softmax outputs
obtained from the first part of the model in order to separate the isolated
signs in the continuous signs. Due to the lack of a large dataset, including
both the sign sequences and the corresponding isolated signs, two public
datasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and
ASLVID, are used for evaluation. Results of the continuous sign videos confirm
the efficiency of the proposed model to deal with isolated sign boundaries
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. (arXiv:2204.00928v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00928">
<div class="article-summary-box-inner">
<span><p>Despite the rapid development of Neural Radiance Field (NeRF), the necessity
of dense covers largely prohibits its wider applications. While several recent
works have attempted to address this issue, they either operate with sparse
views (yet still, a few of them) or on simple objects/scenes. In this work, we
consider a more ambitious task: training neural radiance field, over
realistically complex visual scenes, by "looking only once", i.e., using only a
single view. To attain this goal, we present a Single View NeRF (SinNeRF)
framework consisting of thoughtfully designed semantic and geometry
regularizations. Specifically, SinNeRF constructs a semi-supervised learning
process, where we introduce and propagate geometry pseudo labels and semantic
pseudo labels to guide the progressive training process. Extensive experiments
are conducted on complex scene benchmarks, including NeRF synthetic dataset,
Local Light Field Fusion dataset, and DTU dataset. We show that even without
pre-training on multi-view datasets, SinNeRF can yield photo-realistic
novel-view synthesis results. Under the single image setting, SinNeRF
significantly outperforms the current state-of-the-art NeRF baselines in all
cases. Project page: https://vita-group.github.io/SinNeRF/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A-ACT: Action Anticipation through Cycle Transformations. (arXiv:2204.00942v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00942">
<div class="article-summary-box-inner">
<span><p>While action anticipation has garnered a lot of research interest recently,
most of the works focus on anticipating future action directly through observed
visual cues only. In this work, we take a step back to analyze how the human
capability to anticipate the future can be transferred to machine learning
algorithms. To incorporate this ability in intelligent systems a question worth
pondering upon is how exactly do we anticipate? Is it by anticipating future
actions from past experiences? Or is it by simulating possible scenarios based
on cues from the present? A recent study on human psychology explains that, in
anticipating an occurrence, the human brain counts on both systems. In this
work, we study the impact of each system for the task of action anticipation
and introduce a paradigm to integrate them in a learning framework. We believe
that intelligent systems designed by leveraging the psychological anticipation
models will do a more nuanced job at the task of human action prediction.
Furthermore, we introduce cyclic transformation in the temporal dimension in
feature and semantic label space to instill the human ability of reasoning of
past actions based on the predicted future. Experiments on Epic-Kitchen,
Breakfast, and 50Salads dataset demonstrate that the action anticipation model
learned using a combination of the two systems along with the cycle
transformation performs favorably against various state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TripleNet: A Low Computing Power Platform of Low-Parameter Network. (arXiv:2204.00943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00943">
<div class="article-summary-box-inner">
<span><p>With the excellent performance of deep learning technology in the field of
computer vision, convolutional neural network (CNN) architecture has become the
main backbone of computer vision task technology. With the widespread use of
mobile devices, neural network models based on platforms with low computing
power are gradually being paid attention. This paper proposes a lightweight
convolutional neural network model, TripleNet, an improved convolutional neural
network based on HarDNet and ThreshNet, inheriting the advantages of small
memory usage and low power consumption of the mentioned two models. TripleNet
uses three different convolutional layers combined into a new model
architecture, which has less number of parameters than that of HarDNet and
ThreshNet. CIFAR-10 and SVHN datasets were used for image classification by
employing HarDNet, ThreshNet, and our proposed TripleNet for verification.
Experimental results show that, compared with HarDNet, TripleNet's parameters
are reduced by 66% and its accuracy rate is increased by 18%; compared with
ThreshNet, TripleNet's parameters are reduced by 37% and its accuracy rate is
increased by 5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Minimal Path Method with Embedded CNN. (arXiv:2204.00944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00944">
<div class="article-summary-box-inner">
<span><p>We propose Path-CNN, a method for the segmentation of centerlines of tubular
structures by embedding convolutional neural networks (CNNs) into the
progressive minimal path method. Minimal path methods are widely used for
topology-aware centerline segmentation, but usually these methods rely on weak,
hand-tuned image features. In contrast, CNNs use strong image features which
are learned automatically from images. But CNNs usually do not take the
topology of the results into account, and often require a large amount of
annotations for training. We integrate CNNs into the minimal path method, so
that both techniques benefit from each other: CNNs employ learned image
features to improve the determination of minimal paths, while the minimal path
method ensures the correct topology of the segmented centerlines, provides
strong geometric priors to increase the performance of CNNs, and reduces the
amount of annotations for the training of CNNs significantly. Our method has
lower hardware requirements than many recent methods. Qualitative and
quantitative comparison with other methods shows that Path-CNN achieves better
performance, especially when dealing with tubular structures with complex
shapes in challenging environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Feature Sets for Few-Shot Image Classification. (arXiv:2204.00949v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00949">
<div class="article-summary-box-inner">
<span><p>In image classification, it is common practice to train deep networks to
extract a single feature vector per input image. Few-shot classification
methods also mostly follow this trend. In this work, we depart from this
established direction and instead propose to extract sets of feature vectors
for each image. We argue that a set-based representation intrinsically builds a
richer representation of images from the base classes, which can subsequently
better transfer to the few-shot classes. To do so, we propose to adapt existing
feature extractors to instead produce sets of feature vectors from images. Our
approach, dubbed SetFeat, embeds shallow self-attention mechanisms inside
existing encoder architectures. The attention modules are lightweight, and as
such our method results in encoders that have approximately the same number of
parameters as their original versions. During training and inference, a
set-to-set matching metric is used to perform image classification. The
effectiveness of our proposed architecture and metrics is demonstrated via
thorough experiments on standard few-shot datasets -- namely miniImageNet,
tieredImageNet, and CUB -- in both the 1- and 5-shot scenarios. In all cases
but one, our method outperforms the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning. (arXiv:2204.00951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00951">
<div class="article-summary-box-inner">
<span><p>In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi
country benchmark dataset, tailored for agricultural monitoring applications
with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer
declarations collected via the Land Parcel Identification System (LPIS) for
harmonizing country wide labels. These declarations have only recently been
made available as open data, allowing for the first time the labeling of
satellite imagery from ground truth data. We proceed to propose and standardise
a new crop type taxonomy across Europe that address Common Agriculture Policy
(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative
Crop Classification scheme. Sen4AgriNet is the only multi-country, multi-year
dataset that includes all spectral information. It is constructed to cover the
period 2016-2020 for Catalonia and France, while it can be extended to include
additional countries. Currently, it contains 42.5 million parcels, which makes
it significantly larger than other available archives. We extract two
sub-datasets to highlight its value for diverse Deep Learning applications; the
Object Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD
capitalizes zonal statistics of each parcel, thus creating a powerful
label-to-features instance for classification algorithms. On the other hand,
PAD structure generalizes the classification problem to parcel extraction and
semantic segmentation and labeling. The PAD and OAD are examined under three
different scenarios to showcase and model the effects of spatial and temporal
variability across different years and different countries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaFace: Quality Adaptive Margin for Face Recognition. (arXiv:2204.00964v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00964">
<div class="article-summary-box-inner">
<span><p>Recognition in low quality face datasets is challenging because facial
attributes are obscured and degraded. Advances in margin-based loss functions
have resulted in enhanced discriminability of faces in the embedding space.
Further, previous studies have studied the effect of adaptive losses to assign
more importance to misclassified (hard) examples. In this work, we introduce
another aspect of adaptiveness in the loss function, namely the image quality.
We argue that the strategy to emphasize misclassified samples should be
adjusted according to their image quality. Specifically, the relative
importance of easy or hard samples should be based on the sample's image
quality. We propose a new loss function that emphasizes samples of different
difficulties based on their image quality. Our method achieves this in the form
of an adaptive margin function by approximating the image quality with feature
norms. Extensive experiments show that our method, AdaFace, improves the face
recognition performance over the state-of-the-art (SoTA) on four datasets
(IJB-B, IJB-C, IJB-S and TinyFace). Code and models are released in
https://github.com/mk-minchul/AdaFace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DST: Dynamic Substitute Training for Data-free Black-box Attack. (arXiv:2204.00972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00972">
<div class="article-summary-box-inner">
<span><p>With the wide applications of deep neural network models in various computer
vision tasks, more and more works study the model vulnerability to adversarial
examples. For data-free black box attack scenario, existing methods are
inspired by the knowledge distillation, and thus usually train a substitute
model to learn knowledge from the target model using generated data as input.
However, the substitute model always has a static network structure, which
limits the attack ability for various target models and tasks. In this paper,
we propose a novel dynamic substitute training attack method to encourage
substitute model to learn better and faster from the target model.
Specifically, a dynamic substitute structure learning strategy is proposed to
adaptively generate optimal substitute model structure via a dynamic gate
according to different target models and tasks. Moreover, we introduce a
task-driven graph-based structure information learning constrain to improve the
quality of generated training data, and facilitate the substitute model
learning structural relationships from the target model multiple outputs.
Extensive experiments have been conducted to verify the efficacy of the
proposed attack method, which can achieve better performance compared with the
state-of-the-art competitors on several datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kernel Extreme Learning Machine Optimized by the Sparrow Search Algorithm for Hyperspectral Image Classification. (arXiv:2204.00973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00973">
<div class="article-summary-box-inner">
<span><p>To improve the classification performance and generalization ability of the
hyperspectral image classification algorithm, this paper uses Multi-Scale Total
Variation (MSTV) to extract the spectral features, local binary pattern (LBP)
to extract spatial features, and feature superposition to obtain the fused
features of hyperspectral images. A new swarm intelligence optimization method
with high convergence and strong global search capability, the Sparrow Search
Algorithm (SSA), is used to optimize the kernel parameters and regularization
coefficients of the Kernel Extreme Learning Machine (KELM). In summary, a
multiscale fusion feature hyperspectral image classification method (MLS-KELM)
is proposed in this paper. The Indian Pines, Pavia University and Houston 2013
datasets were selected to validate the classification performance of MLS-KELM,
and the method was applied to ZY1-02D hyperspectral data. The experimental
results show that MLS-KELM has better classification performance and
generalization ability compared with other popular classification methods, and
MLS-KELM shows its strong robustness in the small sample case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Global Shutter: Learn to Restore Video from a Rolling Shutter Camera with Global Reset Feature. (arXiv:2204.00974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00974">
<div class="article-summary-box-inner">
<span><p>Most computer vision systems assume distortion-free images as inputs. The
widely used rolling-shutter (RS) image sensors, however, suffer from geometric
distortion when the camera and object undergo motion during capture. Extensive
researches have been conducted on correcting RS distortions. However, most of
the existing work relies heavily on the prior assumptions of scenes or motions.
Besides, the motion estimation steps are either oversimplified or
computationally inefficient due to the heavy flow warping, limiting their
applicability. In this paper, we investigate using rolling shutter with a
global reset feature (RSGR) to restore clean global shutter (GS) videos. This
feature enables us to turn the rectification problem into a deblur-like one,
getting rid of inaccurate and costly explicit motion estimation. First, we
build an optic system that captures paired RSGR/GS videos. Second, we develop a
novel algorithm incorporating spatial and temporal designs to correct the
spatial-varying RSGR distortion. Third, we demonstrate that existing
image-to-image translation algorithms can recover clean GS videos from
distorted RSGR inputs, yet our algorithm achieves the best performance with the
specific designs. Our rendered results are not only visually appealing but also
beneficial to downstream tasks. Compared to the state-of-the-art RS solution,
our RSGR solution is superior in both effectiveness and efficiency. Considering
it is easy to realize without changing the hardware, we believe our RSGR
solution can potentially replace the RS solution in taking distortion-free
videos with low noise and low budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question-Driven Graph Fusion Network For Visual Question Answering. (arXiv:2204.00975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00975">
<div class="article-summary-box-inner">
<span><p>Existing Visual Question Answering (VQA) models have explored various visual
relationships between objects in the image to answer complex questions, which
inevitably introduces irrelevant information brought by inaccurate object
detection and text grounding. To address the problem, we propose a
Question-Driven Graph Fusion Network (QD-GFN). It first models semantic,
spatial, and implicit visual relations in images by three graph attention
networks, then question information is utilized to guide the aggregation
process of the three graphs, further, our QD-GFN adopts an object filtering
mechanism to remove question-irrelevant objects contained in the image.
Experiment results demonstrate that our QD-GFN outperforms the prior
state-of-the-art on both VQA 2.0 and VQA-CP v2 datasets. Further analysis shows
that both the novel graph aggregation method and object filtering mechanism
play a significant role in improving the performance of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation. (arXiv:2204.00987v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00987">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation is a fundamental task in computer vision and has
drawn increasing attention. Recently, some methods reformulate it as a
classification-regression task to boost the model performance, where continuous
depth is estimated via a linear combination of predicted probability
distributions and discrete bins. In this paper, we present a novel framework
called BinsFormer, tailored for the classification-regression-based depth
estimation. It mainly focuses on two crucial components in the specific task:
1) proper generation of adaptive bins and 2) sufficient interaction between
probability distribution and bins predictions. To specify, we employ the
Transformer decoder to generate bins, novelly viewing it as a direct set-to-set
prediction problem. We further integrate a multi-scale decoder structure to
achieve a comprehensive understanding of spatial geometry information and
estimate depth maps in a coarse-to-fine manner. Moreover, an extra scene
understanding query is proposed to improve the estimation accuracy, which turns
out that models can implicitly learn useful information from an auxiliary
environment classification task. Extensive experiments on the KITTI, NYU, and
SUN RGB-D datasets demonstrate that BinsFormer surpasses state-of-the-art
monocular depth estimation methods with prominent margins. Code and pretrained
models will be made publicly available at
\url{https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POS-BERT: Point Cloud One-Stage BERT Pre-Training. (arXiv:2204.00989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00989">
<div class="article-summary-box-inner">
<span><p>Recently, the pre-training paradigm combining Transformer and masked language
modeling has achieved tremendous success in NLP, images, and point clouds, such
as BERT. However, directly extending BERT from NLP to point clouds requires
training a fixed discrete Variational AutoEncoder (dVAE) before pre-training,
which results in a complex two-stage method called Point-BERT. Inspired by BERT
and MoCo, we propose POS-BERT, a one-stage BERT pre-training method for point
clouds. Specifically, we use the mask patch modeling (MPM) task to perform
point cloud pre-training, which aims to recover masked patches information
under the supervision of the corresponding tokenizer output. Unlike Point-BERT,
its tokenizer is extra-trained and frozen. We propose to use the dynamically
updated momentum encoder as the tokenizer, which is updated and outputs the
dynamic supervision signal along with the training process. Further, in order
to learn high-level semantic representation, we combine contrastive learning to
maximize the class token consistency between different transformation point
clouds. Extensive experiments have demonstrated that POS-BERT can extract
high-quality pre-training features and promote downstream tasks to improve
performance. Using the pre-training model without any fine-tuning to extract
features and train linear SVM on ModelNet40, POS-BERT achieves the
state-of-the-art classification accuracy, which exceeds Point-BERT by 3.5\%. In
addition, our approach has significantly improved many downstream tasks, such
as fine-tuned classification, few-shot classification, part segmentation. The
code and trained-models will be available at:
\url{https://github.com/fukexue/POS-BERT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00993">
<div class="article-summary-box-inner">
<span><p>The transformer models have shown promising effectiveness in dealing with
various vision tasks. However, compared with training Convolutional Neural
Network (CNN) models, training Vision Transformer (ViT) models is more
difficult and relies on the large-scale training set. To explain this
observation we make a hypothesis that ViT models are less effective in
capturing the high-frequency components of images than CNN models, and verify
it by a frequency analysis. Inspired by this finding, we first investigate the
effects of existing techniques for improving ViT models from a new frequency
perspective, and find that the success of some techniques (e.g., RandAugment)
can be attributed to the better usage of the high-frequency components. Then,
to compensate for this insufficient ability of ViT models, we propose HAT,
which directly augments high-frequency components of images via adversarial
training. We show that HAT can consistently boost the performance of various
ViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance
the advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the
superiority can also be maintained on out-of-distribution data and transferred
to downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-aware Attention for Image Inpainting. (arXiv:2204.01004v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01004">
<div class="article-summary-box-inner">
<span><p>Recent attention-based image inpainting methods have made inspiring progress
by modeling long-range dependencies within a single image. However, they tend
to generate blurry contents since the correlation between each pixel pairs is
always misled by ill-predicted features in holes. To handle this problem, we
propose a novel region-aware attention (RA) module. By avoiding the directly
calculating corralation between each pixel pair in a single samples and
considering the correlation between different samples, the misleading of
invalid information in holes can be avoided. Meanwhile, a learnable region
dictionary (LRD) is introduced to store important information in the entire
dataset, which not only simplifies correlation modeling, but also avoids
information redundancy. By applying RA in our architecture, our methodscan
generate semantically plausible results with realistic details. Extensive
experiments on CelebA, Places2 and Paris StreetView datasets validate the
superiority of our method compared with existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gastrointestinal Polyps and Tumors Detection Based on Multi-scale Feature-fusion with WCE Sequences. (arXiv:2204.01012v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01012">
<div class="article-summary-box-inner">
<span><p>Wireless Capsule Endoscopy(WCE) has been widely used for the screening of
gastrointestinal(GI) diseases, especially the small intestine, due to its
advantages of non-invasive and painless imaging of the entire digestive
tract.However, the huge amount of image data captured by WCE makes manual
reading a process that requires a huge amount of tasks and can easily lead to
missed detection and false detection of lesions.Therefore, In this paper, we
propose a \textbf{T}wo-stage \textbf{M}ulti-scale \textbf{F}eature-fusion
learning network(\textbf{TMFNet}) to automatically detect small intestinal
polyps and tumors in WCE image sequences. Specifically, TMFNet consists of
lesion detection network and lesion identification network. Among them, the
former improves the feature extraction module and detection module based on the
traditional Faster R-CNN network, and readjusts the parameters of the anchor in
the region proposal network(RPN) module;the latter combines residual structure
and feature pyramid structure are used to build a small intestinal lesion
recognition network based on feature fusion, for reducing the false positive
rate of the former and improve the overall accuracy.We used 22,335 WCE images
in the experiment, with a total of 123,092 lesion regions used to train the
detection framework of this paper. In the experiment, the detection framework
is trained and tested on the real WCE image dataset provided by the hospital
gastroenterology department. The sensitivity, false positive and accuracy of
the final model on the RPM are 98.81$\%$, 7.43$\%$ and 92.57$\%$,
respectively.Meanwhile,the corresponding results on the lesion images were
98.75$\%$, 5.62$\%$ and 94.39$\%$. The algorithm model proposed in this paper
is obviously superior to other detection algorithms in detection effect and
performance
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting. (arXiv:2204.01018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01018">
<div class="article-summary-box-inner">
<span><p>Counting repetitive actions are widely seen in human activities such as
physical exercise. Existing methods focus on performing repetitive action
counting in short videos, which is tough for dealing with longer videos in more
realistic scenarios. In the data-driven era, the degradation of such
generalization capability is mainly attributed to the lack of long video
datasets. To complement this margin, we introduce a new large-scale repetitive
action counting dataset covering a wide variety of video lengths, along with
more realistic situations where action interruption or action inconsistencies
occur in the video. Besides, we also provide a fine-grained annotation of the
action cycles instead of just counting annotation along with a numerical value.
Such a dataset contains 1,451 videos with about 20,000 annotations, which is
more challenging. For repetitive action counting towards more realistic
scenarios, we further propose encoding multi-scale temporal correlation with
transformers that can take into account both performance and efficiency.
Furthermore, with the help of fine-grained annotation of action cycles, we
propose a density map regression-based method to predict the action period,
which yields better performance with sufficient interpretability. Our proposed
method outperforms state-of-the-art methods on all datasets and also achieves
better performance on the unseen dataset without fine-tuning. The dataset and
code are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes. (arXiv:2204.01026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01026">
<div class="article-summary-box-inner">
<span><p>Accurately detecting and tracking pedestrians in 3D space is challenging due
to large variations in rotations, poses and scales. The situation becomes even
worse for dense crowds with severe occlusions. However, existing benchmarks
either only provide 2D annotations, or have limited 3D annotations with
low-density pedestrian distribution, making it difficult to build a reliable
pedestrian perception system especially in crowded scenes. To better evaluate
pedestrian perception algorithms in crowded scenarios, we introduce a
large-scale multimodal dataset,STCrowd. Specifically, in STCrowd, there are a
total of 219 K pedestrian instances and 20 persons per frame on average, with
various levels of occlusion. We provide synchronized LiDAR point clouds and
camera images as well as their corresponding 3D labels and joint IDs. STCrowd
can be used for various tasks, including LiDAR-only, image-only, and
sensor-fusion based pedestrian detection and tracking. We provide baselines for
most of the tasks. In addition, considering the property of sparse global
distribution and density-varying local distribution of pedestrians, we further
propose a novel method, Density-aware Hierarchical heatmap Aggregation (DHA),
to enhance pedestrian perception in crowded scenes. Extensive experiments show
that our new method achieves state-of-the-art performance for pedestrian
detection on various datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distortion-Aware Self-Supervised 360{\deg} Depth Estimation from A Single Equirectangular Projection Image. (arXiv:2204.01027v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01027">
<div class="article-summary-box-inner">
<span><p>360{\deg} images are widely available over the last few years. This paper
proposes a new technique for single 360{\deg} image depth prediction under open
environments. Depth prediction from a 360{\deg} single image is not easy for
two reasons. One is the limitation of supervision datasets - the currently
available dataset is limited to indoor scenes. The other is the problems caused
by Equirectangular Projection Format (ERP), commonly used for 360{\deg} images,
that are coordinate and distortion. There is only one method existing that uses
cube map projection to produce six perspective images and apply self-supervised
learning using motion pictures for perspective depth prediction to deal with
these problems. Different from the existing method, we directly use the ERP
format. We propose a framework of direct use of ERP with coordinate conversion
of correspondences and distortion-aware upsampling module to deal with the ERP
related problems and extend a self-supervised learning method for open
environments. For the experiments, we firstly built a dataset for the
evaluation, and quantitatively evaluate the depth prediction in outdoor scenes.
We show that it outperforms the state-of-the-art technique
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Based Global Appearance Flow for Virtual Try-On. (arXiv:2204.01046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01046">
<div class="article-summary-box-inner">
<span><p>Image-based virtual try-on aims to fit an in-shop garment into a clothed
person image. To achieve this, a key step is garment warping which spatially
aligns the target garment with the corresponding body parts in the person
image. Prior methods typically adopt a local appearance flow estimation model.
They are thus intrinsically susceptible to difficult body poses/occlusions and
large mis-alignments between person and garment images (see
Fig.~\ref{fig:fig1}). To overcome this limitation, a novel global appearance
flow estimation model is proposed in this work. For the first time, a StyleGAN
based architecture is adopted for appearance flow estimation. This enables us
to take advantage of a global style vector to encode a whole-image context to
cope with the aforementioned challenges. To guide the StyleGAN flow generator
to pay more attention to local garment deformation, a flow refinement module is
introduced to add local context. Experiment results on a popular virtual try-on
benchmark show that our method achieves new state-of-the-art performance. It is
particularly effective in a `in-the-wild' application scenario where the
reference image is full-body resulting in a large mis-alignment with the
garment image (Fig.~\ref{fig:fig1} Top). Code is available at:
\url{https://github.com/SenHe/Flow-Style-VTON}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Rain or Shine: Understanding and Overcoming Dataset Bias for Improving Robustness Against Weather Corruptions for Autonomous Vehicles. (arXiv:2204.01062v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01062">
<div class="article-summary-box-inner">
<span><p>Several popular computer vision (CV) datasets, specifically employed for
Object Detection (OD) in autonomous driving tasks exhibit biases due to a range
of factors including weather and lighting conditions. These biases may impair a
model's generalizability, rendering it ineffective for OD in novel and unseen
datasets. Especially, in autonomous driving, it may prove extremely high risk
and unsafe for the vehicle and its surroundings. This work focuses on
understanding these datasets better by identifying such "good-weather" bias.
Methods to mitigate such bias which allows the OD models to perform better and
improve the robustness are also demonstrated. A simple yet effective OD
framework for studying bias mitigation is proposed. Using this framework, the
performance on popular datasets is analyzed and a significant difference in
model performance is observed. Additionally, a knowledge transfer technique and
a synthetic image corruption technique are proposed to mitigate the identified
bias. Finally, using the DAWN dataset, the findings are validated on the OD
task, demonstrating the effectiveness of our techniques in mitigating
real-world "good-weather" bias. The experiments show that the proposed
techniques outperform baseline methods by averaged fourfold improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ES6D: A Computation Efficient and Symmetry-Aware 6D Pose Regression Framework. (arXiv:2204.01080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01080">
<div class="article-summary-box-inner">
<span><p>In this paper, a computation efficient regression framework is presented for
estimating the 6D pose of rigid objects from a single RGB-D image, which is
applicable to handling symmetric objects. This framework is designed in a
simple architecture that efficiently extracts point-wise features from RGB-D
data using a fully convolutional network, called XYZNet, and directly regresses
the 6D pose without any post refinement. In the case of symmetric object, one
object has multiple ground-truth poses, and this one-to-many relationship may
lead to estimation ambiguity. In order to solve this ambiguity problem, we
design a symmetry-invariant pose distance metric, called average (maximum)
grouped primitives distance or A(M)GPD. The proposed A(M)GPD loss can make the
regression network converge to the correct state, i.e., all minima in the
A(M)GPD loss surface are mapped to the correct poses. Extensive experiments on
YCB-Video and T-LESS datasets demonstrate the proposed framework's
substantially superior performance in top accuracy and low computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faces: AI Blitz XIII Solutions. (arXiv:2204.01081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01081">
<div class="article-summary-box-inner">
<span><p>AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of
five problems: Sentiment Classification, Age Prediction, Mask Prediction, Face
Recognition, and Face De-Blurring. Our team GLaDOS took second place. Here we
present our solutions and results. Code implementation:
https://github.com/ndrwmlnk/ai-blitz-xiii
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarially robust segmentation models learn perceptually-aligned gradients. (arXiv:2204.01099v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01099">
<div class="article-summary-box-inner">
<span><p>The effects of adversarial training on semantic segmentation networks has not
been thoroughly explored. While previous work has shown that
adversarially-trained image classifiers can be used to perform image synthesis,
we have yet to understand how best to leverage an adversarially-trained
segmentation network to do the same. Using a simple optimizer, we demonstrate
that adversarially-trained semantic segmentation networks can be used to
perform image inpainting and generation. Our experiments demonstrate that
adversarially-trained segmentation networks are more robust and indeed exhibit
perceptually-aligned gradients which help in producing plausible image
inpaintings. We seek to place additional weight behind the hypothesis that
adversarially robust models exhibit gradients that are more
perceptually-aligned with human vision. Through image synthesis, we argue that
perceptually-aligned gradients promote a better understanding of a neural
network's learned representations and aid in making neural networks more
interpretable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adjusting for Bias with Procedural Data. (arXiv:2204.01108v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01108">
<div class="article-summary-box-inner">
<span><p>3D softwares are now capable of producing highly realistic images that look
nearly indistinguishable from the real images. This raises the question: can
real datasets be enhanced with 3D rendered data? We investigate this question.
In this paper we demonstrate the use of 3D rendered data, procedural, data for
the adjustment of bias in image datasets. We perform error analysis of images
of animals which shows that the misclassification of some animal breeds is
largely a data issue. We then create procedural images of the poorly classified
breeds and that model further trained on procedural data can better classify
poorly performing breeds on real data. We believe that this approach can be
used for the enhancement of visual data for any underrepresented group,
including rare diseases, or any data bias potentially improving the accuracy
and fairness of models. We find that the resulting representations rival or
even out-perform those learned directly from real data, but that good
performance requires care in the 3D rendered procedural data generation. 3D
image dataset can be viewed as a compressed and organized copy of a real
dataset, and we envision a future where more and more procedural data
proliferate while datasets become increasingly unwieldy, missing, or private.
This paper suggests several techniques for dealing with visual representation
learning in such a future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion. (arXiv:2204.01139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01139">
<div class="article-summary-box-inner">
<span><p>Dense 3D reconstruction from a stream of depth images is the key to many
mixed reality and robotic applications. Although methods based on Truncated
Signed Distance Function (TSDF) Fusion have advanced the field over the years,
the TSDF volume representation is confronted with striking a balance between
the robustness to noisy measurements and maintaining the level of detail. We
present Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent
advances in neural implicit representations and neural rendering for dense 3D
reconstruction. In order to incrementally integrate new depth maps into a
global neural implicit representation, we propose a novel bi-level fusion
strategy that considers both efficiency and reconstruction quality by design.
We evaluate the proposed method on multiple datasets quantitatively and
qualitatively, demonstrating a significant improvement over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indoor Navigation Assistance for Visually Impaired People via Dynamic SLAM and Panoptic Segmentation with an RGB-D Sensor. (arXiv:2204.01154v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01154">
<div class="article-summary-box-inner">
<span><p>Exploring an unfamiliar indoor environment and avoiding obstacles is
challenging for visually impaired people. Currently, several approaches achieve
the avoidance of static obstacles based on the mapping of indoor scenes. To
solve the issue of distinguishing dynamic obstacles, we propose an assistive
system with an RGB-D sensor to detect dynamic information of a scene. Once the
system captures an image, panoptic segmentation is performed to obtain the
prior dynamic object information. With sparse feature points extracted from
images and the depth information, poses of the user can be estimated. After the
ego-motion estimation, the dynamic object can be identified and tracked. Then,
poses and speed of tracked dynamic objects can be estimated, which are passed
to the users through acoustic feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons. (arXiv:2204.01159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01159">
<div class="article-summary-box-inner">
<span><p>We introduce an unsupervised technique for encoding point clouds into a
canonical shape representation, by disentangling shape and pose. Our encoder is
stable and consistent, meaning that the shape encoding is purely
pose-invariant, while the extracted rotation and translation are able to
semantically align different input shapes of the same class to a common
canonical pose. Specifically, we design an auto-encoder based on Vector Neuron
Networks, a rotation-equivariant neural network, whose layers we extend to
provide translation-equivariance in addition to rotation-equivariance only. The
resulting encoder produces pose-invariant shape encoding by construction,
enabling our approach to focus on learning a consistent canonical pose for a
class of objects. Quantitative and qualitative experiments validate the
superior stability and consistency of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Temporal Relations on Radar Perception for Autonomous Driving. (arXiv:2204.01184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01184">
<div class="article-summary-box-inner">
<span><p>We consider the object recognition problem in autonomous driving using
automotive radar sensors. Comparing to Lidar sensors, radar is cost-effective
and robust in all-weather conditions for perception in autonomous driving.
However, radar signals suffer from low angular resolution and precision in
recognizing surrounding objects. To enhance the capacity of automotive radar,
in this work, we exploit the temporal information from successive ego-centric
bird-eye-view radar image frames for radar object recognition. We leverage the
consistency of an object's existence and attributes (size, orientation, etc.),
and propose a temporal relational layer to explicitly model the relations
between objects within successive radar images. In both object detection and
multiple object tracking, we show the superiority of our method compared to
several baseline approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting a kNN-based Image Classification System with High-capacity Storage. (arXiv:2204.01186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01186">
<div class="article-summary-box-inner">
<span><p>In existing image classification systems that use deep neural networks, the
knowledge needed for image classification is implicitly stored in model
parameters. If users want to update this knowledge, then they need to fine-tune
the model parameters. Moreover, users cannot verify the validity of inference
results or evaluate the contribution of knowledge to the results. In this
paper, we investigate a system that stores knowledge for image classification,
such as image feature maps, labels, and original images, not in model
parameters but in external high-capacity storage. Our system refers to the
storage like a database when classifying input images. To increase knowledge,
our system updates the database instead of fine-tuning model parameters, which
avoids catastrophic forgetting in incremental learning scenarios. We revisit a
kNN (k-Nearest Neighbor) classifier and employ it in our system. By analyzing
the neighborhood samples referred by the kNN algorithm, we can interpret how
knowledge learned in the past is used for inference results. Our system
achieves 79.8% top-1 accuracy on the ImageNet dataset without fine-tuning model
parameters after pretraining, and 90.8% accuracy on the Split CIFAR-100 dataset
in the task incremental learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Sliced Wasserstein on Images: From Vectorization to Convolution. (arXiv:2204.01188v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01188">
<div class="article-summary-box-inner">
<span><p>The conventional sliced Wasserstein is defined between two probability
measures that have realizations as vectors. When comparing two probability
measures over images, practitioners first need to vectorize images and then
project them to one-dimensional space by using matrix multiplication between
the sample matrix and the projection matrix. After that, the sliced Wasserstein
is evaluated by averaging the two corresponding one-dimensional projected
probability measures. However, this approach has two limitations. The first
limitation is that the spatial structure of images is not captured efficiently
by the vectorization step; therefore, the later slicing process becomes harder
to gather the discrepancy information. The second limitation is memory
inefficiency since each slicing direction is a vector that has the same
dimension as the images. To address these limitations, we propose novel slicing
methods for sliced Wasserstein between probability measures over images that
are based on the convolution operators. We derive convolution sliced
Wasserstein (CSW) and its variants via incorporating stride, dilation, and
non-linear activation function into the convolution operators. We investigate
the metricity of CSW as well as its sample complexity, its computational
complexity, and its connection to conventional sliced Wasserstein distances.
Finally, we demonstrate the favorable performance of CSW over the conventional
sliced Wasserstein in comparing probability measures over images and in
training deep generative modeling on images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Change Detection Based on Image Reconstruction Loss. (arXiv:2204.01200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01200">
<div class="article-summary-box-inner">
<span><p>To train the change detector, bi-temporal images taken at different times in
the same area are used. However, collecting labeled bi-temporal images is
expensive and time consuming. To solve this problem, various unsupervised
change detection methods have been proposed, but they still require unlabeled
bi-temporal images. In this paper, we propose unsupervised change detection
based on image reconstruction loss using only unlabeled single temporal single
image. The image reconstruction model is trained to reconstruct the original
source image by receiving the source image and the photometrically transformed
source image as a pair. During inference, the model receives bi-temporal images
as the input, and tries to reconstruct one of the inputs. The changed region
between bi-temporal images shows high reconstruction loss. Our change detector
showed significant performance in various change detection benchmark datasets
even though only a single temporal single source image was used. The code and
trained models will be publicly available for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumors through Image Subtraction. (arXiv:2204.01201v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01201">
<div class="article-summary-box-inner">
<span><p>The segmentation of diseases is a popular topic explored by researchers in
the field of machine learning. Brain tumors are extremely dangerous and require
the utmost precision to segment for a successful surgery. Patients with tumors
usually take 4 MRI scans, T1, T1gd, T2, and FLAIR, which are then sent to
radiologists to segment and analyze for possible future surgery. To create a
second segmentation, it would be beneficial to both radiologists and patients
in being more confident in their conclusions. We propose using a method
performed by radiologists called image segmentation and applying it to machine
learning models to prove a better segmentation. Using Mask R-CNN, its ResNet
backbone being pre-trained on the RSNA pneumonia detection challenge dataset,
we can train a model on the Brats2020 Brain Tumor dataset. Center for
Biomedical Image Computing &amp; Analytics provides MRI data on patients with and
without brain tumors and the corresponding segmentations. We can see how well
the method of image subtraction works by comparing it to models without image
subtraction through DICE coefficient (F1 score), recall, and precision on the
untouched test set. Our model performed with a DICE coefficient of 0.75 in
comparison to 0.69 without image subtraction. To further emphasize the
usefulness of image subtraction, we compare our final model to current
state-of-the-art models to segment tumors from MRI scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attribute Prototype Network for Any-Shot Learning. (arXiv:2204.01208v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01208">
<div class="article-summary-box-inner">
<span><p>Any-shot image classification allows to recognize novel classes with only a
few or even zero samples. For the task of zero-shot learning, visual attributes
have been shown to play an important role, while in the few-shot regime, the
effect of attributes is under-explored. To better transfer attribute-based
knowledge from seen to unseen classes, we argue that an image representation
with integrated attribute localization ability would be beneficial for
any-shot, i.e. zero-shot and few-shot, image classification tasks. To this end,
we propose a novel representation learning framework that jointly learns
discriminative global and local features using only class-level attributes.
While a visual-semantic embedding layer learns global features, local features
are learned through an attribute prototype network that simultaneously
regresses and decorrelates attributes from intermediate features. Furthermore,
we introduce a zoom-in module that localizes and crops the informative regions
to encourage the network to learn informative features explicitly. We show that
our locality augmented image representations achieve a new state-of-the-art on
challenging benchmarks, i.e. CUB, AWA2, and SUN. As an additional benefit, our
model points to the visual evidence of the attributes in an image, confirming
the improved attribute localization ability of our image representation. The
attribute localization is evaluated quantitatively with ground truth part
annotations, qualitatively with visualizations, and through well-designed user
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection. (arXiv:2204.01209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01209">
<div class="article-summary-box-inner">
<span><p>This paper analyses the design choices of face detection architecture that
improve efficiency between computation cost and accuracy. Specifically, we
re-examine the effectiveness of the standard convolutional block as a
lightweight backbone architecture on face detection. Unlike the current
tendency of lightweight architecture design, which heavily utilizes depthwise
separable convolution layers, we show that heavily channel-pruned standard
convolution layer can achieve better accuracy and inference speed when using a
similar parameter size. This observation is supported by the analyses
concerning the characteristics of the target data domain, face. Based on our
observation, we propose to employ ResNet with a highly reduced channel, which
surprisingly allows high efficiency compared to other mobile-friendly networks
(e.g., MobileNet-V1,-V2,-V3). From the extensive experiments, we show that the
proposed backbone can replace that of the state-of-the-art face detector with a
faster inference speed. Also, we further propose a new feature aggregation
method maximizing the detection performance. Our proposed detector EResFD
obtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA
image inference in on CPU. Code will be available at
https://github.com/clovaai/EResFD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-Teaching for Unsupervised Domain Adaptation and Expansion. (arXiv:2204.01210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01210">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) is known to trade a model's performance
on a source domain for improving its performance on a target domain. To resolve
the issue, Unsupervised Domain Expansion (UDE) has been proposed recently to
adapt the model for the target domain as UDA does, and in the meantime maintain
its performance on the source domain. For both UDA and UDE, a model tailored to
a given domain, let it be the source or the target domain, is assumed to well
handle samples from the given domain. We question the assumption by reporting
the existence of cross-domain visual ambiguity: Due to the lack of a crystally
clear boundary between the two domains, samples from one domain can be visually
close to the other domain. We exploit this finding and accordingly propose in
this paper Co-Teaching (CT) that consists of knowledge distillation based CT
(kdCT) and mixup based CT (miCT). Specifically, kdCT transfers knowledge from a
leader-teacher network and an assistant-teacher network to a student network,
so the cross-domain visual ambiguity will be better handled by the student.
Meanwhile, miCT further enhances the generalization ability of the student.
Comprehensive experiments on two image-classification benchmarks and two
driving-scene-segmentation benchmarks justify the viability of the proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animatable Neural Radiance Fields from Monocular RGB-D. (arXiv:2204.01218v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01218">
<div class="article-summary-box-inner">
<span><p>This paper aims at representing animatable photo-realistic humans under novel
views and poses. Recent work has shown significant progress with dynamic scenes
by exploring shared canonical neural radiance fields. However learning a
user-controlled model for novel poses remains a challenging task. To tackle
this problem, we introduce a novel method to integrate observations across
frames and encode the appearance at each individual frame by utilizing the
human pose that models the body shape and point clouds which cover partial part
of the human as the input. Specifically, our method simultaneously learns a
shared set of latent codes anchored to the human pose among frames, and learns
an appearance-dependent code anchored to incomplete point clouds generated by
monocular RGB-D at each frame. A human pose-based code models the shape of the
performer whereas a point cloud based code predicts details and reasons about
missing structures at the unseen poses. To further recover non-visible regions
in query frames, we utilize a temporal transformer to integrate features of
points in query frames and tracked body points from automatically-selected key
frames. Experiments on various sequences of humans in motion show that our
method significantly outperforms existing works under unseen poses and novel
views given monocular RGB-D videos as input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft Threshold Ternary Networks. (arXiv:2204.01234v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01234">
<div class="article-summary-box-inner">
<span><p>Large neural networks are difficult to deploy on mobile devices because of
intensive computation and storage. To alleviate it, we study ternarization, a
balance between efficiency and accuracy that quantizes both weights and
activations into ternary values. In previous ternarized neural networks, a hard
threshold {\Delta} is introduced to determine quantization intervals. Although
the selection of {\Delta} greatly affects the training results, previous works
estimate {\Delta} via an approximation or treat it as a hyper-parameter, which
is suboptimal. In this paper, we present the Soft Threshold Ternary Networks
(STTN), which enables the model to automatically determine quantization
intervals instead of depending on a hard threshold. Concretely, we replace the
original ternary kernel with the addition of two binary kernels at training
time, where ternary values are determined by the combination of two
corresponding binary values. At inference time, we add up the two binary
kernels to obtain a single ternary kernel. Our method dramatically outperforms
current state-of-the-arts, lowering the performance gap between full-precision
networks and extreme low bit networks. Experiments on ImageNet with ResNet-18
(Top-1 66.2%) achieves new state-of-the-art.
</p>
<p>Update: In this version, we further fine-tune the experimental
hyperparameters and training procedure. The latest STTN shows that ResNet-18
with ternary weights and ternary activations achieves up to 68.2% Top-1
accuracy on ImageNet. Code is available at: github.com/WeixiangXu/STTN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Focus-aware Positional Queries for Semantic Segmentation. (arXiv:2204.01244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01244">
<div class="article-summary-box-inner">
<span><p>Most of the latest top semantic segmentation approaches are based on vision
Transformers, particularly DETR-like frameworks, which employ a set of queries
in the Transformer decoder. Each query is composed of a content query that
preserves semantic information and a positional query that provides positional
guidance for aggregating the query-specific context. However, the positional
queries in the Transformer decoder layers are typically represented as fixed
learnable weights, which often encode dataset statistics for segments and can
be inaccurate for individual samples. Therefore, in this paper, we propose to
generate positional queries dynamically conditioned on the cross-attention
scores and the localization information of the preceding layer. By doing so,
each query is aware of its previous focus, thus providing more accurate
positional guidance and encouraging the cross-attention consistency across the
decoder layers. In addition, we also propose an efficient way to deal with
high-resolution cross-attention by dynamically determining the contextual
tokens based on the low-resolution cross-attention maps to perform local
relation aggregation. Our overall framework termed FASeg (Focus-Aware semantic
Segmentation) provides a simple yet effective solution for semantic
segmentation. Extensive experiments on ADE20K and Cityscapes show that our
FASeg achieves state-of-the-art performance, e.g., obtaining 48.3% and 49.6%
mIoU respectively for single-scale inference on ADE20K validation set with
ResNet-50 and Swin-T backbones, and barely increases the computation
consumption from Mask2former. Source code will be made publicly available at
https://github.com/zip-group/FASeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Rendering for Synthetic Aperture Radar Imagery. (arXiv:2204.01248v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01248">
<div class="article-summary-box-inner">
<span><p>There is rising interest in integrating signal and image processing pipelines
into deep learning training to incorporate more domain knowledge. This can lead
to deep neural networks that are trained more robustly and with limited data,
as well as the capability to solve ill-posed inverse problems. In particular,
there is rising interest in differentiable rendering, which allows explicitly
modeling geometric priors and constraints in the optimization pipeline using
first-order methods such as backpropagation. Existing efforts in differentiable
rendering have focused on imagery from electro-optical sensors, particularly
conventional RGB-imagery. In this work, we propose an approach for
differentiable rendering of Synthetic Aperture Radar (SAR) imagery, which
combines methods from 3D computer graphics with neural rendering. We
demonstrate the approach on the inverse graphics problem of 3D Object
Reconstruction from limited SAR imagery using high-fidelity simulated SAR data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning. (arXiv:2204.01254v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01254">
<div class="article-summary-box-inner">
<span><p>Attention mechanisms have been very popular in deep neural networks, where
the Transformer architecture has achieved great success in not only natural
language processing but also visual recognition applications. Recently, a new
Transformer module, applying on batch dimension rather than spatial/channel
dimension, i.e., BatchFormer [18], has been introduced to explore sample
relationships for overcoming data scarcity challenges. However, it only works
with image-level representations for classification. In this paper, we devise a
more general batch Transformer module, BatchFormerV2, which further enables
exploring sample relationships for dense representation learning. Specifically,
when applying the proposed module, it employs a two-stream pipeline during
training, i.e., either with or without a BatchFormerV2 module, where the
batchformer stream can be removed for testing. Therefore, the proposed method
is a plug-and-play module and can be easily integrated into different vision
Transformers without any extra inference cost. Without bells and whistles, we
show the effectiveness of the proposed method for a variety of popular visual
recognition tasks, including image classification and two important dense
prediction tasks: object detection and panoptic segmentation. Particularly,
BatchFormerV2 consistently improves current DETR-based detection methods (e.g.,
DETR, Deformable-DETR, Conditional DETR, and SMCA) by over 1.3%. Code will be
made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct Dense Pose Estimation. (arXiv:2204.01263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01263">
<div class="article-summary-box-inner">
<span><p>Dense human pose estimation is the problem of learning dense correspondences
between RGB images and the surfaces of human bodies, which finds various
applications, such as human body reconstruction, human pose transfer, and human
action recognition. Prior dense pose estimation methods are all based on Mask
R-CNN framework and operate in a top-down manner of first attempting to
identify a bounding box for each person and matching dense correspondences in
each bounding box. Consequently, these methods lack robustness due to their
critical dependence on the Mask R-CNN detection, and the runtime increases
drastically as the number of persons in the image increases. We therefore
propose a novel alternative method for solving the dense pose estimation
problem, called Direct Dense Pose (DDP). DDP first predicts the instance mask
and global IUV representation separately and then combines them together. We
also propose a simple yet effective 2D temporal-smoothing scheme to alleviate
the temporal jitters when dealing with video data. Experiments demonstrate that
DDP overcomes the limitations of previous top-down baseline methods and
achieves competitive accuracy. In addition, DDP is computationally more
efficient than previous dense pose estimation methods, and it reduces jitters
when applied to a video sequence, which is a problem plaguing the previous
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Implicit Scene Completion. (arXiv:2204.01264v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01264">
<div class="article-summary-box-inner">
<span><p>We propose a probabilistic shape completion method extended to the continuous
geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a
considerable amount of missing data cluttered with unsegmented objects. The
problem of shape completion is inherently ill-posed, and high-quality result
requires scalable solutions that consider multiple possible outcomes. We employ
the Generative Cellular Automata that learns the multi-modal distribution and
transform the formulation to process large-scale continuous geometry. The local
continuous shape is incrementally generated as a sparse voxel embedding, which
contains the latent code for each occupied cell. We formally derive that our
training objective for the sparse voxel embedding maximizes the variational
lower bound of the complete shape distribution and therefore our progressive
generation constitutes a valid generative model. Experiments show that our
model successfully generates diverse plausible scenes faithful to the input,
especially when the input suffers from a significant amount of missing data. We
also demonstrate that our approach outperforms deterministic models even in
less ambiguous cases with a small amount of missing data, which infers that
probabilistic formulation is crucial for high-quality geometry completion on
input scans exhibiting any levels of completeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modality Associative Bridging through Memory: Speech Sound Recollected from Face Video. (arXiv:2204.01265v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01265">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel audio-visual multi-modal bridging
framework that can utilize both audio and visual information, even with
uni-modal inputs. We exploit a memory network that stores source (i.e., visual)
and target (i.e., audio) modal representations, where source modal
representation is what we are given, and target modal representations are what
we want to obtain from the memory network. We then construct an associative
bridge between source and target memories that considers the interrelationship
between the two memories. By learning the interrelationship through the
associative bridge, the proposed bridging framework is able to obtain the
target modal representations inside the memory network, even with the source
modal input only, and it provides rich information for its downstream tasks. We
apply the proposed framework to two tasks: lip reading and speech
reconstruction from silent video. Through the proposed associative bridge and
modality-specific memories, each task knowledge is enriched with the recalled
audio context, achieving state-of-the-art performance. We also verify that the
associative bridge properly relates the source and target memories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty. (arXiv:2204.01267v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01267">
<div class="article-summary-box-inner">
<span><p>The ability to make educated predictions about their surroundings, and
associate them with certain confidence, is important for intelligent systems,
like autonomous vehicles and robots. It allows them to plan early and decide
accordingly. Motivated by this observation, in this paper we utilize
information from a video sequence with a narrow field-of-view to infer the
scene at a wider field-of-view. To this end, we propose a temporally consistent
field-of-view extrapolation framework, namely FoV-Net, that: (1) leverages 3D
information to propagate the observed scene parts from past frames; (2)
aggregates the propagated multi-frame information using an attention-based
feature aggregation module and a gated self-attention module, simultaneously
hallucinating any unobserved scene parts; and (3) assigns an interpretable
uncertainty value at each pixel. Extensive experiments show that FoV-Net does
not only extrapolate the temporally consistent wide field-of-view scene better
than existing alternatives, but also provides the associated uncertainty which
may benefit critical decision-making downstream applications. Project page is
at <a href="http://charliememory.github.io/RAL21_FoV.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Monocular Visual Odometry Using Learned Depth. (arXiv:2204.01268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01268">
<div class="article-summary-box-inner">
<span><p>Monocular visual odometry (VO) is an important task in robotics and computer
vision. Thus far, how to build accurate and robust monocular VO systems that
can work well in diverse scenarios remains largely unsolved. In this paper, we
propose a framework to exploit monocular depth estimation for improving VO. The
core of our framework is a monocular depth estimation module with a strong
generalization capability for diverse scenes. It consists of two separate
working modes to assist the localization and mapping. With a single monocular
image input, the depth estimation module predicts a relative depth to help the
localization module on improving the accuracy. With a sparse depth map and an
RGB image input, the depth estimation module can generate accurate
scale-consistent depth for dense mapping. Compared with current learning-based
VO methods, our method demonstrates a stronger generalization ability to
diverse scenes. More significantly, our framework is able to boost the
performances of existing geometry-based VO methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust and Reproducible Active Learning Using Neural Networks. (arXiv:2002.09564v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.09564">
<div class="article-summary-box-inner">
<span><p>Active learning (AL) is a promising ML paradigm that has the potential to
parse through large unlabeled data and help reduce annotation cost in domains
where labelling data can be prohibitive. Recently proposed neural network based
AL methods use different heuristics to accomplish this goal. In this study, we
demonstrate that under identical experimental settings, different types of AL
algorithms (uncertainty based, diversity-based, and committee based) produce an
inconsistent gain over random sampling baseline. Through a variety of
experiments, controlling for sources of stochasticity, we show that variance in
performance metrics achieved by AL algorithms can lead to results that are not
consistent with the previously reported results. We also found that under
strong regularization, AL methods show marginal or no advantage over the random
sampling baseline under a variety of experimental conditions. Finally, we
conclude with a set of recommendations on how to assess the results using a new
AL algorithm to ensure results are reproducible and robust under changes in
experimental conditions. We share our codes to facilitate AL evaluations. We
believe our findings and recommendations will help advance reproducible
research in AL using neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks. (arXiv:2006.00356v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00356">
<div class="article-summary-box-inner">
<span><p>Transfer learning is widely used for training machine learning models. Here,
we study the role of transfer learning for training fully convolutional
networks (FCNs) for medical image segmentation. Our experiments show that
although transfer learning reduces the training time on the target task, the
improvement in segmentation accuracy is highly task/data-dependent. Larger
improvements in accuracy are observed when the segmentation task is more
challenging and the target training data is smaller. We observe that
convolutional filters of an FCN change little during training for medical image
segmentation, and still look random at convergence. We further show that quite
accurate FCNs can be built by freezing the encoder section of the network at
random values and only training the decoder section. At least for medical image
segmentation, this finding challenges the common belief that the encoder
section needs to learn data/task-specific representations. We examine the
evolution of FCN representations to gain a better insight into the effects of
transfer learning on the training dynamics. Our analysis shows that although
FCNs trained via transfer learning learn different representations than FCNs
trained with random initialization, the variability among FCNs trained via
transfer learning can be as high as that among FCNs trained with random
initialization. Moreover, feature reuse is not restricted to the early encoder
layers; rather, it can be more significant in deeper layers. These findings
offer new insights and suggest alternative ways of training FCNs for medical
image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tropical time series, iterated-sums signatures and quasisymmetric functions. (arXiv:2009.08443v3 [math.RA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08443">
<div class="article-summary-box-inner">
<span><p>Aiming for a systematic feature-extraction from time series, we introduce the
iterated-sums signature over arbitrary commutative semirings. The case of the
tropical semiring is a central, and our motivating example. It leads to
features of (real-valued) time series that are not easily available using
existing signature-type objects. We demonstrate how the signature extracts
chronological aspects of a time series, and that its calculation is possible in
linear time. We identify quasisymmetric expressions over semirings as the
appropriate framework for iterated-sums signatures over semiring-valued time
series.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCW-Net: Single Image Deraining with Multi-level Connections and Wide Regional Non-local Blocks. (arXiv:2009.13990v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13990">
<div class="article-summary-box-inner">
<span><p>A recent line of convolutional neural network-based works has succeeded in
capturing rain streaks. However, difficulties in detailed recovery still
remain. In this paper, we present a multi-level connection and wide regional
non-local block network (MCW-Net) to properly restore the original background
textures in rainy images. Unlike existing encoder-decoder-based image deraining
models that improve performance with additional branches, MCW-Net improves
performance by maximizing information utilization without additional branches
through the following two proposed methods. The first method is a multi-level
connection that repeatedly connects multi-level features of the encoder network
to the decoder network. Multi-level connection encourages the decoding process
to use the feature information of all levels. In multi-level connection,
channel-wise attention is considered to learn which level of features is
important in the decoding process of the current level. The second method is a
wide regional non-local block. As rain streaks primarily exhibit a vertical
distribution, we divide the grid of the image into horizontally-wide patches
and apply a non-local operation to each region to explore the rich rain-free
background information. Experimental results on both synthetic and real-world
rainy datasets demonstrate that the proposed model significantly outperforms
existing state-of-the-art models. Furthermore, the results of the joint
deraining and segmentation experiment prove that our model contributes
effectively to other vision tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Accurate Active Camera Localization. (arXiv:2012.04263v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04263">
<div class="article-summary-box-inner">
<span><p>In this work, we solve the problem of active camera localization, which
controls the camera movements actively to achieve an accurate camera pose. The
past solutions are mostly based on Markov Localization, which reduces the
position-wise camera uncertainty for localization. These approaches localize
the camera in the discrete pose space and are agnostic to the
localization-driven scene property, which restrict the camera pose accuracy in
the coarse scale. We propose to overcome these limitations via a novel active
camera localization algorithm, composed of a passive and an active localization
module. The former one optimizes the camera pose in the continuous pose space
by establishing the point-wise camera-world correspondences. The latter one
explicitly models the scene and camera uncertainty components to plan the right
path for accurate camera pose estimation. We validate our algorithm on the
challenging localization scenarios from both synthetic and scanned real-world
indoor scenes. Experimental results demonstrate that our algorithm outperforms
both the state-of-the-art Markov Localization based approach and other compared
approaches on the fine-scale camera pose accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04830">
<div class="article-summary-box-inner">
<span><p>Cataracts are the leading cause of visual impairment and blindness globally.
Over the years, researchers have achieved significant progress in developing
state-of-the-art machine learning techniques for automatic cataract
classification and grading, aiming to prevent cataracts early and improve
clinicians' diagnosis efficiency. This survey provides a comprehensive survey
of recent advances in machine learning techniques for cataract
classification/grading based on ophthalmic images. We summarize existing
literature from two research directions: conventional machine learning methods
and deep learning methods. This survey also provides insights into existing
works of both merits and limitations. In addition, we discuss several
challenges of automatic cataract classification/grading based on machine
learning techniques and present possible solutions to these challenges for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonlinear Evolutionary PDE-Based Refinement of Optical Flow. (arXiv:2102.00487v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00487">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to propose two nonlinear variational models for
obtaining a refined motion estimation from an image sequence. Both the proposed
models can be considered as a part of a generalized framework for an accurate
estimation of physics-based flow fields such as rotational and fluid flow. The
first model is novel in the sense that it is divided into two phases: the first
phase obtains a crude estimate of the optical flow and then the second phase
refines this estimate using additional constraints. The correctness of this
model is proved using an evolutionary PDE approach. The second model achieves
the same refinement as the first model, but in a standard manner, using a
single functional. A special feature of our models is that they permit us to
provide efficient numerical implementations through the first-order primal-dual
Chambolle-Pock scheme. Both the models are compared in the context of accurate
estimation of angle by performing an anisotropic regularization of the
divergence and curl of the flow respectively. We observe that, although both
the models obtain the same level of accuracy, the two-phase model is more
efficient. In fact, we empirically demonstrate that the single-phase and the
two-phase models have convergence rates of order $O(1/N^2)$ and $O(1/N)$
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subspace-Based Feature Fusion From Hyperspectral And Multispectral Image For Land Cover Classification. (arXiv:2102.11228v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11228">
<div class="article-summary-box-inner">
<span><p>In remote sensing, hyperspectral (HS) and multispectral (MS) image fusion
have emerged as a synthesis tool to improve the data set resolution. However,
conventional image fusion methods typically degrade the performance of the land
cover classification. In this paper, a feature fusion method from HS and MS
images for pixel-based classification is proposed. More precisely, the proposed
method first extracts spatial features from the MS image using morphological
profiles. Then, the feature fusion model assumes that both the extracted
morphological profiles and the HS image can be described as a feature matrix
lying in different subspaces. An algorithm based on combining alternating
optimization (AO) and the alternating direction method of multipliers (ADMM) is
developed to solve efficiently the feature fusion problem. Finally, extensive
simulations were run to evaluate the performance of the proposed feature fusion
approach for two data sets. In general, the proposed approach exhibits a
competitive performance compared to other feature extraction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolution-Free Medical Image Segmentation using Transformers. (arXiv:2102.13645v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13645">
<div class="article-summary-box-inner">
<span><p>Like other applications in computer vision, medical image segmentation has
been most successfully addressed using deep learning models that rely on the
convolution operation as their main building block. Convolutions enjoy
important properties such as sparse interactions, weight sharing, and
translation equivariance. These properties give convolutional neural networks
(CNNs) a strong and useful inductive bias for vision tasks. In this work we
show that a different method, based entirely on self-attention between
neighboring image patches and without any convolution operations, can achieve
competitive or better results. Given a 3D image block, our network divides it
into $n^3$ 3D patches, where $n=3 \text{ or } 5$ and computes a 1D embedding
for each patch. The network predicts the segmentation map for the center patch
of the block based on the self-attention between these patch embeddings. We
show that the proposed model can achieve segmentation accuracies that are
better than the state of the art CNNs on three datasets. We also propose
methods for pre-training this model on large corpora of unlabeled images. Our
experiments show that with pre-training the advantage of our proposed network
over CNNs can be significant when labeled training data is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Part Segmentation through Unsupervised Domain Adaptation from Synthetic Vehicles. (arXiv:2103.14098v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14098">
<div class="article-summary-box-inner">
<span><p>Part segmentations provide a rich and detailed part-level description of
objects. However, their annotation requires an enormous amount of work, which
makes it difficult to apply standard deep learning methods. In this paper, we
propose the idea of learning part segmentation through unsupervised domain
adaptation (UDA) from synthetic data. We first introduce UDA-Part, a
comprehensive part segmentation dataset for vehicles that can serve as an
adequate benchmark for UDA (https://qliu24.github.io/udapart). In UDA-Part, we
label parts on 3D CAD models which enables us to generate a large set of
annotated synthetic images. We also annotate parts on a number of real images
to provide a real test set. Secondly, to advance the adaptation of part models
trained from the synthetic data to the real images, we introduce a new UDA
algorithm that leverages the object's spatial structure to guide the adaptation
process. Our experimental results on two real test datasets confirm the
superiority of our approach over existing works, and demonstrate the promise of
learning part segmentation for general objects from synthetic data. We believe
our dataset provides a rich testbed to study UDA for part segmentation and will
help to significantly push forward research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Object-level Supervision for Instance Segmentation with Pixel Embeddings. (arXiv:2103.14572v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14572">
<div class="article-summary-box-inner">
<span><p>Most state-of-the-art instance segmentation methods have to be trained on
densely annotated images. While difficult in general, this requirement is
especially daunting for biomedical images, where domain expertise is often
required for annotation and no large public data collections are available for
pre-training. We propose to address the dense annotation bottleneck by
introducing a proposal-free segmentation approach based on non-spatial
embeddings, which exploits the structure of the learned embedding space to
extract individual instances in a differentiable way. The segmentation loss can
then be applied directly to instances and the overall pipeline can be trained
in a fully- or weakly supervised manner. We consider the challenging case of
positive-unlabeled supervision, where a novel self-supervised consistency loss
is introduced for the unlabeled parts of the training data. We evaluate the
proposed method on 2D and 3D segmentation problems in different microscopy
modalities as well as on the Cityscapes and CVPPP instance segmentation
benchmarks, achieving state-of-the-art results on the latter. The code is
available at: https://github.com/kreshuklab/spoco
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Self-Supervised Learning. (arXiv:2103.14653v3 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14653">
<div class="article-summary-box-inner">
<span><p>The resurgence of self-supervised learning, whereby a deep learning model
generates its own supervisory signal from the data, promises a scalable way to
tackle the dramatically increasing size of real-world data sets without human
annotation. However, the staggering computational complexity of these methods
is such that for state-of-the-art performance, classical hardware requirements
represent a significant bottleneck to further progress. Here we take the first
steps to understanding whether quantum neural networks could meet the demand
for more powerful architectures and test its effectiveness in
proof-of-principle hybrid experiments. Interestingly, we observe a numerical
advantage for the learning of visual representations using small-scale quantum
neural networks over equivalently structured classical networks, even when the
quantum circuits are sampled with only 100 shots. Furthermore, we apply our
best quantum model to classify unseen images on the ibmq\_paris quantum
computer and find that current noisy devices can already achieve equal accuracy
to the equivalent classical model on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring. (arXiv:2104.12665v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12665">
<div class="article-summary-box-inner">
<span><p>The goal of dynamic scene deblurring is to remove the motion blur in a given
image. Typical learning-based approaches implement their solutions by
minimizing the L1 or L2 distance between the output and the reference sharp
image. Recent attempts adopt visual recognition features in training to improve
the perceptual quality. However, those features are primarily designed to
capture high-level contexts rather than low-level structures such as
blurriness. Instead, we propose a more direct way to make images sharper by
exploiting the inverse task of deblurring, namely, reblurring. Reblurring
amplifies the remaining blur to rebuild the original blur, however, a
well-deblurred clean image with zero-magnitude blur is hard to reblur. Thus, we
design two types of reblurring loss functions for better deblurring. The
supervised reblurring loss at training stage compares the amplified blur
between the deblurred and the sharp images. The self-supervised reblurring loss
at inference stage inspects if there noticeable blur remains in the deblurred.
Our experimental results on large-scale benchmarks and real images demonstrate
the effectiveness of the reblurring losses in improving the perceptual quality
of the deblurred images in terms of NIQE and LPIPS scores as well as visual
sharpness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Skeleton-based Action Recognition. (arXiv:2104.13586v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13586">
<div class="article-summary-box-inner">
<span><p>Human skeleton, as a compact representation of human action, has received
increasing attention in recent years. Many skeleton-based action recognition
methods adopt graph convolutional networks (GCN) to extract features on top of
human skeletons. Despite the positive results shown in previous works,
GCN-based methods are subject to limitations in robustness, interoperability,
and scalability. In this work, we propose PoseC3D, a new approach to
skeleton-based action recognition, which relies on a 3D heatmap stack instead
of a graph sequence as the base representation of human skeletons. Compared to
GCN-based methods, PoseC3D is more effective in learning spatiotemporal
features, more robust against pose estimation noises, and generalizes better in
cross-dataset settings. Also, PoseC3D can handle multiple-person scenarios
without additional computation cost, and its features can be easily integrated
with other modalities at early fusion stages, which provides a great design
space to further boost the performance. On four challenging datasets, PoseC3D
consistently obtains superior performance, when used alone on skeletons and in
combination with the RGB modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Does Contrastive Visual Representation Learning Work?. (arXiv:2105.05837v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05837">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised representation learning techniques have largely closed
the gap between supervised and unsupervised learning on ImageNet
classification. While the particulars of pretraining on ImageNet are now
relatively well understood, the field still lacks widely accepted best
practices for replicating this success on other datasets. As a first step in
this direction, we study contrastive self-supervised learning on four diverse
large-scale datasets. By looking through the lenses of data quantity, data
domain, data quality, and task granularity, we provide new insights into the
necessary conditions for successful self-supervised learning. Our key findings
include observations such as: (i) the benefit of additional pretraining data
beyond 500k images is modest, (ii) adding pretraining images from another
domain does not lead to more general representations, (iii) corrupted
pretraining images have a disparate impact on supervised and self-supervised
pretraining, and (iv) contrastive learning lags far behind supervised learning
on fine-grained visual classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Person Extreme Motion Prediction. (arXiv:2105.08825v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08825">
<div class="article-summary-box-inner">
<span><p>Human motion prediction aims to forecast future poses given a sequence of
past 3D skeletons. While this problem has recently received increasing
attention, it has mostly been tackled for single humans in isolation. In this
paper, we explore this problem when dealing with humans performing
collaborative tasks, we seek to predict the future motion of two interacted
persons given two sequences of their past skeletons. We propose a novel cross
interaction attention mechanism that exploits historical information of both
persons, and learns to predict cross dependencies between the two pose
sequences. Since no dataset to train such interactive situations is available,
we collected ExPI (Extreme Pose Interaction), a new lab-based person
interaction dataset of professional dancers performing Lindy-hop dancing
actions, which contains 115 sequences with 30K frames annotated with 3D body
poses and shapes. We thoroughly evaluate our cross interaction network on ExPI
and show that both in short- and long-term predictions, it consistently
outperforms state-of-the-art methods for single-person motion prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02852">
<div class="article-summary-box-inner">
<span><p>This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03743">
<div class="article-summary-box-inner">
<span><p>We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network's pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique "Proxy Normalization"
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization's behavior and consistently matches
or exceeds its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04570">
<div class="article-summary-box-inner">
<span><p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06927">
<div class="article-summary-box-inner">
<span><p>Despite unconditional feature inversion being the foundation of many image
synthesis applications, training an inverter demands a high computational
budget, large decoding capacity and imposing conditions such as autoregressive
priors. To address these limitations, we propose the use of adversarially
robust representations as a perceptual primitive for feature inversion. We
train an adversarially robust encoder to extract disentangled and
perceptually-aligned image representations, making them easily invertible. By
training a simple generator with the mirror architecture of the encoder, we
achieve superior reconstruction quality and generalization over standard
models. Based on this, we propose an adversarially robust autoencoder and
demonstrate its improved performance on anomaly detection, style transfer and
image denoising tasks. Comparisons against recent learn-based methods show that
our model attains improved performance with significantly less complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?. (arXiv:2106.11297v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11297">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel visual representation learning which
relies on a handful of adaptively learned tokens, and which is applicable to
both image and video understanding tasks. Instead of relying on hand-designed
splitting strategies to obtain visual tokens and processing a large number of
densely sampled patches for attention, our approach learns to mine important
tokens in visual data. This results in efficiently and effectively finding a
few important visual tokens and enables modeling of pairwise attention between
such tokens, over a longer temporal horizon for videos, or the spatial content
in images. Our experiments demonstrate strong performance on several
challenging benchmarks for both image and video recognition tasks. Importantly,
due to our tokens being adaptive, we accomplish competitive results at
significantly reduced compute amount. We obtain comparable results to the
state-of-the-arts on ImageNet while being computationally more efficient. We
also confirm the effectiveness of the approach on multiple video datasets,
including Kinetics-400, Kinetics-600, Charades, and AViD.
</p>
<p>The code is available at:
https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional Pixel Synthesis. (arXiv:2106.11485v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11485">
<div class="article-summary-box-inner">
<span><p>High-resolution satellite imagery has proven useful for a broad range of
tasks, including measurement of global human population, local economic
livelihoods, and biodiversity, among many others. Unfortunately,
high-resolution imagery is both infrequently collected and expensive to
purchase, making it hard to efficiently and effectively scale these downstream
tasks over both time and space. We propose a new conditional pixel synthesis
model that uses abundant, low-cost, low-resolution imagery to generate accurate
high-resolution imagery at locations and times in which it is unavailable. We
show that our model attains photo-realistic sample quality and outperforms
competing baselines on a key downstream task -- object counting -- particularly
in geographic locations where conditions on the ground are changing rapidly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Bottlenecks for Multimodal Fusion. (arXiv:2107.00135v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00135">
<div class="article-summary-box-inner">
<span><p>Humans perceive the world by concurrently processing and fusing
high-dimensional inputs from multiple modalities such as vision and audio.
Machine perception models, in stark contrast, are typically modality-specific
and optimised for unimodal benchmarks, and hence late-stage fusion of final
representations or predictions from each modality (`late-fusion') is still a
dominant paradigm for multimodal video classification. Instead, we introduce a
novel transformer based architecture that uses `fusion bottlenecks' for
modality fusion at multiple layers. Compared to traditional pairwise
self-attention, our model forces information between different modalities to
pass through a small number of bottleneck latents, requiring the model to
collate and condense the most relevant information in each modality and only
share what is necessary. We find that such a strategy improves fusion
performance, at the same time reducing computational cost. We conduct thorough
ablation studies, and achieve state-of-the-art results on multiple audio-visual
classification benchmarks including Audioset, Epic-Kitchens and VGGSound. All
code and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v3 [physics.optics] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01422">
<div class="article-summary-box-inner">
<span><p>Noninvasive optical imaging through dynamic scattering media has numerous
important biomedical applications but still remains a challenging task. While
standard diffuse imaging methods measure optical absorption or fluorescent
emission, it is also well-established that the temporal correlation of
scattered coherent light diffuses through tissue much like optical intensity.
Few works to date, however, have aimed to experimentally measure and process
such temporal correlation data to demonstrate deep-tissue video reconstruction
of decorrelation dynamics. In this work, we utilize a single-photon avalanche
diode (SPAD) array camera to simultaneously monitor the temporal dynamics of
speckle fluctuations at the single-photon level from 12 different phantom
tissue surface locations delivered via a customized fiber bundle array. We then
apply a deep neural network to convert the acquired single-photon measurements
into video of scattering dynamics beneath rapidly decorrelating tissue
phantoms. We demonstrate the ability to reconstruct images of transient
(0.1-0.4s) dynamic events occurring up to 8 mm beneath a decorrelating tissue
phantom with millimeter-scale resolution, and highlight how our model can
flexibly extend to monitor flow speed within buried phantom vessels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset. (arXiv:2107.04286v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04286">
<div class="article-summary-box-inner">
<span><p>We present UrbanScene3D, a large-scale data platform for research of urban
scene perception and reconstruction. UrbanScene3D contains over 128k
high-resolution images covering 16 scenes including large-scale real urban
regions and synthetic cities with 136 km^2 area in total. The dataset also
contains high-precision LiDAR scans and hundreds of image sets with different
observation patterns, which provide a comprehensive benchmark to design and
evaluate aerial path planning and 3D reconstruction algorithms. In addition,
the dataset, which is built on Unreal Engine and Airsim simulator together with
the manually annotated unique instance label for each building in the dataset,
enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D
bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with
physical engine and lighting system not only produce variety of data but also
enable users to simulate cars or drones in the proposed urban environment for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal MRI Reconstruction Assisted with Spatial Alignment Network. (arXiv:2108.05603v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05603">
<div class="article-summary-box-inner">
<span><p>In clinical practice, multi-modal magnetic resonance imaging (MRI) with
different contrasts is usually acquired in a single study to assess different
properties of the same region of interest in the human body. The whole
acquisition process can be accelerated by having one or more modalities
under-sampled in the $k$-space. Recent research has shown that, considering the
redundancy between different modalities, a target MRI modality under-sampled in
the $k$-space can be more efficiently reconstructed with a fully-sampled
reference MRI modality. However, we find that the performance of the
aforementioned multi-modal reconstruction can be negatively affected by subtle
spatial misalignment between different modalities, which is actually common in
clinical practice. In this paper, we improve the quality of multi-modal
reconstruction by compensating for such spatial misalignment with a spatial
alignment network. First, our spatial alignment network estimates the
displacement between the fully-sampled reference and the under-sampled target
images, and warps the reference image accordingly. Then, the aligned
fully-sampled reference image joins the multi-modal reconstruction of the
under-sampled target image. Also, considering the contrast difference between
the target and reference images, we have designed a
cross-modality-synthesis-based registration loss in combination with the
reconstruction loss, to jointly train the spatial alignment network and the
reconstruction network. The experiments on both clinical MRI and multi-coil
$k$-space raw data demonstrate the superiority and robustness of the
multi-modal MRI reconstruction empowered with our spatial alignment network.
Our code is publicly available at
\url{https://github.com/woxuankai/SpatialAlignmentNetwork}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full-resolution quality assessment for pansharpening. (arXiv:2108.06144v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06144">
<div class="article-summary-box-inner">
<span><p>A reliable quality assessment procedure for pansharpening methods is of
critical importance for the development of the related solutions.
Unfortunately, the lack of ground-truths to be used as guidance for an
objective evaluation has pushed the community to resort to two approaches which
can also be jointly applied. Hence, two kinds of indexes can be found in the
literature: i) reference-based reduced-resolution indexes aimed to assess the
synthesis ability; ii) no-reference subjective quality indexes for
full-resolution datasets aimed to assess spectral and spatial consistency. Both
reference-based and no-reference indexes present critical shortcomings which
motivate the community to explore new solutions. In this work, we propose an
alternative no-reference full-resolution assessment framework. On one side we
introduce a protocol, namely the reprojection protocol, to take care of the
spectral consistency issue. On the other side, a new index of the spatial
consistency between the pansharpened image and the panchromatic band at full
resolution is also proposed. Experimental results carried out on different
datasets/sensors demonstrate the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Navigation: A Survey and Taxonomy. (arXiv:2108.11544v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11544">
<div class="article-summary-box-inner">
<span><p>Vision-Language Navigation (VLN) tasks require an agent to follow human
language instructions to navigate in previously unseen environments. This
challenging field involving problems in natural language processing, computer
vision, robotics, etc., has spawn many excellent works focusing on various VLN
tasks. This paper provides a comprehensive survey and an insightful taxonomy of
these tasks based on the different characteristics of language instructions in
these tasks. Depending on whether the navigation instructions are given for
once or multiple times, this paper divides the tasks into two categories, i.e.,
single-turn and multi-turn tasks. For single-turn tasks, we further subdivide
them into goal-oriented and route-oriented based on whether the instructions
designate a single goal location or specify a sequence of multiple locations.
For multi-turn tasks, we subdivide them into passive and interactive tasks
based on whether the agent is allowed to question the instruction or not. These
tasks require different capabilities of the agent and entail various model
designs. We identify progress made on the tasks and look into the limitations
of existing VLN models and task settings. Finally, we discuss several open
issues of VLN and point out some opportunities in the future, i.e.,
incorporating knowledge with VLN models and implementing them in the real
physical world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04275">
<div class="article-summary-box-inner">
<span><p>Despite the potential of multi-modal pre-training to learn highly
discriminative feature representations from complementary data modalities,
current progress is being slowed by the lack of large-scale modality-diverse
datasets. By leveraging the natural suitability of E-commerce, where different
modalities capture complementary semantic information, we contribute a
large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5
modalities (image, text, table, video, and audio), covers over 6,000 categories
and 5,000 attributes, and is 500 larger than the largest publicly available
dataset with a similar number of modalities. Furthermore, M5Product contains
incomplete modality pairs and noise while also having a long-tailed
distribution, resembling most real-world problems. We further propose
Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework
that integrates the different modalities into a unified model through an
adaptive feature fusion mechanism, where the importance of each modality is
learned directly from the modality embeddings and impacts the inter-modality
contrastive learning and masked tasks within a multi-modal transformer model.
We evaluate the current multi-modal pre-training state-of-the-art approaches
and benchmark their ability to learn from unlabeled data when faced with the
large number of modalities in the M5Product dataset. We conduct extensive
experiments on four downstream tasks and demonstrate the superiority of our
SCALE model, providing insights into the importance of dataset scale and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer. (arXiv:2109.08024v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08024">
<div class="article-summary-box-inner">
<span><p>We propose a semi-supervised network for wide-angle portraits correction.
Wide-angle images often suffer from skew and distortion affected by perspective
distortion, especially noticeable at the face regions. Previous deep learning
based approaches need the ground-truth correction flow maps for training
guidance. However, such labels are expensive, which can only be obtained
manually. In this work, we design a semi-supervised scheme and build a
high-quality unlabeled dataset with rich scenarios, allowing us to
simultaneously use labeled and unlabeled data to improve performance.
Specifically, our semi-supervised scheme takes advantage of the consistency
mechanism, with several novel components such as direction and range
consistency (DRC) and regression consistency (RC). Furthermore, different from
the existing methods, we propose the Multi-Scale Swin-Unet (MS-Unet) based on
the multi-scale swin transformer block (MSTB), which can simultaneously learn
short-distance and long-distance information to avoid artifacts. Extensive
experiments demonstrate that the proposed method is superior to the
state-of-the-art methods and other representative baselines. The source code
and dataset are available at:
https://github.com/megvii-research/Portraits_Correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoPhaseNN: Unsupervised Physics-aware Deep Learning of 3D Nanoscale Bragg Coherent Diffraction Imaging. (arXiv:2109.14053v2 [physics.app-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14053">
<div class="article-summary-box-inner">
<span><p>The problem of phase retrieval, or the algorithmic recovery of lost phase
information from measured intensity alone, underlies various imaging methods
from astronomy to nanoscale imaging. Traditional methods of phase retrieval are
iterative in nature, and are therefore computationally expensive and time
consuming. More recently, deep learning (DL) models have been developed to
either provide learned priors to iterative phase retrieval or in some cases
completely replace phase retrieval with networks that learn to recover the lost
phase information from measured intensity alone. However, such models require
vast amounts of labeled data, which can only be obtained through simulation or
performing computationally prohibitive phase retrieval on hundreds of or even
thousands of experimental datasets. Using a 3D nanoscale X-ray imaging modality
(Bragg Coherent Diffraction Imaging or BCDI) as a representative technique, we
demonstrate AutoPhaseNN, a DL-based approach which learns to solve the phase
problem without labeled data. By incorporating the physics of the imaging
technique into the DL model during training, AutoPhaseNN learns to invert 3D
BCDI data from reciprocal space to real space in a single shot without ever
being shown real space images. Once trained, AutoPhaseNN is about one hundred
times faster than traditional iterative phase retrieval methods while providing
comparable image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Flow Estimation for Spiking Camera. (arXiv:2110.03916v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03916">
<div class="article-summary-box-inner">
<span><p>As a bio-inspired sensor with high temporal resolution, the spiking camera
has an enormous potential in real applications, especially for motion
estimation in high-speed scenes. However, frame-based and event-based methods
are not well suited to spike streams from the spiking camera due to the
different data modalities. To this end, we present, SCFlow, a tailored deep
learning pipeline to estimate optical flow in high-speed scenes from spike
streams. Importantly, a novel input representation is introduced which can
adaptively remove the motion blur in spike streams according to the prior
motion. Further, for training SCFlow, we synthesize two sets of optical flow
data for the spiking camera, SPIkingly Flying Things and Photo-realistic
High-speed Motion, denoted as SPIFT and PHM respectively, corresponding to
random high-speed and well-designed scenes. Experimental results show that the
SCFlow can predict optical flow from spike streams in different high-speed
scenes. Moreover, SCFlow shows promising generalization on \textbf{real spike
streams}. Codes and datasets refer to
https://github.com/Acnext/Optical-Flow-For-Spiking-Camera.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Learning from An Expert in Deep Recommendation Systems with Marginal Distance Probability Distribution. (arXiv:2110.06287v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06287">
<div class="article-summary-box-inner">
<span><p>Recommendation systems play an important role in today's digital world. They
have found applications in various applications such as music platforms, e.g.,
Spotify, and movie streaming services, e.g., Netflix. Less research effort has
been devoted to physical exercise recommendation systems. Sedentary lifestyles
have become the major driver of several diseases as well as healthcare costs.
In this paper, we develop a recommendation system for daily exercise activities
to users based on their history, profile and similar users. The developed
recommendation system uses a deep recurrent neural network with user-profile
attention and temporal attention mechanisms.
</p>
<p>Moreover, exercise recommendation systems are significantly different from
streaming recommendation systems in that we are not able to collect click
feedback from the participants in exercise recommendation systems. Thus, we
propose a real-time, expert-in-the-loop active learning procedure. The active
learners calculate the uncertainty of the recommender at each time step for
each user and ask an expert for a recommendation when the certainty is low. In
this paper, we derive the probability distribution function of marginal
distance, and use it to determine when to ask experts for feedback. Our
experimental results on a mHealth dataset show improved accuracy after
incorporating the real-time active learner with the recommendation system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trigger Hunting with a Topological Prior for Trojan Detection. (arXiv:2110.08335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08335">
<div class="article-summary-box-inner">
<span><p>Despite their success and popularity, deep neural networks (DNNs) are
vulnerable when facing backdoor attacks. This impedes their wider adoption,
especially in mission critical applications. This paper tackles the problem of
Trojan detection, namely, identifying Trojaned models -- models trained with
poisoned data. One popular approach is reverse engineering, i.e., recovering
the triggers on a clean image by manipulating the model's prediction. One major
challenge of reverse engineering approach is the enormous search space of
triggers. To this end, we propose innovative priors such as diversity and
topological simplicity to not only increase the chances of finding the
appropriate triggers but also improve the quality of the found triggers.
Moreover, by encouraging a diverse set of trigger candidates, our method can
perform effectively in cases with unknown target labels. We demonstrate that
these priors can significantly improve the quality of the recovered triggers,
resulting in substantially improved Trojan detection accuracy as validated on
both synthetic and publicly available TrojAI benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynCoLFinGer: Synthetic Contactless Fingerprint Generator. (arXiv:2110.09144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09144">
<div class="article-summary-box-inner">
<span><p>We present the first method for synthetic generation of contactless
fingerprint images, referred to as SynCoLFinGer. To this end, the constituent
components of contactless fingerprint images regarding capturing, subject
characteristics, and environmental influences are modeled and applied to a
synthetically generated ridge pattern using the SFinGe algorithm. The proposed
method is able to generate different synthetic samples corresponding to a
single finger and it can be parameterized to generate contactless fingerprint
images of various quality levels. The resemblance of the synthetically
generated contactless fingerprints to real fingerprints is confirmed by
evaluating biometric sample quality using an adapted NFIQ 2.0 algorithm and
biometric utility using a state-of-the-art contactless fingerprint recognition
system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Redundancy: Separable Group Convolutional Networks on Lie Groups. (arXiv:2110.13059v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13059">
<div class="article-summary-box-inner">
<span><p>Group convolutional neural networks (G-CNNs) have been shown to increase
parameter efficiency and model accuracy by incorporating geometric inductive
biases. In this work, we investigate the properties of representations learned
by regular G-CNNs, and show considerable parameter redundancy in group
convolution kernels. This finding motivates further weight-tying by sharing
convolution kernels over subgroups. To this end, we introduce convolution
kernels that are separable over the subgroup and channel dimensions. In order
to obtain equivariance to arbitrary affine Lie groups we provide a continuous
parameterisation of separable convolution kernels. We evaluate our approach
across several vision datasets, and show that our weight sharing leads to
improved performance and computational efficiency. In many settings, separable
G-CNNs outperform their non-separable counterpart, while only using a fraction
of their training time. In addition, thanks to the increase in computational
efficiency, we are able to implement G-CNNs equivariant to the
$\mathrm{Sim(2)}$ group; the group of dilations, rotations and translations.
$\mathrm{Sim(2)}$-equivariance further improves performance on all tasks
considered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13981">
<div class="article-summary-box-inner">
<span><p>Filter pruning has been widely used for neural network compression because of
its enabled practical acceleration. To date, most of the existing filter
pruning works explore the importance of filters via using intra-channel
information. In this paper, starting from an inter-channel perspective, we
propose to perform efficient filter pruning using Channel Independence, a
metric that measures the correlations among different feature maps. The less
independent feature map is interpreted as containing less useful
information$/$knowledge, and hence its corresponding filter can be pruned
without affecting model capacity. We systematically investigate the
quantification metric, measuring scheme and sensitiveness$/$reliability of
channel independence in the context of filter pruning. Our evaluation results
for different models on various datasets show the superior performance of our
approach. Notably, on CIFAR-10 dataset our solution can bring $0.90\%$ and
$0.94\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models,
respectively, and meanwhile the model size and FLOPs are reduced by $42.8\%$
and $47.4\%$ (for ResNet-56) and $48.3\%$ and $52.1\%$ (for ResNet-110),
respectively. On ImageNet dataset, our approach can achieve $40.8\%$ and
$44.8\%$ storage and computation reductions, respectively, with $0.15\%$
accuracy increase over the baseline ResNet-50 model. The code is available at
https://github.com/Eclipsess/CHIP_NeurIPS2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pansharpening by convolutional neural networks in the full resolution framework. (arXiv:2111.08334v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08334">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been a growing interest in deep learning-based
pansharpening. Thus far, research has mainly focused on architectures.
Nonetheless, model training is an equally important issue. A first problem is
the absence of ground truths, unavoidable in pansharpening. This is often
addressed by training networks in a reduced resolution domain and using the
original data as ground truth, relying on an implicit scale invariance
assumption. However, on full resolution images results are often disappointing,
suggesting such invariance not to hold. A further problem is the scarcity of
training data, which causes a limited generalization ability and a poor
performance on off-training test images. In this paper, we propose a
full-resolution training framework for deep learning-based pansharpening. The
framework is fully general and can be used for any deep learning-based
pansharpening model. Training takes place in the high-resolution domain,
relying only on the original data, thus avoiding any loss of information. To
ensure spectral and spatial fidelity, a suitable two-component loss is defined.
The spectral component enforces consistency between the pansharpened output and
the low-resolution multispectral input. The spatial component, computed at
high-resolution, maximizes the local correlation between each pansharpened band
and the panchromatic input. At testing time, the target-adaptive operating
modality is adopted, achieving good generalization with a limited computational
overhead. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1
images show that methods trained with the proposed framework guarantee a pretty
good performance in terms of both full-resolution numerical indexes and visual
quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09162">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a framework for reading analog clocks in natural
images or videos. Specifically, we make the following contributions: First, we
create a scalable pipeline for generating synthetic clocks, significantly
reducing the requirements for the labour-intensive annotations; Second, we
introduce a clock recognition architecture based on spatial transformer
networks (STN), which is trained end-to-end for clock alignment and
recognition. We show that the model trained on the proposed synthetic dataset
generalises towards real clocks with good accuracy, advocating a Sim2Real
training regime; Third, to further reduce the gap between simulation and real
data, we leverage the special property of time, i.e. uniformity, to generate
reliable pseudo-labels on real unlabelled clock videos, and show that training
on these videos offers further improvements while still requiring zero manual
annotations. Lastly, we introduce three benchmark datasets based on COCO, Open
Images, and The Clock movie, totalling 4,472 images with clocks, with full
annotations for time, accurate to the minute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete Representations Strengthen Vision Transformer Robustness. (arXiv:2111.10493v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10493">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) is emerging as the state-of-the-art architecture for
image recognition. While recent studies suggest that ViTs are more robust than
their convolutional counterparts, our experiments find that ViTs trained on
ImageNet are overly reliant on local textures and fail to make adequate use of
shape information. ViTs thus have difficulties generalizing to
out-of-distribution, real-world data. To address this deficiency, we present a
simple and effective architecture modification to ViT's input layer by adding
discrete tokens produced by a vector-quantized encoder. Different from the
standard continuous pixel tokens, discrete tokens are invariant under small
perturbations and contain less information individually, which promote ViTs to
learn global information that is invariant. Experimental results demonstrate
that adding discrete representation on four architecture variants strengthens
ViT robustness by up to 12% across seven ImageNet robustness benchmarks while
maintaining the performance on ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferability Estimation using Bhattacharyya Class Separability. (arXiv:2111.12780v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12780">
<div class="article-summary-box-inner">
<span><p>Transfer learning has become a popular method for leveraging pre-trained
models in computer vision. However, without performing computationally
expensive fine-tuning, it is difficult to quantify which pre-trained source
models are suitable for a specific target task, or, conversely, to which tasks
a pre-trained source model can be easily adapted to. In this work, we propose
Gaussian Bhattacharyya Coefficient (GBC), a novel method for quantifying
transferability between a source model and a target dataset. In a first step we
embed all target images in the feature space defined by the source model, and
represent them with per-class Gaussians. Then, we estimate their pairwise class
separability using the Bhattacharyya coefficient, yielding a simple and
effective measure of how well the source model transfers to the target task. We
evaluate GBC on image classification tasks in the context of dataset and
architecture selection. Further, we also perform experiments on the more
complex semantic segmentation transferability estimation task. We demonstrate
that GBC outperforms state-of-the-art transferability metrics on most
evaluation criteria in the semantic segmentation settings, matches the
performance of top methods for dataset transferability in image classification,
and performs best on architecture selection problems for image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SurfEmb: Dense and Continuous Correspondence Distributions for Object Pose Estimation with Learnt Surface Embeddings. (arXiv:2111.13489v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13489">
<div class="article-summary-box-inner">
<span><p>We present an approach to learn dense, continuous 2D-3D correspondence
distributions over the surface of objects from data with no prior knowledge of
visual ambiguities like symmetry. We also present a new method for 6D pose
estimation of rigid objects using the learnt distributions to sample, score and
refine pose hypotheses. The correspondence distributions are learnt with a
contrastive loss, represented in object-specific latent spaces by an
encoder-decoder query model and a small fully connected key model. Our method
is unsupervised with respect to visual ambiguities, yet we show that the query-
and key models learn to represent accurate multi-modal surface distributions.
Our pose estimation method improves the state-of-the-art significantly on the
comprehensive BOP Challenge, trained purely on synthetic data, even compared
with methods trained on real data. The project site is at
https://surfemb.github.io/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Relations are Equal: Mining Informative Labels for Scene Graph Generation. (arXiv:2111.13517v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13517">
<div class="article-summary-box-inner">
<span><p>Scene graph generation (SGG) aims to capture a wide variety of interactions
between pairs of objects, which is essential for full scene understanding.
Existing SGG methods trained on the entire set of relations fail to acquire
complex reasoning about visual and textual correlations due to various biases
in training data. Learning on trivial relations that indicate generic spatial
configuration like 'on' instead of informative relations such as 'parked on'
does not enforce this complex reasoning, harming generalization. To address
this problem, we propose a novel framework for SGG training that exploits
relation labels based on their informativeness. Our model-agnostic training
procedure imputes missing informative relations for less informative samples in
the training data and trains a SGG model on the imputed labels along with
existing annotations. We show that this approach can successfully be used in
conjunction with state-of-the-art SGG methods and improves their performance
significantly in multiple metrics on the standard Visual Genome benchmark.
Furthermore, we obtain considerable improvements for unseen triplets in a more
challenging zero-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Referring Video Object Segmentation with Multimodal Transformers. (arXiv:2111.14821v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14821">
<div class="article-summary-box-inner">
<span><p>The referring video object segmentation task (RVOS) involves segmentation of
a text-referred object instance in the frames of a given video. Due to the
complex nature of this multimodal task, which combines text reasoning, video
understanding, instance segmentation and tracking, existing approaches
typically rely on sophisticated pipelines in order to tackle it. In this paper,
we propose a simple Transformer-based approach to RVOS. Our framework, termed
Multimodal Tracking Transformer (MTTR), models the RVOS task as a sequence
prediction problem. Following recent advancements in computer vision and
natural language processing, MTTR is based on the realization that video and
text can be processed together effectively and elegantly by a single multimodal
Transformer model. MTTR is end-to-end trainable, free of text-related inductive
bias components and requires no additional mask-refinement post-processing
steps. As such, it simplifies the RVOS pipeline considerably compared to
existing methods. Evaluation on standard benchmarks reveals that MTTR
significantly outperforms previous art across multiple metrics. In particular,
MTTR shows impressive +5.7 and +5.0 mAP gains on the A2D-Sentences and
JHMDB-Sentences datasets respectively, while processing 76 frames per second.
In addition, we report strong results on the public validation set of
Refer-YouTube-VOS, a more challenging RVOS dataset that has yet to receive the
attention of researchers. The code to reproduce our experiments is available at
https://github.com/mttr2021/MTTR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessment of Data Consistency through Cascades of Independently Recurrent Inference Machines for fast and robust accelerated MRI reconstruction. (arXiv:2111.15498v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15498">
<div class="article-summary-box-inner">
<span><p>Machine Learning methods can learn how to reconstruct Magnetic Resonance
Images and thereby accelerate acquisition, which is of paramount importance to
the clinical workflow. Physics-informed networks incorporate the forward model
of accelerated MRI reconstruction in the learning process. With increasing
network complexity, robustness is not ensured when reconstructing data unseen
during training. We aim to embed data consistency (DC) in deep networks while
balancing the degree of network complexity. While doing so, we will assess
whether either explicit or implicit enforcement of DC in varying network
architectures is preferred to optimize performance. We propose a scheme called
Cascades of Independently Recurrent Inference Machines (CIRIM) to assess DC
through unrolled optimization. Herein we assess DC both implicitly by gradient
descent and explicitly by a designed term. Extensive comparison of the CIRIM to
CS as well as to other methods is performed: the E2EVN, CascadeNet, KIKINet,
LPDNet, RIM, IRIM, and UNet. Models were trained and evaluated on T1-weighted
and FLAIR contrast brain data, and T2-weighted knee data. Both 1D and 2D
undersampling patterns were evaluated. Robustness was tested by reconstructing
7.5x prospectively undersampled 3D FLAIR MRI data of Multiple Sclerosis (MS)
patients with white matter lesions. The CIRIM performed best when implicitly
enforcing DC, while the E2EVN required an explicit DC formulation. In
reconstructing MS patient data, prospectively acquired with a sampling pattern
unseen during model training, the CIRIM maintained lesion contrast while
efficiently denoising the images. The CIRIM showed highly promising
generalization capabilities maintaining a very fair trade-off between
reconstructed image quality and fast reconstruction times, which is crucial in
the clinical workflow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Background Activation Suppression for Weakly Supervised Object Localization. (arXiv:2112.00580v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00580">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) aims to localize objects using
only image-level labels. Recently a new paradigm has emerged by generating a
foreground prediction map (FPM) to achieve localization task. Existing
FPM-based methods use cross-entropy (CE) to evaluate the foreground prediction
map and to guide the learning of generator. We argue for using activation value
to achieve more efficient learning. It is based on the experimental observation
that, for a trained network, CE converges to zero when the foreground mask
covers only part of the object region. While activation value increases until
the mask expands to the object boundary, which indicates that more object areas
can be learned by using activation value. In this paper, we propose a
Background Activation Suppression (BAS) method. Specifically, an Activation Map
Constraint module (AMC) is designed to facilitate the learning of generator by
suppressing the background activation value. Meanwhile, by using the foreground
region guidance and the area constraint, BAS can learn the whole region of the
object. In the inference phase, we consider the prediction maps of different
categories together to obtain the final localization results. Extensive
experiments show that BAS achieves significant and consistent improvement over
the baseline methods on the CUB-200-2011 and ILSVRC datasets. Code and models
are available at https://github.com/wpy1999/BAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperInverter: Improving StyleGAN Inversion via Hypernetwork. (arXiv:2112.00719v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00719">
<div class="article-summary-box-inner">
<span><p>Real-world image manipulation has achieved fantastic progress in recent years
as a result of the exploration and utilization of GAN latent spaces. GAN
inversion is the first step in this pipeline, which aims to map the real image
to the latent code faithfully. Unfortunately, the majority of existing GAN
inversion methods fail to meet at least one of the three requirements listed
below: high reconstruction quality, editability, and fast inference. We present
a novel two-phase strategy in this research that fits all requirements at the
same time. In the first phase, we train an encoder to map the input image to
StyleGAN2 $\mathcal{W}$-space, which was proven to have excellent editability
but lower reconstruction quality. In the second phase, we supplement the
reconstruction ability in the initial phase by leveraging a series of
hypernetworks to recover the missing information during inversion. These two
steps complement each other to yield high reconstruction quality thanks to the
hypernetwork branch and excellent editability due to the inversion done in the
$\mathcal{W}$-space. Our method is entirely encoder-based, resulting in
extremely fast inference. Extensive experiments on two challenging datasets
demonstrate the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GANSeg: Learning to Segment by Unsupervised Hierarchical Image Generation. (arXiv:2112.01036v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01036">
<div class="article-summary-box-inner">
<span><p>Segmenting an image into its parts is a frequent preprocess for high-level
vision tasks such as image editing. However, annotating masks for supervised
training is expensive. Weakly-supervised and unsupervised methods exist, but
they depend on the comparison of pairs of images, such as from multi-views,
frames of videos, and image augmentation, which limits their applicability. To
address this, we propose a GAN-based approach that generates images conditioned
on latent masks, thereby alleviating full or weak annotations required in
previous approaches. We show that such mask-conditioned image generation can be
learned faithfully when conditioning the masks in a hierarchical manner on
latent keypoints that define the position of parts explicitly. Without
requiring supervision of masks or points, this strategy increases robustness to
viewpoint and object positions changes. It also lets us generate image-mask
pairs for training a segmentation network, which outperforms the
state-of-the-art unsupervised segmentation methods on established benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OW-DETR: Open-world Detection Transformer. (arXiv:2112.01513v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01513">
<div class="article-summary-box-inner">
<span><p>Open-world object detection (OWOD) is a challenging computer vision problem,
where the task is to detect a known set of object categories while
simultaneously identifying unknown objects. Additionally, the model must
incrementally learn new classes that become known in the next training
episodes. Distinct from standard object detection, the OWOD setting poses
significant challenges for generating quality candidate proposals on
potentially unknown objects, separating the unknown objects from the background
and detecting diverse unknown objects. Here, we introduce a novel end-to-end
transformer-based framework, OW-DETR, for open-world object detection. The
proposed OW-DETR comprises three dedicated components namely, attention-driven
pseudo-labeling, novelty classification and objectness scoring to explicitly
address the aforementioned OWOD challenges. Our OW-DETR explicitly encodes
multi-scale contextual information, possesses less inductive bias, enables
knowledge transfer from known classes to the unknown class and can better
discriminate between unknown objects and background. Comprehensive experiments
are performed on two benchmarks: MS-COCO and PASCAL VOC. The extensive
ablations reveal the merits of our proposed contributions. Further, our model
outperforms the recently introduced OWOD approach, ORE, with absolute gains
ranging from 1.8% to 3.3% in terms of unknown recall on MS-COCO. In the case of
incremental object detection, OW-DETR outperforms the state-of-the-art for all
settings on PASCAL VOC. Our code is available at
https://github.com/akshitac8/OW-DETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E$^2$(GO)MOTION: Motion Augmented Event Stream for Egocentric Action Recognition. (arXiv:2112.03596v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03596">
<div class="article-summary-box-inner">
<span><p>Event cameras are novel bio-inspired sensors, which asynchronously capture
pixel-level intensity changes in the form of "events". Due to their sensing
mechanism, event cameras have little to no motion blur, a very high temporal
resolution and require significantly less power and memory than traditional
frame-based cameras. These characteristics make them a perfect fit to several
real-world applications such as egocentric action recognition on wearable
devices, where fast camera motion and limited power challenge traditional
vision sensors. However, the ever-growing field of event-based vision has, to
date, overlooked the potential of event cameras in such applications. In this
paper, we show that event data is a very valuable modality for egocentric
action recognition. To do so, we introduce N-EPIC-Kitchens, the first
event-based camera extension of the large-scale EPIC-Kitchens dataset. In this
context, we propose two strategies: (i) directly processing event-camera data
with traditional video-processing architectures (E$^2$(GO)) and (ii) using
event-data to distill optical flow information (E$^2$(GO)MO). On our proposed
benchmark, we show that event data provides a comparable performance to RGB and
optical flow, yet without any additional flow computation at deploy time, and
an improved performance of up to 4% with respect to RGB only information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's Behind the Couch? Directed Ray Distance Functions (DRDF) for 3D Scene Reconstruction. (arXiv:2112.04481v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04481">
<div class="article-summary-box-inner">
<span><p>We present an approach for full 3D scene reconstruction from a single unseen
image. We train on dataset of realistic non-watertight scans of scenes. Our
approach predicts a distance function, since these have shown promise in
handling complex topologies and large spaces. We identify and analyze two key
challenges for predicting such image conditioned distance functions that have
prevented their success on real 3D scene data. First, we show that predicting a
conventional scene distance from an image requires reasoning over a large
receptive field. Second, we analytically show that the optimal output of the
network trained to predict these distance functions does not obey all the
distance function properties. We propose an alternate distance function, the
Directed Ray Distance Function (DRDF), that tackles both challenges. We show
that a deep network trained to predict DRDFs outperforms all other methods
quantitatively and qualitatively on 3D reconstruction from single image on
Matterport3D, 3DFront, and ScanNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Point Transformer. (arXiv:2112.04702v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04702">
<div class="article-summary-box-inner">
<span><p>The recent success of neural networks enables a better interpretation of 3D
point clouds, but processing a large-scale 3D scene remains a challenging
problem. Most current approaches divide a large-scale scene into small regions
and combine the local predictions together. However, this scheme inevitably
involves additional stages for pre- and post-processing and may also degrade
the final output due to predictions in a local perspective. This paper
introduces Fast Point Transformer that consists of a new lightweight
self-attention layer. Our approach encodes continuous 3D coordinates, and the
voxel hashing-based architecture boosts computational efficiency. The proposed
method is demonstrated with 3D semantic segmentation and 3D detection. The
accuracy of our approach is competitive to the best voxel-based method, and our
network achieves 129 times faster inference time than the state-of-the-art,
Point Transformer, with a reasonable accuracy trade-off in 3D semantic
segmentation on S3DIS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Spatio-Temporal Contrastive Learning with Self-Supervision. (arXiv:2112.05181v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05181">
<div class="article-summary-box-inner">
<span><p>Modern self-supervised learning algorithms typically enforce persistency of
instance representations across views. While being very effective on learning
holistic image and video representations, such an objective becomes sub-optimal
for learning spatio-temporally fine-grained features in videos, where scenes
and instances evolve through space and time. In this paper, we present
Contextualized Spatio-Temporal Contrastive Learning (ConST-CL) to effectively
learn spatio-temporally fine-grained video representations via
self-supervision. We first design a region-based pretext task which requires
the model to transform in-stance representations from one view to another,
guided by context features. Further, we introduce a simple network design that
successfully reconciles the simultaneous learning process of both holistic and
local representations. We evaluate our learned representations on a variety of
downstream tasks and show that ConST-CL achieves competitive results on 6
datasets, including Kinetics, UCF, HMDB, AVA-Kinetics, AVA and OTB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes. (arXiv:2112.05298v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05298">
<div class="article-summary-box-inner">
<span><p>Building embodied intelligent agents that can interact with 3D indoor
environments has received increasing research attention in recent years. While
most works focus on single-object or agent-object visual functionality and
affordances, our work proposes to study a new kind of visual relationship that
is also important to perceive and model -- inter-object functional
relationships (e.g., a switch on the wall turns on or off the light, a remote
control operates the TV). Humans often spend little or no effort to infer these
relationships, even when entering a new room, by using our strong prior
knowledge (e.g., we know that buttons control electrical devices) or using only
a few exploratory interactions in cases of uncertainty (e.g., multiple switches
and lights in the same room). In this paper, we take the first step in building
AI system learning inter-object functional relationships in 3D indoor
environments with key technical contributions of modeling prior knowledge by
training over large-scale scenes and designing interactive policies for
effectively exploring the training scenes and quickly adapting to novel test
scenes. We create a new benchmark based on the AI2Thor and PartNet datasets and
perform extensive experiments that prove the effectiveness of our proposed
method. Results show that our model successfully learns priors and
fast-interactive-adaptation strategies for exploring inter-object functional
relationships in complex 3D scenes. Several ablation studies further validate
the usefulness of each proposed module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimedia Datasets for Anomaly Detection: A Review. (arXiv:2112.05410v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05410">
<div class="article-summary-box-inner">
<span><p>Multimedia anomaly datasets play a crucial role in automated surveillance.
They have a wide range of applications expanding from outlier objects/
situation detection to the detection of life-threatening events. For more than
1.5 decades, this field has attracted a lot of research attention, and as a
result, more and more datasets dedicated to anomalous actions and object
detection have been developed. Tapping these public anomaly datasets enable
researchers to generate and compare various anomaly detection frameworks with
the same input data. This paper presents a comprehensive survey on a variety of
video, audio, as well as audio-visual datasets based on the application of
anomaly detection. This survey aims to address the lack of a comprehensive
comparison and analysis of multimedia public datasets based on anomaly
detection. Also, it can assist researchers in selecting the best available
dataset for bench-marking frameworks. Additionally, we discuss gaps in the
existing dataset and insights for future direction towards developing
multimodal anomaly detection datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07471">
<div class="article-summary-box-inner">
<span><p>Traditional 3D morphable face models (3DMMs) provide fine-grained control
over expression but cannot easily capture geometric and appearance details.
Neural volumetric representations approach photorealism but are hard to animate
and do not generalize well to unseen expressions. To tackle this problem, we
propose IMavatar (Implicit Morphable avatar), a novel method for learning
implicit head avatars from monocular videos. Inspired by the fine-grained
control mechanisms afforded by conventional 3DMMs, we represent the expression-
and pose- related deformations via learned blendshapes and skinning fields.
These attributes are pose-independent and can be used to morph the canonical
geometry and texture fields given novel expression and pose parameters. We
employ ray marching and iterative root-finding to locate the canonical surface
intersection for each pixel. A key contribution is our novel analytical
gradient formulation that enables end-to-end training of IMavatars from videos.
We show quantitatively and qualitatively that our method improves geometry and
covers a more complete expression space compared to state-of-the-art methods.
Code, video, and data can be found at
https://ait.ethz.ch/projects/2022/IMavatar/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing the Deep: Finding Class Specific Filters in Deep CNNs. (arXiv:2112.07719v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07719">
<div class="article-summary-box-inner">
<span><p>Interpretability of Deep Neural Networks has become a major area of
exploration. Although these networks have achieved state of the art accuracy in
many tasks, it is extremely difficult to interpret and explain their decisions.
In this work we analyze the final and penultimate layers of Deep Convolutional
Networks and provide an efficient method for identifying subsets of features
that contribute most towards the network's decision for a class. We demonstrate
that the number of such features per class is much lower in comparison to the
dimension of the final layer and therefore the decision surface of Deep CNNs
lies on a low dimensional manifold and is proportional to the network depth.
Our methods allow to decompose the final layer into separate subspaces which is
far more interpretable and has a lower computational cost as compared to the
final layer of the full network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Pursuit: Building a Space of Objects via Discriminative Weight Generation. (arXiv:2112.07954v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07954">
<div class="article-summary-box-inner">
<span><p>We propose a framework to continuously learn object-centric representations
for visual learning and understanding. Existing object-centric representations
either rely on supervisions that individualize objects in the scene, or perform
unsupervised disentanglement that can hardly deal with complex scenes in the
real world. To mitigate the annotation burden and relax the constraints on the
statistical complexity of the data, our method leverages interactions to
effectively sample diverse variations of an object and the corresponding
training signals while learning the object-centric representations. Throughout
learning, objects are streamed one by one in random order with unknown
identities, and are associated with latent codes that can synthesize
discriminative weights for each object through a convolutional hypernetwork.
Moreover, re-identification of learned objects and forgetting prevention are
employed to make the learning process efficient and robust. We perform an
extensive study of the key features of the proposed framework and analyze the
characteristics of the learned representations. Furthermore, we demonstrate the
capability of the proposed framework in learning representations that can
improve label efficiency in downstream tasks. Our code and trained models are
made publicly available at: https://github.com/pptrick/Object-Pursuit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnweaveNet: Unweaving Activity Stories. (arXiv:2112.10194v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10194">
<div class="article-summary-box-inner">
<span><p>Our lives can be seen as a complex weaving of activities; we switch from one
activity to another, to maximise our achievements or in reaction to demands
placed upon us. Observing a video of unscripted daily activities, we parse the
video into its constituent activity threads through a process we call
unweaving. To accomplish this, we introduce a video representation explicitly
capturing activity threads called a thread bank, along with a neural controller
capable of detecting goal changes and resuming of past activities, together
forming UnweaveNet. We train and evaluate UnweaveNet on sequences from the
unscripted egocentric dataset EPIC-KITCHENS. We propose and showcase the
efficacy of pretraining UnweaveNet in a self-supervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis. (arXiv:2201.01683v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01683">
<div class="article-summary-box-inner">
<span><p>We propose a new method for reconstructing controllable implicit 3D human
models from sparse multi-view RGB videos. Our method defines the neural scene
representation on the mesh surface points and signed distances from the surface
of a human body mesh. We identify an indistinguishability issue that arises
when a point in 3D space is mapped to its nearest surface point on a mesh for
learning surface-aligned neural scene representation. To address this issue, we
propose projecting a point onto a mesh surface using a barycentric
interpolation with modified vertex normals. Experiments with the ZJU-MoCap and
Human3.6M datasets show that our approach achieves a higher quality in a
novel-view and novel-pose synthesis than existing methods. We also demonstrate
that our method easily supports the control of body shape and clothes. Project
page: https://pfnet-research.github.io/surface-aligned-nerf/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02409">
<div class="article-summary-box-inner">
<span><p>Synthetic Aperture Radar (SAR) images are a valuable asset for a wide variety
of tasks. In the last few years, many websites have been offering them for free
in the form of easy to manage products, favoring their widespread diffusion and
research work in the SAR field. The drawback of these opportunities is that
such images might be exposed to forgeries and manipulations by malicious users,
raising new concerns about their integrity and trustworthiness. Up to now, the
multimedia forensics literature has proposed various techniques to localize
manipulations in natural photographs, but the integrity assessment of SAR
images was never investigated. This task poses new challenges, since SAR images
are generated with a processing chain completely different from that of natural
photographs. This implies that many forensics methods developed for natural
images are not guaranteed to succeed. In this paper, we investigate the problem
of amplitude SAR imagery splicing localization. Our goal is to localize regions
of an amplitude SAR image that have been copied and pasted from another image,
possibly undergoing some kind of editing in the process. To do so, we leverage
a Convolutional Neural Network (CNN) to extract a fingerprint highlighting
inconsistencies in the processing traces of the analyzed input. Then, we
examine this fingerprint to produce a binary tampering mask indicating the
pixel region under splicing attack. Results show that our proposed method,
tailored to the nature of SAR signals, provides better performances than
state-of-the-art forensic tools developed for natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relieving Long-tailed Instance Segmentation via Pairwise Class Balance. (arXiv:2201.02784v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02784">
<div class="article-summary-box-inner">
<span><p>Long-tailed instance segmentation is a challenging task due to the extreme
imbalance of training samples among classes. It causes severe biases of the
head classes (with majority samples) against the tailed ones. This renders "how
to appropriately define and alleviate the bias" one of the most important
issues. Prior works mainly use label distribution or mean score information to
indicate a coarse-grained bias. In this paper, we explore to excavate the
confusion matrix, which carries the fine-grained misclassification details, to
relieve the pairwise biases, generalizing the coarse one. To this end, we
propose a novel Pairwise Class Balance (PCB) method, built upon a confusion
matrix which is updated during training to accumulate the ongoing prediction
preferences. PCB generates fightback soft labels for regularization during
training. Besides, an iterative learning paradigm is developed to support a
progressive and smooth regularization in such debiasing. PCB can be plugged and
played to any existing method as a complement. Experimental results on LVIS
demonstrate that our method achieves state-of-the-art performance without bells
and whistles. Superior results across various architectures show the
generalization ability. The code and trained models are available at
https://github.com/megvii-research/PCB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAXIM: Multi-Axis MLP for Image Processing. (arXiv:2201.02973v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02973">
<div class="article-summary-box-inner">
<span><p>Recent progress on Transformers and multi-layer perceptron (MLP) models
provide new network architectural designs for computer vision tasks. Although
these models proved to be effective in many vision tasks such as image
recognition, there remain challenges in adapting them for low-level vision. The
inflexibility to support high-resolution images and limitations of local
attention are perhaps the main bottlenecks. In this work, we present a
multi-axis MLP based architecture called MAXIM, that can serve as an efficient
and flexible general-purpose vision backbone for image processing tasks. MAXIM
uses a UNet-shaped hierarchical structure and supports long-range interactions
enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based
building blocks: a multi-axis gated MLP that allows for efficient and scalable
spatial mixing of local and global visual cues, and a cross-gating block, an
alternative to cross-attention, which accounts for cross-feature conditioning.
Both these modules are exclusively based on MLPs, but also benefit from being
both global and `fully-convolutional', two properties that are desirable for
image processing. Our extensive experimental results show that the proposed
MAXIM model achieves state-of-the-art performance on more than ten benchmarks
across a range of image processing tasks, including denoising, deblurring,
deraining, dehazing, and enhancement while requiring fewer or comparable
numbers of parameters and FLOPs than competitive models. The source code and
trained models will be available at
\url{https://github.com/google-research/maxim}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-driven Semantic Segmentation. (arXiv:2201.03546v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03546">
<div class="article-summary-box-inner">
<span><p>We present LSeg, a novel model for language-driven semantic image
segmentation. LSeg uses a text encoder to compute embeddings of descriptive
input labels (e.g., "grass" or "building") together with a transformer-based
image encoder that computes dense per-pixel embeddings of the input image. The
image encoder is trained with a contrastive objective to align pixel embeddings
to the text embedding of the corresponding semantic class. The text embeddings
provide a flexible label representation in which semantically similar labels
map to similar regions in the embedding space (e.g., "cat" and "furry"). This
allows LSeg to generalize to previously unseen categories at test time, without
retraining or even requiring a single additional training sample. We
demonstrate that our approach achieves highly competitive zero-shot performance
compared to existing zero- and few-shot semantic segmentation methods, and even
matches the accuracy of traditional segmentation algorithms when a fixed label
set is provided. Code and demo are available at
https://github.com/isl-org/lang-seg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03891">
<div class="article-summary-box-inner">
<span><p>Among the different modalities to assess emotion, electroencephalogram (EEG),
representing the electrical brain activity, achieved motivating results over
the last decade. Emotion estimation from EEG could help in the diagnosis or
rehabilitation of certain diseases. In this paper, we propose a dual model
considering two different representations of EEG feature maps: 1) a sequential
based representation of EEG band power, 2) an image-based representation of the
feature vectors. We also propose an innovative method to combine the
information based on a saliency analysis of the image-based model to promote
joint learning of both model parts. The model has been evaluated on four
publicly available datasets: SEED-IV, SEED, DEAP and MPED. The achieved results
outperform results from state-of-the-art approaches for three of the proposed
datasets with a lower standard deviation that reflects higher stability. For
sake of reproducibility, the codes and models proposed in this paper are
available at https://github.com/VDelv/Emotion-EEG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Captcha Attack: Turning Captchas Against Humanity. (arXiv:2201.04014v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04014">
<div class="article-summary-box-inner">
<span><p>Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
</p>
<p>In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-free Online Test-time Adaptation. (arXiv:2201.05718v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05718">
<div class="article-summary-box-inner">
<span><p>Training state-of-the-art vision models has become prohibitively expensive
for researchers and practitioners. For the sake of accessibility and resource
reuse, it is important to focus on adapting these models to a variety of
downstream scenarios. An interesting and practical paradigm is online test-time
adaptation, according to which training data is inaccessible, no labelled data
from the test distribution is available, and adaptation can only happen at test
time and on a handful of samples. In this paper, we investigate how test-time
adaptation methods fare for a number of pre-trained models on a variety of
real-world scenarios, significantly extending the way they have been originally
evaluated. We show that they perform well only in narrowly-defined experimental
setups and sometimes fail catastrophically when their hyperparameters are not
selected for the same scenario in which they are being tested. Motivated by the
inherent uncertainty around the conditions that will ultimately be encountered
at test time, we propose a particularly "conservative" approach, which
addresses the problem with a Laplacian Adjusted Maximum-likelihood Estimation
(LAME) objective. By adapting the model's output (not its parameters), and
solving our objective with an efficient concave-convex procedure, our approach
exhibits a much higher average accuracy across scenarios than existing methods,
while being notably faster and have a much lower memory footprint. The code is
available at https://github.com/fiveai/LAME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Weakly Supervised Pre-Training of Visual Perception Models. (arXiv:2201.08371v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08371">
<div class="article-summary-box-inner">
<span><p>Model pre-training is a cornerstone of modern visual recognition systems.
Although fully supervised pre-training on datasets like ImageNet is still the
de-facto standard, recent studies suggest that large-scale weakly supervised
pre-training can outperform fully supervised approaches. This paper revisits
weakly-supervised pre-training of models using hashtag supervision with modern
versions of residual networks and the largest-ever dataset of images and
corresponding hashtags. We study the performance of the resulting models in
various transfer-learning settings including zero-shot transfer. We also
compare our models with those obtained via large-scale self-supervised
learning. We find our weakly-supervised models to be very competitive across
all settings, and find they substantially outperform their self-supervised
counterparts. We also include an investigation into whether our models learned
potentially troubling associations or stereotypes. Overall, our results provide
a compelling argument for the use of weakly supervised learning in the
development of visual recognition systems. Our models, Supervised Weakly
through hashtAGs (SWAG), are available publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Pixel Trajectories with Multiscale Contrastive Random Walks. (arXiv:2201.08379v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08379">
<div class="article-summary-box-inner">
<span><p>A range of video modeling tasks, from optical flow to multiple object
tracking, share the same fundamental challenge: establishing space-time
correspondence. Yet, approaches that dominate each space differ. We take a step
towards bridging this gap by extending the recent contrastive random walk
formulation to much denser, pixel-level space-time graphs. The main
contribution is introducing hierarchy into the search problem by computing the
transition matrix between two frames in a coarse-to-fine manner, forming a
multiscale contrastive random walk when extended in time. This establishes a
unified technique for self-supervised learning of optical flow, keypoint
tracking, and video object segmentation. Experiments demonstrate that, for each
of these tasks, the unified model achieves performance competitive with strong
self-supervised approaches specific to that task. Project webpage:
https://jasonbian97.github.io/flowwalk
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast MRI Reconstruction: How Powerful Transformers Are?. (arXiv:2201.09400v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09400">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) is a widely used non-radiative and
non-invasive method for clinical interrogation of organ structures and
metabolism, with an inherently long scanning time. Methods by k-space
undersampling and deep learning based reconstruction have been popularised to
accelerate the scanning process. This work focuses on investigating how
powerful transformers are for fast MRI by exploiting and comparing different
novel network architectures. In particular, a generative adversarial network
(GAN) based Swin transformer (ST-GAN) was introduced for the fast MRI
reconstruction. To further preserve the edge and texture information, edge
enhanced GAN based Swin transformer (EES-GAN) and texture enhanced GAN based
Swin transformer (TES-GAN) were also developed, where a dual-discriminator GAN
structure was applied. We compared our proposed GAN based transformers,
standalone Swin transformer and other convolutional neural networks based GAN
model in terms of the evaluation metrics PSNR, SSIM and FID. We showed that
transformers work well for the MRI reconstruction from different undersampling
conditions. The utilisation of GAN's adversarial structure improves the quality
of images reconstructed when undersampled for 30% or higher. The code is
publicly available at https://github.com/ayanglab/SwinGANMR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09671">
<div class="article-summary-box-inner">
<span><p>Since severe droughts are occurring more frequently and lengthening the dry
season in the Amazon Rainforest, it is important to detect wildfires promptly
and forecast possible spread for effective suppression response. Though
computer vision researchers have applied algorithms to automatically detect
wildfires, current models are computationally expensive and not versatile
enough for the low technology conditions of South American wildfire hotspots.
This comprehensive deep learning study first trains a Fully Convolutional
Neural Network with skip connections on multispectral Landsat 8 images of
Ecuador and the Galapagos. The model uses Green and Short-wave Infrared (SWIR)
bands as inputs to predict each image's corresponding pixel-level binary fire
mask. This model achieves a 0.962 validation F2 score and a 0.932 F2 score on
test data from Guyana and Suriname. Afterward, image segmentation is conducted
on the Cirrus band using K-Means Clustering to simplify continuous pixel values
into three discrete classes representing differing degrees of cirrus cloud
contamination. Two additional Convolutional Neural Networks are trained to
classify the presence of a wildfire using these segmented cirrus images. The
"experimental" model trained on the segmented inputs and SWIR data achieves a
binary accuracy that is 2.306% higher than that of the "benchmark model" that
is trained only on SWIR data. The difference in performance has a p-value of
0.00968. This proof of concept reveals that feature simplification can improve
the performance of wildfire detection models. Overall, the software built in
this study is useful for early and accurate detection of wildfires in South
America.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Approach for Digital Color Reconstruction of Lenticular Films. (arXiv:2202.05270v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05270">
<div class="article-summary-box-inner">
<span><p>We propose the first accurate digitization and color reconstruction process
for historical lenticular film that is robust to artifacts. Lenticular films
emerged in the 1920s and were one of the first technologies that permitted to
capture full color information in motion. The technology leverages an RGB
filter and cylindrical lenticules embossed on the film surface to encode the
color in the horizontal spatial dimension of the image. To project the pictures
the encoding process was reversed using an appropriate analog device. In this
work, we introduce an automated, fully digital pipeline to process the scan of
lenticular films and colorize the image. Our method merges deep learning with a
model-based approach in order to maximize the performance while making sure
that the reconstructed colored images truthfully match the encoded color
information. Our model employs different strategies to achieve an effective
color reconstruction, in particular (i) we use data augmentation to create a
robust lenticule segmentation network, (ii) we fit the lenticules raster
prediction to obtain a precise vectorial lenticule localization, and (iii) we
train a colorization network that predicts interpolation coefficients in order
to obtain a truthful colorization. We validate the proposed method on a
lenticular film dataset and compare it to other approaches. Since no colored
groundtruth is available as reference, we conduct a user study to validate our
method in a subjective manner. The results of the study show that the proposed
method is largely preferred with respect to other existing and baseline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron. (arXiv:2202.06372v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06372">
<div class="article-summary-box-inner">
<span><p>The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing
threat to humans worldwide, creating a health crisis that infected millions of
lives, as well as devastating the global economy. Deep learning (DL) techniques
have proved helpful in analysis and delineation of infectious regions in
radiological images in a timely manner. This paper makes an in-depth survey of
DL techniques and draws a taxonomy based on diagnostic strategies and learning
approaches. DL techniques are systematically categorized into classification,
segmentation, and multi-stage approaches for COVID-19 diagnosis at image and
region level analysis. Each category includes pre-trained and custom-made
Convolutional Neural Network architectures for detecting COVID-19 infection in
radiographic imaging modalities; X-Ray, and Computer Tomography (CT).
Furthermore, a discussion is made on challenges in developing diagnostic
techniques such as cross-platform interoperability and examining imaging
modality. Similarly, a review of the various methodologies and performance
measures used in these techniques is also presented. This survey provides an
insight into the promising areas of research in DL for analyzing radiographic
images, and further accelerates the research in designing customized DL based
diagnostic tools for effectively dealing with new variants of COVID-19 and
emerging challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08916">
<div class="article-summary-box-inner">
<span><p>Image-based characterization and disease understanding involve integrative
analysis of morphological, spatial, and topological information across
biological scales. The development of graph convolutional networks (GCNs) has
created the opportunity to address this information complexity via graph-driven
architectures, since GCNs can perform feature aggregation, interaction, and
reasoning with remarkable flexibility and efficiency. These GCNs capabilities
have spawned a new wave of research in medical imaging analysis with the
overarching goal of improving quantitative disease understanding, monitoring,
and diagnosis. Yet daunting challenges remain for designing the important
image-to-graph transformation for multi-modality medical imaging and gaining
insights into model interpretation and enhanced clinical decision support. In
this review, we present recent GCNs developments in the context of medical
image analysis including imaging data from radiology and histopathology. We
discuss the fast-growing use of graph network architectures in medical image
analysis to improve disease diagnosis and patient outcomes in clinical
practice. To foster cross-disciplinary research, we present GCNs technical
advancements, emerging medical applications, identify common challenges in the
use of image-based GCNs and their extensions in model interpretation,
large-scale benchmarks that promise to transform the scope of medical image
studies and related graph-driven medical research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00734">
<div class="article-summary-box-inner">
<span><p>Automatic player identification is an essential and complex task in sports
video analysis. Different strategies have been devised over the years, but
identification based on jersey numbers is one of the most common approaches
given its versatility and relative simplicity. However, automatic detection of
jersey numbers is still challenging due to changing camera angles, low video
resolution, small object size in wide-range shots and transient changes in the
player's posture and movement. In this paper we present a novel approach for
jersey number identification in a small, highly imbalanced dataset from the
Seattle Seahawks practice videos. Our results indicate that simple models can
achieve an acceptable performance on the jersey number detection task and that
synthetic data can improve the performance dramatically (accuracy increase of
~9% overall, ~18% on low frequency numbers) making our approach achieve state
of the art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01441">
<div class="article-summary-box-inner">
<span><p>We introduce a set of image transformations that can be used as corruptions
to evaluate the robustness of models as well as data augmentation mechanisms
for training neural networks. The primary distinction of the proposed
transformations is that, unlike existing approaches such as Common Corruptions,
the geometry of the scene is incorporated in the transformations -- thus
leading to corruptions that are more likely to occur in the real world. We also
introduce a set of semantic corruptions (e.g. natural object occlusions). We
show these transformations are `efficient' (can be computed on-the-fly),
`extendable' (can be applied on most image datasets), expose vulnerability of
existing models, and can effectively make models more robust when employed as
`3D data augmentation' mechanisms. The evaluations on several tasks and
datasets suggest incorporating 3D information into benchmarking and training
opens up a promising direction for robustness research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugHover-Net: Augmenting Hover-net for Nucleus Segmentation and Classification. (arXiv:2203.03415v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03415">
<div class="article-summary-box-inner">
<span><p>Nuclei segmentation and classification have been a challenge in digital
pathology due to the specific domain characteristics. First, annotating a
large-scale dataset is quite consuming. It requires specific domain knowledge
and large efforts. Second, some nuclei are clustered together and hard to
segment from each other. Third, the classes are often extremely unbalanced. As
in Lizard, the number of epithelial nuclei is around 67 times larger than the
number of eosinophil nuclei. Fourth, the nuclei often exhibit high inter-class
similarity and intra-class variability. Connective nuclei may look very
different from each other while some of them share a similar shape with the
epithelial ones. Last but not least, pathological patches may have very
different color distributions among different datasets. Thus, a large-scale
generally annotated dataset and a specially-designed algorithm are needed to
solve this problem. The CoNIC challenge aims to promote the automatic
segmentation and classification task and requires researchers to develop
algorithms that perform segmentation, classification, and counting of 6
different types of nuclei with the large-scale annotated dataset: Lizard. Due
to the 60-minute time limit, the algorithm has to be simple and quick. In this
paper, we briefly describe the final method we used in the CoNIC challenge. Our
algorithm is based on Hover-Net and we added several modifications to it to
improve its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language. (arXiv:2203.03598v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03598">
<div class="article-summary-box-inner">
<span><p>Learning to classify video data from classes not included in the training
data, i.e. video-based zero-shot learning, is challenging. We conjecture that
the natural alignment between the audio and visual modalities in video data
provides a rich training signal for learning discriminative multi-modal
representations. Focusing on the relatively underexplored task of audio-visual
zero-shot learning, we propose to learn multi-modal representations from
audio-visual data using cross-modal attention and exploit textual label
embeddings for transferring knowledge from seen classes to unseen classes.
Taking this one step further, in our generalised audio-visual zero-shot
learning setting, we include all the training classes in the test-time search
space which act as distractors and increase the difficulty while making the
setting more realistic. Due to the lack of a unified benchmark in this domain,
we introduce a (generalised) zero-shot learning benchmark on three audio-visual
datasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet,
ensuring that the unseen test classes do not appear in the dataset used for
supervised training of the backbone deep models. Comparing multiple relevant
and recent methods, we demonstrate that our proposed AVCA model achieves
state-of-the-art performance on all three datasets. Code and data are available
at \url{https://github.com/ExplainableML/AVCA-GZSL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Distinctive Margin toward Active Domain Adaptation. (arXiv:2203.05738v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05738">
<div class="article-summary-box-inner">
<span><p>Despite plenty of efforts focusing on improving the domain adaptation ability
(DA) under unsupervised or few-shot semi-supervised settings, recently the
solution of active learning started to attract more attention due to its
suitability in transferring model in a more practical way with limited
annotation resource on target data. Nevertheless, most active learning methods
are not inherently designed to handle domain gap between data distribution, on
the other hand, some active domain adaptation methods (ADA) usually requires
complicated query functions, which is vulnerable to overfitting. In this work,
we propose a concise but effective ADA method called
Select-by-Distinctive-Margin (SDM), which consists of a maximum margin loss and
a margin sampling algorithm for data selection. We provide theoretical analysis
to show that SDM works like a Support Vector Machine, storing hard examples
around decision boundaries and exploiting them to find informative and
transferable data. In addition, we propose two variants of our method, one is
designed to adaptively adjust the gradient from margin loss, the other boosts
the selectivity of margin sampling by taking the gradient direction into
account. We benchmark SDM with standard active learning setting, demonstrating
our algorithm achieves competitive results with good data scalability. Code is
available at https://github.com/TencentYoutuResearch/ActiveLearning-SDM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs. (arXiv:2203.06717v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06717">
<div class="article-summary-box-inner">
<span><p>We revisit large kernel design in modern convolutional neural networks
(CNNs). Inspired by recent advances in vision transformers (ViTs), in this
paper, we demonstrate that using a few large convolutional kernels instead of a
stack of small kernels could be a more powerful paradigm. We suggested five
guidelines, e.g., applying re-parameterized large depth-wise convolutions, to
design efficient high-performance large-kernel CNNs. Following the guidelines,
we propose RepLKNet, a pure CNN architecture whose kernel size is as large as
31x31, in contrast to commonly used 3x3. RepLKNet greatly closes the
performance gap between CNNs and ViTs, e.g., achieving comparable or superior
results than Swin Transformer on ImageNet and a few typical downstream tasks,
with lower latency. RepLKNet also shows nice scalability to big data and large
models, obtaining 87.8% top-1 accuracy on ImageNet and 56.0% mIoU on ADE20K,
which is very competitive among the state-of-the-arts with similar model sizes.
Our study further reveals that, in contrast to small-kernel CNNs, large-kernel
CNNs have much larger effective receptive fields and higher shape bias rather
than texture bias. Code &amp; models at
https://github.com/megvii-research/RepLKNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Minimal Sufficient Representation in Contrastive Learning. (arXiv:2203.07004v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07004">
<div class="article-summary-box-inner">
<span><p>Contrastive learning between different views of the data achieves outstanding
success in the field of self-supervised representation learning and the learned
representations are useful in broad downstream tasks. Since all supervision
information for one view comes from the other view, contrastive learning
approximately obtains the minimal sufficient representation which contains the
shared information and eliminates the non-shared information between views.
Considering the diversity of the downstream tasks, it cannot be guaranteed that
all task-relevant information is shared between views. Therefore, we assume the
non-shared task-relevant information cannot be ignored and theoretically prove
that the minimal sufficient representation in contrastive learning is not
sufficient for the downstream tasks, which causes performance degradation. This
reveals a new problem that the contrastive learning models have the risk of
over-fitting to the shared information between views. To alleviate this
problem, we propose to increase the mutual information between the
representation and input as regularization to approximately introduce more
task-relevant information, since we cannot utilize any downstream task
information during training. Extensive experiments verify the rationality of
our analysis and the effectiveness of our method. It significantly improves the
performance of several classic contrastive learning models in downstream tasks.
Our code is available at https://github.com/Haoqing-Wang/InfoCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation. (arXiv:2203.09811v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09811">
<div class="article-summary-box-inner">
<span><p>Scene Graph Generation, which generally follows a regular encoder-decoder
pipeline, aims to first encode the visual contents within the given image and
then parse them into a compact summary graph. Existing SGG approaches generally
not only neglect the insufficient modality fusion between vision and language,
but also fail to provide informative predicates due to the biased relationship
predictions, leading SGG far from practical. Towards this end, in this paper,
we first present a novel Stacked Hybrid-Attention network, which facilitates
the intra-modal refinement as well as the inter-modal interaction, to serve as
the encoder. We then devise an innovative Group Collaborative Learning strategy
to optimize the decoder. Particularly, based upon the observation that the
recognition capability of one classifier is limited towards an extremely
unbalanced dataset, we first deploy a group of classifiers that are expert in
distinguishing different subsets of classes, and then cooperatively optimize
them from two aspects to promote the unbiased SGG. Experiments conducted on VG
and GQA datasets demonstrate that, we not only establish a new state-of-the-art
in the unbiased metric, but also nearly double the performance compared with
two baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10196">
<div class="article-summary-box-inner">
<span><p>We propose MisMatch, a novel consistency-driven semi-supervised segmentation
framework which produces predictions that are invariant to learnt feature
perturbations. MisMatch consists of an encoder and a two-head decoders. One
decoder learns positive attention to the foreground regions of interest (RoI)
on unlabelled images thereby generating dilated features. The other decoder
learns negative attention to the foreground on the same unlabelled images
thereby generating eroded features. We then apply a consistency regularisation
on the paired predictions. MisMatch outperforms state-of-the-art
semi-supervised methods on a CT-based pulmonary vessel segmentation task and a
MRI-based brain tumour segmentation task. In addition, we show that the
effectiveness of MisMatch comes from better model calibration than its
supervised learning counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis. (arXiv:2203.10977v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10977">
<div class="article-summary-box-inner">
<span><p>Anatomical segmentation is a fundamental task in medical image computing,
generally tackled with fully convolutional neural networks which produce dense
segmentation masks. These models are often trained with loss functions such as
cross-entropy or Dice, which assume pixels to be independent of each other,
thus ignoring topological errors and anatomical inconsistencies. We address
this limitation by moving from pixel-level to graph representations, which
allow to naturally incorporate anatomical constraints by construction. To this
end, we introduce HybridGNet, an encoder-decoder neural architecture that
leverages standard convolutions for image feature encoding and graph
convolutional neural networks (GCNNs) to decode plausible representations of
anatomical structures. We also propose a novel image-to-graph skip connection
layer which allows localized features to flow from standard convolutional
blocks to GCNN blocks, and show that it improves segmentation accuracy. The
proposed architecture is extensively evaluated in a variety of domain shift and
image occlusion scenarios, and audited considering different types of
demographic domain shift. Our comprehensive experimental setup compares
HybridGNet with other landmark and pixel-based models for anatomical
segmentation in chest x-ray images, and shows that it produces anatomically
plausible results in challenging scenarios where other models tend to fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11692">
<div class="article-summary-box-inner">
<span><p>This manuscript describes the panoptic segmentation method we devised for our
submission to the CONIC challenge at ISBI 2022. Key features of our method are
a weighted loss that we specifically engineered for semantic segmentation of
highly imbalanced cell types, and an existing state-of-the art nuclei instance
segmentation model, which we combine in a Hovernet-like architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning. (arXiv:2203.11933v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11933">
<div class="article-summary-box-inner">
<span><p>Vision-language models can encode societal biases and stereotypes, but there
are challenges to measuring and mitigating these harms. Prior proposed bias
measurements lack robustness and feature degradation occurs when mitigating
bias without access to pretraining data. We address both of these challenges in
this paper: First, we evaluate different bias measures and propose the use of
retrieval metrics to image-text representations via a bias measuring framework.
Second, we investigate debiasing methods and show that optimizing for
adversarial loss via learnable token embeddings minimizes various bias measures
without substantially degrading feature representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repairing Group-Level Errors for DNNs Using Weighted Regularization. (arXiv:2203.13612v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13612">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) have been widely used in software making
decisions impacting people's lives. However, they have been found to exhibit
severe erroneous behaviors that may lead to unfortunate outcomes. Previous work
shows that such misbehaviors often occur due to class property violations
rather than errors on a single image. Although methods for detecting such
errors have been proposed, fixing them has not been studied so far. Here, we
propose a generic method called Weighted Regularization (WR) consisting of five
concrete methods targeting the error-producing classes to fix the DNNs. In
particular, it can repair confusion error and bias error of DNN models for both
single-label and multi-label image classifications. A confusion error happens
when a given DNN model tends to confuse between two classes. Each method in WR
assigns more weights at a stage of DNN retraining or inference to mitigate the
confusion between target pair. A bias error can be fixed similarly. We evaluate
and compare the proposed methods along with baselines on six widely-used
datasets and architecture combinations. The results suggest that WR methods
have different trade-offs but under each setting at least one WR method can
greatly reduce confusion/bias errors at a very limited cost of the overall
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13883">
<div class="article-summary-box-inner">
<span><p>As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
changing accordingly. Taking advantage of the fact that visual modalities such
as images and videos are more favorable and attractive to the users, and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual correlations between modalities e.g., text
and image. Thus, many research efforts have been put into development of
automatic techniques for detecting possible cross-modal discordances in
web-based media. In this work, we aim to analyze, categorize and identify
existing approaches in addition to challenges and shortcomings they face in
order to unearth new opportunities in furthering the research in the field of
multi-modal misinformation detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning with Position-Aware Neurons. (arXiv:2203.14666v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14666">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) fuses collaborative models from local nodes without
centralizing users' data. The permutation invariance property of neural
networks and the non-i.i.d. data across clients make the locally updated
parameters imprecisely aligned, disabling the coordinate-based parameter
averaging. Traditional neurons do not explicitly consider position information.
Hence, we propose Position-Aware Neurons (PANs) as an alternative, fusing
position-related values (i.e., position encodings) into neuron outputs. PANs
couple themselves to their positions and minimize the possibility of
dislocation, even updating on heterogeneous data. We turn on/off PANs to
disable/enable the permutation invariance property of neural networks. PANs are
tightly coupled with positions when applied to FL, making parameters across
clients pre-aligned and facilitating coordinate-based parameter averaging. PANs
are algorithm-agnostic and could universally improve existing FL algorithms.
Furthermore, "FL with PANs" is simple to implement and computationally
friendly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition. (arXiv:2203.14779v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14779">
<div class="article-summary-box-inner">
<span><p>Multimodal emotion recognition has recently gained much attention since it
can leverage diverse and complementary relationships over multiple modalities
(e.g., audio, visual, biosignals, etc.), and can provide some robustness to
noisy modalities. Most state-of-the-art methods for audio-visual (A-V) fusion
rely on recurrent networks or conventional attention mechanisms that do not
effectively leverage the complementary nature of A-V modalities. In this paper,
we focus on dimensional emotion recognition based on the fusion of facial and
vocal modalities extracted from videos. Specifically, we propose a joint
cross-attention model that relies on the complementary relationships to extract
the salient features across A-V modalities, allowing for accurate prediction of
continuous values of valence and arousal. The proposed fusion model efficiently
leverages the inter-modal relationships, while reducing the heterogeneity
between the features. In particular, it computes the cross-attention weights
based on correlation between the combined feature representation and individual
modalities. By deploying the combined A-V feature representation into the
cross-attention module, the performance of our fusion module improves
significantly over the vanilla cross-attention module. Experimental results on
validation-set videos from the AffWild2 dataset indicate that our proposed A-V
fusion model provides a cost-effective solution that can outperform
state-of-the-art approaches. The code is available on GitHub:
https://github.com/praveena2j/JointCrossAttentional-AV-Fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15102">
<div class="article-summary-box-inner">
<span><p>Prevalent semantic segmentation solutions, despite their different network
designs (FCN based or attention based) and mask decoding strategies (parametric
softmax based or pixel-query based), can be placed in one category, by
considering the softmax weights or query vectors as learnable class prototypes.
In light of this prototype view, this study uncovers several limitations of
such parametric segmentation regime, and proposes a nonparametric alternative
based on non-learnable prototypes. Instead of prior methods learning a single
weight/query vector for each class in a fully parametric manner, our model
represents each class as a set of non-learnable prototypes, relying solely on
the mean features of several training pixels within that class. The dense
prediction is thus achieved by nonparametric nearest prototype retrieving. This
allows our model to directly shape the pixel embedding space, by optimizing the
arrangement between embedded pixels and anchored prototypes. It is able to
handle arbitrary number of classes with a constant amount of learnable
parameters. We empirically show that, with FCN based and attention based
segmentation models (i.e., HRNet, Swin, SegFormer) and backbones (i.e., ResNet,
HRNet, Swin, MiT), our nonparametric framework yields compelling results over
several datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in
the large-vocabulary situation. We expect this work will provoke a rethink of
the current de facto semantic segmentation model design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification. (arXiv:2203.15210v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15210">
<div class="article-summary-box-inner">
<span><p>To learn camera-view invariant features for person Re-IDentification (Re-ID),
the cross-camera image pairs of each person play an important role. However,
such cross-view training samples could be unavailable under the ISolated Camera
Supervised (ISCS) setting, e.g., a surveillance system deployed across distant
scenes. To handle this challenging problem, a new pipeline is introduced by
synthesizing the cross-camera samples in the feature space for model training.
Specifically, the feature encoder and generator are end-to-end optimized under
a novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint
learning procedure raises concern on the stability of generative model
training. Therefore, a new feature generator, $\sigma$-Regularized Conditional
Variational Autoencoder ($\sigma$-Reg.~CVAE), is proposed with theoretical and
experimental analysis on its robustness. Extensive experiments on two ISCS
person Re-ID datasets demonstrate the superiority of our CCSFG to the
competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation. (arXiv:2203.15227v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15227">
<div class="article-summary-box-inner">
<span><p>Multi-frame human pose estimation has long been a compelling and fundamental
problem in computer vision. This task is challenging due to fast motion and
pose occlusion that frequently occur in videos. State-of-the-art methods strive
to incorporate additional visual evidences from neighboring frames (supporting
frames) to facilitate the pose estimation of the current frame (key frame). One
aspect that has been obviated so far, is the fact that current methods directly
aggregate unaligned contexts across frames. The spatial-misalignment between
pose features of the current frame and neighboring frames might lead to
unsatisfactory results. More importantly, existing approaches build upon the
straightforward pose estimation loss, which unfortunately cannot constrain the
network to fully leverage useful information from neighboring frames. To tackle
these problems, we present a novel hierarchical alignment framework, which
leverages coarse-to-fine deformations to progressively update a neighboring
frame to align with the current frame at the feature level. We further propose
to explicitly supervise the knowledge extraction from neighboring frames,
guaranteeing that useful complementary cues are extracted. To achieve this
goal, we theoretically analyzed the mutual information between the frames and
arrived at a loss that maximizes the task-relevant mutual information. These
allow us to rank No.1 in the Multi-frame Person Pose Estimation Challenge on
benchmark dataset PoseTrack2017, and obtain state-of-the-art performance on
benchmarks Sub-JHMDB and Pose-Track2018. Our code is released at
https://github. com/Pose-Group/FAMI-Pose, hoping that it will be useful to the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SepViT: Separable Vision Transformer. (arXiv:2203.15380v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15380">
<div class="article-summary-box-inner">
<span><p>Vision Transformers have witnessed prevailing success in a series of vision
tasks. However, they often require enormous amount of computations to achieve
high performance, which is burdensome to deploy on resource-constrained
devices. To address these issues, we draw lessons from depthwise separable
convolution and imitate its ideology to design the Separable Vision
Transformer, abbreviated as SepViT. SepViT helps to carry out the information
interaction within and among the windows via a depthwise separable
self-attention. The novel window token embedding and grouped self-attention are
employed to model the attention relationship among windows with negligible
computational cost and capture a long-range visual dependencies of multiple
windows, respectively. Extensive experiments on various benchmark tasks
demonstrate SepViT can achieve state-of-the-art results in terms of trade-off
between accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy
on ImageNet-1K classification while decreasing the latency by 40%, compared to
the ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream
vision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic
segmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box
AP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection. (arXiv:2203.15793v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15793">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) is an effective approach to tackle the
issue of domain shift. Specifically, UDA methods try to align the source and
target representations to improve the generalization on the target domain.
Further, UDA methods work under the assumption that the source data is
accessible during the adaptation process. However, in real-world scenarios, the
labelled source data is often restricted due to privacy regulations, data
transmission constraints, or proprietary data concerns. The Source-Free Domain
Adaptation (SFDA) setting aims to alleviate these concerns by adapting a
source-trained model for the target domain without requiring access to the
source data. In this paper, we explore the SFDA setting for the task of
adaptive object detection. To this end, we propose a novel training strategy
for adapting a source-trained object detector to the target domain without
source data. More precisely, we design a novel contrastive loss to enhance the
target representations by exploiting the objects relations for a given target
domain input. These object instance relations are modelled using an Instance
Relation Graph (IRG) network, which are then used to guide the contrastive
representation learning. In addition, we utilize a student-teacher based
knowledge distillation strategy to avoid overfitting to the noisy pseudo-labels
generated by the source-trained model. Extensive experiments on multiple object
detection benchmark datasets show that the proposed approach is able to
efficiently adapt source-trained object detectors to the target domain,
outperforming previous state-of-the-art domain adaptive detection methods. Code
is available at https://github.com/Vibashan/irg-sfda.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-YOLOE: An evolved version of YOLO. (arXiv:2203.16250v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16250">
<div class="article-summary-box-inner">
<span><p>In this report, we present PP-YOLOE, an industrial state-of-the-art object
detector with high performance and friendly deployment. We optimize on the
basis of the previous PP-YOLOv2, using anchor-free paradigm, more powerful
backbone and neck equipped with CSPRepResStage, ET-head and dynamic label
assignment algorithm TAL. We provide s/m/l/x models for different practice
scenarios. As a result, PP-YOLOE-l achieves 51.4 mAP on COCO test-dev and 78.1
FPS on Tesla V100, yielding a remarkable improvement of (+1.9 AP, +13.35% speed
up) and (+1.3 AP, +24.96% speed up), compared to the previous state-of-the-art
industrial models PP-YOLOv2 and YOLOX respectively. Further, PP-YOLOE inference
speed achieves 149.2 FPS with TensorRT and FP16-precision. We also conduct
extensive experiments to verify the effectiveness of our designs. Source code
and pre-trained models are available at
https://github.com/PaddlePaddle/PaddleDetection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Lightweight YOLOv5 Model Based on Attention Mechanism for Face Mask Detection. (arXiv:2203.16506v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16506">
<div class="article-summary-box-inner">
<span><p>Coronavirus 2019 has brought severe challenges to social stability and public
health worldwide. One effective way of curbing the epidemic is to require
people to wear masks in public places and monitor mask-wearing states by
utilizing suitable automatic detectors. However, existing deep learning based
models struggle to simultaneously achieve the requirements of both high
precision and real-time performance. To solve this problem, we propose an
improved lightweight face mask detector based on YOLOv5, which can achieve an
excellent balance of precision and speed. Firstly, a novel backbone
ShuffleCANet that combines ShuffleNetV2 network with Coordinate Attention
mechanism is proposed as the backbone. Afterwards, an efficient path aggression
network BiFPN is applied as the feature fusion neck. Furthermore, the
localization loss is replaced with alpha-CIoU in model training phase to obtain
higher-quality anchors. Some valuable strategies such as data augmentation,
adaptive image scaling, and anchor cluster operation are also utilized.
Experimental results on AIZOO face mask dataset show the superiority of the
proposed model. Compared with the original YOLOv5, the proposed model increases
the inference speed by 28.3% while still improving the precision by 0.58%. It
achieves the best mean average precision of 95.2% compared with other seven
existing models, which is 4.4% higher than the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ball 3D localization from a single calibrated image. (arXiv:2204.00003v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00003">
<div class="article-summary-box-inner">
<span><p>Ball 3D localization in team sports has various applications including
automatic offside detection in soccer, or shot release localization in
basketball. Today, this task is either resolved by using expensive multi-views
setups, or by restricting the analysis to ballistic trajectories. In this work,
we propose to address the task on a single image from a calibrated monocular
camera by estimating ball diameter in pixels and use the knowledge of real ball
diameter in meters. This approach is suitable for any game situation where the
ball is (even partly) visible. To achieve this, we use a small neural network
trained on image patches around candidates generated by a conventional ball
detector. Besides predicting ball diameter, our network outputs the confidence
of having a ball in the image patch. Validations on 3 basketball datasets
reveals that our model gives remarkable predictions on ball 3D localization. In
addition, through its confidence output, our model improves the detection rate
by filtering the candidates produced by the detector. The contributions of this
work are (i) the first model to address 3D ball localization on a single image,
(ii) an effective method for ball 3D annotation from single calibrated images,
(iii) a high quality 3D ball evaluation dataset annotated from a single
viewpoint. In addition, the code to reproduce this research is be made freely
available at https://github.com/gabriel-vanzandycke/deepsport.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection. (arXiv:2204.00325v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00325">
<div class="article-summary-box-inner">
<span><p>In autonomous driving, LiDAR point-clouds and RGB images are two major data
modalities with complementary cues for 3D object detection. However, it is
quite difficult to sufficiently use them, due to large inter-modal
discrepancies. To address this issue, we propose a novel framework, namely
Contrastively Augmented Transformer for multi-modal 3D object Detection
(CAT-Det). Specifically, CAT-Det adopts a two-stream structure consisting of a
Pointformer (PT) branch, an Imageformer (IT) branch along with a Cross-Modal
Transformer (CMT) module. PT, IT and CMT jointly encode intra-modal and
inter-modal long-range contexts for representing an object, thus fully
exploring multi-modal information for detection. Furthermore, we propose an
effective One-way Multi-modal Data Augmentation (OMDA) approach via
hierarchical contrastive learning at both the point and object levels,
significantly improving the accuracy only by augmenting point-clouds, which is
free from complex generation of paired samples of the two modalities. Extensive
experiments on the KITTI benchmark show that CAT-Det achieves a new
state-of-the-art, highlighting its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00559">
<div class="article-summary-box-inner">
<span><p>We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. Existing photometric-based
methods have trouble on scenes with large photometric distortions, e.g. outdoor
environments. By incorporating an exposure-adaptive novel view synthesis, our
methods can successfully address the challenges. Moreover, by introducing
domain-invariant feature matching, our solution can improve pose regression
accuracy while using semi-supervised learning on unlabeled data. In particular,
the pipeline consists of two components, Novel View Synthesizer and FeatureNet
(DFNet). The former synthesizes novel views compensating for changes in
exposure and the latter regresses camera poses and extracts robust features
that bridge the domain gap between real images and synthetic ones. We show that
domain invariant feature matching effectively enhances camera pose estimation
both in indoor and outdoor scenes. Hence, our method achieves a
state-of-the-art accuracy by outperforming existing single-image APR methods by
as much as 56%, comparable to 3D structure-based methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-05 23:08:35.699539250 UTC">2022-04-05 23:08:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>