<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-03T01:30:00Z">05-03</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00001">
<div class="article-summary-box-inner">
<span><p>Having a rich multimodal inner language is an important component of human
intelligence that enables several necessary core cognitive functions such as
multimodal prediction, translation, and generation. Building upon the Conscious
Turing Machine (CTM), a machine model for consciousness as proposed by Blum and
Blum (2021), we describe the desiderata of a multimodal language called
Brainish, comprising words, images, audio, and sensations combined in
representations that the CTM's processors use to communicate with each other.
We define the syntax and semantics of Brainish before operationalizing this
language through the lens of multimodal artificial intelligence, a vibrant
research area studying the computational tools necessary for processing and
relating information from heterogeneous signals. Our general framework for
learning Brainish involves designing (1) unimodal encoders to segment and
represent unimodal data, (2) a coordinated representation space that relates
and composes unimodal features to derive holistic meaning across multimodal
inputs, and (3) decoders to map multimodal representations into predictions
(for fusion) or raw data (for translation or generation). Through discussing
how Brainish is crucial for communication and coordination in order to achieve
consciousness in the CTM, and by implementing a simple version of Brainish and
evaluating its capability of demonstrating intelligence on multimodal
prediction and retrieval tasks on several real-world image, text, and audio
datasets, we argue that such an inner language will be important for advances
in machine models of intelligence and consciousness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Aware Feedback-Based Self-Learning in Large-Scale Conversational AI. (arXiv:2205.00029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00029">
<div class="article-summary-box-inner">
<span><p>Self-learning paradigms in large-scale conversational AI agents tend to
leverage user feedback in bridging between what they say and what they mean.
However, such learning, particularly in Markov-based query rewriting systems
have far from addressed the impact of these models on future training where
successive feedback is inevitably contingent on the rewrite itself, especially
in a continually updating environment. In this paper, we explore the
consequences of this inherent lack of self-awareness towards impairing the
model performance, ultimately resulting in both Type I and II errors over time.
To that end, we propose augmenting the Markov Graph construction with a
superposition-based adjacency matrix. Here, our method leverages an induced
stochasticity to reactively learn a locally-adaptive decision boundary based on
the performance of the individual rewrites in a bi-variate beta setting. We
also surface a data augmentation strategy that leverages template-based
generation in abridging complex conversation hierarchies of dialogs so as to
simplify the learning process. All in all, we demonstrate that our self-aware
model improves the overall PR-AUC by 27.45%, achieves a relative defect
reduction of up to 31.22%, and is able to adapt quicker to changes in global
preferences across a large number of customers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do we Really Know about State of the Art NER?. (arXiv:2205.00034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00034">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answer Consolidation: Formulation and Benchmarking. (arXiv:2205.00042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00042">
<div class="article-summary-box-inner">
<span><p>Current question answering (QA) systems primarily consider the single-answer
scenario, where each question is assumed to be paired with one correct answer.
However, in many real-world QA applications, multiple answer scenarios arise
where consolidating answers into a comprehensive and non-redundant set of
answers is a more efficient user interface. In this paper, we formulate the
problem of answer consolidation, where answers are partitioned into multiple
groups, each representing different aspects of the answer set. Then, given this
partitioning, a comprehensive and non-redundant set of answers can be
constructed by picking one answer from each group. To initiate research on
answer consolidation, we construct a dataset consisting of 4,699 questions and
24,006 sentences and evaluate multiple models. Despite a promising performance
achieved by the best-performing supervised models, we still believe this task
has room for further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logically Consistent Adversarial Attacks for Soft Theorem Provers. (arXiv:2205.00047v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00047">
<div class="article-summary-box-inner">
<span><p>Recent efforts within the AI community have yielded impressive results
towards "soft theorem proving" over natural language sentences using language
models. We propose a novel, generative adversarial framework for probing and
improving these models' reasoning capabilities. Adversarial attacks in this
domain suffer from the logical inconsistency problem, whereby perturbations to
the input may alter the label. Our Logically consistent AdVersarial Attacker,
LAVA, addresses this by combining a structured generative process with a
symbolic solver, guaranteeing logical consistency. Our framework successfully
generates adversarial attacks and identifies global weaknesses common across
multiple target models. Our analyses reveal naive heuristics and
vulnerabilities in these models' reasoning capabilities, exposing an incomplete
grasp of logical deduction under logic programs. Finally, in addition to
effective probing of these models, we show that training on the generated
samples improves the target model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Consistency for Zero-Shot Task Generalization. (arXiv:2205.00049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00049">
<div class="article-summary-box-inner">
<span><p>One of the most impressive results of recent NLP history is the ability of
pre-trained language models to solve new tasks in a zero-shot setting. To
achieve this, NLP tasks are framed as natural language prompts, generating a
response indicating the predicted output. Nonetheless, the performance in such
settings often lags far behind its supervised counterpart, suggesting a large
space for potential improvement. In this paper, we explore methods to utilize
unlabeled data to improve zero-shot performance. Specifically, we take
advantage of the fact that multiple prompts can be used to specify a single
task, and propose to regularize prompt consistency, encouraging consistent
predictions over this diverse set of prompts. Our method makes it possible to
fine-tune the model either with extra unlabeled training data, or directly on
test input at inference time in an unsupervised manner. In experiments, our
approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al.,
2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points
in terms of accuracy. The gains are often attained with a small number of
unlabeled examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExSum: From Local Explanations to Model Understanding. (arXiv:2205.00130v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00130">
<div class="article-summary-box-inner">
<span><p>Interpretability methods are developed to understand the working mechanisms
of black-box models, which is crucial to their responsible deployment.
Fulfilling this goal requires both that the explanations generated by these
methods are correct and that people can easily and reliably understand them.
While the former has been addressed in prior work, the latter is often
overlooked, resulting in informal model understanding derived from a handful of
local explanations. In this paper, we introduce explanation summary (ExSum), a
mathematical framework for quantifying model understanding, and propose metrics
for its quality assessment. On two domains, ExSum highlights various
limitations in the current practice, helps develop accurate model
understanding, and reveals easily overlooked properties of the model. We also
connect understandability to other properties of explanations such as human
alignment, robustness, and counterfactual minimality and plausibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representation Learning With Text and Images. (arXiv:2205.00142v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00142">
<div class="article-summary-box-inner">
<span><p>In recent years, multimodal AI has seen an upward trend as researchers are
integrating data of different types such as text, images, speech into modelling
to get the best results. This project leverages multimodal AI and matrix
factorization techniques for representation learning, on text and image data
simultaneously, thereby employing the widely used techniques of Natural
Language Processing (NLP) and Computer Vision. The learnt representations are
evaluated using downstream classification and regression tasks. The methodology
adopted can be extended beyond the scope of this project as it uses
Auto-Encoders for unsupervised representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Know by the Company Words Keep and What Else Lies in the Vicinity. (arXiv:2205.00148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00148">
<div class="article-summary-box-inner">
<span><p>The development of state-of-the-art (SOTA) Natural Language Processing (NLP)
systems has steadily been establishing new techniques to absorb the statistics
of linguistic data. These techniques often trace well-known constructs from
traditional theories, and we study these connections to close gaps around key
NLP methods as a means to orient future work. For this, we introduce an
analytic model of the statistics learned by seminal algorithms (including GloVe
and Word2Vec), and derive insights for systems that use these algorithms and
the statistics of co-occurrence, in general. In this work, we derive -- to the
best of our knowledge -- the first known solution to Word2Vec's
softmax-optimized, skip-gram algorithm. This result presents exciting potential
for future development as a direct solution to a deep learning (DL) language
model's (LM's) matrix factorization. However, we use the solution to
demonstrate a seemingly-universal existence of a property that word vectors
exhibit and which allows for the prophylactic discernment of biases in data --
prior to their absorption by DL models. To qualify our work, we conduct an
analysis of independence, i.e., on the density of statistical dependencies in
co-occurrence models, which in turn renders insights on the distributional
hypothesis' partial fulfillment by co-occurrence statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models. (arXiv:2205.00176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00176">
<div class="article-summary-box-inner">
<span><p>Recent open-domain dialogue models have brought numerous breakthroughs.
However, building a chat system is not scalable since it often requires a
considerable volume of human-human dialogue data, especially when enforcing
features such as persona, style, or safety. In this work, we study the
challenge of imposing roles on open-domain dialogue systems, with the goal of
making the systems maintain consistent roles while conversing naturally with
humans. To accomplish this, the system must satisfy a role specification that
includes certain conditions on the stated features as well as a system policy
on whether or not certain types of utterances are allowed. For this, we propose
an efficient data collection framework leveraging in-context few-shot learning
of large-scale language models for building role-satisfying dialogue dataset
from scratch. We then compare various architectures for open-domain dialogue
systems in terms of meeting role specifications while maintaining
conversational abilities. Automatic and human evaluations show that our models
return few out-of-bounds utterances, keeping competitive performance on general
metrics. We release a Korean dialogue dataset we built for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers. (arXiv:2205.00177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00177">
<div class="article-summary-box-inner">
<span><p>Existing Math Word Problem (MWP) solvers have achieved high accuracy on
benchmark datasets. However, prior works have shown that such solvers do not
generalize well and rely on superficial cues to achieve high performance. In
this paper, we first conduct experiments to showcase that this behaviour is
mainly associated with the limited size and diversity present in existing MWP
datasets. Next, we propose several data augmentation techniques broadly
categorized into Substitution and Paraphrasing based methods. By deploying
these methods we increase the size of existing datasets by five folds.
Extensive experiments on two benchmark datasets across three state-of-the-art
MWP solvers show that proposed methods increase the generalization and
robustness of existing solvers. On average, proposed methods significantly
increase the state-of-the-art results by over five percentage points on
benchmark datasets. Further, the solvers trained on the augmented dataset
perform comparatively better on the challenge test set. We also show the
effectiveness of proposed techniques through ablation studies and verify the
quality of augmented samples through human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Evaluation Method: Evaluation Data and Metrics for Chinese Grammar Error Correction. (arXiv:2205.00217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00217">
<div class="article-summary-box-inner">
<span><p>As a fundamental task in natural language processing, Chinese Grammatical
Error Correction (CGEC) has gradually received widespread attention and become
a research hotspot. However, one obvious deficiency for the existing CGEC
evaluation system is that the evaluation values are significantly influenced by
the Chinese word segmentation results or different language models. The
evaluation values of the same error correction model can vary considerably
under different word segmentation systems or different language models.
However, it is expected that these metrics should be independent of the word
segmentation results and language models, as they may lead to a lack of
uniqueness and comparability in the evaluation of different methods. To this
end, we propose three novel evaluation metrics for CGEC in two dimensions:
reference-based and reference-less. In terms of the reference-based metric, we
introduce sentence-level accuracy and char-level BLEU to evaluate the corrected
sentences. Besides, in terms of the reference-less metric, we adopt char-level
meaning preservation to measure the semantic preservation degree of the
corrected sentences. We deeply evaluate and analyze the reasonableness and
validity of the three proposed metrics, and we expect them to become a new
standard for CGEC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction. (arXiv:2205.00241v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00241">
<div class="article-summary-box-inner">
<span><p>Most previous studies aim at extracting events from a single sentence, while
document-level event extraction still remains under-explored. In this paper, we
focus on extracting event arguments from an entire document, which mainly faces
two critical problems: a) the long-distance dependency between trigger and
arguments over sentences; b) the distracting context towards an event in the
document. To address these issues, we propose a Two-Stream Abstract meaning
Representation enhanced extraction model (TSAR). TSAR encodes the document from
different perspectives by a two-stream encoding module, to utilize local and
global information and lower the impact of distracting context. Besides, TSAR
introduces an AMR-guided interaction module to capture both intra-sentential
and inter-sentential features, based on the locally and globally constructed
AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the
boundary information for text spans explicitly. Extensive experiments
illustrate that TSAR outperforms previous state-of-the-art by a large margin,
with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents
datasets respectively, showing the superiority in the cross-sentence arguments
extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing. (arXiv:2205.00258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00258">
<div class="article-summary-box-inner">
<span><p>The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposing Cross-Lingual Lexical Knowledge from Multilingual Sentence Encoders. (arXiv:2205.00267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00267">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual language models (LMs) can be successfully transformed
into multilingual sentence encoders (SEs; e.g., LaBSE, xMPNET) via additional
fine-tuning or model distillation on parallel data. However, it remains
uncertain how to best leverage their knowledge to represent sub-sentence
lexical items (i.e., words and phrases) in cross-lingual lexical tasks. In this
work, we probe these SEs for the amount of cross-lingual lexical knowledge
stored in their parameters, and compare them against the original multilingual
LMs. We also devise a novel method to expose this knowledge by additionally
fine-tuning multilingual models through inexpensive contrastive learning
procedure, requiring only a small amount of word translation pairs. We evaluate
our method on bilingual lexical induction (BLI), cross-lingual lexical semantic
similarity, and cross-lingual entity linking, and report substantial gains on
standard benchmarks (e.g., +10 Precision@1 points in BLI), validating that the
SEs such as LaBSE can be 'rewired' into effective cross-lingual lexical
encoders. Moreover, we show that resulting representations can be successfully
interpolated with static embeddings from cross-lingual word embedding spaces to
further boost the performance in lexical tasks. In sum, our approach provides
an effective tool for exposing and harnessing multilingual lexical knowledge
'hidden' in multilingual sentence encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clues Before Answers: Generation-Enhanced Multiple-Choice QA. (arXiv:2205.00274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00274">
<div class="article-summary-box-inner">
<span><p>A trending paradigm for multiple-choice question answering (MCQA) is using a
text-to-text framework. By unifying data in different tasks into a single
text-to-text format, it trains a generative encoder-decoder model which is both
powerful and universal. However, a side effect of twisting a generation target
to fit the classification nature of MCQA is the under-utilization of the
decoder and the knowledge that can be decoded. To exploit the generation
capability and underlying knowledge of a pre-trained encoder-decoder model, in
this paper, we propose a generation-enhanced MCQA model named GenMC. It
generates a clue from the question and then leverages the clue to enhance a
reader for MCQA. It outperforms text-to-text models on multiple MCQA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Emotion-specific Features to Improve Transformer Performance for Emotion Classification. (arXiv:2205.00283v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00283">
<div class="article-summary-box-inner">
<span><p>This paper describes the approach to the Emotion Classification shared task
held at WASSA 2022 by team PVGs AI Club. This Track 2 sub-task focuses on
building models which can predict a multi-class emotion label based on essays
from news articles where a person, group or another entity is affected.
Baseline transformer models have been demonstrating good results on sequence
classification tasks, and we aim to improve this performance with the help of
ensembling techniques, and by leveraging two variations of emotion-specific
representations. We observe better results than our baseline models and achieve
an accuracy of 0.619 and a macro F1 score of 0.520 on the emotion
classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks. (arXiv:2205.00305v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00305">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models with millions of parameters require
large storage. Recent approaches tackle this shortcoming by training adapters,
but these approaches still require a relatively large number of parameters. In
this study, AdapterBias, a surprisingly simple yet effective adapter
architecture, is proposed. AdapterBias adds a token-dependent shift to the
hidden output of transformer layers to adapt to downstream tasks with only a
vector and a linear layer. Extensive experiments are conducted to demonstrate
the effectiveness of AdapterBias. The experiments show that our proposed method
can dramatically reduce the trainable parameters compared to the previous works
with a minimal decrease in task performances compared with fine-tuned
pre-trained models. We further find that AdapterBias automatically learns to
assign more significant representation shifts to the tokens related to the task
in consideration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detoxifying Language Models with a Toxic Corpus. (arXiv:2205.00320v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00320">
<div class="article-summary-box-inner">
<span><p>Existing studies have investigated the tendency of autoregressive language
models to generate contexts that exhibit undesired biases and toxicity. Various
debiasing approaches have been proposed, which are primarily categorized into
data-based and decoding-based. In our study, we investigate the ensemble of the
two debiasing paradigms, proposing to use toxic corpus as an additional
resource to reduce the toxicity. Our result shows that toxic corpus can indeed
help to reduce the toxicity of the language generation process substantially,
complementing the existing debiasing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HateCheckHIn: Evaluating Hindi Hate Speech Detection Models. (arXiv:2205.00328v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00328">
<div class="article-summary-box-inner">
<span><p>Due to the sheer volume of online hate, the AI and NLP communities have
started building models to detect such hateful content. Recently, multilingual
hate is a major emerging challenge for automated detection where code-mixing or
more than one language have been used for conversation in social media.
Typically, hate speech detection models are evaluated by measuring their
performance on the held-out test data using metrics such as accuracy and
F1-score. While these metrics are useful, it becomes difficult to identify
using them where the model is failing, and how to resolve it. To enable more
targeted diagnostic insights of such multilingual hate speech models, we
introduce a set of functionalities for the purpose of evaluation. We have been
inspired to design this kind of functionalities based on real-world
conversation on social media. Considering Hindi as a base language, we craft
test cases for each functionality. We name our evaluation dataset HateCheckHIn.
To illustrate the utility of these functionalities , we test state-of-the-art
transformer based m-BERT model and the Perspective API.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opponent Modeling in Negotiation Dialogues by Related Data Adaptation. (arXiv:2205.00344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00344">
<div class="article-summary-box-inner">
<span><p>Opponent modeling is the task of inferring another party's mental state
within the context of social interactions. In a multi-issue negotiation, it
involves inferring the relative importance that the opponent assigns to each
issue under discussion, which is crucial for finding high-value deals. A
practical model for this task needs to infer these priorities of the opponent
on the fly based on partial dialogues as input, without needing additional
annotations for training. In this work, we propose a ranker for identifying
these priorities from negotiation dialogues. The model takes in a partial
dialogue as input and predicts the priority order of the opponent. We further
devise ways to adapt related data sources for this task to provide more
explicit supervision for incorporating the opponent's preferences and offers,
as a proxy to relying on granular utterance-level annotations. We show the
utility of our proposed approach through extensive experiments based on two
dialogue datasets. We find that the proposed data adaptations lead to strong
performance in zero-shot and few-shot scenarios. Moreover, they allow the model
to perform better than baselines while accessing fewer utterances from the
opponent. We release our code to support future work in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Process-Oriented, Modular, and Versatile Question Generation that Meets Educational Needs. (arXiv:2205.00355v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00355">
<div class="article-summary-box-inner">
<span><p>NLP-powered automatic question generation (QG) techniques carry great
pedagogical potential of saving educators' time and benefiting student
learning. Yet, QG systems have not been widely adopted in classrooms to date.
In this work, we aim to pinpoint key impediments and investigate how to improve
the usability of automatic QG techniques for educational purposes by
understanding how instructors construct questions and identifying touch points
to enhance the underlying NLP models. We perform an in-depth need finding study
with 11 instructors across 7 different universities, and summarize their
thought processes and needs when creating questions. While instructors show
great interests in using NLP systems to support question design, none of them
has used such tools in practice. They resort to multiple sources of
information, ranging from domain knowledge to students' misconceptions, all of
which missing from today's QG systems. We argue that building effective
human-NLP collaborative QG systems that emphasize instructor control and
explainability is imperative for real-world adoption. We call for QG systems to
provide process-oriented support, use modular design, and handle diverse
sources of input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Spatial Reasoning. (arXiv:2205.00363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00363">
<div class="article-summary-box-inner">
<span><p>Spatial relations are fundamental to human cognition and are the most basic
knowledge for us to understand and communicate about our physical surroundings.
In this paper, we ask the critical question: Are current vision-and-language
models (VLMs) able to correctly understand spatial relations? To answer this
question, we propose Visual Spatial Reasoning (VSR), a novel benchmark task
with human labelled dataset for investigating VLMs' capabilities in recognising
65 types of spatial relationships (e.g., under, in front of, facing etc.) in
natural text-image pairs. Specifically, given a caption and an image, the model
needs to perform binary classification and decide if the caption accurately
describes the spatial relationships of two objects presented in the image.
While being seemingly simple and straightforward, the task shows a large gap
between human and model performance (human ceiling on the VSR task is above 95%
and models only achieve around 70%). With fine-grained categorisation and
control on both concepts and relations, our VSR benchmark enables us to perform
interesting probing analysis to pinpoint VLMs' failure cases and the reasons
behind. We observe that VLMs' by-relation performances have little correlation
with the number of training examples and the tested models are in general
incapable of recognising relations that concern orientations of objects. Also,
VLMs have poor zero-shot generalisation toward unseen concepts. The dataset and
code are released at github.com/cambridgeltl/visual-spatial-reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting COVID-19 Conspiracy Theories with Transformers and TF-IDF. (arXiv:2205.00377v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00377">
<div class="article-summary-box-inner">
<span><p>The sharing of fake news and conspiracy theories on social media has
wide-spread negative effects. By designing and applying different machine
learning models, researchers have made progress in detecting fake news from
text. However, existing research places a heavy emphasis on general,
common-sense fake news, while in reality fake news often involves rapidly
changing topics and domain-specific vocabulary. In this paper, we present our
methods and results for three fake news detection tasks at MediaEval benchmark
2021 that specifically involve COVID-19 related topics. We experiment with a
group of text-based models including Support Vector Machines, Random Forest,
BERT, and RoBERTa. We find that a pre-trained transformer yields the best
validation results, but a randomly initialized transformer with smart design
can also be trained to reach accuracies close to that of the pre-trained
transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Cross-lingual Conversation Summarization Challenge. (arXiv:2205.00379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00379">
<div class="article-summary-box-inner">
<span><p>We propose the shared task of cross-lingual conversation summarization,
\emph{ConvSumX Challenge}, opening new avenues for researchers to investigate
solutions that integrate conversation summarization and machine translation.
This task can be particularly useful due to the emergence of online meetings
and conferences. We construct a new benchmark, covering 2 real-world scenarios
and 3 language directions, including a low-resource language. We hope that
\emph{ConvSumX} can motivate researches to go beyond English and break the
barrier for non-English speakers to benefit from recent advances of
conversation summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crude Oil-related Events Extraction and Processing: A Transfer Learning Approach. (arXiv:2205.00387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00387">
<div class="article-summary-box-inner">
<span><p>One of the challenges in event extraction via traditional supervised learning
paradigm is the need for a sizeable annotated dataset to achieve satisfactory
model performance. It is even more challenging when it comes to event
extraction in the finance and economics domain, a domain with considerably
fewer resources. This paper presents a complete framework for extracting and
processing crude oil-related events found in CrudeOilNews corpus, addressing
the issue of annotation scarcity and class imbalance by leveraging on the
effectiveness of transfer learning. Apart from event extraction, we place
special emphasis on event properties (Polarity, Modality, and Intensity)
classification to determine the factual certainty of each event. We build
baseline models first by supervised learning and then exploit Transfer Learning
methods to boost event extraction model performance despite the limited amount
of annotated data and severe class imbalance. This is done via methods within
the transfer learning framework such as Domain Adaptive Pre-training,
Multi-task Learning and Sequential Transfer Learning. Based on experiment
results, we are able to improve all event extraction sub-task models both in F1
and MCC1-score as compared to baseline models trained via the standard
supervised learning. Accurate and holistic event extraction from crude oil news
is very useful for downstream tasks such as understanding event chains and
learning event-event relations, which can be used for other downstream tasks
such as commodity price prediction, summarisation, etc. to support a wide range
of business decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELQA: A Corpus of Questions and Answers about the English Language. (arXiv:2205.00395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00395">
<div class="article-summary-box-inner">
<span><p>We introduce a community-sourced dataset for English Language Question
Answering (ELQA), which consists of more than 180k questions and answers on
numerous topics about English language such as grammar, meaning, fluency, and
etymology. The ELQA corpus will enable new NLP applications for language
learners. We introduce three tasks based on the ELQA corpus: 1) answer quality
classification, 2) semantic search for finding similar questions, and 3) answer
generation. We present baselines for each task along with analysis, showing the
strengths and weaknesses of current transformer-based models. The ELQA corpus
and scripts are publicly available for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00415">
<div class="article-summary-box-inner">
<span><p>In recent years, progress in NLU has been driven by benchmarks. These
benchmarks are typically collected by crowdsourcing, where annotators write
examples based on annotation instructions crafted by dataset creators. In this
work, we hypothesize that annotators pick up on patterns in the crowdsourcing
instructions, which bias them to write similar examples that are then
over-represented in the collected data. We study this form of bias, termed
instruction bias, in 14 recent NLU benchmarks, showing that instruction
examples often exhibit concrete patterns, which are propagated by crowdworkers
to the collected data. This extends previous work (Geva et al., 2019) and
raises a new concern of whether we are modeling the dataset creator's
instructions, rather than the task. Through a series of experiments, we show
that, indeed, instruction bias can lead to overestimation of model performance,
and that models struggle to generalize beyond biases originating in the
crowdsourcing instructions. We further analyze the influence of instruction
bias in terms of pattern frequency and model size, and derive concrete
recommendations for creating future NLU benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ETMS@IITKGP at SemEval-2022 Task 10: Structured Sentiment Analysis Using A Generative Approach. (arXiv:2205.00440v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00440">
<div class="article-summary-box-inner">
<span><p>Structured Sentiment Analysis (SSA) deals with extracting opinion tuples in a
text, where each tuple (h, e, t, p) consists of h, the holder, who expresses a
sentiment polarity p towards a target t through a sentiment expression e. While
prior works explore graph-based or sequence labeling-based approaches for the
task, we in this paper present a novel unified generative method to solve SSA,
a SemEval2022 shared task. We leverage a BART-based encoder-decoder
architecture and suitably modify it to generate, given a sentence, a sequence
of opinion tuples. Each generated tuple consists of seven integers respectively
representing the indices corresponding to the start and end positions of the
holder, target, and expression spans, followed by the sentiment polarity class
associated between the target and the sentiment expression. We perform rigorous
experiments for both Monolingual and Cross-lingual subtasks, and achieve
competitive Sentiment F1 scores on the leaderboard in both settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. (arXiv:2205.00445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00445">
<div class="article-summary-box-inner">
<span><p>Huge language models (LMs) have ushered in a new era for AI, serving as a
gateway to natural-language-based knowledge tasks. Although an essential
element of modern AI, LMs are also inherently limited in a number of ways. We
discuss these limitations and how they can be avoided by adopting a systems
approach. Conceptualizing the challenge as one that involves knowledge and
reasoning in addition to linguistic processing, we define a flexible
architecture with multiple neural models, complemented by discrete knowledge
and reasoning modules. We describe this neuro-symbolic architecture, dubbed the
Modular Reasoning, Knowledge and Language (MRKL, pronounced "miracle") system,
some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs'
MRKL system implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The use of Data Augmentation as a technique for improving neural network accuracy in detecting fake news about COVID-19. (arXiv:2205.00452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00452">
<div class="article-summary-box-inner">
<span><p>This paper aims to present how the application of Natural Language Processing
(NLP) and data augmentation techniques can improve the performance of a neural
network for better detection of fake news in the Portuguese language. Fake news
is one of the main controversies during the growth of the internet in the last
decade. Verifying what is fact and what is false has proven to be a difficult
task, while the dissemination of false news is much faster, which leads to the
need for the creation of tools that, automated, assist in the process of
verification of what is fact and what is false. In order to bring a solution,
an experiment was developed with neural network using news, real and fake,
which were never seen by artificial intelligence (AI). There was a significant
performance in the news classification after the application of the mentioned
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conceptualizing Treatment Leakage in Text-based Causal Inference. (arXiv:2205.00465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00465">
<div class="article-summary-box-inner">
<span><p>Causal inference methods that control for text-based confounders are becoming
increasingly important in the social sciences and other disciplines where text
is readily available. However, these methods rely on a critical assumption that
there is no treatment leakage: that is, the text only contains information
about the confounder and no information about treatment assignment. When this
assumption does not hold, methods that control for text to adjust for
confounders face the problem of post-treatment (collider) bias. However, the
assumption that there is no treatment leakage may be unrealistic in real-world
situations involving text, as human language is rich and flexible. Language
appearing in a public policy document or health records may refer to the future
and the past simultaneously, and thereby reveal information about the treatment
assignment.
</p>
<p>In this article, we define the treatment-leakage problem, and discuss the
identification as well as the estimation challenges it raises. Second, we
delineate the conditions under which leakage can be addressed by removing the
treatment-related signal from the text in a pre-processing step we define as
text distillation. Lastly, using simulation, we show how treatment leakage
introduces a bias in estimates of the average treatment effect (ATE) and how
text distillation can mitigate this bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">None Class Ranking Loss for Document-Level Relation Extraction. (arXiv:2205.00476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00476">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (RE) aims at extracting relations among
entities expressed across multiple sentences, which can be viewed as a
multi-label classification problem. In a typical document, most entity pairs do
not express any pre-defined relation and are labeled as "none" or "no
relation". For good document-level RE performance, it is crucial to distinguish
such \textit{none} class instances (entity pairs) from those of pre-defined
classes (relations). However, most existing methods only estimate the
probability of pre-defined relations independently without considering the
probability of "no relation". This ignores the context of entity pairs and the
label correlations between the none class and pre-defined classes, leading to
sub-optimal predictions. To address this problem, we propose a new multi-label
loss that encourages large \textit{margins} of label confidence scores between
each pre-defined class and the none class, which enables captured label
correlations and context-dependent thresholding for label prediction. To gain
further robustness against positive-negative imbalance and mislabeled data that
could appear in real-world RE datasets, we propose a margin regularization and
a margin shifting technique. Experimental results demonstrate that our method
significantly outperforms existing multi-label losses for document-level RE and
works well in other multi-label tasks such as emotion classification when none
class instances are available for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nearest Neighbor Knowledge Distillation for Neural Machine Translation. (arXiv:2205.00479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00479">
<div class="article-summary-box-inner">
<span><p>k-nearest-neighbor machine translation (NN-MT), proposed by Khandelwal et al.
(2021), has achieved many state-of-the-art results in machine translation
tasks. Although effective, NN-MT requires conducting NN searches through the
large datastore for each decoding step during inference, prohibitively
increasing the decoding cost and thus leading to the difficulty for the
deployment in real-world applications. In this paper, we propose to move the
time-consuming NN search forward to the preprocessing phase, and then introduce
Nearest Neighbor Knowledge Distillation (NN-KD) that trains the base NMT model
to directly learn the knowledge of NN. Distilling knowledge retrieved by NN can
encourage the NMT model to take more reasonable target tokens into
consideration, thus addressing the overcorrection problem. Extensive
experimental results show that, the proposed method achieves consistent
improvement over the state-of-the-art baselines including NN-MT, while
maintaining the same training and decoding speed as the standard NMT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Programming in Rank Space: Scaling Structured Inference with Low-Rank HMMs and PCFGs. (arXiv:2205.00484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00484">
<div class="article-summary-box-inner">
<span><p>Hidden Markov Models (HMMs) and Probabilistic Context-Free Grammars (PCFGs)
are widely used structured models, both of which can be represented as factor
graph grammars (FGGs), a powerful formalism capable of describing a wide range
of models. Recent research found it beneficial to use large state spaces for
HMMs and PCFGs. However, inference with large state spaces is computationally
demanding, especially for PCFGs. To tackle this challenge, we leverage tensor
rank decomposition (aka.\ CPD) to decrease inference computational complexities
for a subset of FGGs subsuming HMMs and PCFGs. We apply CPD on the factors of
an FGG and then construct a new FGG defined in the rank space. Inference with
the new FGG produces the same result but has a lower time complexity when the
rank size is smaller than the state size. We conduct experiments on HMM
language modeling and unsupervised PCFG parsing, showing better performance
than previous work. Our code is publicly available at
\url{https://github.com/VPeterV/RankSpace-Models}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilingual End-to-End ASR with Byte-Level Subwords. (arXiv:2205.00485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00485">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate how the output representation of an end-to-end
neural network affects multilingual automatic speech recognition (ASR). We
study different representations including character-level, byte-level, byte
pair encoding (BPE), and byte-level byte pair encoding (BBPE) representations,
and analyze their strengths and weaknesses. We focus on developing a single
end-to-end model to support utterance-based bilingual ASR, where speakers do
not alternate between two languages in a single utterance but may change
languages across utterances. We conduct our experiments on English and Mandarin
dictation tasks, and we find that BBPE with penalty schemes can improve
utterance-based bilingual ASR performance by 2% to 5% relative even with
smaller number of outputs and fewer parameters. We conclude with analysis that
indicates directions for further improving multilingual ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction. (arXiv:2205.00498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00498">
<div class="article-summary-box-inner">
<span><p>Implicit event argument extraction (EAE) aims to identify arguments that
could scatter over the document. Most previous work focuses on learning the
direct relations between arguments and the given trigger, while the implicit
relations with long-range dependency are not well studied. Moreover, recent
neural network based approaches rely on a large amount of labeled data for
training, which is unavailable due to the high labelling cost. In this paper,
we propose a Curriculum learning based Prompt tuning (CUP) approach, which
resolves implicit EAE by four learning stages. The stages are defined according
to the relations with the trigger node in a semantic graph, which well captures
the long-range dependency between arguments and the trigger. In addition, we
integrate a prompt-based encoder-decoder model to elicit related knowledge from
pre-trained language models (PLMs) in each stage, where the prompt templates
are adapted with the learning progress to enhance the reasoning for arguments.
Experimental results on two well-known benchmark datasets show the great
advantages of our proposed approach. In particular, we outperform the
state-of-the-art models in both fully-supervised and low-data scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. (arXiv:2205.00501v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00501">
<div class="article-summary-box-inner">
<span><p>Machine learning models are commonly used to detect toxicity in online
conversations. These models are trained on datasets annotated by human raters.
We explore how raters' self-described identities impact how they annotate
toxicity in online comments. We first define the concept of specialized rater
pools: rater pools formed based on raters' self-described identities, rather
than at random. We formed three such rater pools for this study--specialized
rater pools of raters from the U.S. who identify as African American, LGBTQ,
and those who identify as neither. Each of these rater pools annotated the same
set of comments, which contains many references to these identity groups. We
found that rater identity is a statistically significant factor in how raters
will annotate toxicity for identity-related annotations. Using preliminary
content analysis, we examined the comments with the most disagreement between
rater pools and found nuanced differences in the toxicity annotations. Next, we
trained models on the annotations from each of the different rater pools, and
compared the scores of these models on comments from several test sets.
Finally, we discuss how using raters that self-identify with the subjects of
comments can create more inclusive machine learning models, and provide more
nuanced ratings than those by random raters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Stylistic Variation: Choices, Genres and Individuals. (arXiv:2205.00510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00510">
<div class="article-summary-box-inner">
<span><p>This chapter argues for more informed target metrics for the statistical
processing of stylistic variation in text collections. Much as operationalised
relevance proved a useful goal to strive for in information retrieval, research
in textual stylistics, whether application oriented or philologically inclined,
needs goals formulated in terms of pertinence, relevance, and utility - notions
that agree with reader experience of text. Differences readers are aware of are
mostly based on utility - not on textual characteristics per se. Mostly,
readers report stylistic differences in terms of genres. Genres, while vague
and undefined, are well-established and talked about: very early on, readers
learn to distinguish genres. This chapter discusses variation given by genre,
and contrasts it to variation occasioned by individual choice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conventions and Mutual Expectations -- understanding sources for web genres. (arXiv:2205.00512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00512">
<div class="article-summary-box-inner">
<span><p>Genres can be understood in many different ways. They are often perceived as
a primarily sociological construction, or, alternatively, as a
stylostatistically observable objective characteristic of texts. The latter
view is more common in the research field of information and language
technology. These two views can be quite compatible and can inform each other;
this present investigation discusses knowledge sources for studying genre
variation and change by observing reader and author behaviour rather than
performing analyses on the information objects themselves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enumeration Classes Defined by Circuits. (arXiv:2205.00539v1 [cs.CC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00539">
<div class="article-summary-box-inner">
<span><p>We refine the complexity landscape for enumeration problems by introducing
very low classes defined by using Boolean circuits as enumerators. We locate
well-known enumeration problems, e.g., from graph theory, Gray code
enumeration, and propositional satisfiability in our classes. In this way we
obtain a framework to distinguish between the complexity of different problems
known to be in $\mathbf{DelayP}$, for which a formal way of comparison was not
possible to this day.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Multi-Document Summarization with Information Extraction and Compression. (arXiv:2205.00548v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00548">
<div class="article-summary-box-inner">
<span><p>We develop an abstractive summarization framework independent of labeled data
for multiple heterogeneous documents. Unlike existing multi-document
summarization methods, our framework processes documents telling different
stories instead of documents on the same topic. We also enhance an existing
sentence fusion method with a uni-directional language model to prioritize
fused sentences with higher sentence probability with the goal of increasing
readability. Lastly, we construct a total of twelve dataset variations based on
CNN/Daily Mail and the NewsRoom datasets, where each document group contains a
large and diverse collection of documents to evaluate the performance of our
model in comparison with other baseline systems. Our experiments demonstrate
that our framework outperforms current state-of-the-art methods in this more
generic setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Bias in Masked Language Models for Multiple Languages. (arXiv:2205.00551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00551">
<div class="article-summary-box-inner">
<span><p>Masked Language Models (MLMs) pre-trained by predicting masked tokens on
large corpora have been used successfully in natural language processing tasks
for a variety of languages. Unfortunately, it was reported that MLMs also learn
discriminative biases regarding attributes such as gender and race. Because
most studies have focused on MLMs in English, the bias of MLMs in other
languages has rarely been investigated. Manual annotation of evaluation data
for languages other than English has been challenging due to the cost and
difficulty in recruiting annotators. Moreover, the existing bias evaluation
methods require the stereotypical sentence pairs consisting of the same context
with attribute words (e.g. He/She is a nurse). We propose Multilingual Bias
Evaluation (MBE) score, to evaluate bias in various languages using only
English attribute word lists and parallel corpora between the target language
and English without requiring manually annotated data. We evaluated MLMs in
eight languages using the MBE and confirmed that gender-related biases are
encoded in MLMs for all those languages. We manually created datasets for
gender bias in Japanese and Russian to evaluate the validity of the MBE. The
results show that the bias scores reported by the MBE significantly correlates
with that computed from the above manually created datasets and the existing
English datasets for gender bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Informed Slang Interpretation. (arXiv:2205.00616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00616">
<div class="article-summary-box-inner">
<span><p>Slang is a predominant form of informal language making flexible and extended
use of words that is notoriously hard for natural language processing systems
to interpret. Existing approaches to slang interpretation tend to rely on
context but ignore semantic extensions common in slang word usage. We propose a
semantically informed slang interpretation (SSI) framework that considers
jointly the contextual and semantic appropriateness of a candidate
interpretation for a query slang. We perform rigorous evaluation on two
large-scale online slang dictionaries and show that our approach not only
achieves state-of-the-art accuracy for slang interpretation in English, but
also does so in zero-shot and few-shot scenarios where training data is sparse.
Furthermore, we show how the same framework can be applied to enhancing machine
translation of slang from English to other languages. Our work creates
opportunities for the automated interpretation and translation of informal
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection. (arXiv:2205.00619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00619">
<div class="article-summary-box-inner">
<span><p>Ideology is at the core of political science research. Yet, there still does
not exist general-purpose tools to characterize and predict ideology across
different genres of text. To this end, we study Pretrained Language Models
using novel ideology-driven pretraining objectives that rely on the comparison
of articles on the same story written by media of different ideologies. We
further collect a large-scale dataset, consisting of more than 3.6M political
news articles, for pretraining. Our model POLITICS outperforms strong baselines
and the previous state-of-the-art models on ideology prediction and stance
detection tasks. Further analyses show that POLITICS is especially good at
understanding long or formally written texts, and is also robust in few-shot
learning scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching BERT to Wait: Balancing Accuracy and Latency for Streaming Disfluency Detection. (arXiv:2205.00620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00620">
<div class="article-summary-box-inner">
<span><p>In modern interactive speech-based systems, speech is consumed and
transcribed incrementally prior to having disfluencies removed. This
post-processing step is crucial for producing clean transcripts and high
performance on downstream tasks (e.g. machine translation). However, most
current state-of-the-art NLP models such as the Transformer operate
non-incrementally, potentially causing unacceptable delays. We propose a
streaming BERT-based sequence tagging model that, combined with a novel
training objective, is capable of detecting disfluencies in real-time while
balancing accuracy and latency. This is accomplished by training the model to
decide whether to immediately output a prediction for the current input or to
wait for further context. Essentially, the model learns to dynamically size its
lookahead window. Our results demonstrate that our model produces comparably
accurate predictions and does so sooner than our baselines, with lower flicker.
Furthermore, the model attains state-of-the-art latency and stability scores
when compared with recent work on incremental disfluency detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Fine-tuning via Perturbation and Interpolation from In-batch Instances. (arXiv:2205.00633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00633">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained language models (PLMs) on downstream tasks has become
common practice in natural language processing. However, most of the PLMs are
vulnerable, e.g., they are brittle under adversarial attacks or imbalanced
data, which hinders the application of the PLMs on some downstream tasks,
especially in safe-critical scenarios. In this paper, we propose a simple yet
effective fine-tuning method called Match-Tuning to force the PLMs to be more
robust. For each instance in a batch, we involve other instances in the same
batch to interact with it. To be specific, regarding the instances with other
labels as a perturbation, Match-Tuning makes the model more robust to noise at
the beginning of training. While nearing the end, Match-Tuning focuses more on
performing an interpolation among the instances with the same label for better
generalization. Extensive experiments on various tasks in GLUE benchmark show
that Match-Tuning consistently outperforms the vanilla fine-tuning by $1.64$
scores. Moreover, Match-Tuning exhibits remarkable robustness to adversarial
attacks and data imbalance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Two Parameters Equation for Word Rank-Frequency Relation. (arXiv:2205.00638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00638">
<div class="article-summary-box-inner">
<span><p>Let $f (\cdot)$ be the absolute frequency of words and $r$ be the rank of
words in decreasing order of frequency, then the following function can fit the
rank-frequency relation \[ f (r;s,t) = \left(\frac{r_{\tt max}}{r}\right)^{1-s}
\left(\frac{r_{\tt max}+t \cdot r_{\tt exp}}{r+t \cdot r_{\tt
exp}}\right)^{1+(1+t)s} \] where $r_{\tt max}$ and $r_{\tt exp}$ are the
maximum and the expectation of the rank, respectively; $s&gt;0$ and $t&gt;0$ are
parameters estimated from data. On well-behaved data, there should be $s&lt;1$ and
$s \cdot t &lt; 1$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased Contrastive Learning of Unsupervised Sentence Representations. (arXiv:2205.00656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00656">
<div class="article-summary-box-inner">
<span><p>Recently, contrastive learning has been shown to be effective in improving
pre-trained language models (PLM) to derive high-quality sentence
representations. It aims to pull close positive examples to enhance the
alignment while push apart irrelevant negatives for the uniformity of the whole
representation space. However, previous works mostly adopt in-batch negatives
or sample from training data at random. Such a way may cause the sampling bias
that improper negatives (e.g. false negatives and anisotropy representations)
are used to learn sentence representations, which will hurt the uniformity of
the representation space. To address it, we present a new framework
\textbf{DCLR} (\underline{D}ebiased \underline{C}ontrastive
\underline{L}earning of unsupervised sentence \underline{R}epresentations) to
alleviate the influence of these improper negatives. In DCLR, we design an
instance weighting method to punish false negatives and generate noise-based
negatives to guarantee the uniformity of the representation space. Experiments
on seven semantic textual similarity tasks show that our approach is more
effective than competitive baselines. Our code and data are publicly available
at the link: \textcolor{blue}{\url{https://github.com/RUCAIBox/DCLR}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Implicit Length Bias of Label Smoothing on Beam Search Decoding. (arXiv:2205.00659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00659">
<div class="article-summary-box-inner">
<span><p>Label smoothing is ubiquitously applied in Neural Machine Translation (NMT)
training. While label smoothing offers a desired regularization effect during
model training, in this paper we demonstrate that it nevertheless introduces
length biases in the beam search decoding procedure. Our analysis shows that
label smoothing implicitly applies a length penalty term to output sequence,
causing a bias towards shorter translations. We also show that for a model
fully optimized with label smoothing, translation length is implicitly upper
bounded by a fixed constant independent of input. We verify our theory by
applying a simple rectification function at inference time to restore the
unbiased distributions from the label-smoothed model predictions. This
rectification method led to consistent quality improvements on WMT
English-German, English-French, English-Czech and English-Chinese tasks, up to
+0.3 BLEU at beam size 4 and +2.8 BLEU at beam size 200.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Task Effects in Human Reading with Neural Attention. (arXiv:1808.00054v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.00054">
<div class="article-summary-box-inner">
<span><p>Humans read by making a sequence of fixations and saccades. They often skip
words, without apparent detriment to understanding. We offer a novel
explanation for skipping: readers optimize a tradeoff between performing a
language-related task and fixating as few words as possible. We propose a
neural architecture that combines an attention module (deciding whether to skip
words) and a task module (memorizing the input). We show that our model
predicts human skipping behavior, while also modeling reading times well, even
though it skips 40% of the input. A key prediction of our model is that
different reading tasks should result in different skipping behaviors. We
confirm this prediction in an eye-tracking experiment in which participants
answers questions about a text. We are able to capture these experimental
results using the our model, replacing the memorization module with a task
module that performs neural question answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Entity Disambiguation with BERT. (arXiv:1909.00426v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.00426">
<div class="article-summary-box-inner">
<span><p>We propose a global entity disambiguation (ED) model based on BERT. To
capture global contextual information for ED, our model treats not only words
but also entities as input tokens, and solves the task by sequentially
resolving mentions to their referent entities and using resolved entities as
inputs at each step. We train the model using a large entity-annotated corpus
obtained from Wikipedia. We achieve new state-of-the-art results on five
standard ED datasets: AIDA-CoNLL, MSNBC, AQUAINT, ACE2004, and WNED-WIKI. The
source code and model checkpoint are available at
https://github.com/studio-ousia/luke.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational reasoning and generalization using non-symbolic neural networks. (arXiv:2006.07968v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07968">
<div class="article-summary-box-inner">
<span><p>The notion of equality (identity) is simple and ubiquitous, making it a key
case study for broader questions about the representations supporting abstract
relational reasoning. Previous work suggested that neural networks were not
suitable models of human relational reasoning because they could not represent
mathematically identity, the most basic form of equality. We revisit this
question. In our experiments, we assess out-of-sample generalization of
equality using both arbitrary representations and representations that have
been pretrained on separate tasks to imbue them with structure. We find neural
networks are able to learn (1) basic equality (mathematical identity), (2)
sequential equality problems (learning ABA-patterned sequences) with only
positive training instances, and (3) a complex, hierarchical equality problem
with only basic equality training instances ("zero-shot'" generalization). In
the two latter cases, our models perform tasks proposed in previous work to
demarcate human-unique symbolic abilities. These results suggest that essential
aspects of symbolic reasoning can emerge from data-driven, non-symbolic
learning processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring COVID-19 Biological Pathways from Clinical Phenotypes via Topological Analysis. (arXiv:2101.07417v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07417">
<div class="article-summary-box-inner">
<span><p>COVID-19 has caused thousands of deaths around the world and also resulted in
a large international economic disruption. Identifying the pathways associated
with this illness can help medical researchers to better understand the
properties of the condition. This process can be carried out by analyzing the
medical records. It is crucial to develop tools and models that can aid
researchers with this process in a timely manner. However, medical records are
often unstructured clinical notes, and this poses significant challenges to
developing the automated systems. In this article, we propose a pipeline to aid
practitioners in analyzing clinical notes and revealing the pathways associated
with this disease. Our pipeline relies on topological properties and consists
of three steps: 1) pre-processing the clinical notes to extract the salient
concepts, 2) constructing a feature space of the patients to characterize the
extracted concepts, and finally, 3) leveraging the topological properties to
distill the available knowledge and visualize the result. Our experiments on a
publicly available dataset of COVID-19 clinical notes testify that our pipeline
can indeed extract meaningful pathways.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Graph Convolutional Networks for Text Classification. (arXiv:2102.09604v3 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09604">
<div class="article-summary-box-inner">
<span><p>Graph convolutional networks (GCNs) are a powerful architecture for
representation learning on documents that naturally occur as graphs, e.g.,
citation or social networks. However, sensitive personal information, such as
documents with people's profiles or relationships as edges, are prone to
privacy leaks, as the trained model might reveal the original input. Although
differential privacy (DP) offers a well-founded privacy-preserving framework,
GCNs pose theoretical and practical challenges due to their training specifics.
We address these challenges by adapting differentially-private gradient-based
training to GCNs and conduct experiments using two optimizers on five NLP
datasets in two languages. We propose a simple yet efficient method based on
random graph splits that not only improves the baseline privacy bounds by a
factor of 2.7 while retaining competitive F1 scores, but also provides strong
privacy guarantees of epsilon = 1.0. We show that, under certain modeling
choices, privacy-preserving GCNs perform up to 90% of their non-private
variants, while formally guaranteeing strong privacy measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07352">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation models are sensitive to noise in the input texts,
such as misspelled words and ungrammatical constructions. Existing robustness
techniques generally fail when faced with unseen types of noise and their
performance degrades on clean texts. In this paper, we focus on three types of
realistic noise that are commonly generated by humans and introduce the idea of
visual context to improve translation robustness for noisy texts. In addition,
we describe a novel error correction training regime that can be used as an
auxiliary task to further improve translation robustness. Experiments on
English-French and English-German translation show that both multimodal and
error correction components improve model robustness to noisy texts, while
still retaining translation quality on clean texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervising Model Attention with Human Explanations for Robust Natural Language Inference. (arXiv:2104.08142v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08142">
<div class="article-summary-box-inner">
<span><p>Natural Language Inference (NLI) models are known to learn from biases and
artefacts within their training data, impacting how well they generalise to
other unseen datasets. Existing de-biasing approaches focus on preventing the
models from learning these biases, which can result in restrictive models and
lower performance. We instead investigate teaching the model how a human would
approach the NLI task, in order to learn features that will generalise better
to previously unseen examples. Using natural language explanations, we
supervise the model's attention weights to encourage more attention to be paid
to the words present in the explanations, significantly improving model
performance. Our experiments show that the in-distribution improvements of this
method are also accompanied by out-of-distribution improvements, with the
supervised models learning from features that generalise better to other NLI
datasets. Analysis of the model indicates that human explanations encourage
increased attention on the important words, with more attention paid to words
in the premise and less attention paid to punctuation and stop-words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08043">
<div class="article-summary-box-inner">
<span><p>The vast majority of existing methods and systems for causal inference assume
that all variables under consideration are categorical or numerical (e.g.,
gender, price, blood pressure, enrollment). In this paper, we present
CausalNLP, a toolkit for inferring causality from observational data that
includes text in addition to traditional numerical and categorical variables.
CausalNLP employs the use of meta-learners for treatment effect estimation and
supports using raw text and its linguistic properties as both a treatment and a
"controlled-for" variable (e.g., confounder). The library is open-source and
available at: https://github.com/amaiya/causalnlp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Do You Get When You Cross Beam Search with Nucleus Sampling?. (arXiv:2107.09729v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09729">
<div class="article-summary-box-inner">
<span><p>We combine beam search with the probabilistic pruning technique of nucleus
sampling to create two deterministic nucleus search algorithms for natural
language generation. The first algorithm, p-exact search, locally prunes the
next-token distribution and performs an exact search over the remaining space.
The second algorithm, dynamic beam search, shrinks and expands the beam size
according to the entropy of the candidate's probability distribution. Despite
the probabilistic intuition behind nucleus search, experiments on machine
translation and summarization benchmarks show that both algorithms reach the
same performance levels as standard beam search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMChat: Multi-Modal Chat Dataset on Social Media. (arXiv:2108.07154v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07154">
<div class="article-summary-box-inner">
<span><p>Incorporating multi-modal contexts in conversation is important for
developing more engaging dialogue systems. In this work, we explore this
direction by introducing MMChat: a large-scale Chinese multi-modal dialogue
corpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous
corpora that are crowd-sourced or collected from fictitious movies, MMChat
contains image-grounded dialogues collected from real conversations on social
media, in which the sparsity issue is observed. Specifically, image-initiated
dialogues in common communications may deviate to some non-image-grounded
topics as the conversation proceeds. To better investigate this issue, we
manually annotate 100K dialogues from MMChat and further filter the corpus
accordingly, which yields MMChat-hf. We develop a benchmark model to address
the sparsity issue in dialogue generation tasks by adapting the attention
routing mechanism on image features. Experiments demonstrate the usefulness of
incorporating image features and the effectiveness of handling the sparsity of
image features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v4 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08614">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, systems from the IR
and NLP communities have addressed QA over text, but such systems barely
utilize semantic data and knowledge. This paper presents the first QA system
that can seamlessly operate over RDF datasets and text corpora, or both
together, in a unified framework. Our method, called UNIQORN, builds a context
graph on-the-fly, by retrieving question-relevant evidences from the RDF data
and/or a text corpus, using fine-tuned BERT models. The resulting graph is
typically rich but highly noisy. UNIQORN copes with this input by a graph
algorithm for Group Steiner Trees, that identifies the best answer candidates
in the context graph. Experimental results on several benchmarks of complex
questions with multiple entities and relations, show that UNIQORN significantly
outperforms state-of-the-art methods for QA over heterogeneous sources. The
graph-based methodology provides user-interpretable evidence for the complete
answering process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems. (arXiv:2109.04919v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04919">
<div class="article-summary-box-inner">
<span><p>The ability to recognise emotions lends a conversational artificial
intelligence a human touch. While emotions in chit-chat dialogues have received
substantial attention, emotions in task-oriented dialogues remain largely
unaddressed. This is despite emotions and dialogue success having equally
important roles in a natural system. Existing emotion-annotated task-oriented
corpora are limited in size, label richness, and public availability, creating
a bottleneck for downstream tasks. To lay a foundation for studies on emotions
in task-oriented dialogues, we introduce EmoWOZ, a large-scale manually
emotion-annotated corpus of task-oriented dialogues. EmoWOZ is based on
MultiWOZ, a multi-domain task-oriented dialogue dataset. It contains more than
11K dialogues with more than 83K emotion annotations of user utterances. In
addition to Wizard-of-Oz dialogues from MultiWOZ, we collect human-machine
dialogues within the same set of domains to sufficiently cover the space of
various emotions that can happen during the lifetime of a data-driven dialogue
system. To the best of our knowledge, this is the first large-scale open-source
corpus of its kind. We propose a novel emotion labelling scheme, which is
tailored to task-oriented dialogues. We report a set of experimental results to
show the usability of this corpus for emotion recognition and state tracking in
task-oriented dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction. (arXiv:2109.12093v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12093">
<div class="article-summary-box-inner">
<span><p>Stepping from sentence-level to document-level, the research on relation
extraction (RE) confronts increasing text length and more complicated entity
interactions. Consequently, it is more challenging to encode the key
information sources--relevant contexts and entity types. However, existing
methods only implicitly learn to model these critical information sources while
being trained for RE. As a result, they suffer the problems of ineffective
supervision and uninterpretable model predictions. In contrast, we propose to
explicitly teach the model to capture relevant contexts and entity types by
supervising and augmenting intermediate steps (SAIS) for RE. Based on a broad
spectrum of carefully designed tasks, our proposed SAIS method not only
extracts relations of better quality due to more effective supervision, but
also retrieves the corresponding supporting evidence more accurately so as to
enhance interpretability. By assessing model uncertainty, SAIS further boosts
the performance via evidence-based data augmentation and ensemble inference
while reducing the computational cost. Eventually, SAIS delivers
state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and
outperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval
on DocRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skill Induction and Planning with Latent Language. (arXiv:2110.01517v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01517">
<div class="article-summary-box-inner">
<span><p>We present a framework for learning hierarchical policies from
demonstrations, using sparse natural language annotations to guide the
discovery of reusable skills for autonomous decision-making. We formulate a
generative model of action sequences in which goals generate sequences of
high-level subtask descriptions, and these descriptions generate sequences of
low-level actions. We describe how to train this model using primarily
unannotated demonstrations by parsing demonstrations into sequences of named
high-level subtasks, using only a small number of seed annotations to ground
language in action. In trained models, natural language commands index a
combinatorial library of skills; agents can use these skills to plan by
generating high-level instruction sequences tailored to novel goals. We
evaluate this approach in the ALFRED household simulation environment,
providing natural language annotations for only 10% of demonstrations. It
achieves task completion rates comparable to state-of-the-art models
(outperforming several recent methods with access to ground-truth plans during
training and evaluation) while providing structured and human-readable
high-level plans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Language Learning for Entity Matching. (arXiv:2110.03338v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03338">
<div class="article-summary-box-inner">
<span><p>Transformer-based entity matching methods have significantly moved the state
of the art for less-structured matching tasks such as matching product offers
in e-commerce. In order to excel at these tasks, Transformer-based matching
methods require a decent amount of training pairs. Providing enough training
data can be challenging, especially if a matcher for non-English product
descriptions should be learned. This poster explores along the use case of
matching product offers from different e-shops to which extent it is possible
to improve the performance of Transformer-based matchers by complementing a
small set of training pairs in the target language, German in our case, with a
larger set of English-language training pairs. Our experiments using different
Transformers show that extending the German set with English pairs improves the
matching performance in all cases. The impact of adding the English pairs is
especially high in low-resource settings in which only a rather small number of
non-English pairs is available. As it is often possible to automatically gather
English training pairs from the Web by exploiting schema.org annotations, our
results are relevant for many product matching scenarios targeting low-resource
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training. (arXiv:2110.07731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07731">
<div class="article-summary-box-inner">
<span><p>With the rise of large-scale pre-trained language models, open-domain
question-answering (ODQA) has become an important research topic in NLP. Based
on the popular pre-training fine-tuning approach, we posit that an additional
in-domain pre-training stage using a large-scale, natural, and diverse
question-answering (QA) dataset can be beneficial for ODQA. Consequently, we
propose a novel QA dataset based on the Common Crawl project in this paper.
Using the readily available schema.org annotation, we extract around 130
million multilingual question-answer pairs, including about 60 million English
data-points. With this previously unseen number of natural QA pairs, we
pre-train popular language models to show the potential of large-scale
in-domain pre-training for the task of question-answering. In our experiments,
we find that pre-training question-answering models on our Common Crawl
Question Answering dataset (CCQA) achieves promising results in zero-shot, low
resource and fine-tuned settings across multiple tasks, models and benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Pattern based Black-box Model Watermarking for Automatic Speech Recognition. (arXiv:2110.09814v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09814">
<div class="article-summary-box-inner">
<span><p>As an effective method for intellectual property (IP) protection, model
watermarking technology has been applied on a wide variety of deep neural
networks (DNN), including speech classification models. However, how to design
a black-box watermarking scheme for automatic speech recognition (ASR) models
is still an unsolved problem, which is a significant demand for protecting
remote ASR Application Programming Interface (API) deployed in cloud servers.
Due to conditional independence assumption and label-detection-based evasion
attack risk of ASR models, the black-box model watermarking scheme for speech
classification models cannot apply to ASR models. In this paper, we propose the
first black-box model watermarking framework for protecting the IP of ASR
models. Specifically, we synthesize trigger audios by spreading the speech
clips of model owners over the entire input audios and labeling the trigger
audios with the stego texts, which hides the authorship information with
linguistic steganography. Experiments on the state-of-the-art open-source ASR
system DeepSpeech demonstrate the feasibility of the proposed watermarking
scheme, which is robust against five kinds of attacks and has little impact on
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep convolutional forest: a dynamic deep ensemble approach for spam detection in text. (arXiv:2110.15718v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15718">
<div class="article-summary-box-inner">
<span><p>The increase in people's use of mobile messaging services has led to the
spread of social engineering attacks like phishing, considering that spam text
is one of the main factors in the dissemination of phishing attacks to steal
sensitive data such as credit cards and passwords. In addition, rumors and
incorrect medical information regarding the COVID-19 pandemic are widely shared
on social media leading to people's fear and confusion. Thus, filtering spam
content is vital to reduce risks and threats. Previous studies relied on
machine learning and deep learning approaches for spam classification, but
these approaches have two limitations. Machine learning models require manual
feature engineering, whereas deep neural networks require a high computational
cost. This paper introduces a dynamic deep ensemble model for spam detection
that adjusts its complexity and extracts features automatically. The proposed
model utilizes convolutional and pooling layers for feature extraction along
with base classifiers such as random forests and extremely randomized trees for
classifying texts into spam or legitimate ones. Moreover, the model employs
ensemble learning procedures like boosting and bagging. As a result, the model
achieved high precision, recall, f1-score and accuracy of 98.38\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calculating Question Similarity is Enough: A New Method for KBQA Tasks. (arXiv:2111.07658v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07658">
<div class="article-summary-box-inner">
<span><p>Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with the help of an external knowledge base. The core idea is to find
the link between the internal knowledge behind questions and known triples of
the knowledge base. Traditional KBQA task pipelines contain several steps,
including entity recognition, entity linking, answering selection, etc. In this
kind of pipeline methods, errors in any procedure will inevitably propagate to
the final prediction. To address this challenge, this paper proposes a Corpus
Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for
the KBQA task. The major novelty lies in the design of the new method, wherein
our approach, the knowledge enhanced T5 (kT5) model aims to generate natural
language QA pairs based on Knowledge Graph triples and directly solve the QA by
retrieving the synthetic dataset. The new method can extract more information
about the entities from PLM to improve accuracy and simplify the processes. We
test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that
our method improves the performance of KBQA and the out straight-forward method
is competitive with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combined Scaling for Open-Vocabulary Image Classification. (arXiv:2111.10050v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10050">
<div class="article-summary-box-inner">
<span><p>We present a combined scaling method - named BASIC - that achieves 85.7%
top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from
any labeled ImageNet example. This accuracy surpasses best published similar
models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant
improvements in robustness benchmarks. For instance, on 5 test sets with
natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our
model achieves 84.3% top-1 average accuracy, only a small drop from its
original ImageNet accuracy.
</p>
<p>To achieve these results, we scale up the contrastive learning framework of
CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our
dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x
larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in
parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size
is 65536 which is 2x more than CLIP and 4x more than ALIGN.
</p>
<p>We encountered two main challenges with the scaling rules of BASIC. First,
the main challenge with implementing the combined scaling rules of BASIC is the
limited memory of accelerators, such as GPUs and TPUs. To overcome the memory
limit, we propose two simple methods which make use of gradient checkpointing
and model parallelism. Second, while increasing the dataset size and the model
size has been the defacto method to improve the performance of deep learning
models like BASIC, the effect of a large contrastive batch size on such
contrastive-trained image-text models is not well-understood. To shed light on
the benefits of large contrastive batch sizes, we develop a theoretical
framework which shows that larger contrastive batch sizes lead to smaller
generalization gaps for image-text models such as BASIC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering. (arXiv:2112.02732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02732">
<div class="article-summary-box-inner">
<span><p>Existing KG-augmented models for commonsense question answering primarily
focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge
graphs (KGs). However, they ignore (i) the effectively fusing and reasoning
over question context representations and the KG representations, and (ii)
automatically selecting relevant nodes from the noisy KGs during reasoning. In
this paper, we propose a novel model, JointLK, which solves the above
limitations through the joint reasoning of LM and GNN and the dynamic KGs
pruning mechanism. Specifically, JointLK performs joint reasoning between LM
and GNN through a novel dense bidirectional attention module, in which each
question token attends on KG nodes and each KG node attends on question tokens,
and the two modal representations fuse and update mutually by multi-step
interactions. Then, the dynamic pruning module uses the attention weights
generated by joint reasoning to prune irrelevant KG nodes recursively. We
evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate
its improvements to the existing LM and LM+KG models, as well as its capability
to perform interpretable reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Context-Word Biases in Lexical Semantic Datasets. (arXiv:2112.06733v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06733">
<div class="article-summary-box-inner">
<span><p>State-of-the-art contextualized models eg. BERT use tasks such as WiC and WSD
to evaluate their word-in-context representations. This inherently assumes that
performance in these tasks reflect how well a model represents the coupled word
and context semantics. We question this assumption by presenting the first
quantitative analysis on the context-word interaction required and being tested
in major contextual lexical semantic tasks, taking into account that tasks can
be inherently biased and models can learn spurious correlations from datasets.
To achieve this, we run probing baselines on masked input, based on which we
then propose measures to calculate the degree of context or word biases in a
dataset, and plot existing datasets on a continuum. The analysis were performed
on both models and humans to decouple biases inherent to the tasks and biases
learned from the datasets. We found that, (1) to models, most existing datasets
fall into the extreme ends of the continuum: the retrieval-based tasks and
especially the ones in the medical domain (eg. COMETA) exhibit strong target
word bias while WiC-style tasks and WSD show strong context bias; (2) AM2iCo
and Sense Retrieval show less extreme model biases and challenge a model more
to represent both the context and target words. (3) A similar trend of biases
exists in humans but humans are much less biased compared with models as humans
found semantic judgments more difficult with the masked input, indicating
models are learning spurious correlations. This study demonstrates that with
heavy context or target word biases, models are usually not being tested for
word-in-context representations as such in these tasks and results are
therefore open to misinterpretation. We recommend our framework as a sanity
check for context and target word biases in future task design and model
interpretation in lexical semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework. (arXiv:2112.07522v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07522">
<div class="article-summary-box-inner">
<span><p>Vast efforts have been devoted to creating high-performance few-shot
learners, i.e., large-scale pretrained language models (PLMs) that perform well
with little downstream task training data. Training PLMs has incurred
significant cost, but utilizing the few-shot learners is still challenging due
to their enormous size. This work focuses on a crucial question: How to make
effective use of these few-shot learners? We propose LMTurk, a novel approach
that treats few-shot learners as crowdsourcing workers. The rationale is that
crowdsourcing workers are in fact few-shot learners: They are shown a few
illustrative examples to learn about a task and then start annotating. LMTurk
employs few-shot learners built upon PLMs as workers. We show that the
resulting annotations can be utilized to train models that solve the task well
and are small enough to be deployable in practical scenarios. Active learning
is integrated into LMTurk to reduce the amount of queries made to PLMs,
minimizing the computational cost of running PLM inference passes. Altogether,
LMTurk is an important step towards making effective use of current PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONQRR: Conversational Query Rewriting for Retrieval with Reinforcement Learning. (arXiv:2112.08558v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08558">
<div class="article-summary-box-inner">
<span><p>Compared to standard retrieval tasks, passage retrieval for conversational
question answering (CQA) poses new challenges in understanding the current user
question, as each question needs to be interpreted within the dialogue context.
Moreover, it can be expensive to re-train well-established retrievers such as
search engines that are originally developed for non-conversational queries. To
facilitate their use, we develop a query rewriting model CONQRR that rewrites a
conversational question in the context into a standalone question. It is
trained with a novel reward function to directly optimize towards retrieval
using reinforcement learning and can be adapted to any off-the-shelf retriever.
We show that CONQRR achieves state-of-the-art results on a recent open-domain
CQA dataset containing conversations from three different sources, and is
effective for two different off-the-shelf retrievers. Our extensive analysis
also shows the robustness of CONQRR to out-of-domain dialogues as well as to
zero query rewriting supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Korean-Specific Dataset for Table Question Answering. (arXiv:2201.06223v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06223">
<div class="article-summary-box-inner">
<span><p>Existing question answering systems mainly focus on dealing with text data.
However, much of the data produced daily is stored in the form of tables that
can be found in documents and relational databases, or on the web. To solve the
task of question answering over tables, there exist many datasets for table
question answering written in English, but few Korean datasets. In this paper,
we demonstrate how we construct Korean-specific datasets for table question
answering: Korean tabular dataset is a collection of 1.4M tables with
corresponding descriptions for unsupervised pre-training language models.
Korean table question answering corpus consists of 70k pairs of questions and
answers created by crowd-sourced workers. Subsequently, we then build a
pre-trained language model based on Transformer and fine-tune the model for
table question answering with these datasets. We then report the evaluation
results of our model. We make our datasets publicly available via our GitHub
repository and hope that those datasets will help further studies for question
answering over tables, and for the transformation of table formats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04003">
<div class="article-summary-box-inner">
<span><p>ROUGE is a standard automatic evaluation metric based on n-grams for
sequence-to-sequence tasks, while cross-entropy loss is an essential objective
of neural network language model that optimizes at a unigram level. We present
differentiable n-gram objectives, attempting to alleviate the discrepancy
between training criterion and evaluating criterion. The objective maximizes
the probabilistic weight of matched sub-sequences, and the novelty of our work
is the objective weights the matched sub-sequences equally and does not ceil
the number of matched sub-sequences by the ground truth count of n-grams in
reference sequence. We jointly optimize cross-entropy loss and the proposed
objective, providing decent ROUGE score enhancement over abstractive
summarization dataset CNN/DM and XSum, outperforming alternative n-gram
objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-MoE: Designing Stable and Transferable Sparse Expert Models. (arXiv:2202.08906v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08906">
<div class="article-summary-box-inner">
<span><p>Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12163">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel language identification system based on
conformer layers. We propose an attentive temporal pooling mechanism to allow
the model to carry information in long-form audio via a recurrent form, such
that the inference can be performed in a streaming fashion. Additionally, we
investigate two domain adaptation approaches to allow adapting an existing
language identification model without retraining the model parameters for a new
domain. We perform a comparative study of different model topologies under
different constraints of model size, and find that conformer-based models
significantly outperform LSTM and transformer based models. Our experiments
also show that attentive temporal pooling and domain adaptation improve model
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMO-NLP at SemEval-2022 Task 11: A Knowledge-based System for Multilingual Named Entity Recognition. (arXiv:2203.00545v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00545">
<div class="article-summary-box-inner">
<span><p>The MultiCoNER shared task aims at detecting semantically ambiguous and
complex named entities in short and low-context settings for multiple
languages. The lack of contexts makes the recognition of ambiguous named
entities challenging. To alleviate this issue, our team DAMO-NLP proposes a
knowledge-based system, where we build a multilingual knowledge base based on
Wikipedia to provide related context information to the named entity
recognition (NER) model. Given an input sentence, our system effectively
retrieves related contexts from the knowledge base. The original input
sentences are then augmented with such context information, allowing
significantly better contextualized token representations to be captured. Our
system wins 10 out of 13 tracks in the MultiCoNER shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization. (arXiv:2203.08257v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08257">
<div class="article-summary-box-inner">
<span><p>The IMPRESSIONS section of a radiology report about an imaging study is a
summary of the radiologist's reasoning and conclusions, and it also aids the
referring physician in confirming or excluding certain diagnoses. A cascade of
tasks are required to automatically generate an abstractive summary of the
typical information-rich radiology report. These tasks include acquisition of
salient content from the report and generation of a concise, easily consumable
IMPRESSIONS section. Prior research on radiology report summarization has
focused on single-step end-to-end models -- which subsume the task of salient
content acquisition. To fully explore the cascade structure and explainability
of radiology report summarization, we introduce two innovations. First, we
design a two-step approach: extractive summarization followed by abstractive
summarization. Second, we additionally break down the extractive part into two
independent tasks: extraction of salient (1) sentences and (2) keywords.
Experiments on English radiology reports from two clinical sites show our novel
approach leads to a more precise summary compared to single-step and to
two-step-with-single-extractive-process baselines with an overall improvement
in F1 score Of 3-4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?. (arXiv:2203.08850v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08850">
<div class="article-summary-box-inner">
<span><p>What can pre-trained multilingual sequence-to-sequence models like mBART
contribute to translating low-resource languages? We conduct a thorough
empirical experiment in 10 languages to ascertain this, considering five
factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning
data, (3) the amount of pre-training data in the model, (4) the impact of
domain mismatch, and (5) language typology. In addition to yielding several
heuristics, the experiments form a framework for evaluating the data
sensitivities of machine translation systems. While mBART is robust to domain
differences, its translations for unseen and typologically distant languages
remain below 3.0 BLEU. In answer to our title's question, mBART is not a
low-resource panacea; we therefore encourage shifting the emphasis from new
models to new data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRUE: Re-evaluating Factual Consistency Evaluation. (arXiv:2204.04991v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04991">
<div class="article-summary-box-inner">
<span><p>Grounded text generation systems often generate text that contains factual
inconsistencies, hindering their real-world applicability. Automatic factual
consistency evaluation may help alleviate this limitation by accelerating
evaluation cycles, filtering inconsistent outputs and augmenting training data.
While attracting increasing attention, such evaluation metrics are usually
developed and evaluated in silo for a single task or dataset, slowing their
adoption. Moreover, previous meta-evaluation protocols focused on system-level
correlations with human annotations, which leave the example-level accuracy of
such metrics unclear. In this work, we introduce TRUE: a comprehensive survey
and assessment of factual consistency metrics on a standardized collection of
existing texts from diverse tasks, manually annotated for factual consistency.
Our standardization enables an example-level meta-evaluation protocol that is
more actionable and interpretable than previously reported correlations,
yielding clearer quality measures. Across diverse state-of-the-art metrics and
11 datasets we find that large-scale NLI and question
generation-and-answering-based approaches achieve strong and complementary
results. We recommend those methods as a starting point for model and metric
developers, and hope TRUE will foster progress towards even better evaluation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation. (arXiv:2204.05953v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05953">
<div class="article-summary-box-inner">
<span><p>Sign language recognition and translation first uses a recognition module to
generate glosses from sign language videos and then employs a translation
module to translate glosses into spoken sentences. Most existing works focus on
the recognition step, while paying less attention to sign language translation.
In this work, we propose a task-aware instruction network, namely TIN-SLT, for
sign language translation, by introducing the instruction module and the
learning-based feature fuse strategy into a Transformer network. In this way,
the pre-trained model's language ability can be well explored and utilized to
further boost the translation performance. Moreover, by exploring the
representation space of sign language glosses and target spoken language, we
propose a multi-level data augmentation scheme to adjust the data distribution
of the training set. We conduct extensive experiments on two challenging
benchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method
outperforms former best solutions by 1.65 and 1.42 in terms of BLEU-4. Our code
is published at https://github.com/yongcaoplus/TIN-SLT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Adapt Domain Shifts of Moral Values via Instance Weighting. (arXiv:2204.07603v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07603">
<div class="article-summary-box-inner">
<span><p>Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks. (arXiv:2204.07705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07705">
<div class="article-summary-box-inner">
<span><p>How can we measure the generalization of models to a variety of unseen tasks
when provided with their language instructions? To facilitate progress in this
goal, we introduce Natural-Instructions v2, a benchmark of 1,600+ diverse
language tasks and their expert-written instructions. It covers 70+ distinct
task types, such as tagging, in-filling, and rewriting. These tasks are
collected with contributions of NLP practitioners in the community and through
an iterative peer review process to ensure their quality. With this large and
diverse collection of tasks, we are able to rigorously benchmark cross-task
generalization of models -- training on a subset of tasks and evaluating on the
remaining unseen ones. For instance, we quantify generalization as a function
of various scaling parameters, such as the number of observed tasks, the number
of instances, and model sizes. Based on these insights, we introduce
Tk-Instruct, an encoder-decoder Transformer that is trained to follow a variety
of in-context instructions (plain language task definitions or k-shot examples)
which outperforms existing larger models on our benchmark. We hope this
benchmark facilitates future progress toward more general-purpose language
understanding models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08110">
<div class="article-summary-box-inner">
<span><p>English pretrained language models, which make up the backbone of many modern
NLP systems, require huge amounts of unlabeled training data. These models are
generally presented as being trained only on English text but have been found
to transfer surprisingly well to other languages. We investigate this
phenomenon and find that common English pretraining corpora actually contain
significant amounts of non-English text: even when less than 1% of data is not
English (well within the error rate of strong language classifiers), this leads
to hundreds of millions of foreign language tokens in large-scale datasets. We
then demonstrate that even these small percentages of non-English data
facilitate cross-lingual transfer for models trained on them, with target
language performance strongly correlated to the amount of in-language data seen
during pretraining. In light of these findings, we argue that no model is truly
monolingual when pretrained at scale, which should be considered when
evaluating cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11406">
<div class="article-summary-box-inner">
<span><p>Self-augmentation has received increasing research interest recently to
improve named entity recognition (NER) performance in low-resource scenarios.
Token substitution and mixup are two feasible heterogeneous self-augmentation
techniques for NER that can achieve effective performance with certain
specialized efforts. Noticeably, self-augmentation may introduce potentially
noisy augmented data. Prior research has mainly resorted to heuristic
rule-based constraints to reduce the noise for specific self-augmentation
methods individually. In this paper, we revisit these two typical
self-augmentation methods for NER, and propose a unified meta-reweighting
strategy for them to achieve a natural integration. Our method is easily
extensible, imposing little effort on a specific self-augmentation method.
Experiments on different Chinese and English NER benchmarks show that our token
substitution and mixup method, as well as their integration, can achieve
effective performance improvement. Based on the meta-reweighting mechanism, we
can enhance the advantages of the self-augmentation techniques without much
extra effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeaNF: Weak Supervision with Normalizing Flows. (arXiv:2204.13409v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13409">
<div class="article-summary-box-inner">
<span><p>A popular approach to decrease the need for costly manual annotation of large
data sets is weak supervision, which introduces problems of noisy labels,
coverage and bias. Methods for overcoming these problems have either relied on
discriminative models, trained with cost functions specific to weak
supervision, and more recently, generative models, trying to model the output
of the automatic annotation process. In this work, we explore a novel direction
of generative modeling for weak supervision: Instead of modeling the output of
the annotation process (the labeling function matches), we generatively model
the input-side data distributions (the feature space) covered by labeling
functions. Specifically, we estimate a density for each weak labeling source,
or labeling function, by using normalizing flows. An integral part of our
method is the flow-based modeling of multiple simultaneously matching labeling
functions, and therefore phenomena such as labeling function overlap and
correlations are captured. We analyze the effectiveness and modeling
capabilities on various commonly used weak supervision data sets, and show that
weakly supervised normalizing flows compare favorably to standard weak
supervision baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"My nose is running.""Are you also coughing?": Building A Medical Diagnosis Agent with Interpretable Inquiry Logics. (arXiv:2204.13953v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13953">
<div class="article-summary-box-inner">
<span><p>With the rise of telemedicine, the task of developing Dialogue Systems for
Medical Diagnosis (DSMD) has received much attention in recent years. Different
from early researches that needed to rely on extra human resources and
expertise to help construct the system, recent researches focused on how to
build DSMD in a purely data-driven manner. However, the previous data-driven
DSMD methods largely overlooked the system interpretability, which is critical
for a medical application, and they also suffered from the data sparsity issue
at the same time. In this paper, we explore how to bring interpretability to
data-driven DSMD. Specifically, we propose a more interpretable decision
process to implement the dialogue manager of DSMD by reasonably mimicking real
doctors' inquiry logics, and we devise a model with highly transparent
components to conduct the inference. Moreover, we collect a new DSMD dataset,
which has a much larger scale, more diverse patterns and is of higher quality
than the existing ones. The experiments show that our method obtains 7.7%,
10.0%, 3.0% absolute improvement in diagnosis accuracy respectively on three
datasets, demonstrating the effectiveness of its rational decision process and
model design. Our codes and the GMD-12 dataset are available at
https://github.com/lwgkzl/BR-Agent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Language Models with Natural Language Feedback. (arXiv:2204.14146v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14146">
<div class="article-summary-box-inner">
<span><p>Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level
summarization.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Birds' Eye View: Measuring Behavior and Posture of Chickens as a Metric for Their Well-Being. (arXiv:2205.00069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00069">
<div class="article-summary-box-inner">
<span><p>Chicken well-being is important for ensuring food security and better
nutrition for a growing global human population. In this research, we represent
behavior and posture as a metric to measure chicken well-being. With the
objective of detecting chicken posture and behavior in a pen, we employ two
algorithms: Mask R-CNN for instance segmentation and YOLOv4 in combination with
ResNet50 for classification. Our results indicate a weighted F1 score of 88.46%
for posture and behavior detection using Mask R-CNN and an average of 91%
accuracy in behavior detection and 86.5% average accuracy in posture detection
using YOLOv4. These experiments are conducted under uncontrolled scenarios for
both posture and behavior measurements. These metrics establish a strong
foundation to obtain a decent indication of individual and group behaviors and
postures. Such outcomes would help improve the overall well-being of the
chickens. The dataset used in this research is collected in-house and will be
made public after the publication as it would serve as a very useful resource
for future research. To the best of our knowledge no other research work has
been conducted in this specific setup used for this work involving multiple
behaviors and postures simultaneously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Negative Sampling for Audio-Visual Contrastive Learning from Movies. (arXiv:2205.00073v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00073">
<div class="article-summary-box-inner">
<span><p>The abundance and ease of utilizing sound, along with the fact that auditory
clues reveal a plethora of information about what happens in a scene, make the
audio-visual space an intuitive choice for representation learning. In this
paper, we explore the efficacy of audio-visual self-supervised learning from
uncurated long-form content i.e movies. Studying its differences with
conventional short-form content, we identify a non-i.i.d distribution of data,
driven by the nature of movies. Specifically, we find long-form content to
naturally contain a diverse set of semantic concepts (semantic diversity),
where a large portion of them, such as main characters and environments often
reappear frequently throughout the movie (reoccurring semantic concepts). In
addition, movies often contain content-exclusive artistic artifacts, such as
color palettes or thematic music, which are strong signals for uniquely
distinguishing a movie (non-semantic consistency). Capitalizing on these
observations, we comprehensively study the effect of emphasizing within-movie
negative sampling in a contrastive learning setup. Our view is different from
those of prior works who consider within-video positive sampling, inspired by
the notion of semantic persistency over time, and operate in a short-video
regime. Our empirical findings suggest that, with certain modifications,
training on uncurated long-form videos yields representations which transfer
competitively with the state-of-the-art to a variety of action recognition and
audio classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Method to Boost Human Pose Estimation Accuracy by Correcting the Joint Regressor for the Human3.6m Dataset. (arXiv:2205.00076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00076">
<div class="article-summary-box-inner">
<span><p>Many human pose estimation methods estimate Skinned Multi-Person Linear
(SMPL) models and regress the human joints from these SMPL estimates. In this
work, we show that the most widely used SMPL-to-joint linear layer (joint
regressor) is inaccurate, which may mislead pose evaluation results. To achieve
a more accurate joint regressor, we propose a method to create
pseudo-ground-truth SMPL poses, which can then be used to train an improved
regressor. Specifically, we optimize SMPL estimates coming from a
state-of-the-art method so that its projection matches the silhouettes of
humans in the scene, as well as the ground-truth 2D joint locations. While the
quality of this pseudo-ground-truth is challenging to assess due to the lack of
actual ground-truth SMPL, with the Human 3.6m dataset, we qualitatively show
that our joint locations are more accurate and that our regressor leads to
improved pose estimations results on the test set without any need for
retraining. We release our code and joint regressor at
https://github.com/ubc-vision/joint-regressor-refinement
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Contrastive Learning based Transformer for Lung Nodule Detection. (arXiv:2205.00122v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00122">
<div class="article-summary-box-inner">
<span><p>Early detection of lung nodules with computed tomography (CT) is critical for
the longer survival of lung cancer patients and better quality of life.
Computer-aided detection/diagnosis (CAD) is proven valuable as a second or
concurrent reader in this context. However, accurate detection of lung nodules
remains a challenge for such CAD systems and even radiologists due to not only
the variability in size, location, and appearance of lung nodules but also the
complexity of lung structures. This leads to a high false-positive rate with
CAD, compromising its clinical efficacy. Motivated by recent computer vision
techniques, here we present a self-supervised region-based 3D transformer model
to identify lung nodules among a set of candidate regions. Specifically, a 3D
vision transformer (ViT) is developed that divides a CT image volume into a
sequence of non-overlap cubes, extracts embedding features from each cube with
an embedding layer, and analyzes all embedding features with a self-attention
mechanism for the prediction. To effectively train the transformer model on a
relatively small dataset, the region-based contrastive learning method is used
to boost the performance by pre-training the 3D transformer with public CT
images. Our experiments show that the proposed method can significantly improve
the performance of lung nodule screening in comparison with the commonly used
3D convolutional neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaze-enhanced Crossmodal Embeddings for Emotion Recognition. (arXiv:2205.00129v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00129">
<div class="article-summary-box-inner">
<span><p>Emotional expressions are inherently multimodal -- integrating facial
behavior, speech, and gaze -- but their automatic recognition is often limited
to a single modality, e.g. speech during a phone call. While previous work
proposed crossmodal emotion embeddings to improve monomodal recognition
performance, despite its importance, an explicit representation of gaze was not
included. We propose a new approach to emotion recognition that incorporates an
explicit representation of gaze in a crossmodal emotion embedding framework. We
show that our method outperforms the previous state of the art for both
audio-only and video-only emotion classification on the popular One-Minute
Gradual Emotion Recognition dataset. Furthermore, we report extensive ablation
experiments and provide detailed insights into the performance of different
state-of-the-art gaze representations and integration strategies. Our results
not only underline the importance of gaze for emotion recognition but also
demonstrate a practical and highly effective approach to leveraging gaze
information for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Understand Negation in Video Retrieval. (arXiv:2205.00132v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00132">
<div class="article-summary-box-inner">
<span><p>Negation is a common linguistic skill that allows human to express what we do
NOT want. Naturally, one might expect video retrieval to support
natural-language queries with negation, e.g., finding shots of kids sitting on
the floor and not playing with the dog. However, the state-of-the-art deep
learning based video retrieval models lack such ability, as they are typically
trained on video description datasets such as MSR-VTT and VATEX that lack
negated descriptions. Their retrieved results basically ignore the negator in
the sample query, incorrectly returning videos showing kids playing with the
dog. In this paper, we present the first study on learning to understand
negation in video retrieval and make contributions as follows. First, by
re-purposing two existing datasets, i.e. MSR-VTT and VATEX, we propose a new
evaluation protocol for testing video retrieval with negation. Second, we
propose a learning based method for training a negation-aware video retrieval
model. The key idea is to first construct a soft negative caption for a
specific training video by partially negating its original caption, and then
compute a bidirectionally constrained loss on the triplet. This auxiliary loss
is then weightedly added to a standard retrieval loss. Experiments on the
re-purposed benchmarks show that re-training the CLIP (Contrastive
Language-Image Pre-Training) model by the proposed method clearly improves its
ability to handle queries with negation. In addition, its performance on the
original benchmarks is also improved. Data and source code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representation Learning With Text and Images. (arXiv:2205.00142v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00142">
<div class="article-summary-box-inner">
<span><p>In recent years, multimodal AI has seen an upward trend as researchers are
integrating data of different types such as text, images, speech into modelling
to get the best results. This project leverages multimodal AI and matrix
factorization techniques for representation learning, on text and image data
simultaneously, thereby employing the widely used techniques of Natural
Language Processing (NLP) and Computer Vision. The learnt representations are
evaluated using downstream classification and regression tasks. The methodology
adopted can be extended beyond the scope of this project as it uses
Auto-Encoders for unsupervised representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator. (arXiv:2205.00146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00146">
<div class="article-summary-box-inner">
<span><p>Automatic font generation remains a challenging research issue due to the
large amounts of characters with complicated structures. Typically, only a few
samples can serve as the style/content reference (termed few-shot learning),
which further increases the difficulty to preserve local style patterns or
detailed glyph structures. We investigate the drawbacks of previous studies and
find that a coarse-grained discriminator is insufficient for supervising a font
generator. To this end, we propose a novel Component-Aware Module (CAM), which
supervises the generator to decouple content and style at a more fine-grained
level, \textit{i.e.}, the component level. Different from previous studies
struggling to increase the complexity of generators, we aim to perform more
effective supervision for a relatively simple generator to achieve its full
potential, which is a brand new perspective for font generation. The whole
framework achieves remarkable results by coupling component-level supervision
with adversarial learning, hence we call it Component-Guided GAN, shortly
CG-GAN. Extensive experiments show that our approach outperforms
state-of-the-art one-shot font generation methods. Furthermore, it can be
applied to handwritten word synthesis and scene text image editing, suggesting
the generalization of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnimalTrack: A Large-scale Benchmark for Multi-Animal Tracking in the Wild. (arXiv:2205.00158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00158">
<div class="article-summary-box-inner">
<span><p>Multi-animal tracking (MAT), a multi-object tracking (MOT) problem, is
crucial for animal motion and behavior analysis and has many crucial
applications such as biology, ecology, animal conservation and so forth.
Despite its importance, MAT is largely under-explored compared to other MOT
problems such as multi-human tracking due to the scarcity of large-scale
benchmark. To address this problem, we introduce AnimalTrack, a large-scale
benchmark for multi-animal tracking in the wild. Specifically, AnimalTrack
consists of 58 sequences from a diverse selection of 10 common animal
categories. On average, each sequence comprises of 33 target objects for
tracking. In order to ensure high quality, every frame in AnimalTrack is
manually labeled with careful inspection and refinement. To our best knowledge,
AnimalTrack is the first benchmark dedicated to multi-animal tracking. In
addition, to understand how existing MOT algorithms perform on AnimalTrack and
provide baselines for future comparison, we extensively evaluate 14
state-of-the-art representative trackers. The evaluation results demonstrate
that, not surprisingly, most of these trackers become degenerated due to the
differences between pedestrians and animals in various aspects (e.g., pose,
motion, appearance, etc), and more efforts are desired to improve multi-animal
tracking. We hope that AnimalTrack together with evaluation and analysis will
foster further progress on multi-animal tracking. The dataset and evaluation as
well as our analysis will be made available upon the acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVTR: Scene Text Recognition with a Single Visual Model. (arXiv:2205.00159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00159">
<div class="article-summary-box-inner">
<span><p>Dominant scene text recognition models commonly contain two building blocks,
a visual model for feature extraction and a sequence model for text
transcription. This hybrid architecture, although accurate, is complex and less
efficient. In this study, we propose a Single Visual model for Scene Text
recognition within the patch-wise image tokenization framework, which dispenses
with the sequential modeling entirely. The method, termed SVTR, firstly
decomposes an image text into small patches named character components.
Afterward, hierarchical stages are recurrently carried out by component-level
mixing, merging and/or combining. Global and local mixing blocks are devised to
perceive the inter-character and intra-character patterns, leading to a
multi-grained character component perception. Thus, characters are recognized
by a simple linear prediction. Experimental results on both English and Chinese
scene text recognition tasks demonstrate the effectiveness of SVTR. SVTR-L
(Large) achieves highly competitive accuracy in English and outperforms
existing methods by a large margin in Chinese, while running faster. In
addition, SVTR-T (Tiny) is an effective and much smaller model, which shows
appealing speed at inference. The code is publicly available at
https://github.com/PaddlePaddle/PaddleOCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elucidating Meta-Structures of Noisy Labels in Semantic Segmentation by Deep Neural Networks. (arXiv:2205.00160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00160">
<div class="article-summary-box-inner">
<span><p>The supervised training of deep neural networks (DNNs) by noisy labels has
been studied extensively in image classification but much less in image
segmentation. So far, our understanding of the learning behavior of DNNs
trained by noisy segmentation labels remains limited. In this study, we address
this deficiency in both binary segmentation of biological microscopy images and
multi-class segmentation of natural images. We classify segmentation labels
according to their noise transition matrices (NTM) and compare performance of
DNNs trained by different types of labels. When we randomly sample a small
fraction (e.g., 10%) or flipping a large fraction (e.g., 90%) of the
ground-truth labels to train DNNs, their segmentation performance remains
largely the same. This indicates that DNNs learn structures hidden in labels
rather than pixel-level labels per se in their supervised training for semantic
segmentation. We call these hidden structures "meta-structures". When we use
labels with different perturbations to the meta-structures to train DNNs, their
performance in feature extraction and segmentation degrades consistently. In
contrast, addition of meta-structure information substantially improves
performance of an unsupervised model in binary semantic segmentation. We
formulate meta-structures mathematically as spatial density distributions and
quantify semantic information of different types of labels, which we find to
correlate strongly with ranks of their NTM. We show theoretically and
experimentally how this formulation explains key observed learning behavior of
DNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClusterQ: Semantic Feature Distribution Alignment for Data-Free Quantization. (arXiv:2205.00179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00179">
<div class="article-summary-box-inner">
<span><p>Network quantization has emerged as a promising method for model compression
and inference acceleration. However, tradtional quantization methods (such as
quantization aware training and post training quantization) require original
data for the fine-tuning or calibration of quantized model, which makes them
inapplicable to the cases that original data are not accessed due to privacy or
security. This gives birth to the data-free quantization with synthetic data
generation. While current DFQ methods still suffer from severe performance
degradation when quantizing a model into lower bit, caused by the low
inter-class separability of semantic features. To this end, we propose a new
and effective data-free quantization method termed ClusterQ, which utilizes the
semantic feature distribution alignment for synthetic data generation. To
obtain high inter-class separability of semantic features, we cluster and align
the feature distribution statistics to imitate the distribution of real data,
so that the performance degradation is alleviated. Moreover, we incorporate the
intra-class variance to solve class-wise mode collapse. We also employ the
exponential moving average to update the centroid of each cluster for further
feature distribution improvement. Extensive experiments across various deep
models (e.g., ResNet-18 and MobileNet-V2) over the ImageNet dataset demonstrate
that our ClusterQ obtains state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Label Correction is a Good Booster When Learning with Extremely Noisy Labels. (arXiv:2205.00186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00186">
<div class="article-summary-box-inner">
<span><p>Learning with noisy labels has aroused much research interest since data
annotations, especially for large-scale datasets, may be inevitably imperfect.
Recent approaches resort to a semi-supervised learning problem by dividing
training samples into clean and noisy sets. This paradigm, however, is prone to
significant degeneration under heavy label noise, as the number of clean
samples is too small for conventional methods to behave well. In this paper, we
introduce a novel framework, termed as LC-Booster, to explicitly tackle
learning under extreme noise. The core idea of LC-Booster is to incorporate
label correction into the sample selection, so that more purified samples,
through the reliable label correction, can be utilized for training, thereby
alleviating the confirmation bias. Experiments show that LC-Booster advances
state-of-the-art results on several noisy-label benchmarks, including CIFAR-10,
CIFAR-100, Clothing1M and WebVision. Remarkably, under the extreme 90\% noise
ratio, LC-Booster achieves 93.5\% and 48.4\% accuracy on CIFAR-10 and
CIFAR-100, surpassing the state-of-the-art by 1.6\% and 7.2\% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"And Then There Were None": Cracking White-box DNN Watermarks via Invariant Neuron Transforms. (arXiv:2205.00199v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00199">
<div class="article-summary-box-inner">
<span><p>Recently, how to protect the Intellectual Property (IP) of deep neural
networks (DNN) becomes a major concern for the AI industry. To combat potential
model piracy, recent works explore various watermarking strategies to embed
secret identity messages into the prediction behaviors or the internals (e.g.,
weights and neuron activation) of the target model. Sacrificing less
functionality and involving more knowledge about the target model, the latter
branch of watermarking schemes (i.e., white-box model watermarking) is claimed
to be accurate, credible and secure against most known watermark removal
attacks, with emerging research efforts and applications in the industry.
</p>
<p>In this paper, we present the first effective removal attack which cracks
almost all the existing white-box watermarking schemes with provably no
performance overhead and no required prior knowledge. By analyzing these IP
protection mechanisms at the granularity of neurons, we for the first time
discover their common dependence on a set of fragile features of a local neuron
group, all of which can be arbitrarily tampered by our proposed chain of
invariant neuron transforms. On $9$ state-of-the-art white-box watermarking
schemes and a broad set of industry-level DNN architectures, our attack for the
first time reduces the embedded identity message in the protected models to be
almost random. Meanwhile, unlike known removal attacks, our attack requires no
prior knowledge on the training data distribution or the adopted watermark
algorithms, and leaves model functionality intact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DefakeHop++: An Enhanced Lightweight Deepfake Detector. (arXiv:2205.00211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00211">
<div class="article-summary-box-inner">
<span><p>On the basis of DefakeHop, an enhanced lightweight Deepfake detector called
DefakeHop++ is proposed in this work. The improvements lie in two areas. First,
DefakeHop examines three facial regions (i.e., two eyes and mouth) while
DefakeHop++ includes eight more landmarks for broader coverage. Second, for
discriminant features selection, DefakeHop uses an unsupervised approach while
DefakeHop++ adopts a more effective approach with supervision, called the
Discriminant Feature Test (DFT). In DefakeHop++, rich spatial and spectral
features are first derived from facial regions and landmarks automatically.
Then, DFT is used to select a subset of discriminant features for classifier
training. As compared with MobileNet v3 (a lightweight CNN model of 1.5M
parameters targeting at mobile applications), DefakeHop++ has a model of 238K
parameters, which is 16% of MobileNet v3. Furthermore, DefakeHop++ outperforms
MobileNet v3 in Deepfake image detection performance in a weakly-supervised
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer. (arXiv:2205.00214v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00214">
<div class="article-summary-box-inner">
<span><p>Video denoising aims to recover high-quality frames from the noisy video.
While most existing approaches adopt convolutional neural networks(CNNs) to
separate the noise from the original visual content, however, CNNs focus on
local information and ignore the interactions between long-range regions.
Furthermore, most related works directly take the output after spatio-temporal
denoising as the final result, neglecting the fine-grained denoising process.
In this paper, we propose a Dual-stage Spatial-Channel Transformer (DSCT) for
coarse-to-fine video denoising, which inherits the advantages of both
Transformer and CNNs. Specifically, DSCT is proposed based on a progressive
dual-stage architecture, namely a coarse-level and a fine-level to extract
dynamic feature and static feature, respectively. At both stages, a
Spatial-Channel Encoding Module(SCEM) is designed to model the long-range
contextual dependencies at spatial and channel levels. Meanwhile, we design a
Multi-scale Residual Structure to preserve multiple aspects of information at
different stages, which contains a Temporal Features Aggregation Module(TFAM)
to summarize the dynamic representation. Extensive experiments on four publicly
available datasets demonstrate our proposed DSCT achieves significant
improvements compared to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognising Known Configurations of Garments For Dual-Arm Robotic Flattening. (arXiv:2205.00225v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00225">
<div class="article-summary-box-inner">
<span><p>Robotic deformable-object manipulation is a challenge in the robotic industry
because deformable objects have complicated and various object states.
Predicting those object states and updating manipulation planning are
time-consuming and computationally expensive. In this paper, we propose an
effective robotic manipulation approach for recognising 'known configurations'
of garments with a 'Known Configuration neural Network' (KCNet) and choosing
pre-designed manipulation plans based on the recognised known configurations.
Our robotic manipulation plan features a four-action strategy: finding two
critical grasping points, stretching the garments, and lifting down the
garments. We demonstrate that our approach only needs 98 seconds on average to
flatten garments of five categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Visible-light Images Guided Cross-Spectrum Depth Estimation from Dual-Modality Cameras. (arXiv:2205.00257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00257">
<div class="article-summary-box-inner">
<span><p>Cross-spectrum depth estimation aims to provide a depth map in all
illumination conditions with a pair of dual-spectrum images. It is valuable for
autonomous vehicle applications when the vehicle is equipped with two cameras
of different modalities. However, images captured by different-modality cameras
can be photometrically quite different. Therefore, cross-spectrum depth
estimation is a very challenging problem. Moreover, the shortage of large-scale
open-source datasets also retards further research in this field. In this
paper, we propose an unsupervised visible-light image guided cross-spectrum
(i.e., thermal and visible-light, TIR-VIS in short) depth estimation framework
given a pair of RGB and thermal images captured from a visible-light camera and
a thermal one. We first adopt a base depth estimation network using RGB-image
pairs. Then we propose a multi-scale feature transfer network to transfer
features from the TIR-VIS domain to the VIS domain at the feature level to fit
the trained depth estimation network. At last, we propose a cross-spectrum
depth cycle consistency to improve the depth result of dual-spectrum image
pairs. Meanwhile, we release a large dual-spectrum depth estimation dataset
with visible-light and far-infrared stereo images captured in different scenes
to the society. The experiment result shows that our method achieves better
performance than the compared existing methods. Our datasets is available at
https://github.com/whitecrow1027/VIS-TIR-Datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. (arXiv:2205.00272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00272">
<div class="article-summary-box-inner">
<span><p>Visual grounding is a task to locate the target indicated by a natural
language expression. Existing methods extend the generic object detection
framework to this problem. They base the visual grounding on the features from
pre-generated proposals or anchors, and fuse these features with the text
embeddings to locate the target mentioned by the text. However, modeling the
visual features from these predefined locations may fail to fully exploit the
visual context and attribute information in the text query, which limits their
performance. In this paper, we propose a transformer-based framework for
accurate visual grounding by establishing text-conditioned discriminative
features and performing multi-stage cross-modal reasoning. Specifically, we
develop a visual-linguistic verification module to focus the visual features on
regions relevant to the textual descriptions while suppressing the unrelated
areas. A language-guided feature encoder is also devised to aggregate the
visual contexts of the target object to improve the object's distinctiveness.
To retrieve the target from the encoded visual features, we further propose a
multi-stage cross-modal decoder to iteratively speculate on the correlations
between the image and text for accurate target localization. Extensive
experiments on five widely used datasets validate the efficacy of our proposed
components and demonstrate state-of-the-art performance. Our code is public at
https://github.com/yangli18/VLTVG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Curriculum Learning for Great Ape Detection in the Wild. (arXiv:2205.00275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00275">
<div class="article-summary-box-inner">
<span><p>We propose a novel end-to-end curriculum learning approach that leverages
large volumes of unlabelled great ape camera trap footage to improve supervised
species detector construction in challenging real-world jungle environments. In
contrast to previous semi-supervised methods, our approach gradually improves
detection quality by steering training towards virtuous self-reinforcement. To
achieve this, we propose integrating pseudo-labelling with dynamic curriculum
learning policies. We show that such dynamics and controls can avoid learning
collapse and gradually tie detector adjustments to higher model quality. We
provide theoretical arguments and ablations, and confirm significant
performance improvements against various state-of-the-art systems when
evaluating on the Extended PanAfrican Dataset holding several thousand camera
trap videos of great apes. We note that system performance is strongest for
smaller labelled ratios, which are common in ecological applications. Our
approach, although designed with wildlife data in mind, also shows competitive
benchmarks for generic object detection in the MS-COCO dataset, indicating
wider applicability of introduced concepts. The code is available at
https://github.com/youshyee/DCL-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ONCE-3DLanes: Building Monocular 3D Lane Detection. (arXiv:2205.00301v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00301">
<div class="article-summary-box-inner">
<span><p>We present ONCE-3DLanes, a real-world autonomous driving dataset with lane
layout annotation in 3D space. Conventional 2D lane detection from a monocular
image yields poor performance of following planning and control tasks in
autonomous driving due to the case of uneven road. Predicting the 3D lane
layout is thus necessary and enables effective and safe driving. However,
existing 3D lane detection datasets are either unpublished or synthesized from
a simulated environment, severely hampering the development of this field. In
this paper, we take steps towards addressing these issues. By exploiting the
explicit relationship between point clouds and image pixels, a dataset
annotation pipeline is designed to automatically generate high-quality 3D lane
locations from 2D lane annotations in 211K road scenes. In addition, we present
an extrinsic-free, anchor-free method, called SALAD, regressing the 3D
coordinates of lanes in image view without converting the feature map into the
bird's-eye view (BEV). To facilitate future research on 3D lane detection, we
benchmark the dataset and provide a novel evaluation metric, performing
extensive experiments of both existing approaches and our proposed method. The
aim of our work is to revive the interest of 3D lane detection in a real-world
scenario. We believe our work can lead to the expected and unexpected
innovations in both academia and industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs. (arXiv:2205.00303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00303">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the graphic layout generation problem of producing
high-quality visual-textual presentation designs for given images. We note that
image compositions, which contain not only global semantics but also spatial
information, would largely affect layout results. Hence, we propose a deep
generative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to
synthesize layouts based on the global and spatial visual contents of input
images. To obtain training images from images that already contain manually
designed graphic layout data, previous work suggests masking design elements
(e.g., texts and embellishments) as model inputs, which inevitably leaves hint
of the ground truth. We study the misalignment between the training inputs
(with hint masks) and test inputs (without masks), and design a novel domain
alignment module (DAM) to narrow this gap. For training, we built a large-scale
layout dataset which consists of 60,548 advertising posters with annotated
layout information. To evaluate the generated layouts, we propose three novel
metrics according to aesthetic intuitions. Through both quantitative and
qualitative evaluations, we demonstrate that the proposed model can synthesize
high-quality graphic layouts according to image compositions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source Domain Subset Sampling for Semi-Supervised Domain Adaptation in Semantic Segmentation. (arXiv:2205.00312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00312">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce source domain subset sampling (SDSS) as a new
perspective of semi-supervised domain adaptation. We propose domain adaptation
by sampling and exploiting only a meaningful subset from source data for
training. Our key assumption is that the entire source domain data may contain
samples that are unhelpful for the adaptation. Therefore, the domain adaptation
can benefit from a subset of source data composed solely of helpful and
relevant samples. The proposed method effectively subsamples full source data
to generate a small-scale meaningful subset. Therefore, training time is
reduced, and performance is improved with our subsampled source data. To
further verify the scalability of our method, we construct a new dataset called
Ocean Ship, which comprises 500 real and 200K synthetic sample images with
ground-truth labels. The SDSS achieved a state-of-the-art performance when
applied on GTA5 to Cityscapes and SYNTHIA to Cityscapes public benchmark
datasets and a 9.13 mIoU improvement on our Ocean Ship dataset over a baseline
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutBERT: Masked Language Layout Model for Object Insertion. (arXiv:2205.00347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00347">
<div class="article-summary-box-inner">
<span><p>Image compositing is one of the most fundamental steps in creative workflows.
It involves taking objects/parts of several images to create a new image,
called a composite. Currently, this process is done manually by creating
accurate masks of objects to be inserted and carefully blending them with the
target scene or images, usually with the help of tools such as Photoshop or
GIMP. While there have been several works on automatic selection of objects for
creating masks, the problem of object placement within an image with the
correct position, scale, and harmony remains a difficult problem with limited
exploration. Automatic object insertion in images or designs is a difficult
problem as it requires understanding of the scene geometry and the color
harmony between objects. We propose LayoutBERT for the object insertion task.
It uses a novel self-supervised masked language model objective and
bidirectional multi-head self-attention. It outperforms previous layout-based
likelihood models and shows favorable properties in terms of model capacity. We
demonstrate the effectiveness of our approach for object insertion in the image
compositing setting and other settings like documents and design templates. We
further demonstrate the usefulness of the learned representations for
layout-based retrieval tasks. We provide both qualitative and quantitative
evaluations on datasets from diverse domains like COCO, PublayNet, and two new
datasets which we call Image Layouts and Template Layouts. Image Layouts which
consists of 5.8 million images with layout annotations is the largest image
layout dataset to our knowledge. We also share ablation study results on the
effect of dataset size, model size and class sample size for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Spatial Reasoning. (arXiv:2205.00363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00363">
<div class="article-summary-box-inner">
<span><p>Spatial relations are fundamental to human cognition and are the most basic
knowledge for us to understand and communicate about our physical surroundings.
In this paper, we ask the critical question: Are current vision-and-language
models (VLMs) able to correctly understand spatial relations? To answer this
question, we propose Visual Spatial Reasoning (VSR), a novel benchmark task
with human labelled dataset for investigating VLMs' capabilities in recognising
65 types of spatial relationships (e.g., under, in front of, facing etc.) in
natural text-image pairs. Specifically, given a caption and an image, the model
needs to perform binary classification and decide if the caption accurately
describes the spatial relationships of two objects presented in the image.
While being seemingly simple and straightforward, the task shows a large gap
between human and model performance (human ceiling on the VSR task is above 95%
and models only achieve around 70%). With fine-grained categorisation and
control on both concepts and relations, our VSR benchmark enables us to perform
interesting probing analysis to pinpoint VLMs' failure cases and the reasons
behind. We observe that VLMs' by-relation performances have little correlation
with the number of training examples and the tested models are in general
incapable of recognising relations that concern orientations of objects. Also,
VLMs have poor zero-shot generalisation toward unseen concepts. The dataset and
code are released at github.com/cambridgeltl/visual-spatial-reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RADNet: A Deep Neural Network Model for Robust Perception in Moving Autonomous Systems. (arXiv:2205.00364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00364">
<div class="article-summary-box-inner">
<span><p>Interactive autonomous applications require robustness of the perception
engine to artifacts in unconstrained videos. In this paper, we examine the
effect of camera motion on the task of action detection. We develop a novel
ranking method to rank videos based on the degree of global camera motion. For
the high ranking camera videos we show that the accuracy of action detection is
decreased. We propose an action detection pipeline that is robust to the camera
motion effect and verify it empirically. Specifically, we do actor feature
alignment across frames and couple global scene features with local
actor-specific features. We do feature alignment using a novel formulation of
the Spatio-temporal Sampling Network (STSN) but with multi-scale offset
prediction and refinement using a pyramid structure. We also propose a novel
input dependent weighted averaging strategy for fusing local and global
features. We show the applicability of our network on our dataset of moving
camera videos with high camera motion (MOVE dataset) with a 4.1% increase in
frame mAP and 17% increase in video mAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fractional Vegetation Cover Estimation using Hough Lines and Linear Iterative Clustering. (arXiv:2205.00366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00366">
<div class="article-summary-box-inner">
<span><p>A common requirement of plant breeding programs across the country is
companion planting -- growing different species of plants in close proximity so
they can mutually benefit each other. However, the determination of companion
plants requires meticulous monitoring of plant growth. The technique of ocular
monitoring is often laborious and error prone. The availability of image
processing techniques can be used to address the challenge of plant growth
monitoring and provide robust solutions that assist plant scientists to
identify companion plants. This paper presents a new image processing algorithm
to determine the amount of vegetation cover present in a given area, called
fractional vegetation cover. The proposed technique draws inspiration from the
trusted Daubenmire method for vegetation cover estimation and expands upon it.
Briefly, the idea is to estimate vegetation cover from images containing
multiple rows of plant species growing in close proximity separated by a
multi-segment PVC frame of known size. The proposed algorithm applies a Hough
Transform and Simple Linear Iterative Clustering (SLIC) to estimate the amount
of vegetation cover within each segment of the PVC frame. The analysis when
repeated over images captured at regular intervals of time provides crucial
insights into plant growth. As a means of comparison, the proposed algorithm is
compared with SamplePoint and Canopeo, two trusted applications used for
vegetation cover estimation. The comparison shows a 99% similarity with both
SamplePoint and Canopeo demonstrating the accuracy and feasibility of the
algorithm for fractional vegetation cover estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traffic Context Aware Data Augmentation for Rare Object Detection in Autonomous Driving. (arXiv:2205.00376v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00376">
<div class="article-summary-box-inner">
<span><p>Detection of rare objects (e.g., traffic cones, traffic barrels and traffic
warning triangles) is an important perception task to improve the safety of
autonomous driving. Training of such models typically requires a large number
of annotated data which is expensive and time consuming to obtain. To address
the above problem, an emerging approach is to apply data augmentation to
automatically generate cost-free training samples. In this work, we propose a
systematic study on simple Copy-Paste data augmentation for rare object
detection in autonomous driving. Specifically, local adaptive instance-level
image transformation is introduced to generate realistic rare object masks from
source domain to the target domain. Moreover, traffic scene context is utilized
to guide the placement of masks of rare objects. To this end, our data
augmentation generates training data with high quality and realistic
characteristics by leveraging both local and global consistency. In addition,
we build a new dataset named NM10k consisting 10k training images, 4k
validation images and the corresponding labels with a diverse range of
scenarios in autonomous driving. Experiments on NM10k show that our method
achieves promising results on rare object detection. We also present a thorough
study to illustrate the effectiveness of our local-adaptive and global
constraints based Copy-Paste data augmentation for rare object detection. The
data, development kit and more information of NM10k dataset are available
online at: \url{https://nullmax-vision.github.io}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition. (arXiv:2205.00380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00380">
<div class="article-summary-box-inner">
<span><p>Micro-expression recognition (MER) is valuable because the involuntary nature
of micro-expressions (MEs) can reveal genuine emotions. Most works recognize
MEs by taking RGB videos or images as input. In fact, the activated facial
regions in ME images are very small and the subtle motion can be easily
submerged in the unrelated information. Facial landmarks are a low-dimensional
and compact modality, which leads to much lower computational cost and can
potentially concentrate more on ME-related features. However, the
discriminability of landmarks for MER is not clear. Thus, this paper explores
the contribution of facial landmarks and constructs a new framework to
efficiently recognize MEs with sole facial landmark information. Specially, we
design a separate structure module to separately aggregate the spatial and
temporal information in the geometric movement graph based on facial landmarks,
and a Geometric Two-Stream Graph Network is constructed to aggregate the
low-order geometric information and high-order semantic information of facial
landmarks. Furthermore, two core components are proposed to enhance features.
Specifically, a semantic adjacency matrix can automatically model the
relationship between nodes even long-distance nodes in a self-learning fashion;
and an Adaptive Action Unit loss is introduced to guide the learning process
such that the learned features are forced to have a synchronized pattern with
facial action units. Notably, this work tackles MER only utilizing geometric
features, processed based on a graph model, which provides a new idea with much
higher efficiency to promote MER. The experimental results demonstrate that the
proposed method can achieve competitive or even superior performance with a
significantly reduced computational cost, and facial landmarks can
significantly contribute to MER and are worth further study for efficient ME
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convex Combination Consistency between Neighbors for Weakly-supervised Action Localization. (arXiv:2205.00400v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00400">
<div class="article-summary-box-inner">
<span><p>In weakly-supervised temporal action localization (WS-TAL), the methods
commonly follow the "localization by classification" procedure, which uses the
snippet predictions to form video class scores and then optimizes a video
classification loss. In this procedure, the snippet predictions (or snippet
attention weights) are used to separate foreground and background. However, the
snippet predictions are usually inaccurate due to absence of frame-wise labels,
and then the overall performance is hindered. In this paper, we propose a novel
C$^3$BN to achieve robust snippet predictions. C$^3$BN includes two key designs
by exploring the inherent characteristics of video data. First, because of the
natural continuity of adjacent snippets, we propose a micro data augmentation
strategy to increase the diversity of snippets with convex combination of
adjacent snippets. Second, we propose a macro-micro consistency regularization
strategy to force the model to be invariant (or equivariant) to the
transformations of snippets with respect to video semantics, snippet
predictions and snippet features. Experimental results demonstrate the
effectiveness of our proposed method on top of baselines for the WS-TAL tasks
with video-level and point-level supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions. (arXiv:2205.00415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00415">
<div class="article-summary-box-inner">
<span><p>In recent years, progress in NLU has been driven by benchmarks. These
benchmarks are typically collected by crowdsourcing, where annotators write
examples based on annotation instructions crafted by dataset creators. In this
work, we hypothesize that annotators pick up on patterns in the crowdsourcing
instructions, which bias them to write similar examples that are then
over-represented in the collected data. We study this form of bias, termed
instruction bias, in 14 recent NLU benchmarks, showing that instruction
examples often exhibit concrete patterns, which are propagated by crowdworkers
to the collected data. This extends previous work (Geva et al., 2019) and
raises a new concern of whether we are modeling the dataset creator's
instructions, rather than the task. Through a series of experiments, we show
that, indeed, instruction bias can lead to overestimation of model performance,
and that models struggle to generalize beyond biases originating in the
crowdsourcing instructions. We further analyze the influence of instruction
bias in terms of pattern frequency and model size, and derive concrete
recommendations for creating future NLU benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog. (arXiv:2205.00423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00423">
<div class="article-summary-box-inner">
<span><p>Visual Dialog aims to answer multi-round, interactive questions based on the
dialog history and image content. Existing methods either consider answer
ranking and generating individually or only weakly capture the relation across
the two tasks implicitly by two separate models. The research on a universal
framework that jointly learns to rank and generate answers in a single model is
seldom explored. In this paper, we propose a contrastive learning-based
framework UTC to unify and facilitate both discriminative and generative tasks
in visual dialog with a single model. Specifically, considering the inherent
limitation of the previous learning paradigm, we devise two inter-task
contrastive losses i.e., context contrastive loss and answer contrastive loss
to make the discriminative and generative tasks mutually reinforce each other.
These two complementary contrastive losses exploit dialog context and target
answer as anchor points to provide representation learning signals from
different perspectives. We evaluate our proposed UTC on the VisDial v1.0
dataset, where our method outperforms the state-of-the-art on both
discriminative and generative tasks and surpasses previous state-of-the-art
generative methods by more than 2 absolute points on Recall@1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Diffractive Neural Networks for Seeing Through Random Diffusers. (arXiv:2205.00428v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00428">
<div class="article-summary-box-inner">
<span><p>Imaging through diffusive media is a challenging problem, where the existing
solutions heavily rely on digital computers to reconstruct distorted images. We
provide a detailed analysis of a computer-free, all-optical imaging method for
seeing through random, unknown phase diffusers using diffractive neural
networks, covering different deep learning-based training strategies. By
analyzing various diffractive networks designed to image through random
diffusers with different correlation lengths, a trade-off between the image
reconstruction fidelity and distortion reduction capability of the diffractive
network was observed. During its training, random diffusers with a range of
correlation lengths were used to improve the diffractive network's
generalization performance. Increasing the number of random diffusers used in
each epoch reduced the overfitting of the diffractive network's imaging
performance to known diffusers. We also demonstrated that the use of additional
diffractive layers improved the generalization capability to see through new,
random diffusers. Finally, we introduced deliberate misalignments in training
to 'vaccinate' the network against random layer-to-layer shifts that might
arise due to the imperfect assembly of the diffractive networks. These analyses
provide a comprehensive guide in designing diffractive networks to see through
random diffusers, which might profoundly impact many fields, such as biomedical
imaging, atmospheric physics, and autonomous driving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Swin-Convs Transformer for Underwater Image Enhancement. (arXiv:2205.00434v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00434">
<div class="article-summary-box-inner">
<span><p>Underwater Image Enhancement (UIE) technology aims to tackle the challenge of
restoring the degraded underwater images due to light absorption and
scattering. To address problems, a novel U-Net based Reinforced Swin-Convs
Transformer for the Underwater Image Enhancement method (URSCT-UIE) is
proposed. Specifically, with the deficiency of U-Net based on pure
convolutions, we embedded the Swin Transformer into U-Net for improving the
ability to capture the global dependency. Then, given the inadequacy of the
Swin Transformer capturing the local attention, the reintroduction of
convolutions may capture more local attention. Thus, we provide an ingenious
manner for the fusion of convolutions and the core attention mechanism to build
a Reinforced Swin-Convs Transformer Block (RSCTB) for capturing more local
attention, which is reinforced in the channel and the spatial attention of the
Swin Transformer. Finally, the experimental results on available datasets
demonstrate that the proposed URSCT-UIE achieves state-of-the-art performance
compared with other methods in terms of both subjective and objective
evaluations. The code will be released on GitHub after acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset-free Deep learning Method for Low-Dose CT Image Reconstruction. (arXiv:2205.00463v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00463">
<div class="article-summary-box-inner">
<span><p>Low-dose CT (LDCT) imaging attracted a considerable interest for the
reduction of the object's exposure to X-ray radiation. In recent years,
supervised deep learning has been extensively studied for LDCT image
reconstruction, which trains a network over a dataset containing many pairs of
normal-dose and low-dose images. However, the challenge on collecting many such
pairs in the clinical setup limits the application of such
supervised-learning-based methods for LDCT image reconstruction in practice.
Aiming at addressing the challenges raised by the collection of training
dataset, this paper proposed a unsupervised deep learning method for LDCT image
reconstruction, which does not require any external training data. The proposed
method is built on a re-parametrization technique for Bayesian inference via
deep network with random weights, combined with additional total variational
(TV) regularization. The experiments show that the proposed method noticeably
outperforms existing dataset-free image reconstruction methods on the test
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preserve Pre-trained Knowledge: Transfer Learning With Self-Distillation For Action Recognition. (arXiv:2205.00506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00506">
<div class="article-summary-box-inner">
<span><p>Video-based action recognition is one of the most popular topics in computer
vision. With recent advances of selfsupervised video representation learning
approaches, action recognition usually follows a two-stage training framework,
i.e., self-supervised pre-training on large-scale unlabeled sets and transfer
learning on a downstream labeled set. However, catastrophic forgetting of the
pre-trained knowledge becomes the main issue in the downstream transfer
learning of action recognition, resulting in a sub-optimal solution. In this
paper, to alleviate the above issue, we propose a novel transfer learning
approach that combines self-distillation in fine-tuning to preserve knowledge
from the pre-trained model learned from the large-scale dataset. Specifically,
we fix the encoder from the last epoch as the teacher model to guide the
training of the encoder from the current epoch in the transfer learning. With
such a simple yet effective learning strategy, we outperform state-of-the-art
methods on widely used UCF101 and HMDB51 datasets in action recognition task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Best of Both Worlds: Combining Model-based and Nonparametric Approaches for 3D Human Body Estimation. (arXiv:2205.00508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00508">
<div class="article-summary-box-inner">
<span><p>Nonparametric based methods have recently shown promising results in
reconstructing human bodies from monocular images while model-based methods can
help correct these estimates and improve prediction. However, estimating model
parameters from global image features may lead to noticeable misalignment
between the estimated meshes and image evidence. To address this issue and
leverage the best of both worlds, we propose a framework of three consecutive
modules. A dense map prediction module explicitly establishes the dense UV
correspondence between the image evidence and each part of the body model. The
inverse kinematics module refines the key point prediction and generates a
posed template mesh. Finally, a UV inpainting module relies on the
corresponding feature, prediction and the posed template, and completes the
predictions of occluded body shape. Our framework leverages the best of
non-parametric and model-based methods and is also robust to partial occlusion.
Experiments demonstrate that our framework outperforms existing 3D human
estimation methods on multiple public benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep vs. Shallow Learning: A Benchmark Study in Low Magnitude Earthquake Detection. (arXiv:2205.00525v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00525">
<div class="article-summary-box-inner">
<span><p>While deep learning models have seen recent high uptake in the geosciences,
and are appealing in their ability to learn from minimally processed input
data, as black box models they do not provide an easy means to understand how a
decision is reached, which in safety-critical tasks especially can be
problematical. An alternative route is to use simpler, more transparent white
box models, in which task-specific feature construction replaces the more
opaque feature discovery process performed automatically within deep learning
models. Using data from the Groningen Gas Field in the Netherlands, we build on
an existing logistic regression model by the addition of four further features
discovered using elastic net driven data mining within the catch22 time series
analysis package. We then evaluate the performance of the augmented logistic
regression model relative to a deep (CNN) model, pre-trained on the Groningen
data, on progressively increasing noise-to-signal ratios. We discover that, for
each ratio, our logistic regression model correctly detects every earthquake,
while the deep model fails to detect nearly 20 % of seismic events, thus
justifying at least a degree of caution in the application of deep models,
especially to data with higher noise-to-signal ratios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COUCH: Towards Controllable Human-Chair Interactions. (arXiv:2205.00541v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00541">
<div class="article-summary-box-inner">
<span><p>Humans interact with an object in many different ways by making contact at
different locations, creating a highly complex motion space that can be
difficult to learn, particularly when synthesizing such human interactions in a
controllable manner. Existing works on synthesizing human scene interaction
focus on the high-level control of action but do not consider the fine-grained
control of motion. In this work, we study the problem of synthesizing scene
interactions conditioned on different contact positions on the object. As a
testbed to investigate this new problem, we focus on human-chair interaction as
one of the most common actions which exhibit large variability in terms of
contacts. We propose a novel synthesis framework COUCH that plans ahead the
motion by predicting contact-aware control signals of the hands, which are then
used to synthesize contact-conditioned interactions. Furthermore, we contribute
a large human-chair interaction dataset with clean annotations, the COUCH
Dataset. Our method shows significant quantitative and qualitative improvements
over existing methods for human-object interactions. More importantly, our
method enables control of the motion through user-specified or automatically
predicted contacts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using a novel fractional-order gradient method for CNN back-propagation. (arXiv:2205.00581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00581">
<div class="article-summary-box-inner">
<span><p>Computer-aided diagnosis tools have experienced rapid growth and development
in recent years. Among all, deep learning is the most sophisticated and popular
tool. In this paper, researchers propose a novel deep learning model and apply
it to COVID-19 diagnosis. Our model uses the tool of fractional calculus, which
has the potential to improve the performance of gradient methods. To this end,
the researcher proposes a fractional-order gradient method for the
back-propagation of convolutional neural networks based on the Caputo
definition. However, if only the first term of the infinite series of the
Caputo definition is used to approximate the fractional-order derivative, the
length of the memory is truncated. Therefore, the fractional-order gradient
(FGD) method with a fixed memory step and an adjustable number of terms is used
to update the weights of the layers. Experiments were performed on the COVIDx
dataset to demonstrate fast convergence, good accuracy, and the ability to
bypass the local optimal point. We also compared the performance of the
developed fractional-order neural networks and Integer-order neural networks.
The results confirmed the effectiveness of our proposed model in the diagnosis
of COVID-19.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries. (arXiv:2205.00613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00613">
<div class="article-summary-box-inner">
<span><p>Accurate and consistent 3D tracking from multiple cameras is a key component
in a vision-based autonomous driving system. It involves modeling 3D dynamic
objects in complex scenes across multiple cameras. This problem is inherently
challenging due to depth estimation, visual occlusions, appearance ambiguity,
etc. Moreover, objects are not consistently associated across time and cameras.
To address that, we propose an end-to-end \textbf{MU}lti-camera
\textbf{TR}acking framework called MUTR3D. In contrast to prior works, MUTR3D
does not explicitly rely on the spatial and appearance similarity of objects.
Instead, our method introduces \textit{3D track query} to model spatial and
appearance coherent track for each object that appears in multiple cameras and
multiple frames. We use camera transformations to link 3D trackers with their
observations in 2D images. Each tracker is further refined according to the
features that are obtained from camera images. MUTR3D uses a set-to-set loss to
measure the difference between the predicted tracking results and the ground
truths. Therefore, it does not require any post-processing such as non-maximum
suppression and/or bounding box association. MUTR3D outperforms
state-of-the-art methods by 5.3 AMOTA on the nuScenes dataset. Code is
available at: \url{https://github.com/a1600012888/MUTR3D}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFC: Anatomically Informed Fiber Clustering with Self-supervised Deep Learning for Fast and Effective Tractography Parcellation. (arXiv:2205.00627v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00627">
<div class="article-summary-box-inner">
<span><p>White matter fiber clustering (WMFC) parcellates tractography data into
anatomically meaningful fiber bundles, usually in an unsupervised manner
without the need of labeled ground truth data. While widely used WMFC
approaches have shown good performance using classical machine learning
techniques, recent advances in deep learning reveal a promising direction
towards fast and effective WMFC. In this work, we propose a novel deep learning
framework for WMFC, Deep Fiber Clustering (DFC), which solves the unsupervised
clustering problem as a self-supervised learning task with a domain-specific
pretext task to predict pairwise fiber distances. This accelerates the fiber
representation learning to handle a known challenge in WMFC, i.e., the
sensitivity of clustering results to the point ordering along fibers. We design
a novel network architecture that represents input fibers as point clouds and
allows the incorporation of additional sources of input information from gray
matter parcellation. Thus DFC makes use of the combined white matter fiber
geometry and gray matter anatomical parcellation to improve anatomical
coherence of fiber clusters. In addition, DFC conducts outlier removal in a
natural way by rejecting fibers with low cluster assignment probabilities. We
evaluate DFC on three independently acquired cohorts (including data from 220
subjects) and compare it to several state-of-the-art WMFC algorithms.
Experimental results demonstrate superior performance of DFC in terms of
cluster compactness, generalization ability, anatomical coherence, and
computational efficiency. In addition, DFC parcellates whole brain tractography
with 50k fibers in about 1.5 minutes, providing a fast and efficient tool for
large data analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design equivariant neural networks for 3D point cloud. (arXiv:2205.00630v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00630">
<div class="article-summary-box-inner">
<span><p>This work seeks to improve the generalization and robustness of existing
neural networks for 3D point clouds by inducing group equivariance under
general group transformations. The main challenge when designing equivariant
models for point clouds is how to trade-off the performance of the model and
the complexity. Existing equivariant models are either too complicate to
implement or very high complexity. The main aim of this study is to build a
general procedure to introduce group equivariant property to SOTA models for 3D
point clouds. The group equivariant models built form our procedure are simple
to implement, less complexity in comparison with the existing ones, and they
preserve the strengths of the original SOTA backbone. From the results of the
experiments on object classification, it is shown that our methods are superior
to other group equivariant models in performance and complexity. Moreover, our
method also helps to improve the mIoU of semantic segmentation models. Overall,
by using a combination of only-finite-rotation equivariance and augmentation,
our models can outperform existing full $SO(3)$-equivariance models with much
cheaper complexity and GPU memory. The proposed procedure is general and forms
a fundamental approach to group equivariant neural networks. We believe that it
can be easily adapted to other SOTA models in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Adversarial Training with Feature Separability. (arXiv:2205.00637v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00637">
<div class="article-summary-box-inner">
<span><p>Deep Neural Network (DNN) are vulnerable to adversarial attacks. As a
countermeasure, adversarial training aims to achieve robustness based on the
min-max optimization problem and it has shown to be one of the most effective
defense strategies. However, in this work, we found that compared with natural
training, adversarial training fails to learn better feature representations
for either clean or adversarial samples, which can be one reason why
adversarial training tends to have severe overfitting issues and less satisfied
generalize performance. Specifically, we observe two major shortcomings of the
features learned by existing adversarial training methods:(1) low intra-class
feature similarity; and (2) conservative inter-classes feature variance. To
overcome these shortcomings, we introduce a new concept of adversarial training
graph (ATG) with which the proposed adversarial training with feature
separability (ATFS) enables to coherently boost the intra-class feature
similarity and increase inter-class feature variance. Through comprehensive
experiments, we demonstrate that the proposed ATFS framework significantly
improves both clean and robust performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Application to Generate Style Guided Compatible Outfit. (arXiv:2205.00663v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00663">
<div class="article-summary-box-inner">
<span><p>Fashion recommendation has witnessed a phenomenal growth of research,
particularly in the domains of shop-the-look, contextaware outfit creation,
personalizing outfit creation etc. Majority of the work in this area focuses on
better understanding of the notion of complimentary relationship between
lifestyle items. Quite recently, some works have realised that style plays a
vital role in fashion, especially in the understanding of compatibility
learning and outfit creation. In this paper, we would like to present the
end-to-end design of a methodology in which we aim to generate outfits guided
by styles or themes using a novel style encoder network. We present an
extensive analysis of different aspects of our method through various
experiments. We also provide a demonstration api to showcase the ability of our
work in generating outfits based on an anchor item and styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Classical Multiclass Linear Discriminant Analysis with a Novel Prototype-based Interpretable Solution. (arXiv:2205.00668v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00668">
<div class="article-summary-box-inner">
<span><p>Linear discriminant analysis (LDA) is a fundamental method for feature
extraction and dimensionality reduction. Despite having many variants,
classical LDA has its importance, as it is a keystone in human knowledge about
pattern recognition. For a dataset containing $C$ clusters, the classical
solution to LDA extracts at most $C-1$ features. In this paper, we introduce a
novel solution to classical LDA, called LDA++, that yields $C$ features, each
one interpretable as measuring similarity to one cluster. This novel solution
bridges between dimensionality reduction and multiclass classification.
Specifically, we prove that, under some mild conditions, the optimal weights of
a linear multiclass classifier for homoscedastic Gaussian data also make an
optimal solution to LDA. In addition, this novel interpretable solution reveals
some new facts about LDA and its relation with PCA. We provide a complete
numerical solution for our novel method, covering the cases 1) when the scatter
matrices can be constructed explicitly, 2) when constructing the scatter
matrices is infeasible, and 3) the kernel extension. The code is available at
https://github.com/k-ghiasi/LDA-plus-plus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Video Harmonization with Color Mapping Consistency. (arXiv:2205.00687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00687">
<div class="article-summary-box-inner">
<span><p>Video harmonization aims to adjust the foreground of a composite video to
make it compatible with the background. So far, video harmonization has only
received limited attention and there is no public dataset for video
harmonization. In this work, we construct a new video harmonization dataset
HYouTube by adjusting the foreground of real videos to create synthetic
composite videos. Moreover, we consider the temporal consistency in video
harmonization task. Unlike previous works which establish the spatial
correspondence, we design a novel framework based on the assumption of color
mapping consistency, which leverages the color mapping of neighboring frames to
refine the current frame. Extensive experiments on our HYouTube dataset prove
the effectiveness of our proposed framework. Our dataset and code are available
at https://github.com/bcmi/Video-Harmonization-Dataset-HYouTube.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Graph Message Passing Networks. (arXiv:1908.06955v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.06955">
<div class="article-summary-box-inner">
<span><p>Modelling long-range dependencies is critical for scene understanding tasks
in computer vision. Although convolution neural networks (CNNs) have excelled
in many vision tasks, they are still limited in capturing long-range structured
relationships as they typically consist of layers of local kernels. A
fully-connected graph, such as the self-attention operation in Transformers, is
beneficial for such modelling, however, its computational overhead is
prohibitive. In this paper, we propose a dynamic graph message passing network,
that significantly reduces the computational complexity compared to related
works modelling a fully-connected graph. This is achieved by adaptively
sampling nodes in the graph, conditioned on the input, for message passing.
Based on the sampled nodes, we dynamically predict node-dependent filter
weights and the affinity matrix for propagating information between them. This
formulation allows us to design a self-attention module, and more importantly a
new Transformer-based backbone network, that we use for both image
classification pretraining, and for addressing various downstream tasks (e.g.
object detection, instance and semantic segmentation). Using this model, we
show significant improvements with respect to strong, state-of-the-art
baselines on four different tasks. Our approach also outperforms
fully-connected graphs while using substantially fewer floating-point
operations and parameters. Code and models will be made publicly available at
https://github.com/fudan-zvg/DGMN2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perception and Navigation in Autonomous Systems in the Era of Learning: A Survey. (arXiv:2001.02319v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.02319">
<div class="article-summary-box-inner">
<span><p>Autonomous systems possess the features of inferring their own state,
understanding their surroundings, and performing autonomous navigation. With
the applications of learning systems, like deep learning and reinforcement
learning, the visual-based self-state estimation, environment perception and
navigation capabilities of autonomous systems have been efficiently addressed,
and many new learning-based algorithms have surfaced with respect to autonomous
visual perception and navigation. In this review, we focus on the applications
of learning-based monocular approaches in ego-motion perception, environment
perception and navigation in autonomous systems, which is different from
previous reviews that discussed traditional methods. First, we delineate the
shortcomings of existing classical visual simultaneous localization and mapping
(vSLAM) solutions, which demonstrate the necessity to integrate deep learning
techniques. Second, we review the visual-based environmental perception and
understanding methods based on deep learning, including deep learning-based
monocular depth estimation, monocular ego-motion prediction, image enhancement,
object detection, semantic segmentation, and their combinations with
traditional vSLAM frameworks. Then, we focus on the visual navigation based on
learning systems, mainly including reinforcement learning and deep
reinforcement learning. Finally, we examine several challenges and promising
directions discussed and concluded in related research of learning systems in
the era of computer science and robotics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Frame based Deep View Synchronization for Unsynchronized Multi-Camera Surveillance. (arXiv:2007.03891v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03891">
<div class="article-summary-box-inner">
<span><p>Multi-camera surveillance has been an active research topic for understanding
and modeling scenes. Compared to a single camera, multi-cameras provide larger
field-of-view and more object cues, and the related applications are multi-view
counting, multi-view tracking, 3D pose estimation or 3D reconstruction, etc. It
is usually assumed that the cameras are all temporally synchronized when
designing models for these multi-camera based tasks. However, this assumption
is not always valid,especially for multi-camera systems with network
transmission delay and low frame-rates due to limited network bandwidth,
resulting in desynchronization of the captured frames across cameras. To handle
the issue of unsynchronized multi-cameras, in this paper, we propose a
synchronization model that works in conjunction with existing DNN-based
multi-view models, thus avoiding the redesign of the whole model. Under the
low-fps regime, we assume that only a single relevant frame is available from
each view, and synchronization is achieved by matching together image contents
guided by epipolar geometry. We consider two variants of the model, based on
where in the pipeline the synchronization occurs, scene-level synchronization
and camera-level synchronization. The view synchronization step and the
task-specific view fusion and prediction step are unified in the same framework
and trained in an end-to-end fashion. Our view synchronization models are
applied to different DNNs-based multi-camera vision tasks under the
unsynchronized setting, including multi-view counting and 3D pose estimation,
and achieve good performance compared to baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Scale Recovery for Monocular Depth and Egomotion Estimation. (arXiv:2009.03787v5 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.03787">
<div class="article-summary-box-inner">
<span><p>The self-supervised loss formulation for jointly training depth and egomotion
neural networks with monocular images is well studied and has demonstrated
state-of-the-art accuracy. One of the main limitations of this approach,
however, is that the depth and egomotion estimates are only determined up to an
unknown scale. In this paper, we present a novel scale recovery loss that
enforces consistency between a known camera height and the estimated camera
height, generating metric (scaled) depth and egomotion predictions. We show
that our proposed method is competitive with other scale recovery techniques
that require more information. Further, we demonstrate that our method
facilitates network retraining within new environments, whereas other
scale-resolving approaches are incapable of doing so. Notably, our egomotion
network is able to produce more accurate estimates than a similar method which
recovers scale at test time only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-tailed Recognition by Routing Diverse Distribution-Aware Experts. (arXiv:2010.01809v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01809">
<div class="article-summary-box-inner">
<span><p>Natural data are often long-tail distributed over semantic classes. Existing
recognition methods tackle this imbalanced classification by placing more
emphasis on the tail data, through class re-balancing/re-weighting or
ensembling over different data groups, resulting in increased tail accuracies
but reduced head accuracies.
</p>
<p>We take a dynamic view of the training data and provide a principled model
bias and variance analysis as the training data fluctuates: Existing long-tail
classifiers invariably increase the model variance and the head-tail model bias
gap remains large, due to more and larger confusion with hard negatives for the
tail.
</p>
<p>We propose a new long-tailed classifier called RoutIng Diverse Experts
(RIDE). It reduces the model variance with multiple experts, reduces the model
bias with a distribution-aware diversity loss, reduces the computational cost
with a dynamic expert routing module. RIDE outperforms the state-of-the-art by
5% to 7% on CIFAR100-LT, ImageNet-LT and iNaturalist 2018 benchmarks. It is
also a universal framework that is applicable to various backbone networks,
long-tailed algorithms, and training mechanisms for consistent performance
gains. Our code is available at:
https://github.com/frank-xwang/RIDE-LongTailRecognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shedding Light on Blind Spots: Developing a Reference Architecture to Leverage Video Data for Process Mining. (arXiv:2010.11289v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11289">
<div class="article-summary-box-inner">
<span><p>Process mining is one of the most active research streams in business process
management. In recent years, numerous methods have been proposed for analyzing
structured process data. Yet, in many cases, it is only the digitized parts of
processes that are directly captured from process-aware information systems,
and manual activities often result in blind spots. While the use of video
cameras to observe these activities could help to fill this gap, a standardized
approach to extracting event logs from unstructured video data remains lacking.
Here, we propose a reference architecture to bridge the gap between computer
vision and process mining. Various evaluation activities (i.e., competing
artifact analysis, prototyping, and real-world application) ensured that the
proposed reference architecture allows flexible, use-case-driven, and
context-specific instantiations. Our results also show that an exemplary
software prototype instantiation of the proposed reference architecture is
capable of automatically extracting most of the process-relevant events from
unstructured video data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wide-Area Crowd Counting: Multi-View Fusion Networks for Counting in Large Scenes. (arXiv:2012.00946v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00946">
<div class="article-summary-box-inner">
<span><p>Crowd counting in single-view images has achieved outstanding performance on
existing counting datasets. However, single-view counting is not applicable to
large and wide scenes (e.g., public parks, long subway platforms, or event
spaces) because a single camera cannot capture the whole scene in adequate
detail for counting, e.g., when the scene is too large to fit into the
field-of-view of the camera, too long so that the resolution is too low on
faraway crowds, or when there are too many large objects that occlude large
portions of the crowd. Therefore, to solve the wide-area counting task requires
multiple cameras with overlapping fields-of-view. In this paper, we propose a
deep neural network framework for multi-view crowd counting, which fuses
information from multiple camera views to predict a scene-level density map on
the ground-plane of the 3D world. We consider three versions of the fusion
framework: the late fusion model fuses camera-view density map; the naive early
fusion model fuses camera-view feature maps; and the multi-view multi-scale
early fusion model ensures that features aligned to the same ground-plane point
have consistent scales. A rotation selection module further ensures consistent
rotation alignment of the features. We test our 3 fusion models on 3 multi-view
counting datasets, PETS2009, DukeMTMC, and a newly collected multi-view
counting dataset containing a crowded street intersection. Our methods achieve
state-of-the-art results compared to other multi-view counting baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CosSGD: Communication-Efficient Federated Learning with a Simple Cosine-Based Quantization. (arXiv:2012.08241v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08241">
<div class="article-summary-box-inner">
<span><p>Federated learning is a promising framework to mitigate data privacy and
computation concerns. However, the communication cost between the server and
clients has become the major bottleneck for successful deployment. Despite
notable progress in gradient compression, the existing quantization methods
require further improvement when low-bits compression is applied, especially
the overall systems often degenerate a lot when quantization are applied in
double directions to compress model weights and gradients. In this work, we
propose a simple cosine-based nonlinear quantization and achieve impressive
results in compressing round-trip communication costs. We are not only able to
compress model weights and gradients at higher ratios than previous methods,
but also achieve competing model performance at the same time. Further, our
approach is highly suitable for federated learning problems since it has low
computational complexity and requires only a little additional data to recover
the compressed information. Extensive experiments have been conducted on image
classification and brain tumor semantic segmentation using the CIFAR-10, and
BraTS datasets where we show state-of-the-art effectiveness and impressive
communication efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slimmable Compressive Autoencoders for Practical Neural Image Compression. (arXiv:2103.15726v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15726">
<div class="article-summary-box-inner">
<span><p>Neural image compression leverages deep neural networks to outperform
traditional image codecs in rate-distortion performance. However, the resulting
models are also heavy, computationally demanding and generally optimized for a
single rate, limiting their practical use. Focusing on practical image
compression, we propose slimmable compressive autoencoders (SlimCAEs), where
rate (R) and distortion (D) are jointly optimized for different capacities.
Once trained, encoders and decoders can be executed at different capacities,
leading to different rates and complexities. We show that a successful
implementation of SlimCAEs requires suitable capacity-specific RD tradeoffs.
Our experiments show that SlimCAEs are highly flexible models that provide
excellent rate-distortion performance, variable rate, and dynamic adjustment of
memory, computational cost and latency, thus addressing the main requirements
of practical image compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Generalized Face Presentation Attack Detection. (arXiv:2104.06595v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06595">
<div class="article-summary-box-inner">
<span><p>Face presentation attack detection plays a critical role in the modern face
recognition pipeline. A face presentation attack detection model with good
generalization can be obtained when it is trained with face images from
different input distributions and different types of spoof attacks. In reality,
training data (both real face images and spoof images) are not directly shared
between data owners due to legal and privacy issues. In this paper, with the
motivation of circumventing this challenge, we propose a Federated Face
Presentation Attack Detection (FedPAD) framework that simultaneously takes
advantage of rich fPAD information available at different data owners while
preserving data privacy. In the proposed framework, each data center locally
trains its own fPAD model. A server learns a global fPAD model by iteratively
aggregating model updates from all data centers without accessing private data
in each of them. To equip the aggregated fPAD model in the server with better
generalization ability to unseen attacks from users, following the basic idea
of FedPAD, we further propose a Federated Generalized Face Presentation Attack
Detection (FedGPAD) framework. A federated domain disentanglement strategy is
introduced in FedGPAD, which treats each data center as one domain and
decomposes the fPAD model into domain-invariant and domain-specific parts in
each data center. Two parts disentangle the domain-invariant and
domain-specific features from images in each local data center, respectively. A
server learns a global fPAD model by only aggregating domain-invariant parts of
the fPAD models from data centers and thus a more generalized fPAD model can be
aggregated in server. We introduce the experimental setting to evaluate the
proposed FedPAD and FedGPAD frameworks and carry out extensive experiments to
provide various insights about federated learning for fPAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HINet: Half Instance Normalization Network for Image Restoration. (arXiv:2105.06086v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06086">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the role of Instance Normalization in low-level
vision tasks. Specifically, we present a novel block: Half Instance
Normalization Block (HIN Block), to boost the performance of image restoration
networks. Based on HIN Block, we design a simple and powerful multi-stage
network named HINet, which consists of two subnetworks. With the help of HIN
Block, HINet surpasses the state-of-the-art (SOTA) on various image restoration
tasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD
dataset, with only 7.5% and 30% of its multiplier-accumulator operations
(MACs), 6.8 times and 2.9 times speedup respectively. For image deblurring, we
get comparable performance with 22.5% of its MACs and 3.3 times speedup on REDS
and GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the
average result of multiple datasets with 1.4 times speedup. With HINet, we won
1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG
Artifacts, with a PSNR of 29.70. The code is available at
https://github.com/megvii-model/HINet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing the Effect of Selection Bias on Generalization: A Thought Experiment. (arXiv:2105.09934v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09934">
<div class="article-summary-box-inner">
<span><p>Learned systems in the domain of visual recognition and cognition impress in
part because even though they are trained with datasets many orders of
magnitude smaller than the full population of possible images, they exhibit
sufficient generalization to be applicable to new and previously unseen data.
Since training data sets typically represent small sampling of a domain, the
possibility of bias in their composition is very real. But what are the limits
of generalization given such bias, and up to what point might it be sufficient
for a real problem task? Although many have examined issues regarding
generalization, this question may require examining the data itself. Here, we
focus on the characteristics of the training data that may play a role. Other
disciplines have grappled with these problems, most interestingly epidemiology,
where experimental bias is a critical concern. The range and nature of data
biases seen clinically are really quite relatable to learned vision systems.
One obvious way to deal with bias is to ensure a large enough training set, but
this might be infeasible for many domains. Another approach might be to perform
a statistical analysis of the actual training set, to determine if all aspects
of the domain are fairly captured. This too is difficult, in part because the
full set of variables might not be known, or perhaps not even knowable. Here,
we try a different approach in the tradition of the Thought Experiment, whose
most famous instance may be Schr\"odinger's Cat. There are many types of bias
as will be seen, but we focus only on one, selection bias. The point of the
thought experiment is not to demonstrate problems with all learned systems.
Rather, this might be a simple theoretical tool to probe into bias during data
collection to highlight deficiencies that might then deserve extra attention
either in data collection or system development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow. (arXiv:2106.14259v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14259">
<div class="article-summary-box-inner">
<span><p>Multiple human tracking is a fundamental problem for scene understanding.
Although both accuracy and speed are required in real-world applications,
recent tracking methods based on deep learning have focused on accuracy and
require substantial running time. This study aims to improve running speed by
performing human detection at a certain frame interval because it accounts for
most of the running time. The question is how to maintain accuracy while
skipping human detection. In this paper, we propose a method that complements
the detection results with optical flow, based on the fact that someone's
appearance does not change much between adjacent frames. To maintain the
tracking accuracy, we introduce robust interest point selection within human
regions and a tracking termination metric calculated by the distribution of the
interest points. On the MOT20 dataset in the MOTChallenge, the proposed
SDOF-Tracker achieved the best performance in terms of the total running speed
while maintaining the MOTA metric. Our code is available at
https://github.com/hitottiez/sdof-tracker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boggart: Towards General-Purpose Acceleration of Retrospective Video Analytics. (arXiv:2106.15315v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15315">
<div class="article-summary-box-inner">
<span><p>Commercial retrospective video analytics platforms have increasingly adopted
general interfaces to support the custom queries and convolutional neural
networks (CNNs) that different applications require. However, existing
optimizations were designed for settings where CNNs were platform- (not user-)
determined, and fail to meet at least one of the following key platform goals
when that condition is violated: reliable accuracy, low latency, and minimal
wasted work.
</p>
<p>We present Boggart, a system that simultaneously meets all three goals while
supporting the generality that today's platforms seek. Prior to queries being
issued, Boggart carefully employs traditional computer vision algorithms to
generate indices that are imprecise, but are fundamentally comprehensive across
different CNNs/queries. For each issued query, Boggart employs new techniques
to quickly characterize the imprecision of its index, and sparingly run CNNs
(and propagate the results to other frames) in a way that bounds accuracy
drops. Our results highlight that Boggart's improved generality comes at low
cost, with speedups that match (and most often, exceed) prior, model-specific
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02041">
<div class="article-summary-box-inner">
<span><p>To improve the viewer's Quality of Experience (QoE) and optimize computer
graphics applications, 3D model quality assessment (3D-QA) has become an
important task in the multimedia area. Point cloud and mesh are the two most
widely used digital representation formats of 3D models, the visual quality of
which is quite sensitive to lossy operations like simplification and
compression. Therefore, many related studies such as point cloud quality
assessment (PCQA) and mesh quality assessment (MQA) have been carried out to
measure the visual quality degradations of 3D models. However, a large part of
previous studies utilize full-reference (FR) metrics, which indicates they can
not predict the quality level with the absence of the reference 3D model.
Furthermore, few 3D-QA metrics consider color information, which significantly
restricts their effectiveness and scope of application. In this paper, we
propose a no-reference (NR) quality assessment metric for colored 3D models
represented by both point cloud and mesh. First, we project the 3D models from
3D space into quality-related geometry and color feature domains. Then, the 3D
natural scene statistics (3D-NSS) and entropy are utilized to extract
quality-aware features. Finally, machine learning is employed to regress the
quality-aware features into visual quality scores. Our method is validated on
the colored point cloud quality assessment database (SJTU-PCQA), the Waterloo
point cloud assessment database (WPC), and the colored mesh quality assessment
database (CMDM). The experimental results show that the proposed method
outperforms most compared NR 3D-QA metrics with competitive computational
resources and greatly reduces the performance gap with the state-of-the-art FR
3D-QA metrics. The code of the proposed model is publicly available now to
facilitate further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-supervised NeRF: Fewer Views and Faster Training for Free. (arXiv:2107.02791v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02791">
<div class="article-summary-box-inner">
<span><p>A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting
incorrect geometries when given an insufficient number of input views. One
potential reason is that standard volumetric rendering does not enforce the
constraint that most of a scene's geometry consist of empty space and opaque
surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised
Neural Radiance Fields), a loss for learning radiance fields that takes
advantage of readily-available depth supervision. We leverage the fact that
current NeRF pipelines require images with known camera poses that are
typically estimated by running structure-from-motion (SFM). Crucially, SFM also
produces sparse 3D points that can be used as "free" depth supervision during
training: we add a loss to encourage the distribution of a ray's terminating
depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can
render better images given fewer training views while training 2-3x faster.
Further, we show that our loss is compatible with other recently proposed NeRF
methods, demonstrating that depth is a cheap and easily digestible supervisory
signal. And finally, we find that DS-NeRF can support other types of depth
supervision such as scanned depth sensors and RGB-D reconstruction outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RewriteNet: Reliable Scene Text Editing with Implicit Decomposition of Text Contents and Styles. (arXiv:2107.11041v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11041">
<div class="article-summary-box-inner">
<span><p>Scene text editing (STE), which converts a text in a scene image into the
desired text while preserving an original style, is a challenging task due to a
complex intervention between text and style. In this paper, we propose a novel
STE model, referred to as RewriteNet, that decomposes text images into content
and style features and re-writes a text in the original image. Specifically,
RewriteNet implicitly distinguishes the content from the style by introducing
scene text recognition. Additionally, independent of the exact supervisions
with synthetic examples, we propose a self-supervised training scheme for
unlabeled real-world images, which bridges the domain gap between synthetic and
real data. Our experiments present that RewriteNet achieves better generation
performances than other comparisons. Further analysis proves the feature
decomposition of RewriteNet and demonstrates the reliability and robustness
through diverse experiments. Our implementation is publicly available at
\url{https://github.com/clovaai/rewritenet}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMChat: Multi-Modal Chat Dataset on Social Media. (arXiv:2108.07154v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07154">
<div class="article-summary-box-inner">
<span><p>Incorporating multi-modal contexts in conversation is important for
developing more engaging dialogue systems. In this work, we explore this
direction by introducing MMChat: a large-scale Chinese multi-modal dialogue
corpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous
corpora that are crowd-sourced or collected from fictitious movies, MMChat
contains image-grounded dialogues collected from real conversations on social
media, in which the sparsity issue is observed. Specifically, image-initiated
dialogues in common communications may deviate to some non-image-grounded
topics as the conversation proceeds. To better investigate this issue, we
manually annotate 100K dialogues from MMChat and further filter the corpus
accordingly, which yields MMChat-hf. We develop a benchmark model to address
the sparsity issue in dialogue generation tasks by adapting the attention
routing mechanism on image features. Experiments demonstrate the usefulness of
incorporating image features and the effectiveness of handling the sparsity of
image features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Misalignment Problem in Dense Object Detection. (arXiv:2108.12176v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12176">
<div class="article-summary-box-inner">
<span><p>Object detection aims to localize and classify the objects in a given image,
and these two tasks are sensitive to different object regions. Therefore, some
locations predict high-quality bounding boxes but low classification scores,
and some locations are quite the opposite. A misalignment exists between the
two tasks, and their features are spatially entangled. In order to solve the
misalignment problem, we propose a plug-in Spatial-disentangled and
Task-aligned operator (SALT). By predicting two task-aware point sets that are
located in each task's sensitive regions, SALT can reassign features from those
regions and align them to the corresponding anchor point. Therefore, features
for the two tasks are spatially aligned and disentangled. To minimize the
difference between the two regression stages, we propose a Self-distillation
regression (SDR) loss that can transfer knowledge from the refined regression
results to the coarse regression results. On the basis of SALT and SDR loss, we
propose SALT-Net, which explicitly exploits task-aligned point-set features for
accurate detection results. Extensive experiments on the MS-COCO dataset show
that our proposed methods can consistently boost different state-of-the-art
dense detectors by $\sim$2 AP. Notably, SALT-Net with Res2Net-101-DCN backbone
achieves 53.8 AP on the MS-COCO test-dev.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Wind Power Curve Modeling Via Machine Vision: A Self-learning Deep Convolutional Network Based Method. (arXiv:2109.00894v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00894">
<div class="article-summary-box-inner">
<span><p>This paper develops a novel self-training U-net (STU-net) based method for
the automated WPC model generation without requiring data pre-processing. The
self-training (ST) process of STU-net has two steps. First, different from
traditional studies regarding the WPC modeling as a curve fitting problem, in
this paper, we renovate the WPC modeling formulation from a machine vision
aspect. To develop sufficiently diversified training samples, we synthesize
supervisory control and data acquisition (SCADA) data based on a set of S-shape
functions depicting WPCs. These synthesized SCADA data and WPC functions are
visualized as images and paired as training samples(I_x, I_wpc). A U-net is
then developed to approximate the model recovering I_wpc from I_x. The
developed U-net is applied into observed SCADA data and can successfully
generate the I_wpc. Moreover, we develop a pixel mapping and correction process
to derive a mathematical form f_wpc representing I_wpcgenerated previously. The
proposed STU-net only needs to train once and does not require any data
preprocessing in applications. Numerical experiments based on 76 WTs are
conducted to validate the superiority of the proposed method by benchmarking
against classical WPC modeling methods. To demonstrate the repeatability of the
presented research, we release our code at https://github.com/IkeYang/STU-net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03082">
<div class="article-summary-box-inner">
<span><p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach
with recurrent conditional GAN. We employ the recurrent auto-encoder-based
compression network as the generator, and most importantly, we propose a
recurrent conditional discriminator, which judges on raw vs. compressed video
conditioned on both spatial and temporal features, including the latent
representation, temporal motion and hidden states in recurrent cells. This way,
the adversarial training pushes the generated video to be not only spatially
photo-realistic but also temporally consistent with the groundtruth and
coherent among video frames. The experimental results show that the learned
PLVC model compresses video with good perceptual quality at low bit-rate, and
that it outperforms the official HEVC test model (HM 16.20) and the existing
learned video compression approaches for several perceptual quality metrics and
user studies. The codes will be released at the project page:
https://github.com/RenYang-home/PLVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Collaborative Representation Learning for Dynamic Saliency Prediction. (arXiv:2109.08371v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08371">
<div class="article-summary-box-inner">
<span><p>The Dynamic Saliency Prediction (DSP) task simulates the human selective
attention mechanism to perceive the dynamic scene, which is significant and
imperative in many vision tasks. Most of existing methods only consider visual
cues, while neglect the accompanied audio information, which can provide
complementary information for the scene understanding. In fact, there exists a
strong relation between auditory and visual cues, and humans generally perceive
the surrounding scene by collaboratively sensing these cues. Motivated by this,
an audio-visual collaborative representation learning method is proposed for
the DSP task, which explores the audio modality to better predict the dynamic
saliency map by assisting vision modality. The proposed method consists of
three parts: 1) audio-visual encoding, 2) audio-visual location, and 3)
collaborative integration parts. Firstly, a refined SoundNet architecture is
adopted to encode audio modality for obtaining corresponding features, and a
modified 3D ResNet-50 architecture is employed to learn visual features,
containing both spatial location and temporal motion information. Secondly, an
audio-visual location part is devised to locate the sound source in the visual
scene by learning the correspondence between audio-visual information. Thirdly,
a collaborative integration part is devised to adaptively aggregate
audio-visual information and center-bias prior to generate the final saliency
map. Extensive experiments are conducted on six challenging audiovisual
eye-tracking datasets, including DIEM, AVAD, Coutrot1, Coutrot2, SumMe, and
ETMD, which shows significant superiority over state-of-the-art DSP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12925">
<div class="article-summary-box-inner">
<span><p>Due to its role in many computer vision tasks, image matching has been
subjected to an active investigation by researchers, which has lead to better
and more discriminant feature descriptors and to more robust matching
strategies, also thanks to the advent of the deep learning and the increased
computational power of the modern hardware. Despite of these achievements, the
keypoint extraction process at the base of the image matching pipeline has not
seen equivalent progresses. This paper presents HarrisZ$^+$, an upgrade to the
HarrisZ corner detector, optimized to synergically take advance of the recent
improvements of the other steps of the image matching pipeline. HarrisZ$^+$
does not only consists of a tuning of the setup parameters, but introduces
further refinements to the selection criteria delineated by HarrisZ, so
providing more, yet discriminative, keypoints, which are better distributed on
the image and with higher localization accuracy. The image matching pipeline
including HarrisZ$^+$, together with the other modern components, obtained in
different recent matching benchmarks state-of-the-art results among the classic
image matching pipelines. These results are quite close to those obtained by
the more recent fully deep end-to-end trainable approaches and show that there
is still a proper margin of improvement that can be granted by the research in
classic image matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skill Induction and Planning with Latent Language. (arXiv:2110.01517v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01517">
<div class="article-summary-box-inner">
<span><p>We present a framework for learning hierarchical policies from
demonstrations, using sparse natural language annotations to guide the
discovery of reusable skills for autonomous decision-making. We formulate a
generative model of action sequences in which goals generate sequences of
high-level subtask descriptions, and these descriptions generate sequences of
low-level actions. We describe how to train this model using primarily
unannotated demonstrations by parsing demonstrations into sequences of named
high-level subtasks, using only a small number of seed annotations to ground
language in action. In trained models, natural language commands index a
combinatorial library of skills; agents can use these skills to plan by
generating high-level instruction sequences tailored to novel goals. We
evaluate this approach in the ALFRED household simulation environment,
providing natural language annotations for only 10% of demonstrations. It
achieves task completion rates comparable to state-of-the-art models
(outperforming several recent methods with access to ground-truth plans during
training and evaluation) while providing structured and human-readable
high-level plans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v5 [cs.IT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06986">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new deep unfolding neural network based on the
ADMM algorithm for analysis Compressed Sensing. The proposed network jointly
learns a redundant analysis operator for sparsification and reconstructs the
signal of interest. We compare our proposed network with a state-of-the-art
unfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we
consider not only image, but also speech datasets as test examples.
Computational experiments demonstrate that our proposed network outperforms the
state-of-the-art deep unfolding network, consistently for both real-world image
and speech datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection. (arXiv:2110.11867v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11867">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel road marking benchmark dataset for road
marking detection, addressing the limitations in the existing publicly
available datasets such as lack of challenging scenarios, prominence given to
lane markings, unavailability of an evaluation script, lack of annotation
formats and lower resolutions. Our dataset consists of 2887 total images with
4706 road marking instances belonging to 11 classes. The images have a high
resolution of 1920 x 1080 and capture a wide range of traffic, lighting and
weather conditions. We provide road marking annotations in polygons, bounding
boxes and pixel-level segmentation masks to facilitate a diverse range of road
marking detection algorithms. The evaluation metrics and the evaluation script
we provide, will further promote direct comparison of novel approaches for road
marking detection with existing methods. Furthermore, we evaluate the
effectiveness of using both instance segmentation and object detection based
approaches for the road marking detection task. Speed and accuracy scores for
two instance segmentation models and two object detector models are provided as
a performance baseline for our benchmark dataset. The dataset and the
evaluation script is publicly available at https://github.com/oshadajay/CeyMo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOFT: Softmax-free Transformer with Linear Complexity. (arXiv:2110.11945v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11945">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have pushed the state-of-the-art for various
visual recognition tasks by patch-wise image tokenization followed by
self-attention. However, the employment of self-attention modules results in a
quadratic complexity in both computation and memory usage. Various attempts on
approximating the self-attention computation with linear complexity have been
made in Natural Language Processing. However, an in-depth analysis in this work
shows that they are either theoretically flawed or empirically ineffective for
visual recognition. We further identify that their limitations are rooted in
keeping the softmax self-attention during approximations. Specifically,
conventional self-attention is computed by normalizing the scaled dot-product
between token feature vectors. Keeping this softmax operation challenges any
subsequent linearization efforts. Based on this insight, for the first time, a
softmax-free transformer or SOFT is proposed. To remove softmax in
self-attention, Gaussian kernel function is used to replace the dot-product
similarity without further normalization. This enables a full self-attention
matrix to be approximated via a low-rank matrix decomposition. The robustness
of the approximation is achieved by calculating its Moore-Penrose inverse using
a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT
significantly improves the computational efficiency of existing ViT variants.
Crucially, with a linear complexity, much longer token sequences are permitted
in SOFT, resulting in superior trade-off between accuracy and complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A vectorized sea horizon edge filter for maritime video processing tasks. (arXiv:2110.13694v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13694">
<div class="article-summary-box-inner">
<span><p>The horizon line is a fundamental semantic feature in several maritime video
processing tasks, such as digital video stabilization, camera calibration,
target tracking, and target distance estimation. Visible range Electro-Optical
(EO) sensors capture richer information in the daytime, which often comes with
challenging clutter. The best methods rely on tailored filters to keep,
ideally, only horizon edge pixels. These methods work well but often fail in
the case of edge-degraded horizons. Our first aim is to solve this problem
while taking the real-time constraint into account; we propose a tailored edge
filter that relies on growing line segments with a low edge threshold and
filters them based on their slope, length, and relative position. Next, we
build the filtered edge map by computing Cartesian coordinates of pixels across
line segments that survived the filter. We infer the horizon from the filtered
edge map using line fitting techniques and simple temporal information. We
consider the real-time constraint by vectorizing the computations and proposing
a better way to leverage image downsizing. Extensive experiments on 26,125
visible range frames show that the proposed method achieves significant
robustness while satisfying the real-time constraint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Visual Transformers. (arXiv:2111.06091v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06091">
<div class="article-summary-box-inner">
<span><p>Transformer, an attention-based encoder-decoder model, has already
revolutionized the field of natural language processing (NLP). Inspired by such
significant achievements, some pioneering works have recently been done on
employing Transformer-liked architectures in the computer vision (CV) field,
which have demonstrated their effectiveness on three fundamental CV tasks
(classification, detection, and segmentation) as well as multiple sensory data
stream (images, point clouds, and vision-language data). Because of their
competitive modeling capabilities, the visual Transformers have achieved
impressive performance improvements over multiple benchmarks as compared with
modern Convolution Neural Networks (CNNs). In this survey, we have reviewed
over one hundred of different visual Transformers comprehensively according to
three fundamental CV tasks and different data stream types, where a taxonomy is
proposed to organize the representative methods according to their motivations,
structures, and application scenarios. Because of their differences on training
settings and dedicated vision tasks, we have also evaluated and compared all
these existing visual Transformers under different configurations. Furthermore,
we have revealed a series of essential but unexploited aspects that may empower
such visual Transformers to stand out from numerous architectures, e.g., slack
high-level semantic embeddings to bridge the gap between the visual
Transformers and the sequential ones. Finally, three promising research
directions are suggested for future investment. We will continue to update the
latest articles and their released source codes at
https://github.com/liuyang-ict/awesome-visual-transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIMEcraft: Handcrafted superpixel selection and inspection for Visual eXplanations. (arXiv:2111.08094v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08094">
<div class="article-summary-box-inner">
<span><p>The increased interest in deep learning applications, and their
hard-to-detect biases result in the need to validate and explain complex
models. However, current explanation methods are limited as far as both the
explanation of the reasoning process and prediction results are concerned. They
usually only show the location in the image that was important for model
prediction. The lack of possibility to interact with explanations makes it
difficult to verify and understand exactly how the model works. This creates a
significant risk when using the model. The risk is compounded by the fact that
explanations do not take into account the semantic meaning of the explained
objects. To escape from the trap of static and meaningless explanations, we
propose a tool and a process called LIMEcraft. LIMEcraft enhances the process
of explanation by allowing a user to interactively select semantically
consistent areas and thoroughly examine the prediction for the image instance
in case of many image features. Experiments on several models show that our
tool improves model safety by inspecting model fairness for image pieces that
may indicate model bias. The code is available at:
<a href="http://github.com/MI2DataLab/LIMEcraft">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combined Scaling for Open-Vocabulary Image Classification. (arXiv:2111.10050v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10050">
<div class="article-summary-box-inner">
<span><p>We present a combined scaling method - named BASIC - that achieves 85.7%
top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from
any labeled ImageNet example. This accuracy surpasses best published similar
models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant
improvements in robustness benchmarks. For instance, on 5 test sets with
natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our
model achieves 84.3% top-1 average accuracy, only a small drop from its
original ImageNet accuracy.
</p>
<p>To achieve these results, we scale up the contrastive learning framework of
CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our
dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x
larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in
parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size
is 65536 which is 2x more than CLIP and 4x more than ALIGN.
</p>
<p>We encountered two main challenges with the scaling rules of BASIC. First,
the main challenge with implementing the combined scaling rules of BASIC is the
limited memory of accelerators, such as GPUs and TPUs. To overcome the memory
limit, we propose two simple methods which make use of gradient checkpointing
and model parallelism. Second, while increasing the dataset size and the model
size has been the defacto method to improve the performance of deep learning
models like BASIC, the effect of a large contrastive batch size on such
contrastive-trained image-text models is not well-understood. To shed light on
the benefits of large contrastive batch sizes, we develop a theoretical
framework which shows that larger contrastive batch sizes lead to smaller
generalization gaps for image-text models such as BASIC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking. (arXiv:2111.11892v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11892">
<div class="article-summary-box-inner">
<span><p>Multi-Camera Multi-Object Tracking is currently drawing attention in the
computer vision field due to its superior performance in real-world
applications such as video surveillance in crowded scenes or in wide spaces. In
this work, we propose a mathematically elegant multi-camera multiple object
tracking approach based on a spatial-temporal lifted multicut formulation. Our
model utilizes state-of-the-art tracklets produced by single-camera trackers as
proposals. As these tracklets may contain ID-Switch errors, we refine them
through a novel pre-clustering obtained from 3D geometry projections. As a
result, we derive a better tracking graph without ID switches and more precise
affinity costs for the data association phase. Tracklets are then matched to
multi-camera trajectories by solving a global lifted multicut formulation that
incorporates short and long-range temporal interactions on tracklets located in
the same camera as well as inter-camera ones. Experimental results on the
WildTrack dataset yield near-perfect performance, outperforming
state-of-the-art trackers on Campus while being on par on the PETS-09 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPCL: A New Framework for Domain Adaptive Semantic Segmentation via Semantic Prototype-based Contrastive Learning. (arXiv:2111.12358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12358">
<div class="article-summary-box-inner">
<span><p>Although there is significant progress in supervised semantic segmentation,
it remains challenging to deploy the segmentation models to unseen domains due
to domain biases. Domain adaptation can help in this regard by transferring
knowledge from a labeled source domain to an unlabeled target domain. Previous
methods typically attempt to perform the adaptation on global features,
however, the local semantic affiliations accounting for each pixel in the
feature space are often ignored, resulting in less discriminability. To solve
this issue, we propose a novel semantic prototype-based contrastive learning
framework for fine-grained class alignment. Specifically, the semantic
prototypes provide supervisory signals for per-pixel discriminative
representation learning and each pixel of source and target domains in the
feature space is required to reflect the content of the corresponding semantic
prototype. In this way, our framework is able to explicitly make intra-class
pixel representations closer and inter-class pixel representations further
apart to improve the robustness of the segmentation model as well as alleviate
the domain shift problem. Our method is easy to implement and attains superior
results compared to state-of-the-art approaches, as is demonstrated with a
number of experiments. The code is publicly available at
https://github.com/BinhuiXie/SPCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Targeted Supervised Contrastive Learning for Long-Tailed Recognition. (arXiv:2111.13998v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13998">
<div class="article-summary-box-inner">
<span><p>Real-world data often exhibits long tail distributions with heavy class
imbalance, where the majority classes can dominate the training process and
alter the decision boundaries of the minority classes. Recently, researchers
have investigated the potential of supervised contrastive learning for
long-tailed recognition, and demonstrated that it provides a strong performance
gain. In this paper, we show that while supervised contrastive learning can
help improve performance, past baselines suffer from poor uniformity brought in
by imbalanced data distribution. This poor uniformity manifests in samples from
the minority class having poor separability in the feature space. To address
this problem, we propose targeted supervised contrastive learning (TSC), which
improves the uniformity of the feature distribution on the hypersphere. TSC
first generates a set of targets uniformly distributed on a hypersphere. It
then makes the features of different classes converge to these distinct and
uniformly distributed targets during training. This forces all classes,
including minority classes, to maintain a uniform distribution in the feature
space, improves class boundaries, and provides better generalization even in
the presence of long-tail data. Experiments on multiple datasets show that TSC
achieves state-of-the-art performance on long-tailed recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-Based Video Coding with Joint Deep Compression and Enhancement. (arXiv:2111.14474v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14474">
<div class="article-summary-box-inner">
<span><p>The end-to-end learning-based video compression has attracted substantial
attentions by paving another way to compress video signals as stacked visual
features. This paper proposes an efficient end-to-end deep video codec with
jointly optimized compression and enhancement modules (JCEVC). First, we
propose a dual-path generative adversarial network (DPEG) to reconstruct video
details after compression. An $\alpha$-path facilitates the structure
information reconstruction with a large receptive field and multi-frame
references, while a $\beta$-path facilitates the reconstruction of local
textures. Both paths are fused and co-trained within a generative-adversarial
process. Second, we reuse the DPEG network in both motion compensation and
quality enhancement modules, which are further combined with other necessary
modules to formulate our JCEVC framework. Third, we employ a joint training of
deep video compression and enhancement that further improves the
rate-distortion (RD) performance of compression. Compared with x265 LDP very
fast mode, our JCEVC reduces the average bit-per-pixel (bpp) by 39.39\%/54.92\%
at the same PSNR/MS-SSIM, which outperforms the state-of-the-art deep video
codecs by a considerable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSG0: Continual Urban Scene Generation with Zero Forgetting. (arXiv:2112.03252v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03252">
<div class="article-summary-box-inner">
<span><p>With the rapid advances in generative adversarial networks (GANs), the visual
quality of synthesised scenes keeps improving, including for complex urban
scenes with applications to automated driving. We address in this work a
continual scene generation setup in which GANs are trained on a stream of
distinct domains; ideally, the learned models should eventually be able to
generate new scenes in all seen domains. This setup reflects the real-life
scenario where data are continuously acquired in different places at different
times. In such a continual setup, we aim for learning with zero forgetting,
\IE, with no degradation in synthesis quality over earlier domains due to
catastrophic forgetting. To this end, we introduce a novel framework that not
only (i) enables seamless knowledge transfer in continual training but also
(ii) guarantees zero forgetting with a small overhead cost. While being more
memory efficient, thanks to continual learning, our model obtains better
synthesis quality as compared against the brute-force solution that trains one
full model for each domain. Especially, under extreme low-data regimes, our
approach outperforms the brute-force one by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection. (arXiv:2112.04532v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04532">
<div class="article-summary-box-inner">
<span><p>Object detection plays a key role in many security-critical systems.
Adversarial patch attacks, which are easy to implement in the physical world,
pose a serious threat to state-of-the-art object detectors. Developing reliable
defenses for object detectors against patch attacks is critical but severely
understudied. In this paper, we propose Segment and Complete defense (SAC), a
general framework for defending object detectors against patch attacks through
detection and removal of adversarial patches. We first train a patch segmenter
that outputs patch masks which provide pixel-level localization of adversarial
patches. We then propose a self adversarial training algorithm to robustify the
patch segmenter. In addition, we design a robust shape completion algorithm,
which is guaranteed to remove the entire patch from the images if the outputs
of the patch segmenter are within a certain Hamming distance of the
ground-truth patch masks. Our experiments on COCO and xView datasets
demonstrate that SAC achieves superior robustness even under strong adaptive
attacks with no reduction in performance on clean images, and generalizes well
to unseen patch shapes, attack budgets, and unseen attack methods. Furthermore,
we present the APRICOT-Mask dataset, which augments the APRICOT dataset with
pixel-level annotations of adversarial patches. We show SAC can significantly
reduce the targeted attack success rate of physical patch attacks. Our code is
available at https://github.com/joellliu/SegmentAndComplete.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HeadNeRF: A Real-time NeRF-based Parametric Head Model. (arXiv:2112.05637v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05637">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose HeadNeRF, a novel NeRF-based parametric head model
that integrates the neural radiance field to the parametric representation of
the human head. It can render high fidelity head images in real-time on modern
GPUs, and supports directly controlling the generated images' rendering pose
and various semantic attributes. Different from existing related parametric
models, we use the neural radiance fields as a novel 3D proxy instead of the
traditional 3D textured mesh, which makes that HeadNeRF is able to generate
high fidelity images. However, the computationally expensive rendering process
of the original NeRF hinders the construction of the parametric NeRF model. To
address this issue, we adopt the strategy of integrating 2D neural rendering to
the rendering process of NeRF and design novel loss terms. As a result, the
rendering speed of HeadNeRF can be significantly accelerated, and the rendering
time of one frame is reduced from 5s to 25ms. The well designed loss terms also
improve the rendering accuracy, and the fine-level details of the human head,
such as the gaps between teeth, wrinkles, and beards, can be represented and
synthesized by HeadNeRF. Extensive experimental results and several
applications demonstrate its effectiveness. The trained parametric model is
available at https://github.com/CrisHY1995/headnerf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Planner-Actor-Critic for Unsupervised Deformable Image Registration. (arXiv:2112.07415v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07415">
<div class="article-summary-box-inner">
<span><p>Large deformations of organs, caused by diverse shapes and nonlinear shape
changes, pose a significant challenge for medical image registration.
Traditional registration methods need to iteratively optimize an objective
function via a specific deformation model along with meticulous parameter
tuning, but which have limited capabilities in registering images with large
deformations. While deep learning-based methods can learn the complex mapping
from input images to their respective deformation field, it is regression-based
and is prone to be stuck at local minima, particularly when large deformations
are involved. To this end, we present Stochastic Planner-Actor-Critic (SPAC), a
novel reinforcement learning-based framework that performs step-wise
registration. The key notion is warping a moving image successively by each
time step to finally align to a fixed image. Considering that it is challenging
to handle high dimensional continuous action and state spaces in the
conventional reinforcement learning (RL) framework, we introduce a new concept
`Plan' to the standard Actor-Critic model, which is of low dimension and can
facilitate the actor to generate a tractable high dimensional action. The
entire framework is based on unsupervised training and operates in an
end-to-end manner. We evaluate our method on several 2D and 3D medical image
datasets, some of which contain large deformations. Our empirical results
highlight that our work achieves consistent, significant gains and outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continually Learning Self-Supervised Representations with Projected Functional Regularization. (arXiv:2112.15022v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15022">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised learning methods are able to learn high-quality image
representations and are closing the gap with supervised approaches. However,
these methods are unable to acquire new knowledge incrementally -- they are, in
fact, mostly used only as a pre-training phase over IID data. In this work we
investigate self-supervised methods in continual learning regimes without any
replay mechanism. We show that naive functional regularization, also known as
feature distillation, leads to lower plasticity and limits continual learning
performance. Instead, we propose Projected Functional Regularization in which a
separate temporal projection network ensures that the newly learned feature
space preserves information of the previous one, while at the same time
allowing for the learning of new features. This prevents forgetting while
maintaining the plasticity of the learner. Comparison with other incremental
learning approaches applied to self-supervision demonstrates that our method
obtains competitive performance in different scenarios and on multiple
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Dual Contouring. (arXiv:2202.01999v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01999">
<div class="article-summary-box-inner">
<span><p>We introduce neural dual contouring (NDC), a new data-driven approach to mesh
reconstruction based on dual contouring (DC). Like traditional DC, it produces
exactly one vertex per grid cell and one quad for each grid edge intersection,
a natural and efficient structure for reproducing sharp features. However,
rather than computing vertex locations and edge crossings with hand-crafted
functions that depend directly on difficult-to-obtain surface gradients, NDC
uses a neural network to predict them. As a result, NDC can be trained to
produce meshes from signed or unsigned distance fields, binary voxel grids, or
point clouds (with or without normals); and it can produce open surfaces in
cases where the input represents a sheet or partial surface. During experiments
with five prominent datasets, we find that NDC, when trained on one of the
datasets, generalizes well to the others. Furthermore, NDC provides better
surface reconstruction accuracy, feature preservation, output complexity,
triangle quality, and inference time in comparison to previous learned (e.g.,
neural marching cubes, convolutional occupancy networks) and traditional (e.g.,
Poisson) methods. Code and data are available at
https://github.com/czq142857/NDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heed the Noise in Performance Evaluations in Neural Architecture Search. (arXiv:2202.02078v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02078">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) has recently become a topic of great
interest. However, there is a potentially impactful issue within NAS that
remains largely unrecognized: noise. Due to stochastic factors in neural
network initialization, training, and the chosen train/validation dataset
split, the performance evaluation of a neural network architecture, which is
often based on a single learning run, is also stochastic. This may have a
particularly large impact if a dataset is small. We therefore propose to reduce
this noise by evaluating architectures based on average performance over
multiple network training runs using different random seeds and
cross-validation. We perform experiments for a combinatorial optimization
formulation of NAS in which we vary noise reduction levels. We use the same
computational budget for each noise level in terms of network training runs,
i.e., we allow less architecture evaluations when averaging over more training
runs. Multiple search algorithms are considered, including evolutionary
algorithms which generally perform well for NAS. We use two publicly available
datasets from the medical image segmentation domain where datasets are often
limited and variability among samples is often high. Our results show that
reducing noise in architecture evaluations enables finding better architectures
by all considered search algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification. (arXiv:2202.02472v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02472">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has been widely investigated in a vast majority of
applications in electroencephalography (EEG)-based brain-computer interfaces
(BCIs), especially for motor imagery (MI) classification in the past five
years. The mainstream DL methodology for the MI-EEG classification exploits the
temporospatial patterns of EEG signals using convolutional neural networks
(CNNs), which have been particularly successful in visual images. However,
since the statistical characteristics of visual images depart radically from
EEG signals, a natural question arises whether an alternative network
architecture exists apart from CNNs. To address this question, we propose a
novel geometric deep learning (GDL) framework called Tensor-CSPNet, which
characterizes spatial covariance matrices derived from EEG signals on symmetric
positive definite (SPD) manifolds and fully captures the temporospatiofrequency
patterns using existing deep neural networks on SPD manifolds, integrating with
experiences from many successful MI-EEG classifiers to optimize the framework.
In the experiments, Tensor-CSPNet attains or slightly outperforms the current
state-of-the-art performance on the cross-validation and holdout scenarios in
two commonly-used MI-EEG datasets. Moreover, the visualization and
interpretability analyses also exhibit the validity of Tensor-CSPNet for the
MI-EEG classification. To conclude, in this study, we provide a feasible answer
to the question by generalizing the DL methodologies on SPD manifolds, which
indicates the start of a specific GDL methodology for the MI-EEG
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08227">
<div class="article-summary-box-inner">
<span><p>Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements. (arXiv:2202.09020v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09020">
<div class="article-summary-box-inner">
<span><p>With the acceleration of urbanization and living standards, microorganisms
play increasingly important roles in industrial production, bio-technique, and
food safety testing. Microorganism biovolume measurements are one of the
essential parts of microbial analysis. However, traditional manual measurement
methods are time-consuming and challenging to measure the characteristics
precisely. With the development of digital image processing techniques, the
characteristics of the microbial population can be detected and quantified. The
changing trend can be adjusted in time and provided a basis for the
improvement. The applications of the microorganism biovolume measurement method
have developed since the 1980s. More than 62 articles are reviewed in this
study, and the articles are grouped by digital image segmentation methods with
periods. This study has high research significance and application value, which
can be referred to microbial researchers to have a comprehensive understanding
of microorganism biovolume measurements using digital image analysis methods
and potential applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Molecular Prior Distribution for Bayesian Inference Based on Wilson Statistics. (arXiv:2202.09388v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09388">
<div class="article-summary-box-inner">
<span><p>Background and Objective: Wilson statistics describe well the power spectrum
of proteins at high frequencies. Therefore, it has found several applications
in structural biology, e.g., it is the basis for sharpening steps used in
cryogenic electron microscopy (cryo-EM). A recent paper gave the first rigorous
proof of Wilson statistics based on a formalism of Wilson's original argument.
This new analysis also leads to statistical estimates of the scattering
potential of proteins that reveal a correlation between neighboring Fourier
coefficients. Here we exploit these estimates to craft a novel prior that can
be used for Bayesian inference of molecular structures. Methods: We describe
the properties of the prior and the computation of its hyperparameters. We then
evaluate the prior on two synthetic linear inverse problems, and compare
against a popular prior in cryo-EM reconstruction at a range of SNRs. Results:
We show that the new prior effectively suppresses noise and fills-in low SNR
regions in the spectral domain. Furthermore, it improves the resolution of
estimates on the problems considered for a wide range of SNR and produces
Fourier Shell Correlation curves that are insensitive to masking effects.
Conclusions: We analyze the assumptions in the model, discuss relations to
other regularization strategies, and postulate on potential implications for
structure determination in cryo-EM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Cross-Layer Attention for Image Restoration. (arXiv:2203.03619v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03619">
<div class="article-summary-box-inner">
<span><p>Non-local attention module has been proven to be crucial for image
restoration. Conventional non-local attention processes features of each layer
separately, so it risks missing correlation between features among different
layers. To address this problem, we propose Cross-Layer Attention (CLA) module
in this paper. Instead of finding correlated key pixels within the same layer,
each query pixel can attend to key pixels at previous layers of the network. In
order to further enhance the learning capability and reduce the inference cost
of CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two
adaptive designs are proposed for ACLA: 1) adaptively selecting the keys for
non-local attention at each layer; 2) automatically searching for the insertion
locations for ACLA modules. By these two adaptive designs, ACLA dynamically
selects the number of keys to be aggregated for non-local attention at layer.
In addition, ACLA searches for the optimal insert positions of ACLA modules by
a neural architecture search method to render a compact neural network with
compelling performance. Extensive experiments on image restoration tasks,
including single image super-resolution, image denoising, image demosaicing,
and image compression artifacts reduction, validate the effectiveness and
efficiency of ACLA. The code of CLA and ACLA is available at
\url{https://github.com/SDL-ASU/ACLA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers. (arXiv:2203.03682v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03682">
<div class="article-summary-box-inner">
<span><p>In this work, we consider the problem of learning a perception model for
monocular robot navigation using few annotated images. Using a Vision
Transformer (ViT) pretrained with a label-free self-supervised method, we
successfully train a coarse image segmentation model for the Duckietown
environment using 70 training images. Our model performs coarse image
segmentation at the 8x8 patch level, and the inference resolution can be
adjusted to balance prediction granularity and real-time perception
constraints. We study how best to adapt a ViT to our task and environment, and
find that some lightweight architectures can yield good single-image
segmentation at a usable frame rate, even on CPU. The resulting perception
model is used as the backbone for a simple yet robust visual servoing agent,
which we deploy on a differential drive mobile robot to perform two tasks: lane
following and obstacle avoidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07669">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new query-based detection framework for crowd
detection. Previous query-based detectors suffer from two drawbacks: first,
multiple predictions will be inferred for a single object, typically in crowded
scenes; second, the performance saturates as the depth of the decoding stage
increases. Benefiting from the nature of the one-to-one label assignment rule,
we propose a progressive predicting method to address the above issues.
Specifically, we first select accepted queries prone to generate true positive
predictions, then refine the rest noisy queries according to the previously
accepted predictions. Experiments show that our method can significantly boost
the performance of query-based detectors in crowded scenes. Equipped with our
approach, Sparse RCNN achieves 92.0\% $\text{AP}$, 41.4\% $\text{MR}^{-2}$ and
83.2\% $\text{JI}$ on the challenging CrowdHuman \cite{shao2018crowdhuman}
dataset, outperforming the box-based method MIP \cite{chu2020detection} that
specifies in handling crowded scenarios. Moreover, the proposed method, robust
to crowdedness, can still obtain consistent improvements on moderately and
slightly crowded datasets like CityPersons \cite{zhang2017citypersons} and COCO
\cite{lin2014microsoft}. Code will be made publicly available at
https://github.com/megvii-model/Iter-E2EDET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11480">
<div class="article-summary-box-inner">
<span><p>Compared with the domain-specific model, the vision-language pre-training
models (VLPMs) have shown superior performance on downstream tasks with fast
fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with
a uniform transformers stack architecture and large amounts of image-text
paired data, achieving remarkable results on downstream tasks such as
image-text reference(IR and TR), vision question answering (VQA) and image
captioning (IC) etc. During the training phase, VLPMs are always fed with a
combination of multiple public datasets to meet the demand of large-scare
training data. However, due to the unevenness of data distribution including
size, task type and quality, using the mixture of multiple datasets for model
training can be problematic. In this work, we introduce a large-scale
multi-modal corpora named WuDaoMM, totally containing more than 650M image-text
pairs. Specifically, about 600 million pairs of data are collected from
multiple webpages in which image and caption present weak correlation, and the
other 50 million strong-related image-text pairs are collected from some
high-quality graphic websites. We also release a base version of WuDaoMM with 5
million strong-correlated image-text pairs, which is sufficient to support the
common cross-modal model pre-training. Besides, we trained both an
understanding and a generation vision-language (VL) model to test the dataset
effectiveness. The results show that WuDaoMM can be applied as an efficient
dataset for VLPMs, especially for the model in text-to-image generation task.
The data is released at https://data.wudaoai.cn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities. (arXiv:2203.14712v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14712">
<div class="article-summary-box-inner">
<span><p>Assembly101 is a new procedural activity dataset featuring 4321 videos of
people assembling and disassembling 101 "take-apart" toy vehicles. Participants
work without fixed instructions, and the sequences feature rich and natural
variations in action ordering, mistakes, and corrections. Assembly101 is the
first multi-view action dataset, with simultaneous static (8) and egocentric
(4) recordings. Sequences are annotated with more than 100K coarse and 1M
fine-grained action segments, and 18M 3D hand poses. We benchmark on three
action understanding tasks: recognition, anticipation and temporal
segmentation. Additionally, we propose a novel task of detecting mistakes. The
unique recording format and rich set of annotations allow us to investigate
generalization to new toys, cross-view transfer, long-tailed distributions, and
pose vs. appearance. We envision that Assembly101 will serve as a new challenge
to investigate various activity understanding problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition. (arXiv:2203.16670v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16670">
<div class="article-summary-box-inner">
<span><p>Intrinsic image decomposition is the process of recovering the image
formation components (reflectance and shading) from an image. Previous methods
employ either explicit priors to constrain the problem or implicit constraints
as formulated by their losses (deep learning). These methods can be negatively
influenced by strong illumination conditions causing shading-reflectance
leakages.
</p>
<p>Therefore, in this paper, an end-to-end edge-driven hybrid CNN approach is
proposed for intrinsic image decomposition. Edges correspond to illumination
invariant gradients. To handle hard negative illumination transitions, a
hierarchical approach is taken including global and local refinement layers. We
make use of attention layers to further strengthen the learning process.
</p>
<p>An extensive ablation study and large scale experiments are conducted showing
that it is beneficial for edge-driven hybrid IID networks to make use of
illumination invariant descriptors and that separating global and local cues
helps in improving the performance of the network. Finally, it is shown that
the proposed method obtains state of the art performance and is able to
generalise well to real world images. The project page with pretrained models,
finetuned models and network code can be found at
https://ivi.fnwi.uva.nl/cv/pienet/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00298">
<div class="article-summary-box-inner">
<span><p>To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Logit Adjustment. (arXiv:2204.11822v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11822">
<div class="article-summary-box-inner">
<span><p>Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses
challenges in recognizing novel classes in the test phase. The development of
generative models enables current GZSL techniques to probe further into the
semantic-visual link, culminating in a two-stage form that includes a generator
and a classifier. However, existing generation-based methods focus on enhancing
the generator's effect while neglecting the improvement of the classifier. In
this paper, we first analyze of two properties of the generated pseudo unseen
samples: bias and homogeneity. Then, we perform variational Bayesian inference
to back-derive the evaluation metrics, which reflects the balance of the seen
and unseen classes. As a consequence of our derivation, the aforementioned two
properties are incorporated into the classifier training as seen-unseen priors
via logit adjustment. The Zero-Shot Logit Adjustment further puts
semantic-based classifiers into effect in generation-based GZSL. Our
experiments demonstrate that the proposed technique achieves state-of-the-art
when combined with the basic generator, and it can improve various generative
Zero-Shot Learning frameworks. Our codes are available on
https://github.com/cdb342/IJCAI-2022-ZLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12037">
<div class="article-summary-box-inner">
<span><p>Spatial-temporal representation learning is ubiquitous in various real-world
applications, including visual comprehension, video understanding, multi-modal
analysis, human-computer interaction, and urban computing. Due to the emergence
of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal
data in big data era, the lack of interpretability, robustness, and
out-of-distribution generalization are becoming the challenges of the existing
visual models. The majority of the existing methods tend to fit the original
data/variable distributions, which lack an unified guidance and analysis about
why modern spatial-temporal representation learning methods are easily collapse
into data bias and have limited cognitive ability. Inspired by the strong
inference ability of human-level agents, recent years have therefore witnessed
great effort in developing causal reasoning paradigms to realize robust
representation and model learning with good cognitive ability. In this paper,
we conduct a comprehensive review of existing causal reasoning methods for
spatial-temporal representation learning, covering fundamental theories,
models, and datasets. The limitations of current methods and datasets are also
discussed. Moreover, we propose some primary challenges, opportunities, and
future research directions for benchmarking causal reasoning algorithms in
spatial-temporal representation learning. This paper aims to provide a
comprehensive overview of this emerging field, attract attention, encourage
discussions, bring to the forefront the urgency of developing novel causal
reasoning methods, publicly available benchmarks, and consensus-building
standards for reliable spatial-temporal representation learning and related
real-world applications more efficiently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment. (arXiv:2204.12283v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12283">
<div class="article-summary-box-inner">
<span><p>Understanding the impact of tumor biology on the composition of nearby cells
often requires characterizing the impact of biologically distinct tumor
regions. Biomarkers have been developed to label biologically distinct tumor
regions, but challenges arise because of differences in the spatial extent and
distribution of differentially labeled regions. In this work, we present a
framework for systematically investigating the impact of distinct tumor regions
on cells near the tumor borders, accounting their cross spatial distributions.
We apply the framework to multiplex immunohistochemistry (mIHC) studies of
pancreatic cancer and show its efficacy in demonstrating how biologically
different tumor regions impact the immune response in the tumor
microenvironment. Furthermore, we show that the proposed framework can be
extended to largescale whole slide image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12454">
<div class="article-summary-box-inner">
<span><p>Multiple Instance Learning (MIL) methods have become increasingly popular for
classifying giga-pixel sized Whole-Slide Images (WSIs) in digital pathology.
Most MIL methods operate at a single WSI magnification, by processing all the
tissue patches. Such a formulation induces high computational requirements, and
constrains the contextualization of the WSI-level representation to a single
scale. A few MIL methods extend to multiple scales, but are computationally
more demanding. In this paper, inspired by the pathological diagnostic process,
we propose ZoomMIL, a method that learns to perform multi-level zooming in an
end-to-end manner. ZoomMIL builds WSI representations by aggregating
tissue-context information from multiple magnifications. The proposed method
outperforms the state-of-the-art MIL methods in WSI classification on two large
datasets, while significantly reducing the computational demands with regard to
Floating-Point Operations (FLOPs) and processing time by up to 40x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-free few-shot learning via representation learning with weight averaging. (arXiv:2204.12466v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12466">
<div class="article-summary-box-inner">
<span><p>Recent studies on few-shot classification using transfer learning pose
challenges to the effectiveness and efficiency of episodic meta-learning
algorithms. Transfer learning approaches are a natural alternative, but they
are restricted to few-shot classification. Moreover, little attention has been
on the development of probabilistic models with well-calibrated uncertainty
from few-shot samples, except for some Bayesian episodic learning algorithms.
To tackle the aforementioned issues, we propose a new transfer learning method
to obtain accurate and reliable models for few-shot regression and
classification. The resulting method does not require episodic meta-learning
and is called meta-free representation learning (MFRL). MFRL first finds
low-rank representation generalizing well on meta-test tasks. Given the learned
representation, probabilistic linear models are fine-tuned with few-shot
samples to obtain models with well-calibrated uncertainty. The proposed method
not only achieves the highest accuracy on a wide range of few-shot learning
benchmark datasets but also correctly quantifies the prediction uncertainty. In
addition, weight averaging and temperature scaling are effective in improving
the accuracy and reliability of few-shot learning in existing meta-learning
algorithms with a wide range of learning paradigms and model architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-fine Q-attention with Tree Expansion. (arXiv:2204.12471v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12471">
<div class="article-summary-box-inner">
<span><p>Coarse-to-fine Q-attention enables sample-efficient robot manipulation by
discretizing the translation space in a coarse-to-fine manner, where the
resolution gradually increases at each layer in the hierarchy. Although
effective, Q-attention suffers from "coarse ambiguity" - when voxelization is
significantly coarse, it is not feasible to distinguish similar-looking objects
without first inspecting at a finer resolution. To combat this, we propose to
envision Q-attention as a tree that can be expanded and used to accumulate
value estimates across the top-k voxels at each Q-attention depth. When our
extension, Q-attention with Tree Expansion (QTE), replaces standard Q-attention
in the Attention-driven Robot Manipulation (ARM) system, we are able to
accomplish a larger set of tasks; especially on those that suffer from "coarse
ambiguity". In addition to evaluating our approach across 12 RLBench tasks, we
also show that the improved performance is visible in a real-world task
involving small objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13483">
<div class="article-summary-box-inner">
<span><p>The new generation of 4D high-resolution imaging radar provides not only a
huge amount of point cloud but also additional elevation measurement, which has
a great potential of 3D sensing in autonomous driving. In this paper, we
introduce an autonomous driving dataset named TJ4DRadSet, including multi-modal
sensors that are 4D radar, lidar, camera and GNSS, with about 40K frames in
total. 7757 frames within 44 consecutive sequences in various driving scenarios
are well annotated with 3D bounding boxes and track id. We provide a 4D
radar-based 3D object detection baseline for our dataset to demonstrate the
effectiveness of deep learning methods for 4D radar point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14079">
<div class="article-summary-box-inner">
<span><p>Transfer learning of StyleGAN has recently shown great potential to solve
diverse tasks, especially in domain translation. Previous methods utilized a
source model by swapping or freezing weights during transfer learning, however,
they have limitations on visual quality and controlling source features. In
other words, they require additional models that are computationally demanding
and have restricted control steps that prevent a smooth transition. In this
paper, we propose a new approach to overcome these limitations. Instead of
swapping or freezing, we introduce a simple feature matching loss to improve
generation quality. In addition, to control the degree of source features, we
train a target model with the proposed strategy, FixNoise, to preserve the
source features only in a disentangled subspace of a target feature space.
Owing to the disentangled feature space, our method can smoothly control the
degree of the source features in a single model. Extensive experiments
demonstrate that the proposed method can generate more consistent and realistic
images than previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v2 [cs.SD] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14057">
<div class="article-summary-box-inner">
<span><p>We present an approach to learn voice-face representations from the talking
face videos, without any identity labels. Previous works employ cross-modal
instance discrimination tasks to establish the correlation of voice and face.
These methods neglect the semantic content of different videos, introducing
false-negative pairs as training noise. Furthermore, the positive pairs are
constructed based on the natural correlation between audio clips and visual
frames. However, this correlation might be weak or inaccurate in a large amount
of real-world data, which leads to deviating positives into the contrastive
paradigm. To address these issues, we propose the cross-modal prototype
contrastive learning (CMPC), which takes advantage of contrastive methods and
resists adverse effects of false negatives and deviate positives. On one hand,
CMPC could learn the intra-class invariance by constructing semantic-wise
positives via unsupervised clustering in different modalities. On the other
hand, by comparing the similarities of cross-modal instances from that of
cross-modal prototypes, we dynamically recalibrate the unlearnable instances'
contribution to overall loss. Experiments show that the proposed approach
outperforms state-of-the-art unsupervised methods on various voice-face
association evaluation protocols. Additionally, in the low-shot supervision
setting, our method also has a significant improvement compared to previous
instance-wise contrastive learning.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-03 23:08:10.547434124 UTC">2022-05-03 23:08:10 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>