<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-07T01:30:00Z">02-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Platform Difference in Facebook and Text Messages Language Use: Illustrated by Depression Diagnosis. (arXiv:2202.01802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01802">
<div class="article-summary-box-inner">
<span><p>How does language differ across one's Facebook status updates vs. one's text
messages (SMS)? In this study, we show how Facebook and SMS use differs in
psycho-linguistic characteristics and how these differences drive downstream
analyses with an illustration of depression diagnosis. We use a sample of
consenting participants who shared Facebook status updates, SMS data, and
answered a standard psychological depression screener. We quantify domain
differences using psychologically driven lexical methods and find that language
on Facebook involves more personal concerns, experiences, and content features
while the language in SMS contains more informal and style features. Next, we
estimate depression from both text domains, using a depression model trained on
Facebook data, and find a drop in accuracy when predicting self-reported
depression assessments from the SMS-based depression estimates. Finally, we
evaluate a simple domain adaption correction based on words driving the
cross-platform differences and applied it to the SMS-derived depression
estimates, resulting in significant improvement in prediction. Our work shows
the Facebook vs. SMS difference in language use and suggests the necessity of
cross-domain adaption for text-based predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning with Random-projection Quantizer for Speech Recognition. (arXiv:2202.01855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01855">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective self-supervised learning approach for
speech recognition. The approach learns a model to predict the masked speech
signals, in the form of discrete labels generated with a random-projection
quantizer. In particular the quantizer projects speech inputs with a randomly
initialized matrix, and does a nearest-neighbor lookup in a
randomly-initialized codebook. Neither the matrix nor the codebook is updated
during self-supervised learning. Since the random-projection quantizer is not
trained and is separated from the speech recognition model, the design makes
the approach flexible and is compatible with universal speech recognition
architecture. On LibriSpeech our approach achieves similar word-error-rates as
previous work using self-supervised learning with non-streaming models, and
provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with
streaming models. On multilingual tasks the approach also provides significant
improvement over wav2vec 2.0 and w2v-BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Aspect-Based Sentiment Analysis. (arXiv:2202.01924v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01924">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) typically requires in-domain annotated
data for supervised training/fine-tuning. It is a big challenge to scale ABSA
to a large number of new domains. This paper aims to train a unified model that
can perform zero-shot ABSA without using any annotated data for a new domain.
We propose a method called contrastive post-training on review Natural Language
Inference (CORN). Later ABSA tasks can be cast into NLI for zero-shot transfer.
We evaluate CORN on ABSA tasks, ranging from aspect extraction (AE), aspect
sentiment classification (ASC), to end-to-end aspect-based sentiment analysis
(E2E ABSA), which show ABSA can be conducted without any human annotated ABSA
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01993">
<div class="article-summary-box-inner">
<span><p>Visual question answering is the task of answering questions about images. We
introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually
grounds answers to visual questions asked by people with visual impairments. We
analyze our dataset and compare it with five VQA-Grounding datasets to
demonstrate what makes it similar and different. We then evaluate the SOTA VQA
and VQA-Grounding models and demonstrate that current SOTA algorithms often
fail to identify the correct visual evidence where the answer is located. These
models regularly struggle when the visual evidence occupies a small fraction of
the image, for images that are higher quality, as well as for visual questions
that require skills in text recognition. The dataset, evaluation server, and
leaderboard all can be found at the following link:
https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Scaling Laws in NMT: The Effect of Noise and Architecture. (arXiv:2202.01994v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01994">
<div class="article-summary-box-inner">
<span><p>In this work, we study the effect of varying the architecture and training
data quality on the data scaling properties of Neural Machine Translation
(NMT). First, we establish that the test loss of encoder-decoder transformer
models scales as a power law in the number of training samples, with a
dependence on the model size. Then, we systematically vary aspects of the
training setup to understand how they impact the data scaling laws. In
particular, we change the following (1) Architecture and task setup: We compare
to a transformer-LSTM hybrid, and a decoder-only transformer with a language
modeling loss (2) Noise level in the training distribution: We experiment with
filtering, and adding iid synthetic noise. In all the above cases, we find that
the data scaling exponents are minimally impacted, suggesting that marginally
worse architectures or training data can be compensated for by adding more
data. Lastly, we find that using back-translated data instead of parallel data,
can significantly degrade the scaling exponent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark Corpus for the Detection of Automatically Generated Text in Academic Publications. (arXiv:2202.02013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02013">
<div class="article-summary-box-inner">
<span><p>Automatic text generation based on neural language models has achieved
performance levels that make the generated text almost indistinguishable from
those written by humans. Despite the value that text generation can have in
various applications, it can also be employed for malicious tasks. The
diffusion of such practices represent a threat to the quality of academic
publishing. To address these problems, we propose in this paper two datasets
comprised of artificially generated research content: a completely synthetic
dataset and a partial text substitution dataset. In the first case, the content
is completely generated by the GPT-2 model after a short prompt extracted from
original papers. The partial or hybrid dataset is created by replacing several
sentences of abstracts with sentences that are generated by the Arxiv-NLP
model. We evaluate the quality of the datasets comparing the generated texts to
aligned original texts using fluency metrics such as BLEU and ROUGE. The more
natural the artificial texts seem, the more difficult they are to detect and
the better is the benchmark. We also evaluate the difficulty of the task of
distinguishing original from generated text by using state-of-the-art
classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking Discourse Influence in Darknet Forums. (arXiv:2202.02081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02081">
<div class="article-summary-box-inner">
<span><p>This technical report documents our efforts in addressing the tasks set forth
by the 2021 AMoC (Advanced Modelling of Cyber Criminal Careers) Hackathon. Our
main contribution is a joint visualisation of semantic and temporal features,
generating insight into the supplied data on darknet cybercrime through the
aspects of novelty, transience, and resonance, which describe the potential
impact a message might have on the overall discourse in darknet communities.
All code and data produced by us as part of this hackathon is publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Attention for Language Models. (arXiv:2202.02093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02093">
<div class="article-summary-box-inner">
<span><p>Pretrained language models based on the transformer architecture have shown
great success in NLP. Textual training data often comes from the web and is
thus tagged with time-specific information, but most language models ignore
this information. They are trained on the textual data alone, limiting their
ability to generalize temporally. In this work, we extend the key component of
the transformer architecture, i.e., the self-attention mechanism, and propose
temporal attention - a time-aware self-attention mechanism. Temporal attention
can be applied to any transformer model and requires the input texts to be
accompanied with their relevant time points. It allows the transformer to
capture this temporal information and create time-specific contextualized word
representations. We leverage these representations for the task of semantic
change detection; we apply our proposed mechanism to BERT and experiment on
three datasets in different languages (English, German, and Latin) that also
vary in time, size, and genre. Our proposed model achieves state-of-the-art
results on all the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02113">
<div class="article-summary-box-inner">
<span><p>Knowledge graph completion aims to address the problem of extending a KG with
missing triples. In this paper, we provide an approach GenKGC, which converts
knowledge graph completion to sequence-to-sequence generation task with the
pre-trained language model. We further introduce relation-guided demonstration
and entity-aware hierarchical decoding for better representation learning and
fast inference. Experimental results on three datasets show that our approach
can obtain better or comparable performance than baselines and achieve faster
inference speed compared with previous methods with pre-trained language
models. We also release a new large-scale Chinese knowledge graph dataset
AliopenKG500 for research purpose. Code and datasets are available in
https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Ecological Footprint of Neural Machine Translation Systems. (arXiv:2202.02170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02170">
<div class="article-summary-box-inner">
<span><p>Over the past decade, deep learning (DL) has led to significant advancements
in various fields of artificial intelligence, including machine translation
(MT). These advancements would not be possible without the ever-growing volumes
of data and the hardware that allows large DL models to be trained efficiently.
Due to the large amount of computing cores as well as dedicated memory,
graphics processing units (GPUs) are a more effective hardware solution for
training and inference with DL models than central processing units (CPUs).
However, the former is very power demanding. The electrical power consumption
has economical as well as ecological implications.
</p>
<p>This chapter focuses on the ecological footprint of neural MT systems. It
starts from the power drain during the training of and the inference with
neural MT models and moves towards the environment impact, in terms of carbon
dioxide emissions. Different architectures (RNN and Transformer) and different
GPUs (consumer-grate NVidia 1080Ti and workstation-grade NVidia P100) are
compared. Then, the overall CO2 offload is calculated for Ireland and the
Netherlands. The NMT models and their ecological impact are compared to common
household appliances to draw a more clear picture.
</p>
<p>The last part of this chapter analyses quantization, a technique for reducing
the size and complexity of models, as a way to reduce power consumption. As
quantized models can run on CPUs, they present a power-efficient inference
solution without depending on a GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?. (arXiv:2202.02268v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02268">
<div class="article-summary-box-inner">
<span><p>To answer this question, we fine-tune transformer-based language models,
including BERT, on different sources of company-related text data for a
classification task to predict the one-year stock price performance. We use
three different types of text data: News articles, blogs, and annual reports.
This allows us to analyze to what extent the performance of language models is
dependent on the type of the underlying document. StonkBERT, our
transformer-based stock performance classifier, shows substantial improvement
in predictive accuracy compared to traditional language models. The highest
performance was achieved with news articles as text source. Performance
simulations indicate that these improvements in classification accuracy also
translate into above-average stock market returns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Trained Neural Language Models for Automatic Mobile App User Feedback Answer Generation. (arXiv:2202.02294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02294">
<div class="article-summary-box-inner">
<span><p>Studies show that developers' answers to the mobile app users' feedbacks on
app stores can increase the apps' star rating. To help app developers generate
answers that are related to the users' issues, recent studies develop models to
generate the answers automatically. Aims: The app response generation models
use deep neural networks and require training data. Pre-Trained neural language
Models (PTM) used in Natural Language Processing (NLP) take advantage of the
information they learned from a large corpora in an unsupervised manner, and
can reduce the amount of required training data. In this paper, we evaluate
PTMs to generate replies to the mobile app user feedbacks. Method: We train a
Transformer model from scratch and fine-tune two PTMs to evaluate the generated
responses, which are compared to RRGEN, a current app response model. We also
evaluate the models with different portions of the training data. Results: The
results on a large dataset evaluated by automatic metrics show that PTMs obtain
lower scores than the baselines. However, our human evaluation confirms that
PTMs can generate more relevant and meaningful responses to the posted
feedbacks. Moreover, the performance of PTMs has less drop compared to other
models when the amount of training data is reduced to 1/3. Conclusion: PTMs are
useful in generating responses to app reviews and are more robust models to the
amount of training data provided. However, the prediction time is 19X than
RRGEN. This study can provide new avenues for research in adapting the PTMs for
analyzing mobile app user feedbacks. Index Terms-mobile app user feedback
analysis, neural pre-trained language models, automatic answer generation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Mobile App Navigation with Uncertain or Under-specified Natural Language Commands. (arXiv:2202.02312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02312">
<div class="article-summary-box-inner">
<span><p>We introduce Mobile app Tasks with Iterative Feedback (MoTIF), a new dataset
where the goal is to complete a natural language query in a mobile app. Current
datasets for related tasks in interactive question answering, visual common
sense reasoning, and question-answer plausibility prediction do not support
research in resolving ambiguous natural language requests or operating in
diverse digital domains. As a result, they fail to capture complexities of real
question answering or interactive tasks. In contrast, MoTIF contains natural
language requests that are not satisfiable, the first such work to investigate
this issue for interactive vision-language tasks. MoTIF also contains follow up
questions for ambiguous queries to enable research on task uncertainty
resolution. We introduce task feasibility prediction and propose an initial
model which obtains an F1 score of 61.1. We next benchmark task automation with
our dataset and find adaptations of prior work perform poorly due to our
realistic language requests, obtaining an accuracy of only 20.2% when mapping
commands to grounded actions. We analyze performance and gain insight for
future work that may bridge the gap between current model ability and what is
needed for successful use in application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02317">
<div class="article-summary-box-inner">
<span><p>General purpose vision (GPV) systems are models that are designed to solve a
wide array of visual tasks without requiring architectural changes. Today, GPVs
primarily learn both skills and concepts from large fully supervised datasets.
Scaling GPVs to tens of thousands of concepts by acquiring data to learn each
concept for every skill quickly becomes prohibitive. This work presents an
effective and inexpensive alternative: learn skills from fully supervised
datasets, learn concepts from web image search results, and leverage a key
characteristic of GPVs -- the ability to transfer visual knowledge across
skills. We use a dataset of 1M+ images spanning 10k+ visual concepts to
demonstrate webly-supervised concept expansion for two existing GPVs (GPV-1 and
VL-T5) on 3 benchmarks - 5 COCO based datasets (80 primary concepts), a newly
curated series of 5 datasets based on the OpenImages and VisualGenome
repositories (~500 concepts) and the Web-derived dataset (10k+ concepts). We
also propose a new architecture, GPV-2 that supports a variety of tasks -- from
vision tasks like classification and localization to vision+language tasks like
QA and captioning to more niche ones like human-object interaction recognition.
GPV-2 benefits hugely from web data, outperforms GPV-1 and VL-T5 across these
benchmarks, and does well in a 0-shot setting at action and attribute
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Incorrectness Logic and Kleene Algebra with Top and Tests. (arXiv:2108.07707v3 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07707">
<div class="article-summary-box-inner">
<span><p>Kleene algebra with tests (KAT) is a foundational equational framework for
reasoning about programs, which has found applications in program
transformations, networking and compiler optimizations, among many other areas.
In his seminal work, Kozen proved that KAT subsumes propositional Hoare logic,
showing that one can reason about the (partial) correctness of while programs
by means of the equational theory of KAT. In this work, we investigate the
support that KAT provides for reasoning about incorrectness, instead, as
embodied by Ohearn's recently proposed incorrectness logic. We show that KAT
cannot directly express incorrectness logic. The main reason for this
limitation can be traced to the fact that KAT cannot express explicitly the
notion of codomain, which is essential to express incorrectness triples. To
address this issue, we study Kleene Algebra with Top and Tests (TopKAT), an
extension of KAT with a top element. We show that TopKAT is powerful enough to
express a codomain operation, to express incorrectness triples, and to prove
all the rules of incorrectness logic sound. This shows that one can reason
about the incorrectness of while-like programs by means of the equational
theory of TopKAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering. (arXiv:2109.12264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12264">
<div class="article-summary-box-inner">
<span><p>Textual Question Answering (QA) aims to provide precise answers to user's
questions in natural language using unstructured data. One of the most popular
approaches to this goal is machine reading comprehension(MRC). In recent years,
many novel datasets and evaluation metrics based on classical MRC tasks have
been proposed for broader textual QA tasks. In this paper, we survey 47 recent
textual QA benchmark datasets and propose a new taxonomy from an application
point of view. In addition, We summarize 8 evaluation metrics of textual QA
tasks. Finally, we discuss current trends in constructing textual QA benchmarks
and suggest directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-conditioning pre-trained language models. (arXiv:2110.02802v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02802">
<div class="article-summary-box-inner">
<span><p>In this paper we aim to investigate the mechanisms that guide text generation
with pre-trained Transformer-based Language Models (TLMs). Grounded on the
Product of Experts formulation by Hinton (1999), we describe a generative
mechanism that exploits expert units which naturally exist in TLMs. Such units
are responsible for detecting concepts in the input and conditioning text
generation on such concepts. We describe how to identify expert units and how
to activate them during inference in order to induce any desired concept in the
generated output. We find that the activation of a surprisingly small amount of
units is sufficient to steer text generation (as little as 3 units in a model
with 345M parameters). While the objective of this work is to learn more about
how TLMs work, we show that our method is effective for conditioning without
fine-tuning or using extra parameters, even on fine-grained homograph concepts.
Additionally, we show that our method can be used to correct gender bias
present in the output of TLMs and achieves gender parity for all evaluated
contexts. We compare our method with FUDGE and PPLM-BoW, and show that our
approach is able to achieve gender parity at a lower perplexity. The proposed
method is accessible to a wide audience thanks to its simplicity and minimal
compute needs. The findings in this paper are a step forward in understanding
the generative mechanisms of TLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phone-to-audio alignment without text: A Semi-supervised Approach. (arXiv:2110.03876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03876">
<div class="article-summary-box-inner">
<span><p>The task of phone-to-audio alignment has many applications in speech
research. Here we introduce two Wav2Vec2-based models for both text-dependent
and text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a
semi-supervised model, directly learns phone-to-audio alignment through
contrastive learning and a forward sum loss, and can be coupled with a
pretrained phone recognizer to achieve text-independent alignment. The other
model, Wav2Vec2-FC, is a frame classification model trained on forced aligned
labels that can both perform forced alignment and text-independent
segmentation. Evaluation results suggest that both proposed methods, even when
transcriptions are not available, generate highly close results to existing
forced alignment tools. Our work presents a neural pipeline of fully automated
phone-to-audio alignment. Code and pretrained models are available at
https://github.com/lingjzhu/charsiu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04260">
<div class="article-summary-box-inner">
<span><p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can
easily scale to have outrageously large amounts of parameters without
significant increase in computational cost. However, SAMs are reported to be
parameter inefficient such that larger models do not always lead to better
performance. While most on-going research focuses on improving SAMs models by
exploring methods of routing inputs to experts, our analysis reveals that such
research might not lead to the solution we expect, i.e., the commonly-used
routing methods based on gating mechanisms do not work better than randomly
routing inputs to experts. In this paper, we propose a new expert-based model,
THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,
such as the Switch Transformer, experts in THOR are randomly activated for each
input during training and inference. THOR models are trained using a
consistency regularized loss, where experts learn not only from training data
but also from other experts as teachers, such that all the experts make
consistent predictions. We validate the effectiveness of THOR on machine
translation tasks. Results show that THOR models are more parameter efficient
in that they significantly outperform the Transformer and MoE models across
various settings. For example, in multilingual translation, THOR outperforms
the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as
that of a state-of-the-art MoE model that is 18 times larger. Our code is
publicly available at:
https://github.com/microsoft/Stochastic-Mixture-of-Experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Heterogeneous Characteristics of Layers in ASR Models for More Efficient Training. (arXiv:2110.04267v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04267">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures have been the subject of research aimed at
understanding their overparameterization and the non-uniform importance of
their layers. Applying these approaches to Automatic Speech Recognition, we
demonstrate that the state-of-the-art Conformer models generally have multiple
ambient layers. We study the stability of these layers across runs and model
sizes, propose that group normalization may be used without disrupting their
formation, and examine their correlation with model weight updates in each
layer. Finally, we apply these findings to Federated Learning in order to
improve the training procedure, by targeting Federated Dropout to layers by
importance. This allows us to reduce the model size optimized by clients
without quality degradation, and shows potential for future exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. (arXiv:2201.11990v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11990">
<div class="article-summary-box-inner">
<span><p>Pretrained general-purpose language models can achieve state-of-the-art
accuracies in various natural language processing domains by adapting to
downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of
their success, the size of these models has increased rapidly, requiring
high-performance hardware, software, and algorithmic techniques to enable
training such large models. As the result of a joint effort between Microsoft
and NVIDIA, we present details on the training of the largest monolithic
transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530
billion parameters. In this paper, we first focus on the infrastructure as well
as the 3D parallelism methodology used to train this model using DeepSpeed and
Megatron. Next, we detail the training process, the design of our training
corpus, and our data curation techniques, which we believe is a key ingredient
to the success of the model. Finally, we discuss various evaluation results, as
well as other interesting observations and new properties exhibited by MT-NLG.
We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning
accuracies on several NLP benchmarks and establishes new state-of-the-art
results. We believe that our contributions will help further the development of
large-scale training infrastructures, large-scale language models, and natural
language generations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFA: TDNN with Multi-scale Frequency-channel Attention for Text-independent Speaker Verification with Short Utterances. (arXiv:2202.01624v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01624">
<div class="article-summary-box-inner">
<span><p>The time delay neural network (TDNN) represents one of the state-of-the-art
of neural solutions to text-independent speaker verification. However, they
require a large number of filters to capture the speaker characteristics at any
local frequency region. In addition, the performance of such systems may
degrade under short utterance scenarios. To address these issues, we propose a
multi-scale frequency-channel attention (MFA), where we characterize speakers
at different scales through a novel dual-path design which consists of a
convolutional neural network and TDNN. We evaluate the proposed MFA on the
VoxCeleb database and observe that the proposed framework with MFA can achieve
state-of-the-art performance while reducing parameters and computation
complexity. Further, the MFA mechanism is found to be effective for speaker
verification with short test utterances.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the impact of urban change in pedestrian and road safety. (arXiv:2202.01781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01781">
<div class="article-summary-box-inner">
<span><p>Increased interaction between and among pedestrians and vehicles in the
crowded urban environments of today gives rise to a negative side-effect: a
growth in traffic accidents, with pedestrians being the most vulnerable
elements. Recent work has shown that Convolutional Neural Networks are able to
accurately predict accident rates exploiting Street View imagery along urban
roads. The promising results point to the plausibility of aided design of safe
urban landscapes, for both pedestrians and vehicles. In this paper, by
considering historical accident data and Street View images, we detail how to
automatically predict the impact (increase or decrease) of urban interventions
on accident incidence. The results are positive, rendering an accuracies
ranging from 60 to 80%. We additionally provide an interpretability analysis to
unveil which specific categories of urban features impact accident rates
positively or negatively. Considering the transportation network substrates
(sidewalk and road networks) and their demand, we integrate these results to a
complex network framework, to estimate the effective impact of urban change on
the safety of pedestrians and vehicles. Results show that public authorities
may leverage on machine learning tools to prioritize targeted interventions,
since our analysis show that limited improvement is obtained with current
tools. Further, our findings have a wider application range such as the design
of safe urban routes for pedestrians or to the field of driver-assistance
technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retinal Vessel Segmentation with Pixel-wise Adaptive Filters. (arXiv:2202.01782v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01782">
<div class="article-summary-box-inner">
<span><p>Accurate retinal vessel segmentation is challenging because of the complex
texture of retinal vessels and low imaging contrast. Previous methods generally
refine segmentation results by cascading multiple deep networks, which are
time-consuming and inefficient. In this paper, we propose two novel methods to
address these challenges. First, we devise a light-weight module, named
multi-scale residual similarity gathering (MRSG), to generate pixel-wise
adaptive filters (PA-Filters). Different from cascading multiple deep networks,
only one PA-Filter layer can improve the segmentation results. Second, we
introduce a response cue erasing (RCE) strategy to enhance the segmentation
accuracy. Experimental results on the DRIVE, CHASE_DB1, and STARE datasets
demonstrate that our proposed method outperforms state-of-the-art methods while
maintaining a compact structure. Code is available at
https://github.com/Limingxing00/Retinal-Vessel-Segmentation-ISBI20222.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Oral cancer detection and interpretation: Deep multiple instance learning versus conventional deep single instance learning. (arXiv:2202.01783v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01783">
<div class="article-summary-box-inner">
<span><p>The current medical standard for setting an oral cancer (OC) diagnosis is
histological examination of a tissue sample from the oral cavity. This process
is time consuming and more invasive than an alternative approach of acquiring a
brush sample followed by cytological analysis. Skilled cytotechnologists are
able to detect changes due to malignancy, however, to introduce this approach
into clinical routine is associated with challenges such as a lack of experts
and labour-intensive work. To design a trustworthy OC detection system that
would assist cytotechnologists, we are interested in AI-based methods that
reliably can detect cancer given only per-patient labels (minimizing annotation
bias), and also provide information on which cells are most relevant for the
diagnosis (enabling supervision and understanding). We, therefore, perform a
comparison of a conventional single instance learning (SIL) approach and a
modern multiple instance learning (MIL) method suitable for OC detection and
interpretation, utilizing three different neural network architectures. To
facilitate systematic evaluation of the considered approaches, we introduce a
synthetic PAP-QMNIST dataset, that serves as a model of OC data, while offering
access to per-instance ground truth. Our study indicates that on PAP-QMNIST,
the SIL performs better, on average, than the MIL approach. Performance at the
bag level on real-world cytological data is similar for both methods, yet the
single instance approach performs better on average. Visual examination by
cytotechnologist indicates that the methods manage to identify cells which
deviate from normality, including malignant cells as well as those suspicious
for dysplasia. We share the code as open source at
https://github.com/MIDA-group/OralCancerMILvsSIL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Surface Reconstruction from Point Clouds with Visibility Information. (arXiv:2202.01810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01810">
<div class="article-summary-box-inner">
<span><p>Most current neural networks for reconstructing surfaces from point clouds
ignore sensor poses and only operate on raw point locations. Sensor visibility,
however, holds meaningful information regarding space occupancy and surface
orientation. In this paper, we present two simple ways to augment raw point
clouds with visibility information, so it can directly be leveraged by surface
reconstruction networks with minimal adaptation. Our proposed modifications
consistently improve the accuracy of generated surfaces as well as the
generalization ability of the networks to unseen shape domains. Our code and
data is available at https://github.com/raphaelsulzer/dsrv-data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking. (arXiv:2202.01811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01811">
<div class="article-summary-box-inner">
<span><p>Object detectors, which are widely deployed in security-critical systems such
as autonomous vehicles, have been found vulnerable to physical-world patch
hiding attacks. The attacker can use a single physically-realizable adversarial
patch to make the object detector miss the detection of victim objects and
completely undermines the functionality of object detection applications. In
this paper, we propose ObjectSeeker as a defense framework for building
certifiably robust object detectors against patch hiding attacks. The core
operation of ObjectSeeker is patch-agnostic masking: we aim to mask out the
entire adversarial patch without any prior knowledge of the shape, size, and
location of the patch. This masking operation neutralizes the adversarial
effect and allows any vanilla object detector to safely detect objects on the
masked images. Remarkably, we develop a certification procedure to determine if
ObjectSeeker can detect certain objects with a provable guarantee against any
adaptive attacker within the threat model. Our evaluation with two object
detectors and three datasets demonstrates a significant (~10%-40% absolute and
~2-6x relative) improvement in certified robustness over the prior work, as
well as high clean performance (~1% performance drop compared with vanilla
undefended models).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAFE-OCC: A Novelty Detection Framework for Convolutional Neural Network Sensors and its Application in Process Control. (arXiv:2202.01816v1 [math.OC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01816">
<div class="article-summary-box-inner">
<span><p>We present a novelty detection framework for Convolutional Neural Network
(CNN) sensors that we call Sensor-Activated Feature Extraction One-Class
Classification (SAFE-OCC). We show that this framework enables the safe use of
computer vision sensors in process control architectures. Emergent control
applications use CNN models to map visual data to a state signal that can be
interpreted by the controller. Incorporating such sensors introduces a
significant system operation vulnerability because CNN sensors can exhibit high
prediction errors when exposed to novel (abnormal) visual data. Unfortunately,
identifying such novelties in real-time is nontrivial. To address this issue,
the SAFE-OCC framework leverages the convolutional blocks of the CNN to create
an effective feature space to conduct novelty detection using a desired
one-class classification technique. This approach engenders a feature space
that directly corresponds to that used by the CNN sensor and avoids the need to
derive an independent latent space. We demonstrate the effectiveness of
SAFE-OCC via simulated control environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Danish Airs and Grounds: A Dataset for Aerial-to-Street-Level Place Recognition and Localization. (arXiv:2202.01821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01821">
<div class="article-summary-box-inner">
<span><p>Place recognition and visual localization are particularly challenging in
wide baseline configurations. In this paper, we contribute with the
\emph{Danish Airs and Grounds} (DAG) dataset, a large collection of
street-level and aerial images targeting such cases. Its main challenge lies in
the extreme viewing-angle difference between query and reference images with
consequent changes in illumination and perspective. The dataset is larger and
more diverse than current publicly available data, including more than 50 km of
road in urban, suburban and rural areas. All images are associated with
accurate 6-DoF metadata that allows the benchmarking of visual localization
methods.
</p>
<p>We also propose a map-to-image re-localization pipeline, that first estimates
a dense 3D reconstruction from the aerial images and then matches query
street-level images to street-level renderings of the 3D model. The dataset can
be downloaded at: https://frederikwarburg.github.io/DAG
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRBF-Fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly implicits. (arXiv:2202.01829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01829">
<div class="article-summary-box-inner">
<span><p>Reconstruction of high-fidelity 3D objects or scenes is a fundamental
research problem. Recent advances in RGB-D fusion have demonstrated the
potential of producing 3D models from consumer-level RGB-D cameras. However,
due to the discrete nature and limited resolution of their surface
representations (e.g., point- or voxel-based), existing approaches suffer from
the accumulation of errors in camera tracking and distortion in the
reconstruction, which leads to an unsatisfactory 3D reconstruction. In this
paper, we present a method using on-the-fly implicits of Hermite Radial Basis
Functions (HRBFs) as a continuous surface representation for camera tracking in
an existing RGB-D fusion framework. Furthermore, curvature estimation and
confidence evaluation are coherently derived from the inherent surface
properties of the on-the-fly HRBF implicits, which devote to a data fusion with
better quality. We argue that our continuous but on-the-fly surface
representation can effectively mitigate the impact of noise with its robustness
and constrain the reconstruction with inherent surface smoothness when being
compared with discrete representations. Experimental results on various
real-world and synthetic datasets demonstrate that our HRBF-fusion outperforms
the state-of-the-art approaches in terms of tracking robustness and
reconstruction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brain Cancer Survival Prediction on Treatment-na ive MRI using Deep Anchor Attention Learning with Vision Transformer. (arXiv:2202.01857v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01857">
<div class="article-summary-box-inner">
<span><p>Image-based brain cancer prediction models, based on radiomics, quantify the
radiologic phenotype from magnetic resonance imaging (MRI). However, these
features are difficult to reproduce because of variability in acquisition and
preprocessing pipelines. Despite evidence of intra-tumor phenotypic
heterogeneity, the spatial diversity between different slices within an MRI
scan has been relatively unexplored using such methods. In this work, we
propose a deep anchor attention aggregation strategy with a Vision Transformer
to predict survival risk for brain cancer patients. A Deep Anchor Attention
Learning (DAAL) algorithm is proposed to assign different weights to
slice-level representations with trainable distance measurements. We evaluated
our method on N = 326 MRIs. Our results outperformed attention multiple
instance learning-based techniques. DAAL highlights the importance of critical
slices and corroborates the clinical intuition that inter-slice spatial
diversity can reflect disease severity and is implicated in outcome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Best Practices and Scoring System on Reviewing A.I. based Medical Imaging Papers: Part 1 Classification. (arXiv:2202.01863v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01863">
<div class="article-summary-box-inner">
<span><p>With the recent advances in A.I. methodologies and their application to
medical imaging, there has been an explosion of related research programs
utilizing these techniques to produce state-of-the-art classification
performance. Ultimately, these research programs culminate in submission of
their work for consideration in peer reviewed journals. To date, the criteria
for acceptance vs. rejection is often subjective; however, reproducible science
requires reproducible review. The Machine Learning Education Sub-Committee of
SIIM has identified a knowledge gap and a serious need to establish guidelines
for reviewing these studies. Although there have been several recent papers
with this goal, this present work is written from the machine learning
practitioners standpoint. In this series, the committee will address the best
practices to be followed in an A.I.-based study and present the required
sections in terms of examples and discussion of what should be included to make
the studies cohesive, reproducible, accurate, and self-contained. This first
entry in the series focuses on the task of image classification. Elements such
as dataset curation, data pre-processing steps, defining an appropriate
reference standard, data partitioning, model architecture and training are
discussed. The sections are presented as they would be detailed in a typical
manuscript, with content describing the necessary information that should be
included to make sure the study is of sufficient quality to be considered for
publication. The goal of this series is to provide resources to not only help
improve the review process for A.I.-based medical imaging papers, but to
facilitate a standard for the information that is presented within all
components of the research study. We hope to provide quantitative metrics in
what otherwise may be a qualitative review process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Organ at Risk Segmentation with Improved Deep Neural Networks. (arXiv:2202.01866v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01866">
<div class="article-summary-box-inner">
<span><p>Organ at risk (OAR) segmentation is a crucial step for treatment planning and
outcome determination in radiotherapy treatments of cancer patients. Several
deep learning based segmentation algorithms have been developed in recent
years, however, U-Net remains the de facto algorithm designed specifically for
biomedical image segmentation and has spawned many variants with known
weaknesses. In this study, our goal is to present simple architectural changes
in U-Net to improve its accuracy and generalization properties. Unlike many
other available studies evaluating their algorithms on single center data, we
thoroughly evaluate several variations of U-Net as well as our proposed
enhanced architecture on multiple data sets for an extensive and reliable study
of the OAR segmentation problem. Our enhanced segmentation model includes
(a)architectural changes in the loss function, (b)optimization framework, and
(c)convolution type. Testing on three publicly available multi-object
segmentation data sets, we achieved an average of 80% dice score compared to
the baseline U-Net performance of 63%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research on Patch Attentive Neural Process. (arXiv:2202.01884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01884">
<div class="article-summary-box-inner">
<span><p>Attentive Neural Process (ANP) improves the fitting ability of Neural Process
(NP) and improves its prediction accuracy, but the higher time complexity of
the model imposes a limitation on the length of the input sequence. Inspired by
models such as Vision Transformer (ViT) and Masked Auto-Encoder (MAE), we
propose Patch Attentive Neural Process (PANP) using image patches as input and
improve the structure of deterministic paths based on ANP, which allows the
model to extract image features more accurately and efficiently reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advances in MetaDL: AAAI 2021 challenge and workshop. (arXiv:2202.01890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01890">
<div class="article-summary-box-inner">
<span><p>To stimulate advances in metalearning using deep learning techniques
(MetaDL), we organized in 2021 a challenge and an associated workshop. This
paper presents the design of the challenge and its results, and summarizes
presentations made at the workshop. The challenge focused on few-shot learning
classification tasks of small images. Participants' code submissions were run
in a uniform manner, under tight computational constraints. This put pressure
on solution designs to use existing architecture backbones and/or pre-trained
networks. Winning methods featured various classifiers trained on top of the
second last layer of popular CNN backbones, fined-tuned on the meta-training
data (not necessarily in an episodic manner), then trained on the labeled
support and tested on the unlabeled query sets of the meta-test data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modified ResNet Model for MSI and MSS Classification of Gastrointestinal Cancer. (arXiv:2202.01905v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01905">
<div class="article-summary-box-inner">
<span><p>In this work, a modified ResNet model is proposed for the classification of
Microsatellite instability(MSI) and Microsatellite stability(MSS) of
gastrointestinal cancer. The performance of this model is analyzed and compared
with existing models. The proposed model surpassed the existing models with an
accuracy of 0.8981 and F1 score of 0.9178.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Projection-based Point Convolution for Efficient Point Cloud Segmentation. (arXiv:2202.01991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01991">
<div class="article-summary-box-inner">
<span><p>Understanding point cloud has recently gained huge interests following the
development of 3D scanning devices and the accumulation of large-scale 3D data.
Most point cloud processing algorithms can be classified as either point-based
or voxel-based methods, both of which have severe limitations in processing
time or memory, or both. To overcome these limitations, we propose
Projection-based Point Convolution (PPConv), a point convolutional module that
uses 2D convolutions and multi-layer perceptrons (MLPs) as its components. In
PPConv, point features are processed through two branches: point branch and
projection branch. Point branch consists of MLPs, while projection branch
transforms point features into a 2D feature map and then apply 2D convolutions.
As PPConv does not use point-based or voxel-based convolutions, it has
advantages in fast point cloud processing. When combined with a learnable
projection and effective feature fusion strategy, PPConv achieves superior
efficiency compared to state-of-the-art methods, even with a simple
architecture based on PointNet++. We demonstrate the efficiency of PPConv in
terms of the trade-off between inference time and segmentation performance. The
experimental results on S3DIS and ShapeNetPart show that PPConv is the most
efficient method among the compared ones. The code is available at
github.com/pahn04/PPConv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Answers for Visual Questions Asked by Visually Impaired People. (arXiv:2202.01993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01993">
<div class="article-summary-box-inner">
<span><p>Visual question answering is the task of answering questions about images. We
introduce the VizWiz-VQA-Grounding dataset, the first dataset that visually
grounds answers to visual questions asked by people with visual impairments. We
analyze our dataset and compare it with five VQA-Grounding datasets to
demonstrate what makes it similar and different. We then evaluate the SOTA VQA
and VQA-Grounding models and demonstrate that current SOTA algorithms often
fail to identify the correct visual evidence where the answer is located. These
models regularly struggle when the visual evidence occupies a small fraction of
the image, for images that are higher quality, as well as for visual questions
that require skills in text recognition. The dataset, evaluation server, and
leaderboard all can be found at the following link:
https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Dual Contouring. (arXiv:2202.01999v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01999">
<div class="article-summary-box-inner">
<span><p>We introduce neural dual contouring (NDC), a new data-driven approach to mesh
reconstruction based on dual contouring (DC). Like traditional DC, it produces
exactly one vertex per grid cell and one quad for each grid edge intersection,
a natural and efficient structure for reproducing sharp features. However,
rather than computing vertex locations and edge crossings with hand-crafted
functions that depend directly on difficult-to-obtain surface gradients, NDC
uses a neural network to predict them. As a result, NDC can be trained to
produce meshes from signed or unsigned distance fields, binary voxel grids, or
point clouds (with or without normals); and it can produce open surfaces in
cases where the input represents a sheet or partial surface. During experiments
with five prominent datasets, we find that NDC, when trained on one of the
datasets, generalizes well to the others. Furthermore, NDC provides better
surface reconstruction accuracy, feature preservation, output complexity,
triangle quality, and inference time in comparison to previous learned (e.g.,
neural marching cubes, convolutional occupancy networks) and traditional (e.g.,
Poisson) methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Multi-Atlas Segmentation Using Deep Neural Networks. (arXiv:2202.02000v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02000">
<div class="article-summary-box-inner">
<span><p>Multi-atlas segmentation (MAS) is a promising framework for medical image
segmentation. Generally, MAS methods register multiple atlases, i.e., medical
images with corresponding labels, to a target image; and the transformed atlas
labels can be combined to generate target segmentation via label fusion
schemes. Many conventional MAS methods employed the atlases from the same
modality as the target image. However, the number of atlases with the same
modality may be limited or even missing in many clinical applications. Besides,
conventional MAS methods suffer from the computational burden of registration
or label fusion procedures. In this work, we design a novel cross-modality MAS
framework, which uses available atlases from a certain modality to segment a
target image from another modality. To boost the computational efficiency of
the framework, both the image registration and label fusion are achieved by
well-designed deep neural networks. For the atlas-to-target image registration,
we propose a bi-directional registration network (BiRegNet), which can
efficiently align images from different modalities. For the label fusion, we
design a similarity estimation network (SimNet), which estimates the fusion
weight of each atlas by measuring its similarity to the target image. SimNet
can learn multi-scale information for similarity estimation to improve the
performance of label fusion. The proposed framework was evaluated by the left
ventricle and liver segmentation tasks on the MM-WHS and CHAOS datasets,
respectively. Results have shown that the framework is effective for
cross-modality MAS in both registration and label fusion. The code will be
released publicly on \url{https://github.com/NanYoMy/cmmas} once the manuscript
is accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The devil is in the labels: Semantic segmentation from sentences. (arXiv:2202.02002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02002">
<div class="article-summary-box-inner">
<span><p>We propose an approach to semantic segmentation that achieves
state-of-the-art supervised performance when applied in a zero-shot setting. It
thus achieves results equivalent to those of the supervised methods, on each of
the major semantic segmentation datasets, without training on those datasets.
This is achieved by replacing each class label with a vector-valued embedding
of a short paragraph that describes the class. The generality and simplicity of
this approach enables merging multiple datasets from different domains, each
with varying class labels and semantics. The resulting merged semantic
segmentation dataset of over 2 Million images enables training a model that
achieves performance equal to that of state-of-the-art supervised methods on 7
benchmark datasets, despite not using any images therefrom. By fine-tuning the
model on standard semantic segmentation datasets, we also achieve a significant
improvement over the state-of-the-art supervised segmentation on NYUD-V2 and
PASCAL-context at 60% and 65% mIoU, respectively. Based on the closeness of
language embeddings, our method can even segment unseen labels. Extensive
experiments demonstrate strong generalization to unseen image domains and
unseen labels, and that the method enables impressive performance improvements
in downstream applications, including depth estimation and instance
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-to-Image MLP-mixer for Image Reconstruction. (arXiv:2202.02018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02018">
<div class="article-summary-box-inner">
<span><p>Neural networks are highly effective tools for image reconstruction problems
such as denoising and compressive sensing. To date, neural networks for image
reconstruction are almost exclusively convolutional. The most popular
architecture is the U-Net, a convolutional network with a multi-resolution
architecture. In this work, we show that a simple network based on the
multi-layer perceptron (MLP)-mixer enables state-of-the art image
reconstruction performance without convolutions and without a multi-resolution
architecture, provided that the training set and the size of the network are
moderately large. Similar to the original MLP-mixer, the image-to-image
MLP-mixer is based exclusively on MLPs operating on linearly-transformed image
patches. Contrary to the original MLP-mixer, we incorporate structure by
retaining the relative positions of the image patches. This imposes an
inductive bias towards natural images which enables the image-to-image
MLP-mixer to learn to denoise images based on fewer examples than the original
MLP-mixer. Moreover, the image-to-image MLP-mixer requires fewer parameters to
achieve the same denoising performance than the U-Net and its parameters scale
linearly in the image resolution instead of quadratically as for the original
MLP-mixer. If trained on a moderate amount of examples for denoising, the
image-to-image MLP-mixer outperforms the U-Net by a slight margin. It also
outperforms the vision transformer tailored for image reconstruction and
classical un-trained methods such as BM3D, making it a very effective tool for
image reconstruction problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Image Inpainting via Robust Pure Quaternion Matrix Completion: Error Bound and Weighted Loss. (arXiv:2202.02063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02063">
<div class="article-summary-box-inner">
<span><p>In this paper, we study color image inpainting as a pure quaternion matrix
completion problem. In the literature, the theoretical guarantee for quaternion
matrix completion is not well-established. Our main aim is to propose a new
minimization problem with an objective combining nuclear norm and a quadratic
loss weighted among three channels. To fill the theoretical vacancy, we obtain
the error bound in both clean and corrupted regimes, which relies on some new
results of quaternion matrices. A general Gaussian noise is considered in
robust completion where all observations are corrupted. Motivated by the error
bound, we propose to handle unbalanced or correlated noise via a cross-channel
weight in the quadratic loss, with the main purpose of rebalancing noise level,
or removing noise correlation. Extensive experimental results on synthetic and
color image data are presented to confirm and demonstrate our theoretical
findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CGS-Net: Aggregating Colour, Geometry and Semantic Features for Large-Scale Indoor Place Recognition. (arXiv:2202.02070v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02070">
<div class="article-summary-box-inner">
<span><p>We describe an approach to large-scale indoor place recognition that
aggregates low-level colour and geometric features with high-level semantic
features. We use a deep learning network that takes in RGB point clouds and
extracts local features with five 3-D kernel point convolutional (KPConv)
layers. We specifically train the KPConv layers on the semantic segmentation
task to ensure that the extracted local features are semantically meaningful.
Then, feature maps from all the five KPConv layers are concatenated together
and fed into the NetVLAD layer to generate the global descriptors. The approach
is trained and evaluated using a large-scale indoor place recognition dataset
derived from the ScanNet dataset, with a test set comprising 3,608 point clouds
generated from 100 different rooms. Comparison with a traditional feature based
method and three state-of-the-art deep learning methods demonstrate that the
approach significantly outperforms all four methods, achieving, for example, a
top-3 average recall rate of 75% compared with 41% for the closest rival
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heed the Noise in Performance Evaluations in Neural Architecture Search. (arXiv:2202.02078v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02078">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) has recently become a topic of great
interest. However, there is a potentially impactful issue within NAS that
remains largely unrecognized: noise. Due to stochastic factors in neural
network initialization, training, and the chosen train/validation dataset
split, the performance evaluation of a neural network architecture, which is
often based on a single learning run, is also stochastic. This may have a
particularly large impact if a dataset is small. We therefore propose to reduce
the noise by having architecture evaluations comprise averaging of scores over
multiple network training runs using different random seeds and
cross-validation. We perform experiments for a combinatorial optimization
formulation of NAS in which we vary noise reduction levels. We use the same
computational budget for each noise level in terms of network training runs,
i.e., we allow less architecture evaluations when averaging over more training
runs. Multiple search algorithms are considered, including evolutionary
algorithms which generally perform well for NAS. We use two publicly available
datasets from the medical image segmentation domain where datasets are often
limited and variability among samples is often high. Our results show that
reducing noise in architecture evaluations enables finding better architectures
by all considered search algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge-Selective Feature Weaving for Point Cloud Matching. (arXiv:2202.02149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02149">
<div class="article-summary-box-inner">
<span><p>This paper tackles the problem of accurately matching the points of two 3D
point clouds. Most conventional methods improve their performance by extracting
representative features from each point via deep-learning-based algorithms. On
the other hand, the correspondence calculation between the extracted features
has not been examined in depth, and non-trainable algorithms (e.g. the Sinkhorn
algorithm) are frequently applied. As a result, the extracted features may be
forcibly fitted to a non-trainable algorithm. Furthermore, the extracted
features frequently contain stochastically unavoidable errors, which degrades
the matching accuracy. In this paper, instead of using a non-trainable
algorithm, we propose a differentiable matching network that can be jointly
optimized with the feature extraction procedure. Our network first constructs
graphs with edges connecting the points of each point cloud and then extracts
discriminative edge features by using two main components: a shared set-encoder
and an edge-selective cross-concatenation. These components enable us to
symmetrically consider two point clouds and to extract discriminative edge
features, respectively. By using the extracted discriminative edge features,
our network can accurately calculate the correspondence between points. Our
experimental results show that the proposed network can significantly improve
the performance of point cloud matching. Our code is available at
https://github.com/yanarin/ESFW
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeAT: Neural Adaptive Tomography. (arXiv:2202.02171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02171">
<div class="article-summary-box-inner">
<span><p>In this paper, we present Neural Adaptive Tomography (NeAT), the first
adaptive, hierarchical neural rendering pipeline for multi-view inverse
rendering. Through a combination of neural features with an adaptive explicit
representation, we achieve reconstruction times far superior to existing neural
inverse rendering methods. The adaptive explicit representation improves
efficiency by facilitating empty space culling and concentrating samples in
complex regions, while the neural features act as a neural regularizer for the
3D reconstruction. The NeAT framework is designed specifically for the
tomographic setting, which consists only of semi-transparent volumetric scenes
instead of opaque objects. In this setting, NeAT outperforms the quality of
existing optimization-based tomography solvers while being substantially
faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-Style Encoder for Style-Based GAN Inversion. (arXiv:2202.02183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02183">
<div class="article-summary-box-inner">
<span><p>We propose a novel architecture for GAN inversion, which we call
Feature-Style encoder. The style encoder is key for the manipulation of the
obtained latent codes, while the feature encoder is crucial for optimal image
reconstruction. Our model achieves accurate inversion of real images from the
latent space of a pre-trained style-based GAN model, obtaining better
perceptual quality and lower reconstruction error than existing methods. Thanks
to its encoder structure, the model allows fast and accurate image editing.
Additionally, we demonstrate that the proposed encoder is especially
well-suited for inversion and editing on videos. We conduct extensive
experiments for several style-based generators pre-trained on different data
domains. Our proposed method yields state-of-the-art results for style-based
GAN inversion, significantly outperforming competing approaches. Source codes
are available at https://github.com/InterDigitalInc/FeatureStyleEncoder .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Neighbor Consistency for Noisy Labels. (arXiv:2202.02200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02200">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have relied on large, labelled datasets to
train high-capacity models. However, collecting large datasets in a time- and
cost-efficient manner often results in label noise. We present a method for
learning from noisy labels that leverages similarities between training
examples in feature space, encouraging the prediction of each example to be
similar to its nearest neighbours. Compared to training algorithms that use
multiple models or distinct stages, our approach takes the form of a simple,
additional regularization term. It can be interpreted as an inductive version
of the classical, transductive label propagation algorithm. We thoroughly
evaluate our method on datasets evaluating both synthetic (CIFAR-10, CIFAR-100)
and realistic (mini-WebVision, Clothing1M, mini-ImageNet-Red) noise, and
achieve competitive or state-of-the-art accuracies across all of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Violence Recognition and Localization using a Semi-Supervised Hard-Attention Model. (arXiv:2202.02212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02212">
<div class="article-summary-box-inner">
<span><p>Empowering automated violence monitoring and surveillance systems amid the
growing social violence and extremist activities worldwide could keep
communities safe and save lives. The questionable reliability of human
monitoring personnel and the increasing number of surveillance cameras makes
automated artificial intelligence-based solutions compelling. Improving the
current state-of-the-art deep learning approaches to video violence recognition
to higher levels of accuracy and performance could enable surveillance systems
to be more reliable and scalable. The main contribution of the proposed deep
reinforcement learning method is to achieve state-of-the-art accuracy on RWF,
Hockey, and Movies datasets while removing some of the computationally
expensive processes and input features used in the previous solutions. The
implementation of hard attention using a semi-supervised learning method made
the proposed method capable of rough violence localization and added increased
agent interpretability to the violence detection system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapped Representation Learning for Skeleton-Based Action Recognition. (arXiv:2202.02232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02232">
<div class="article-summary-box-inner">
<span><p>In this work, we study self-supervised representation learning for 3D
skeleton-based action recognition. We extend Bootstrap Your Own Latent (BYOL)
for representation learning on skeleton sequence data and propose a new data
augmentation strategy including two asymmetric transformation pipelines. We
also introduce a multi-viewpoint sampling method that leverages multiple
viewing angles of the same action captured by different cameras. In the
semi-supervised setting, we show that the performance can be further improved
by knowledge distillation from wider networks, leveraging once more the
unlabeled samples. We conduct extensive experiments on the NTU-60 and NTU-120
datasets to demonstrate the performance of our proposed method. Our method
consistently outperforms the current state of the art on both linear evaluation
and semi-supervised benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized visual encoding model construction with small data. (arXiv:2202.02245v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02245">
<div class="article-summary-box-inner">
<span><p>Encoding models that predict brain response patterns to stimuli are one way
to capture this relationship between variability in bottom-up neural systems
and individual's behavior or pathological state. However, they generally need a
large amount of training data to achieve optimal accuracy. Here, we propose and
test an alternative personalized ensemble encoding model approach to utilize
existing encoding models, to create encoding models for novel individuals with
relatively little stimuli-response data. We show that these personalized
ensemble encoding models trained with small amounts of data for a specific
individual, i.e. ~400 image-response pairs, achieve accuracy not different from
models trained on ~24,000 image-response pairs for the same individual.
Importantly, the personalized ensemble encoding models preserve patterns of
inter-individual variability in the image-response relationship. Additionally,
we use our personalized ensemble encoding model within the recently developed
NeuroGen framework to generate optimal stimuli designed to maximize specific
regions' activations for a specific individual. We show that the
inter-individual differences in face area responses to images of dog vs human
faces observed previously is replicated using NeuroGen with the ensemble
encoding model. Finally, and most importantly, we show the proposed approach is
robust against domain shift by validating on a prospectively collected set of
image-response data in novel individuals with a different scanner and
experimental setup. Our approach shows the potential to use previously
collected, deeply sampled data to efficiently create accurate, personalized
encoding models and, subsequently, personalized optimal synthetic images for
new individuals scanned under different experimental conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Self Knowledge Distillation -- From Pothole Classification to Fine-Grained and COVID Recognition. (arXiv:2202.02265v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02265">
<div class="article-summary-box-inner">
<span><p>Pothole classification has become an important task for road inspection
vehicles to save drivers from potential car accidents and repair bills. Given
the limited computational power and fixed number of training epochs, we propose
iterative self knowledge distillation (ISKD) to train lightweight pothole
classifiers. Designed to improve both the teacher and student models over time
in knowledge distillation, ISKD outperforms the state-of-the-art self knowledge
distillation method on three pothole classification datasets across four
lightweight network architectures, which supports that self knowledge
distillation should be done iteratively instead of just once. The accuracy
relation between the teacher and student models shows that the student model
can still benefit from a moderately trained teacher model. Implying that better
teacher models generally produce better student models, our results justify the
design of ISKD. In addition to pothole classification, we also demonstrate the
efficacy of ISKD on six additional datasets associated with generic
classification, fine-grained classification, and medical imaging application,
which supports that ISKD can serve as a general-purpose performance booster
without the need of a given teacher model and extra trainable parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality Assessment of Low Light Restored Images: A Subjective Study and an Unsupervised Model. (arXiv:2202.02277v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02277">
<div class="article-summary-box-inner">
<span><p>The quality assessment (QA) of restored low light images is an important tool
for benchmarking and improving low light restoration (LLR) algorithms. While
several LLR algorithms exist, the subjective perception of the restored images
has been much less studied. Challenges in capturing aligned low light and
well-lit image pairs and collecting a large number of human opinion scores of
quality for training, warrant the design of unsupervised (or opinion unaware)
no-reference (NR) QA methods. This work studies the subjective perception of
low light restored images and their unsupervised NR QA. Our contributions are
two-fold. We first create a dataset of restored low light images using various
LLR methods, conduct a subjective QA study and benchmark the performance of
existing QA methods. We then present a self-supervised contrastive learning
technique to extract distortion aware features from the restored low light
images. We show that these features can be effectively used to build an opinion
unaware image quality analyzer. Detailed experiments reveal that our
unsupervised NR QA model achieves state-of-the-art performance among all such
quality measures for low light restored images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task head pose estimation in-the-wild. (arXiv:2202.02299v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02299">
<div class="article-summary-box-inner">
<span><p>We present a deep learning-based multi-task approach for head pose estimation
in images. We contribute with a network architecture and training strategy that
harness the strong dependencies among face pose, alignment and visibility, to
produce a top performing model for all three tasks. Our architecture is an
encoder-decoder CNN with residual blocks and lateral skip connections. We show
that the combination of head pose estimation and landmark-based face alignment
significantly improve the performance of the former task. Further, the location
of the pose task at the bottleneck layer, at the end of the encoder, and that
of tasks depending on spatial information, such as visibility and alignment, in
the final decoder layer, also contribute to increase the final performance. In
the experiments conducted the proposed model outperforms the state-of-the-art
in the face pose and visibility tasks. By including a final landmark regression
step it also produces face alignment results on par with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Mobile App Navigation with Uncertain or Under-specified Natural Language Commands. (arXiv:2202.02312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02312">
<div class="article-summary-box-inner">
<span><p>We introduce Mobile app Tasks with Iterative Feedback (MoTIF), a new dataset
where the goal is to complete a natural language query in a mobile app. Current
datasets for related tasks in interactive question answering, visual common
sense reasoning, and question-answer plausibility prediction do not support
research in resolving ambiguous natural language requests or operating in
diverse digital domains. As a result, they fail to capture complexities of real
question answering or interactive tasks. In contrast, MoTIF contains natural
language requests that are not satisfiable, the first such work to investigate
this issue for interactive vision-language tasks. MoTIF also contains follow up
questions for ambiguous queries to enable research on task uncertainty
resolution. We introduce task feasibility prediction and propose an initial
model which obtains an F1 score of 61.1. We next benchmark task automation with
our dataset and find adaptations of prior work perform poorly due to our
realistic language requests, obtaining an accuracy of only 20.2% when mapping
commands to grounded actions. We analyze performance and gain insight for
future work that may bridge the gap between current model ability and what is
needed for successful use in application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards To-a-T Spatio-Temporal Focus for Skeleton-Based Action Recognition. (arXiv:2202.02314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02314">
<div class="article-summary-box-inner">
<span><p>Graph Convolutional Networks (GCNs) have been widely used to model the
high-order dynamic dependencies for skeleton-based action recognition. Most
existing approaches do not explicitly embed the high-order spatio-temporal
importance to joints' spatial connection topology and intensity, and they do
not have direct objectives on their attention module to jointly learn when and
where to focus on in the action sequence. To address these problems, we propose
the To-a-T Spatio-Temporal Focus (STF), a skeleton-based action recognition
framework that utilizes the spatio-temporal gradient to focus on relevant
spatio-temporal features. We first propose the STF modules with learnable
gradient-enforced and instance-dependent adjacency matrices to model the
high-order spatio-temporal dynamics. Second, we propose three loss terms
defined on the gradient-based spatio-temporal focus to explicitly guide the
classifier when and where to look at, distinguish confusing classes, and
optimize the stacked STF modules. STF outperforms the state-of-the-art methods
on the NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400 datasets in all
15 settings over different views, subjects, setups, and input modalities, and
STF also shows better accuracy on scarce data and dataset shifting settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Webly Supervised Concept Expansion for General Purpose Vision Models. (arXiv:2202.02317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02317">
<div class="article-summary-box-inner">
<span><p>General purpose vision (GPV) systems are models that are designed to solve a
wide array of visual tasks without requiring architectural changes. Today, GPVs
primarily learn both skills and concepts from large fully supervised datasets.
Scaling GPVs to tens of thousands of concepts by acquiring data to learn each
concept for every skill quickly becomes prohibitive. This work presents an
effective and inexpensive alternative: learn skills from fully supervised
datasets, learn concepts from web image search results, and leverage a key
characteristic of GPVs -- the ability to transfer visual knowledge across
skills. We use a dataset of 1M+ images spanning 10k+ visual concepts to
demonstrate webly-supervised concept expansion for two existing GPVs (GPV-1 and
VL-T5) on 3 benchmarks - 5 COCO based datasets (80 primary concepts), a newly
curated series of 5 datasets based on the OpenImages and VisualGenome
repositories (~500 concepts) and the Web-derived dataset (10k+ concepts). We
also propose a new architecture, GPV-2 that supports a variety of tasks -- from
vision tasks like classification and localization to vision+language tasks like
QA and captioning to more niche ones like human-object interaction recognition.
GPV-2 benefits hugely from web data, outperforms GPV-1 and VL-T5 across these
benchmarks, and does well in a 0-shot setting at action and attribute
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ocular Recognition Databases and Competitions: A Survey. (arXiv:1911.09646v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.09646">
<div class="article-summary-box-inner">
<span><p>The use of the iris and periocular region as biometric traits has been
extensively investigated, mainly due to the singularity of the iris features
and the use of the periocular region when the image resolution is not
sufficient to extract iris information. In addition to providing information
about an individual's identity, features extracted from these traits can also
be explored to obtain other information such as the individual's gender, the
influence of drug use, the use of contact lenses, spoofing, among others. This
work presents a survey of the databases created for ocular recognition,
detailing their protocols and how their images were acquired. We also describe
and discuss the most popular ocular recognition competitions (contests),
highlighting the submitted algorithms that achieved the best results using only
iris trait and also fusing iris and periocular region information. Finally, we
describe some relevant works applying deep learning techniques to ocular
recognition and point out new challenges and future directions. Considering
that there are a large number of ocular databases, and each one is usually
designed for a specific problem, we believe this survey can provide a broad
overview of the challenges in ocular biometrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Image-to-Image Translation via a Single Generative Adversarial Network. (arXiv:2008.01681v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01681">
<div class="article-summary-box-inner">
<span><p>Despite significant advances in image-to-image (I2I) translation with
generative adversarial networks (GANs), it remains challenging to effectively
translate an image to a set of diverse images in multiple target domains using
a pair of generators and discriminators. Existing multimodal I2I translation
methods adopt multiple domain-specific content encoders for different domains,
where each domain-specific content encoder is trained with images from the same
domain only. Nevertheless, we argue that the content (domain-invariance)
features should be learned from images among all of the domains. Consequently,
each domain-specific content encoder of existing schemes fails to extract the
domain-invariant features efficiently. To address this issue, we present a
flexible and general SoloGAN model for efficient multimodal I2I translation
among multiple domains with unpaired data. In contrast to existing methods, the
SoloGAN algorithm uses a single projection discriminator with an additional
auxiliary classifier and shares the encoder and generator for all domains. As
such, the SoloGAN model can be trained effectively with images from all domains
so that the domain-invariance content representation can be efficiently
extracted. Qualitative and quantitative results over a wide range of datasets
against several counterparts and variants of the SoloGAN model demonstrate the
merits of the method, especially for challenging I2I translation tasks, i.e.
tasks that involve extreme shape variations or need to keep the complex
backgrounds unchanged after translations. Furthermore, we demonstrate the
contribution of each component using ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From noisy point clouds to complete ear shapes: unsupervised pipeline. (arXiv:2008.09831v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09831">
<div class="article-summary-box-inner">
<span><p>Ears are a particularly difficult region of the human face to model, not only
due to the non-rigid deformations existing between shapes but also to the
challenges in processing the retrieved data. The first step towards obtaining a
good model is to have complete scans in correspondence, but these usually
present a higher amount of occlusions, noise and outliers when compared to most
face regions, thus requiring a specific procedure. Therefore, we propose a
complete pipeline taking as input unordered 3D point clouds with the
aforementioned problems, and producing as output a dataset in correspondence,
with completion of the missing data. We provide a comparison of several
state-of-the-art registration methods and propose a new approach for one of the
steps of the pipeline, with better performance for our data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.15081">
<div class="article-summary-box-inner">
<span><p>We propose Deep Estimators of Features (DEFs), a learning-based framework for
predicting sharp geometric features in sampled 3D shapes. Differently from
existing data-driven methods, which reduce this problem to feature
classification, we propose to regress a scalar field representing the distance
from point samples to the closest feature line on local patches. Our approach
is the first that scales to massive point clouds by fusing distance-to-feature
estimates obtained on individual patches. We extensively evaluate our approach
against related state-of-the-art methods on newly proposed synthetic and
real-world 3D CAD model benchmarks. Our approach not only outperforms these
(with improvements in Recall and False Positives Rates), but generalizes to
real-world scans after training our model on synthetic data and fine-tuning it
on a small dataset of scanned data. We demonstrate a downstream application,
where we reconstruct an explicit representation of straight and curved sharp
feature lines from range scan data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. (arXiv:2103.04922v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04922">
<div class="article-summary-box-inner">
<span><p>Deep generative models are a class of techniques that train deep neural
networks to model the distribution of training samples. Research has fragmented
into various interconnected approaches, each of which make trade-offs including
run-time, diversity, and architectural restrictions. In particular, this
compendium covers energy-based models, variational autoencoders, generative
adversarial networks, autoregressive models, normalizing flows, in addition to
numerous hybrid approaches. These techniques are compared and contrasted,
explaining the premises behind each and how they are interrelated, while
reviewing current state-of-the-art advances and implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lottery Jackpots Exist in Pre-trained Models. (arXiv:2104.08700v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08700">
<div class="article-summary-box-inner">
<span><p>Network pruning is an effective approach to reduce network complexity with
acceptable performance compromise. Existing studies achieve the sparsity of
neural networks via time-consuming weight tuning or complex search on networks
with expanded width, which greatly limits the applications of network pruning.
In this paper, we show that high-performing and sparse sub-networks without the
involvement of weight tuning, termed "lottery jackpots", exist in pre-trained
models with unexpanded width. For example, we obtain a lottery jackpot that has
only 10% parameters and still reaches the performance of the original dense
VGGNet-19 without any modifications on the pre-trained weights on CIFAR-10.
Furthermore, we observe that the sparse masks derived from many existing
pruning criteria have a high overlap with the searched mask of our lottery
jackpot, among which, the magnitude-based pruning results in the most similar
mask with ours. Based on this insight, we initialize our sparse mask using the
magnitude-based pruning, resulting in at least 3x cost reduction on the lottery
jackpot search while achieving comparable or even better performance.
Specifically, our magnitude-based lottery jackpot removes 90% weights in
ResNet-50, while it easily obtains more than 70% top-1 accuracy using only 10
searching epochs on ImageNet. Our project is available at
https://github.com/lottery-jackpot/lottery-jackpot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Model Checking of Continuous Space. (arXiv:2105.06194v2 [cs.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06194">
<div class="article-summary-box-inner">
<span><p>Topological Spatial Model Checking is a recent paradigm where model checking
techniques are developed for the topological interpretation of Modal Logic. The
Spatial Logic of Closure Spaces, SLCS, extends Modal Logic with reachability
connectives that, in turn, can be used for expressing interesting spatial
properties, such as "being near to" or "being surrounded by". SLCS constitutes
the kernel of a solid logical framework for reasoning about discrete space,
such as graphs and digital images, interpreted as quasi discrete closure
spaces. Following a recently developed geometric semantics of Modal Logic, we
propose an interpretation of SLCS in continuous space, admitting a geometric
spatial model checking procedure, by resorting to models based on polyhedra.
Such representations of space are increasingly relevant in many domains of
application, due to recent developments of 3D scanning and visualisation
techniques that exploit mesh processing. We introduce PolyLogicA, a geometric
spatial model checker for SLCS formulas on polyhedra and demonstrate
feasibility of our approach on two 3D polyhedral models of realistic size.
Finally, we introduce a geometric definition of bisimilarity, proving that it
characterises logical equivalence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Q-attention: Enabling Efficient Learning for Vision-based Robotic Manipulation. (arXiv:2105.14829v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14829">
<div class="article-summary-box-inner">
<span><p>Despite the success of reinforcement learning methods, they have yet to have
their breakthrough moment when applied to a broad range of robotic manipulation
tasks. This is partly due to the fact that reinforcement learning algorithms
are notoriously difficult and time consuming to train, which is exacerbated
when training from images rather than full-state inputs. As humans perform
manipulation tasks, our eyes closely monitor every step of the process with our
gaze focusing sequentially on the objects being manipulated. With this in mind,
we present our Attention-driven Robotic Manipulation (ARM) algorithm, which is
a general manipulation algorithm that can be applied to a range of
sparse-rewarded tasks, given only a small number of demonstrations. ARM splits
the complex task of manipulation into a 3 stage pipeline: (1) a Q-attention
agent extracts relevant pixel locations from RGB and point cloud inputs, (2) a
next-best pose agent that accepts crops from the Q-attention agent and outputs
poses, and (3) a control agent that takes the goal pose and outputs joint
actions. We show that current learning algorithms fail on a range of RLBench
tasks, whilst ARM is successful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervision & Meta-Learning for One-Shot Unsupervised Cross-Domain Detection. (arXiv:2106.03496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03496">
<div class="article-summary-box-inner">
<span><p>Deep detection approaches are powerful in controlled conditions, but appear
brittle and fail when source models are used off-the-shelf on unseen domains.
Most of the existing works on domain adaptation simplify the setting and access
jointly both a large source dataset and a sizable amount of target samples.
However this scenario is unrealistic in many practical cases as when monitoring
image feeds from social media: only a pretrained source model is available and
every target image uploaded by the users belongs to a different domain not
foreseen during training. We address this challenging setting by presenting an
object detection algorithm able to exploit a pre-trained source model and
perform unsupervised adaptation by using only one target sample seen at test
time. Our multi-task architecture includes a self-supervised branch that we
exploit to meta-train the whole model with single-sample cross-domain episodes,
and prepare to the test condition. At deployment time the self-supervised task
is iteratively solved on any incoming sample to one-shot adapt on it. We
introduce a new dataset of social media image feeds and present a thorough
benchmark with the most recent cross-domain detection methods showing the
advantages of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06959">
<div class="article-summary-box-inner">
<span><p>The discovery of the disentanglement properties of the latent space in GANs
motivated a lot of research to find the semantically meaningful directions on
it. In this paper, we suggest that the disentanglement property is closely
related to the geometry of the latent space. In this regard, we propose an
unsupervised method for finding the semantic-factorizing directions on the
intermediate latent space of GANs based on the local geometry. Intuitively, our
proposed method, called Local Basis, finds the principal variation of the
latent space in the neighborhood of the base latent variable. Experimental
results show that the local principal variation corresponds to the semantic
factorization and traversing along it provides strong robustness to image
traversal. Moreover, we suggest an explanation for the limited success in
finding the global traversal directions in the latent space, especially W-space
of StyleGAN2. We show that W-space is warped globally by comparing the local
geometry, discovered from Local Basis, through the metric on Grassmannian
Manifold. The global warpage implies that the latent space is not well-aligned
globally and therefore the global traversal directions are bound to show
limited success on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11810">
<div class="article-summary-box-inner">
<span><p>In this work, we propose the world's first closed-loop ML-based planning
benchmark for autonomous driving. While there is a growing body of ML-based
motion planners, the lack of established datasets and metrics has limited the
progress in this area. Existing benchmarks for autonomous vehicle motion
prediction have focused on short-term motion forecasting, rather than long-term
planning. This has led previous works to use open-loop evaluation with L2-based
metrics, which are not suitable for fairly evaluating long-term planning. Our
benchmark overcomes these limitations by introducing a large-scale driving
dataset, lightweight closed-loop simulator, and motion-planning-specific
metrics. We provide a high-quality dataset with 1500h of human driving data
from 4 cities across the US and Asia with widely varying traffic patterns
(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop
simulation framework with reactive agents and provide a large set of both
general and scenario-specific planning metrics. We plan to release the dataset
at NeurIPS 2021 and organize benchmark challenges starting in early 2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Striking the Right Balance: Recall Loss for Semantic Segmentation. (arXiv:2106.14917v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14917">
<div class="article-summary-box-inner">
<span><p>Class imbalance is a fundamental problem in computer vision applications such
as semantic segmentation. Specifically, uneven class distributions in a
training dataset often result in unsatisfactory performance on
under-represented classes. Many works have proposed to weight the standard
cross entropy loss function with pre-computed weights based on class
statistics, such as the number of samples and class margins. There are two
major drawbacks to these methods: 1) constantly up-weighting minority classes
can introduce excessive false positives in semantic segmentation; 2) a minority
class is not necessarily a hard class. The consequence is low precision due to
excessive false positives. In this regard, we propose a hard-class mining loss
by reshaping the vanilla cross entropy loss such that it weights the loss for
each class dynamically based on instantaneous recall performance. We show that
the novel recall loss changes gradually between the standard cross entropy loss
and the inverse frequency weighted loss. Recall loss also leads to improved
mean accuracy while offering competitive mean Intersection over Union (IoU)
performance. On Synthia dataset, recall loss achieves $9\%$ relative
improvement on mean accuracy with competitive mean IoU using DeepLab-ResNet18
compared to the cross entropy loss. Code available at
https://github.com/PotatoTian/recall-semseg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01775">
<div class="article-summary-box-inner">
<span><p>This paper presents solo-learn, a library of self-supervised methods for
visual representation learning. Implemented in Python, using Pytorch and
Pytorch lightning, the library fits both research and industry needs by
featuring distributed training pipelines with mixed-precision, faster data
loading via Nvidia DALI, online linear evaluation for better prototyping, and
many additional training tricks. Our goal is to provide an easy-to-use library
comprising a large amount of Self-supervised Learning (SSL) methods, that can
be easily extended and fine-tuned by the community. solo-learn opens up avenues
for exploiting large-budget SSL solutions on inexpensive smaller
infrastructures and seeks to democratize SSL by making it accessible to all.
The source code is available at https://github.com/vturrisi/solo-learn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIGRoC: Boosting Image Generation via a Robust Classifier. (arXiv:2108.03702v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03702">
<div class="article-summary-box-inner">
<span><p>The interest of the machine learning community in image synthesis has grown
significantly in recent years, with the introduction of a wide range of deep
generative models and means for training them. In this work, we propose a
general model-agnostic technique for improving the image quality and the
distribution fidelity of generated images, obtained by any generative model.
Our method, termed BIGRoC (Boosting Image Generation via a Robust Classifier),
is based on a post-processing procedure via the guidance of a given robust
classifier and without a need for additional training of the generative model.
Given a synthesized image, we propose to update it through projected gradient
steps over the robust classifier, in an attempt to refine its recognition. We
demonstrate this post-processing algorithm on various image synthesis methods
and show a significant improvement of the generated images, both quantitatively
and qualitatively, on CIFAR-10 and ImageNet. Specifically, BIGRoC improves the
image synthesis state of the art on ImageNet 128x128 by 14.81%, attaining an
FID score of 2.53 and on 256x256 by 7.87%, achieving an FID of 3.63.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03201">
<div class="article-summary-box-inner">
<span><p>Transformer, the model of choice for natural language processing, has drawn
scant attention from the medical imaging community. Given the ability to
exploit long-term dependencies, transformers are promising to help atypical
convolutional neural networks to overcome their inherent shortcomings of
spatial inductive bias. However, most of recently proposed transformer-based
segmentation approaches simply treated transformers as assisted modules to help
encode global context into convolutional representations. To address this
issue, we introduce nnFormer, a 3D transformer for volumetric medical image
segmentation. nnFormer not only exploits the combination of interleaved
convolution and self-attention operations, but also introduces local and global
volume-based self-attention mechanism to learn volume representations.
Moreover, nnFormer proposes to use skip attention to replace the traditional
concatenation/summation operations in skip connections in U-Net like
architecture. Experiments show that nnFormer significantly outperforms previous
transformer-based counterparts by large margins on three public datasets.
Compared to nnUNet, nnFormer produces significantly lower HD95 and comparable
DSC results. Furthermore, we show that nnFormer and nnUNet are highly
complementary to each other in model ensembling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus on Impact: Indoor Exploration with Intrinsic Motivation. (arXiv:2109.08521v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08521">
<div class="article-summary-box-inner">
<span><p>Exploration of indoor environments has recently experienced a significant
interest, also thanks to the introduction of deep neural agents built in a
hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on
simulated environments. Current state-of-the-art methods employ a dense
extrinsic reward that requires the complete a priori knowledge of the layout of
the training environment to learn an effective exploration policy. However,
such information is expensive to gather in terms of time and resources. In this
work, we propose to train the model with a purely intrinsic reward signal to
guide exploration, which is based on the impact of the robot's actions on its
internal representation of the environment. So far, impact-based rewards have
been employed for simple tasks and in procedurally generated synthetic
environments with countable states. Since the number of states observable by
the agent in realistic indoor environments is non-countable, we include a
neural-based density model and replace the traditional count-based
regularization with an estimated pseudo-count of previously visited states. The
proposed exploration approach outperforms DRL-based competitors relying on
intrinsic rewards and surpasses the agents trained with a dense extrinsic
reward computed with the environment layouts. We also show that a robot
equipped with the proposed approach seamlessly adapts to point-goal navigation
and real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scale Convolutional Neural Network for Automated AMD Classification using Retinal OCT Images. (arXiv:2110.03002v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03002">
<div class="article-summary-box-inner">
<span><p>Age-related macular degeneration (AMD) is the most common cause of blindness
in developed countries, especially in people over 60 years of age. The workload
of specialists and the healthcare system in this field has increased in recent
years mainly due to the prevalence of population aging worldwide and the
chronic nature of AMD. Recent developments in deep learning have provided a
unique opportunity to develop fully automated diagnosis frameworks. Considering
the presence of AMD-related retinal pathologies in varying sizes in OCT images,
our objective was to propose a multi-scale convolutional neural network (CNN)
capable of distinguishing pathologies using receptive fields with various
sizes. The multi-scale CNN was designed based on the feature pyramid network
(FPN) structure and was used to diagnose normal and two common clinical
characteristics of dry and wet AMD, namely drusen and choroidal
neovascularization (CNV). The proposed method was evaluated on a national
dataset gathered at Noor Eye Hospital (NEH) and the UCSD public dataset.
Experimental results show the superior performance of our proposed multi-scale
structure over several well-known OCT classification frameworks. This feature
combination strategy has proved to be effective on all tested backbone models,
with improvements ranging from 0.4% to 3.3%. In addition, gradual learning has
proven to improve performance in two consecutive stages. In the first stage,
the performance was boosted from 87.2%+-2.5% to 92.0%+-1.6% using pre-trained
ImageNet weights. In the second stage, another performance boost from
92.0%+-1.6% to 93.4%+-1.4% was observed due to fine-tuning the previous model
on the UCSD dataset. Lastly, generating heatmaps provided additional proof for
the effectiveness of our multi-scale structure, enabling the detection of
retinal pathologies appearing in different sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Gradient Non-sign Methods. (arXiv:2110.12734v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12734">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks make their success in DNNs, and among them,
gradient-based algorithms become one of the mainstreams. Based on the linearity
hypothesis, under $\ell_\infty$ constraint, $sign$ operation applied to the
gradients is a good choice for generating perturbations. However, side-effects
from such operation exist since it leads to the bias of direction between real
gradients and perturbations. In other words, current methods contain a gap
between real gradients and actual noises, which leads to biased and inefficient
attacks. Therefore in this paper, based on the Taylor expansion, the bias is
analyzed theoretically, and the correction of $sign$, i.e., Fast Gradient
Non-sign Method (FGNM), is further proposed. Notably, FGNM is a general routine
that seamlessly replaces the conventional $sign$ operation in gradient-based
attacks with negligible extra computational cost. Extensive experiments
demonstrate the effectiveness of our methods. Specifically, for untargeted
black-box attacks, ours outperform them by 27.5% at most and 9.5% on average.
For targeted attacks against defense models, it is 15.1% and 12.7%. Our
anonymous code is publicly available at https://github.com/yaya-cheng/FGNM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Non-Compression Auto-Encoder for Driving Noise-based Road Surface Anomaly Detection. (arXiv:2111.10985v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10985">
<div class="article-summary-box-inner">
<span><p>Wet weather makes water film over the road and that film causes lower
friction between tire and road surface. When a vehicle passes the low-friction
road, the accident can occur up to 35% higher frequency than a normal condition
road. In order to prevent accidents as above, identifying the road condition in
real-time is essential. Thus, we propose a convolutional auto-encoder-based
anomaly detection model for taking both less computational resources and
achieving higher anomaly detection performance. The proposed model adopts a
non-compression method rather than a conventional bottleneck structured
auto-encoder. As a result, the computational cost of the neural network is
reduced up to 1 over 25 compared to the conventional models and the anomaly
detection performance is improved by up to 7.72%. Thus, we conclude the
proposed model as a cutting-edge algorithm for real-time anomaly detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdiBERT, a generative model for image editing. (arXiv:2111.15264v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15264">
<div class="article-summary-box-inner">
<span><p>Advances in computer vision are pushing the limits of im-age manipulation,
with generative models sampling detailed images on various tasks. However, a
specialized model is often developed and trained for each specific task, even
though many image edition tasks share similarities. In denoising, inpainting,
or image compositing, one always aims at generating a realistic image from a
low-quality one. In this paper, we aim at making a step towards a unified
approach for image editing. To do so, we propose EdiBERT, a bi-directional
transformer trained in the discrete latent space built by a vector-quantized
auto-encoder. We argue that such a bidirectional model is suited for image
manipulation since any patch can be re-sampled conditionally to the whole
image. Using this unique and straightforward training objective, we show that
the resulting model matches state-of-the-art performances on a wide variety of
tasks: image denoising, image completion, and image composition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. (arXiv:2112.05139v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05139">
<div class="article-summary-box-inner">
<span><p>We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural
radiance fields (NeRF). By leveraging the joint language-image embedding space
of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose
a unified framework that allows manipulating NeRF in a user-friendly way, using
either a short text prompt or an exemplar image. Specifically, to combine the
novel view synthesis capability of NeRF and the controllable manipulation
ability of latent representations from generative models, we introduce a
disentangled conditional NeRF architecture that allows individual control over
both shape and appearance. This is achieved by performing the shape
conditioning via applying a learned deformation field to the positional
encoding and deferring color conditioning to the volumetric rendering stage. To
bridge this disentangled latent representation to the CLIP embedding, we design
two code mappers that take a CLIP embedding as input and update the latent
codes to reflect the targeted editing. The mappers are trained with a
CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we
propose an inverse optimization method that accurately projects an input image
to the latent codes for manipulation to enable editing on real images. We
evaluate our approach by extensive experiments on a variety of text prompts and
exemplar images and also provide an intuitive interface for interactive
editing. Our implementation is available at
https://cassiepython.github.io/clipnerf/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Inception Attention for Image Synthesis and Image Recognition. (arXiv:2112.14804v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14804">
<div class="article-summary-box-inner">
<span><p>Image synthesis and image recognition have witnessed remarkable progress, but
often at the expense of computationally expensive training and inference.
Learning lightweight yet expressive deep model has emerged as an important and
interesting direction. Inspired by the well-known split-transform-aggregate
design heuristic in the Inception building block, this paper proposes a
Skip-Layer Inception Module (SLIM) that facilitates efficient learning of image
synthesis models, and a same-layer variant (dubbed as SLIM too) as a stronger
alternative to the well-known ResNeXts for image recognition. In SLIM, the
input feature map is first split into a number of groups (e.g., 4).Each group
is then transformed to a latent style vector(via channel-wise attention) and a
latent spatial mask (via spatial attention). The learned latent masks and
latent style vectors are aggregated to modulate the target feature map. For
generative learning, SLIM is built on a recently proposed lightweight
Generative Adversarial Networks (i.e., FastGANs) which present a skip-layer
excitation(SLE) module. For few-shot image synthesis tasks, the proposed SLIM
achieves better performance than the SLE work and other related methods. For
one-shot image synthesis tasks, it shows stronger capability of preserving
images structures than prior arts such as the SinGANs. For image classification
tasks, the proposed SLIM is used as a drop-in replacement for convolution
layers in ResNets (resulting in ResNeXt-like models) and achieves better
accuracy in theImageNet-1000 dataset, with significantly smaller model
complexity
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Semantic Ambiguities for Zero-Shot Learning. (arXiv:2201.01823v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01823">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims at recognizing classes for which no visual
sample is available at training time. To address this issue, one can rely on a
semantic description of each class. A typical ZSL model learns a mapping
between the visual samples of seen classes and the corresponding semantic
descriptions, in order to do the same on unseen classes at test time. State of
the art approaches rely on generative models that synthesize visual features
from the prototype of a class, such that a classifier can then be learned in a
supervised manner. However, these approaches are usually biased towards seen
classes whose visual instances are the only one that can be matched to a given
class prototype. We propose a regularization method that can be applied to any
conditional generative-based ZSL method, by leveraging only the semantic class
prototypes. It learns to synthesize discriminative features for possible
semantic description that are not available at training time, that is the
unseen ones. The approach is evaluated for ZSL and GZSL on four datasets
commonly used in the literature, either in inductive and transductive settings,
with results on-par or above state of the art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Leaning-Based Ultra-Fast Stair Detection. (arXiv:2201.05275v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05275">
<div class="article-summary-box-inner">
<span><p>Staircases are some of the most common building structures in urban
environments. Stair detection is an important task for various applications,
including the environmental perception of exoskeleton robots, humanoid robots,
and rescue robots and the navigation of visually impaired people. Most existing
stair detection algorithms have difficulty dealing with the diversity of stair
structure materials, extreme light and serious occlusion. Inspired by human
perception, we propose an end-to-end method based on deep learning.
Specifically, we treat the process of stair line detection as a multitask
involving coarse-grained semantic segmentation and object detection. The input
images are divided into cells, and a simple neural network is used to judge
whether each cell contains stair lines. For cells containing stair lines, the
locations of the stair lines relative to each cell are regressed. Extensive
experiments on our dataset show that our method can achieve high performance in
terms of both speed and accuracy. A lightweight version can even achieve 300+
frames per second with the same resolution. Our code and dataset will be soon
available at GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-consistent Generative Adversarial Networks for multi-modal Medical segmentation maps. (arXiv:2201.09693v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09693">
<div class="article-summary-box-inner">
<span><p>Image translation across domains for unpaired datasets has gained interest
and great improvement lately. In medical imaging, there are multiple imaging
modalities, with very different characteristics. Our goal is to use
cross-modality adaptation between CT and MRI whole cardiac scans for semantic
segmentation. We present a segmentation network using synthesised cardiac
volumes for extremely limited datasets. Our solution is based on a 3D
cross-modality generative adversarial network to share information between
modalities and generate synthesized data using unpaired datasets. Our network
utilizes semantic segmentation to improve generator shape consistency, thus
creating more realistic synthesised volumes to be used when re-training the
segmentation network. We show that improved segmentation can be achieved on
small datasets when using spatial augmentations to improve a generative
adversarial network. These augmentations improve the generator capabilities,
thus enhancing the performance of the Segmentor. Using only 16 CT and 16 MRI
cardiovascular volumes, improved results are shown over other segmentation
methods while using the suggested architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning. (arXiv:2201.09765v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09765">
<div class="article-summary-box-inner">
<span><p>Standard model-free reinforcement learning algorithms optimize a policy that
generates the action to be taken in the current time step in order to maximize
expected future return. While flexible, it faces difficulties arising from the
inefficient exploration due to its single step nature. In this work, we present
Generative Planning method (GPM), which can generate actions not only for the
current step, but also for a number of future steps (thus termed as generative
planning). This brings several benefits to GPM. Firstly, since GPM is trained
by maximizing value, the plans generated from it can be regarded as intentional
action sequences for reaching high value regions. GPM can therefore leverage
its generated multi-step plans for temporally coordinated exploration towards
high value regions, which is potentially more effective than a sequence of
actions generated by perturbing each action at single step level, whose
consistent movement decays exponentially with the number of exploration steps.
Secondly, starting from a crude initial plan generator, GPM can refine it to be
adaptive to the task, which, in return, benefits future explorations. This is
potentially more effective than commonly used action-repeat strategy, which is
non-adaptive in its form of plans. Additionally, since the multi-step plan can
be interpreted as the intent of the agent from now to a span of time period
into the future, it offers a more informative and intuitive signal for
interpretation. Experiments are conducted on several benchmark environments and
the results demonstrated its effectiveness compared with several baseline
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network. (arXiv:2201.10147v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10147">
<div class="article-summary-box-inner">
<span><p>The end-to-end image fusion framework has achieved promising performance,
with dedicated convolutional networks aggregating the multi-modal local
appearance. However, long-range dependencies are directly neglected in existing
CNN fusion approaches, impeding balancing the entire image-level perception for
complex scenario fusion. In this paper, therefore, we propose an infrared and
visible image fusion algorithm based on a lightweight transformer module and
adversarial learning. Inspired by the global interaction power, we use the
transformer technique to learn the effective global fusion relations. In
particular, shallow features extracted by CNN are interacted in the proposed
transformer fusion module to refine the fusion relationship within the spatial
scope and across channels simultaneously. Besides, adversarial learning is
designed in the training process to improve the output discrimination via
imposing competitive consistency from the inputs, reflecting the specific
characteristics in infrared and visible images. The experimental performance
demonstrates the effectiveness of the proposed modules, with superior
improvement against the state-of-the-art, generalising a novel paradigm via
transformer and adversarial learning in the fusion task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks. (arXiv:2202.00838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00838">
<div class="article-summary-box-inner">
<span><p>Recent work suggests that representations learned by adversarially robust
networks are more human perceptually-aligned than non-robust networks via image
manipulations. Despite appearing closer to human visual perception, it is
unclear if the constraints in robust DNN representations match biological
constraints found in human vision. Human vision seems to rely on
texture-based/summary statistic representations in the periphery, which have
been shown to explain phenomena such as crowding and performance on visual
search tasks. To understand how adversarially robust
optimizations/representations compare to human vision, we performed a
psychophysics experiment using a set of metameric discrimination tasks where we
evaluated how well human observers could distinguish between images synthesized
to match adversarially robust representations compared to non-robust
representations and a texture synthesis model of peripheral vision (Texforms).
We found that the discriminability of robust representation and texture model
images decreased to near chance performance as stimuli were presented farther
in the periphery. Moreover, performance on robust and texture-model images
showed similar trends within participants, while performance on non-robust
representations changed minimally across the visual field. These results
together suggest that (1) adversarially robust representations capture
peripheral computation better than non-robust representations and (2) robust
representations capture peripheral computation similar to current
state-of-the-art texture peripheral vision models. More broadly, our findings
support the idea that localized texture summary statistic representations may
drive human invariance to adversarial perturbations and that the incorporation
of such representations in DNNs could give rise to useful properties like
adversarial robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extension -- Adaptive Sampling with Implicit Radiance Field. (arXiv:2202.00855v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00855">
<div class="article-summary-box-inner">
<span><p>This paper aims to explore and summarize the state-of-the-art progress in
Monte Carlo adaptive light field sampling and reconstruction using deep
reinforcement learning, with possible extension to it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Transfer: Learning to Route Transferrable Representations. (arXiv:2202.01011v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01011">
<div class="article-summary-box-inner">
<span><p>Knowledge transfer between heterogeneous source and target networks and tasks
has received a lot of attention in recent times as large amounts of quality
labelled data can be difficult to obtain in many applications. Existing
approaches typically constrain the target deep neural network (DNN) feature
representations to be close to the source DNNs feature representations, which
can be limiting. We, in this paper, propose a novel adversarial multi-armed
bandit approach which automatically learns to route source representations to
appropriate target representations following which they are combined in
meaningful ways to produce accurate target models. We see upwards of 5%
accuracy improvements compared with the state-of-the-art knowledge transfer
methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67,
and Stanford40 where the source dataset is ImageNet. We qualitatively analyze
the goodness of our transfer scheme by showing individual examples of the
important features our target network focuses on in different layers compared
with the (closest) competitors. We also observe that our improvement over other
methods is higher for smaller target datasets making it an effective tool for
small data applications that may benefit from transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VOS: Learning What You Don't Know by Virtual Outlier Synthesis. (arXiv:2202.01197v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01197">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection has received much attention lately due to
its importance in the safe deployment of neural networks. One of the key
challenges is that models lack supervision signals from unknown data, and as a
result, can produce overconfident predictions on OOD data. Previous approaches
rely on real outlier datasets for model regularization, which can be costly and
sometimes infeasible to obtain in practice. In this paper, we present VOS, a
novel framework for OOD detection by adaptively synthesizing virtual outliers
that can meaningfully regularize the model's decision boundary during training.
Specifically, VOS samples virtual outliers from the low-likelihood region of
the class-conditional distribution estimated in the feature space. Alongside,
we introduce a novel unknown-aware training objective, which contrastively
shapes the uncertainty space between the ID data and synthesized outlier data.
VOS achieves state-of-the-art performance on both object detection and image
classification models, reducing the FPR95 by up to 7.87% compared to the
previous best method. Code is available at
https://github.com/deeplearning-wisc/vos.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-07 23:08:37.804798971 UTC">2022-02-07 23:08:37 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>