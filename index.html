<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-04T01:30:00Z">03-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture. (arXiv:2203.01322v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01445">
<div class="article-summary-box-inner">
<span><p>The volume of available data has grown dramatically in recent years in many
applications. Furthermore, the age of networks that used multiple modalities
separately has practically ended. Therefore, enabling bidirectional
cross-modality data retrieval capable of processing has become a requirement
for many domains and disciplines of research. This is especially true in the
medical field, as data comes in a multitude of types, including various types
of images and reports as well as molecular data. Most contemporary works apply
cross attention to highlight the essential elements of an image or text in
relation to the other modalities and try to match them together. However,
regardless of their importance in their own modality, these approaches usually
consider features of each modality equally. In this study, self-attention as an
additional loss term will be proposed to enrich the internal representation
provided into the cross attention module. This work suggests a novel
architecture with a new loss term to help represent images and texts in the
joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO
and ARCH, show the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding. (arXiv:2203.01515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01515">
<div class="article-summary-box-inner">
<span><p>Automatic ICD coding is defined as assigning disease codes to electronic
medical records (EMRs). Existing methods usually apply label attention with
code representations to match related text snippets. Unlike these works that
model the label with the code hierarchy or description, we argue that the code
synonyms can provide more comprehensive knowledge based on the observation that
the code expressions in EMRs vary from their descriptions in ICD. By aligning
codes to concepts in UMLS, we collect synonyms of every code. Then, we propose
a multiple synonyms matching network to leverage synonyms for better code
representation learning, and finally help the code classification. Experiments
on the MIMIC-III dataset show that our proposed method outperforms previous
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QaNER: Prompting Question Answering Models for Few-shot Named Entity Recognition. (arXiv:2203.01543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01543">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-based learning for pre-trained language models has succeeded
in few-shot Named Entity Recognition (NER) by exploiting prompts as task
guidance to increase label efficiency. However, previous prompt-based methods
for few-shot NER have limitations such as a higher computational complexity,
poor zero-shot ability, requiring manual prompt engineering, or lack of prompt
robustness. In this work, we address these shortcomings by proposing a new
prompt-based learning NER method with Question Answering (QA), called QaNER.
Our approach includes 1) a refined strategy for converting NER problems into
the QA formulation; 2) NER prompt generation for QA models; 3) prompt-based
tuning with QA models on a few annotated NER examples; 4) zero-shot NER by
prompting the QA model. Comparing the proposed approach with previous methods,
QaNER is faster at inference, insensitive to the prompt quality, and robust to
hyper-parameters, as well as demonstrating significantly better low-resource
performance and zero-shot capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking. (arXiv:2203.01552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01552">
<div class="article-summary-box-inner">
<span><p>Annotating task-oriented dialogues is notorious for the expensive and
difficult data collection process. Few-shot dialogue state tracking (DST) is a
realistic solution to this problem. In this paper, we hypothesize that dialogue
summaries are essentially unstructured dialogue states; hence, we propose to
reformulate dialogue state tracking as a dialogue summarization problem. To
elaborate, we train a text-to-text language model with synthetic template-based
dialogue summaries, generated by a set of rules from the dialogue states. Then,
the dialogue states can be recovered by inversely applying the summary
generation rules. We empirically show that our method DS2 outperforms previous
works on few-shot DST in MultiWoZ 2.0 and 2.1, in both cross-domain and
multi-domain settings. Our method also exhibits vast speedup during both
training and inference as it can generate all states at once. Finally, based on
our analysis, we discover that the naturalness of the summary templates plays a
key role for successful training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism. (arXiv:2203.01594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01594">
<div class="article-summary-box-inner">
<span><p>Image captioning is a fast-growing research field of computer vision and
natural language processing that involves creating text explanations for
images. This study aims to develop a system that uses a pre-trained
convolutional neural network (CNN) to extract features from an image,
integrates the features with an attention mechanism, and creates captions using
a recurrent neural network (RNN). To encode an image into a feature vector as
graphical attributes, we employed multiple pre-trained convolutional neural
networks. Following that, a language model known as GRU is chosen as the
decoder to construct the descriptive sentence. In order to increase
performance, we merge the Bahdanau attention model with GRU to allow learning
to be focused on a specific portion of the image. On the MSCOCO dataset, the
experimental results achieve competitive performance against state-of-the-art
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UDAAN - Machine Learning based Post-Editing tool for Document Translation. (arXiv:2203.01644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01644">
<div class="article-summary-box-inner">
<span><p>We introduce UDAAN, an open-source post-editing tool that can reduce manual
editing efforts to quickly produce publishable-standard documents in different
languages. UDAAN has an end-to-end Machine Translation (MT) plus post-editing
pipeline wherein users can upload a document to obtain raw MT output. Further,
users can edit the raw translations using our tool. UDAAN offers several
advantages: a) Domain-aware, vocabulary-based lexical constrained MT. b)
source-target and target-target lexicon suggestions for users. Replacements are
based on the source and target texts lexicon alignment. c) Suggestions for
translations are based on logs created during user interaction. d)
Source-target sentence alignment visualisation that reduces the cognitive load
of users during editing. e) Translated outputs from our tool are available in
multiple formats: docs, latex, and PDF. Although we limit our experiments to
English-to-Hindi translation for the current study, our tool is independent of
the source and target languages. Experimental results based on the usage of the
tools and users feedback show that our tool speeds up the translation time
approximately by a factor of three compared to the baseline method of
translating documents from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation. (arXiv:2203.01670v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01670">
<div class="article-summary-box-inner">
<span><p>Early exiting allows instances to exit at different layers according to the
estimation of difficulty. Previous works usually adopt heuristic metrics such
as the entropy of internal outputs to measure instance difficulty, which
suffers from generalization and threshold-tuning. In contrast, learning to
exit, or learning to predict instance difficulty is a more appealing way.
Though some effort has been devoted to employing such "learn-to-exit" modules,
it is still unknown whether and how well the instance difficulty can be
learned. As a response, we first conduct experiments on the learnability of
instance difficulty, which demonstrates that modern neural models perform
poorly on predicting instance difficulty. Based on this observation, we propose
a simple-yet-effective Hash-based Early Exiting approach (HashEE) that replaces
the learn-to-exit modules with hash functions to assign each token to a fixed
exiting layer. Different from previous methods, HashEE requires no internal
classifiers nor extra parameters, and therefore is more efficient. Experimental
results on classification, regression, and generation tasks demonstrate that
HashEE can achieve higher performance with fewer FLOPs and inference time
compared with previous state-of-the-art early exiting methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Word Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation. (arXiv:2203.01677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01677">
<div class="article-summary-box-inner">
<span><p>Word-level adversarial attacks have shown success in NLP models, drastically
decreasing the performance of transformer-based models in recent years. As a
countermeasure, adversarial defense has been explored, but relatively few
efforts have been made to detect adversarial examples. However, detecting
adversarial examples may be crucial for automated tasks (e.g. review sentiment
analysis) that wish to amass information about a certain population and
additionally be a step towards a robust defense system. To this end, we release
a dataset for four popular attack methods on four datasets and four models to
encourage further research in this field. Along with it, we propose a
competitive baseline based on density estimation that has the highest AUC on 29
out of 30 dataset-attack-model combinations. Source code is available in
https://github.com/anoymous92874838/text-adv-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeerSum: A Peer Review Dataset for Abstractive Multi-document Summarization. (arXiv:2203.01769v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01769">
<div class="article-summary-box-inner">
<span><p>We present PeerSum, a new MDS dataset using peer reviews of scientific
publications. Our dataset differs from the existing MDS datasets in that our
summaries (i.e., the meta-reviews) are highly abstractive and they are real
summaries of the source documents (i.e., the reviews) and it also features
disagreements among source documents. We found that current state-of-the-art
MDS models struggle to generate high-quality summaries for PeerSum, offering
new research opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Enhanced Short Text Matching using Clickthrough Data. (arXiv:2203.01849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01849">
<div class="article-summary-box-inner">
<span><p>The short text matching task employs a model to determine whether two short
texts have the same semantic meaning or intent. Existing short text matching
models usually rely on the content of short texts which are lack information or
missing some key clues. Therefore, the short texts need external knowledge to
complete their semantic meaning. To address this issue, we propose a new short
text matching framework for introducing external knowledge to enhance the short
text contextual representation. In detail, we apply a self-attention mechanism
to enrich short text representation with external contexts. Experiments on two
Chinese datasets and one English dataset demonstrate that our framework
outperforms the state-of-the-art short text matching models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Intelligence: Tasks, Representation Learning, and Large Models. (arXiv:2203.01922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01922">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of vision-language (VL)
intelligence from the perspective of time. This survey is inspired by the
remarkable progress in both computer vision and natural language processing,
and recent trends shifting from single modality processing to multiple modality
comprehension. We summarize the development in this field into three time
periods, namely task-specific methods, vision-language pre-training (VLP)
methods, and larger models empowered by large-scale weakly-labeled data. We
first take some common VL tasks as examples to introduce the development of
task-specific methods. Then we focus on VLP methods and comprehensively review
key components of the model structures and training methods. After that, we
show how recent work utilizes large-scale raw image-text data to learn
language-aligned visual representations that generalize better on zero or few
shot learning tasks. Finally, we discuss some potential future trends towards
modality cooperation, unified representation, and knowledge incorporation. We
believe that this review will be of help for researchers and practitioners of
AI and ML, especially those interested in computer vision and natural language
processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning. (arXiv:2203.01927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01927">
<div class="article-summary-box-inner">
<span><p>Omission and addition of content is a typical issue in neural machine
translation. We propose a method for detecting such phenomena with
off-the-shelf translation models. Using contrastive conditioning, we compare
the likelihood of a full sequence under a translation model to the likelihood
of its parts, given the corresponding source or target sequence. This allows to
pinpoint superfluous words in the translation and untranslated words in the
source even in the absence of a reference translation. The accuracy of our
method is comparable to a supervised method that requires a custom quality
estimation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Robustness of Visual Question Answering Models. (arXiv:1912.01452v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.01452">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been playing an essential role in the task of
Visual Question Answering (VQA). Until recently, their accuracy has been the
main focus of research. Now there is a trend toward assessing the robustness of
these models against adversarial attacks by evaluating the accuracy of these
models under increasing levels of noisiness in the inputs of VQA models. In
VQA, the attack can target the image and/or the proposed query question, dubbed
main question, and yet there is a lack of proper analysis of this aspect of
VQA. In this work, we propose a new method that uses semantically related
questions, dubbed basic questions, acting as noise to evaluate the robustness
of VQA models. We hypothesize that as the similarity of a basic question to the
main question decreases, the level of noise increases. To generate a reasonable
noise level for a given main question, we rank a pool of basic questions based
on their similarity with this main question. We cast this ranking problem as a
LASSO optimization problem. We also propose a novel robustness measure Rscore
and two large-scale basic question datasets in order to standardize robustness
analysis of VQA models. The experimental results demonstrate that the proposed
evaluation method is able to effectively analyze the robustness of VQA models.
To foster the VQA research, we will publish our proposed datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity. (arXiv:2104.08786v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08786">
<div class="article-summary-box-inner">
<span><p>When primed with only a handful of training samples, very large, pretrained
language models such as GPT-3 have shown competitive results when compared to
fully-supervised, fine-tuned, large, pretrained language models. We demonstrate
that the order in which the samples are provided can make the difference
between near state-of-the-art and random guess performance: essentially some
permutations are "fantastic" and some not. We analyse this phenomenon in
detail, establishing that: it is present across model sizes (even for the
largest current models), it is not related to a specific subset of samples, and
that a given good permutation for one model is not transferable to another.
While one could use a development set to determine which permutations are
performant, this would deviate from the true few-shot setting as it requires
additional annotated data. Instead, we use the generative nature of language
models to construct an artificial development set and based on entropy
statistics of the candidate permutations on this set, we identify performant
prompts. Our method yields a 13% relative improvement for GPT-family models
across eleven different established text classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training ELECTRA Augmented with Multi-word Selection. (arXiv:2106.00139v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00139">
<div class="article-summary-box-inner">
<span><p>Pre-trained text encoders such as BERT and its variants have recently
achieved state-of-the-art performances on many NLP tasks. While being
effective, these pre-training methods typically demand massive computation
resources. To accelerate pre-training, ELECTRA trains a discriminator that
predicts whether each input token is replaced by a generator. However, this new
task, as a binary classification, is less semantically informative. In this
study, we present a new text encoder pre-training method that improves ELECTRA
based on multi-task learning. Specifically, we train the discriminator to
simultaneously detect replaced tokens and select original tokens from candidate
sets. We further develop two techniques to effectively combine all pre-training
tasks: (1) using attention-based networks for task-specific heads, and (2)
sharing bottom layers of the generator and the discriminator. Extensive
experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and
the efficiency of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04570">
<div class="article-summary-box-inner">
<span><p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00967">
<div class="article-summary-box-inner">
<span><p>Human language understanding operates at multiple levels of granularity
(e.g., words, phrases, and sentences) with increasing levels of abstraction
that can be hierarchically combined. However, existing deep models with stacked
layers do not explicitly model any sort of hierarchical process. This paper
proposes a recursive Transformer model based on differentiable CKY style binary
trees to emulate the composition process. We extend the bidirectional language
model pre-training objective to this architecture, attempting to predict each
word given its left and right abstraction nodes. To scale up our approach, we
also introduce an efficient pruned tree induction algorithm to enable encoding
in just a linear number of composition steps. Experimental results on language
modeling and unsupervised parsing show the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00578">
<div class="article-summary-box-inner">
<span><p>Neural models command state-of-the-art performance across NLP tasks,
including ones involving "reasoning". Models claiming to reason about the
evidence presented to them should attend to the correct parts of the input
avoiding spurious patterns therein, be self-consistent in their predictions
across inputs, and be immune to biases derived from their pre-training in a
nuanced, context-sensitive fashion. {\em Do the prevalent *BERT-family of
models do so?} In this paper, we study this question using the problem of
reasoning on tabular data. Tabular inputs are especially well-suited for the
study -- they admit systematic probes targeting the properties listed above.
Our experiments demonstrate that a RoBERTa-based model, representative of the
current state-of-the-art, fails at reasoning on the following counts: it (a)
ignores relevant parts of the evidence, (b) is over-sensitive to annotation
artifacts, and (c) relies on the knowledge encoded in the pre-trained language
model rather than the evidence presented in its tabular inputs. Finally,
through inoculation experiments, we show that fine-tuning the model on
perturbed data does not help it overcome the above challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Transformers Solve Compositional Tasks. (arXiv:2108.04378v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04378">
<div class="article-summary-box-inner">
<span><p>Several studies have reported the inability of Transformer models to
generalize compositionally, a key type of generalization in many NLP tasks such
as semantic parsing. In this paper we explore the design space of Transformer
models showing that the inductive biases given to the model by several design
decisions significantly impact compositional generalization. Through this
exploration, we identified Transformer configurations that generalize
compositionally significantly better than previously reported in the literature
in a diverse set of compositional tasks, and that achieve state-of-the-art
results in a semantic parsing compositional generalization benchmark (COGS),
and a string edit operation composition benchmark (PCFG).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparison of Latent Semantic Analysis and Correspondence Analysis of Document-Term Matrices. (arXiv:2108.06197v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06197">
<div class="article-summary-box-inner">
<span><p>Latent semantic analysis (LSA) and correspondence analysis (CA) are two
techniques that use a singular value decomposition (SVD) for dimensionality
reduction. LSA has been extensively used to obtain low-dimensional and dense
vectors that capture relationships among documents and terms. In this article,
we present a theoretical analysis and comparison of the two techniques in the
context of document-term matrices. We show that CA has some attractive
properties as compared to LSA, for instance that effects of margins arising
from differing document-lengths and term-frequencies are effectively
eliminated, so that the CA solution is optimally suited to focus on
relationships among documents and terms. A unifying framework is proposed that
includes both CA and LSA as special cases. We empirically compare CA to various
LSA based methods on two tasks, a document classification task in English and
an authorship attribution task on historical Dutch texts, and find that CA
performs significantly better. We also apply CA to a long-standing question
regarding the authorship of the Dutch national anthem Wilhelmus and provide
further support that it can be attributed to the author Datheen, amongst
several contenders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Relation Modeling: Learning to Define Relations between Entities. (arXiv:2108.09241v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09241">
<div class="article-summary-box-inner">
<span><p>Relations between entities can be represented by different instances, e.g., a
sentence containing both entities or a fact in a Knowledge Graph (KG). However,
these instances may not well capture the general relations between entities,
may be difficult to understand by humans, even may not be found due to the
incompleteness of the knowledge source. In this paper, we introduce the Open
Relation Modeling problem - given two entities, generate a coherent sentence
describing the relation between them. To solve this problem, we propose to
teach machines to generate definition-like relation descriptions by letting
them learn from defining entities. Specifically, we fine-tune Pre-trained
Language Models (PLMs) to produce definitions conditioned on extracted entity
pairs. To help PLMs reason between entities and provide additional relational
knowledge to PLMs for open relation modeling, we incorporate reasoning paths in
KGs and include a reasoning path selection mechanism. Experimental results show
that our model can generate concise but informative relation descriptions that
capture the representative characteristics of entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish. (arXiv:2109.08615v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08615">
<div class="article-summary-box-inner">
<span><p>A morphological analyzer, which is a significant component of many natural
language processing applications especially for morphologically rich languages,
divides an input word into all its composing morphemes and identifies their
morphological roles. In this paper, we introduce a comprehensive morphological
analyzer for Central Kurdish (CK), a low-resourced language with a rich
morphology. Building upon the limited existing literature, we first assembled
and systematically categorized a comprehensive collection of the morphological
and morphophonological rules of the language. Additionally, we collected and
manually labeled a generative lexicon containing nearly 10,000 verb, noun and
adjective stems, named entities, and other types of word stems. We used these
rule sets and resources to implement CKMorph Analyzer based on finite-state
transducers. In order to provide a benchmark for future research, we collected,
manually labeled, and publicly shared test sets for evaluating accuracy and
coverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the
accuracy test set, containing 1,000 CK words morphologically analyzed according
to the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M
CK tokens of the coverage test set. The demonstration of the application and
resources including CK verb database and test sets are openly accessible at
https://github.com/CKMorph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Molecular Representation Learning via Contrastive Pre-training. (arXiv:2109.08830v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08830">
<div class="article-summary-box-inner">
<span><p>Molecular representation learning plays an essential role in cheminformatics.
Recently, language model-based approaches have gained popularity as an
alternative to traditional expert-designed features to encode molecules.
However, these approaches only utilize a single molecular language for
representation learning. Motivated by the fact that a given molecule can be
described using different languages such as Simplified Molecular Line Entry
System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC),
and The IUPAC International Chemical Identifier (InChI), we propose a
multilingual molecular embedding generation approach called MM-Deacon
(multilingual molecular domain embedding analysis via contrastive learning).
MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on
large-scale molecules. We evaluated the robustness of our method on seven
molecular property prediction tasks from MoleculeNet benchmark, zero-shot
cross-lingual retrieval, and a drug-drug interaction prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESPnet-SLU: Advancing Spoken Language Understanding through ESPnet. (arXiv:2111.14706v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14706">
<div class="article-summary-box-inner">
<span><p>As Automatic Speech Processing (ASR) systems are getting better, there is an
increasing interest of using the ASR output to do downstream Natural Language
Processing (NLP) tasks. However, there are few open source toolkits that can be
used to generate reproducible results on different Spoken Language
Understanding (SLU) benchmarks. Hence, there is a need to build an open source
standard that can be used to have a faster start into SLU research. We present
ESPnet-SLU, which is designed for quick development of spoken language
understanding in a single framework. ESPnet-SLU is a project inside end-to-end
speech processing toolkit, ESPnet, which is a widely used open-source standard
for various speech processing tasks like ASR, Text to Speech (TTS) and Speech
Translation (ST). We enhance the toolkit to provide implementations for various
SLU benchmarks that enable researchers to seamlessly mix-and-match different
ASR and NLU models. We also provide pretrained models with intensively tuned
hyper-parameters that can match or even outperform the current state-of-the-art
performances. The toolkit is publicly available at
https://github.com/espnet/espnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking. (arXiv:2112.01206v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01206">
<div class="article-summary-box-inner">
<span><p>The goal of local citation recommendation is to recommend a missing reference
from the local citation context and optionally also from the global context. To
balance the tradeoff between speed and accuracy of citation recommendation in
the context of a large-scale paper database, a viable approach is to first
prefetch a limited number of relevant documents using efficient ranking methods
and then to perform a fine-grained reranking using more sophisticated models.
In that vein, BM25 has been found to be a tough-to-beat approach to
prefetching, which is why recent work has focused mainly on the reranking step.
Even so, we explore prefetching with nearest neighbor search among text
embeddings constructed by a hierarchical attention network. When coupled with a
SciBERT reranker fine-tuned on local citation recommendation tasks, our
hierarchical Attention encoder (HAtten) achieves high prefetch recall for a
given number of candidates to be reranked. Consequently, our reranker requires
fewer prefetch candidates to rerank, yet still achieves state-of-the-art
performance on various local citation recommendation datasets such as ACL-200,
FullTextPeerRead, RefSeer, and arXiv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Term Rewriting Based On Set Automaton Matching. (arXiv:2202.08687v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08687">
<div class="article-summary-box-inner">
<span><p>In previous work we have proposed an efficient pattern matching algorithm
based on the notion of set automaton. In this article we investigate how set
automata can be exploited to implement efficient term rewriting procedures.
These procedures interleave pattern matching steps and rewriting steps and thus
smoothly integrate redex discovery and subterm replacement. Concretely, we
propose an optimised algorithm for outermost rewriting of left-linear term
rewriting systems, prove its correctness, and present the results of some
implementation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How reparametrization trick broke differentially-private text representation learning. (arXiv:2202.12138v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12138">
<div class="article-summary-box-inner">
<span><p>As privacy gains traction in the NLP community, researchers have started
adopting various approaches to privacy-preserving methods. One of the favorite
privacy frameworks, differential privacy (DP), is perhaps the most compelling
thanks to its fundamental theoretical guarantees. Despite the apparent
simplicity of the general concept of differential privacy, it seems non-trivial
to get it right when applying it to NLP. In this short paper, we formally
analyze several recent NLP papers proposing text representation learning using
DPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and
reveal their false claims of being differentially private. Furthermore, we also
show a simple yet general empirical sanity check to determine whether a given
implementation of a DP mechanism almost certainly violates the privacy loss
guarantees. Our main goal is to raise awareness and help the community
understand potential pitfalls of applying differential privacy to text
representation learning.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Colon Nuclei Instance Segmentation using a Probabilistic Two-Stage Detector. (arXiv:2203.01321v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01321">
<div class="article-summary-box-inner">
<span><p>Cancer is one of the leading causes of death in the developed world. Cancer
diagnosis is performed through the microscopic analysis of a sample of
suspicious tissue. This process is time consuming and error prone, but Deep
Learning models could be helpful for pathologists during cancer diagnosis. We
propose to change the CenterNet2 object detection model to also perform
instance segmentation, which we call SegCenterNet2. We train SegCenterNet2 in
the CoNIC challenge dataset and show that it performs better than Mask R-CNN in
the competition metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture. (arXiv:2203.01322v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2203.01323v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01323">
<div class="article-summary-box-inner">
<span><p>Accuracies of deep learning (DL) classifiers are often unstable in that they
may change significantly when retested on adversarial images, imperfect images,
or perturbed images. This paper adds to the fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. To measure
robust DL classifiers, previous research reported on single-factor corruption.
We created comprehensive 69 benchmarking image sets, including a clean set,
sets with single factor perturbations, and sets with two-factor perturbation
conditions. The state-of-the-art two-factor perturbation includes (a) two
digital perturbations (salt &amp; pepper noise and Gaussian noise) applied in both
sequences, and (b) one digital perturbation (salt &amp; pepper noise) and a
geometric perturbation (rotation) applied in both sequences. Previous research
evaluating DL classifiers has often used top-1/top-5 accuracy. We innovate a
new two-dimensional, statistical matrix to evaluating robustness of DL
classifiers. Also, we introduce a new visualization tool, including minimum
accuracy, maximum accuracy, mean accuracies, and coefficient of variation (CV),
for benchmarking robustness of DL classifiers. Comparing with single factor
corruption, we first report that using two-factor perturbed images improves
both robustness and accuracy of DL classifiers. All source codes and related
image sets are shared on the Website at <a href="http://cslinux.semo.edu/david/data">this http URL</a> to
support future academic research and industry projects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation. (arXiv:2203.01324v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01324">
<div class="article-summary-box-inner">
<span><p>Semi-supervised segmentation remains challenging in medical imaging since the
amount of annotated medical data is often limited and there are many blurred
pixels near the adhesive edges or low-contrast regions. To address the issues,
we advocate to firstly constrain the consistency of samples with and without
strong perturbations to apply sufficient smoothness regularization and further
encourage the class-level separation to exploit the unlabeled ambiguous pixels
for the model training. Particularly, in this paper, we propose the SS-Net for
semi-supervised medical image segmentation tasks, via exploring the pixel-level
Smoothness and inter-class Separation at the same time. The pixel-level
smoothness forces the model to generate invariant results under adversarial
perturbations. Meanwhile, the inter-class separation constrains individual
class features should approach their corresponding high-quality prototypes, in
order to make each class distribution compact and separate different classes.
We evaluated our SS-Net against five recent methods on the public LA and ACDC
datasets. The experimental results under two semi-supervised settings
demonstrate the superiority of our proposed SS-Net, achieving new
state-of-the-art (SOTA) performance on both datasets. The codes will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations. (arXiv:2203.01325v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01325">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider two challenging issues in reference-based
super-resolution (RefSR), (i) how to choose a proper reference image, and (ii)
how to learn real-world RefSR in a self-supervised manner. Particularly, we
present a novel self-supervised learning approach for real-world image SR from
observations at dual camera zooms (SelfDZSR). For the first issue, the more
zoomed (telephoto) image can be naturally leveraged as the reference to guide
the SR of the lesser zoomed (short-focus) image. For the second issue, SelfDZSR
learns a deep network to obtain the SR result of short-focal image and with the
same resolution as the telephoto image. For this purpose, we take the telephoto
image instead of an additional high-resolution image as the supervision
information and select a patch from it as the reference to super-resolve the
corresponding short-focus image patch. To mitigate the effect of various
misalignment between the short-focus low-resolution (LR) image and telephoto
ground-truth (GT) image, we design a degradation model and map the GT to a
pseudo-LR image aligned with GT. Then the pseudo-LR and LR image can be fed
into the proposed adaptive spatial transformer networks (AdaSTN) to deform the
LR features. During testing, SelfDZSR can be directly deployed to super-solve
the whole short-focus image with the reference of telephoto image. Experiments
show that our method achieves better quantitative and qualitative performance
against state-of-the-arts. The code and pre-trained models will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder. (arXiv:2203.01327v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01327">
<div class="article-summary-box-inner">
<span><p>Hyperspectral pixel intensities result from a mixing of reflectances from
several materials. This paper develops a method of hyperspectral pixel {\it
unmixing} that aims to recover the "pure" spectral signal of each material
(hereafter referred to as {\it endmembers}) together with the mixing ratios
({\it abundances}) given the spectrum of a single pixel. The unmixing problem
is particularly relevant in the case of low-resolution hyperspectral images
captured in a remote sensing setting, where individual pixels can cover large
regions of the scene. Under the assumptions that (1) a multivariate Normal
distribution can represent the spectra of an endmember and (2) a Dirichlet
distribution can encode abundances of different endmembers, we develop a Latent
Dirichlet Variational Autoencoder for hyperspectral pixel unmixing. Our
approach achieves state-of-the-art results on standard benchmarks and on
synthetic data generated using United States Geological Survey spectral
library.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Reconstruction for Open-set Semantic Segmentation. (arXiv:2203.01368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01368">
<div class="article-summary-box-inner">
<span><p>Open set segmentation is a relatively new and unexploredtask, with just a
handful of methods proposed to model suchtasks.We propose a novel method called
CoReSeg thattackles the issue using class conditional reconstruction ofthe
input images according to their pixelwise mask. Ourmethod conditions each input
pixel to all known classes,expecting higher errors for pixels of unknown
classes. Itwas observed that the proposed method produces better se-mantic
consistency in its predictions, resulting in cleanersegmentation maps that
better fit object boundaries. CoRe-Seg outperforms state-of-the-art methods on
the Vaihin-gen and Potsdam ISPRS datasets, while also being com-petitive on the
Houston 2018 IEEE GRSS Data Fusiondataset. Official implementation for CoReSeg
is availableat:https://github.com/iannunes/CoReSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification. (arXiv:2203.01386v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01386">
<div class="article-summary-box-inner">
<span><p>The main question we address in this paper is how to scale up visual
recognition of unseen classes, also known as zero-shot learning, to tens of
thousands of categories as in the ImageNet-21K benchmark. At this scale,
especially with many fine-grained categories included in ImageNet-21K, it is
critical to learn quality visual semantic representations that are
discriminative enough to recognize unseen classes and distinguish them from
seen ones. We propose a Hierarchical Graphical knowledge Representation
framework for the confidence-based classification method, dubbed as HGR-Net.
Our experimental results demonstrate that HGR-Net can grasp class inheritance
relations by utilizing hierarchical conceptual knowledge. Our method
significantly outperformed all existing techniques, boosting the performance 7%
compared to the runner-up approach on the ImageNet-21K benchmark. We show that
HGR-Net is learning-efficient in few-shot scenarios. We also analyzed our
method on smaller datasets like ImageNet-21K-P, 2-hops and 3-hops,
demonstrating its generalization ability. Our benchmark and code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iMVS: Improving MVS Networks by Learning Depth Discontinuities. (arXiv:2203.01391v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01391">
<div class="article-summary-box-inner">
<span><p>Existing learning-based multi-view stereo (MVS) techniques are effective in
terms of completeness in reconstruction. We further improve these techniques by
learning depth continuities. Our idea is to jointly estimate the depth and
boundary maps. To this end, we introduce learning-based MVS strategies to
improve the quality of depth maps via mixture density and depth discontinuity
learning. We validate our idea and demonstrate that our strategies can be
easily integrated into existing learning-based MVS pipelines where the
reconstruction depends on high-quality depth map estimation. We also introduce
a bimodal depth representation and a novel spatial regularization approach to
the MVS networks. Extensive experiments on various datasets show that our
method sets a new state of the art in terms of completeness and overall
reconstruction quality. Experiments also demonstrate that the presented model
and strategies have good generalization capabilities. The source code will be
available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Timing Error: A Case Study of Navigation Camera. (arXiv:2203.01412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01412">
<div class="article-summary-box-inner">
<span><p>We focus on the problem of timing errors in navigation camera as a case study
in a broader problem of the effect of a timing error in cyber-physical systems.
These systems rely on the requirement that certain things happen at the same
time or certain things happen periodically at some period $T$. However, as
these systems get more complex, timing errors can occur between the components
thereby violating the assumption about events being simultaneous (or periodic).
</p>
<p>We consider the problem of a surgical navigation system where optical markers
detected in the 2D pictures taken by two cameras are used to localize the
markers in 3D space. A predefined array of such markers, known as a reference
element, is used to navigate the corresponding CAD model of a surgical
instrument on patient's images. The cameras rely on the assumption that the
pictures from both cameras are taken exactly at the same time. If a timing
error occurs then the instrument may have moved between the pictures. We find
that, depending upon the location of the instrument, this can lead to a
substantial error in the localization of the instrument. Specifically, we find
that if the actual movement is $\delta$ then the observed movement may be as
high as $5\delta$ in the operating range of the camera. Furthermore, we also
identify potential issues that could affect the error in case there are changes
to the camera system or to the operating range.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUAD: Multiple Uncertainties for Autonomous Driving benchmark for multiple uncertainty types and tasks. (arXiv:2203.01437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01437">
<div class="article-summary-box-inner">
<span><p>Predictive uncertainty estimation is essential for deploying Deep Neural
Networks in real-world autonomous systems. However, disentangling the different
types and sources of uncertainty is non trivial in most datasets, especially
since there is no ground truth for uncertainty. In addition, different degrees
of weather conditions can disrupt neural networks, resulting in inconsistent
training data quality. Thus, we introduce the MUAD dataset (Multiple
Uncertainties for Autonomous Driving), consisting of 8,500 realistic synthetic
images with diverse adverse weather conditions (night, fog, rain, snow),
out-of-distribution objects and annotations for semantic segmentation, depth
estimation, object and instance detection. MUAD allows to better assess the
impact of different sources of uncertainty on model performance. We propose a
study that shows the importance of having reliable Deep Neural Networks (DNNs)
in multiple experiments, and will release our dataset to allow researchers to
benchmark their algorithm methodically in ad-verse conditions. More information
and the download link for MUAD are available at https://muad-dataset.github.io/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Adversarial Robustness for Deep Metric Learning. (arXiv:2203.01439v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01439">
<div class="article-summary-box-inner">
<span><p>Owing to security implications of adversarial vulnerability, adversarial
robustness of deep metric learning models has to be improved. In order to avoid
model collapse due to excessively hard examples, the existing defenses dismiss
the min-max adversarial training, but instead learn from a weak adversary
inefficiently. Conversely, we propose Hardness Manipulation to efficiently
perturb the training triplet till a specified level of hardness for adversarial
training, according to a harder benign triplet or a pseudo-hardness function.
It is flexible since regular training and min-max adversarial training are its
boundary cases. Besides, Gradual Adversary, a family of pseudo-hardness
functions is proposed to gradually increase the specified hardness level during
training for a better balance between performance and robustness. Additionally,
an Intra-Class Structure loss term among benign and adversarial examples
further improves model robustness and efficiency. Comprehensive experimental
results suggest that the proposed method, although simple in its form,
overwhelmingly outperforms the state-of-the-art defenses in terms of
robustness, training efficiency, as well as performance on benign examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Common Corruptions and Data Augmentation. (arXiv:2203.01441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01441">
<div class="article-summary-box-inner">
<span><p>We introduce a set of image transformations that can be used as `corruptions'
to evaluate the robustness of models as well as `data augmentation' mechanisms
for training neural networks. The primary distinction of the proposed
transformations is that, unlike existing approaches such as Common Corruptions,
the geometry of the scene is incorporated in the transformations -- thus
leading to corruptions that are more likely to occur in the real world. We show
these transformations are `efficient' (can be computed on-the-fly),
`extendable' (can be applied on most datasets of real images), expose
vulnerability of existing models, and can effectively make models more robust
when employed as `3D data augmentation' mechanisms. Our evaluations performed
on several tasks and datasets suggest incorporating 3D information into
robustness benchmarking and training opens up a promising direction for
robustness research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives. (arXiv:2203.01445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01445">
<div class="article-summary-box-inner">
<span><p>The volume of available data has grown dramatically in recent years in many
applications. Furthermore, the age of networks that used multiple modalities
separately has practically ended. Therefore, enabling bidirectional
cross-modality data retrieval capable of processing has become a requirement
for many domains and disciplines of research. This is especially true in the
medical field, as data comes in a multitude of types, including various types
of images and reports as well as molecular data. Most contemporary works apply
cross attention to highlight the essential elements of an image or text in
relation to the other modalities and try to match them together. However,
regardless of their importance in their own modality, these approaches usually
consider features of each modality equally. In this study, self-attention as an
additional loss term will be proposed to enrich the internal representation
provided into the cross attention module. This work suggests a novel
architecture with a new loss term to help represent images and texts in the
joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO
and ARCH, show the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Pose Estimation using Mid-level Visual Representations. (arXiv:2203.01449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01449">
<div class="article-summary-box-inner">
<span><p>This work proposes a novel pose estimation model for object categories that
can be effectively transferred to previously unseen environments. The deep
convolutional network models (CNN) for pose estimation are typically trained
and evaluated on datasets specifically curated for object detection, pose
estimation, or 3D reconstruction, which requires large amounts of training
data. In this work, we propose a model for pose estimation that can be trained
with small amount of data and is built on the top of generic mid-level
representations \cite{taskonomy2018} (e.g. surface normal estimation and
re-shading). These representations are trained on a large dataset without
requiring pose and object annotations. Later on, the predictions are refined
with a small CNN neural network that exploits object masks and silhouette
retrieval. The presented approach achieves superior performance on the Pix3D
dataset \cite{pix3d} and shows nearly 35\% improvement over the existing models
when only 25\% of the training data is available. We show that the approach is
favorable when it comes to generalization and transfer to novel environments.
Towards this end, we introduce a new pose estimation benchmark for commonly
encountered furniture categories on challenging Active Vision Dataset
\cite{Ammirato2017ADF} and evaluated the models trained on the Pix3D dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation. (arXiv:2203.01452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01452">
<div class="article-summary-box-inner">
<span><p>Panoramic images with their 360-degree directional view encompass exhaustive
information about the surrounding space, providing a rich foundation for scene
understanding. To unfold this potential in the form of robust panoramic
segmentation models, large quantities of expensive, pixel-wise annotations are
crucial for success. Such annotations are available, but predominantly for
narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal
resources for training panoramic models. Distortions and the distinct
image-feature distribution in 360-degree panoramas impede the transfer from the
annotation-rich pinhole domain and therefore come with a big dent in
performance. To get around this domain difference and bring together semantic
annotations from pinhole- and 360-degree surround-visuals, we propose to learn
object deformations and panoramic image distortions in the Deformable Patch
Embedding (DPE) and Deformable MLP (DMLP) components which blend into our
Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we
tie together shared semantics in pinhole- and panoramic feature embeddings by
generating multi-scale prototype features and aligning them in our Mutual
Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor
Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance
to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled
panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by
14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available
at https://github.com/jamycheung/Trans4PASS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Gating-Adjacency GCN for Human Motion Prediction. (arXiv:2203.01474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01474">
<div class="article-summary-box-inner">
<span><p>Predicting future motion based on historical motion sequence is a fundamental
problem in computer vision, and it has wide applications in autonomous driving
and robotics. Some recent works have shown that Graph Convolutional
Networks(GCN) are instrumental in modeling the relationship between different
joints. However, considering the variants and diverse action types in human
motion data, the cross-dependency of the spatial-temporal relationships will be
difficult to depict due to the decoupled modeling strategy, which may also
exacerbate the problem of insufficient generalization. Therefore, we propose
the Spatial-Temporal Gating-Adjacency GCN(GAGCN) to learn the complex
spatial-temporal dependencies over diverse action types. Specifically, we adopt
gating networks to enhance the generalization of GCN via the trainable adaptive
adjacency matrix obtained by blending the candidate spatial-temporal adjacency
matrices. Moreover, GAGCN addresses the cross-dependency of space and time by
balancing the weights of spatial-temporal modeling and fusing the decoupled
spatial-temporal features. Extensive experiments on Human 3.6M, AMASS, and 3DPW
demonstrate that GAGCN achieves state-of-the-art performance in both short-term
and long-term predictions. Our code will be released in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. (arXiv:2203.01475v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01475">
<div class="article-summary-box-inner">
<span><p>Curating a large set of fully annotated training data can be costly,
especially for the tasks of medical image segmentation. Scribble, a weaker form
of annotation, is more obtainable in practice, but training segmentation models
from limited supervision of scribbles is still challenging. To address the
difficulties, we propose a new framework for scribble learning-based medical
image segmentation, which is composed of mix augmentation and cycle consistency
and thus is referred to as CycleMix. For augmentation of supervision, CycleMix
adopts the mixup strategy with a dedicated design of random occlusion, to
perform increments and decrements of scribbles. For regularization of
supervision, CycleMix intensifies the training objective with consistency
losses to penalize inconsistent segmentation, which results in significant
improvement of segmentation performance. Results on two open datasets, i.e.,
ACDC and MSCMRseg, showed that the proposed method achieved exhilarating
performance, demonstrating comparable or even better accuracy than the
fully-supervised methods. The code and expert-made scribble annotations for
MSCMRseg will be released once this article is accepted for publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaDT: Meta Decision Tree for Interpretable Few-Shot Learning. (arXiv:2203.01482v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01482">
<div class="article-summary-box-inner">
<span><p>Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel
classes with few examples. Recently, lots of methods have been proposed from
the perspective of meta-learning and representation learning for improving FSL
performance. However, few works focus on the interpretability of FSL decision
process. In this paper, we take a step towards the interpretable FSL by
proposing a novel decision tree-based meta-learning framework, namely, MetaDT.
Our insight is replacing the last black-box FSL classifier of the existing
representation learning methods by an interpretable decision tree with
meta-learning. The key challenge is how to effectively learn the decision tree
(i.e., the tree structure and the parameters of each node) in the FSL setting.
To address the challenge, we introduce a tree-like class hierarchy as our
prior: 1) the hierarchy is directly employed as the tree structure; 2) by
regarding the class hierarchy as an undirected graph, a graph convolution-based
decision tree inference network is designed as our meta-learner to learn to
infer the parameters of each node. At last, a two-loop optimization mechanism
is incorporated into our framework for a fast adaptation of the decision tree
with few examples. Extensive experiments on performance comparison and
interpretability analysis show the effectiveness and superiority of our MetaDT.
Our code will be publicly available upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PetsGAN: Rethinking Priors for Single Image Generation. (arXiv:2203.01488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01488">
<div class="article-summary-box-inner">
<span><p>Single image generation (SIG), described as generating diverse samples that
have similar visual content with the given single image, is first introduced by
SinGAN which builds a pyramid of GANs to progressively learn the internal patch
distribution of the single image. It also shows great potentials in a wide
range of image manipulation tasks. However, the paradigm of SinGAN has
limitations in terms of generation quality and training time. Firstly, due to
the lack of high-level information, SinGAN cannot handle the object images well
as it does on the scene and texture images. Secondly, the separate progressive
training scheme is time-consuming and easy to cause artifact accumulation. To
tackle these problems, in this paper, we dig into the SIG problem and improve
SinGAN by fully-utilization of internal and external priors. The main
contributions of this paper include: 1) We introduce to SIG a regularized
latent variable model. To the best of our knowledge, it is the first time to
give a clear formulation and optimization goal of SIG, and all the existing
methods for SIG can be regarded as special cases of this model. 2) We design a
novel Prior-based end-to-end training GAN (PetsGAN) to overcome the problems of
SinGAN. Our method gets rid of the time-consuming progressive training scheme
and can be trained end-to-end. 3) We construct abundant qualitative and
quantitative experiments to show the superiority of our method on both
generated image quality, diversity, and the training speed. Moreover, we apply
our method to other image manipulation tasks (e.g., style transfer,
harmonization), and the results further prove the effectiveness and efficiency
of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation. (arXiv:2203.01502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01502">
<div class="article-summary-box-inner">
<span><p>Estimating the accurate depth from a single image is challenging since it is
inherently ambiguous and ill-posed. While recent works design increasingly
complicated and powerful networks to directly regress the depth map, we take
the path of CRFs optimization. Due to the expensive computation, CRFs are
usually performed between neighborhoods rather than the whole graph. To
leverage the potential of fully-connected CRFs, we split the input into windows
and perform the FC-CRFs optimization within each window, which reduces the
computation complexity and makes FC-CRFs feasible. To better capture the
relationships between nodes in the graph, we exploit the multi-head attention
mechanism to compute a multi-head potential function, which is fed to the
networks to output an optimized depth map. Then we build a bottom-up-top-down
structure, where this neural window FC-CRFs module serves as the decoder, and a
vision transformer serves as the encoder. The experiments demonstrate that our
method significantly improves the performance across all metrics on both the
KITTI and NYUv2 datasets, compared to previous methods. Furthermore, the
proposed method can be directly applied to panorama images and outperforms all
previous panorama methods on the MatterPort3D dataset. The source code of our
method will be made public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoftGroup for 3D Instance Segmentation on Point Clouds. (arXiv:2203.01509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01509">
<div class="article-summary-box-inner">
<span><p>Existing state-of-the-art 3D instance segmentation methods perform semantic
segmentation followed by grouping. The hard predictions are made when
performing semantic segmentation such that each point is associated with a
single class. However, the errors stemming from hard decision propagate into
grouping that results in (1) low overlaps between the predicted instance with
the ground truth and (2) substantial false positives. To address the
aforementioned problems, this paper proposes a 3D instance segmentation method
referred to as SoftGroup by performing bottom-up soft grouping followed by
top-down refinement. SoftGroup allows each point to be associated with multiple
classes to mitigate the problems stemming from semantic prediction errors and
suppresses false positive instances by learning to categorize them as
background. Experimental results on different datasets and multiple evaluation
metrics demonstrate the efficacy of SoftGroup. Its performance surpasses the
strongest prior method by a significant margin of +6.2% on the ScanNet v2
hidden test set and +6.8% on S3DIS Area 5 in terms of AP_50. SoftGroup is also
fast, running at 345ms per scan with a single Titan X on ScanNet v2 dataset.
The source code and trained models for both datasets are available at
\url{https://github.com/thangvubk/SoftGroup.git}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking. (arXiv:2203.01516v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01516">
<div class="article-summary-box-inner">
<span><p>Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related
applications, which leads to a highly demanding requirement on the robustness
of UAV trackers. However, adding imperceptible perturbations can easily fool
the tracker and cause tracking failures. This risk is often overlooked and
rarely researched at present. Therefore, to help increase awareness of the
potential risk and the robustness of UAV tracking, this work proposes a novel
adaptive adversarial attack approach, i.e., Ad$^2$Attack, against UAV object
tracking. Specifically, adversarial examples are generated online during the
resampling of the search patch image, which leads trackers to lose the target
in the following frames. Ad$^2$Attack is composed of a direct downsampling
module and a super-resolution upsampling module with adaptive stages. A novel
optimization function is proposed for balancing the imperceptibility and
efficiency of the attack. Comprehensive experiments on several well-known
benchmarks and real-world conditions show the effectiveness of our attack
method, which dramatically reduces the performance of the most advanced Siamese
trackers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01522">
<div class="article-summary-box-inner">
<span><p>Despite the success of deep neural networks, there are still many challenges
in deep representation learning due to the data scarcity issues such as data
imbalance, unseen distribution, and domain shift. To address the
above-mentioned issues, a variety of methods have been devised to explore the
sample relationships in a vanilla way (i.e., from the perspectives of either
the input or the loss function), failing to explore the internal structure of
deep neural networks for learning with sample relationships. Inspired by this,
we propose to enable deep neural networks themselves with the ability to learn
the sample relationships from each mini-batch. Specifically, we introduce a
batch transformer module or BatchFormer, which is then applied into the batch
dimension of each mini-batch to implicitly explore sample relationships during
training. By doing this, the proposed method enables the collaboration of
different samples, e.g., the head-class samples can also contribute to the
learning of the tail classes for long-tailed recognition. Furthermore, to
mitigate the gap between training and testing, we share the classifier between
with or without the BatchFormer during training, which can thus be removed
during testing. We perform extensive experiments on over ten datasets and the
proposed method achieves significant improvements on different data scarcity
applications without any bells and whistles, including the tasks of long-tailed
recognition, compositional zero-shot learning, domain generalization, and
contrastive learning. Code will be made publicly available at
\url{https://github.com/zhihou7/BatchFormer}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAFE: Learning to Condense Dataset by Aligning Features. (arXiv:2203.01531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01531">
<div class="article-summary-box-inner">
<span><p>Dataset condensation aims at reducing the network training effort through
condensing a cumbersome training set into a compact synthetic one.
State-of-the-art approaches largely rely on learning the synthetic data by
matching the gradients between the real and synthetic data batches. Despite the
intuitive motivation and promising results, such gradient-based methods, by
nature, easily overfit to a biased set of samples that produce dominant
gradients, and thus lack global supervision of data distribution. In this
paper, we propose a novel scheme to Condense dataset by Aligning FEatures
(CAFE), which explicitly attempts to preserve the real-feature distribution as
well as the discriminant power of the resulting synthetic set, lending itself
to strong generalization capability to various architectures. At the heart of
our approach is an effective strategy to align features from the real and
synthetic data across various scales, while accounting for the classification
of real samples. Our scheme is further backed up by a novel dynamic bi-level
optimization, which adaptively adjusts parameter updates to prevent
over-/under-fitting. We validate the proposed CAFE across various datasets, and
demonstrate that it generally outperforms the state of the art: on the SVHN
dataset, for example, the performance gain is up to 11%. Extensive experiments
and analyses verify the effectiveness and necessity of proposed designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Patch-wise Semantic Relation for Contrastive Learning in Image-to-Image Translation Tasks. (arXiv:2203.01532v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01532">
<div class="article-summary-box-inner">
<span><p>Recently, contrastive learning-based image translation methods have been
proposed, which contrasts different spatial locations to enhance the spatial
correspondence. However, the methods often ignore the diverse semantic relation
within the images. To address this, here we propose a novel semantic relation
consistency (SRC) regularization along with the decoupled contrastive learning,
which utilize the diverse semantics by focusing on the heterogeneous semantics
between the image patches of a single image. To further improve the
performance, we present a hard negative mining by exploiting the semantic
relation. We verified our method for three tasks: single-modal and multi-modal
image translations, and GAN compression task for image translation.
Experimental results confirmed the state-of-art performance of our method in
all the three tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. (arXiv:2203.01536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01536">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) are becoming more popular and dominating technique
for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a
demanding technique in computer vision, ViTs have been successfully solved
various vision problems while focusing on long-range relationships. In this
paper, we begin by introducing the fundamental concepts and background of the
self-attention mechanism. Next, we provide a comprehensive overview of recent
top-performing ViT methods describing in terms of strength and weakness,
computational cost as well as training and testing dataset. We thoroughly
compare the performance of various ViT algorithms and most representative CNN
methods on popular benchmark datasets. Finally, we explore some limitations
with insightful observations and provide further research direction. The
project page along with the collections of papers are available at
https://github.com/khawar512/ViT-Survey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Transparent Liquid Segmentation for Robotic Pouring. (arXiv:2203.01538v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01538">
<div class="article-summary-box-inner">
<span><p>Liquid state estimation is important for robotics tasks such as pouring;
however, estimating the state of transparent liquids is a challenging problem.
We propose a novel segmentation pipeline that can segment transparent liquids
such as water from a static, RGB image without requiring any manual annotations
or heating of the liquid for training. Instead, we use a generative model that
is capable of translating images of colored liquids into synthetically
generated transparent liquid images, trained only on an unpaired dataset of
colored and transparent liquid images. Segmentation labels of colored liquids
are obtained automatically using background subtraction. Our experiments show
that we are able to accurately predict a segmentation mask for transparent
liquids without requiring any manual annotations. We demonstrate the utility of
transparent liquid segmentation in a robotic pouring task that controls pouring
by perceiving the liquid height in a transparent cup. Accompanying video and
supplementary materials can be found
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum-style Local-to-global Adaptation for Cross-domain Remote Sensing Image Segmentation. (arXiv:2203.01539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01539">
<div class="article-summary-box-inner">
<span><p>Although domain adaptation has been extensively studied in natural
image-based segmentation task, the research on cross-domain segmentation for
very high resolution (VHR) remote sensing images (RSIs) still remains
underexplored. The VHR RSIs-based cross-domain segmentation mainly faces two
critical challenges: 1) Large area land covers with many diverse object
categories bring severe local patch-level data distribution deviations, thus
yielding different adaptation difficulties for different local patches; 2)
Different VHR sensor types or dynamically changing modes cause the VHR images
to go through intensive data distribution differences even for the same
geographical location, resulting in different global feature-level domain gap.
To address these challenges, we propose a curriculum-style local-to-global
cross-domain adaptation framework for the segmentation of VHR RSIs. The
proposed curriculum-style adaptation performs the adaptation process in an
easy-to-hard way according to the adaptation difficulties that can be obtained
using an entropy-based score for each patch of the target domain, and thus well
aligns the local patches in a domain image. The proposed local-to-global
adaptation performs the feature alignment process from the locally semantic to
globally structural feature discrepancies, and consists of a semantic-level
domain classifier and an entropy-level domain classifier that can reduce the
above cross-domain feature discrepancies. Extensive experiments have been
conducted in various cross-domain scenarios, including geographic location
variations and imaging mode variations, and the experimental results
demonstrate that the proposed method can significantly boost the domain
adaptability of segmentation networks for VHR RSIs. Our code is available at:
https://github.com/BOBrown/CCDA_LGFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegTAD: Precise Temporal Action Detection via Semantic Segmentation. (arXiv:2203.01542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01542">
<div class="article-summary-box-inner">
<span><p>Temporal action detection (TAD) is an important yet challenging task in video
analysis. Most existing works draw inspiration from image object detection and
tend to reformulate it as a proposal generation - classification problem.
However, there are two caveats with this paradigm. First, proposals are not
equipped with annotated labels, which have to be empirically compiled, thus the
information in the annotations is not necessarily precisely employed in the
model training process. Second, there are large variations in the temporal
scale of actions, and neglecting this fact may lead to deficient representation
in the video features. To address these issues and precisely model temporal
action detection, we formulate the task of temporal action detection in a novel
perspective of semantic segmentation. Owing to the 1-dimensional property of
TAD, we are able to convert the coarse-grained detection annotations to
fine-grained semantic segmentation annotations for free. We take advantage of
them to provide precise supervision so as to mitigate the impact induced by the
imprecise proposal labels. We propose an end-to-end framework SegTAD composed
of a 1D semantic segmentation network (1D-SSN) and a proposal detection network
(PDN).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the role of normalization and residual blocks for spiking neural networks. (arXiv:2203.01544v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01544">
<div class="article-summary-box-inner">
<span><p>Biologically inspired spiking neural networks (SNNs) are widely used to
realize ultralow-power energy consumption. However, deep SNNs are not easy to
train due to the excessive firing of spiking neurons in the hidden layers. To
tackle this problem, we propose a novel but simple normalization technique
called postsynaptic potential normalization. This normalization removes the
subtraction term from the standard normalization and uses the second raw moment
instead of the variance as the division term. The spike firing can be
controlled, enabling the training to proceed appropriating, by conducting this
simple normalization to the postsynaptic potential. The experimental results
show that SNNs with our normalization outperformed other models using other
normalizations. Furthermore, through the pre-activation residual blocks, the
proposed model can train with more than 100 layers without other special
techniques dedicated to SNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Addressing the Shape-Radiance Ambiguity in View-Dependent Radiance Fields. (arXiv:2203.01553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01553">
<div class="article-summary-box-inner">
<span><p>We present a method for handling view-dependent information in radiance
fields to help with convergence and quality of 3D reconstruction. Radiance
fields with view-dependence suffers from the so called shape-radiance
ambiguity, which can lead to incorrect geometry given a high angular resolution
of view-dependent colors. We propose the addition of a difference plane in
front of each camera, with the purpose of separating view-dependent and
Lambertian components during training. We also propose an additional step where
we train, but do not store, a low-resolution view-dependent function that helps
to isolate the surface if such a separation is proven difficult. These
additions have a small impact on performance and memory usage but enables
reconstruction of scenes with highly specular components without any other
explicit handling of view-dependence such as Spherical Harmonics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Ego-Motion Estimation Based on Multi-Layer Fusion of RGB and Inferred Depth. (arXiv:2203.01557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01557">
<div class="article-summary-box-inner">
<span><p>In existing self-supervised depth and ego-motion estimation methods,
ego-motion estimation is usually limited to only leveraging RGB information.
Recently, several methods have been proposed to further improve the accuracy of
self-supervised ego-motion estimation by fusing information from other
modalities, e.g., depth, acceleration, and angular velocity. However, they
rarely focus on how different fusion strategies affect performance. In this
paper, we investigate the effect of different fusion strategies for ego-motion
estimation and propose a new framework for self-supervised learning of depth
and ego-motion estimation, which performs ego-motion estimation by leveraging
RGB and inferred depth information in a Multi-Layer Fusion manner. As a result,
we have achieved state-of-the-art performance among learning-based methods on
the KITTI odometry benchmark. Detailed studies on the design choices of
leveraging inferred depth information and fusion strategies have also been
carried out, which clearly demonstrate the advantages of our proposed
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection. (arXiv:2203.01562v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01562">
<div class="article-summary-box-inner">
<span><p>Face Presentation Attack Detection (PAD) is an important measure to prevent
spoof attacks for face biometric systems. Many works based on Convolution
Neural Networks (CNNs) for face PAD formulate the problem as an image-level
binary classification task without considering the context. Alternatively,
Vision Transformers (ViT) using self-attention to attend the context of an
image become the mainstreams in face PAD. Inspired by ViT, we propose a
Video-based Transformer for face PAD (ViTransPAD) with short/long-range
spatio-temporal attention which can not only focus on local details with short
attention within a frame but also capture long-range dependencies over frames.
Instead of using coarse image patches with single-scale as in ViT, we propose
the Multi-scale Multi-Head Self-Attention (MsMHSA) architecture to accommodate
multi-scale patch partitions of Q, K, V feature maps to the heads of
transformer in a coarse-to-fine manner, which enables to learn a fine-grained
representation to perform pixel-level discrimination for face PAD. Due to lack
inductive biases of convolutions in pure transformers, we also introduce
convolutions to the proposed ViTransPAD to integrate the desirable properties
of CNNs by using convolution patch embedding and convolution projection. The
extensive experiments show the effectiveness of our proposed ViTransPAD with a
preferable accuracy-computation balance, which can serve as a new backbone for
face PAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occlusion-Aware Cost Constructor for Light Field Depth Estimation. (arXiv:2203.01576v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01576">
<div class="article-summary-box-inner">
<span><p>Matching cost construction is a key step in light field (LF) depth
estimation, but was rarely studied in the deep learning era. Recent deep
learning-based LF depth estimation methods construct matching cost by
sequentially shifting each sub-aperture image (SAI) with a series of predefined
offsets, which is complex and time-consuming. In this paper, we propose a
simple and fast cost constructor to construct matching cost for LF depth
estimation. Our cost constructor is composed by a series of convolutions with
specifically designed dilation rates. By applying our cost constructor to SAI
arrays, pixels under predefined disparities can be integrated and matching cost
can be constructed without using any shifting operation. More importantly, the
proposed cost constructor is occlusion-aware and can handle occlusions by
dynamically modulating pixels from different views. Based on the proposed cost
constructor, we develop a deep network for LF depth estimation. Our network
ranks first on the commonly used 4D LF benchmark in terms of the mean square
error (MSE), and achieves a faster running time than other state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01577">
<div class="article-summary-box-inner">
<span><p>We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 3M RGB-D egocentric video frames over 5000 sequences collected by 9
participants interacting with 1000 different object instances from 20
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities. We will release the dataset soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Universal Backward-Compatible Representation Learning. (arXiv:2203.01583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01583">
<div class="article-summary-box-inner">
<span><p>Conventional model upgrades for visual search systems require offline refresh
of gallery features by feeding gallery images into new models (dubbed as
"backfill"), which is time-consuming and expensive, especially in large-scale
applications. The task of backward-compatible representation learning is
therefore introduced to support backfill-free model upgrades, where the new
query features are interoperable with the old gallery features. Despite the
success, previous works only investigated a close-set training scenario (i.e.,
the new training set shares the same classes as the old one), and are limited
by more realistic and challenging open-set scenarios. To this end, we first
introduce a new problem of universal backward-compatible representation
learning, covering all possible data split in model upgrades. We further
propose a simple yet effective method, dubbed as Universal Backward-Compatible
Training (UniBCT) with a novel structural prototype refinement algorithm, to
learn compatible representations in all kinds of model upgrading benchmarks in
a unified manner. Comprehensive experiments on the large-scale face recognition
datasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Tailed Vision Transformer for Efficient Inference. (arXiv:2203.01587v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01587">
<div class="article-summary-box-inner">
<span><p>Recently, Vision Transformer (ViT) has achieved promising performance in
image recognition and gradually serves as a powerful backbone in various vision
tasks. To satisfy the sequential input of Transformer, the tail of ViT first
splits each image into a sequence of visual tokens with a fixed length. Then
the following self-attention layers constructs the global relationship between
tokens to produce useful representation for the downstream tasks. Empirically,
representing the image with more tokens leads to better performance, yet the
quadratic computational complexity of self-attention layer to the number of
tokens could seriously influence the efficiency of ViT's inference. For
computational reduction, a few pruning methods progressively prune
uninformative tokens in the Transformer encoder, while leaving the number of
tokens before the Transformer untouched. In fact, fewer tokens as the input for
the Transformer encoder can directly reduce the following computational cost.
In this spirit, we propose a Multi-Tailed Vision Transformer (MT-ViT) in the
paper. MT-ViT adopts multiple tails to produce visual sequences of different
lengths for the following Transformer encoder. A tail predictor is introduced
to decide which tail is the most efficient for the image to produce accurate
prediction. Both modules are optimized in an end-to-end fashion, with the
Gumbel-Softmax trick. Experiments on ImageNet-1K demonstrate that MT-ViT can
achieve a significant reduction on FLOPs with no degradation of the accuracy
and outperform other compared methods in both accuracy and FLOPs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Human Motion Prediction: A Survey. (arXiv:2203.01593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01593">
<div class="article-summary-box-inner">
<span><p>3D human motion prediction, predicting future poses from a given sequence, is
an issue of great significance and challenge in computer vision and machine
intelligence, which can help machines in understanding human behaviors. Due to
the increasing development and understanding of Deep Neural Networks (DNNs) and
the availability of large-scale human motion datasets, the human motion
prediction has been remarkably advanced with a surge of interest among academia
and industrial community. In this context, a comprehensive survey on 3D human
motion prediction is conducted for the purpose of retrospecting and analyzing
relevant works from existing released literature. In addition, a pertinent
taxonomy is constructed to categorize these existing approaches for 3D human
motion prediction. In this survey, relevant methods are categorized into three
categories: human pose representation, network structure design, and
\textit{prediction target}. We systematically review all relevant journal and
conference papers in the field of human motion prediction since 2015, which are
presented in detail based on proposed categorizations in this survey.
Furthermore, the outline for the public benchmark datasets, evaluation
criteria, and performance comparisons are respectively presented in this paper.
The limitations of the state-of-the-art methods are discussed as well, hoping
for paving the way for future explorations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Neural Framework for Image Caption Generation Using GRU-Based Attention Mechanism. (arXiv:2203.01594v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01594">
<div class="article-summary-box-inner">
<span><p>Image captioning is a fast-growing research field of computer vision and
natural language processing that involves creating text explanations for
images. This study aims to develop a system that uses a pre-trained
convolutional neural network (CNN) to extract features from an image,
integrates the features with an attention mechanism, and creates captions using
a recurrent neural network (RNN). To encode an image into a feature vector as
graphical attributes, we employed multiple pre-trained convolutional neural
networks. Following that, a language model known as GRU is chosen as the
decoder to construct the descriptive sentence. In order to increase
performance, we merge the Bahdanau attention model with GRU to allow learning
to be focused on a specific portion of the image. On the MSCOCO dataset, the
experimental results achieve competitive performance against state-of-the-art
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntax-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2203.01601v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01601">
<div class="article-summary-box-inner">
<span><p>Handwritten mathematical expression recognition (HMER) is a challenging task
that has many potential applications. Recent methods for HMER have achieved
outstanding performance with an encoder-decoder architecture. However, these
methods adhere to the paradigm that the prediction is made "from one character
to another", which inevitably yields prediction errors due to the complicated
structures of mathematical expressions or crabbed handwritings. In this paper,
we propose a simple and efficient method for HMER, which is the first to
incorporate syntax information into an encoder-decoder network. Specifically,
we present a set of grammar rules for converting the LaTeX markup sequence of
each expression into a parsing tree; then, we model the markup sequence
prediction as a tree traverse process with a deep neural network. In this way,
the proposed method can effectively describe the syntax context of expressions,
avoiding the structure prediction errors of HMER. Experiments on two benchmark
datasets demonstrate that our method achieves significantly better recognition
performance than prior arts. To further validate the effectiveness of our
method, we create a large-scale dataset consisting of 100k handwritten
mathematical expression images acquired from ten thousand writers. The source
code, new dataset, and pre-trained models of this work will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Path Planning for UAVs for Multi-Resolution Semantic Segmentation. (arXiv:2203.01642v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01642">
<div class="article-summary-box-inner">
<span><p>Efficient data collection methods play a major role in helping us better
understand the Earth and its ecosystems. In many applications, the usage of
unmanned aerial vehicles (UAVs) for monitoring and remote sensing is rapidly
gaining momentum due to their high mobility, low cost, and flexible deployment.
A key challenge is planning missions to maximize the value of acquired data in
large environments given flight time limitations. This is, for example,
relevant for monitoring agricultural fields. This paper addresses the problem
of adaptive path planning for accurate semantic segmentation of using UAVs. We
propose an online planning algorithm which adapts the UAV paths to obtain
high-resolution semantic segmentations necessary in areas with fine details as
they are detected in incoming images. This enables us to perform close
inspections at low altitudes only where required, without wasting energy on
exhaustive mapping at maximum image resolution. A key feature of our approach
is a new accuracy model for deep learning-based architectures that captures the
relationship between UAV altitude and semantic segmentation accuracy. We
evaluate our approach on different domains using real-world data, proving the
efficacy and generability of our solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Residual M-Net for Real Image Denoising. (arXiv:2203.01645v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01645">
<div class="article-summary-box-inner">
<span><p>Image restoration is a low-level vision task which is to restore degraded
images to noise-free images. With the success of deep neural networks, the
convolutional neural networks surpass the traditional restoration methods and
become the mainstream in the computer vision area. To advance the performanceof
denoising algorithms, we propose a blind real image denoising network (SRMNet)
by employing a hierarchical architecture improved from U-Net. Specifically, we
use a selective kernel with residual block on the hierarchical structure called
M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet
has competitive performance results on two synthetic and two real-world noisy
datasets in terms of quantitative metrics and visual quality. The source code
and pretrained model are available at
https://github.com/TentativeGitHub/SRMNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correlation-Aware Deep Tracking. (arXiv:2203.01666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01666">
<div class="article-summary-box-inner">
<span><p>Robustness and discrimination power are two fundamental requirements in
visual object tracking. In most tracking paradigms, we find that the features
extracted by the popular Siamese-like networks cannot fully discriminatively
model the tracked targets and distractor objects, hindering them from
simultaneously meeting these two requirements. While most methods focus on
designing robust correlation operations, we propose a novel target-dependent
feature network inspired by the self-/cross-attention scheme. In contrast to
the Siamese-like feature extraction, our network deeply embeds cross-image
feature correlation in multiple layers of the feature network. By extensively
matching the features of the two images through multiple layers, it is able to
suppress non-target features, resulting in instance-varying feature extraction.
The output features of the search image can be directly used for predicting
target locations without extra correlation step. Moreover, our model can be
flexibly pre-trained on abundant unpaired images, leading to notably faster
convergence than the existing methods. Extensive experiments show our method
achieves the state-of-the-art results while running at real-time. Our feature
networks also can be applied to existing tracking pipelines seamlessly to raise
the tracking performance. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translational Lung Imaging Analysis Through Disentangled Representations. (arXiv:2203.01668v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01668">
<div class="article-summary-box-inner">
<span><p>The development of new treatments often requires clinical trials with
translational animal models using (pre)-clinical imaging to characterize
inter-species pathological processes. Deep Learning (DL) models are commonly
used to automate retrieving relevant information from the images. Nevertheless,
they typically suffer from low generability and explainability as a product of
their entangled design, resulting in a specific DL model per animal model.
Consequently, it is not possible to take advantage of the high capacity of DL
to discover statistical relationships from inter-species images.
</p>
<p>To alleviate this problem, in this work, we present a model capable of
extracting disentangled information from images of different animal models and
the mechanisms that generate the images. Our method is located at the
intersection between deep generative models, disentanglement and causal
representation learning. It is optimized from images of pathological lung
infected by Tuberculosis and is able: a) from an input slice, infer its
position in a volume, the animal model to which it belongs, the damage present
and even more, generate a mask covering the whole lung (similar overlap
measures to the nnU-Net), b) generate realistic lung images by setting the
above variables and c) generate counterfactual images, namely, healthy versions
of a damaged input slice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained unsupervised anomaly segmentation. (arXiv:2203.01671v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01671">
<div class="article-summary-box-inner">
<span><p>Current unsupervised anomaly localization approaches rely on generative
models to learn the distribution of normal images, which is later used to
identify potential anomalous regions derived from errors on the reconstructed
images. However, a main limitation of nearly all prior literature is the need
of employing anomalous images to set a class-specific threshold to locate the
anomalies. This limits their usability in realistic scenarios, where only
normal data is typically accessible. Despite this major drawback, only a
handful of works have addressed this limitation, by integrating supervision on
attention maps during training. In this work, we propose a novel formulation
that does not require accessing images with abnormalities to define the
threshold. Furthermore, and in contrast to very recent work, the proposed
constraint is formulated in a more principled manner, leveraging well-known
knowledge in constrained optimization. In particular, the equality constraint
on the attention maps in prior work is replaced by an inequality constraint,
which allows more flexibility. In addition, to address the limitations of
penalty-based functions we employ an extension of the popular log-barrier
methods to handle the constraint. Last, we propose an alternative
regularization term that maximizes the Shannon entropy of the attention maps,
reducing the amount of hyperparameters of the proposed model. Comprehensive
experiments on two publicly available datasets on brain lesion segmentation
demonstrate that the proposed approach substantially outperforms relevant
literature, establishing new state-of-the-art results for unsupervised lesion
segmentation, and without the need to access anomalous images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Earth Mover's Distance for Visible Thermal Person Re-Identification. (arXiv:2203.01675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01675">
<div class="article-summary-box-inner">
<span><p>Visible thermal person re-identification (VT-ReID) suffers from the
inter-modality discrepancy and intra-identity variations. Distribution
alignment is a popular solution for VT-ReID, which, however, is usually
restricted to the influence of the intra-identity variations. In this paper, we
propose the Cross-Modality Earth Mover's Distance (CM-EMD) that can alleviate
the impact of the intra-identity variations during modality alignment. CM-EMD
selects an optimal transport strategy and assigns high weights to pairs that
have a smaller intra-identity variation. In this manner, the model will focus
on reducing the inter-modality discrepancy while paying less attention to
intra-identity variations, leading to a more effective modality alignment.
Moreover, we introduce two techniques to improve the advantage of CM-EMD.
First, the Cross-Modality Discrimination Learning (CM-DL) is designed to
overcome the discrimination degradation problem caused by modality alignment.
By reducing the ratio between intra-identity and inter-identity variances,
CM-DL leads the model to learn more discriminative representations. Second, we
construct the Multi-Granularity Structure (MGS), enabling us to align
modalities from both coarse- and fine-grained levels with the proposed CM-EMD.
Extensive experiments show the benefits of the proposed CM-EMD and its
auxiliary techniques (CM-DL and MGS). Our method achieves state-of-the-art
performance on two VT-ReID benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Source-to-target Gap for Cross-domain Person Re-Identification with Intermediate Domains. (arXiv:2203.01682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01682">
<div class="article-summary-box-inner">
<span><p>Cross-domain person re-identification (re-ID), such as unsupervised domain
adaptive (UDA) re-ID, aims to transfer the identity-discriminative knowledge
from the source to the target domain. Existing methods commonly consider the
source and target domains are isolated from each other, i.e., no intermediate
status is modeled between both domains. Directly transferring the knowledge
between two isolated domains can be very difficult, especially when the domain
gap is large. From a novel perspective, we assume these two domains are not
completely isolated, but can be connected through intermediate domains. Instead
of directly aligning the source and target domains against each other, we
propose to align the source and target domains against their intermediate
domains for a smooth knowledge transfer. To discover and utilize these
intermediate domains, we propose an Intermediate Domain Module (IDM) and a
Mirrors Generation Module (MGM). IDM has two functions: 1) it generates
multiple intermediate domains by mixing the hidden-layer features from source
and target domains and 2) it dynamically reduces the domain gap between the
source / target domain features and the intermediate domain features. While IDM
achieves good domain alignment, it introduces a side effect, i.e., the mix-up
operation may mix the identities into a new identity and lose the original
identities. To compensate this, MGM is introduced by mapping the features into
the IDM-generated intermediate domains without changing their original
identity. It allows to focus on minimizing domain variations to promote the
alignment between the source / target domain and intermediate domains, which
reinforces IDM into IDM++. We extensively evaluate our method under both the
UDA and domain generalization (DG) scenarios and observe that IDM++ yields
consistent performance improvement for cross-domain re-ID, achieving new state
of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relative distance matters for one-shot landmark detection. (arXiv:2203.01687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01687">
<div class="article-summary-box-inner">
<span><p>Contrastive learning based methods such as cascade comparing to detect (CC2D)
have shown great potential for one-shot medical landmark detection. However,
the important cue of relative distance between landmarks is ignored in CC2D. In
this paper, we upgrade CC2D to version II by incorporating a
simple-yet-effective relative distance bias in the training stage, which is
theoretically proved to encourage the encoder to project the relatively distant
landmarks to the embeddings with low similarities. As consequence, CC2Dv2 is
less possible to detect a wrong point far from the correct landmark.
Furthermore, we present an open-source, landmark-labeled dataset for the
measurement of biomechanical parameters of the lower extremity to alleviate the
burden of orthopedic surgeons. The effectiveness of CC2Dv2 is evaluated on the
public dataset from the ISBI 2015 Grand-Challenge of cephalometric radiographs
and our new dataset, which greatly outperforms the state-of-the-art one-shot
landmark detection approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction. (arXiv:2203.01703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01703">
<div class="article-summary-box-inner">
<span><p>Reconstructing 3D objects from 2D images is both challenging for our brains
and machine learning algorithms. To support this spatial reasoning task,
contextual information about the overall shape of an object is critical.
However, such information is not captured by established loss terms (e.g. Dice
loss). We propose to complement geometrical shape information by including
multi-scale topological features, such as connected components, cycles, and
voids, in the reconstruction loss. Our method calculates topological features
from 3D volumetric data based on cubical complexes and uses an optimal
transport distance to guide the reconstruction process. This topology-aware
loss is fully differentiable, computationally efficient, and can be added to
any neural network. We demonstrate the utility of our loss by incorporating it
into SHAPR, a model for predicting the 3D cell shape of individual cells based
on 2D microscopy images. Using a hybrid loss that leverages both geometrical
and topological information of single objects to assess their shape, we find
that topological information substantially improves the quality of
reconstructions, thus highlighting its ability to extract more relevant
features from image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Object Localization as Domain Adaption. (arXiv:2203.01714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01714">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization (WSOL) focuses on localizing objects
only with the supervision of image-level classification masks. Most previous
WSOL methods follow the classification activation map (CAM) that localizes
objects based on the classification structure with the multi-instance learning
(MIL) mechanism. However, the MIL mechanism makes CAM only activate
discriminative object parts rather than the whole object, weakening its
performance for localizing objects. To avoid this problem, this work provides a
novel perspective that models WSOL as a domain adaption (DA) task, where the
score estimator trained on the source/image domain is tested on the
target/pixel domain to locate objects. Under this perspective, a DA-WSOL
pipeline is designed to better engage DA approaches into WSOL to enhance
localization performance. It utilizes a proposed target sampling strategy to
select different types of target samples. Based on these types of target
samples, domain adaption localization (DAL) loss is elaborated. It aligns the
feature distribution between the two domains by DA and makes the estimator
perceive target domain cues by Universum regularization. Experiments show that
our pipeline outperforms SOTA methods on multi benchmarks. Code are released at
\url{https://github.com/zh460045050/DA-WSOL_CVPR2022}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting High-Quality GAN-Generated Face Images using Neural Networks. (arXiv:2203.01716v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01716">
<div class="article-summary-box-inner">
<span><p>In the past decades, the excessive use of the last-generation GAN (Generative
Adversarial Networks) models in computer vision has enabled the creation of
artificial face images that are visually indistinguishable from genuine ones.
These images are particularly used in adversarial settings to create fake
social media accounts and other fake online profiles. Such malicious activities
can negatively impact the trustworthiness of users identities. On the other
hand, the recent development of GAN models may create high-quality face images
without evidence of spatial artifacts. Therefore, reassembling uniform color
channel correlations is a challenging research problem. To face these
challenges, we need to develop efficient tools able to differentiate between
fake and authentic face images. In this chapter, we propose a new strategy to
differentiate GAN-generated images from authentic images by leveraging spectral
band discrepancies, focusing on artificial face image synthesis. In particular,
we enable the digital preservation of face images using the Cross-band
co-occurrence matrix and spatial co-occurrence matrix. Then, we implement these
techniques and feed them to a Convolutional Neural Networks (CNN) architecture
to identify the real from artificial faces. Additionally, we show that the
performance boost is particularly significant and achieves more than 92% in
different post-processing environments. Finally, we provide several research
observations demonstrating that this strategy improves a comparable detection
method based only on intra-band spatial co-occurrences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased Batch Normalization via Gaussian Process for Generalizable Person Re-Identification. (arXiv:2203.01723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01723">
<div class="article-summary-box-inner">
<span><p>Generalizable person re-identification aims to learn a model with only
several labeled source domains that can perform well on unseen domains. Without
access to the unseen domain, the feature statistics of the batch normalization
(BN) layer learned from a limited number of source domains is doubtlessly
biased for unseen domain. This would mislead the feature representation
learning for unseen domain and deteriorate the generalizaiton ability of the
model. In this paper, we propose a novel Debiased Batch Normalization via
Gaussian Process approach (GDNorm) for generalizable person re-identification,
which models the feature statistic estimation from BN layers as a dynamically
self-refining Gaussian process to alleviate the bias to unseen domain for
improving the generalization. Specifically, we establish a lightweight model
with multiple set of domain-specific BN layers to capture the discriminability
of individual source domain, and learn the corresponding parameters of the
domain-specific BN layers. These parameters of different source domains are
employed to deduce a Gaussian process. We randomly sample several paths from
this Gaussian process served as the BN estimations of potential new domains
outside of existing source domains, which can further optimize these learned
parameters from source domains, and estimate more accurate Gaussian process by
them in return, tending to real data distribution. Even without a large number
of source domains, GDNorm can still provide debiased BN estimation by using the
mean path of the Gaussian process, while maintaining low computational cost
during testing. Extensive experiments demonstrate that our GDNorm effectively
improves the generalization ability of the model on unseen domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensembles of Vision Transformers as a New Paradigm for Automated Classification in Ecology. (arXiv:2203.01726v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01726">
<div class="article-summary-box-inner">
<span><p>Monitoring biodiversity is paramount to manage and protect natural resources,
particularly in times of global change. Collecting images of organisms over
large temporal or spatial scales is a promising practice to monitor and study
biodiversity change of natural ecosystems, providing large amounts of data with
minimal interference with the environment. Deep learning models are currently
used to automate classification of organisms into taxonomic units. However,
imprecision in these classifiers introduce a measurement noise that is
difficult to control and can significantly hinder the analysis and
interpretation of data. In our study, we show that this limitation can be
overcome by ensembles of Data-efficient image Transformers (DeiTs), which
significantly outperform the previous state of the art (SOTA). We validate our
results on a large number of ecological imaging datasets of diverse origin, and
organisms of study ranging from plankton to insects, birds, dog breeds, animals
in the wild, and corals. On all the data sets we test, we achieve a new SOTA,
with a reduction of the error with respect to the previous SOTA ranging from
18.48% to 87.50%, depending on the data set, and often achieving performances
very close to perfect classification. The main reason why ensembles of DeiTs
perform better is not due to the single-model performance of DeiTs, but rather
to the fact that predictions by independent models have a smaller overlap, and
this maximizes the profit gained by ensembling. This positions DeiT ensembles
as the best candidate for image classification in biodiversity monitoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds. (arXiv:2203.01730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01730">
<div class="article-summary-box-inner">
<span><p>3D single object tracking (3D SOT) in LiDAR point clouds plays a crucial role
in autonomous driving. Current approaches all follow the Siamese paradigm based
on appearance matching. However, LiDAR point clouds are usually textureless and
incomplete, which hinders effective appearance matching. Besides, previous
methods greatly overlook the critical motion clues among targets. In this work,
beyond 3D Siamese tracking, we introduce a motion-centric paradigm to handle 3D
SOT from a new perspective. Following this paradigm, we propose a matching-free
two-stage tracker M^2-Track. At the 1^st-stage, M^2-Track localizes the target
within successive frames via motion transformation. Then it refines the target
box through motion-assisted shape completion at the 2^nd-stage. Extensive
experiments confirm that M^2-Track significantly outperforms previous
state-of-the-arts on three large-scale datasets while running at 57FPS (~8%,
~17%, and ~22%) precision gains on KITTI, NuScenes, and Waymo Open Dataset
respectively). Further analysis verifies each component's effectiveness and
shows the motion-centric paradigm's promising potential when combined with
appearance matching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-Identification. (arXiv:2203.01735v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01735">
<div class="article-summary-box-inner">
<span><p>RGB-infrared person re-identification is an emerging cross-modality
re-identification task, which is very challenging due to significant modality
discrepancy between RGB and infrared images. In this work, we propose a novel
modality-adaptive mixup and invariant decomposition (MID) approach for
RGB-infrared person re-identification towards learning modality-invariant and
discriminative representations. MID designs a modality-adaptive mixup scheme to
generate suitable mixed modality images between RGB and infrared images for
mitigating the inherent modality discrepancy at the pixel-level. It formulates
modality mixup procedure as Markov decision process, where an actor-critic
agent learns dynamical and local linear interpolation policy between different
regions of cross-modality images under a deep reinforcement learning framework.
Such policy guarantees modality-invariance in a more continuous latent space
and avoids manifold intrusion by the corrupted mixed modality samples.
Moreover, to further counter modality discrepancy and enforce invariant visual
semantics at the feature-level, MID employs modality-adaptive convolution
decomposition to disassemble a regular convolution layer into modality-specific
basis layers and a modality-shared coefficient layer. Extensive experimental
results on two challenging benchmarks demonstrate superior performance of MID
over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence. (arXiv:2203.01754v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01754">
<div class="article-summary-box-inner">
<span><p>We present a novel method to learn Personalized Implicit Neural Avatars
(PINA) from a short RGB-D sequence. This allows non-expert users to create a
detailed and personalized virtual copy of themselves, which can be animated
with realistic clothing deformations. PINA does not require complete scans, nor
does it require a prior learned from large datasets of clothed humans. Learning
a complete avatar in this setting is challenging, since only few depth
observations are available, which are noisy and incomplete (i.e.only partial
visibility of the body per frame). We propose a method to learn the shape and
non-rigid deformations via a pose-conditioned implicit surface and a
deformation field, defined in canonical space. This allows us to fuse all
partial observations into a single consistent canonical representation. Fusion
is formulated as a global optimization problem over the pose, shape and
skinning parameters. The method can learn neural avatars from real noisy RGB-D
sequences for a diverse set of people and clothing styles and these avatars can
be animated given unseen motion sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuroFluid: Fluid Dynamics Grounding with Particle-Driven Neural Radiance Fields. (arXiv:2203.01762v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01762">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown great potential for modeling the physical dynamics of
complex particle systems such as fluids (in Lagrangian descriptions). Existing
approaches, however, require the supervision of consecutive particle
properties, including positions and velocities. In this paper, we consider a
partially observable scenario known as fluid dynamics grounding, that is,
inferring the state transitions and interactions within the fluid particle
systems from sequential visual observations of the fluid surface. We propose a
differentiable two-stage network named NeuroFluid. Our approach consists of (i)
a particle-driven neural renderer, which involves fluid physical properties
into the volume rendering function, and (ii) a particle transition model
optimized to reduce the differences between the rendered and the observed
images. NeuroFluid provides the first solution to unsupervised learning of
particle-based fluid dynamics by training these two models jointly. It is shown
to reasonably estimate the underlying physics of fluids with different initial
shapes, viscosity, and densities. It is a potential alternative approach to
understanding complex fluid mechanics, such as turbulence, that are difficult
to model using traditional methods of mathematical physics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Quantum Neural Networks (RQNN) for Noisy Image Recognition. (arXiv:2203.01764v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01764">
<div class="article-summary-box-inner">
<span><p>Classical Random Neural Networks (RNNs) have demonstrated effective
applications in decision making, signal processing, and image recognition
tasks. However, their implementation has been limited to deterministic digital
systems that output probability distributions in lieu of stochastic behaviors
of random spiking signals. We introduce the novel class of supervised Random
Quantum Neural Networks (RQNNs) with a robust training strategy to better
exploit the random nature of the spiking RNN. The proposed RQNN employs hybrid
classical-quantum algorithms with superposition state and amplitude encoding
features, inspired by quantum information theory and the brain's
spatial-temporal stochastic spiking property of neuron information encoding. We
have extensively validated our proposed RQNN model, relying on hybrid
classical-quantum algorithms via the PennyLane Quantum simulator with a limited
number of \emph{qubits}. Experiments on the MNIST, FashionMNIST, and KMNIST
datasets demonstrate that the proposed RQNN model achieves an average
classification accuracy of $94.9\%$. Additionally, the experimental findings
illustrate the proposed RQNN's effectiveness and resilience in noisy settings,
with enhanced image classification accuracy when compared to the classical
counterparts (RNNs), classical Spiking Neural Networks (SNNs), and the
classical convolutional neural network (AlexNet). Furthermore, the RQNN can
deal with noise, which is useful for various applications, including computer
vision in NISQ devices. The PyTorch code (https://github.com/darthsimpus/RQN)
is made available on GitHub to reproduce the results reported in this
manuscript.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Click-based Interactive Video Object Segmentation. (arXiv:2203.01784v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01784">
<div class="article-summary-box-inner">
<span><p>While current methods for interactive Video Object Segmentation (iVOS) rely
on scribble-based interactions to generate precise object masks, we propose a
Click-based interactive Video Object Segmentation (CiVOS) framework to simplify
the required user workload as much as possible. CiVOS builds on de-coupled
modules reflecting user interaction and mask propagation. The interaction
module converts click-based interactions into an object mask, which is then
inferred to the remaining frames by the propagation module. Additional user
interactions allow for a refinement of the object mask. The approach is
extensively evaluated on the popular interactive~DAVIS dataset, but with an
inevitable adaptation of scribble-based interactions with click-based
counterparts. We consider several strategies for generating clicks during our
evaluation to reflect various user inputs and adjust the DAVIS performance
metric to perform a hardware-independent comparison. The presented CiVOS
pipeline achieves competitive results, although requiring a lower user
workload.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01785">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are able to memorize noisy labels easily with a softmax
cross-entropy (CE) loss. Previous studies attempted to address this issue focus
on incorporating a noise-robust loss function to the CE loss. However, the
memorization issue is alleviated but still remains due to the non-robust CE
loss. To address this issue, we focus on learning robust contrastive
representations of data on which the classifier is hard to memorize the label
noise under the CE loss. We propose a novel contrastive regularization function
to learn such representations over noisy data where label noise does not
dominate the representation learning. By theoretically investigating the
representations induced by the proposed regularization function, we reveal that
the learned representations keep information related to true labels and discard
information related to corrupted labels. Moreover, our theoretical results also
indicate that the learned representations are robust to the label noise. The
effectiveness of this method is demonstrated with experiments on benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Local-Global Relational Network for Facial Action Units Recognition and Facial Paralysis Estimation. (arXiv:2203.01800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01800">
<div class="article-summary-box-inner">
<span><p>Facial action units (AUs) refer to a unique set of facial muscle movements at
certain facial locations defined by the Facial Action Coding System (FACS),
which can be used for describing nearly any anatomically possible facial
expression. Many existing facial action units (AUs) recognition approaches
often enhance the AU representation by combining local features from multiple
independent branches, each corresponding to a different AU, which usually
neglect potential mutual assistance and exclusion relationship between AU
branches or simply employ a pre-defined and fixed knowledge-graph as a prior.
In addition, extracting features from pre-defined AU regions of regular shapes
limits the representation ability. In this paper, we propose a novel Adaptive
Local-Global Relational Network (ALGRNet) for facial AU recognition and apply
it to facial paralysis estimation. ALGRNet mainly consists of three novel
structures, i.e., an adaptive region learning module which learns the adaptive
muscle regions based on the detected landmarks, a skip-BiLSTM module which
models the latent mutual assistance and exclusion relationship among local AU
features, and a feature fusion\&amp;refining module which explores the
complementarity between local AUs and the whole face for the local AU
refinement. In order to evaluate our proposed method, we migrated ALGRNet to a
facial paralysis dataset which is collected and annotated by medical
professionals. Experiments on the BP4D and DISFA AU datasets show that the
proposed approach outperforms the state-of-the-art methods by a large margin.
Additionally, we also demonstrated the effectiveness of the proposed ALGRNet in
applications to facial paralysis estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intensity Image-based LiDAR Fiducial Marker System. (arXiv:2203.01816v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01816">
<div class="article-summary-box-inner">
<span><p>The fiducial marker system for LiDAR is crucial for the robotic application
but it is still rare to date. In this paper, an Intensity Image-based LiDAR
Fiducial Marker (IILFM) system is developed. This system only requires an
unstructured point cloud with intensity as the input and it has no restriction
on marker placement and shape. A marker detection method that locates the
predefined 3D fiducials in the point cloud through the intensity image is
introduced. Then, an approach that utilizes the detected 3D fiducials to
estimate the LiDAR 6-DOF pose that describes the transmission from the world
coordinate system to the LiDAR coordinate system is developed. Moreover, all
these processes run in real-time (approx 40 Hz on Livox Mid-40 and approx 143
Hz on VLP-16). Qualitative and quantitative experiments are conducted to
demonstrate that the proposed system has similar convenience and accuracy as
the conventional visual fiducial marker system. The codes and results are
available at: https://github.com/York-SDCNLab/IILFM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network. (arXiv:2203.01824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01824">
<div class="article-summary-box-inner">
<span><p>3D room layout estimation by a single panorama using deep neural networks has
made great progress. However, previous approaches can not obtain efficient
geometry awareness of room layout with the only latitude of boundaries or
horizon-depth. We present that using horizon-depth along with room height can
obtain omnidirectional-geometry awareness of room layout in both horizontal and
vertical directions. In addition, we propose a planar-geometry aware loss
function with normals and gradients of normals to supervise the planeness of
walls and turning of corners. We propose an efficient network, LGT-Net, for
room layout estimation, which contains a novel Transformer architecture called
SWG Transformer to model geometry relations. SWG Transformer consists of
(Shifted) Window Blocks and Global Blocks to combine the local and global
geometry relations. Moreover, we design a novel relative position embedding of
Transformer to enhance the spatial identification ability for the panorama.
Experiments show that the proposed LGT-Net achieves better performance than
current state-of-the-arts (SOTA) on benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors. (arXiv:2203.01825v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01825">
<div class="article-summary-box-inner">
<span><p>Transfer learning is a standard technique to transfer knowledge from one
domain to another. For applications in medical imaging, transfer from ImageNet
has become the de-facto approach, despite differences in the tasks and image
characteristics between the domains. However, it is unclear what factors
determine whether - and to what extent - transfer learning to the medical
domain is useful. The long-standing assumption that features from the source
domain get reused has recently been called into question. Through a series of
experiments on several medical image benchmark datasets, we explore the
relationship between transfer learning, data size, the capacity and inductive
bias of the model, as well as the distance between the source and target
domain. Our findings suggest that transfer learning is beneficial in most
cases, and we characterize the important role feature reuse plays in its
success.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STUN: Self-Teaching Uncertainty Estimation for Place Recognition. (arXiv:2203.01851v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01851">
<div class="article-summary-box-inner">
<span><p>Place recognition is key to Simultaneous Localization and Mapping (SLAM) and
spatial perception. However, a place recognition in the wild often suffers from
erroneous predictions due to image variations, e.g., changing viewpoints and
street appearance. Integrating uncertainty estimation into the life cycle of
place recognition is a promising method to mitigate the impact of variations on
place recognition performance. However, existing uncertainty estimation
approaches in this vein are either computationally inefficient (e.g., Monte
Carlo dropout) or at the cost of dropped accuracy. This paper proposes STUN, a
self-teaching framework that learns to simultaneously predict the place and
estimate the prediction uncertainty given an input image. To this end, we first
train a teacher net using a standard metric learning pipeline to produce
embedding priors. Then, supervised by the pretrained teacher net, a student net
with an additional variance branch is trained to finetune the embedding priors
and estimate the uncertainty sample by sample. During the online inference
phase, we only use the student net to generate a place prediction in
conjunction with the uncertainty. When compared with place recognition systems
that are ignorant to the uncertainty, our framework features the uncertainty
estimation for free without sacrificing any prediction accuracy. Our
experimental results on the large-scale Pittsburgh30k dataset demonstrate that
STUN outperforms the state-of-the-art methods in both recognition accuracy and
the quality of uncertainty estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Video Instance Segmentation via Tracklet Query and Proposal. (arXiv:2203.01853v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01853">
<div class="article-summary-box-inner">
<span><p>Video Instance Segmentation (VIS) aims to simultaneously classify, segment,
and track multiple object instances in videos. Recent clip-level VIS takes a
short video clip as input each time showing stronger performance than
frame-level VIS (tracking-by-segmentation), as more temporal context from
multiple frames is utilized. Yet, most clip-level methods are neither
end-to-end learnable nor real-time. These limitations are addressed by the
recent VIS transformer (VisTR) which performs VIS end-to-end within a clip.
However, VisTR suffers from long training time due to its frame-wise dense
attention. In addition, VisTR is not fully end-to-end learnable in multiple
video clips as it requires a hand-crafted data association to link instance
tracklets between successive clips. This paper proposes EfficientVIS, a fully
end-to-end framework with efficient training and inference. At the core are
tracklet query and tracklet proposal that associate and segment
regions-of-interest (RoIs) across space and time by an iterative query-video
interaction. We further propose a correspondence learning that makes tracklets
linking between clips end-to-end learnable. Compared to VisTR, EfficientVIS
requires 15x fewer training epochs while achieving state-of-the-art accuracy on
the YouTube-VIS benchmark. Meanwhile, our method enables whole video instance
segmentation in a single end-to-end pass without data association at all.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A study on the distribution of social biases in self-supervised learning visual models. (arXiv:2203.01854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01854">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are efficient at learning the data distribution if it is
sufficiently sampled. However, they can be strongly biased by non-relevant
factors implicitly incorporated in the training data. These include operational
biases, such as ineffective or uneven data sampling, but also ethical concerns,
as the social biases are implicitly present\textemdash even inadvertently, in
the training data or explicitly defined in unfair training schedules. In tasks
having impact on human processes, the learning of social biases may produce
discriminatory, unethical and untrustworthy consequences. It is often assumed
that social biases stem from supervised learning on labelled data, and thus,
Self-Supervised Learning (SSL) wrongly appears as an efficient and bias-free
solution, as it does not require labelled data. However, it was recently proven
that a popular SSL method also incorporates biases. In this paper, we study the
biases of a varied set of SSL visual models, trained using ImageNet data, using
a method and dataset designed by psychological experts to measure social
biases. We show that there is a correlation between the type of the SSL model
and the number of biases that it incorporates. Furthermore, the results also
suggest that this number does not strictly depend on the model's accuracy and
changes throughout the network. Finally, we conclude that a careful SSL model
selection process can reduce the number of social biases in the deployed model,
whilst keeping high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness and Adaptation to Hidden Factors of Variation. (arXiv:2203.01864v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01864">
<div class="article-summary-box-inner">
<span><p>We tackle here a specific, still not widely addressed aspect, of AI
robustness, which consists of seeking invariance / insensitivity of model
performance to hidden factors of variations in the data. Towards this end, we
employ a two step strategy that a) does unsupervised discovery, via generative
models, of sensitive factors that cause models to under-perform, and b)
intervenes models to make their performance invariant to these sensitive
factors' influence. We consider 3 separate interventions for robustness,
including: data augmentation, semantic consistency, and adversarial alignment.
We evaluate our method using metrics that measure trade offs between invariance
(insensitivity) and overall performance (utility) and show the benefits of our
method for 3 settings (unsupervised, semi-supervised and generalization).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LatentFormer: Multi-Agent Transformer-Based Interaction Modeling and Trajectory Prediction. (arXiv:2203.01880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01880">
<div class="article-summary-box-inner">
<span><p>Multi-agent trajectory prediction is a fundamental problem in autonomous
driving. The key challenges in prediction are accurately anticipating the
behavior of surrounding agents and understanding the scene context. To address
these problems, we propose LatentFormer, a transformer-based model for
predicting future vehicle trajectories. The proposed method leverages a novel
technique for modeling interactions among dynamic objects in the scene.
Contrary to many existing approaches which model cross-agent interactions
during the observation time, our method additionally exploits the future states
of the agents. This is accomplished using a hierarchical attention mechanism
where the evolving states of the agents autoregressively control the
contributions of past trajectories and scene encodings in the final prediction.
Furthermore, we propose a multi-resolution map encoding scheme that relies on a
vision transformer module to effectively capture both local and global scene
context to guide the generation of more admissible future trajectories. We
evaluate the proposed method on the nuScenes benchmark dataset and show that
our approach achieves state-of-the-art performance and improves upon trajectory
metrics by up to 40%. We further investigate the contributions of various
components of the proposed technique via extensive ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Failure Modes of Self-Supervised Learning. (arXiv:2203.01881v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01881">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning methods have shown impressive results in downstream
classification tasks. However, there is limited work in understanding their
failure models and interpreting the learned representations of these models. In
this paper, we tackle these issues and study the representation space of
self-supervised models by understanding the underlying reasons for
misclassifications in a downstream task. Over several state-of-the-art
self-supervised models including SimCLR, SwaV, MoCo V2 and BYOL, we observe
that representations of correctly classified samples have few discriminative
features with highly deviated values compared to other features. This is in a
clear contrast with representations of misclassified samples. We also observe
that noisy features in the representation space often correspond to spurious
attributes in images making the models less interpretable. Building on these
observations, we propose a sample-wise Self-Supervised Representation Quality
Score (or, Q-Score) that, without access to any label information, is able to
predict if a given sample is likely to be misclassified in the downstream task,
achieving an AUPRC of up to 0.90. Q-Score can also be used as a regularization
to remedy low-quality representations leading to 3.26% relative improvement in
accuracy of SimCLR on ImageNet-100. Moreover, we show that Q-Score
regularization increases representation sparsity, thus reducing noise and
improving interpretability through gradient heatmaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with Fuchs dystrophy. (arXiv:2203.01882v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01882">
<div class="article-summary-box-inner">
<span><p>To estimate the corneal endothelial parameters from specular microscopy
images depicting cornea guttata (Fuchs endothelial dystrophy), we propose a new
deep learning methodology that includes a novel attention mechanism named
feedback non-local attention (fNLA). Our approach first infers the cell edges,
then selects the cells that are well detected, and finally applies a
postprocessing method to correct mistakes and provide the binary segmentation
from which the corneal parameters are estimated (cell density [ECD],
coefficient of variation [CV], and hexagonality [HEX]). In this study, we
analyzed 1203 images acquired with a Topcon SP-1P microscope, 500 of which
contained guttae. Manual segmentation was performed in all images. We compared
the results of different networks (UNet, ResUNeXt, DenseUNets, UNet++) and
found that DenseUNets with fNLA provided the best performance, with a mean
absolute error of 23.16 [cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%]
in HEX, which was 3-6 times smaller than the error obtained by Topcon's
built-in software. Our approach handled the cells affected by guttae remarkably
well, detecting cell edges occluded by small guttae while discarding areas
covered by large guttae. fNLA made use of the local information, providing
sharper edges in guttae areas and better results in the selection of
well-detected cells. Overall, the proposed method obtained reliable and
accurate estimations in extremely challenging specular images with guttae,
being the first method in the literature to solve this problem adequately. Code
is available in our GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROCT-Net: A new ensemble deep convolutional model with improved spatial resolution learning for detecting common diseases from retinal OCT images. (arXiv:2203.01883v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01883">
<div class="article-summary-box-inner">
<span><p>Optical coherence tomography (OCT) imaging is a well-known technology for
visualizing retinal layers and helps ophthalmologists to detect possible
diseases. Accurate and early diagnosis of common retinal diseases can prevent
the patients from suffering critical damages to their vision. Computer-aided
diagnosis (CAD) systems can significantly assist ophthalmologists in improving
their examinations. This paper presents a new enhanced deep ensemble
convolutional neural network for detecting retinal diseases from OCT images.
Our model generates rich and multi-resolution features by employing the
learning architectures of two robust convolutional models. Spatial resolution
is a critical factor in medical images, especially the OCT images that contain
tiny essential points. To empower our model, we apply a new post-architecture
model to our ensemble model for enhancing spatial resolution learning without
increasing computational costs. The introduced post-architecture model can be
deployed to any feature extraction model to improve the utilization of the
feature map's spatial values. We have collected two open-source datasets for
our experiments to make our models capable of detecting six crucial retinal
diseases: Age-related Macular Degeneration (AMD), Central Serous Retinopathy
(CSR), Diabetic Retinopathy (DR), Choroidal Neovascularization (CNV), Diabetic
Macular Edema (DME), and Drusen alongside the normal cases. Our experiments on
two datasets and comparing our model with some other well-known deep
convolutional neural networks have proven that our architecture can increase
the classification accuracy up to 5%. We hope that our proposed methods create
the next step of CAD systems development and help future researches. The code
of this paper is shared at https://github.com/mr7495/OCT-classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCTrack: Temporal Contexts for Aerial Tracking. (arXiv:2203.01885v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01885">
<div class="article-summary-box-inner">
<span><p>Temporal contexts among consecutive frames are far from been fully utilized
in existing visual trackers. In this work, we present TCTrack, a comprehensive
framework to fully exploit temporal contexts for aerial tracking. The temporal
contexts are incorporated at \textbf{two levels}: the extraction of
\textbf{features} and the refinement of \textbf{similarity maps}. Specifically,
for feature extraction, an online temporally adaptive convolution is proposed
to enhance the spatial features using temporal information, which is achieved
by dynamically calibrating the convolution weights according to the previous
frames. For similarity map refinement, we propose an adaptive temporal
transformer, which first effectively encodes temporal knowledge in a
memory-efficient way, before the temporal knowledge is decoded for accurate
adjustment of the similarity map. TCTrack is effective and efficient:
evaluation on four aerial tracking benchmarks shows its impressive performance;
real-world UAV tests show its high speed of over 27 FPS on NVIDIA Jetson AGX
Xavier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance Segmentation for Autonomous Log Grasping in Forestry Operations. (arXiv:2203.01902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01902">
<div class="article-summary-box-inner">
<span><p>Wood logs picking is a challenging task to automate. Indeed, logs usually
come in cluttered configurations, randomly orientated and overlapping. Recent
work on log picking automation usually assume that the logs' pose is known,
with little consideration given to the actual perception problem. In this
paper, we squarely address the latter, using a data-driven approach. First, we
introduce a novel dataset, named TimberSeg 1.0, that is densely annotated,
i.e., that includes both bounding boxes and pixel-level mask annotations for
logs. This dataset comprises 220 images with 2500 individually segmented logs.
Using our dataset, we then compare three neural network architectures on the
task of individual logs detection and segmentation; two region-based methods
and one attention-based method. Unsurprisingly, our results show that
axis-aligned proposals, failing to take into account the directional nature of
logs, underperform with 19.03 mAP. A rotation-aware proposal method
significantly improve results to 31.83 mAP. More interestingly, a
Transformer-based approach, without any inductive bias on rotations,
outperformed the two others, achieving a mAP of 57.53 on our dataset. Our use
case demonstrates the limitations of region-based approaches for cluttered,
elongated objects. It also highlights the potential of attention-based methods
on this specific task, as they work directly at the pixel-level. These
encouraging results indicate that such a perception system could be used to
assist the operators on the short-term, or to fully automate log picking
operations in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision Aided Blockage Prediction in Real-World Millimeter Wave Deployments. (arXiv:2203.01907v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01907">
<div class="article-summary-box-inner">
<span><p>This paper provides the first real-world evaluation of using visual (RGB
camera) data and machine learning for proactively predicting millimeter wave
(mmWave) dynamic link blockages before they happen. Proactively predicting
line-of-sight (LOS) link blockages enables mmWave/sub-THz networks to make
proactive network management decisions, such as proactive beam switching and
hand-off) before a link failure happens. This can significantly enhance the
network reliability and latency while efficiently utilizing the wireless
resources. To evaluate this gain in reality, this paper (i) develops a computer
vision based solution that processes the visual data captured by a camera
installed at the infrastructure node and (ii) studies the feasibility of the
proposed solution based on the large-scale real-world dataset, DeepSense 6G,
that comprises multi-modal sensing and communication data. Based on the adopted
real-world dataset, the developed solution achieves $\approx 90\%$ accuracy in
predicting blockages happening within the future $0.1$s and $\approx 80\%$ for
blockages happening within $1$s, which highlights a promising solution for
mmWave/sub-THz communication networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields. (arXiv:2203.01913v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01913">
<div class="article-summary-box-inner">
<span><p>Thin, reflective objects such as forks and whisks are common in our daily
lives, but they are particularly challenging for robot perception because it is
hard to reconstruct them using commodity RGB-D cameras or multi-view stereo
techniques. While traditional pipelines struggle with objects like these,
Neural Radiance Fields (NeRFs) have recently been shown to be remarkably
effective for performing view synthesis on objects with thin structures or
reflective materials. In this paper we explore the use of NeRF as a new source
of supervision for robust robot vision systems. In particular, we demonstrate
that a NeRF representation of a scene can be used to train dense object
descriptors. We use an optimized NeRF to extract dense correspondences between
multiple views of an object, and then use these correspondences as training
data for learning a view-invariant representation of the object. NeRF's usage
of a density field allows us to reformulate the correspondence problem with a
novel distribution-of-depths formulation, as opposed to the conventional
approach of using a depth map. Dense correspondence models supervised with our
method significantly outperform off-the-shelf learned descriptors by 106%
(PCK@3px metric, more than doubling performance) and outperform our baseline
supervised with multi-view stereo by 29%. Furthermore, we demonstrate the
learned dense descriptors enable robots to perform accurate 6-degree of freedom
(6-DoF) pick and place of thin and reflective objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Playable Environments: Video Manipulation in Space and Time. (arXiv:2203.01914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01914">
<div class="article-summary-box-inner">
<span><p>We present Playable Environments - a new representation for interactive video
generation and manipulation in space and time. With a single image at inference
time, our novel framework allows the user to move objects in 3D while
generating a video by providing a sequence of desired actions. The actions are
learnt in an unsupervised manner. The camera can be controlled to get the
desired viewpoint. Our method builds an environment state for each frame, which
can be manipulated by our proposed action module and decoded back to the image
space with volumetric rendering. To support diverse appearances of objects, we
extend neural radiance fields with style-based modulation. Our method trains on
a collection of various monocular videos requiring only the estimated camera
parameters and 2D object locations. To set a challenging benchmark, we
introduce two large scale video datasets with significant camera movements. As
evidenced by our experiments, playable environments enable several creative
applications not attainable by prior video synthesis works, including playable
3D video generation, stylization and manipulation. Further details, code and
examples are available at
https://willi-menapace.github.io/playable-environments-website
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the limited performance of a deep-learning-based SPECT denoising approach: An observer-study-based characterization. (arXiv:2203.01918v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01918">
<div class="article-summary-box-inner">
<span><p>Multiple objective assessment of image-quality-based studies have reported
that several deep-learning-based denoising methods show limited performance on
signal-detection tasks. Our goal was to investigate the reasons for this
limited performance. To achieve this goal, we conducted a task-based
characterization of a DL-based denoising approach for individual signal
properties. We conducted this study in the context of evaluating a DL-based
approach for denoising SPECT images. The training data consisted of signals of
different sizes and shapes within a clustered-lumpy background, imaged with a
2D parallel-hole-collimator SPECT system. The projections were generated at
normal and 20% low count level, both of which were reconstructed using an OSEM
algorithm. A CNN-based denoiser was trained to process the low-count images.
The performance of this CNN was characterized for five different signal sizes
and four different SBR by designing each evaluation as an SKE/BKS
signal-detection task. Performance on this task was evaluated using an
anthropomorphic CHO. As in previous studies, we observed that the DL-based
denoising method did not improve performance on signal-detection tasks.
Evaluation using the idea of observer-study-based characterization demonstrated
that the DL-based denoising approach did not improve performance on the
signal-detection task for any of the signal types. Overall, these results
provide new insights on the performance of the DL-based denoising approach as a
function of signal size and contrast. More generally, the observer study-based
characterization provides a mechanism to evaluate the sensitivity of the method
to specific object properties and may be explored as analogous to
characterizations such as modulation transfer function for linear systems.
Finally, this work underscores the need for objective task-based evaluation of
DL-based denoising approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification. (arXiv:2203.01921v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01921">
<div class="article-summary-box-inner">
<span><p>Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue
micro-architecture, which can, in turn, be used to reconstruct tissue
microstructure and white matter pathways. The accuracy of such tasks is
hampered by the low signal-to-noise ratio in dMRI. Today, the noise is
characterized mainly by visual inspection of residual maps and estimated
standard deviation. However, it is hard to estimate the impact of noise on
downstream tasks based only on such qualitative assessments. To address this
issue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for
quantitative image quality analysis in the absence of a ground truth reference
image. NUQ uses a recent Bayesian formulation of dMRI models to estimate the
uncertainty of microstructural measures. Specifically, NUQ uses the maximum
mean discrepancy metric to compute a pooled quality score by comparing samples
drawn from the posterior distribution of the microstructure measures. We show
that NUQ allows a fine-grained analysis of noise, capturing details that are
visually imperceptible. We perform qualitative and quantitative comparisons on
real datasets, showing that NUQ generates consistent scores across different
denoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics
and controls, we quantify the substantial impact of denoising on group
differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Intelligence: Tasks, Representation Learning, and Large Models. (arXiv:2203.01922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01922">
<div class="article-summary-box-inner">
<span><p>This paper presents a comprehensive survey of vision-language (VL)
intelligence from the perspective of time. This survey is inspired by the
remarkable progress in both computer vision and natural language processing,
and recent trends shifting from single modality processing to multiple modality
comprehension. We summarize the development in this field into three time
periods, namely task-specific methods, vision-language pre-training (VLP)
methods, and larger models empowered by large-scale weakly-labeled data. We
first take some common VL tasks as examples to introduce the development of
task-specific methods. Then we focus on VLP methods and comprehensively review
key components of the model structures and training methods. After that, we
show how recent work utilizes large-scale raw image-text data to learn
language-aligned visual representations that generalize better on zero or few
shot learning tasks. Finally, we discuss some potential future trends towards
modality cooperation, unified representation, and knowledge incorporation. We
believe that this review will be of help for researchers and practitioners of
AI and ML, especially those interested in computer vision and natural language
processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovering 3D Human Mesh from Monocular Images: A Survey. (arXiv:2203.01923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01923">
<div class="article-summary-box-inner">
<span><p>Estimating human pose and shape from monocular images is a long-standing
problem in computer vision. Since the release of statistical body models, 3D
human mesh recovery has been drawing broader attention. With the same goal of
obtaining well-aligned and physically plausible mesh results, two paradigms
have been developed to overcome challenges in the 2D-to-3D lifting process: i)
an optimization-based paradigm, where different data terms and regularization
terms are exploited as optimization objectives; and ii) a regression-based
paradigm, where deep learning techniques are embraced to solve the problem in
an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving
the quality of 3D mesh labels for a wide range of datasets. Though remarkable
progress has been achieved in the past decade, the task is still challenging
due to flexible body motions, diverse appearances, complex environments, and
insufficient in-the-wild annotations. To the best of our knowledge, this is the
first survey to focus on the task of monocular 3D human mesh recovery. We start
with the introduction of body models, and then introduce recovery frameworks
and training objectives by providing in-depth analyses of their strengths and
weaknesses. We also summarize datasets, evaluation metrics, and benchmark
results. Open issues and future directions are discussed in the end, hoping to
motivate researchers and facilitate their research in this area. A regularly
updated project page can be found at https://github.com/tinatiansjz/hmr-survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Only Model Inversion Attacks via Boundary Repulsion. (arXiv:2203.01925v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01925">
<div class="article-summary-box-inner">
<span><p>Recent studies show that the state-of-the-art deep neural networks are
vulnerable to model inversion attacks, in which access to a model is abused to
reconstruct private training data of any given target class. Existing attacks
rely on having access to either the complete target model (whitebox) or the
model's soft-labels (blackbox). However, no prior work has been done in the
harder but more practical scenario, in which the attacker only has access to
the model's predicted label, without a confidence measure. In this paper, we
introduce an algorithm, Boundary-Repelling Model Inversion (BREP-MI), to invert
private training data using only the target model's predicted labels. The key
idea of our algorithm is to evaluate the model's predicted labels over a sphere
and then estimate the direction to reach the target class's centroid. Using the
example of face recognition, we show that the images reconstructed by BREP-MI
successfully reproduce the semantics of the private training data for various
datasets and target model architectures. We compare BREP-MI with the
state-of-the-art whitebox and blackbox model inversion attacks and the results
show that despite assuming less knowledge about the target model, BREP-MI
outperforms the blackbox attack and achieves comparable results to the whitebox
attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation. (arXiv:2203.01929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01929">
<div class="article-summary-box-inner">
<span><p>This paper studies the complex task of simultaneous multi-object 3D
reconstruction, 6D pose and size estimation from a single-view RGB-D
observation. In contrast to instance-level pose estimation, we focus on a more
challenging problem where CAD models are not available at inference time.
Existing approaches mainly follow a complex multi-stage pipeline which first
localizes and detects each object instance in the image and then regresses to
either their 3D meshes or 6D poses. These approaches suffer from
high-computational cost and low performance in complex multi-object scenarios,
where occlusions can be present. Hence, we present a simple one-stage approach
to predict both the 3D shape and estimate the 6D pose and size jointly in a
bounding-box free manner. In particular, our method treats object instances as
spatial centers where each center denotes the complete shape of an object along
with its 6D pose and size. Through this per-pixel representation, our approach
can reconstruct in real-time (40 FPS) multiple novel object instances and
predict their 6D pose and sizes in a single-forward pass. Through extensive
experiments, we demonstrate that our approach significantly outperforms all
shape completion and categorical 6D pose and size estimation baselines on
multi-object ShapeNet and NOCS datasets respectively with a 12.6% absolute
improvement in mAP for 6D pose for novel real-world object instances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the mathematics of beauty: beautiful images. (arXiv:1705.08244v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1705.08244">
<div class="article-summary-box-inner">
<span><p>In this paper, we will study the simplest kind of beauty which can be found
in simple visual patterns. The proposed approach shows that aesthetically
appealing patterns deliver higher amount of information over multiple levels in
comparison with less aesthetically appealing patterns when the same amount of
energy is used. The proposed approach is used to classify aesthetically
appealing patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Robustness of Visual Question Answering Models. (arXiv:1912.01452v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.01452">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been playing an essential role in the task of
Visual Question Answering (VQA). Until recently, their accuracy has been the
main focus of research. Now there is a trend toward assessing the robustness of
these models against adversarial attacks by evaluating the accuracy of these
models under increasing levels of noisiness in the inputs of VQA models. In
VQA, the attack can target the image and/or the proposed query question, dubbed
main question, and yet there is a lack of proper analysis of this aspect of
VQA. In this work, we propose a new method that uses semantically related
questions, dubbed basic questions, acting as noise to evaluate the robustness
of VQA models. We hypothesize that as the similarity of a basic question to the
main question decreases, the level of noise increases. To generate a reasonable
noise level for a given main question, we rank a pool of basic questions based
on their similarity with this main question. We cast this ranking problem as a
LASSO optimization problem. We also propose a novel robustness measure Rscore
and two large-scale basic question datasets in order to standardize robustness
analysis of VQA models. The experimental results demonstrate that the proposed
evaluation method is able to effectively analyze the robustness of VQA models.
To foster the VQA research, we will publish our proposed datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radon cumulative distribution transform subspace modeling for image classification. (arXiv:2004.03669v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03669">
<div class="article-summary-box-inner">
<span><p>We present a new supervised image classification method applicable to a broad
class of image deformation models. The method makes use of the previously
described Radon Cumulative Distribution Transform (R-CDT) for image data, whose
mathematical properties are exploited to express the image data in a form that
is more suitable for machine learning. While certain operations such as
translation, scaling, and higher-order transformations are challenging to model
in native image space, we show the R-CDT can capture some of these variations
and thus render the associated image classification problems easier to solve.
The method -- utilizing a nearest-subspace algorithm in R-CDT space -- is
simple to implement, non-iterative, has no hyper-parameters to tune, is
computationally efficient, label efficient, and provides competitive accuracies
to state-of-the-art neural networks for many types of classification problems.
In addition to the test accuracy performances, we show improvements (with
respect to neural network-based methods) in terms of computational efficiency
(it can be implemented without the use of GPUs), number of training samples
needed for training, as well as out-of-distribution generalization. The Python
code for reproducing our results is available at
https://github.com/rohdelab/rcdt_ns_classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynthMorph: learning contrast-invariant registration without acquired images. (arXiv:2004.10282v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.10282">
<div class="article-summary-box-inner">
<span><p>We introduce a strategy for learning image registration without acquired
imaging data, producing powerful networks agnostic to contrast introduced by
magnetic resonance imaging (MRI). While classical registration methods
accurately estimate the spatial correspondence between images, they solve an
optimization problem for every new image pair. Learning-based techniques are
fast at test time but limited to registering images with contrasts and
geometric content similar to those seen during training. We propose to remove
this dependency on training data by leveraging a generative strategy for
diverse synthetic label maps and images that exposes networks to a wide range
of variability, forcing them to learn more invariant features. This approach
results in powerful networks that accurately generalize to a broad array of MRI
contrasts. We present extensive experiments with a focus on 3D neuroimaging,
showing that this strategy enables robust and accurate registration of
arbitrary MRI contrasts even if the target contrast is not seen by the networks
during training. We demonstrate registration accuracy surpassing the state of
the art both within and across contrasts, using a single model. Critically,
training on arbitrary shapes synthesized from noise distributions results in
competitive performance, removing the dependency on acquired data of any kind.
Additionally, since anatomical label maps are often available for the anatomy
of interest, we show that synthesizing images from these dramatically boosts
performance, while still avoiding the need for real intensity images. Our code
is available at https://w3id.org/synthmorph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Localization Networks for Language-based Moment Localization. (arXiv:2102.01282v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01282">
<div class="article-summary-box-inner">
<span><p>This paper targets the task of language-based video moment localization. The
language-based setting of this task allows for an open set of target
activities, resulting in a large variation of the temporal lengths of video
moments. Most existing methods prefer to first sample sufficient candidate
moments with various temporal lengths, and then match them with the given query
to determine the target moment. However, candidate moments generated with a
fixed temporal granularity may be suboptimal to handle the large variation in
moment lengths. To this end, we propose a novel multi-stage Progressive
Localization Network (PLN) which progressively localizes the target moment in a
coarse-to-fine manner. Specifically, each stage of PLN has a localization
branch, and focuses on candidate moments that are generated with a specific
temporal granularity. The temporal granularities of candidate moments are
different across the stages. Moreover, we devise a conditional feature
manipulation module and an upsampling connection to bridge the multiple
localization branches. In this fashion, the later stages are able to absorb the
previously learned information, thus facilitating the more fine-grained
localization. Extensive experiments on three public datasets demonstrate the
effectiveness of our proposed PLN for language-based moment localization,
especially for localizing short moments in long videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04053">
<div class="article-summary-box-inner">
<span><p>Real-world large-scale medical image analysis (MIA) datasets havethree
challenges: 1) they contain noisy-labelled samples that affect training
con-vergence and generalisation, 2) they usually have an imbalanced
distribution ofsamples per class, and 3) they normally comprise a multi-label
problem, wheresamples can have multiple diagnoses. Current approaches are
commonly trainedto solve a subset of those problems, but we are unaware of
methods that ad-dress the three problems simultaneously. In this paper, we
propose a new trainingmodule called Non-Volatile Unbiased Memory (NVUM), which
non-volatilitystores running average of model logits for a new regularization
loss on noisymulti-label problem. We further unbias the classification
prediction in NVUMupdate for imbalanced learning problem. We run extensive
experiments to eval-uate NVUM on new benchmarks proposed by this paper, where
training is per-formed on noisy multi-label imbalanced chest X-ray (CXR)
training sets, formedby Chest-Xray14 and CheXpert, and the testing is performed
on the clean multi-label CXR datasets OpenI and PadChest. Our method
outperforms previous state-of-the-art CXR classifiers and previous methods that
can deal with noisy labelson all evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?. (arXiv:2104.09425v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09425">
<div class="article-summary-box-inner">
<span><p>While additional training data improves the robustness of deep neural
networks against adversarial examples, it presents the challenge of curating a
large number of specific real-world samples. We circumvent this challenge by
using additional data from proxy distributions learned by advanced generative
models. We first seek to formally understand the transfer of robustness from
classifiers trained on proxy distributions to the real data distribution. We
prove that the difference between the robustness of a classifier on the two
distributions is upper bounded by the conditional Wasserstein distance between
them. Next we use proxy distributions to significantly improve the performance
of adversarial training on five different datasets. For example, we improve
robust accuracy by up to 7.5% and 6.7% in $\ell_{\infty}$ and $\ell_2$ threat
model over baselines that are not using proxy distributions on the CIFAR-10
dataset. We also improve certified robust accuracy by 7.6% on the CIFAR-10
dataset. We further demonstrate that different generative models bring a
disparate improvement in the performance in robust training. We propose a
robust discrimination approach to characterize the impact of individual
generative models and further provide a deeper understanding of why current
state-of-the-art in diffusion-based generative models are a better choice for
proxy distribution than generative adversarial networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterogeneous-Agent Trajectory Forecasting Incorporating Class Uncertainty. (arXiv:2104.12446v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12446">
<div class="article-summary-box-inner">
<span><p>Reasoning about the future behavior of other agents is critical to safe robot
navigation. The multiplicity of plausible futures is further amplified by the
uncertainty inherent to agent state estimation from data, including positions,
velocities, and semantic class. Forecasting methods, however, typically neglect
class uncertainty, conditioning instead only on the agent's most likely class,
even though perception models often return full class distributions. To exploit
this information, we present HAICU, a method for heterogeneous-agent trajectory
forecasting that explicitly incorporates agents' class probabilities. We
additionally present PUP, a new challenging real-world autonomous driving
dataset, to investigate the impact of Perceptual Uncertainty in Prediction. It
contains challenging crowded scenes with unfiltered agent class probabilities
that reflect the long-tail of current state-of-the-art perception systems. We
demonstrate that incorporating class probabilities in trajectory forecasting
significantly improves performance in the face of uncertainty, and enables new
forecasting capabilities such as counterfactual predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Partial Multi-view Learning. (arXiv:2105.02046v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02046">
<div class="article-summary-box-inner">
<span><p>It is often the case that data are with multiple views in real-world
applications. Fully exploring the information of each view is significant for
making data more representative. However, real data tend to suffer from
arbitrary view-missing due to various failures in data collection and
pre-processing. Besides, as obtaining large-scale labeled data is laborious and
expensive, the collected training samples in the target task may be scarce as
well. The co-existence of these two problems makes it more challenging to
achieve the pattern classification task. Currently, to our best knowledge, few
appropriate methods can well-handle these two problems simultaneously. To draw
more attention from the community to this challenge, we present a new task in
this paper called Few-shot Partial Multi-view Learning (FPML), aiming to
alleviate the view-missing effects in the low-data regime. The challenges of
this task are twofold: (1) under the interference of the missing views, it is
difficult to overcome the negative impact brought by data scarcity; (2) the
limited number of data exacerbates information scarcity, thereby making it
harder to address the view-missing problem. In this paper, we propose a novel
method called Unified Gaussian Dense-anchoring (UGD) for this task. We propose
to learn the unified dense Gaussian anchors for each training sample.
Therefore, the incomplete multi-view data can be densely anchored into a
unified representation space, where data scarcity and view missing are both
relieved by the unified dense anchor representations. In particular, we also
extend our FPML to the multimodal and the cross-domain scenario, and validate
our UGD method on them. The extensive experiments firmly demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse to Dense Dynamic 3D Facial Expression Generation. (arXiv:2105.07463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07463">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a solution to the task of generating dynamic 3D
facial expressions from a neutral 3D face and an expression label. This
involves solving two sub-problems: (i)modeling the temporal dynamics of
expressions, and (ii) deforming the neutral mesh to obtain the expressive
counterpart. We represent the temporal evolution of expressions using the
motion of a sparse set of 3D landmarks that we learn to generate by training a
manifold-valued GAN (Motion3DGAN). To better encode the expression-induced
deformation and disentangle it from the identity information, the generated
motion is represented as per-frame displacement from a neutral configuration.
To generate the expressive meshes, we train a Sparse2Dense mesh Decoder
(S2D-Dec) that maps the landmark displacements to a dense, per-vertex
displacement. This allows us to learn how the motion of a sparse set of
landmarks influences the deformation of the overall face surface, independently
from the identity. Experimental results on the CoMA and D3DFACS datasets show
that our solution brings significant improvements with respect to previous
solutions in terms of both dynamic expression generation and mesh
reconstruction, while retaining good generalization to unseen data. The code
and the pretrained model will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10375">
<div class="article-summary-box-inner">
<span><p>Face recognition has achieved significant progress in deep learning era due
to the ultra-large-scale and welllabeled datasets. However, training on the
outsize datasets is time-consuming and takes up a lot of hardware resource.
Therefore, designing an efficient training approach is indispensable. The heavy
computational and memory costs mainly result from the million-level
dimensionality of thefully connected (FC) layer. To this end, we propose a
novel training approach, termed Faster Face Classification (F2C), to alleviate
time and cost without sacrificing the performance. This method adopts Dynamic
Class Pool (DCP) for storing and updating the identities features dynamically,
which could be regarded as a substitute for the FC layer. DCP is efficiently
time-saving and cost-saving, as its smaller size with the independence from the
whole face identities together. We further validate the proposed F2C method
across several face benchmarks and private datasets, and display comparable
results, meanwhile the speed is faster than state-of-the-art FC-based methods
in terms of recognition accuracy and hardware costs. Moreover, our method is
further improved by a well-designed dual data loader including indentity-based
and instancebased loaders, which makes it more efficient for the updating DCP
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch Tracking-based Online Tensor Ring Completion for Streaming Visual Data. (arXiv:2105.14620v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14620">
<div class="article-summary-box-inner">
<span><p>Tensor completion is the problem of estimating the missing entries of a
partially observed tensor by exploiting its low-rank structure. In streaming
applications where the frames arrive sequentially such as video completion, the
missing entries of the tensor need to be dynamically recovered in an online
fashion. Traditional online completion algorithms treat the entire visual data
as a tensor, which may not work satisfactorily when there is a big change in
the tensor subspace along the temporal dimension, such as due to strong motion
across the video frames. In this paper, we develop a novel patch tracking-based
online tensor completion framework for streaming data. Each incoming tensor is
extracted into small patches, and similar patches are tracked along the
temporal domain. We propose a new patch tracking strategy that can accurately
and efficiently track the patches with missing data. Further, a new online
tensor ring completion method is proposed which can efficiently and accurately
update the latent core tensors and complete the missing entries of the tracked
patches. Extensive experimental results demonstrate the superior performance of
the proposed algorithms compared with both online and offline state-of-the-art
tensor completion methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Improving Adversarial Transferability of Vision Transformers. (arXiv:2106.04169v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04169">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) process input images as sequences of patches via
self-attention; a radically different architecture than convolutional neural
networks (CNNs). This makes it interesting to study the adversarial feature
space of ViT models and their transferability. In particular, we observe that
adversarial patterns found via conventional adversarial attacks show very
\emph{low} black-box transferability even for large ViT models. We show that
this phenomenon is only due to the sub-optimal attack procedures that do not
leverage the true representation potential of ViTs. A deep ViT is composed of
multiple blocks, with a consistent architecture comprising of self-attention
and feed-forward layers, where each block is capable of independently producing
a class token. Formulating an attack using only the last class token
(conventional approach) does not directly leverage the discriminative
information stored in the earlier tokens, leading to poor adversarial
transferability of ViTs. Using the compositional nature of ViT models, we
enhance transferability of existing attacks by introducing two novel strategies
specific to the architecture of ViT models. (i) Self-Ensemble: We propose a
method to find multiple discriminative pathways by dissecting a single ViT
model into an ensemble of networks. This allows explicitly utilizing
class-specific information at each ViT block. (ii) Token Refinement: We then
propose to refine the tokens to further enhance the discriminative capacity at
each block of ViT. Our token refinement systematically combines the class
tokens with structural information preserved within the patch tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Learns to Teach: Knowledge Distillation with Meta Learning. (arXiv:2106.04570v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04570">
<div class="article-summary-box-inner">
<span><p>We present Knowledge Distillation with Meta Learning (MetaDistil), a simple
yet effective alternative to traditional knowledge distillation (KD) methods
where the teacher model is fixed during training. We show the teacher network
can learn to better transfer knowledge to the student network (i.e., learning
to teach) with the feedback from the performance of the distilled student
network in a meta learning framework. Moreover, we introduce a pilot update
mechanism to improve the alignment between the inner-learner and meta-learner
in meta learning algorithms that focus on an improved inner-learner.
Experiments on various benchmarks show that MetaDistil can yield significant
improvements compared with traditional KD algorithms and is less sensitive to
the choice of different student capacity and hyperparameters, facilitating the
use of KD on different tasks and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST++: Make Self-training Work Better for Semi-supervised Semantic Segmentation. (arXiv:2106.05095v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05095">
<div class="article-summary-box-inner">
<span><p>Self-training via pseudo labeling is a conventional, simple, and popular
pipeline to leverage unlabeled data. In this work, we first construct a strong
baseline of self-training (namely ST) for semi-supervised semantic segmentation
via injecting strong data augmentations (SDA) on unlabeled images to alleviate
overfitting noisy labels as well as decouple similar predictions between the
teacher and student. With this simple mechanism, our ST outperforms all
existing methods without any bells and whistles, e.g., iterative re-training.
Inspired by the impressive results, we thoroughly investigate the SDA and
provide some empirical analysis. Nevertheless, incorrect pseudo labels are
still prone to accumulate and degrade the performance. To this end, we further
propose an advanced self-training framework (namely ST++), that performs
selective re-training via prioritizing reliable unlabeled images based on
holistic prediction-level stability. Concretely, several model checkpoints are
saved in the first stage supervised training, and the discrepancy of their
predictions on the unlabeled image serves as a measurement for reliability. Our
image-level selection offers holistic contextual information for learning. We
demonstrate that it is more suitable for segmentation than common pixel-wise
selection. As a result, ST++ further boosts the performance of our ST. Code is
available at https://github.com/LiheYoung/ST-PlusPlus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are conditional GANs explicitly conditional?. (arXiv:2106.15011v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15011">
<div class="article-summary-box-inner">
<span><p>This paper proposes two important contributions for conditional Generative
Adversarial Networks (cGANs) to improve the wide variety of applications that
exploit this architecture. The first main contribution is an analysis of cGANs
to show that they are not explicitly conditional. In particular, it will be
shown that the discriminator and subsequently the cGAN does not automatically
learn the conditionality between inputs. The second contribution is a new
method, called a contrario cGAN, that explicitly models conditionality for both
parts of the adversarial architecture via a novel a contrario loss that
involves training the discriminator to learn unconditional (adverse) examples.
This leads to a novel type of data augmentation approach for GANs (a contrario
learning) which allows to restrict the search space of the generator to
conditional outputs using adverse examples. Extensive experimentation is
carried out to evaluate the conditionality of the discriminator by proposing a
probability distribution analysis. Comparisons with the cGAN architecture for
different applications show significant improvements in performance on well
known datasets including, semantic image synthesis, image segmentation,
monocular depth prediction and "single label"-to-image using different metrics
including Fr\'echet Inception Distance (FID), mean Intersection over Union
(mIoU), Root Mean Square Error log (RMSE log) and Number of
statistically-Different Bins (NDB).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition. (arXiv:2106.15125v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15125">
<div class="article-summary-box-inner">
<span><p>One essential problem in skeleton-based action recognition is how to extract
discriminative features over all skeleton joints. However, the complexity of
the recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly
sophisticated and over-parameterized. The low efficiency in model training and
inference has increased the validation costs of model architectures in
large-scale datasets. To address the above issue, recent advanced separable
convolutional layers are embedded into an early fused Multiple Input Branches
(MIB) network, constructing an efficient Graph Convolutional Network (GCN)
baseline for skeleton-based action recognition. In addition, based on such the
baseline, we design a compound scaling strategy to expand the model's width and
depth synchronously, and eventually obtain a family of efficient GCN baselines
with high accuracies and small amounts of trainable parameters, termed
EfficientGCN-Bx, where "x" denotes the scaling coefficient. On two large-scale
datasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4 baseline
outperforms other SOTA methods, e.g., achieving 91.7% accuracy on the
cross-subject benchmark of NTU 60 dataset, while being 3.15x smaller and 3.21x
faster than MS-G3D, which is one of the best SOTA methods. The source code in
PyTorch version and the pretrained models are available at
https://github.com/yfsong0709/EfficientGCNv1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mobile-Former: Bridging MobileNet and Transformer. (arXiv:2108.05895v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05895">
<div class="article-summary-box-inner">
<span><p>We present Mobile-Former, a parallel design of MobileNet and transformer with
a two-way bridge in between. This structure leverages the advantages of
MobileNet at local processing and transformer at global interaction. And the
bridge enables bidirectional fusion of local and global features. Different
from recent works on vision transformer, the transformer in Mobile-Former
contains very few tokens (e.g. 6 or fewer tokens) that are randomly initialized
to learn global priors, resulting in low computational cost. Combining with the
proposed light-weight cross attention to model the bridge, Mobile-Former is not
only computationally efficient, but also has more representation power. It
outperforms MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet
classification. For instance, Mobile-Former achieves 77.9\% top-1 accuracy at
294M FLOPs, gaining 1.3\% over MobileNetV3 but saving 17\% of computations.
When transferring to object detection, Mobile-Former outperforms MobileNetV3 by
8.6 AP in RetinaNet framework. Furthermore, we build an efficient end-to-end
detector by replacing backbone, encoder and decoder in DETR with Mobile-Former,
which outperforms DETR by 1.1 AP but saves 52\% of computational cost and 36\%
of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02173">
<div class="article-summary-box-inner">
<span><p>Autonomous vehicles must reason about spatial occlusions in urban
environments to ensure safety without being overly cautious. Prior work
explored occlusion inference from observed social behaviors of road agents,
hence treating people as sensors. Inferring occupancy from agent behaviors is
an inherently multimodal problem; a driver may behave similarly for different
occupancy patterns ahead of them (e.g., a driver may move at constant speed in
traffic or on an open road). Past work, however, does not account for this
multimodality, thus neglecting to model this source of aleatoric uncertainty in
the relationship between driver behaviors and their environment. We propose an
occlusion inference method that characterizes observed behaviors of human
agents as sensor measurements, and fuses them with those from a standard sensor
suite. To capture the aleatoric uncertainty, we train a conditional variational
autoencoder with a discrete latent space to learn a multimodal mapping from
observed driver trajectories to an occupancy grid representation of the view
ahead of the driver. Our method handles multi-agent scenarios, combining
measurements from multiple observed drivers using evidential theory to solve
the sensor fusion problem. Our approach is validated on a cluttered, real-world
intersection, outperforming baselines and demonstrating real-time capable
performance. Our code is available at
https://github.com/sisl/MultiAgentVariationalOcclusionInference .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. (arXiv:2109.03223v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03223">
<div class="article-summary-box-inner">
<span><p>Out of all existing frameworks for surgical workflow analysis in endoscopic
videos, action triplet recognition stands out as the only one aiming to provide
truly fine-grained and comprehensive information on surgical activities. This
information, presented as &lt;instrument, verb, target&gt; combinations, is highly
challenging to be accurately identified. Triplet components can be difficult to
recognize individually; in this task, it requires not only performing
recognition simultaneously for all three triplet components, but also correctly
establishing the data association between them. To achieve this task, we
introduce our new model, the Rendezvous (RDV), which recognizes triplets
directly from surgical videos by leveraging attention at two different levels.
We first introduce a new form of spatial attention to capture individual action
triplet components in a scene; called Class Activation Guided Attention
Mechanism (CAGAM). This technique focuses on the recognition of verbs and
targets using activations resulting from instruments. To solve the association
problem, our RDV model adds a new form of semantic attention inspired by
Transformer networks; called Multi-Head of Mixed Attention (MHMA). This
technique uses several cross and self attentions to effectively capture
relationships between instruments, verbs, and targets. We also introduce
CholecT50 - a dataset of 50 endoscopic videos in which every frame has been
annotated with labels from 100 triplet classes. Our proposed RDV model
significantly improves the triplet prediction mean AP by over 9% compared to
the state-of-the-art methods on this dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04275">
<div class="article-summary-box-inner">
<span><p>Despite the potential of multi-modal pre-training to learn highly
discriminative feature representations from complementary data modalities,
current progress is being slowed by the lack of large-scale modality-diverse
datasets. By leveraging the natural suitability of E-commerce, where different
modalities capture complementary semantic information, we contribute a
large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5
modalities (image, text, table, video, and audio), covers over 6,000 categories
and 5,000 attributes, and is 500 larger than the largest publicly available
dataset with a similar number of modalities. Furthermore, M5Product contains
incomplete modality pairs and noise while also having a long-tailed
distribution, resembling most real-world problems. We further propose
Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework
that integrates the different modalities into a unified model through an
adaptive feature fusion mechanism, where the importance of each modality is
learned directly from the modality embeddings and impacts the inter-modality
contrastive learning and masked tasks within a multi-modal transformer model.
We evaluate the current multi-modal pre-training state-of-the-art approaches
and benchmark their ability to learn from unlabeled data when faced with the
large number of modalities in the M5Product dataset. We conduct extensive
experiments on four downstream tasks and demonstrate the superiority of our
SCALE model, providing insights into the importance of dataset scale and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07644">
<div class="article-summary-box-inner">
<span><p>Employing Vehicle-to-Vehicle communication to enhance perception performance
in self-driving technology has attracted considerable attention recently;
however, the absence of a suitable open dataset for benchmarking algorithms has
made it difficult to develop and assess cooperative perception technologies. To
this end, we present the first large-scale open simulated dataset for
Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464
frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns
in CARLA and a digital town of Culver City, Los Angeles. We then construct a
comprehensive benchmark with a total of 16 implemented models to evaluate
several information fusion strategies~(i.e. early, late, and intermediate
fusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose
a new Attentive Intermediate Fusion pipeline to aggregate information from
multiple connected vehicles. Our experiments show that the proposed pipeline
can be easily integrated with existing 3D LiDAR detectors and achieve
outstanding performance even with large compression rates. To encourage more
researchers to investigate Vehicle-to-Vehicle perception, we will release the
dataset, benchmark methods, and all related codes in
https://mobility-lab.seas.ucla.edu/opv2v/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Horizon Detection Algorithm for Maritime Surveillance. (arXiv:2110.13694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13694">
<div class="article-summary-box-inner">
<span><p>The horizon line is a valuable feature in the maritime environment as it has
a high persistence when compared to other features (e.g., shore corners,
waves). It is used in several applications, especially in maritime
surveillance. The task of horizon detection may be easy for humans, but it is
hard on computers due to the high change of color and texture on maritime
scenes. Moreover, the computational complexity is an important constraint to
take into account while developing the algorithm. In this paper, we propose a
new method that we expect to enhance the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Authentication Attacks on Projection-based Cancelable Biometric Schemes. (arXiv:2110.15163v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15163">
<div class="article-summary-box-inner">
<span><p>Cancelable biometric schemes aim at generating secure biometric templates by
combining user specific tokens, such as password, stored secret or salt, along
with biometric data. This type of transformation is constructed as a
composition of a biometric transformation with a feature extraction algorithm.
The security requirements of cancelable biometric schemes concern the
irreversibility, unlinkability and revocability of templates, without losing in
accuracy of comparison. While several schemes were recently attacked regarding
these requirements, full reversibility of such a composition in order to
produce colliding biometric characteristics, and specifically presentation
attacks, were never demonstrated to the best of our knowledge. In this paper,
we formalize these attacks for a traditional cancelable scheme with the help of
integer linear programming (ILP) and quadratically constrained quadratic
programming (QCQP). Solving these optimization problems allows an adversary to
slightly alter its fingerprint image in order to impersonate any individual.
Moreover, in an even more severe scenario, it is possible to simultaneously
impersonate several individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closed-Loop Data Transcription to an LDR via Minimaxing Rate Reduction. (arXiv:2111.06636v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06636">
<div class="article-summary-box-inner">
<span><p>This work proposes a new computational framework for learning a structured
generative model for real-world datasets. In particular, we propose to learn a
closed-loop transcription between a multi-class multi-dimensional data
distribution and a linear discriminative representation (LDR) in the feature
space that consists of multiple independent multi-dimensional linear subspaces.
In particular, we argue that the optimal encoding and decoding mappings sought
can be formulated as the equilibrium point of a two-player minimax game between
the encoder and decoder. A natural utility function for this game is the
so-called rate reduction, a simple information-theoretic measure for distances
between mixtures of subspace-like Gaussians in the feature space. Our
formulation draws inspiration from closed-loop error feedback from control
systems and avoids expensive evaluating and minimizing approximated distances
between arbitrary distributions in either the data space or the feature space.
To a large extent, this new formulation unifies the concepts and benefits of
Auto-Encoding and GAN and naturally extends them to the settings of learning a
both discriminative and generative representation for multi-class and
multi-dimensional real-world data. Our extensive experiments on many benchmark
imagery datasets demonstrate tremendous potential of this new closed-loop
formulation: under fair comparison, visual quality of the learned decoder and
classification performance of the encoder is competitive and often better than
existing methods based on GAN, VAE, or a combination of both. Unlike existing
generative models, the so learned features of the multiple classes are
structured: different classes are explicitly mapped onto corresponding
independent principal subspaces in the feature space. Source code can be found
at https://github.com/Delay-Xili/LDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection. (arXiv:2111.09099v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09099">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is commonly pursued as a one-class classification problem,
where models can only learn from normal training samples, while being evaluated
on both normal and abnormal test samples. Among the successful approaches for
anomaly detection, a distinguished category of methods relies on predicting
masked information (e.g. patches, future frames, etc.) and leveraging the
reconstruction error with respect to the masked information as an abnormality
score. Different from related methods, we propose to integrate the
reconstruction-based functionality into a novel self-supervised predictive
architectural building block. The proposed self-supervised block is generic and
can easily be incorporated into various state-of-the-art anomaly detection
methods. Our block starts with a convolutional layer with dilated filters,
where the center area of the receptive field is masked. The resulting
activation maps are passed through a channel attention module. Our block is
equipped with a loss that minimizes the reconstruction error with respect to
the masked area in the receptive field. We demonstrate the generality of our
block by integrating it into several state-of-the-art frameworks for anomaly
detection on image and video, providing empirical evidence that shows
considerable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We
release our code as open source at https://github.com/ristea/sspcab.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Aegis: Robust adversarial protectors for medical images. (arXiv:2111.10969v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10969">
<div class="article-summary-box-inner">
<span><p>Deep neural network based medical image systems are vulnerable to adversarial
examples. Many defense mechanisms have been proposed in the literature,
however, the existing defenses assume a passive attacker who knows little about
the defense system and does not change the attack strategy according to the
defense. Recent works have shown that a strong adaptive attack, where an
attacker is assumed to have full knowledge about the defense system, can easily
bypass the existing defenses. In this paper, we propose a novel adversarial
example defense system called Medical Aegis. To the best of our knowledge,
Medical Aegis is the first defense in the literature that successfully
addresses the strong adaptive adversarial example attacks to medical images.
Medical Aegis boasts two-tier protectors: The first tier of Cushion weakens the
adversarial manipulation capability of an attack by removing its high-frequency
components, yet posing a minimal effect on classification performance of the
original image; the second tier of Shield learns a set of per-class DNN models
to predict the logits of the protected model. Deviation from the Shield's
prediction indicates adversarial examples. Shield is inspired by the
observations in our stress tests that there exist robust trails in the shallow
layers of a DNN model, which the adaptive attacks can hardly destruct.
Experimental results show that the proposed defense accurately detects adaptive
attacks, with negligible overhead for model inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Object Detection via Adaptive Self-Training. (arXiv:2111.13216v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13216">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of domain adaptation in object detection, where there
is a significant domain shift between a source (a domain with supervision) and
a target domain (a domain of interest without supervision). As a widely adopted
domain adaptation method, the self-training teacher-student framework (a
student model learns from pseudo labels generated from a teacher model) has
yielded remarkable accuracy gain on the target domain. However, it still
suffers from the large amount of low-quality pseudo labels (e.g., false
positives) generated from the teacher due to its bias toward the source domain.
To address this issue, we propose a self-training framework called Adaptive
Unbiased Teacher (AUT) leveraging adversarial learning and weak-strong data
augmentation during mutual learning to address domain shift. Specifically, we
employ feature-level adversarial training in the student model, ensuring
features extracted from the source and target domains share similar statistics.
This enables the student model to capture domain-invariant features.
Furthermore, we apply weak-strong augmentation and mutual learning between the
teacher model on the target domain and the student model on both domains. This
enables the teacher model to gradually benefit from the student model without
suffering domain shift. We show that AUT demonstrates superiority over all
existing approaches and even Oracle (fully supervised) models by a large
margin. For example, we achieve 50.9% (49.3%) mAP on Foggy Cityscape
(Clipart1K), which is 9.2% (5.2%) and 8.2% (11.0%) higher than previous
state-of-the-art and Oracle, respectively
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Frame Interpolation Transformer. (arXiv:2111.13817v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13817">
<div class="article-summary-box-inner">
<span><p>Existing methods for video interpolation heavily rely on deep convolution
neural networks, and thus suffer from their intrinsic limitations, such as
content-agnostic kernel weights and restricted receptive field. To address
these issues, we propose a Transformer-based video interpolation framework that
allows content-aware aggregation weights and considers long-range dependencies
with the self-attention operations. To avoid the high computational cost of
global self-attention, we introduce the concept of local attention into video
interpolation and extend it to the spatial-temporal domain. Furthermore, we
propose a space-time separation strategy to save memory usage, which also
improves performance. In addition, we develop a multi-scale frame synthesis
scheme to fully realize the potential of Transformers. Extensive experiments
demonstrate the proposed model performs favorably against the state-of-the-art
methods both quantitatively and qualitatively on a variety of benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector Quantized Diffusion Model for Text-to-Image Synthesis. (arXiv:2111.14822v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14822">
<div class="article-summary-box-inner">
<span><p>We present the vector quantized diffusion (VQ-Diffusion) model for
text-to-image generation. This method is based on a vector quantized
variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional
variant of the recently developed Denoising Diffusion Probabilistic Model
(DDPM). We find that this latent-space method is well-suited for text-to-image
generation tasks because it not only eliminates the unidirectional bias with
existing methods but also allows us to incorporate a mask-and-replace diffusion
strategy to avoid the accumulation of errors, which is a serious problem with
existing methods. Our experiments show that the VQ-Diffusion produces
significantly better text-to-image generation results when compared with
conventional autoregressive (AR) models with similar numbers of parameters.
Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can
handle more complex scenes and improve the synthesized image quality by a large
margin. Finally, we show that the image generation computation in our method
can be made highly efficient by reparameterization. With traditional AR
methods, the text-to-image generation time increases linearly with the output
image resolution and hence is quite time consuming even for normal size images.
The VQ-Diffusion allows us to achieve a better trade-off between quality and
speed. Our experiments indicate that the VQ-Diffusion model with the
reparameterization is fifteen times faster than traditional AR methods while
achieving a better image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15430">
<div class="article-summary-box-inner">
<span><p>In spite of the dominant performances of deep neural networks, recent works
have shown that they are poorly calibrated, resulting in over-confident
predictions. Miscalibration can be exacerbated by overfitting due to the
minimization of the cross-entropy during training, as it promotes the predicted
softmax probabilities to match the one-hot label assignments. This yields a
pre-softmax activation of the correct class that is significantly larger than
the remaining activations. Recent evidence from the literature suggests that
loss functions that embed implicit or explicit maximization of the entropy of
predictions yield state-of-the-art calibration performances. We provide a
unifying constrained-optimization perspective of current state-of-the-art
calibration losses. Specifically, these losses could be viewed as
approximations of a linear penalty (or a Lagrangian) imposing equality
constraints on logit distances. This points to an important limitation of such
underlying equality constraints, whose ensuing gradients constantly push
towards a non-informative solution, which might prevent from reaching the best
compromise between the discriminative performance and calibration of the model
during gradient-based optimization. Following our observations, we propose a
simple and flexible generalization based on inequality constraints, which
imposes a controllable margin on logit distances. Comprehensive experiments on
a variety of image classification, semantic segmentation and NLP benchmarks
demonstrate that our method sets novel state-of-the-art results on these tasks
in terms of network calibration, without affecting the discriminative
performance. The code is available at https://github.com/by-liu/MbLS .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic travel pattern extraction from visa page stamps using CNN models. (arXiv:2112.00348v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00348">
<div class="article-summary-box-inner">
<span><p>Manual travel pattern inference from visa page stamps is a time consuming
activity and constitutes an important bottleneck in the efficiency of traveler
inspection at border crossings. Despite efforts to digitize and record the
border crossing information into databases, travel pattern inference from
stamps will remain a problem until every country in the world is incorporated
into such a unified system. This could take decades. We propose an automated
document analysis system that processes scanned visa pages and automatically
extracts the travel pattern from detected stamps. The system processes the page
via the following pipeline: stamp detection in the visa page; general stamp
country and entry/exit recognition; Schengen area stamp country and entry/exit
recognition; Schengen area stamp date extraction. For each stage of the
proposed pipeline we construct neural network models and train then on a
mixture of real and synthetic data. We integrated Schengen area stamp detection
and date, country, entry/exit recognition models together with a graphical user
interface into a prototype of an automatic travel pattern extraction tool. We
find that by combining simple neural network models into our proposed pipeline
a useful tool can be created which can speed up the travel pattern extraction
significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVT: BERT Pretraining of Video Transformers. (arXiv:2112.01529v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01529">
<div class="article-summary-box-inner">
<span><p>This paper studies the BERT pretraining of video transformers. It is a
straightforward but worth-studying extension given the recent success from BERT
pretraining of image transformers. We introduce BEVT which decouples video
representation learning into spatial representation learning and temporal
dynamics learning. In particular, BEVT first performs masked image modeling on
image data, and then conducts masked image modeling jointly with masked video
modeling on video data. This design is motivated by two observations: 1)
transformers learned on image datasets provide decent spatial priors that can
ease the learning of video transformers, which are often times
computationally-intensive if trained from scratch; 2) discriminative clues,
i.e., spatial and temporal information, needed to make correct predictions vary
among different videos due to large intra-class and inter-class variations. We
conduct extensive experiments on three challenging video benchmarks where BEVT
achieves very promising results. On Kinetics 400, for which recognition mostly
relies on discriminative spatial representations, BEVT achieves comparable
results to strong supervised baselines. On Something-Something-V2 and Diving
48, which contain videos relying on temporal dynamics, BEVT outperforms by
clear margins all alternative baselines and achieves state-of-the-art
performance with a 71.4\% and 87.2\% Top-1 accuracy respectively. Code will be
made available at \url{https://github.com/xyzforever/BEVT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Next Day Wildfire Spread: A Machine Learning Data Set to Predict Wildfire Spreading from Remote-Sensing Data. (arXiv:2112.02447v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02447">
<div class="article-summary-box-inner">
<span><p>Predicting wildfire spread is critical for land management and disaster
preparedness. To this end, we present `Next Day Wildfire Spread,' a curated,
large-scale, multivariate data set of historical wildfires aggregating nearly a
decade of remote-sensing data across the United States. In contrast to existing
fire data sets based on Earth observation satellites, our data set combines 2D
fire data with multiple explanatory variables (e.g., topography, vegetation,
weather, drought index, population density) aligned over 2D regions, providing
a feature-rich data set for machine learning. To demonstrate the usefulness of
this data set, we implement a neural network that takes advantage of the
spatial information of this data to predict wildfire spread. We compare the
performance of the neural network with other machine learning models: logistic
regression and random forest. This data set can be used as a benchmark for
developing wildfire propagation models based on remote sensing data for a lead
time of one day.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Attentional Network for Semantic Segmentation. (arXiv:2112.04108v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04108">
<div class="article-summary-box-inner">
<span><p>Recent non-local self-attention methods have proven to be effective in
capturing long-range dependencies for semantic segmentation. These methods
usually form a similarity map of RC*C (by compressing spatial dimensions) or
RHW*HW (by compressing channels) to describe the feature relations along either
channel or spatial dimensions, where C is the number of channels, H and W are
the spatial dimensions of the input feature map. However, such practices tend
to condense feature dependencies along the other dimensions,hence causing
attention missing, which might lead to inferior results for small/thin
categories or inconsistent segmentation inside large objects. To address this
problem, we propose anew approach, namely Fully Attentional Network (FLANet),to
encode both spatial and channel attentions in a single similarity map while
maintaining high computational efficiency. Specifically, for each channel map,
our FLANet can harvest feature responses from all other channel maps, and the
associated spatial positions as well, through a novel fully attentional module.
Our new method has achieved state-of-the-art performance on three challenging
semantic segmentation datasets,i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes
test set,the ADE20K validation set, and the PASCAL VOC test set,respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs. (arXiv:2112.04222v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04222">
<div class="article-summary-box-inner">
<span><p>Today's VidSGG models are all proposal-based methods, i.e., they first
generate numerous paired subject-object snippets as proposals, and then conduct
predicate classification for each proposal. In this paper, we argue that this
prevalent proposal-based framework has three inherent drawbacks: 1) The
ground-truth predicate labels for proposals are partially correct. 2) They
break the high-order relations among different predicate instances of a same
subject-object pair. 3) VidSGG performance is upper-bounded by the quality of
the proposals. To this end, we propose a new classification-then-grounding
framework for VidSGG, which can avoid all the three overlooked drawbacks.
Meanwhile, under this framework, we reformulate the video scene graphs as
temporal bipartite graphs, where the entities and predicates are two types of
nodes with time slots, and the edges denote different semantic roles between
these nodes. This formulation takes full advantage of our new framework.
Accordingly, we further propose a novel BIpartite Graph based SGG model: BIG.
Specifically, BIG consists of two parts: a classification stage and a grounding
stage, where the former aims to classify the categories of all the nodes and
the edges, and the latter tries to localize the temporal location of each
relation instance. Extensive ablations on two VidSGG datasets have attested to
the effectiveness of our framework and BIG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images. (arXiv:2112.11325v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11325">
<div class="article-summary-box-inner">
<span><p>We propose iSegFormer, a memory-efficient transformer that combines a Swin
transformer with a lightweight multilayer perceptron (MLP) decoder. With the
efficient Swin transformer blocks for hierarchical self-attention and the
simple MLP decoder for aggregating both local and global attention, iSegFormer
learns powerful representations while achieving high computational
efficiencies. Specifically, we apply iSegFormer to interactive 3D medical image
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction. (arXiv:2201.01367v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01367">
<div class="article-summary-box-inner">
<span><p>Increasing the performance of tactile sensing in robots enables versatile,
in-hand manipulation. Vision-based tactile sensors have been widely used as
rich tactile feedback has been shown to be correlated with increased
performance in manipulation tasks. Existing tactile sensor solutions with high
resolution have limitations that include low accuracy, expensive components, or
lack of scalability. In this paper, an inexpensive, scalable, and compact
tactile sensor with high-resolution surface deformation modeling for surface
reconstruction of the 3D sensor surface is proposed. By measuring the image
from the fisheye camera, it is shown that the sensor can successfully estimate
the surface deformation in real-time (1.8ms) by using deep convolutional neural
networks. This sensor in its design and sensing abilities represents a
significant step toward better object in-hand localization, classification, and
surface estimation all enabled by high-resolution shape reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign Language Recognition System using TensorFlow Object Detection API. (arXiv:2201.01486v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01486">
<div class="article-summary-box-inner">
<span><p>Communication is defined as the act of sharing or exchanging information,
ideas or feelings. To establish communication between two people, both of them
are required to have knowledge and understanding of a common language. But in
the case of deaf and dumb people, the means of communication are different.
Deaf is the inability to hear and dumb is the inability to speak. They
communicate using sign language among themselves and with normal people but
normal people do not take seriously the importance of sign language. Not
everyone possesses the knowledge and understanding of sign language which makes
communication difficult between a normal person and a deaf and dumb person. To
overcome this barrier, one can build a model based on machine learning. A model
can be trained to recognize different gestures of sign language and translate
them into English. This will help a lot of people in communicating and
conversing with deaf and dumb people. The existing Indian Sing Language
Recognition systems are designed using machine learning algorithms with single
and double-handed gestures but they are not real-time. In this paper, we
propose a method to create an Indian Sign Language dataset using a webcam and
then using transfer learning, train a TensorFlow model to create a real-time
Sign Language Recognition system. The system achieves a good level of accuracy
even with a limited size dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02001">
<div class="article-summary-box-inner">
<span><p>Visual place recognition is a challenging task for applications such as
autonomous driving navigation and mobile robot localization. Distracting
elements presenting in complex scenes often lead to deviations in the
perception of visual place. To address this problem, it is crucial to integrate
information from only task-relevant regions into image representations. In this
paper, we introduce a novel holistic place recognition model, TransVPR, based
on vision Transformers. It benefits from the desirable property of the
self-attention operation in Transformers which can naturally aggregate
task-relevant features. Attentions from multiple levels of the Transformer,
which focus on different regions of interest, are further combined to generate
a global image representation. In addition, the output tokens from Transformer
layers filtered by the fused attention mask are considered as key-patch
descriptors, which are used to perform spatial matching to re-rank the
candidates retrieved by the global image features. The whole model allows
end-to-end training with a single objective and image-level supervision.
TransVPR achieves state-of-the-art performance on several real-world benchmarks
while maintaining low computational time and storage requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval. (arXiv:2201.09168v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09168">
<div class="article-summary-box-inner">
<span><p>This paper aims for the task of text-to-video retrieval, where given a query
in the form of a natural-language sentence, it is asked to retrieve videos
which are semantically relevant to the given query, from a great number of
unlabeled videos. The success of this task depends on cross-modal
representation learning that projects both videos and sentences into common
spaces for semantic similarity computation. In this work, we concentrate on
video representation learning, an essential component for text-to-video
retrieval. Inspired by the reading strategy of humans, we propose a
Reading-strategy Inspired Visual Representation Learning (RIVRL) to represent
videos, which consists of two branches: a previewing branch and an
intensive-reading branch. The previewing branch is designed to briefly capture
the overview information of videos, while the intensive-reading branch is
designed to obtain more in-depth information. Moreover, the intensive-reading
branch is aware of the video overview captured by the previewing branch. Such
holistic information is found to be useful for the intensive-reading branch to
extract more fine-grained features. Extensive experiments on three datasets are
conducted, where our model RIVRL achieves a new state-of-the-art on TGIF and
VATEX. Moreover, on MSR-VTT, our model using two video features shows
comparable performance to the state-of-the-art using seven video features and
even outperforms models pre-trained on the large-scale HowTo100M dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HCSC: Hierarchical Contrastive Selective Coding. (arXiv:2202.00455v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00455">
<div class="article-summary-box-inner">
<span><p>Hierarchical semantic structures naturally exist in an image dataset, in
which several semantically relevant image clusters can be further integrated
into a larger cluster with coarser-grained semantics. Capturing such structures
with image representations can greatly benefit the semantic understanding on
various downstream tasks. Existing contrastive representation learning methods
lack such an important model capability. In addition, the negative pairs used
in these methods are not guaranteed to be semantically distinct, which could
further hamper the structural correctness of learned image representations. To
tackle these limitations, we propose a novel contrastive learning framework
called Hierarchical Contrastive Selective Coding (HCSC). In this framework, a
set of hierarchical prototypes are constructed and also dynamically updated to
represent the hierarchical semantic structures underlying the data in the
latent space. To make image representations better fit such semantic
structures, we employ and further improve conventional instance-wise and
prototypical contrastive learning via an elaborate pair selection scheme. This
scheme seeks to select more diverse positive pairs with similar semantics and
more precise negative pairs with truly distinct semantics. On extensive
downstream tasks, we verify the superior performance of HCSC over
state-of-the-art contrastive methods, and the effectiveness of major model
components is proved by plentiful analytical studies. We build a comprehensive
model zoo in Sec. D. Our source code and model weights are available at
https://github.com/gyfastas/HCSC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Network. (arXiv:2202.09741v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09741">
<div class="article-summary-box-inner">
<span><p>While originally designed for natural language processing (NLP) tasks, the
self-attention mechanism has recently taken various computer vision areas by
storm. However, the 2D nature of images brings three challenges for applying
self-attention in computer vision. (1) Treating images as 1D sequences neglects
their 2D structures. (2) The quadratic complexity is too expensive for
high-resolution images. (3) It only captures spatial adaptability but ignores
channel adaptability. In this paper, we propose a novel large kernel attention
(LKA) module to enable self-adaptive and long-range correlations in
self-attention while avoiding the above issues. We further introduce a novel
neural network based on LKA, namely Visual Attention Network (VAN). While
extremely simple and efficient, VAN outperforms the state-of-the-art vision
transformers and convolutional neural networks with a large margin in extensive
experiments, including image classification, object detection, semantic
segmentation, instance segmentation, etc. Code is available at
https://github.com/Visual-Attention-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Deterministic Face Mask Removal Based On 3D Priors. (arXiv:2202.09856v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09856">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel image inpainting framework for face mask removal.
Although current methods have demonstrated their impressive ability in
recovering damaged face images, they suffer from two main problems: the
dependence on manually labeled missing regions and the deterministic result
corresponding to each input. The proposed approach tackles these problems by
integrating a multi-task 3D face reconstruction module with a face inpainting
module. Given a masked face image, the former predicts a 3DMM-based
reconstructed face together with a binary occlusion map, providing dense
geometrical and textural priors that greatly facilitate the inpainting task of
the latter. By gradually controlling the 3D shape parameters, our method
generates high-quality dynamic inpainting results with different expressions
and mouth movements. Qualitative and quantitative experiments verify the
effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10401">
<div class="article-summary-box-inner">
<span><p>Vision-language representation learning largely benefits from image-text
alignment through contrastive losses (e.g., InfoNCE loss). The success of this
alignment strategy is attributed to its capability in maximizing the mutual
information (MI) between an image and its matched text. However, simply
performing cross-modal alignment (CMA) ignores data potential within each
modality, which may result in degraded representations. For instance, although
CMA-based models are able to map image-text pairs close together in the
embedding space, they fail to ensure that similar inputs from the same modality
stay close by. This problem can get even worse when the pre-training data is
noisy. In this paper, we propose triple contrastive learning (TCL) for
vision-language pre-training by leveraging both cross-modal and intra-modal
self-supervision. Besides CMA, TCL introduces an intra-modal contrastive
objective to provide complementary benefits in representation learning. To take
advantage of localized and structural information from image and text input,
TCL further maximizes the average MI between local regions of image/text and
their global summary. To the best of our knowledge, ours is the first work that
takes into account local structure information for multi-modality
representation learning. Experimental evaluations show that our approach is
competitive and achieve the new state of the art on various common down-stream
vision-language tasks such as image-text retrieval and visual question
answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-RangeSeg: LiDAR Sequence Semantic Segmentation Using Multiple Feature Aggregation. (arXiv:2202.13377v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13377">
<div class="article-summary-box-inner">
<span><p>LiDAR sensor is essential to the perception system in autonomous vehicles and
intelligent robots. To fulfill the real-time requirements in real-world
applications, it is necessary to efficiently segment the LiDAR scans. Most of
previous approaches directly project 3D point cloud onto the 2D spherical range
image so that they can make use of the efficient 2D convolutional operations
for image segmentation. Although having achieved the encouraging results, the
neighborhood information is not well-preserved in the spherical projection.
Moreover, the temporal information is not taken into consideration in the
single scan segmentation task. To tackle these problems, we propose a novel
approach to semantic segmentation for LiDAR sequences named Meta-RangeSeg,
where a novel range residual image representation is introduced to capture the
spatial-temporal information. Specifically, Meta-Kernel is employed to extract
the meta features, which reduces the inconsistency between the 2D range image
coordinates input and Cartesian coordinates output. An efficient U-Net backbone
is used to obtain the multi-scale features. Furthermore, Feature Aggregation
Module (FAM) aggregates the meta features and multi-scale features, which tends
to strengthen the role of range channel. We have conducted extensive
experiments for performance evaluation on SemanticKITTI, which is the de-facto
dataset for LiDAR semantic segmentation. The promising results show that our
proposed Meta-RangeSeg method is more efficient and effective than the existing
approaches. Our full implementation is publicly available at
https://github.com/songw-zju/Meta-RangeSeg .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Alignment using Representation Codebook. (arXiv:2203.00048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00048">
<div class="article-summary-box-inner">
<span><p>Aligning signals from different modalities is an important step in
vision-language representation learning as it affects the performance of later
stages such as cross-modality fusion. Since image and text typically reside in
different regions of the feature space, directly aligning them at instance
level is challenging especially when features are still evolving during
training. In this paper, we propose to align at a higher and more stable level
using cluster representation. Specifically, we treat image and text as two
"views" of the same entity, and encode them into a joint vision-language coding
space spanned by a dictionary of cluster centers (codebook). We contrast
positive and negative samples via their cluster assignments while
simultaneously optimizing the cluster centers. To further smooth out the
learning process, we adopt a teacher-student distillation paradigm, where the
momentum teacher of one view guides the student learning of the other. We
evaluated our approach on common vision language benchmarks and obtain new SoTA
on zero-shot cross modality retrieval while being competitive on various other
transfer tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-scale Transformer for Medical Image Segmentation: Architectures, Model Efficiency, and Benchmarks. (arXiv:2203.00131v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00131">
<div class="article-summary-box-inner">
<span><p>Transformers have emerged to be successful in a number of natural language
processing and vision tasks, but their potential applications to medical
imaging remain largely unexplored due to the unique difficulties of this field.
In this study, we present UTNetV2, a simple yet powerful backbone model that
combines the strengths of the convolutional neural network and Transformer for
enhancing performance and efficiency in medical image segmentation. The
critical design of UTNetV2 includes three innovations: (1) We used a hybrid
hierarchical architecture by introducing depthwise separable convolution to
projection and feed-forward network in the Transformer block, which brings
local relationship modeling and desirable properties of CNNs (translation
invariance) to Transformer, thus eliminate the requirement of large-scale
pre-training. (2) We proposed efficient bidirectional attention (B-MHA) that
reduces the quadratic computation complexity of self-attention to linear by
introducing an adaptively updated semantic map. The efficient attention makes
it possible to capture long-range relationship and correct the fine-grained
errors in high-resolution token maps. (3) The semantic maps in the B-MHA allow
us to perform semantically and spatially global multi-scale feature fusion
without introducing much computational overhead. Furthermore, we provide a fair
comparison codebase of CNN-based and Transformer-based on various medical image
segmentation tasks to evaluate the merits and defects of both architectures.
UTNetV2 demonstrated state-of-the-art performance across various settings,
including large-scale datasets, small-scale datasets, 2D and 3D settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Creativity Characterization of Generative Models via Group-based Subset Scanning. (arXiv:2203.00523v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00523">
<div class="article-summary-box-inner">
<span><p>Deep generative models, such as Variational Autoencoders (VAEs) and
Generative Adversarial Networks (GANs), have been employed widely in
computational creativity research. However, such models discourage
out-of-distribution generation to avoid spurious sample generation, thereby
limiting their creativity. Thus, incorporating research on human creativity
into generative deep learning techniques presents an opportunity to make their
outputs more compelling and human-like. As we see the emergence of generative
models directed toward creativity research, a need for machine learning-based
surrogate metrics to characterize creative output from these models is
imperative. We propose group-based subset scanning to identify, quantify, and
characterize creative processes by detecting a subset of anomalous
node-activations in the hidden layers of the generative models. Our experiments
on the standard image benchmarks, and their "creatively generated" variants,
reveal that the proposed subset scores distribution is more useful for
detecting creative processes in the activation space rather than the pixel
space. Further, we found that creative samples generate larger subsets of
anomalies than normal or non-creative samples across datasets. The node
activations highlighted during the creative decoding process are different from
those responsible for the normal sample generation. Lastly, we assess if the
images from the subsets selected by our method were also found creative by
human evaluators, presenting a link between creativity perception in humans and
node activations within deep neural nets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00859">
<div class="article-summary-box-inner">
<span><p>Recent transformer-based solutions have been introduced to estimate 3D human
pose from 2D keypoint sequence by considering body joints among all frames
globally to learn spatio-temporal correlation. We observe that the motions of
different joints differ significantly. However, the previous methods cannot
efficiently model the solid inter-frame correspondence of each joint, leading
to insufficient learning of spatial-temporal correlation. We propose MixSTE
(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to
separately model the temporal motion of each joint and a spatial transformer
block to learn inter-joint spatial correlation. These two blocks are utilized
alternately to obtain better spatio-temporal feature encoding. In addition, the
network output is extended from the central frame to entire frames of the input
video, thereby improving the coherence between the input and output sequences.
Extensive experiments are conducted on three benchmarks (i.e. Human3.6M,
MPI-INF-3DHP, and HumanEva) to evaluate the proposed method. The results show
that our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and
7.6% MPJPE on the Human3.6M dataset. Code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Universal Rotation Equivariant Point-cloud Network. (arXiv:2203.01216v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01216">
<div class="article-summary-box-inner">
<span><p>Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Query-based Paradigm for Point Cloud Understanding. (arXiv:2203.01252v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01252">
<div class="article-summary-box-inner">
<span><p>3D point cloud understanding is an important component in autonomous driving
and robotics. In this paper, we present a novel Embedding-Querying paradigm
(EQ-Paradigm) for 3D understanding tasks including detection, segmentation and
classification. EQ-Paradigm is a unified paradigm that enables the combination
of any existing 3D backbone architectures with different task heads. Under the
EQ-Paradigm, the input is firstly encoded in the embedding stage with an
arbitrary feature extraction architecture, which is independent of tasks and
heads. Then, the querying stage enables the encoded features to be applicable
for diverse task heads. This is achieved by introducing an intermediate
representation, i.e., Q-representation, in the querying stage to serve as a
bridge between the embedding stage and task heads. We design a novel Q-Net as
the querying stage network. Extensive experimental results on various 3D tasks
including semantic segmentation, object detection and shape classification show
that EQ-Paradigm in tandem with Q-Net is a general and effective pipeline,
which enables a flexible collaboration of backbones and heads, and further
boosts the performance of the state-of-the-art methods. All codes and models
will be published soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Protecting Celebrities with Identity Consistency Transformer. (arXiv:2203.01318v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01318">
<div class="article-summary-box-inner">
<span><p>In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-04 23:07:40.806586490 UTC">2022-03-04 23:07:40 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>