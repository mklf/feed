<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-21T01:30:00Z">02-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving English to Sinhala Neural Machine Translation using Part-of-Speech Tag. (arXiv:2202.08882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08882">
<div class="article-summary-box-inner">
<span><p>The performance of Neural Machine Translation (NMT) depends significantly on
the size of the available parallel corpus. Due to this fact, low resource
language pairs demonstrate low translation performance compared to high
resource language pairs. The translation quality further degrades when NMT is
performed for morphologically rich languages. Even though the web contains a
large amount of information, most people in Sri Lanka are unable to read and
understand English properly. Therefore, there is a huge requirement of
translating English content to local languages to share information among
locals. Sinhala language is the primary language in Sri Lanka and building an
NMT system that can produce quality English to Sinhala translations is
difficult due to the syntactic divergence between these two languages under low
resource constraints. Thus, in this research, we explore effective methods of
incorporating Part of Speech (POS) tags to the Transformer input embedding and
positional encoding to further enhance the performance of the baseline English
to Sinhala neural machine translation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effects of Interactive AI Design on User Behavior: An Eye-tracking Study of Fact-checking COVID-19 Claims. (arXiv:2202.08901v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08901">
<div class="article-summary-box-inner">
<span><p>We conducted a lab-based eye-tracking study to investigate how the
interactivity of an AI-powered fact-checking system affects user interactions,
such as dwell time, attention, and mental resources involved in using the
system. A within-subject experiment was conducted, where participants used an
interactive and a non-interactive version of a mock AI fact-checking system and
rated their perceived correctness of COVID-19 related claims. We collected
web-page interactions, eye-tracking data, and mental workload using NASA-TLX.
We found that the presence of the affordance of interactively manipulating the
AI system's prediction parameters affected users' dwell times, and
eye-fixations on AOIs, but not mental workload. In the interactive system,
participants spent the most time evaluating claims' correctness, followed by
reading news. This promising result shows a positive role of interactivity in a
mixed-initiative AI-powered system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGPT: GPT Sentence Embeddings for Semantic Search. (arXiv:2202.08904v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08904">
<div class="article-summary-box-inner">
<span><p>GPT transformers are the largest language models available, yet semantic
search is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for
applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric
search.
</p>
<p>SGPT-BE produces semantically meaningful sentence embeddings by contrastive
fine-tuning of only bias tensors and a novel pooling method. A 5.8 billion
parameter SGPT-BE outperforms the best available sentence embeddings by 6%
setting a new state-of-the-art on BEIR. It outperforms the concurrently
proposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes
250,000 times more parameters.
</p>
<p>SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1
billion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It
beats the supervised state-of-the-art on 7 datasets, but significantly loses on
other datasets. We show how this can be alleviated by adapting the prompt.
</p>
<p>SGPT-BE and SGPT-CE performance scales with model size. Yet, increased
latency, storage and compute costs should be considered. Code, models and
result files are freely available at https://github.com/Muennighoff/sgpt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing Effective Sparse Expert Models. (arXiv:2202.08906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08906">
<div class="article-summary-box-inner">
<span><p>Scale has opened new frontiers in natural language processing -- but at a
high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have
been proposed as an energy efficient path to even larger and more capable
language models. But advancing the state-of-the-art across a broad set of
natural language tasks has been hindered by training instabilities and
uncertain quality during fine-tuning. Our work focuses on these issues and acts
as a design guide. We conclude by scaling a sparse model to 269B parameters,
with a computational cost comparable to a 32B dense encoder-decoder Transformer
(Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time,
a sparse model achieves state-of-the-art performance in transfer learning,
across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC
Challenge), summarization (XSum, CNN-DM), closed book question answering
(WebQA, Natural Questions), and adversarially constructed tasks (Winogrande,
ANLI R3).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Fine-Grained Semantics in Knowledge Graph Relations. (arXiv:2202.08917v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08917">
<div class="article-summary-box-inner">
<span><p>When it comes to comprehending and analyzing multi-relational data, the
semantics of relations are crucial. Polysemous relations between different
types of entities, that represent multiple semantics, are common in real-world
relational datasets represented by knowledge graphs. For numerous use cases,
such as entity type classification, question answering and knowledge graph
completion, the correct semantic interpretation of these relations is
necessary. In this work, we provide a strategy for discovering the different
semantics associated with abstract relations and deriving many sub-relations
with fine-grained meaning. To do this, we leverage the types of the entities
associated with the relations and cluster the vector representations of
entities and relations. The suggested method is able to automatically discover
the best number of sub-relations for a polysemous relation and determine their
semantic interpretation, according to our empirical evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Intrinsic Exploration with Language Abstractions. (arXiv:2202.08938v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08938">
<div class="article-summary-box-inner">
<span><p>Reinforcement learning (RL) agents are particularly hard to train when
rewards are sparse. One common solution is to use intrinsic rewards to
encourage agents to explore their environment. However, recent intrinsic
exploration methods often use state-based novelty measures which reward
low-level exploration and may not scale to domains requiring more abstract
skills. Instead, we explore natural language as a general medium for
highlighting relevant abstractions in an environment. Unlike previous work, we
evaluate whether language can improve over existing exploration methods by
directly extending (and comparing to) competitive intrinsic exploration
baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These
language-based variants outperform their non-linguistic forms by 45-85% across
13 challenging tasks from the MiniGrid and MiniHack environment suites.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Pretrained Models of Source Code. (arXiv:2202.08975v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08975">
<div class="article-summary-box-inner">
<span><p>Deep learning models are widely used for solving challenging code processing
tasks, such as code generation or code summarization. Traditionally, a specific
model architecture was carefully built to solve a particular code processing
task. However, recently general pretrained models such as CodeBERT or CodeT5
have been shown to outperform task-specific models in many applications. While
pretrained models are known to learn complex patterns from data, they may fail
to understand some properties of source code. To test diverse aspects of code
understanding, we introduce a set of diagnosting probing tasks. We show that
pretrained models of code indeed contain information about code syntactic
structure and correctness, the notions of identifiers, data flow and
namespaces, and natural language naming. We also investigate how probing
results are affected by using code-specific pretraining objectives, varying the
model size, or finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end contextual asr based on posterior distribution adaptation for hybrid ctc/attention system. (arXiv:2202.09003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09003">
<div class="article-summary-box-inner">
<span><p>End-to-end (E2E) speech recognition architectures assemble all components of
traditional speech recognition system into a single model. Although it
simplifies ASR system, it introduces contextual ASR drawback: the E2E model has
worse performance on utterances containing infrequent proper nouns. In this
work, we propose to add a contextual bias attention (CBA) module to attention
based encoder decoder (AED) model to improve its ability of recognizing the
contextual phrases. Specifically, CBA utilizes the context vector of source
attention in decoder to attend to a specific bias embedding. Jointly learned
with the basic AED parameters, CBA can tell the model when and where to bias
its output probability distribution. At inference stage, a list of bias phrases
is preloaded and we adapt the posterior distributions of both CTC and attention
decoder according to the attended bias phrase of CBA. We evaluate the proposed
method on GigaSpeech and achieve a consistent relative improvement on recall
rate of bias phrases ranging from 15% to 28% compared to the baseline model.
Meanwhile, our method shows a strong anti-bias ability as the performance on
general tests only degrades 1.7% even 2,000 bias phrases are present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TURNER: The Uncertainty-based Retrieval Framework for Chinese NER. (arXiv:2202.09022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09022">
<div class="article-summary-box-inner">
<span><p>Chinese NER is a difficult undertaking due to the ambiguity of Chinese
characters and the absence of word boundaries. Previous work on Chinese NER
focus on lexicon-based methods to introduce boundary information and reduce
out-of-vocabulary (OOV) cases during prediction. However, it is expensive to
obtain and dynamically maintain high-quality lexicons in specific domains,
which motivates us to utilize more general knowledge resources, e.g., search
engines. In this paper, we propose TURNER: The Uncertainty-based Retrieval
framework for Chinese NER. The idea behind TURNER is to imitate human behavior:
we frequently retrieve auxiliary knowledge as assistance when encountering an
unknown or uncertain entity. To improve the efficiency and effectiveness of
retrieval, we first propose two types of uncertainty sampling methods for
selecting the most ambiguous entity-level uncertain components of the input
text. Then, the Knowledge Fusion Model re-predict the uncertain samples by
combining retrieved knowledge. Experiments on four benchmark datasets
demonstrate TURNER's effectiveness. TURNER outperforms existing lexicon-based
approaches and achieves the new SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLSEG: Contrastive Learning of Story Ending Generation. (arXiv:2202.09049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09049">
<div class="article-summary-box-inner">
<span><p>Story Ending Generation (SEG) is a challenging task in natural language
generation. Recently, methods based on Pre-trained Language Models (PLM) have
achieved great prosperity, which can produce fluent and coherent story endings.
However, the pre-training objective of PLM-based methods is unable to model the
consistency between story context and ending. The goal of this paper is to
adopt contrastive learning to generate endings more consistent with story
context, while there are two main challenges in contrastive learning of SEG.
First is the negative sampling of wrong endings inconsistent with story
contexts. The second challenge is the adaptation of contrastive learning for
SEG. To address these two issues, we propose a novel Contrastive Learning
framework for Story Ending Generation (CLSEG), which has two steps:
multi-aspect sampling and story-specific contrastive learning. Particularly,
for the first issue, we utilize novel multi-aspect sampling mechanisms to
obtain wrong endings considering the consistency of order, causality, and
sentiment. To solve the second issue, we well-design a story-specific
contrastive training strategy that is adapted for SEG. Experiments show that
CLSEG outperforms baselines and can produce story endings with stronger
consistency and rationality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09061">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of pre-training models has brought
uni-modal fields such as computer vision (CV) and natural language processing
(NLP) to a new era. Substantial works have shown they are beneficial for
downstream uni-modal tasks and avoid training a new model from scratch. So can
such pre-trained models be applied to multi-modal tasks? Researchers have
explored this problem and made significant progress. This paper surveys recent
advances and new frontiers in vision-language pre-training (VLP), including
image-text and video-text pre-training. To give readers a better overall grasp
of VLP, we first review its recent advances from five aspects: feature
extraction, model architecture, pre-training objectives, pre-training datasets,
and downstream tasks. Then, we summarize the specific VLP models in detail.
Finally, we discuss the new frontiers in VLP. To the best of our knowledge,
this is the first survey on VLP. We hope that this survey can shed light on
future research in the VLP field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Identity Preservation in Dysarthric Speech Reconstruction by Adversarial Speaker Adaptation. (arXiv:2202.09082v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09082">
<div class="article-summary-box-inner">
<span><p>Dysarthric speech reconstruction (DSR), which aims to improve the quality of
dysarthric speech, remains a challenge, not only because we need to restore the
speech to be normal, but also must preserve the speaker's identity. The speaker
representation extracted by the speaker encoder (SE) optimized for speaker
verification has been explored to control the speaker identity. However, the SE
may not be able to fully capture the characteristics of dysarthric speakers
that are previously unseen. To address this research problem, we propose a
novel multi-task learning strategy, i.e., adversarial speaker adaptation (ASA).
The primary task of ASA fine-tunes the SE with the speech of the target
dysarthric speaker to effectively capture identity-related information, and the
secondary task applies adversarial training to avoid the incorporation of
abnormal speaking patterns into the reconstructed speech, by regularizing the
distribution of reconstructed speech to be close to that of reference speech
with high quality. Experiments show that the proposed approach can achieve
enhanced speaker similarity and comparable speech naturalness with a strong
baseline approach. Compared with dysarthric speech, the reconstructed speech
achieves 22.3% and 31.5% absolute word error rate reduction for speakers with
moderate and moderate-severe dysarthria respectively. Our demo page is released
here: https://wendison.github.io/ASA-DSR-demo/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMS_ADRN at SemEval-2022 Task 5: A Suitable Image-text Multimodal Joint Modeling Method for Multi-task Misogyny Identification. (arXiv:2202.09099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09099">
<div class="article-summary-box-inner">
<span><p>Women are influential online, especially in image-based social media such as
Twitter and Instagram. However, many in the network environment contain gender
discrimination and aggressive information, which magnify gender stereotypes and
gender inequality. Therefore, the filtering of illegal content such as gender
discrimination is essential to maintain a healthy social network environment.
In this paper, we describe the system developed by our team for SemEval-2022
Task 5: Multimedia Automatic Misogyny Identification. More specifically, we
introduce two novel system to analyze these posts: a multimodal multi-task
learning architecture that combines Bertweet for text encoding with ResNet-18
for image representation, and a single-flow transformer structure which
combines text embeddings from BERT-Embeddings and image embeddings from several
different modules such as EfficientNet and ResNet. In this manner, we show that
the information behind them can be properly revealed. Our approach achieves
good performance on each of the two subtasks of the current competition,
ranking 15th for Subtask A (0.746 macro F1-score), 11th for Subtask B (0.706
macro F1-score) while exceeding the official baseline results by high margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Acoustic Characterization of Singaporean Children's English Pronunciation. (arXiv:2202.09108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09108">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate pronunciation differences in English spoken by
Singaporean children in relation to their American and British counterparts by
conducting Kmeans clustering and Archetypal analysis on selected vowel pairs
and approximants. Given that Singapore adopts British English as the
institutional standard due to historical reasons, one might expect Singaporean
children to follow British pronunciation patterns. Indeed, Singaporean and
British children are more similar in their production of syllable-final /r/ --
they do not lower their third formant nearly as much as American children do,
suggesting a lack of rhoticity. Interestingly, Singaporean children also
present similar patterns to American children when it comes to their fronting
of vowels as demonstrated across various vowels including TRAP-BATH split
vowels. Singaporean children's English also demonstrated characteristics that
do not resemble any of the other two populations. We observe that Singaporean
children's vowel height characteristics are distinct from both that of American
and British children. In tense and lax vowel pairs, we also consistently
observe that the distinction is less conspicuous for Singaporean children
compared to the other speaker groups. Further, while American and British
children demonstrate lowering of F1 and F2 formants in transitions into
syllable-final /l/s, a wide gap between F2 and F3 formants, and small
difference between F1 and F2 formants, all of these are not exhibited in
Singaporean children's pronunciation. These findings point towards potential
sociolinguistic implications of how Singapore English might be evolving to
embody more than British pronunciation characteristics. Furthermore, these
findings also suggest that Singapore English could be have been influenced by
languages beyond American and British English, potentially due to Singapore's
multilingual environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modelling the semantics of text in complex document layouts using graph transformer networks. (arXiv:2202.09144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09144">
<div class="article-summary-box-inner">
<span><p>Representing structured text from complex documents typically calls for
different machine learning techniques, such as language models for paragraphs
and convolutional neural networks (CNNs) for table extraction, which prohibits
drawing links between text spans from different content types. In this article
we propose a model that approximates the human reading pattern of a document
and outputs a unique semantic representation for every text span irrespective
of the content type they are found in. We base our architecture on a graph
representation of the structured text, and we demonstrate that not only can we
retrieve semantically similar information across documents but also that the
embedding space we generate captures useful semantic information, similar to
language models that work only on text sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Construct Validity of Text Embeddings with Application to Survey Questions. (arXiv:2202.09166v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09166">
<div class="article-summary-box-inner">
<span><p>Text embedding models from Natural Language Processing can map text data
(e.g. words, sentences, documents) to supposedly meaningful numerical
representations (a.k.a. text embeddings). While such models are increasingly
applied in social science research, one important issue is often not addressed:
the extent to which these embeddings are valid representations of constructs
relevant for social science research. We therefore propose the use of the
classic construct validity framework to evaluate the validity of text
embeddings. We show how this framework can be adapted to the opaque and
high-dimensional nature of text embeddings, with application to survey
questions. We include several popular text embedding methods (e.g. fastText,
GloVe, BERT, Sentence-BERT, Universal Sentence Encoder) in our construct
validity analyses. We find evidence of convergent and discriminant validity in
some cases. We also show that embeddings can be used to predict respondent's
answers to completely new survey questions. Furthermore, BERT-based embedding
techniques and the Universal Sentence Encoder provide more valid
representations of survey questions than do others. Our results thus highlight
the necessity to examine the construct validity of text embeddings before
deploying them in social science research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialectal Layers in West Iranian: a Hierarchical Dirichlet Process Approach to Linguistic Relationships. (arXiv:2001.05297v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.05297">
<div class="article-summary-box-inner">
<span><p>This paper addresses a series of complex and unresolved issues in the
historical phonology of West Iranian languages. The West Iranian languages
(Persian, Kurdish, Balochi, and other languages) display a high degree of
non-Lautgesetzlich behavior. Most of this irregularity is undoubtedly due to
language contact; we argue, however, that an oversimplified view of the
processes at work has prevailed in the literature on West Iranian dialectology,
with specialists assuming that deviations from an expected outcome in a given
non-Persian language are due to lexical borrowing from some chronological stage
of Persian. It is demonstrated that this qualitative approach yields at times
problematic conclusions stemming from the lack of explicit probabilistic
inferences regarding the distribution of the data: Persian may not be the sole
donor language; additionally, borrowing at the lexical level is not always the
mechanism that introduces irregularity. In many cases, the possibility that
West Iranian languages show different reflexes in different conditioning
environments remains under-explored. We employ a novel Bayesian approach
designed to overcome these problems and tease apart the different determinants
of irregularity in patterns of West Iranian sound change. Our methodology
allows us to provisionally resolve a number of outstanding questions in the
literature on West Iranian dialectology concerning the dialectal affiliation of
certain sound changes. We outline future directions for work of this sort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concadia: Tackling Image Accessibility with Descriptive Texts and Context. (arXiv:2104.08376v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08376">
<div class="article-summary-box-inner">
<span><p>Images have become an integral part of online media. This has enhanced the
dissemination of knowledge, but it poses serious accessibility challenges. The
HTML "alt" field is hidden by default and designated for supplying a
description that could replace the image, but it is rarely used. By contrast,
image captions appear alongside the image and are more abundant, but they are
written to supply additional information and generally lack the details
required for accessibility. These terms are often treated as synonyms, but we
argue that a distinction is essential. To address this, we introduce the
publicly available Wikipedia-based corpus Concadia, which consists of 96,918
images with corresponding English-language descriptions, captions, and
surrounding context. We use Concadia to characterize the commonalities and
differences between descriptions and captions. This leads us to the hypothesis
that captions, while not substitutes for descriptions, can provide a useful
signal for creating effective descriptions. We substantiate this hypothesis by
showing that image description systems trained on Concadia benefit from having
caption embeddings as part of their inputs. Finally, we provide evidence from a
human-subjects experiment that human-created captions and descriptions have
distinct communicative purposes, and that our generated texts follow this same
pattern. These experiments begin to show how Concadia can be a powerful tool in
addressing the underlying accessibility issues posed by image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Text Classification via Contrastive Adversarial Training. (arXiv:2107.10137v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10137">
<div class="article-summary-box-inner">
<span><p>We propose a simple and general method to regularize the fine-tuning of
Transformer-based encoders for text classification tasks. Specifically, during
fine-tuning we generate adversarial examples by perturbing the word embeddings
of the model and perform contrastive learning on clean and adversarial examples
in order to teach the model to learn noise-invariant representations. By
training on both clean and adversarial examples along with the additional
contrastive objective, we observe consistent improvement over standard
fine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned
BERT Large model outperforms BERT Large baseline by 1.7% on average, and our
fine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We
additionally validate our method in different domains using three intent
classification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa
Large baseline by 1-2% on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03537">
<div class="article-summary-box-inner">
<span><p>Pre-training language models (LMs) on large-scale unlabeled text data makes
the model much easier to achieve exceptional downstream performance than their
counterparts directly trained on the downstream tasks. In this work, we study
what specific traits in the pre-training data, other than the semantics, make a
pre-trained LM superior to their counterparts trained from scratch on
downstream tasks. We propose to use artificially constructed datasets as the
pre-training data to exclude the effect of semantics, and further control what
characteristics the pre-training corpora have. By fine-tuning the pre-trained
models on GLUE benchmark, we can learn how beneficial it is to transfer the
knowledge from the model trained on the dataset possessing that specific trait.
We define and discuss three different characteristics in the artificial
dataset: 1) matching the token's uni-gram or bi-gram distribution between
pre-training and downstream fine-tuning, 2) the presence of the explicit
dependencies among the tokens in a sequence, 3) the length of the implicit
dependencies among the tokens in a sequence. Our experiments show that the
explicit dependencies in the sequences of the pre-training data are critical to
the downstream performance. Our results also reveal that models achieve better
downstream performance when pre-trained on a dataset with a longer range of
implicit dependencies. Based on our analysis, we find that models pre-trained
with artificial datasets are prone to learn spurious correlation in downstream
tasks. Our work reveals that even if the LMs are not pre-trained on natural
language, they still gain transferability on certain human language downstream
tasks once the LMs learn to model the token dependencies in the sequences. This
result helps us understand the exceptional transferability of pre-trained LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Case-based Reasoning for Better Generalization in Text-Adventure Games. (arXiv:2110.08470v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08470">
<div class="article-summary-box-inner">
<span><p>Text-based games (TBG) have emerged as promising environments for driving
research in grounded language understanding and studying problems like
generalization and sample efficiency. Several deep reinforcement learning (RL)
methods with varying architectures and learning schemes have been proposed for
TBGs. However, these methods fail to generalize efficiently, especially under
distributional shifts. In a departure from deep RL approaches, in this paper,
we propose a general method inspired by case-based reasoning to train agents
and generalize out of the training distribution. The case-based reasoner
collects instances of positive experiences from the agent's interaction with
the world in the past and later reuses the collected experiences to act
efficiently. The method can be applied in conjunction with any existing
on-policy neural agent in the literature for TBGs. Our experiments show that
the proposed approach consistently improves existing methods, obtains good
out-of-distribution generalization, and achieves new state-of-the-art results
on widely used environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two heads are better than one: Enhancing medical representations by pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10113">
<div class="article-summary-box-inner">
<span><p>The massive amount of electronic health records (EHRs) has created enormous
potentials for improving healthcare, among which structured (coded) data and
unstructured (clinical narratives) data are two important textual modalities.
They do not exist in isolation and can complement each other in many real-life
clinical scenarios. Most existing studies in medical informatics, however,
either only focus on a particular modality or apply simple and na\"ive ways to
concatenate data from different modalities, which ignores the interactions
between them. To address these issues, we proposed a Unified Medical Multimodal
Pre-trained Language Model, named UMM-PLM, to jointly learn enhanced
representations from both structured and unstructured EHRs. In UMM-PLM, an
unimodal information extraction module is used to learn representative
characteristics from each data modality respectively, where two
Transformer-based components are adopted. A cross-modal module is then
introduced to model the interactions between the two modalities. We pre-trained
the model on a large EHR dataset containing both structured data and
unstructured data, and verified the effectiveness of the model on three
downstream clinical tasks, i.e., medication recommendation, 30-day readmission,
and ICD coding, through extensive experiments. The results demonstrate the
power of UMM-PLM compared with benchmark methods and state-of-the-art
baselines. Further analyses show that UMM-PLM can effectively integrate
multimodal textual information and potentially provide more comprehensive
interpretations for clinical decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding. (arXiv:2201.11766v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11766">
<div class="article-summary-box-inner">
<span><p>Compositional generalization is a troubling blind spot for neural language
models. Recent efforts have presented techniques for improving a model's
ability to encode novel combinations of known inputs, but less work has focused
on generating novel combinations of known outputs. Here we focus on this latter
"decode-side" form of generalization in the context of gSCAN, a synthetic
benchmark for compositional generalization in grounded language understanding.
We present Recursive Decoding (RD), a novel procedure for training and using
seq2seq models, targeted towards decode-side generalization. Rather than
generating an entire output sequence in one pass, models are trained to predict
one token at a time. Inputs (i.e., the external gSCAN environment) are then
incrementally updated based on predicted tokens, and re-encoded for the next
decoder time step. RD thus decomposes a complex, out-of-distribution sequence
generation task into a series of incremental predictions that each resemble
what the model has already seen during training. RD yields dramatic improvement
on two previously neglected generalization tasks in gSCAN. We provide analyses
to elucidate these gains over failure of a baseline, and then discuss
implications for generalization in naturalistic grounded language
understanding, and seq2seq more generally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RescoreBERT: Discriminative Speech Recognition Rescoring with BERT. (arXiv:2202.01094v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01094">
<div class="article-summary-box-inner">
<span><p>Second-pass rescoring is an important component in automatic speech
recognition (ASR) systems that is used to improve the outputs from a first-pass
decoder by implementing a lattice rescoring or $n$-best re-ranking. While
pretraining with a masked language model (MLM) objective has received great
success in various natural language understanding (NLU) tasks, it has not
gained traction as a rescoring model for ASR. Specifically, training a
bidirectional model like BERT on a discriminative objective such as minimum WER
(MWER) has not been explored. Here we show how to train a BERT-based rescoring
model with MWER loss, to incorporate the improvements of a discriminative loss
into fine-tuning of deep bidirectional pretrained models for ASR. Specifically,
we propose a fusion strategy that incorporates the MLM into the discriminative
training process to effectively distill knowledge from a pretrained model. We
further propose an alternative discriminative loss. This approach, which we
call RescoreBERT, reduces WER by 6.6%/3.4% relative on the LibriSpeech
clean/other test sets over a BERT baseline without discriminative objective. We
also evaluate our method on an internal dataset from a conversational agent and
find that it reduces both latency and WER (by 3 to 8% relative) over an LSTM
rescoring model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers. (arXiv:2202.06690v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06690">
<div class="article-summary-box-inner">
<span><p>The applications of conversational agents for scientific disciplines (as
expert domains) are understudied due to the lack of dialogue data to train such
agents. While most data collection frameworks, such as Amazon Mechanical Turk,
foster data collection for generic domains by connecting crowd workers and task
designers, these frameworks are not much optimized for data collection in
expert domains. Scientists are rarely present in these frameworks due to their
limited time budget. Therefore, we introduce a novel framework to collect
dialogues between scientists as domain experts on scientific papers. Our
framework lets scientists present their scientific papers as groundings for
dialogues and participate in dialogue they like its paper title. We use our
framework to collect a novel argumentative dialogue dataset, ArgSciChat. It
consists of 498 messages collected from 41 dialogues on 20 scientific papers.
Alongside extensive analysis on ArgSciChat, we evaluate a recent conversational
agent on our dataset. Experimental results show that this agent poorly performs
on ArgSciChat, motivating further research on argumentative scientific agents.
We release our framework and the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Based Action-Model Acquisition for Planning. (arXiv:2202.08373v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08373">
<div class="article-summary-box-inner">
<span><p>Although there have been approaches that are capable of learning action
models from plan traces, there is no work on learning action models from
textual observations, which is pervasive and much easier to collect from
real-world applications compared to plan traces. In this paper we propose a
novel approach to learning action models from natural language texts by
integrating Constraint Satisfaction and Natural Language Processing techniques.
Specifically, we first build a novel language model to extract plan traces from
texts, and then build a set of constraints to generate action models based on
the extracted plan traces. After that, we iteratively improve the language
model and constraints until we achieve the convergent language model and action
models. We empirically exhibit that our approach is both effective and
efficient.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Transfer Learning on Satellite Imagery Improves Air Quality Estimates in Developing Nations. (arXiv:2202.08890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08890">
<div class="article-summary-box-inner">
<span><p>Urban air pollution is a public health challenge in low- and middle-income
countries (LMICs). However, LMICs lack adequate air quality (AQ) monitoring
infrastructure. A persistent challenge has been our inability to estimate AQ
accurately in LMIC cities, which hinders emergency preparedness and risk
mitigation. Deep learning-based models that map satellite imagery to AQ can be
built for high-income countries (HICs) with adequate ground data. Here we
demonstrate that a scalable approach that adapts deep transfer learning on
satellite imagery for AQ can extract meaningful estimates and insights in LMIC
cities based on spatiotemporal patterns learned in HIC cities. The approach is
demonstrated for Accra in Ghana, Africa, with AQ patterns learned from two US
cities, specifically Los Angeles and New York.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing Imperceptible Adversarial Patches to Camouflage Military Assets From Computer Vision Enabled Technologies. (arXiv:2202.08892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08892">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have demonstrated rapid progress and a
high level of success in object detection. However, recent evidence has
highlighted their vulnerability to adversarial attacks. These attacks are
calculated image perturbations or adversarial patches that result in object
misclassification or detection suppression. Traditional camouflage methods are
impractical when applied to disguise aircraft and other large mobile assets
from autonomous detection in intelligence, surveillance and reconnaissance
technologies and fifth generation missiles. In this paper we present a unique
method that produces imperceptible patches capable of camouflaging large
military assets from computer vision-enabled technologies. We developed these
patches by maximising object detection loss whilst limiting the patch's colour
perceptibility. This work also aims to further the understanding of adversarial
examples and their effects on object detection algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous-Time vs. Discrete-Time Vision-based SLAM: A Comparative Study. (arXiv:2202.08894v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08894">
<div class="article-summary-box-inner">
<span><p>Robotic practitioners generally approach the vision-based SLAM problem
through discrete-time formulations. This has the advantage of a consolidated
theory and very good understanding of success and failure cases. However,
discrete-time SLAM needs tailored algorithms and simplifying assumptions when
high-rate and/or asynchronous measurements, coming from different sensors, are
present in the estimation process. Conversely, continuous-time SLAM, often
overlooked by practitioners, does not suffer from these limitations. Indeed, it
allows integrating new sensor data asynchronously without adding a new
optimization variable for each new measurement. In this way, the integration of
asynchronous or continuous high-rate streams of sensor data does not require
tailored and highly-engineered algorithms, enabling the fusion of multiple
sensor modalities in an intuitive fashion. On the down side, continuous time
introduces a prior that could worsen the trajectory estimates in some
unfavorable situations. In this work, we aim at systematically comparing the
advantages and limitations of the two formulations in vision-based SLAM. To do
so, we perform an extensive experimental analysis, varying robot type, speed of
motion, and sensor modalities. Our experimental analysis suggests that,
independently of the trajectory type, continuous-time SLAM is superior to its
discrete counterpart whenever the sensors are not time-synchronized. In the
context of this work, we developed, and open source, a modular and efficient
software architecture containing state-of-the-art algorithms to solve the SLAM
problem in discrete and continuous time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine learning models and facial regions videos for estimating heart rate: a review on Patents, Datasets and Literature. (arXiv:2202.08913v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08913">
<div class="article-summary-box-inner">
<span><p>Estimating heart rate is important for monitoring users in various
situations. Estimates based on facial videos are increasingly being researched
because it makes it possible to monitor cardiac information in a non-invasive
way and because the devices are simpler, requiring only cameras that capture
the user's face. From these videos of the user's face, machine learning is able
to estimate heart rate. This study investigates the benefits and challenges of
using machine learning models to estimate heart rate from facial videos,
through patents, datasets, and articles review. We searched Derwent Innovation,
IEEE Xplore, Scopus, and Web of Science knowledge bases and identified 7 patent
filings, 11 datasets, and 20 articles on heart rate, photoplethysmography, or
electrocardiogram data. In terms of patents, we note the advantages of
inventions related to heart rate estimation, as described by the authors. In
terms of datasets, we discovered that most of them are for academic purposes
and with different signs and annotations that allow coverage for subjects other
than heartbeat estimation. In terms of articles, we discovered techniques, such
as extracting regions of interest for heart rate reading and using Video
Magnification for small motion extraction, and models such as EVM-CNN and
VGG-16, that extract the observed individual's heart rate, the best regions of
interest for signal extraction and ways to process them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications. (arXiv:2202.08916v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08916">
<div class="article-summary-box-inner">
<span><p>Image-based characterization and disease understanding involve integrative
analysis of morphological, spatial, and topological information across
biological scales. The development of graph convolutional networks (GCNs) has
created the opportunity to address this information complexity via graph-driven
architectures, since GCNs can perform feature aggregation, interaction, and
reasoning with remarkable flexibility and efficiency. These GCNs capabilities
have spawned a new wave of research in medical imaging analysis with the
overarching goal of improving quantitative disease understanding, monitoring,
and diagnosis. Yet daunting challenges remain for designing the important
image-to-graph transformation for multi-modality medical imaging and gaining
insights into model interpretation and enhanced clinical decision support. In
this review, we present recent GCNs developments in the context of medical
image analysis including imaging data from radiology and histopathology. We
discuss the fast-growing use of graph network architectures in medical image
analysis to improve disease diagnosis and patient outcomes in clinical
practice. To foster cross-disciplinary research, we present GCNs technical
advancements, emerging medical applications, identify common challenges in the
use of image-based GCNs and their extensions in model interpretation,
large-scale benchmarks that promise to transform the scope of medical image
studies and related graph-driven medical research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Guiding Visual Attention with Language Specification. (arXiv:2202.08926v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08926">
<div class="article-summary-box-inner">
<span><p>While real world challenges typically define visual categories with language
words or phrases, most visual classification methods define categories with
numerical indices. However, the language specification of the classes provides
an especially useful prior for biased and noisy datasets, where it can help
disambiguate what features are task-relevant. Recently, large-scale multimodal
models have been shown to recognize a wide variety of high-level concepts from
a language specification even without additional image training data, but they
are often unable to distinguish classes for more fine-grained tasks. CNNs, in
contrast, can extract subtle image features that are required for fine-grained
discrimination, but will overfit to any bias or noise in datasets. Our insight
is to use high-level language specification as advice for constraining the
classification evidence to task-relevant features, instead of distractors. To
do this, we ground task-relevant words or phrases with attention maps from a
pretrained large-scale model. We then use this grounding to supervise a
classifier's spatial attention away from distracting context. We show that
supervising spatial attention in this way improves performance on
classification tasks with biased and noisy data, including about 3-15%
worst-group accuracy improvements and 41-45% relative improvements on fairness
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prior image-based medical image reconstruction using a style-based generative adversarial network. (arXiv:2202.08936v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08936">
<div class="article-summary-box-inner">
<span><p>Computed medical imaging systems require a computational reconstruction
procedure for image formation. In order to recover a useful estimate of the
object to-be-imaged when the recorded measurements are incomplete, prior
knowledge about the nature of object must be utilized. In order to improve the
conditioning of an ill-posed imaging inverse problem, deep learning approaches
are being actively investigated for better representing object priors and
constraints. This work proposes to use a style-based generative adversarial
network (StyleGAN) to constrain an image reconstruction problem in the case
where additional information in the form of a prior image of the sought-after
object is available. An optimization problem is formulated in the intermediate
latent-space of a StyleGAN, that is disentangled with respect to meaningful
image attributes or "styles", such as the contrast used in magnetic resonance
imaging (MRI). Discrepancy between the sought-after and prior images is
measured in the disentangled latent-space, and is used to regularize the
inverse problem in the form of constraints on specific styles of the
disentangled latent-space. A stylized numerical study inspired by MR imaging is
designed, where the sought-after and the prior image are structurally similar,
but belong to different contrast mechanisms. The presented numerical studies
demonstrate the superiority of the proposed approach as compared to classical
approaches in the form of traditional metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When, Why, and Which Pretrained GANs Are Useful?. (arXiv:2202.08937v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08937">
<div class="article-summary-box-inner">
<span><p>The literature has proposed several methods to finetune pretrained GANs on
new datasets, which typically results in higher performance compared to
training from scratch, especially in the limited-data regime. However, despite
the apparent empirical benefits of GAN pretraining, its inner mechanisms were
not analyzed in-depth, and understanding of its role is not entirely clear.
Moreover, the essential practical details, e.g., selecting a proper pretrained
GAN checkpoint, currently do not have rigorous grounding and are typically
determined by trial and error.
</p>
<p>This work aims to dissect the process of GAN finetuning. First, we show that
initializing the GAN training process by a pretrained checkpoint primarily
affects the model's coverage rather than the fidelity of individual samples.
Second, we explicitly describe how pretrained generators and discriminators
contribute to the finetuning process and explain the previous evidence on the
importance of pretraining both of them. Finally, as an immediate practical
benefit of our analysis, we describe a simple recipe to choose an appropriate
GAN checkpoint that is the most suitable for finetuning to a particular target
task. Importantly, for most of the target tasks, Imagenet-pretrained GAN,
despite having poor visual quality, appears to be an excellent starting point
for finetuning, resembling the typical pretraining scenario of discriminative
computer vision models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of ADHD Patients by Kernel Hierarchical Extreme Learning Machine. (arXiv:2202.08953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08953">
<div class="article-summary-box-inner">
<span><p>These days, the diagnosis of neuropsychiatric diseases through brain imaging
technology has received more and more attention. The exploration of
interactions in brain functional connectivity based on functional magnetic
resonance imaging (fMRI) data is critical for the study of mental illness.
Because attention-deficit/hyperactivity disorder (ADHD) is a chronic disease
that affects millions of children, it is difficult to diagnose, so there is
still much space for improvement in the accuracy of the diagnosis of the
disease. In this paper, we consider the dynamics of brain functional
connectivity, modeling a functional brain dynamics model from medical imaging,
which helps to find differences in brain function interactions between normal
control (NC) children and ADHD children. In more detail, our method is used by
Bayesian Connectivity Change Point Model for dynamic detection, Local Binary
Encoding Method for local feature extraction, and Kernel Hierarchical Extreme
Learning Machine implementation classification. To validate our approach,
experimental comparisons of fMRI imaging data on 23 ADHD and 45 NC children
were performed, and our experimental methods achieved better classification
results than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2-D2: Repetitive Reprediction Deep Decipher for Semi-Supervised Deep Learning. (arXiv:2202.08955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08955">
<div class="article-summary-box-inner">
<span><p>Most recent semi-supervised deep learning (deep SSL) methods used a similar
paradigm: use network predictions to update pseudo-labels and use pseudo-labels
to update network parameters iteratively. However, they lack theoretical
support and cannot explain why predictions are good candidates for
pseudo-labels in the deep learning paradigm. In this paper, we propose a
principled end-to-end framework named deep decipher (D2) for SSL. Within the D2
framework, we prove that pseudo-labels are related to network predictions by an
exponential link function, which gives a theoretical support for using
predictions as pseudo-labels. Furthermore, we demonstrate that updating
pseudo-labels by network predictions will make them uncertain. To mitigate this
problem, we propose a training strategy called repetitive reprediction (R2).
Finally, the proposed R2-D2 method is tested on the large-scale ImageNet
dataset and outperforms state-of-the-art methods by 5 percentage points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Efficient Parking Analytics System using Deep Reinforcement Learning. (arXiv:2202.08973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08973">
<div class="article-summary-box-inner">
<span><p>Advances in deep vision techniques and ubiquity of smart cameras will drive
the next generation of video analytics. However, video analytics applications
consume vast amounts of energy as both deep learning techniques and cameras are
power-hungry. In this paper, we focus on a parking video analytics platform and
propose RL-CamSleep, a deep reinforcement learning-based technique, to actuate
the cameras to reduce the energy footprint while retaining the system's
utility. Our key insight is that many video-analytics applications do not
always need to be operational, and we can design policies to activate video
analytics only when necessary. Moreover, our work is complementary to existing
work that focuses on improving hardware and software efficiency. We evaluate
our approach on a city-scale parking dataset having 76 streets spread across
the city. Our analysis demonstrates how streets have various parking patterns,
highlighting the importance of an adaptive policy. Our approach can learn such
an adaptive policy that can reduce the average energy consumption by 76.38% and
achieve an average accuracy of more than 98% in performing video analytics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyclical Focal Loss. (arXiv:2202.08978v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08978">
<div class="article-summary-box-inner">
<span><p>The cross-entropy softmax loss is the primary loss function used to train
deep neural networks. On the other hand, the focal loss function has been
demonstrated to provide improved performance when there is an imbalance in the
number of training samples in each class, such as in long-tailed datasets. In
this paper, we introduce a novel cyclical focal loss and demonstrate that it is
a more universal loss function than cross-entropy softmax loss or focal loss.
We describe the intuition behind the cyclical focal loss and our experiments
provide evidence that cyclical focal loss provides superior performance for
balanced, imbalanced, or long-tailed datasets. We provide numerous experimental
results for CIFAR-10/CIFAR-100, ImageNet, balanced and imbalanced 4,000
training sample versions of CIFAR-10/CIFAR-100, and ImageNet-LT and Places-LT
from the Open Long-Tailed Recognition (OLTR) challenge. Implementing the
cyclical focal loss function requires only a few lines of code and does not
increase training time. In the spirit of reproducibility, our code is available
at \url{https://github.com/lnsmith54/CFL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Learning of Frequency and Spatial Domains for Dense Predictions. (arXiv:2202.08991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08991">
<div class="article-summary-box-inner">
<span><p>Current artificial neural networks mainly conduct the learning process in the
spatial domain but neglect the frequency domain learning. However, the learning
course performed in the frequency domain can be more efficient than that in the
spatial domain. In this paper, we fully explore frequency domain learning and
propose a joint learning paradigm of frequency and spatial domains. This
paradigm can take full advantage of the preponderances of frequency learning
and spatial learning; specifically, frequency and spatial domain learning can
effectively capture global and local information, respectively. Exhaustive
experiments on two dense prediction tasks, i.e., self-supervised depth
estimation and semantic segmentation, demonstrate that the proposed joint
learning paradigm can 1) achieve performance competitive to those of
state-of-the-art methods in both depth estimation and semantic segmentation
tasks, even without pretraining; and 2) significantly reduce the number of
parameters compared to other state-of-the-art methods, which provides more
chance to develop real-world applications. We hope that the proposed method can
encourage more research in cross-domain learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFUGE2 Challenge: Treasure for Multi-Domain Learning in Glaucoma Assessment. (arXiv:2202.08994v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08994">
<div class="article-summary-box-inner">
<span><p>Glaucoma is the second leading cause of blindness and is the leading cause of
irreversible blindness disease in the world. Early screening for glaucoma in
the population is significant. Color fundus photography is the most cost
effective imaging modality to screen for ocular diseases. Deep learning network
is often used in color fundus image analysis due to its powful feature
extraction capability. However, the model training of deep learning method
needs a large amount of data, and the distribution of data should be abundant
for the robustness of model performance. To promote the research of deep
learning in color fundus photography and help researchers further explore the
clinical application signification of AI technology, we held a REFUGE2
challenge. This challenge released 2,000 color fundus images of four models,
including Zeiss, Canon, Kowa and Topcon, which can validate the stabilization
and generalization of algorithms on multi-domain. Moreover, three sub-tasks
were designed in the challenge, including glaucoma classification, cup/optic
disc segmentation, and macular fovea localization. These sub-tasks technically
cover the three main problems of computer vision and clinicly cover the main
researchs of glaucoma diagnosis. Over 1,300 international competitors joined
the REFUGE2 challenge, 134 teams submitted more than 3,000 valid preliminary
results, and 22 teams reached the final. This article summarizes the methods of
some of the finalists and analyzes their results. In particular, we observed
that the teams using domain adaptation strategies had high and robust
performance on the dataset with multi-domain. This indicates that UDA and other
multi-domain related researches will be the trend of deep learning field in the
future, and our REFUGE2 datasets will play an important role in these
researches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Active and Contrastive Learning Framework for Fine-Grained Off-Road Semantic Segmentation. (arXiv:2202.09002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09002">
<div class="article-summary-box-inner">
<span><p>Off-road semantic segmentation with fine-grained labels is necessary for
autonomous vehicles to understand driving scenes, as the coarse-grained road
detection can not satisfy off-road vehicles with various mechanical properties.
Fine-grained semantic segmentation in off-road scenes usually has no unified
category definition due to ambiguous nature environments, and the cost of
pixel-wise labeling is extremely high. Furthermore, semantic properties of
off-road scenes can be very changeable due to various precipitations,
temperature, defoliation, etc. To address these challenges, this research
proposes an active and contrastive learning-based method that does not rely on
pixel-wise labels, but only on patch-based weak annotations for model learning.
There is no need for predefined semantic categories, the contrastive
learning-based feature representation and adaptive clustering will discover the
category model from scene data. In order to actively adapt to new scenes, a
risk evaluation method is proposed to discover and select hard frames with
high-risk predictions for supplemental labeling, so as to update the model
efficiently. Experiments conducted on our self-developed off-road dataset and
DeepScene dataset demonstrate that fine-grained semantic segmentation can be
learned with only dozens of weakly labeled frames, and the model can
efficiently adapt across scenes by weak supervision, while achieving almost the
same level of performance as typical fully supervised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KINet: Keypoint Interaction Networks for Unsupervised Forward Modeling. (arXiv:2202.09006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09006">
<div class="article-summary-box-inner">
<span><p>Object-centric representation is an essential abstraction for physical
reasoning and forward prediction. Most existing approaches learn this
representation through extensive supervision (e.g., object class and bounding
box) although such ground-truth information is not readily accessible in
reality. To address this, we introduce KINet (Keypoint Interaction Network) --
an end-to-end unsupervised framework to reason about object interactions in
complex systems based on a keypoint representation. Using visual observations,
our model learns to associate objects with keypoint coordinates and discovers a
graph representation of the system as a set of keypoint embeddings and their
relations. It then learns an action-conditioned forward model using contrastive
estimation to predict future keypoint states. By learning to perform physical
reasoning in the keypoint space, our model automatically generalizes to
scenarios with a different number of objects, and novel object geometries.
Experiments demonstrate the effectiveness of our model to accurately perform
forward prediction and learn plannable object-centric representations which can
also be used in downstream model-based control tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LG-LSQ: Learned Gradient Linear Symmetric Quantization. (arXiv:2202.09009v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09009">
<div class="article-summary-box-inner">
<span><p>Deep neural networks with lower precision weights and operations at inference
time have advantages in terms of the cost of memory space and accelerator
power. The main challenge associated with the quantization algorithm is
maintaining accuracy at low bit-widths. We propose learned gradient linear
symmetric quantization (LG-LSQ) as a method for quantizing weights and
activation functions to low bit-widths with high accuracy in integer neural
network processors. First, we introduce the scaling simulated gradient (SSG)
method for determining the appropriate gradient for the scaling factor of the
linear quantizer during the training process. Second, we introduce the
arctangent soft round (ASR) method, which differs from the straight-through
estimator (STE) method in its ability to prevent the gradient from becoming
zero, thereby solving the discrete problem caused by the rounding process.
Finally, to bridge the gap between full-precision and low-bit quantization
networks, we propose the minimize discretization error (MDE) method to
determine an accurate gradient in backpropagation. The ASR+MDE method is a
simple alternative to the STE method and is practical for use in different
uniform quantization methods. In our evaluation, the proposed quantizer
achieved full-precision baseline accuracy in various 3-bit networks, including
ResNet18, ResNet34, and ResNet50, and an accuracy drop of less than 1% in the
quantization of 4-bit weights and 4-bit activations in lightweight models such
as MobileNetV2 and ShuffleNetV2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot Learning?. (arXiv:2202.09014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09014">
<div class="article-summary-box-inner">
<span><p>Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem in
the area of computer vision, while self-supervised learning presents a
promising solution. Both learning methods attempt to alleviate the dependency
of deep networks on the requirement of large-scale labeled data. Although
self-supervised methods have recently advanced dramatically, their utility on
CDFSL is relatively unexplored. In this paper, we investigate the role of
self-supervised representation learning in the context of CDFSL via a thorough
evaluation of existing methods. It comes as a surprise that even with shallow
architectures or small training datasets, self-supervised methods can perform
favorably compared to the existing SOTA methods. Nevertheless, no single
self-supervised approach dominates all datasets indicating that existing
self-supervised methods are not universally applicable. In addition, we find
that representations extracted from self-supervised methods exhibit stronger
robustness than the supervised method. Intriguingly, whether self-supervised
representations perform well on the source domain has little correlation with
their applicability on the target domain. As part of our study, we conduct an
objective measurement of the performance for six kinds of representative
classifiers. The results suggest Prototypical Classifier as the standard
evaluation recipe for CDFSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements. (arXiv:2202.09020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09020">
<div class="article-summary-box-inner">
<span><p>With the acceleration of urbanization and living standards, microorganisms
play increasingly important roles in industrial production, bio-technique, and
food safety testing. Microorganism biovolume measurements are one of the
essential parts of microbial analysis. However, traditional manual measurement
methods are time-consuming and challenging to measure the characteristics
precisely. With the development of digital image processing techniques, the
characteristics of the microbial population can be detected and quantified. The
changing trend can be adjusted in time and provided a basis for the
improvement. The applications of the microorganism biovolume measurement method
have developed since the 1980s. More than 60 articles are reviewed in this
study, and the articles are grouped by digital image segmentation methods with
periods. This study has high research significance and application value, which
can be referred to microbial researchers to have a comprehensive understanding
of microorganism biovolume measurements using digital image analysis methods
and potential applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Critical Checkpoints for Evaluating Defence Models Against Adversarial Attack and Robustness. (arXiv:2202.09039v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09039">
<div class="article-summary-box-inner">
<span><p>From past couple of years there is a cycle of researchers proposing a defence
model for adversaries in machine learning which is arguably defensible to most
of the existing attacks in restricted condition (they evaluate on some bounded
inputs or datasets). And then shortly another set of researcher finding the
vulnerabilities in that defence model and breaking it by proposing a stronger
attack model. Some common flaws are been noticed in the past defence models
that were broken in very short time. Defence models being broken so easily is a
point of concern as decision of many crucial activities are taken with the help
of machine learning models. So there is an utter need of some defence
checkpoints that any researcher should keep in mind while evaluating the
soundness of technique and declaring it to be decent defence technique. In this
paper, we have suggested few checkpoints that should be taken into
consideration while building and evaluating the soundness of defence models.
All these points are recommended after observing why some past defence models
failed and how some model remained adamant and proved their soundness against
some of the very strong attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09048">
<div class="article-summary-box-inner">
<span><p>Various models have been proposed to solve the object detection problem.
However, most of them require many hand-designed components to demonstrate good
performance. To mitigate these issues, Transformer based DETR and its variant
Deformable DETR were suggested. They solved much of the complex issue of
designing a head of object detection model but it has not been generally clear
that the Transformer-based models could be considered as the state-of-the-art
method in object detection without doubt. Furthermore, as DETR adapted
Transformer method only for the detection head, but still with including CNN
for the backbone body, it has not been certain that it would be possible to
build the competent end-to-end pipeline with the combination of attention
modules. In this paper, we propose that combining several attention modules
with our new Task Specific Split Transformer(TSST) is a fairly good enough
method to produce the best COCO results without traditionally hand-designed
components. By splitting generally purposed attention module into two separated
mission specific attention module, the proposed method addresses the way to
design simpler object detection models than before. Extensive experiments on
the COCO benchmark demonstrate the effectiveness of our approach. Code is
released at https://github.com/navervision/tsst
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guide Local Feature Matching by Overlap Estimation. (arXiv:2202.09050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09050">
<div class="article-summary-box-inner">
<span><p>Local image feature matching under large appearance, viewpoint, and distance
changes is challenging yet important. Conventional methods detect and match
tentative local features across the whole images, with heuristic consistency
checks to guarantee reliable matches. In this paper, we introduce a novel
Overlap Estimation method conditioned on image pairs with TRansformer, named
OETR, to constrain local feature matching in the commonly visible region. OETR
performs overlap estimation in a two-step process of feature correlation and
then overlap regression. As a preprocessing module, OETR can be plugged into
any existing local feature detection and matching pipeline, to mitigate
potential view angle or scale variance. Intensive experiments show that OETR
can boost state-of-the-art local feature matching performance substantially,
especially for image pairs with small shared regions. The code will be publicly
available at https://github.com/AbyssGaze/OETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards better understanding and better generalization of few-shot classification in histology images with contrastive learning. (arXiv:2202.09059v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09059">
<div class="article-summary-box-inner">
<span><p>Few-shot learning is an established topic in natural images for years, but
few work is attended to histology images, which is of high clinical value since
well-labeled datasets and rare abnormal samples are expensive to collect. Here,
we facilitate the study of few-shot learning in histology images by setting up
three cross-domain tasks that simulate real clinics problems. To enable
label-efficient learning and better generalizability, we propose to incorporate
contrastive learning (CL) with latent augmentation (LA) to build a few-shot
system. CL learns useful representations without manual labels, while LA
transfers semantic variations of the base dataset in an unsupervised way. These
two components fully exploit unlabeled training data and can scale gracefully
to other label-hungry problems. In experiments, we find i) models learned by CL
generalize better than supervised learning for histology images in unseen
classes, and ii) LA brings consistent gains over baselines. Prior studies of
self-supervised learning mainly focus on ImageNet-like images, which only
present a dominant object in their centers. Recent attention has been paid to
images with multi-objects and multi-textures. Histology images are a natural
choice for such a study. We show the superiority of CL over supervised learning
in terms of generalization for such data and provide our empirical
understanding for this observation. The findings in this work could contribute
to understanding how the model generalizes in the context of both
representation learning and histological image analysis. Code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLP: A Survey on Vision-Language Pre-training. (arXiv:2202.09061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09061">
<div class="article-summary-box-inner">
<span><p>In the past few years, the emergence of pre-training models has brought
uni-modal fields such as computer vision (CV) and natural language processing
(NLP) to a new era. Substantial works have shown they are beneficial for
downstream uni-modal tasks and avoid training a new model from scratch. So can
such pre-trained models be applied to multi-modal tasks? Researchers have
explored this problem and made significant progress. This paper surveys recent
advances and new frontiers in vision-language pre-training (VLP), including
image-text and video-text pre-training. To give readers a better overall grasp
of VLP, we first review its recent advances from five aspects: feature
extraction, model architecture, pre-training objectives, pre-training datasets,
and downstream tasks. Then, we summarize the specific VLP models in detail.
Finally, we discuss the new frontiers in VLP. To the best of our knowledge,
this is the first survey on VLP. We hope that this survey can shed light on
future research in the VLP field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion. (arXiv:2202.09081v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09081">
<div class="article-summary-box-inner">
<span><p>Though significant progress has been made for speaker-dependent
Video-to-Speech (VTS) synthesis, little attention is devoted to multi-speaker
VTS that can map silent video to speech, while allowing flexible control of
speaker identity, all in a single system. This paper proposes a novel
multi-speaker VTS system based on cross-modal knowledge transfer from voice
conversion (VC), where vector quantization with contrastive predictive coding
(VQCPC) is used for the content encoder of VC to derive discrete phoneme-like
acoustic units, which are transferred to a Lip-to-Index (Lip2Ind) network to
infer the index sequence of acoustic units. The Lip2Ind network can then
substitute the content encoder of VC to form a multi-speaker VTS system to
convert silent video to acoustic units for reconstructing accurate spoken
content. The VTS system also inherits the advantages of VC by using a speaker
encoder to produce speaker representations to effectively control the speaker
identity of generated speech. Extensive evaluations verify the effectiveness of
proposed approach, which can be applied in both constrained vocabulary and open
vocabulary conditions, achieving state-of-the-art performance in generating
high-quality speech with high naturalness, intelligibility and speaker
similarity. Our demo page is released here:
https://wendison.github.io/VCVTS-demo/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Multi-Drone Detection and 3D-Localization via YOLO. (arXiv:2202.09097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09097">
<div class="article-summary-box-inner">
<span><p>In this work, we present and evaluate a method to perform real-time multiple
drone detection and three-dimensional localization using state-of-the-art
tiny-YOLOv4 object detection algorithm and stereo triangulation. Our computer
vision approach eliminates the need for computationally expensive stereo
matching algorithms, thereby significantly reducing the memory footprint and
making it deployable on embedded systems. Our drone detection system is highly
modular (with support for various detection algorithms) and capable of
identifying multiple drones in a system, with real-time detection accuracy of
up to 77\% with an average FPS of 332 (on Nvidia Titan Xp). We also test the
complete pipeline in AirSim environment, detecting drones at a maximum distance
of 8 meters, with a mean error of $23\%$ of the distance. We also release the
source code for the project, with pre-trained models and the curated synthetic
stereo dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Learning for Instance Segmentation. (arXiv:2202.09110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09110">
<div class="article-summary-box-inner">
<span><p>Instance segmentation is a computer vision task where separate objects in an
image are detected and segmented. State-of-the-art deep neural network models
require large amounts of labeled data in order to perform well in this task.
Making these annotations is time-consuming. We propose for the first time, an
iterative learning and annotation method that is able to detect, segment and
annotate instances in datasets composed of multiple similar objects. The
approach requires minimal human intervention and needs only a bootstrapping set
containing very few annotations. Experiments on two different datasets show the
validity of the approach in different applications related to visual
inspection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Simple and Accurate Human Pose Estimation with Stair Network. (arXiv:2202.09115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09115">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on tackling the precise keypoint coordinates
regression task. Most existing approaches adopt complicated networks with a
large number of parameters, leading to a heavy model with poor
cost-effectiveness in practice. To overcome this limitation, we develop a small
yet discrimicative model called STair Network, which can be simply stacked
towards an accurate multi-stage pose estimation system. Specifically, to reduce
computational cost, STair Network is composed of novel basic feature extraction
blocks which focus on promoting feature diversity and obtaining rich local
representations with fewer parameters, enabling a satisfactory balance on
efficiency and performance. To further improve the performance, we introduce
two mechanisms with negligible computational cost, focusing on feature fusion
and replenish. We demonstrate the effectiveness of the STair Network on two
standard datasets, e.g., 1-stage STair Network achieves a higher accuracy than
HRNet by 5.5% on COCO test dataset with 80\% fewer parameters and 68% fewer
GFLOPs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing Aggregation Functions in GNNs:High-Capacity GNNs via Nonlinear Neighborhood Aggregators. (arXiv:2202.09145v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09145">
<div class="article-summary-box-inner">
<span><p>Graph neural networks (GNNs) have achieved great success in many graph
learning tasks. The main aspect powering existing GNNs is the multi-layer
network architecture to learn the nonlinear graph representations for the
specific learning tasks. The core operation in GNNs is message propagation in
which each node updates its representation by aggregating its neighbors'
representations. Existing GNNs mainly adopt either linear neighborhood
aggregation (mean,sum) or max aggregator in their message propagation. (1) For
linear aggregators, the whole nonlinearity and network's capacity of GNNs are
generally limited due to deeper GNNs usually suffer from over-smoothing issue.
(2) For max aggregator, it usually fails to be aware of the detailed
information of node representations within neighborhood. To overcome these
issues, we re-think the message propagation mechanism in GNNs and aim to
develop the general nonlinear aggregators for neighborhood information
aggregation in GNNs. One main aspect of our proposed nonlinear aggregators is
that they provide the optimally balanced aggregators between max and mean/sum
aggregations. Thus, our aggregators can inherit both (i) high nonlinearity that
increases network's capacity and (ii) detail-sensitivity that preserves the
detailed information of representations together in GNNs' message propagation.
Promising experiments on several datasets show the effectiveness of the
proposed nonlinear aggregators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiRes-NetVLAD: Augmenting Place Recognition Training with Low-Resolution Imagery. (arXiv:2202.09146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09146">
<div class="article-summary-box-inner">
<span><p>Visual Place Recognition (VPR) is a crucial component of 6-DoF localization,
visual SLAM and structure-from-motion pipelines, tasked to generate an initial
list of place match hypotheses by matching global place descriptors. However,
commonly-used CNN-based methods either process multiple image resolutions after
training or use a single resolution and limit multi-scale feature extraction to
the last convolutional layer during training. In this paper, we augment NetVLAD
representation learning with low-resolution image pyramid encoding which leads
to richer place representations. The resultant multi-resolution feature pyramid
can be conveniently aggregated through VLAD into a single compact
representation, avoiding the need for concatenation or summation of multiple
patches in recent multi-scale approaches. Furthermore, we show that the
underlying learnt feature tensor can be combined with existing multi-scale
approaches to improve their baseline performance. Evaluation on 15
viewpoint-varying and viewpoint-consistent benchmarking datasets confirm that
the proposed MultiRes-NetVLAD leads to state-of-the-art Recall@N performance
for global descriptor based retrieval, compared against 11 existing techniques.
Source code is publicly available at
https://github.com/Ahmedest61/MultiRes-NetVLAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images. (arXiv:2202.09179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09179">
<div class="article-summary-box-inner">
<span><p>High-dimensional imaging is becoming increasingly relevant in many fields
from astronomy and cultural heritage to systems biology. Visual exploration of
such high-dimensional data is commonly facilitated by dimensionality reduction.
However, common dimensionality reduction methods do not include spatial
information present in images, such as local texture features, into the
construction of low-dimensional embeddings. Consequently, exploration of such
data is typically split into a step focusing on the attribute space followed by
a step focusing on spatial information, or vice versa. In this paper, we
present a method for incorporating spatial neighborhood information into
distance-based dimensionality reduction methods, such as t-Distributed
Stochastic Neighbor Embedding (t-SNE). We achieve this by modifying the
distance measure between high-dimensional attribute vectors associated with
each pixel such that it takes the pixel's spatial neighborhood into account.
Based on a classification of different methods for comparing image patches, we
explore a number of different approaches. We compare these approaches from a
theoretical and experimental point of view. Finally, we illustrate the value of
the proposed methods by qualitative and quantitative evaluation on synthetic
data and two real-world use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Outdoor Lighting Aggregation on Image Sequences using Transformer Networks. (arXiv:2202.09206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09206">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on outdoor lighting estimation by aggregating
individual noisy estimates from images, exploiting the rich image information
from wide-angle cameras and/or temporal image sequences. Photographs inherently
encode information about the scene's lighting in the form of shading and
shadows. Recovering the lighting is an inverse rendering problem and as that
ill-posed. Recent work based on deep neural networks has shown promising
results for single image lighting estimation, but suffers from robustness. We
tackle this problem by combining lighting estimates from several image views
sampled in the angular and temporal domain of an image sequence. For this task,
we introduce a transformer architecture that is trained in an end-2-end fashion
without any statistical post-processing as required by previous work. Thereby,
we propose a positional encoding that takes into account the camera calibration
and ego-motion estimation to globally register the individual estimates when
computing attention between visual words. We show that our method leads to
improved lighting estimation while requiring less hyper-parameters compared to
the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoencoding Low-Resolution MRI for Semantically Smooth Interpolation of Anisotropic MRI. (arXiv:2202.09258v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09258">
<div class="article-summary-box-inner">
<span><p>High-resolution medical images are beneficial for analysis but their
acquisition may not always be feasible. Alternatively, high-resolution images
can be created from low-resolution acquisitions using conventional upsampling
methods, but such methods cannot exploit high-level contextual information
contained in the images. Recently, better performing deep-learning based
super-resolution methods have been introduced. However, these methods are
limited by their supervised character, i.e. they require high-resolution
examples for training. Instead, we propose an unsupervised deep learning
semantic interpolation approach that synthesizes new intermediate slices from
encoded low-resolution examples. To achieve semantically smooth interpolation
in through-plane direction, the method exploits the latent space generated by
autoencoders. To generate new intermediate slices, latent space encodings of
two spatially adjacent slices are combined using their convex combination.
Subsequently, the combined encoding is decoded to an intermediate slice. To
constrain the model, a notion of semantic similarity is defined for a given
dataset. For this, a new loss is introduced that exploits the spatial
relationship between slices of the same volume. During training, an existing
in-between slice is generated using a convex combination of its neighboring
slice encodings. The method was trained and evaluated using publicly available
cardiac cine, neonatal brain and adult brain MRI scans. In all evaluations, the
new method produces significantly better results in terms of Structural
Similarity Index Measure and Peak Signal-to-Noise Ratio (p&lt; 0.001 using
one-sided Wilcoxon signed-rank test) than a cubic B-spline interpolation
approach. Given the unsupervised nature of the method, high-resolution training
data is not required and hence, the method can be readily applied in clinical
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering. (arXiv:2202.09277v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09277">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal scene-graph approaches to video-based reasoning tasks such as
video question-answering (QA) typically construct such graphs for every video
frame. Such approaches often ignore the fact that videos are essentially
sequences of 2D "views" of events happening in a 3D space, and that the
semantics of the 3D scene can thus be carried over from frame to frame.
Leveraging this insight, we propose a (2.5+1)D scene graph representation to
better capture the spatio-temporal information flows inside the videos.
Specifically, we first create a 2.5D (pseudo-3D) scene graph by transforming
every 2D frame to have an inferred 3D structure using an off-the-shelf 2D-to-3D
transformation module, following which we register the video frames into a
shared (2.5+1)D spatio-temporal space and ground each 2D scene graph within it.
Such a (2.5+1)D graph is then segregated into a static sub-graph and a dynamic
sub-graph, corresponding to whether the objects within them usually move in the
world. The nodes in the dynamic graph are enriched with motion features
capturing their interactions with other graph nodes. Next, for the video QA
task, we present a novel transformer-based reasoning pipeline that embeds the
(2.5+1)D graph into a spatio-temporal hierarchical latent space, where the
sub-graphs and their interactions are captured at varied granularity. To
demonstrate the effectiveness of our approach, we present experiments on the
NExT-QA and AVSD-QA datasets. Our results show that our proposed (2.5+1)D
representation leads to faster training and inference, while our hierarchical
model showcases superior performance on the video QA task versus the state of
the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Adversarially Robust Training for Unsupervised Domain Adaptation. (arXiv:2202.09300v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09300">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) methods aim to transfer knowledge from a
labeled source domain to an unlabeled target domain. UDA has been extensively
studied in the computer vision literature. Deep networks have been shown to be
vulnerable to adversarial attacks. However, very little focus is devoted to
improving the adversarial robustness of deep UDA models, causing serious
concerns about model reliability. Adversarial Training (AT) has been considered
to be the most successful adversarial defense approach. Nevertheless,
conventional AT requires ground-truth labels to generate adversarial examples
and train models, which limits its effectiveness in the unlabeled target
domain. In this paper, we aim to explore AT to robustify UDA models: How to
enhance the unlabeled data robustness via AT while learning domain-invariant
features for UDA? To answer this, we provide a systematic study into multiple
AT variants that potentially apply to UDA. Moreover, we propose a novel
Adversarially Robust Training method for UDA accordingly, referred to as
ARTUDA. Extensive experiments on multiple attacks and benchmarks show that
ARTUDA consistently improves the adversarial robustness of UDA models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multiple-Object Tracking with a Dynamical Variational Autoencoder. (arXiv:2202.09315v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09315">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an unsupervised probabilistic model and associated
estimation algorithm for multi-object tracking (MOT) based on a dynamical
variational autoencoder (DVAE), called DVAE-UMOT. The DVAE is a latent-variable
deep generative model that can be seen as an extension of the variational
autoencoder for the modeling of temporal sequences. It is included in DVAE-UMOT
to model the objects' dynamics, after being pre-trained on an unlabeled
synthetic dataset of single-object trajectories. Then the distributions and
parameters of DVAE-UMOT are estimated on each multi-object sequence to track
using the principles of variational inference: Definition of an approximate
posterior distribution of the latent variables and maximization of the
corresponding evidence lower bound of the data likehood function. DVAE-UMOT is
shown experimentally to compete well with and even surpass the performance of
two state-of-the-art probabilistic MOT models. Code and data are publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable's Clouds More Real than His Contemporaries?. (arXiv:2202.09348v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09348">
<div class="article-summary-box-inner">
<span><p>European artists have sought to create life-like images since the
Renaissance. The techniques used by artists to impart realism to their
paintings often rely on approaches based in mathematics, like linear
perspective; yet the means used to assess the verisimilitude of realist
paintings have remained subjective, even intuitive. An exploration of
alternative and relatively objective methods for evaluating pictorial realism
could enhance existing art historical research. We propose a
machine-learning-based paradigm for studying pictorial realism in an
explainable way. Unlike subjective evaluations made by art historians or
computer-based painting analysis exploiting inexplicable learned features, our
framework assesses realism by measuring the similarity between clouds painted
by exceptionally skillful 19th-century landscape painters like John Constable
and photographs of clouds. The experimental results of cloud classification
show that Constable approximates more consistently than his contemporaries the
formal features of actual clouds in his paintings. Our analyses suggest that
artists working in the decades leading up to the invention of photography
worked in a mode that anticipated some of the stylistic features of
photography. The study is a springboard for deeper analyses of pictorial
realism using computer vision and machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No-Reference Light Field Image Quality Assessment Based on Spatial-Angular Measurement. (arXiv:1908.06280v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.06280">
<div class="article-summary-box-inner">
<span><p>Light field image quality assessment (LFI-QA) is a significant and
challenging research problem. It helps to better guide light field acquisition,
processing and applications. However, only a few objective models have been
proposed and none of them completely consider intrinsic factors affecting the
LFI quality. In this paper, we propose a No-Reference Light Field image Quality
Assessment (NR-LFQA) scheme, where the main idea is to quantify the LFI quality
degradation through evaluating the spatial quality and angular consistency. We
first measure the spatial quality deterioration by capturing the naturalness
distribution of the light field cyclopean image array, which is formed when
human observes the LFI. Then, as a transformed representation of LFI, the
Epipolar Plane Image (EPI) contains the slopes of lines and involves the
angular information. Therefore, EPI is utilized to extract the global and local
features from LFI to measure angular consistency degradation. Specifically, the
distribution of gradient direction map of EPI is proposed to measure the global
angular consistency distortion in the LFI. We further propose the weighted
local binary pattern to capture the characteristics of local angular
consistency degradation. Extensive experimental results on four publicly
available LFI quality datasets demonstrate that the proposed method outperforms
state-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Oriented No-Reference Light Field Image Quality Assessment. (arXiv:1909.02358v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.02358">
<div class="article-summary-box-inner">
<span><p>Light field image (LFI) quality assessment is becoming more and more
important, which helps to better guide the acquisition, processing and
application of immersive media. However, due to the inherent high dimensional
characteristics of LFI, the LFI quality assessment turns into a
multi-dimensional problem that requires consideration of the quality
degradation in both spatial and angular dimensions. Therefore, we propose a
novel Tensor oriented No-reference Light Field image Quality evaluator
(Tensor-NLFQ) based on tensor theory. Specifically, since the LFI is regarded
as a low-rank 4D tensor, the principal components of four oriented sub-aperture
view stacks are obtained via Tucker decomposition. Then, the Principal
Component Spatial Characteristic (PCSC) is designed to measure the
spatial-dimensional quality of LFI considering its global naturalness and local
frequency properties. Finally, the Tensor Angular Variation Index (TAVI) is
proposed to measure angular consistency quality by analyzing the structural
similarity distribution between the first principal component and each view in
the view stack. Extensive experimental results on four publicly available LFI
quality databases demonstrate that the proposed Tensor-NLFQ model outperforms
state-of-the-art 2D, 3D, multi-view, and LFI quality assessment algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point and Ask: Incorporating Pointing into Visual Question Answering. (arXiv:2011.13681v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13681">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) has become one of the key benchmarks of
visual recognition progress. Multiple VQA extensions have been explored to
better simulate real-world settings: different question formulations, changing
training and test distributions, conversational consistency in dialogues, and
explanation-based answering. In this work, we further expand this space by
considering visual questions that include a spatial point of reference.
Pointing is a nearly universal gesture among humans, and real-world VQA is
likely to involve a gesture towards the target region.
</p>
<p>Concretely, we (1) introduce and motivate point-input questions as an
extension of VQA, (2) define three novel classes of questions within this
space, and (3) for each class, introduce both a benchmark dataset and a series
of baseline models to handle its unique challenges. There are two key
distinctions from prior work. First, we explicitly design the benchmarks to
require the point input, i.e., we ensure that the visual question cannot be
answered accurately without the spatial reference. Second, we explicitly
explore the more realistic point spatial input rather than the standard but
unnatural bounding box input. Through our exploration we uncover and address
several visual recognition challenges, including the ability to infer human
intent, reason both locally and globally about the image, and effectively
combine visual, language and spatial inputs. Code is available at:
https://github.com/princetonvisualai/pointingqa .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target Detection and Segmentation in Circular-Scan Synthetic-Aperture-Sonar Images using Semi-Supervised Convolutional Encoder-Decoders. (arXiv:2101.03603v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03603">
<div class="article-summary-box-inner">
<span><p>We propose a framework for saliency-based, multi-target detection and
segmentation of circular-scan, synthetic-aperture-sonar (CSAS) imagery. Our
framework relies on a multi-branch, convolutional encoder-decoder network
(MB-CEDN). The encoder portion of the MB-CEDN extracts visual contrast features
from CSAS images. These features are fed into dual decoders that perform
pixel-level segmentation to mask targets. Each decoder provides different
perspectives as to what constitutes a salient target. These opinions are
aggregated and cascaded into a deep-parsing network to refine the segmentation.
</p>
<p>We evaluate our framework using real-world CSAS imagery consisting of five
broad target classes. We compare against existing approaches from the
computer-vision literature. We show that our framework outperforms supervised,
deep-saliency networks designed for natural imagery. It greatly outperforms
unsupervised saliency approaches developed for natural imagery. This
illustrates that natural-image-based models may need to be altered to be
effective for this imaging-sonar modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Doesn't Kill You Makes You Robust(er): How to Adversarially Train against Data Poisoning. (arXiv:2102.13624v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13624">
<div class="article-summary-box-inner">
<span><p>Data poisoning is a threat model in which a malicious actor tampers with
training data to manipulate outcomes at inference time. A variety of defenses
against this threat model have been proposed, but each suffers from at least
one of the following flaws: they are easily overcome by adaptive attacks, they
severely reduce testing performance, or they cannot generalize to diverse data
poisoning threat models. Adversarial training, and its variants, are currently
considered the only empirically strong defense against (inference-time)
adversarial attacks. In this work, we extend the adversarial training framework
to defend against (training-time) data poisoning, including targeted and
backdoor attacks. Our method desensitizes networks to the effects of such
attacks by creating poisons during training and injecting them into training
batches. We show that this defense withstands adaptive attacks, generalizes to
diverse threat models, and incurs a better performance trade-off than previous
defenses such as DP-SGD or (evasion) adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Compression Auto-Encoder for Detecting Road Surface Abnormality via Vehicle Driving Noise. (arXiv:2103.12992v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12992">
<div class="article-summary-box-inner">
<span><p>Road accident can be triggered by wet road because it decreases skid
resistance. To prevent the road accident, detecting road surface abnomality is
highly useful. In this paper, we propose the deep learning based cost-effective
real-time anomaly detection architecture, naming with non-compression
auto-encoder (NCAE). The proposed architecture can reflect forward and backward
causality of time series information via convolutional operation. Moreover, the
above architecture shows higher anomaly detection performance of published
anomaly detection model via experiments. We conclude that NCAE as a
cutting-edge model for road surface anomaly detection with 4.20\% higher AUROC
and 2.99 times faster decision than before.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Local Descriptor for Improved Few-Shot Classification. (arXiv:2103.16009v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16009">
<div class="article-summary-box-inner">
<span><p>Few-shot classification studies the problem of quickly adapting a deep
learner to understanding novel classes based on few support images. In this
context, recent research efforts have been aimed at designing more and more
complex classifiers that measure similarities between query and support images,
but left the importance of feature embeddings seldom explored. We show that the
reliance on sophisticated classifiers is not necessary, and a simple classifier
applied directly to improved feature embeddings can instead outperform most of
the leading methods in the literature. To this end, we present a new method
named \textbf{DCAP} for few-shot classification, in which we investigate how
one can improve the quality of embeddings by leveraging \textbf{D}ense
\textbf{C}lassification and \textbf{A}ttentive \textbf{P}ooling. Specifically,
we propose to train a learner on base classes with abundant samples to solve
dense classification problem first and then meta-train the learner on a bunch
of randomly sampled few-shot tasks to adapt it to few-shot scenario or the test
time scenario. During meta-training, we suggest to pool feature maps by
applying attentive pooling instead of the widely used global average pooling
(GAP) to prepare embeddings for few-shot classification. Attentive pooling
learns to reweight local descriptors, explaining what the learner is looking
for as evidence for decision making. Experiments on two benchmark datasets show
the proposed method to be superior in multiple few-shot settings while being
simpler and more explainable. Code is available at:
\url{https://github.com/Ukeyboard/dcap/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09124">
<div class="article-summary-box-inner">
<span><p>While self-supervised representation learning (SSL) has received widespread
attention from the community, recent research argue that its performance will
suffer a cliff fall when the model size decreases. The current method mainly
relies on contrastive learning to train the network and in this work, we
propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease
the issue by a large margin. Specifically, we find the final embedding obtained
by the mainstream SSL methods contains the most fruitful information, and
propose to distill the final embedding to maximally transmit a teacher's
knowledge to a lightweight model by constraining the last embedding of the
student to be consistent with that of the teacher. In addition, in the
experiment, we find that there exists a phenomenon termed Distilling BottleNeck
and present to enlarge the embedding dimension to alleviate this problem. Our
method does not introduce any extra parameter to lightweight models during
deployment. Experimental results demonstrate that our method achieves the
state-of-the-art on all lightweight models. Particularly, when
ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear
result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,
but the number of parameters of EfficientNet-B0 is only 9.4\%/16.3\% of
ResNet-101/ResNet-50. Code is available at https://github.
com/Yuting-Gao/DisCo-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-Aware Long Short-Term Memory Network for 3D Cephalometric Landmark Detection. (arXiv:2107.09899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09899">
<div class="article-summary-box-inner">
<span><p>Detecting 3D landmarks on cone-beam computed tomography (CBCT) is crucial to
assessing and quantifying the anatomical abnormalities in 3D cephalometric
analysis. However, the current methods are time-consuming and suffer from large
biases in landmark localization, leading to unreliable diagnosis results. In
this work, we propose a novel Structure-Aware Long Short-Term Memory framework
(SA-LSTM) for efficient and accurate 3D landmark detection. To reduce the
computational burden, SA-LSTM is designed in two stages. It first locates the
coarse landmarks via heatmap regression on a down-sampled CBCT volume and then
progressively refines landmarks by attentive offset regression using
multi-resolution cropped patches. To boost accuracy, SA-LSTM captures
global-local dependence among the cropping patches via self-attention.
Specifically, a novel graph attention module implicitly encodes the landmark's
global structure to rationalize the predicted position. Moreover, a novel
attention-gated module recursively filters irrelevant local features and
maintains high-confident local predictions for aggregating the final result.
Experiments conducted on an in-house dataset and a public dataset show that our
method outperforms state-of-the-art methods, achieving 1.64 mm and 2.37 mm
average errors, respectively. Furthermore, our method is very efficient, taking
only 0.5 seconds for inferring the whole CBCT volume of resolution
768$\times$768$\times$576.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Knowledge Distillation for Efficient Pose Estimation. (arXiv:2108.02092v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02092">
<div class="article-summary-box-inner">
<span><p>Existing state-of-the-art human pose estimation methods require heavy
computational resources for accurate predictions. One promising technique to
obtain an accurate yet lightweight pose estimator is knowledge distillation,
which distills the pose knowledge from a powerful teacher model to a
less-parameterized student model. However, existing pose distillation works
rely on a heavy pre-trained estimator to perform knowledge transfer and require
a complex two-stage learning procedure. In this work, we investigate a novel
Online Knowledge Distillation framework by distilling Human Pose structure
knowledge in a one-stage manner to guarantee the distillation efficiency,
termed OKDHP. Specifically, OKDHP trains a single multi-branch network and
acquires the predicted heatmaps from each, which are then assembled by a
Feature Aggregation Unit (FAU) as the target heatmaps to teach each branch in
reverse. Instead of simply averaging the heatmaps, FAU which consists of
multiple parallel transformations with different receptive fields, leverages
the multi-scale information, thus obtains target heatmaps with higher-quality.
Specifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to
minimize the discrepancy between the target heatmaps and the predicted ones,
which enables the student network to learn the implicit keypoint relationship.
Besides, an unbalanced OKDHP scheme is introduced to customize the student
networks with different compression rates. The effectiveness of our approach is
demonstrated by extensive experiments on two common benchmark datasets, MPII
and COCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Adaptation for Cross-Domain Object Detection. (arXiv:2110.02578v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02578">
<div class="article-summary-box-inner">
<span><p>Cross-domain object detection is more challenging than object classification
since multiple objects exist in an image and the location of each object is
unknown in the unlabeled target domain. As a result, when we adapt features of
different objects to enhance the transferability of the detector, the features
of the foreground and the background are easy to be confused, which may hurt
the discriminability of the detector. Besides, previous methods focused on
category adaptation but ignored another important part for object detection,
i.e., the adaptation on bounding box regression. To this end, we propose
D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation
and the training of the detector. Besides, we fill the blank of regression
domain adaptation in object detection by introducing a bounding box adaptor.
Experiments show that D-adapt achieves state-of-the-art results on four
cross-domain object detection tasks and yields 17% and 21% relative improvement
on benchmark datasets Clipart1k and Comic2k in particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth360: Self-supervised Learning for Monocular Depth Estimation using Learnable Camera Distortion Model. (arXiv:2110.10415v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10415">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth estimation has been widely investigated to
estimate depth images and relative poses from RGB images. This framework is
attractive for researchers because the depth and pose networks can be trained
from just time sequence images without the need for the ground truth depth and
poses.
</p>
<p>In this work, we estimate the depth around a robot (360 degree view) using
time sequence spherical camera images, from a camera whose parameters are
unknown. We propose a learnable axisymmetric camera model which accepts
distorted spherical camera images with two fisheye camera images. In addition,
we trained our models with a photo-realistic simulator to generate ground truth
depth images to provide supervision. Moreover, we introduced loss functions to
provide floor constraints to reduce artifacts that can result from reflective
floor surfaces. We demonstrate the efficacy of our method using the spherical
camera images from the GO Stanford dataset and pinhole camera images from the
KITTI dataset to compare our method's performance with that of baseline method
in learning the camera parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vertebrae segmentation, identification and localization using a graph optimization and a synergistic cycle. (arXiv:2110.12177v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12177">
<div class="article-summary-box-inner">
<span><p>This paper considers the segmentation, identification and localization of
vertebrae in CT images. Although these three tasks are related, they face
specific problems that add up when they are addressed together. For example
neighboring vertebrae with similar shapes perturb the identification and
vertebrae with complex or even pathological morphologies impact the
segmentation. Consequently, the three tasks tend to be approached
independently, e.g. labelling (localization and identification) or segmenting
only, or, when treated globally, a sequential strategy is used. Sequential
methods however are prone to accumulate errors as they are not able to recover
from mistakes of the previous module. In this work, we propose to combine all
three tasks and leverage their interdependence: locations ease the
segmentation, the segmentations in turn improve the locations and they all
contribute and benefit from the identification task. To this purpose we propose
a virtuous cycle to enforce coherence between the three tasks. Within such a
cycle, the tasks interoperate and are iterated until a global consistency
criterion is satisfied. Our experiments validate this strategy with
anatomically coherent results that outperform the state of the art on the
VerSe20 challenge benchmark. Our code and model are openly available for
research purposes at https://gitlab.inria.fr/spine/vertebrae_segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Motion History Images with 3D Convolutional Networks in Isolated Sign Language Recognition. (arXiv:2110.12396v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12396">
<div class="article-summary-box-inner">
<span><p>Sign language recognition using computational models is a challenging problem
that requires simultaneous spatio-temporal modeling of the multiple sources,
i.e. faces, hands, body, etc. In this paper, we propose an isolated sign
language recognition model based on a model trained using Motion History Images
(MHI) that are generated from RGB video frames. RGB-MHI images represent
spatio-temporal summary of each sign video effectively in a single RGB image.
We propose two different approaches using this RGB-MHI model. In the first
approach, we use the RGB-MHI model as a motion-based spatial attention module
integrated into a 3D-CNN architecture. In the second approach, we use RGB-MHI
model features directly with the features of a 3D-CNN model using a late fusion
technique. We perform extensive experiments on two recently released
large-scale isolated sign language datasets, namely AUTSL and BosphorusSign22k.
Our experiments show that our models, which use only RGB data, can compete with
the state-of-the-art models in the literature that use multi-modal data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAQF: Fully Automatic Software-Hardware Co-Design Framework for Low-Bit Vision Transformer. (arXiv:2201.06618v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06618">
<div class="article-summary-box-inner">
<span><p>The transformer architectures with attention mechanisms have obtained success
in Nature Language Processing (NLP), and Vision Transformers (ViTs) have
recently extended the application domains to various vision tasks. While
achieving high performance, ViTs suffer from large model size and high
computation complexity that hinders the deployment of them on edge devices. To
achieve high throughput on hardware and preserve the model accuracy
simultaneously, we propose VAQF, a framework that builds inference accelerators
on FPGA platforms for quantized ViTs with binary weights and low-precision
activations. Given the model structure and the desired frame rate, VAQF will
automatically output the required quantization precision for activations as
well as the optimized parameter settings of the accelerator that fulfill the
hardware requirements. The implementations are developed with Vivado High-Level
Synthesis (HLS) on the Xilinx ZCU102 FPGA board, and the evaluation results
with the DeiT-base model indicate that a frame rate requirement of 24 frames
per second (FPS) is satisfied with 8-bit activation quantization, and a target
of 30 FPS is met with 6-bit activation quantization. To the best of our
knowledge, this is the first time quantization has been incorporated into ViT
acceleration on FPGAs with the help of a fully automatic framework to guide the
quantization strategy on the software side and the accelerator implementations
on the hardware side given the target frame rate. Very small compilation time
cost is incurred compared with quantization training, and the generated
accelerators show the capability of achieving real-time execution for
state-of-the-art ViT models on FPGAs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Balanced Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12559">
<div class="article-summary-box-inner">
<span><p>Batch Normalization (BN) is an essential layer for training neural network
models in various computer vision tasks. It has been widely used in continual
learning scenarios with little discussion, but we find that BN should be
carefully applied, particularly for the exemplar memory based class incremental
learning (CIL). We first analyze that the empirical mean and variance obtained
for normalization in a BN layer become highly biased toward the current task.
To tackle its significant problems in training and test phases, we propose
Task-Balanced Batch Normalization (TBBN). Given each mini-batch imbalanced
between the current and previous tasks, TBBN first reshapes and repeats the
batch, calculating near task-balanced mean and variance. Second, we show that
when the affine transformation parameters of BN are learned from a reshaped
feature map, they become less-biased toward the current task. Based on our
extensive CIL experiments with CIFAR-100 and ImageNet-100 datasets, we
demonstrate that our TBBN is easily applicable to most of existing
exemplar-based CIL algorithms, improving their performance by decreasing the
forgetting on the previous tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Online Meta-Learning Without Task Boundaries. (arXiv:2202.00263v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00263">
<div class="article-summary-box-inner">
<span><p>While deep networks can learn complex functions such as classifiers,
detectors, and trackers, many applications require models that continually
adapt to changing input distributions, changing tasks, and changing
environmental conditions. Indeed, this ability to continuously accrue knowledge
and use past experience to learn new tasks quickly in continual settings is one
of the key properties of an intelligent system. For complex and
high-dimensional problems, simply updating the model continually with standard
learning algorithms such as gradient descent may result in slow adaptation.
Meta-learning can provide a powerful tool to accelerate adaptation yet is
conventionally studied in batch settings. In this paper, we study how
meta-learning can be applied to tackle online problems of this nature,
simultaneously adapting to changing tasks and input distributions and
meta-training the model in order to adapt more quickly in the future. Extending
meta-learning into the online setting presents its own challenges, and although
several prior methods have studied related problems, they generally require a
discrete notion of tasks, with known ground-truth task boundaries. Such methods
typically adapt to each task in sequence, resetting the model between tasks,
rather than adapting continuously across tasks. In many real-world settings,
such discrete boundaries are unavailable, and may not even exist. To address
these settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which
does not require any ground truth knowledge about the task boundaries and stays
fully online without resetting back to pre-trained weights. Our experiments
show that FOML was able to learn new tasks faster than the state-of-the-art
online learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00886">
<div class="article-summary-box-inner">
<span><p>Multi-perspective cameras are quickly gaining importance in many applications
such as smart vehicles and virtual or augmented reality. However, a large
system size or absence of overlap in neighbouring fields-of-view often
complicate their calibration. We present a novel solution which relies on the
availability of an external motion capture system. Our core contribution
consists of an extension to the hand-eye calibration problem which jointly
solves multi-eye-to-base problems in closed form. We furthermore demonstrate
its equivalence to the multi-eye-in-hand problem. The practical validity of our
approach is supported by our experiments, indicating that the method is highly
efficient and accurate, and outperforms existing closed-form alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSHA: Video Violence Recognition and Localization Using a Semi-Supervised Hard Attention Model. (arXiv:2202.02212v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02212">
<div class="article-summary-box-inner">
<span><p>Empowering automated violence monitoring and surveillance systems amid the
growing social violence and extremist activities worldwide could keep
communities safe and save lives. The questionable reliability of human
monitoring personnel and the increasing number of surveillance cameras makes
automated artificial intelligence-based solutions compelling. Improving the
current state-of-the-art deep learning approaches to video violence recognition
to higher levels of accuracy and performance could enable surveillance systems
to be more reliable and scalable. The main contribution of the proposed deep
reinforcement learning method is to achieve state-of-the-art accuracy on RWF,
Hockey, and Movies datasets while removing some of the computationally
expensive processes and input features used in the previous solutions. The
implementation of hard attention using a semi-supervised learning method made
the proposed method capable of rough violence localization and added increased
agent interpretability to the violence detection system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-driven Neural Physically-based Facial Asset for Production. (arXiv:2202.05592v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05592">
<div class="article-summary-box-inner">
<span><p>Production-level workflows for producing convincing 3D dynamic human faces
have long relied on a disarray of labor-intensive tools for geometry and
texture generation, motion capture and rigging, and expression synthesis.
Recent neural approaches automate individual components but the corresponding
latent representations cannot provide artists explicit controls as in
conventional tools. In this paper, we present a new learning-based,
video-driven approach for generating dynamic facial geometries with
high-quality physically-based assets. Two key components are well-structured
latent spaces due to dense temporal samplings from videos and explicit facial
expression controls to regulate the latent spaces. For data collection, we
construct a hybrid multiview-photometric capture stage, coupling with an
ultra-fast video camera to obtain raw 3D facial assets. We then model the
facial expression, geometry and physically-based textures using separate VAEs
with a global MLP-based expression mapping across the latent spaces, to
preserve characteristics across respective attributes while maintaining
explicit controls over geometry and texture. We also introduce to model the
delta information as wrinkle maps for physically-base textures, achieving
high-quality rendering of dynamic textures. We demonstrate our approach in
high-fidelity performer-specific facial capture and cross-identity facial
motion retargeting. In addition, our neural asset along with fast adaptation
schemes can also be deployed to handle in-the-wild videos. Besides, we motivate
the utility of our explicit facial disentangle strategy by providing promising
physically-based editing results like geometry and material editing or winkle
transfer with high realism. Comprehensive experiments show that our technique
provides higher accuracy and visual fidelity than previous video-driven facial
reconstruction and animation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation. (arXiv:2202.06344v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06344">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of glioma and its subregions is of great significance
for diagnosis, treatment and monitoring of disease. In this paper, an
augmentation method, called TensorMixup, was proposed and applied to the three
dimensional U-Net architecture for brain tumor segmentation. The main ideas
included that first, two image patches with size of 128 in three dimensions
were selected according to glioma information of ground truth labels from the
magnetic resonance imaging data of any two patients with the same modality.
Next, a tensor in which all elements were independently sampled from Beta
distribution was used to mix the image patches. Then the tensor was mapped to a
matrix which was used to mix the one-hot encoded labels of the above image
patches. Therefore, a new image and its one-hot encoded label were synthesized.
Finally, the new data was used to train the model which could be used to
segment glioma. The experimental results show that the mean accuracy of Dice
scores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor
core, and enhancing tumor segmentation, which proves that the proposed
TensorMixup is feasible and effective for brain tumor segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased Pseudo Labeling in Self-Training. (arXiv:2202.07136v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07136">
<div class="article-summary-box-inner">
<span><p>Deep neural networks achieve remarkable performances on a wide range of tasks
with the aid of large-scale labeled datasets. However, large-scale annotations
are time-consuming and labor-exhaustive to obtain on realistic tasks. To
mitigate the requirement for labeled data, self-training is widely used in both
academia and industry by pseudo labeling on readily-available unlabeled data.
Despite its popularity, pseudo labeling is well-believed to be unreliable and
often leads to training instability. Our experimental studies further reveal
that the performance of self-training is biased due to data sampling,
pre-trained models, and training strategies, especially the inappropriate
utilization of pseudo labels. To this end, we propose Debiased, in which the
generation and utilization of pseudo labels are decoupled by two independent
heads. To further improve the quality of pseudo labels, we introduce a
worst-case estimation of pseudo labeling and seamlessly optimize the
representations to avoid the worst-case. Extensive experiments justify that the
proposed Debiased not only yields an average improvement of $14.4$\% against
state-of-the-art algorithms on $11$ tasks (covering generic object recognition,
fine-grained object recognition, texture classification, and scene
classification) but also helps stabilize training and balance performance
across classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Human Sperm Head Morphology Classification with Unsupervised Anatomical Feature Distillation. (arXiv:2202.07191v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07191">
<div class="article-summary-box-inner">
<span><p>With rising male infertility, sperm head morphology classification becomes
critical for accurate and timely clinical diagnosis. Recent deep learning (DL)
morphology analysis methods achieve promising benchmark results, but leave
performance and robustness on the table by relying on limited and possibly
noisy class labels. To address this, we introduce a new DL training framework
that leverages anatomical and image priors from human sperm microscopy crops to
extract useful features without additional labeling cost. Our core idea is to
distill sperm head information with reliably-generated pseudo-masks and
unsupervised spatial prediction tasks. The predicted foreground masks from this
distillation step are then leveraged to regularize and reduce image and label
noise in the tuning stage. We evaluate our new approach on two public sperm
datasets and achieve state-of-the-art performances (e.g. 65.9% SCIAN accuracy
and 96.5% HuSHeM accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images. (arXiv:2202.07983v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07983">
<div class="article-summary-box-inner">
<span><p>Age-related macular degeneration (AMD) is the leading cause of visual
impairment among elderly in the world. Early detection of AMD is of great
importance as the vision loss caused by AMD is irreversible and permanent.
Color fundus photography is the most cost-effective imaging modality to screen
for retinal disorders. \textcolor{red}{Recently, some algorithms based on deep
learning had been developed for fundus image analysis and automatic AMD
detection. However, a comprehensive annotated dataset and a standard evaluation
benchmark are still missing.} To deal with this issue, we set up the Automatic
Detection challenge on Age-related Macular degeneration (ADAM) for the first
time, held as a satellite event of the ISBI 2020 conference. The ADAM challenge
consisted of four tasks which cover the main topics in detecting AMD from
fundus images, including classification of AMD, detection and segmentation of
optic disc, localization of fovea, and detection and segmentation of lesions.
The ADAM challenge has released a comprehensive dataset of 1200 fundus images
with the category labels of AMD, the pixel-wise segmentation masks of the full
optic disc and lesions (drusen, exudate, hemorrhage, scar, and other), as well
as the location coordinates of the macular fovea. A uniform evaluation
framework has been built to make a fair comparison of different models. During
the ADAM challenge, 610 results were submitted for online evaluation, and
finally, 11 teams participated in the onsite challenge. This paper introduces
the challenge, dataset, and evaluation methods, as well as summarizes the
methods and analyzes the results of the participating teams of each task. In
particular, we observed that ensembling strategy and clinical prior knowledge
can better improve the performances of the deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-Aware Indoor Scene Synthesis with Depth Priors. (arXiv:2202.08553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08553">
<div class="article-summary-box-inner">
<span><p>Despite the recent advancement of Generative Adversarial Networks (GANs) in
learning 3D-aware image synthesis from 2D data, existing methods fail to model
indoor scenes due to the large diversity of room layouts and the objects
inside. We argue that indoor scenes do not have a shared intrinsic structure,
and hence only using 2D images cannot adequately guide the model with the 3D
geometry. In this work, we fill in this gap by introducing depth as a 3D prior.
Compared with other 3D data formats, depth better fits the convolution-based
generation mechanism and is more easily accessible in practice. Specifically,
we propose a dual-path generator, where one path is responsible for depth
generation, whose intermediate features are injected into the other path as the
condition for appearance rendering. Such a design eases the 3D-aware synthesis
with explicit geometry information. Meanwhile, we introduce a switchable
discriminator both to differentiate real v.s. fake domains and to predict the
depth from a given input. In this way, the discriminator can take the spatial
arrangement into account and advise the generator to learn an appropriate depth
condition. Extensive experimental results suggest that our approach is capable
of synthesizing indoor scenes with impressively good quality and 3D
consistency, significantly outperforming state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Architectural Fine-Tuning with Neural Architecture Search using Early-Stopping in Image Classification. (arXiv:2202.08604v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08604">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (NN) perform well in various tasks (e.g., computer
vision) because of the convolutional neural networks (CNN). However, the
difficulty of gathering quality data in the industry field hinders the
practical use of NN. To cope with this issue, the concept of transfer learning
(TL) has emerged, which leverages the fine-tuning of NNs trained on large-scale
datasets in data-scarce situations. Therefore, this paper suggests a two-stage
architectural fine-tuning method for image classification, inspired by the
concept of neural architecture search (NAS). One of the main ideas of our
proposed method is a mutation with base architectures, which reduces the search
cost by using given architectural information. Moreover, an early-stopping is
also considered which directly reduces NAS costs. Experimental results verify
that our proposed method reduces computational and searching costs by up to
28.2% and 22.3%, compared to existing methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-21 23:07:54.282240325 UTC">2022-02-21 23:07:54 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>