<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-15T01:30:00Z">02-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05932">
<div class="article-summary-box-inner">
<span><p>Large-scale multi-label text classification (LMTC) aims to associate a
document with its relevant labels from a large candidate set. Most existing
LMTC approaches rely on massive human-annotated training data, which are often
costly to obtain and suffer from a long-tailed label distribution (i.e., many
labels occur only a few times in the training set). In this paper, we study
LMTC under the zero-shot setting, which does not require any annotated
documents with labels and only relies on label surface names and descriptions.
To train a classifier that calculates the similarity score between a document
and a label, we propose a novel metadata-induced contrastive learning (MICoL)
method. Different from previous text-based contrastive learning techniques,
MICoL exploits document metadata (e.g., authors, venues, and references of
research papers), which are widely available on the Web, to derive similar
document-document pairs. Experimental results on two large-scale datasets show
that: (1) MICoL significantly outperforms strong zero-shot text classification
and contrastive learning baselines; (2) MICoL is on par with the
state-of-the-art supervised metadata-aware LMTC method trained on 10K-200K
labeled documents; and (3) MICoL tends to predict more infrequent labels than
supervised methods, thus alleviates the deteriorated performance on long-tailed
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention. (arXiv:2202.05943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05943">
<div class="article-summary-box-inner">
<span><p>Most event extraction methods have traditionally relied on an annotated set
of event types. However, creating event ontologies and annotating supervised
training data are expensive and time-consuming. Previous work has proposed
semi-supervised approaches which leverage seen (annotated) types to learn how
to automatically discover new event types. State-of-the-art methods, both
semi-supervised or fully unsupervised, use a form of reconstruction loss on
specific tokens in a context. In contrast, we present a novel approach to
semi-supervised new event type induction using a masked contrastive loss, which
learns similarities between event mentions by enforcing an attention mechanism
over the data minibatch. We further disentangle the discovered clusters by
approximating the underlying manifolds in the data, which allows us to increase
normalized mutual information and Fowlkes-Mallows scores by over 20% absolute.
Building on these clustering results, we extend our approach to two new tasks:
predicting the type name of the discovered clusters and linking them to
FrameNet frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Contextual Coherence in Variational Personalized and Empathetic Dialogue Agents. (arXiv:2202.05971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05971">
<div class="article-summary-box-inner">
<span><p>In recent years, latent variable models, such as the Conditional Variational
Auto Encoder (CVAE), have been applied to both personalized and empathetic
dialogue generation. Prior work have largely focused on generating diverse
dialogue responses that exhibit persona consistency and empathy. However, when
it comes to the contextual coherence of the generated responses, there is still
room for improvement. Hence, to improve the contextual coherence, we propose a
novel Uncertainty Aware CVAE (UA-CVAE) framework. The UA-CVAE framework
involves approximating and incorporating the aleatoric uncertainty during
response generation. We apply our framework to both personalized and empathetic
dialogue generation. Empirical results show that our framework significantly
improves the contextual coherence of the generated response. Additionally, we
introduce a novel automatic metric for measuring contextual coherence, which
was found to correlate positively with human judgement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multi-task semi-supervised framework for Text2Graph & Graph2Text. (arXiv:2202.06041v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06041">
<div class="article-summary-box-inner">
<span><p>The Artificial Intelligence industry regularly develops applications that
mostly rely on Knowledge Bases, a data repository about specific, or general,
domains, usually represented in a graph shape. Similar to other databases, they
face two main challenges: information ingestion and information retrieval. We
approach these challenges by jointly learning graph extraction from text and
text generation from graphs. The proposed solution, a T5 architecture, is
trained in a multi-task semi-supervised environment, with our collected
non-parallel data, following a cycle training regime. Experiments on WebNLG
dataset show that our approach surpasses unsupervised state-of-the-art results
in text-to-graph and graph-to-text. More relevantly, our framework is more
consistent across seen and unseen domains than supervised models. The resulting
model can be easily trained in any new domain with non-parallel data, by simply
adding text and graphs about it, in our cycle framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder. (arXiv:2202.06045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06045">
<div class="article-summary-box-inner">
<span><p>Improving end-to-end speech recognition by incorporating external text data
has been a longstanding research topic. There has been a recent focus on
training E2E ASR models that get the performance benefits of external text data
without incurring the extra cost of evaluating an external language model at
inference time. In this work, we propose training ASR model jointly with a set
of text-to-text auxiliary tasks with which it shares a decoder and parts of the
encoder. When we jointly train ASR and masked language model with the 960-hour
Librispeech and Opensubtitles data respectively, we observe WER reductions of
16% and 20% on test-other and test-clean respectively over an ASR-only baseline
without any extra cost at inference time, and reductions of 6% and 8% compared
to a stronger MUTE-L baseline which trains the decoder with the same text data
as our model. We achieve further improvements when we train masked language
model on Librispeech data or when we use machine translation as the auxiliary
task, without significantly sacrificing performance on the task itself.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Oriented Unlabeled Priming for Large-Scale Language Models. (arXiv:2202.06133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06133">
<div class="article-summary-box-inner">
<span><p>Due to the high costs associated with finetuning large language models,
various recent works propose to adapt them to specific tasks without any
parameter updates through in-context learning. Unfortunately, for in-context
learning there is currently no way to leverage unlabeled data, which is often
much easier to obtain in large quantities than labeled examples. In this work,
we therefore investigate ways to make use of unlabeled examples to improve the
zero-shot performance of pretrained language models without any finetuning: We
introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies
examples by retrieving semantically similar unlabeled examples, assigning
labels to them in a zero-shot fashion, and then using them for in-context
learning. We also propose bag-of-contexts priming, a new priming strategy that
is more suitable for our setting and enables the usage of more examples than
fit into the context window.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference. (arXiv:2202.06167v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06167">
<div class="article-summary-box-inner">
<span><p>The task of ultra-fine entity typing (UFET) seeks to predict diverse and
free-form words or phrases that describe the appropriate types of entities
mentioned in sentences. A key challenge for this task lies in the large amount
of types and the scarcity of annotated data per type. Existing systems
formulate the task as a multi-way classification problem and train directly or
distantly supervised classifiers. This causes two issues: (i) the classifiers
do not capture the type semantics since types are often converted into indices;
(ii) systems developed in this way are limited to predicting within a
pre-defined type set, and often fall short of generalizing to types that are
rarely seen or unseen in training. This work presents LITE, a new approach that
formulates entity typing as a natural language inference (NLI) problem, making
use of (i) the indirect supervision from NLI to infer type information
meaningfully represented as textual hypotheses and alleviate the data scarcity
issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining
of a type set. Experiments show that, with limited training data, LITE obtains
state-of-the-art performance on the UFET task. In addition, LITE demonstrates
its strong generalizability, by not only yielding best results on other
fine-grained entity typing benchmarks, more importantly, a pre-trained LITE
system works well on new data containing unseen types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement. (arXiv:2202.06205v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06205">
<div class="article-summary-box-inner">
<span><p>Despite its benefits for children's skill development and parent-child
bonding, many parents do not often engage in interactive storytelling by having
story-related dialogues with their child due to limited availability or
challenges in coming up with appropriate questions. While recent advances made
AI generation of questions from stories possible, the fully-automated approach
excludes parent involvement, disregards educational goals, and underoptimizes
for child engagement. Informed by need-finding interviews and participatory
design (PD) results, we developed StoryBuddy, an AI-enabled system for parents
to create interactive storytelling experiences. StoryBuddy's design highlighted
the need for accommodating dynamic user needs between the desire for parent
involvement and parent-child bonding and the goal of minimizing parent
intervention when busy. The PD revealed varied assessment and educational goals
of parents, which StoryBuddy addressed by supporting configuring question types
and tracking child progress. A user study validated StoryBuddy's usability and
suggested design insights for future parent-AI collaboration systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-Retriever: Towards Learning The Unified Embedding Based Retriever in Bing Sponsored Search. (arXiv:2202.06212v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06212">
<div class="article-summary-box-inner">
<span><p>Embedding based retrieval (EBR) is a fundamental building block in many web
applications. However, EBR in sponsored search is distinguished from other
generic scenarios and technically challenging due to the need of serving
multiple retrieval purposes: firstly, it has to retrieve high-relevance ads,
which may exactly serve user's search intent; secondly, it needs to retrieve
high-CTR ads so as to maximize the overall user clicks. In this paper, we
present a novel representation learning framework Uni-Retriever developed for
Bing Search, which unifies two different training modes knowledge distillation
and contrastive learning to realize both required objectives. On one hand, the
capability of making high-relevance retrieval is established by distilling
knowledge from the ``relevance teacher model''. On the other hand, the
capability of making high-CTR retrieval is optimized by learning to
discriminate user's clicked ads from the entire corpus. The two training modes
are jointly performed as a multi-objective learning process, such that the ads
of high relevance and CTR can be favored by the generated embeddings. Besides
the learning strategy, we also elaborate our solution for EBR serving pipeline
built upon the substantially optimized DiskANN, where massive-scale EBR can be
performed with competitive time and memory efficiency, and accomplished in
high-quality. We make comprehensive offline and online experiments to evaluate
the proposed techniques, whose findings may provide useful insights for the
future development of EBR systems. Uni-Retriever has been mainstreamed as the
major retrieval path in Bing's production thanks to the notable improvements on
the representation and EBR serving quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Based Hate Speech Detection using Multimodal Learning. (arXiv:2202.06218v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06218">
<div class="article-summary-box-inner">
<span><p>In recent years, monitoring hate speech and offensive language on social
media platforms has become paramount due to its widespread usage among all age
groups, races, and ethnicities. Consequently, there have been substantial
research efforts towards automated detection of such content using Natural
Language Processing (NLP). While successfully filtering textual data, no
research has focused on detecting hateful content in multimedia data. With
increased ease of data storage and the exponential growth of social media
platforms, multimedia content proliferates the internet as much as text data.
Nevertheless, it escapes the automatic filtering systems. Hate speech and
offensiveness can be detected in multimedia primarily via three modalities,
i.e., visual, acoustic, and verbal. Our preliminary study concluded that the
most essential features in classifying hate speech would be the speaker's
emotional state and its influence on the spoken words, therefore limiting our
current research to these modalities. This paper proposes the first multimodal
deep learning framework to combine the auditory features representing emotion
and the semantic features to detect hateful content. Our results demonstrate
that incorporating emotional attributes leads to significant improvement over
text-based models in detecting hateful multimedia content. This paper also
presents a new Hate Speech Detection Video Dataset (HSDVD) collected for the
purpose of multimodal learning as no such dataset exists today.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PQuAD: A Persian Question Answering Dataset. (arXiv:2202.06219v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06219">
<div class="article-summary-box-inner">
<span><p>We present Persian Question Answering Dataset (PQuAD), a crowdsourced reading
comprehension dataset on Persian Wikipedia articles. It includes 80,000
questions along with their answers, with 25% of the questions being
adversarially unanswerable. We examine various properties of the dataset to
show the diversity and the level of its difficulty as an MRC benchmark. By
releasing this dataset, we aim to ease research on Persian reading
comprehension and development of Persian question answering systems. Our
experiments on different state-of-the-art pre-trained contextualized language
models show 74.8% Exact Match (EM) and 87.6% F1-score that can be used as the
baseline results for further research on Persian QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Depression Classification Using Articulatory Coordination Features And Hierarchical Attention Based Text Embeddings. (arXiv:2202.06238v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06238">
<div class="article-summary-box-inner">
<span><p>Multimodal depression classification has gained immense popularity over the
recent years. We develop a multimodal depression classification system using
articulatory coordination features extracted from vocal tract variables and
text transcriptions obtained from an automatic speech recognition tool that
yields improvements of area under the receiver operating characteristics curve
compared to uni-modal classifiers (7.5% and 13.7% for audio and text
respectively). We show that in the case of limited training data, a
segment-level classifier can first be trained to then obtain a session-wise
prediction without hindering the performance, using a multi-stage convolutional
recurrent neural network. A text model is trained using a Hierarchical
Attention Network (HAN). The multimodal system is developed by combining
embeddings from the session-level audio model and the HAN text model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simplified Variant of G\"odel's Ontological Argument. (arXiv:2202.06264v1 [cs.LO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06264">
<div class="article-summary-box-inner">
<span><p>A simplified variant of G\"odel's ontological argument is presented. The
simplified argument is valid already in basic modal logics K or KT, it does not
suffer from modal collapse, and it avoids the rather complex predicates of
essence (Ess.) and necessary existence (NE) as used by G\"odel. The variant
presented has been obtained as a side result of a series of theory
simplification experiments conducted in interaction with a modern proof
assistant system. The starting point for these experiments was the computer
encoding of G\"odel's argument, and then automated reasoning techniques were
systematically applied to arrive at the simplified variant presented. The
presented work thus exemplifies a fruitful human-computer interaction in
computational metaphysics. Whether the presented result increases or decreases
the attractiveness and persuasiveness of the ontological argument is a question
I would like to pass on to philosophy and theology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental user embedding modeling for personalized text classification. (arXiv:2202.06369v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06369">
<div class="article-summary-box-inner">
<span><p>Individual user profiles and interaction histories play a significant role in
providing customized experiences in real-world applications such as chatbots,
social media, retail, and education. Adaptive user representation learning by
utilizing user personalized information has become increasingly challenging due
to ever-growing history data. In this work, we propose an incremental user
embedding modeling approach, in which embeddings of user's recent interaction
histories are dynamically integrated into the accumulated history vectors via a
transformer encoder. This modeling paradigm allows us to create generalized
user representations in a consecutive manner and also alleviate the challenges
of data management. We demonstrate the effectiveness of this approach by
applying it to a personalized multi-class classification task based on the
Reddit dataset, and achieve 9% and 30% relative improvement on prediction
accuracy over a baseline system for two experiment settings through appropriate
comment history encoding and task modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments. (arXiv:2202.06387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06387">
<div class="article-summary-box-inner">
<span><p>Neural scaling laws define a predictable relationship between a model's
parameter count and its performance after training in the form of a power law.
However, most research to date has not explicitly investigated whether scaling
laws can be used to accelerate model development. In this work, we perform such
an empirical investigation across a wide range of language understanding tasks,
starting from models with as few as 10K parameters, and evaluate downstream
performance across 9 language understanding tasks. We find that scaling laws
emerge at finetuning time in some NLP tasks, and that they can also be
exploited for debugging convergence when training large models. Moreover, for
tasks where scaling laws exist, they can be used to predict the performance of
larger models, which enables effective model selection. However, revealing
scaling laws requires careful hyperparameter tuning and multiple runs for the
purpose of uncertainty estimation, which incurs additional overhead, partially
offsetting the computational benefits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Approaches for Legal Text Processing. (arXiv:2202.06397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06397">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce our approaches using Transformer-based models for
different problems of the COLIEE 2021 automatic legal text processing
competition. Automated processing of legal documents is a challenging task
because of the characteristics of legal documents as well as the limitation of
the amount of data. With our detailed experiments, we found that
Transformer-based pretrained language models can perform well with automated
legal text processing problems with appropriate approaches. We describe in
detail the processing steps for each task such as problem formulation, data
processing and augmentation, pretraining, finetuning. In addition, we introduce
to the community two pretrained models that take advantage of parallel
translations in legal domain, NFSP and NMSP. In which, NFSP achieves the
state-of-the-art result in Task 5 of the competition. Although the paper
focuses on technical reporting, the novelty of its approaches can also be an
useful reference in automated legal document processing using Transformer-based
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution augmentation for low-resource expressive text-to-speech. (arXiv:2202.06409v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06409">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel data augmentation technique for text-to-speech
(TTS), that allows to generate new (text, audio) training examples without
requiring any additional data. Our goal is to increase diversity of text
conditionings available during training. This helps to reduce overfitting,
especially in low-resource settings. Our method relies on substituting text and
audio fragments in a way that preserves syntactical correctness. We take
additional measures to ensure that synthesized speech does not contain
artifacts caused by combining inconsistent audio samples. The perceptual
evaluations show that our method improves speech quality over a number of
datasets, speakers, and TTS architectures. We also demonstrate that it greatly
improves robustness of attention-based TTS models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending Neural Keyword Extraction with TF-IDF tagset matching. (arXiv:2102.00472v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00472">
<div class="article-summary-box-inner">
<span><p>Keyword extraction is the task of identifying words (or multi-word
expressions) that best describe a given document and serve in news portals to
link articles of similar topics. In this work we develop and evaluate our
methods on four novel data sets covering less represented, morphologically-rich
languages in European news media industry (Croatian, Estonian, Latvian and
Russian). First, we perform evaluation of two supervised neural
transformer-based methods (TNT-KID and BERT+BiLSTM CRF) and compare them to a
baseline TF-IDF based unsupervised approach. Next, we show that by combining
the keywords retrieved by both neural transformer based methods and extending
the final set of keywords with an unsupervised TF-IDF based technique, we can
drastically improve the recall of the system, making it appropriate to be used
as a recommendation system in the media house environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do. (arXiv:2103.11790v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11790">
<div class="article-summary-box-inner">
<span><p>Artificial writing is permeating our lives due to recent advances in
large-scale, transformer-based language models (LMs) such as BERT, its
variants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning
them for specific tasks, researchers have extended state of the art for many
NLP tasks and shown that they capture not only linguistic knowledge but also
retain general knowledge implicitly present in the data. Unfortunately, LMs
trained on unfiltered text corpora suffer from degenerated and biased
behaviour. While this is well established, we show that recent LMs also contain
human-like biases of what is right and wrong to do, some form of ethical and
moral norms of the society -- they bring a "moral direction" to surface. That
is, we show that these norms can be captured geometrically by a direction,
which can be computed, e.g., by a PCA, in the embedding space, reflecting well
the agreement of phrases to social norms implicitly expressed in the training
texts and providing a path for attenuating or even preventing toxic
degeneration in LMs. Being able to rate the (non-)normativity of arbitrary
phrases without explicitly training the LM for this task, we demonstrate the
capabilities of the "moral direction" for guiding (even other) LMs towards
producing normative text and showcase it on RealToxicityPrompts testbed,
preventing the neural toxic degeneration in GPT-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Statistical Model of Word Rank Evolution. (arXiv:2107.09948v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09948">
<div class="article-summary-box-inner">
<span><p>The availability of large linguistic data sets enables data-driven approaches
to study linguistic change. The Google Books corpus unigram frequency data set
is used to investigate the word rank dynamics in eight languages. We observed
the rank changes of the unigrams from 1900 to 2008 and compared it to a
Wright-Fisher inspired model that we developed for our analysis. The model
simulates a neutral evolutionary process with the restriction of having no
disappearing and added words. This work explains the mathematical framework of
the model - written as a Markov Chain with multinomial transition probabilities
- to show how frequencies of words change in time. From our observations in the
data and our model, word rank stability shows two types of characteristics: (1)
the increase/decrease in ranks are monotonic, or (2) the rank stays the same.
Based on our model, high-ranked words tend to be more stable while low-ranked
words tend to be more volatile. Some words change in ranks in two ways: (a) by
an accumulation of small increasing/decreasing rank changes in time and (b) by
shocks of increase/decrease in ranks. Most words in all of the languages we
have looked at are rank stable, but not as stable as a neutral model would
predict. The stopwords and Swadesh words are observed to be rank stable across
eight languages indicating linguistic conformity in established languages.
These signatures suggest unigram frequencies in all languages have changed in a
manner inconsistent with a purely neutral evolutionary process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02352">
<div class="article-summary-box-inner">
<span><p>Aspect-level sentiment classification (ASC) aims to predict the fine-grained
sentiment polarity towards a given aspect mentioned in a review. Despite recent
advances in ASC, enabling machines to preciously infer aspect sentiments is
still challenging. This paper tackles two challenges in ASC: (1) due to lack of
aspect knowledge, aspect representation derived in prior works is inadequate to
represent aspect's exact meaning and property information; (2) prior works only
capture either local syntactic information or global relational information,
thus missing either one of them leads to insufficient syntactic information. To
tackle these challenges, we propose a novel ASC model which not only end-to-end
embeds and leverages aspect knowledge but also marries the two kinds of
syntactic information and lets them compensate for each other. Our model
includes three key components: (1) a knowledge-aware gated recurrent memory
network recurrently integrates dynamically summarized aspect knowledge; (2) a
dual syntax graph network combines both kinds of syntactic information to
comprehensively capture sufficient syntactic information; (3) a knowledge
integrating gate re-enhances the final representation with further needed
aspect knowledge; (4) an aspect-to-context attention mechanism aggregates the
aspect-related semantics from all hidden states into the final representation.
Experimental results on several benchmark datasets demonstrate the
effectiveness of our model, which overpass previous state-of-the-art models by
large margins in terms of both Accuracy and Macro-F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification with Multi-modal Explanations. (arXiv:2108.11656v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11656">
<div class="article-summary-box-inner">
<span><p>Aspect level sentiment classification (ALSC) is a difficult problem with
state-of-the-art models showing less than 80% macro-F1 score on benchmark
datasets. Existing models do not incorporate information on aspect-aspect
relations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem
from inaccurate disambiguation of aspects to KG entities, and the inability to
learn aspect representations from the large KGs in joint training with ALSC
models. We propose AR-BERT, a novel two-level global-local entity embedding
scheme that allows efficient joint training of KG-based aspect embeddings and
ALSC models. A novel incorrect disambiguation detection technique addresses the
problem of inaccuracy in aspect disambiguation. We also introduce the problem
of determining mode significance in multi-modal explanation generation, and
propose a two step solution. The proposed methods show a consistent improvement
of 2.5 - 4.1 percentage points, over the recent BERT-based baselines on
benchmark datasets. The code is available at
https://github.com/mainuliitkgp/AR-BERT.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08186">
<div class="article-summary-box-inner">
<span><p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.
FaST-VGS is a Transformer-based model for learning the associations between raw
speech waveforms and visual images. The model unifies dual-encoder and
cross-attention architectures into a single model, reaping the superior
retrieval speed of the former along with the accuracy of the latter. FaST-VGS
achieves state-of-the-art speech-image retrieval accuracy on benchmark
datasets, and its learned representations exhibit strong performance on the
ZeroSpeech 2021 phonetic and semantic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JuriBERT: A Masked-Language Model Adaptation for French Legal Text. (arXiv:2110.01485v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01485">
<div class="article-summary-box-inner">
<span><p>Language models have proven to be very useful when adapted to specific
domains. Nonetheless, little research has been done on the adaptation of
domain-specific BERT models in the French language. In this paper, we focus on
creating a language model adapted to French legal text with the goal of helping
law professionals. We conclude that some specific tasks do not benefit from
generic language models pre-trained on large amounts of data. We explore the
use of smaller architectures in domain-specific sub-languages and their
benefits for French legal text. We prove that domain-specific pre-trained
models can perform better than their equivalent generalised ones in the legal
domain. Finally, we release JuriBERT, a new set of BERT models adapted to the
French legal domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation. (arXiv:2110.03067v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03067">
<div class="article-summary-box-inner">
<span><p>To gain insight into the role neurons play, we study the activation patterns
corresponding to meaning preserving paraphrases (e.g., active-passive). We
compile a dataset of controlled syntactic paraphrases in English with their
reference German translations and demonstrate our model-agnostic approach on
the Transformer translation model. First, we identify neurons that correlate
across paraphrases and examine the observed correlation for possible confounds.
Although lower-level components (e.g., position embeddings) are found as the
cause of similar activations, we identify no localizable set of neurons that
specifically encode these paraphrases. We further manipulate neuron activations
to influence translation towards a particular syntactic form. We find that a
simple value shift is effective, and more so when many neurons are modified.
This may suggest that complex syntactic constructions are indeed encoded in the
model. We conclude by discussing how to better manipulate it using the
correlations we first obtained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sm{\aa}prat: DialoGPT for Natural Language Generation of Swedish Dialogue by Transfer Learning. (arXiv:2110.06273v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06273">
<div class="article-summary-box-inner">
<span><p>Building open-domain conversational systems (or chatbots) that produce
convincing responses is a recognized challenge. Recent state-of-the-art (SoTA)
transformer-based models for the generation of natural language dialogue have
demonstrated impressive performance in simulating human-like, single-turn
conversations in English. This work investigates, by an empirical study, the
potential for transfer learning of such models to Swedish language. DialoGPT,
an English language pre-trained model, is adapted by training on three
different Swedish language conversational datasets obtained from publicly
available sources. Perplexity score (an automated intrinsic language model
metric) and surveys by human evaluation were used to assess the performances of
the fine-tuned models, with results that indicate that the capacity for
transfer learning can be exploited with considerable success. Human evaluators
asked to score the simulated dialogue judged over 57% of the chatbot responses
to be human-like for the model trained on the largest (Swedish) dataset. We
provide the demos and model checkpoints of our English and Swedish chatbots on
the HuggingFace platform for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Explanations of Recommendations. (arXiv:2111.00670v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00670">
<div class="article-summary-box-inner">
<span><p>As recommendation is essentially a comparative (or ranking) process, a good
explanation should illustrate to users why an item is believed to be better
than another, i.e., comparative explanations about the recommended items.
Ideally, after reading the explanations, a user should reach the same ranking
of items as the system's. Unfortunately, little research attention has yet been
paid on such comparative explanations.
</p>
<p>In this work, we develop an extract-and-refine architecture to explain the
relative comparisons among a set of ranked items from a recommender system. For
each recommended item, we first extract one sentence from its associated
reviews that best suits the desired comparison against a set of reference
items. Then this extracted sentence is further articulated with respect to the
target user through a generative model to better explain why the item is
recommended. We design a new explanation quality metric based on BLEU to guide
the end-to-end training of the extraction and refinement components, which
avoids generation of generic content. Extensive offline evaluations on two
large recommendation benchmark datasets and serious user studies against an
array of state-of-the-art explainable recommendation algorithms demonstrate the
necessity of comparative explanations and the effectiveness of our solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections. (arXiv:2111.00701v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00701">
<div class="article-summary-box-inner">
<span><p>While there has been substantial progress in text comprehension through
simple factoid question answering, more holistic comprehension of a discourse
still presents a major challenge. Someone critically reflecting on a text as
they read it will pose curiosity-driven, often open-ended questions, which
reflect deep understanding of the content and require complex reasoning to
answer. A key challenge in building and evaluating models for this type of
discourse comprehension is the lack of annotated data, especially since finding
answers to such questions (which may not be answered at all) requires high
cognitive load for annotators over long documents. This paper presents a novel
paradigm that enables scalable data collection targeting the comprehension of
news documents, viewing these questions through the lens of discourse. The
resulting corpus, DCQA (Discourse Comprehension by Question Answering),
consists of 22,430 question-answer pairs across 607 English documents. DCQA
captures both discourse and semantic links between sentences in the form of
free-form, open-ended questions. On an evaluation set that we annotated on
questions from the INQUISITIVE dataset, we show that DCQA provides valuable
supervision for answering open-ended questions. We additionally design
pre-training methods utilizing existing question-answering resources, and use
synthetic data to accommodate unanswerable questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How News Evolves? Modeling News Text and Coverage using Graphs and Hawkes Process. (arXiv:2112.03008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03008">
<div class="article-summary-box-inner">
<span><p>Monitoring news content automatically is an important problem. The news
content, unlike traditional text, has a temporal component. However, few works
have explored the combination of natural language processing and dynamic system
models. One reason is that it is challenging to mathematically model the
nuances of natural language. In this paper, we discuss how we built a novel
dataset of news articles collected over time. Then, we present a method of
converting news text collected over time to a sequence of directed
multi-graphs, which represent semantic triples (Subject -&gt; Predicate}
-&gt;Object). We model the dynamics of specific topological changes in these
graphs using a set of multivariate count series, which we fit the discrete-time
Hawkes process. With our real-world data, we show that the multivariate time
series contain both dynamic information of how many articles/words were
published each day and semantic information of the content of the articles.
This yields novel insights into how news events are covered. We show with the
experiment that our approach can be used to infer from a sequence of news
articles if the articles were published by major or entertainment news outlets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Automated Error Analysis: Learning to Characterize Errors. (arXiv:2201.05017v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05017">
<div class="article-summary-box-inner">
<span><p>Characterizing the patterns of errors that a system makes helps researchers
focus future development on increasing its accuracy and robustness. We propose
a novel form of "meta learning" that automatically learns interpretable rules
that characterize the types of errors that a system makes, and demonstrate
these rules' ability to help understand and improve two NLP systems. Our
approach works by collecting error cases on validation data, extracting
meta-features describing these samples, and finally learning rules that
characterize errors using these features. We apply our approach to VilBERT, for
Visual Question Answering, and RoBERTa, for Common Sense Question Answering.
Our system learns interpretable rules that provide insights into systemic
errors these systems make on the given tasks. Using these insights, we are also
able to "close the loop" and modestly improve performance of these systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences. (arXiv:2201.11838v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11838">
<div class="article-summary-box-inner">
<span><p>Transformers-based models, such as BERT, have dramatically improved the
performance for various natural language processing tasks. The clinical
knowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art
results when performed on clinical named entity recognition and natural
language inference tasks. One of the core limitations of these transformers is
the substantial memory consumption due to their full self-attention mechanism.
To overcome this, long sequence transformer models, e.g. Longformer and
BigBird, were proposed with the idea of sparse attention mechanism to reduce
the memory usage from quadratic to the sequence length to a linear scale. These
models extended the maximum input sequence length from 512 to 4096, which
enhanced the ability of modeling long-term dependency and consequently achieved
optimal results in a variety of tasks. Inspired by the success of these long
sequence transformer models, we introduce two domain enriched language models,
namely Clinical-Longformer and Clinical-BigBird, which are pre-trained from
large-scale clinical corpora. We evaluate both pre-trained models using 10
baseline tasks including named entity recognition, question answering, and
document classification tasks. The results demonstrate that Clinical-Longformer
and Clinical-BigBird consistently and significantly outperform ClinicalBERT as
well as other short-sequence transformers in all downstream tasks. We have made
our source code available at
[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models
available for public download at:
[https://huggingface.co/yikuan8/Clinical-Longformer].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Retrieval-Augmented Text Generation. (arXiv:2202.01110v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01110">
<div class="article-summary-box-inner">
<span><p>Recently, retrieval-augmented text generation attracted increasing attention
of the computational linguistics community. Compared with conventional
generation models, retrieval-augmented text generation has remarkable
advantages and particularly has achieved state-of-the-art performance in many
NLP tasks. This paper aims to conduct a survey about retrieval-augmented text
generation. It firstly highlights the generic paradigm of retrieval-augmented
generation, and then it reviews notable approaches according to different tasks
including dialogue response generation, machine translation, and other
generation tasks. Finally, it points out some important directions on top of
recent methods to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models. (arXiv:2202.02664v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02664">
<div class="article-summary-box-inner">
<span><p>Recent research has shown the existence of significant redundancy in large
Transformer models. One can prune the redundant parameters without
significantly sacrificing the generalization performance. However, we question
whether the redundant parameters could have contributed more if they were
properly trained. To answer this question, we propose a novel training strategy
that encourages all parameters to be trained sufficiently. Specifically, we
adaptively adjust the learning rate for each parameter according to its
sensitivity, a robust gradient-based measure reflecting this parameter's
contribution to the model performance. A parameter with low sensitivity is
redundant, and we improve its fitting by increasing its learning rate. In
contrast, a parameter with high sensitivity is well-trained, and we regularize
it by decreasing its learning rate to prevent further overfitting. We conduct
extensive experiments on natural language understanding, neural machine
translation, and image classification to demonstrate the effectiveness of the
proposed schedule. Analysis shows that the proposed schedule indeed reduces the
redundancy and improves generalization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05508">
<div class="article-summary-box-inner">
<span><p>Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Does it Mean for a Language Model to Preserve Privacy?. (arXiv:2202.05520v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05520">
<div class="article-summary-box-inner">
<span><p>Natural language reflects our private lives and identities, making its
privacy concerns as broad as those of real life. Language models lack the
ability to understand the context and sensitivity of text, and tend to memorize
phrases present in their training sets. An adversary can exploit this tendency
to extract training data. Depending on the nature of the content and the
context in which this data was collected, this could violate expectations of
privacy. Thus there is a growing interest in techniques for training language
models that preserve privacy. In this paper, we discuss the mismatch between
the narrow assumptions made by popular data protection techniques (data
sanitization and differential privacy), and the broadness of natural language
and of privacy as a social norm. We argue that existing protection methods
cannot guarantee a generic and meaningful notion of privacy for language
models. We conclude that language models should be trained on text data which
was explicitly produced for public use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White-Box Attacks on Hate-speech BERT Classifiers in German with Explicit and Implicit Character Level Defense. (arXiv:2202.05778v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05778">
<div class="article-summary-box-inner">
<span><p>In this work, we evaluate the adversarial robustness of BERT models trained
on German Hate Speech datasets. We also complement our evaluation with two
novel white-box character and word level attacks thereby contributing to the
range of attacks available. Furthermore, we also perform a comparison of two
novel character-level defense strategies and evaluate their robustness with one
another.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Correction and Volumetric Reconstruction for Fetal Functional Magnetic Resonance Imaging Data. (arXiv:2202.05863v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05863">
<div class="article-summary-box-inner">
<span><p>Motion correction is an essential preprocessing step in functional Magnetic
Resonance Imaging (fMRI) of the fetal brain with the aim to remove artifacts
caused by fetal movement and maternal breathing and consequently to suppress
erroneous signal correlations. Current motion correction approaches for fetal
fMRI choose a single 3D volume from a specific acquisition timepoint with least
motion artefacts as reference volume, and perform interpolation for the
reconstruction of the motion corrected time series. The results can suffer, if
no low-motion frame is available, and if reconstruction does not exploit any
assumptions about the continuity of the fMRI signal. Here, we propose a novel
framework, which estimates a high-resolution reference volume by using
outlier-robust motion correction, and by utilizing Huber L2 regularization for
intra-stack volumetric reconstruction of the motion-corrected fetal brain fMRI.
We performed an extensive parameter study to investigate the effectiveness of
motion estimation and present in this work benchmark metrics to quantify the
effect of motion correction and regularised volumetric reconstruction
approaches on functional connectivity computations. We demonstrate the proposed
framework's ability to improve functional connectivity estimates,
reproducibility and signal interpretability, which is clinically highly
desirable for the establishment of prognostic noninvasive imaging biomarkers.
The motion correction and volumetric reconstruction framework is made available
as an open-source package of NiftyMIC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Latent Space Structuring for Generative Control. (arXiv:2202.05910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05910">
<div class="article-summary-box-inner">
<span><p>Truncation is widely used in generative models for improving the quality of
the generated samples, at the expense of reducing their diversity. We propose
to leverage the StyleGAN generative architecture to devise a new truncation
technique, based on a decomposition of the latent space into clusters, enabling
customized truncation to be performed at multiple semantic levels. We do so by
learning to re-generate W-space, the extended intermediate latent space of
StyleGAN, using a learnable mixture of Gaussians, while simultaneously training
a classifier to identify, for each latent vector, the cluster that it belongs
to. The resulting truncation scheme is more faithful to the original
untruncated samples and allows a better trade-off between quality and
diversity. We compare our method to other truncation approaches for StyleGAN,
both qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Signatures -- Learning Invariants of Planar Curves. (arXiv:2202.05922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05922">
<div class="article-summary-box-inner">
<span><p>We propose a learning paradigm for numerical approximation of differential
invariants of planar curves. Deep neural-networks' (DNNs) universal
approximation properties are utilized to estimate geometric measures. The
proposed framework is shown to be a preferable alternative to axiomatic
constructions. Specifically, we show that DNNs can learn to overcome
instabilities and sampling artifacts and produce numerically-stable signatures
for curves subject to a given group of transformations in the plane. We compare
the proposed schemes to alternative state-of-the-art axiomatic constructions of
group invariant arc-lengths and curvatures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting out-of-context objects using contextual cues. (arXiv:2202.05930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05930">
<div class="article-summary-box-inner">
<span><p>This paper presents an approach to detect out-of-context (OOC) objects in an
image. Given an image with a set of objects, our goal is to determine if an
object is inconsistent with the scene context and detect the OOC object with a
bounding box. In this work, we consider commonly explored contextual relations
such as co-occurrence relations, the relative size of an object with respect to
other objects, and the position of the object in the scene. We posit that
contextual cues are useful to determine object labels for in-context objects
and inconsistent context cues are detrimental to determining object labels for
out-of-context objects. To realize this hypothesis, we propose a graph
contextual reasoning network (GCRN) to detect OOC objects. GCRN consists of two
separate graphs to predict object labels based on the contextual cues in the
image: 1) a representation graph to learn object features based on the
neighboring objects and 2) a context graph to explicitly capture contextual
cues from the neighboring objects. GCRN explicitly captures the contextual cues
to improve the detection of in-context objects and identify objects that
violate contextual relations. In order to evaluate our approach, we create a
large-scale dataset by adding OOC object instances to the COCO images. We also
evaluate on recent OCD benchmark. Our results show that GCRN outperforms
competitive baselines in detecting OOC objects and correctly detecting
in-context objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Invariant Proposals based on a Balanced Domain Classifier for Object Detection. (arXiv:2202.05941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05941">
<div class="article-summary-box-inner">
<span><p>Object recognition from images means to automatically find object(s) of
interest and to return their category and location information. Benefiting from
research on deep learning, like convolutional neural networks~(CNNs) and
generative adversarial networks, the performance in this field has been
improved significantly, especially when training and test data are drawn from
similar distributions. However, mismatching distributions, i.e., domain shifts,
lead to a significant performance drop. In this paper, we build
domain-invariant detectors by learning domain classifiers via adversarial
training. Based on the previous works that align image and instance level
features, we mitigate the domain shift further by introducing a domain
adaptation component at the region level within Faster \mbox{R-CNN}. We embed a
domain classification network in the region proposal network~(RPN) using
adversarial learning. The RPN can now generate accurate region proposals in
different domains by effectively aligning the features between them. To
mitigate the unstable convergence during the adversarial learning, we introduce
a balanced domain classifier as well as a network learning rate adjustment
strategy. We conduct comprehensive experiments using four standard datasets.
The results demonstrate the effectiveness and robustness of our object
detection approach in domain shift scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-set Adversarial Defense with Clean-Adversarial Mutual Learning. (arXiv:2202.05953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05953">
<div class="article-summary-box-inner">
<span><p>Open-set recognition and adversarial defense study two key aspects of deep
learning that are vital for real-world deployment. The objective of open-set
recognition is to identify samples from open-set classes during testing, while
adversarial defense aims to robustify the network against images perturbed by
imperceptible adversarial noise. This paper demonstrates that open-set
recognition systems are vulnerable to adversarial samples. Furthermore, this
paper shows that adversarial defense mechanisms trained on known classes are
unable to generalize well to open-set samples. Motivated by these observations,
we emphasize the necessity of an Open-Set Adversarial Defense (OSAD) mechanism.
This paper proposes an Open-Set Defense Network with Clean-Adversarial Mutual
Learning (OSDN-CAML) as a solution to the OSAD problem. The proposed network
designs an encoder with dual-attentive feature-denoising layers coupled with a
classifier to learn a noise-free latent feature representation, which
adaptively removes adversarial noise guided by channel and spatial-wise
attentive filters. Several techniques are exploited to learn a noise-free and
informative latent feature space with the aim of improving the performance of
adversarial defense and open-set recognition. First, we incorporate a decoder
to ensure that clean images can be well reconstructed from the obtained latent
features. Then, self-supervision is used to ensure that the latent features are
informative enough to carry out an auxiliary task. Finally, to exploit more
complementary knowledge from clean image classification to facilitate feature
denoising and search for a more generalized local minimum for open-set
recognition, we further propose clean-adversarial mutual learning, where a peer
network (classifying clean images) is further introduced to mutually learn with
the classifier (classifying adversarial images).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Fusion Layers for Event Type Aware Video Recognition. (arXiv:2202.05961v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05961">
<div class="article-summary-box-inner">
<span><p>Human brain is continuously inundated with the multisensory information and
their complex interactions coming from the outside world at any given moment.
Such information is automatically analyzed by binding or segregating in our
brain. While this task might seem effortless for human brains, it is extremely
challenging to build a machine that can perform similar tasks since complex
interactions cannot be dealt with single type of integration but requires more
sophisticated approaches. In this paper, we propose a new model to address the
multisensory integration problem with individual event-specific layers in a
multi-task learning scheme. Unlike previous works where single type of fusion
is used, we design event-specific layers to deal with different audio-visual
relationship tasks, enabling different ways of audio-visual formation.
Experimental results show that our event-specific layers can discover unique
properties of the audio-visual relationships in the videos. Moreover, although
our network is formulated with single labels, it can output additional true
multi-labels to represent the given videos. We demonstrate that our proposed
framework also exposes the modality bias of the video data category-wise and
dataset-wise manner in popular benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-light Image Enhancement by Retinex Based Algorithm Unrolling and Adjustment. (arXiv:2202.05972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05972">
<div class="article-summary-box-inner">
<span><p>Motivated by their recent advances, deep learning techniques have been widely
applied to low-light image enhancement (LIE) problem. Among which, Retinex
theory based ones, mostly following a decomposition-adjustment pipeline, have
taken an important place due to its physical interpretation and promising
performance. However, current investigations on Retinex based deep learning are
still not sufficient, ignoring many useful experiences from traditional
methods. Besides, the adjustment step is either performed with simple image
processing techniques, or by complicated networks, both of which are
unsatisfactory in practice. To address these issues, we propose a new deep
learning framework for the LIE problem. The proposed framework contains a
decomposition network inspired by algorithm unrolling, and adjustment networks
considering both global brightness and local brightness sensitivity. By virtue
of algorithm unrolling, both implicit priors learned from data and explicit
priors borrowed from traditional methods can be embedded in the network,
facilitate to better decomposition. Meanwhile, the consideration of global and
local brightness can guide designing simple yet effective network modules for
adjustment. Besides, to avoid manually parameter tuning, we also propose a
self-supervised fine-tuning strategy, which can always guarantee a promising
performance. Experiments on a series of typical LIE datasets demonstrated the
effectiveness of the proposed method, both quantitatively and visually, as
compared with existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncalibrated Models Can Improve Human-AI Collaboration. (arXiv:2202.05983v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05983">
<div class="article-summary-box-inner">
<span><p>In many practical applications of AI, an AI model is used as a decision aid
for human users. The AI provides advice that a human (sometimes) incorporates
into their decision-making process. The AI advice is often presented with some
measure of "confidence" that the human can use to calibrate how much they
depend on or trust the advice. In this paper, we demonstrate that presenting AI
models as more confident than they actually are, even when the original AI is
well-calibrated, can improve human-AI performance (measured as the accuracy and
confidence of the human's final prediction after seeing the AI advice). We
first learn a model for how humans incorporate AI advice using data from
thousands of human interactions. This enables us to explicitly estimate how to
transform the AI's prediction confidence, making the AI uncalibrated, in order
to improve the final human prediction. We empirically validate our results
across four different tasks -- dealing with images, text and tabular data --
involving hundreds of human participants. We further support our findings with
simulation analysis. Our findings suggest the importance of and a framework for
jointly optimizing the human-AI system as opposed to the standard paradigm of
optimizing the AI model alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RSINet: Inpainting Remotely Sensed Images Using Triple GAN Framework. (arXiv:2202.05988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05988">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of image inpainting in the remote sensing domain.
Remote sensing images possess high resolution and geographical variations, that
render the conventional inpainting methods less effective. This further entails
the requirement of models with high complexity to sufficiently capture the
spectral, spatial and textural nuances within an image, emerging from its high
spatial variability. To this end, we propose a novel inpainting method that
individually focuses on each aspect of an image such as edges, colour and
texture using a task specific GAN. Moreover, each individual GAN also
incorporates the attention mechanism that explicitly extracts the spectral and
spatial features. To ensure consistent gradient flow, the model uses residual
learning paradigm, thus simultaneously working with high and low level
features. We evaluate our model, alongwith previous state of the art models, on
the two well known remote sensing datasets, Open Cities AI and Earth on Canvas,
and achieve competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval. (arXiv:2202.06014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06014">
<div class="article-summary-box-inner">
<span><p>In video surveillance, pedestrian retrieval (also called person
re-identification) is a critical task. This task aims to retrieve the
pedestrian of interest from non-overlapping cameras. Recently,
transformer-based models have achieved significant progress for this task.
However, these models still suffer from ignoring fine-grained, part-informed
information. This paper proposes a multi-direction and multi-scale Pyramid in
Transformer (PiT) to solve this problem. In transformer-based architecture,
each pedestrian image is split into many patches. Then, these patches are fed
to transformer layers to obtain the feature representation of this image. To
explore the fine-grained information, this paper proposes to apply vertical
division and horizontal division on these patches to generate
different-direction human parts. These parts provide more fine-grained
information. To fuse multi-scale feature representation, this paper presents a
pyramid structure containing global-level information and many pieces of
local-level information from different scales. The feature pyramids of all the
pedestrian images from the same video are fused to form the final
multi-direction and multi-scale feature representation. Experimental results on
two challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed
PiT achieves state-of-the-art performance. Extensive ablation studies
demonstrate the superiority of the proposed pyramid structure. The code is
available at https://git.openi.org.cn/zangxh/PiT.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fun Selfie Filters in Face Recognition: Impact Assessment and Removal. (arXiv:2202.06022v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06022">
<div class="article-summary-box-inner">
<span><p>This work investigates the impact of fun selfie filters, which are frequently
used to modify selfies, on face recognition systems. Based on a qualitative
assessment and classification of freely available mobile applications, ten
relevant fun selfie filters are selected to create a database. To this end, the
selected filters are automatically applied to face images of public face image
databases. Different state-of-the-art methods are used to evaluate the
influence of fun selfie filters on the performance of face detection using
dlib, RetinaFace, and a COTS method, sample quality estimated by FaceQNet and
MagFace, and recognition accuracy employing ArcFace and a COTS algorithm. The
obtained results indicate that selfie filters negatively affect face
recognition modules, especially if fun selfie filters cover a large region of
the face, where the mouth, nose, and eyes are covered. To mitigate such
unwanted effects, a GAN-based selfie filter removal algorithm is proposed which
consists of a segmentation module, a perceptual network, and a generation
module. In a cross-database experiment the application of the presented selfie
filter removal technique has shown to significantly improve the biometric
performance of the underlying face recognition systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Reinforcement Learning of Robotic Manipulation with Robust Keypoints Representation. (arXiv:2202.06027v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06027">
<div class="article-summary-box-inner">
<span><p>We present an end-to-end Reinforcement Learning(RL) framework for robotic
manipulation tasks, using a robust and efficient keypoints representation. The
proposed method learns keypoints from camera images as the state
representation, through a self-supervised autoencoder architecture. The
keypoints encode the geometric information, as well as the relationship of the
tool and target in a compact representation to ensure efficient and robust
learning. After keypoints learning, the RL step then learns the robot motion
from the extracted keypoints state representation. The keypoints and RL
learning processes are entirely done in the simulated environment. We
demonstrate the effectiveness of the proposed method on robotic manipulation
tasks including grasping and pushing, in different scenarios. We also
investigate the generalization capability of the trained model. In addition to
the robust keypoints representation, we further apply domain randomization and
adversarial training examples to achieve zero-shot sim-to-real transfer in
real-world robotic manipulation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OctAttention: Octree-based Large-scale Contexts Model for Point Cloud Compression. (arXiv:2202.06028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06028">
<div class="article-summary-box-inner">
<span><p>In point cloud compression, sufficient contexts are significant for modeling
the point cloud distribution. However, the contexts gathered by the previous
voxel-based methods decrease when handling sparse point clouds. To address this
problem, we propose a multiple-contexts deep learning framework called
OctAttention employing the octree structure, a memory-efficient representation
for point clouds. Our approach encodes octree symbol sequences in a lossless
way by gathering the information of sibling and ancestor nodes. Expressly, we
first represent point clouds with octree to reduce spatial redundancy, which is
robust for point clouds with different resolutions. We then design a
conditional entropy model with a large receptive field that models the sibling
and ancestor contexts to exploit the strong dependency among the neighboring
nodes and employ an attention mechanism to emphasize the correlated nodes in
the context. Furthermore, we introduce a mask operation during training and
testing to make a trade-off between encoding time and performance. Compared to
the previous state-of-the-art works, our approach obtains a 10%-35% BD-Rate
gain on the LiDAR benchmark (e.g. SemanticKITTI) and object point cloud dataset
(e.g. MPEG 8i, MVUB), and saves 95% coding time compared to the voxel-based
baseline. The code is available at https://github.com/zb12138/OctAttention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Cooperated Trimodal Network for Video Salient Object Detection. (arXiv:2202.06060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06060">
<div class="article-summary-box-inner">
<span><p>Depth can provide useful geographical cues for salient object detection
(SOD), and has been proven helpful in recent RGB-D SOD methods. However,
existing video salient object detection (VSOD) methods only utilize
spatiotemporal information and seldom exploit depth information for detection.
In this paper, we propose a depth-cooperated trimodal network, called DCTNet
for VSOD, which is a pioneering work to incorporate depth information to assist
VSOD. To this end, we first generate depth from RGB frames, and then propose an
approach to treat the three modalities unequally. Specifically, a multi-modal
attention module (MAM) is designed to model multi-modal long-range dependencies
between the main modality (RGB) and the two auxiliary modalities (depth,
optical flow). We also introduce a refinement fusion module (RFM) to suppress
noises in each modality and select useful information dynamically for further
feature refinement. Lastly, a progressive fusion strategy is adopted after the
refined features to achieve final cross-modal fusion. Experiments on five
benchmark datasets demonstrate the superiority of our depth-cooperated model
against 12 state-of-the-art methods, and the necessity of depth is also
validated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Microscopy Images of Breast Tissue: Region Duplication based Self-Supervision vs. Off-the Shelf Deep Representations. (arXiv:2202.06073v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06073">
<div class="article-summary-box-inner">
<span><p>Breast cancer is one of the leading causes of female mortality in the world.
This can be reduced when diagnoses are performed at the early stages of
progression. Further, the efficiency of the process can be significantly
improved with computer aided diagnosis. Deep learning based approaches have
been successfully applied to achieve this. One of the limiting factors for
training deep networks in a supervised manner is the dependency on large
amounts of expert annotated data. In reality, large amounts of unlabelled data
and only small amounts of expert annotated data are available. In such
scenarios, transfer learning approaches and self-supervised learning (SSL)
based approaches can be leveraged. In this study, we propose a novel
self-supervision pretext task to train a convolutional neural network (CNN) and
extract domain specific features. This method was compared with deep features
extracted using pre-trained CNNs such as DenseNet-121 and ResNet-50 trained on
ImageNet. Additionally, two types of patch-combination methods were introduced
and compared with majority voting. The methods were validated on the BACH
microscopy images dataset. Results indicated that the best performance of 99%
sensitivity was achieved for the deep features extracted using ResNet50 with
concatenation of patch-level embedding. Preliminary results of SSL to extract
domain specific features indicated that with just 15% of unlabelled data a high
sensitivity of 94% can be achieved for a four class classification of
microscopy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers. (arXiv:2202.06076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06076">
<div class="article-summary-box-inner">
<span><p>When a clinician refers a patient for an imaging exam, they include the
reason (e.g. relevant patient history, suspected disease) in the scan request;
this appears as the indication field in the radiology report. The
interpretation and reporting of the image are substantially influenced by this
request text, steering the radiologist to focus on particular aspects of the
image. We use the indication field to drive better image classification, by
taking a transformer network which is unimodally pre-trained on text (BERT) and
fine-tuning it for multimodal classification of a dual image-text input. We
evaluate the method on the MIMIC-CXR dataset, and present ablation studies to
investigate the effect of the indication field on the classification
performance. The experimental results show our approach achieves 87.8 average
micro AUROC, outperforming the state-of-the-art methods for unimodal (84.4) and
multimodal (86.0) classification. Our code is available at
https://github.com/jacenkow/mmbt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text and Image Guided 3D Avatar Generation and Manipulation. (arXiv:2202.06079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06079">
<div class="article-summary-box-inner">
<span><p>The manipulation of latent space has recently become an interesting topic in
the field of generative models. Recent research shows that latent directions
can be used to manipulate images towards certain attributes. However,
controlling the generation process of 3D generative models remains a challenge.
In this work, we propose a novel 3D manipulation method that can manipulate
both the shape and texture of the model using text or image-based prompts such
as 'a young face' or 'a surprised face'. We leverage the power of Contrastive
Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model
designed to generate face avatars, and create a fully differentiable rendering
pipeline to manipulate meshes. More specifically, our method takes an input
latent code and modifies it such that the target attribute specified by a text
or image prompt is present or enhanced, while leaving other attributes largely
unaffected. Our method requires only 5 minutes per manipulation, and we
demonstrate the effectiveness of our approach with extensive results and
comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognition-free Question Answering on Handwritten Document Collections. (arXiv:2202.06080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06080">
<div class="article-summary-box-inner">
<span><p>In recent years, considerable progress has been made in the research area of
Question Answering (QA) on document images. Current QA approaches from the
Document Image Analysis community are mainly focusing on machine-printed
documents and perform rather limited on handwriting. This is mainly due to the
reduced recognition performance on handwritten documents. To tackle this
problem, we propose a recognition-free QA approach, especially designed for
handwritten document image collections. We present a robust document retrieval
method, as well as two QA models. Our approaches outperform the
state-of-the-art recognition-free models on the challenging BenthamQA and
HW-SQuAD datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing. (arXiv:2202.06088v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06088">
<div class="article-summary-box-inner">
<span><p>Some of the most exciting experiences that Metaverse promises to offer, for
instance, live interactions with virtual characters in virtual environments,
require real-time photo-realistic rendering. 3D reconstruction approaches to
rendering, active or passive, still require extensive cleanup work to fix the
meshes or point clouds. In this paper, we present a neural volumography
technique called neural volumetric video or NeuVV to support immersive,
interactive, and spatial-temporal rendering of volumetric video contents with
photo-realism and in real-time. The core of NeuVV is to efficiently encode a
dynamic neural radiance field (NeRF) into renderable and editable primitives.
We introduce two types of factorization schemes: a hyper-spherical harmonics
(HH) decomposition for modeling smooth color variations over space and time and
a learnable basis representation for modeling abrupt density and color changes
caused by motion. NeuVV factorization can be integrated into a Video Octree
(VOctree) analogous to PlenOctree to significantly accelerate training while
reducing memory overhead. Real-time NeuVV rendering further enables a class of
immersive content editing tools. Specifically, NeuVV treats each VOctree as a
primitive and implements volume-based depth ordering and alpha blending to
realize spatial-temporal compositions for content re-purposing. For example, we
demonstrate positioning varied manifestations of the same performance at
different 3D locations with different timing, adjusting color/texture of the
performer's clothing, casting spotlight shadows and synthesizing distance
falloff lighting, etc, all at an interactive speed. We further develop a hybrid
neural-rasterization rendering framework to support consumer-level VR headsets
so that the aforementioned volumetric video viewing and editing, for the first
time, can be conducted immersively in virtual 3D space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Deep Learning-based Approaches for Deepfake Content Detection. (arXiv:2202.06095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06095">
<div class="article-summary-box-inner">
<span><p>The fast-spreading information over the internet is essential to support the
rapid supply of numerous public utility services and entertainment to users.
Social networks and online media paved the way for modern,
timely-communication-fashion and convenient access to all types of information.
However, it also provides new chances for ill use of the massive amount of
available data, such as spreading fake content to manipulate public opinion.
Detection of counterfeit content has raised attention in the last few years for
the advances in deepfake generation. The rapid growth of machine learning
techniques, particularly deep learning, can predict fake content in several
application domains, including fake image and video manipulation. This paper
presents a comprehensive review of recent studies for deepfake content
detection using deep learning-based approaches. We aim to broaden the
state-of-the-art research by systematically reviewing the different categories
of fake content detection. Furthermore, we report the advantages and drawbacks
of the examined works and future directions towards the issues and shortcomings
still unsolved on deepfake detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Medical Image Segmentation via Geometry-aware Consistency Training. (arXiv:2202.06104v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06104">
<div class="article-summary-box-inner">
<span><p>The performance of supervised deep learning methods for medical image
segmentation is often limited by the scarcity of labeled data. As a promising
research direction, semi-supervised learning addresses this dilemma by
leveraging unlabeled data information to assist the learning process. In this
paper, a novel geometry-aware semi-supervised learning framework is proposed
for medical image segmentation, which is a consistency-based method.
Considering that the hard-to-segment regions are mainly located around the
object boundary, we introduce an auxiliary prediction task to learn the global
geometric information. Based on the geometric constraint, the ambiguous
boundary regions are emphasized through an exponentially weighted strategy for
the model training to better exploit both labeled and unlabeled data. In
addition, a dual-view network is designed to perform segmentation from
different perspectives and reduce the prediction uncertainty. The proposed
method is evaluated on the public left atrium benchmark dataset and improves
fully supervised method by 8.7% in Dice with 10% labeled images, while 4.3%
with 20% labeled images. Meanwhile, our framework outperforms six
state-of-the-art semi-supervised segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breast Cancer Detection using Histopathological Images. (arXiv:2202.06109v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06109">
<div class="article-summary-box-inner">
<span><p>Cancer is one of the most common and fatal diseases in the world. Breast
cancer affects one in every eight women and one in every eight hundred men.
Hence, our prime target should be early detection of cancer because the early
detection of cancer can be helpful to cure cancer effectively. Therefore, we
propose a saliency detection system with the help of advanced deep learning
techniques, such that the machine will be taught to emulate actions of
pathologists for localization of diagnostically pertinent regions. We study
identification of five diagnostic categories of breast cancer by training a CNN
(VGG16, ResNet architecture). We have used BreakHis dataset to train our model.
We focus on both detection and classification of cancerous regions in
histopathology images. The diagnostically relevant regions are salient. The
detection system will be available as an open source web application which can
be used by pathologists and medical institutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Deep Learning for Cerebrovascular Disease Classification and MRI-to-PET Translation. (arXiv:2202.06142v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06142">
<div class="article-summary-box-inner">
<span><p>Accurate quantification of cerebral blood flow (CBF) is essential for the
diagnosis and assessment of cerebrovascular diseases such as Moyamoya, carotid
stenosis, aneurysms, and stroke. Positron emission tomography (PET) is
currently regarded as the gold standard for the measurement of CBF in the human
brain. PET imaging, however, is not widely available because of its prohibitive
costs, use of ionizing radiation, and logistical challenges, which require a
co-localized cyclotron to deliver the 2 min half-life Oxygen-15 radioisotope.
Magnetic resonance imaging (MRI), in contrast, is more readily available and
does not involve ionizing radiation. In this study, we propose a multi-task
learning framework for brain MRI-to-PET translation and disease diagnosis. The
proposed framework comprises two prime networks: (1) an attention-based 3D
encoder-decoder convolutional neural network (CNN) that synthesizes
high-quality PET CBF maps from multi-contrast MRI images, and (2) a multi-scale
3D CNN that identifies the brain disease corresponding to the input MRI images.
Our multi-task framework yields promising results on the task of MRI-to-PET
translation, achieving an average structural similarity index (SSIM) of 0.94
and peak signal-to-noise ratio (PSNR) of 38dB on a cohort of 120 subjects. In
addition, we show that integrating multiple MRI modalities can improve the
clinical diagnosis of brain diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfraredTags: Embedding Invisible AR Markers and Barcodes Using Low-Cost, Infrared-Based 3D Printing and Imaging Tools. (arXiv:2202.06165v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06165">
<div class="article-summary-box-inner">
<span><p>Existing approaches for embedding unobtrusive tags inside 3D objects require
either complex fabrication or high-cost imaging equipment. We present
InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye
that can be 3D printed as part of objects, and detected rapidly by low-cost
near-infrared cameras. We achieve this by printing objects from an
infrared-transmitting filament, which infrared cameras can see through, and by
having air gaps inside for the tag's bits, which appear at a different
intensity in the infrared image.
</p>
<p>We built a user interface that facilitates the integration of common tags (QR
codes, ArUco markers) with the object geometry to make them 3D printable as
InfraredTags. We also developed a low-cost infrared imaging module that
augments existing mobile devices and decodes tags using our image processing
pipeline. Our evaluation shows that the tags can be detected with little
near-infrared illumination (0.2lux) and from distances as far as 250cm. We
demonstrate how our method enables various applications, such as object
tracking and embedding metadata for augmented reality and tangible
interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-Free Progressive Graph Learning for Open-Set Domain Adaptation. (arXiv:2202.06174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06174">
<div class="article-summary-box-inner">
<span><p>Open-set domain adaptation (OSDA) has gained considerable attention in many
visual recognition tasks. However, most existing OSDA approaches are limited
due to three main reasons, including: (1) the lack of essential theoretical
analysis of generalization bound, (2) the reliance on the coexistence of source
and target data during adaptation, and (3) failing to accurately estimate the
uncertainty of model predictions. We propose a Progressive Graph Learning (PGL)
framework that decomposes the target hypothesis space into the shared and
unknown subspaces, and then progressively pseudo-labels the most confident
known samples from the target domain for hypothesis adaptation. Moreover, we
tackle a more realistic source-free open-set domain adaptation (SF-OSDA)
setting that makes no assumption about the coexistence of source and target
domains, and introduce a balanced pseudo-labeling (BP-L) strategy in a
two-stage framework, namely SF-PGL. Different from PGL that applies a
class-agnostic constant threshold for all target samples for pseudo-labeling,
the SF-PGL model uniformly selects the most confident target instances from
each category at a fixed ratio. The confidence thresholds in each class are
regarded as the 'uncertainty' of learning the semantic information, which are
then used to weigh the classification loss in the adaptation step. We conducted
unsupervised and semi-supervised OSDA and SF-OSDA experiments on the benchmark
image classification and action recognition datasets. Additionally, we find
that balanced pseudo-labeling plays a significant role in improving
calibration, which makes the trained model less prone to over-confident or
under-confident predictions on the target data. Source code is available at
https://github.com/Luoyadan/SF-PGL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lip movements information disentanglement for lip sync. (arXiv:2202.06198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06198">
<div class="article-summary-box-inner">
<span><p>The lip movements information is critical for many audio-visual tasks.
However, extracting lip movements information from videos is challenging, as it
can be easily perturbed by factors like personal identities and head poses.
This paper proposes utilizing the parametric 3D face model to disentangle lip
movements information explicitly. Building on top of the recent 3D face
reconstruction advances, we firstly offer a method that can consistently
disentangle expression information, where the lip movements information lies.
Then we demonstrate that once the influences of perturbing factors are
alleviated by synthesizing faces with the disentangled lip movements
information, the lip-sync task can be done better with much fewer data.
Finally, we show its effectiveness in the wild by testing it on an unseen
dataset for the active speaker detection task and achieving competitive
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Disentanglement with Tensor Product Representations on the Torus. (arXiv:2202.06201v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06201">
<div class="article-summary-box-inner">
<span><p>The current methods for learning representations with auto-encoders almost
exclusively employ vectors as the latent representations. In this work, we
propose to employ a tensor product structure for this purpose. This way, the
obtained representations are naturally disentangled. In contrast to the
conventional variations methods, which are targeted toward normally distributed
features, the latent space in our representation is distributed uniformly over
a set of unit circles. We argue that the torus structure of the latent space
captures the generative factors effectively. We employ recent tools for
measuring unsupervised disentanglement, and in an extensive set of experiments
demonstrate the advantage of our method in terms of disentanglement,
completeness, and informativeness. The code for our proposed method is
available at https://github.com/rotmanmi/Unsupervised-Disentanglement-Torus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Deepfake On Unrestricted Media: Generation And Detection. (arXiv:2202.06228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06228">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have led to substantial improvements in
deepfake generation, resulting in fake media with a more realistic appearance.
Although deepfake media have potential application in a wide range of areas and
are drawing much attention from both the academic and industrial communities,
it also leads to serious social and criminal concerns. This chapter explores
the evolution of and challenges in deepfake generation and detection. It also
discusses possible ways to improve the robustness of deepfake detection for a
wide variety of media (e.g., in-the-wild images and videos). Finally, it
suggests a focus for future fake media research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations. (arXiv:2202.06240v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06240">
<div class="article-summary-box-inner">
<span><p>Recent advances in generative adversarial networks have shown that it is
possible to generate high-resolution and hyperrealistic images. However, the
images produced by GANs are only as fair and representative as the datasets on
which they are trained. In this paper, we propose a method for directly
modifying a pre-trained StyleGAN2 model that can be used to generate a balanced
set of images with respect to one (e.g., eyeglasses) or more attributes (e.g.,
gender and eyeglasses). Our method takes advantage of the style space of the
StyleGAN2 model to perform disentangled control of the target attributes to be
debiased. Our method does not require training additional models and directly
debiases the GAN model, paving the way for its use in various downstream
applications. Our experiments show that our method successfully debiases the
GAN model within a few minutes without compromising the quality of the
generated images. To promote fair generative models, we share the code and
debiased models at <a href="http://catlab-team.github.io/fairstyle.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy protection based on mask template. (arXiv:2202.06250v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06250">
<div class="article-summary-box-inner">
<span><p>Powerful recognition algorithms are widely used in the Internet or important
medical systems, which poses a serious threat to personal privacy. Although the
law provides for diversity protection, e.g. The General Data Protection
Regulation (GDPR) in Europe and Articles 1032 to 1039 of the civil code in
China. However, as an important privacy disclosure event, biometric data is
often hidden, which is difficult for the owner to detect and trace to the
source. Human biometrics generally exist in images. In order to avoid the
disclosure of personal privacy, we should prevent unauthorized recognition
algorithms from acquiring the real features of the original image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Drone Swarm Navigation and Multi-target Tracking in 3D Environments with Dynamic Obstacles. (arXiv:2202.06253v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06253">
<div class="article-summary-box-inner">
<span><p>Autonomous modeling of artificial swarms is necessary because manual creation
is a time intensive and complicated procedure which makes it impractical. An
autonomous approach employing deep reinforcement learning is presented in this
study for swarm navigation. In this approach, complex 3D environments with
static and dynamic obstacles and resistive forces (like linear drag, angular
drag, and gravity) are modeled to track multiple dynamic targets. Moreover,
reward functions for robust swarm formation and target tracking are devised for
learning complex swarm behaviors. Since the number of agents is not fixed and
has only the partial observance of the environment, swarm formation and
navigation become challenging. In this regard, the proposed strategy consists
of three main phases to tackle the aforementioned challenges: 1) A methodology
for dynamic swarm management, 2) Avoiding obstacles, Finding the shortest path
towards the targets, 3) Tracking the targets and Island modeling. The dynamic
swarm management phase translates basic sensory input to high level commands to
enhance swarm navigation and decentralized setup while maintaining the swarms
size fluctuations. While, in the island modeling, the swarm can split into
individual subswarms according to the number of targets, conversely, these
subswarms may join to form a single huge swarm, giving the swarm ability to
track multiple targets. Customized state of the art policy based deep
reinforcement learning algorithms are employed to achieve significant results.
The promising results show that our proposed strategy enhances swarm navigation
and can track multiple static and dynamic targets in complex dynamic
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RandomSEMO: Normality Learning Of Moving Objects For Video Anomaly Detection. (arXiv:2202.06256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06256">
<div class="article-summary-box-inner">
<span><p>Recent anomaly detection algorithms have shown powerful performance by
adopting frame predicting autoencoders. However, these methods face two
challenging circumstances. First, they are likely to be trained to be
excessively powerful, generating even abnormal frames well, which leads to
failure in detecting anomalies. Second, they are distracted by the large number
of objects captured in both foreground and background. To solve these problems,
we propose a novel superpixel-based video data transformation technique named
Random Superpixel Erasing on Moving Objects (RandomSEMO) and Moving Object Loss
(MOLoss), built on top of a simple lightweight autoencoder. RandomSEMO is
applied to the moving object regions by randomly erasing their superpixels. It
enforces the network to pay attention to the foreground objects and learn the
normal features more effectively, rather than simply predicting the future
frame. Moreover, MOLoss urges the model to focus on learning normal objects
captured within RandomSEMO by amplifying the loss on the pixels near the moving
objects. The experimental results show that our model outperforms
state-of-the-arts on three benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LTSP: Long-Term Slice Propagation for Accurate Airway Segmentation. (arXiv:2202.06260v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06260">
<div class="article-summary-box-inner">
<span><p>Purpose: Bronchoscopic intervention is a widely-used clinical technique for
pulmonary diseases, which requires an accurate and topological complete airway
map for its localization and guidance. The airway map could be extracted from
chest computed tomography (CT) scans automatically by airway segmentation
methods. Due to the complex tree-like structure of the airway, preserving its
topology completeness while maintaining the segmentation accuracy is a
challenging task.
</p>
<p>Methods: In this paper, a long-term slice propagation (LTSP) method is
proposed for accurate airway segmentation from pathological CT scans. We also
design a two-stage end-to-end segmentation framework utilizing the LTSP method
in the decoding process. Stage 1 is used to generate a coarse feature map by an
encoder-decoder architecture. Stage 2 is to adopt the proposed LTSP method for
exploiting the continuity information and enhancing the weak airway features in
the coarse feature map. The final segmentation result is predicted from the
refined feature map.
</p>
<p>Results: Extensive experiments were conducted to evaluate the performance of
the proposed method on 70 clinical CT scans. The results demonstrate the
considerable improvements of the proposed method compared to some
state-of-the-art methods as most breakages are eliminated and more tiny bronchi
are detected. The ablation studies further confirm the effectiveness of the
constituents of the proposed method.
</p>
<p>Conclusion: Slice continuity information is beneficial to accurate airway
segmentation. Furthermore, by propagating the long-term slice feature, the
airway topology connectivity is preserved with overall segmentation accuracy
maintained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling. (arXiv:2202.06263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06263">
<div class="article-summary-box-inner">
<span><p>Compared with traditional task-irrelevant downsampling methods, task-oriented
neural networks have shown improved performance in point cloud downsampling
range. Recently, Transformer family of networks has shown a more powerful
learning capacity in visual tasks. However, Transformer-based architectures
potentially consume too many resources which are usually worthless for low
overhead task networks in downsampling range. This paper proposes a novel
light-weight Transformer network (LighTN) for task-oriented point cloud
downsampling, as an end-to-end and plug-and-play solution. In LighTN, a
single-head self-correlation module is presented to extract refined global
contextual features, where three projection matrices are simultaneously
eliminated to save resource overhead, and the output of symmetric matrix
satisfies the permutation invariant. Then, we design a novel downsampling loss
function to guide LighTN focuses on critical point cloud regions with more
uniform distribution and prominent points coverage. Furthermore, We introduce a
feed-forward network scaling mechanism to enhance the learnable capacity of
LighTN according to the expand-reduce strategy. The result of extensive
experiments on classification and registration tasks demonstrates LighTN can
achieve state-of-the-art performance with limited resource overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Deep Image Inpainting by Emphasizing the Complexity of Missing Regions. (arXiv:2202.06266v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06266">
<div class="article-summary-box-inner">
<span><p>Deep image inpainting research mainly focuses on constructing various neural
network architectures or imposing novel optimization objectives. However, on
the one hand, building a state-of-the-art deep inpainting model is an extremely
complex task, and on the other hand, the resulting performance gains are
sometimes very limited. We believe that besides the frameworks of inpainting
models, lightweight traditional image processing techniques, which are often
overlooked, can actually be helpful to these deep models. In this paper, we
enhance the deep image inpainting models with the help of classical image
complexity metrics. A knowledge-assisted index composed of missingness
complexity and forward loss is presented to guide the batch selection in the
training procedure. This index helps find samples that are more conducive to
optimization in each iteration and ultimately boost the overall inpainting
performance. The proposed approach is simple and can be plugged into many deep
inpainting models by changing only a few lines of code. We experimentally
demonstrate the improvements for several recently developed image inpainting
models on various datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BViT: Broad Attention based Vision Transformer. (arXiv:2202.06268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06268">
<div class="article-summary-box-inner">
<span><p>Recent works have demonstrated that transformer can achieve promising
performance in computer vision, by exploiting the relationship among image
patches with self-attention. While they only consider the attention in a single
feature layer, but ignore the complementarity of attention in different levels.
In this paper, we propose the broad attention to improve the performance by
incorporating the attention relationship of different layers for vision
transformer, which is called BViT. The broad attention is implemented by broad
connection and parameter-free attention. Broad connection of each transformer
layer promotes the transmission and integration of information for BViT.
Without introducing additional trainable parameters, parameter-free attention
jointly focuses on the already available attention information in different
layers for extracting useful information and building their relationship.
Experiments on image classification tasks demonstrate that BViT delivers
state-of-the-art accuracy of 74.8\%/81.6\% top-1 accuracy on ImageNet with
5M/22M parameters. Moreover, we transfer BViT to downstream object recognition
benchmarks to achieve 98.9\% and 89.9\% on CIFAR10 and CIFAR100 respectively
that exceed ViT with fewer parameters. For the generalization test, the broad
attention in Swin Transformer and T2T-ViT also bring an improvement of more
than 1\%. To sum up, broad attention is promising to promote the performance of
attention based models. Code and pre-trained models are available at
https://github.com/DRL-CASIA/Broad_ViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Image Stitching Using Depth Maps. (arXiv:2202.06276v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06276">
<div class="article-summary-box-inner">
<span><p>Natural image stitching (NIS) aims to create one natural-looking mosaic from
two overlapping images that capture a same 3D scene from different viewing
positions. Challenges inevitably arise when the scene is non-planar and the
camera baseline is wide, since parallax becomes not negligible in such cases.
In this paper, we propose a novel NIS method using depth maps, which generates
natural-looking mosaics against parallax in both overlapping and
non-overlapping regions. Firstly, we estimate a pixel-to-pixel transformation
based on feature matches and their depth values. Then, we draw a triangulation
of the target image and estimate multiple local homographies, one per triangle,
based on the locations of their vertices and the rectified depth values.
Finally, the warping image is composited by the backward mapping of piece-wise
homographies. Experimental results demonstrate that the proposed method not
only provides accurate alignment in the overlapping regions, but also virtual
naturalness in the non-overlapping region.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Reference Image Restoration for Under-Display Camera of UAV. (arXiv:2202.06283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06283">
<div class="article-summary-box-inner">
<span><p>The exposed cameras of UAV can shake, shift, or even malfunction under the
influence of harsh weather, while the add-on devices (Dupont lines) are very
vulnerable to damage.
</p>
<p>We can place a low-cost T-OLED overlay around the camera to protect it, but
this would also introduce image degradation issues.
</p>
<p>In particular, the temperature variations in the atmosphere can create mist
that adsorbs to the T-OLED, which can cause secondary disasters (i.e., more
severe image degradation) during the UAV's filming process.
</p>
<p>To solve the image degradation problem caused by overlaying T-OLEDs, in this
paper we propose a new method to enhance the visual experience by enhancing the
texture and color of images.
</p>
<p>Specifically, our method trains a lightweight network to estimate a low-rank
affine grid on the input image, and then utilizes the grid to enhance the input
image at block granularity.
</p>
<p>The advantages of our method are that no reference image is required and the
loss function is developed from visual experience.
</p>
<p>In addition, our model can perform high-quality recovery of images of
arbitrary resolution in real time.
</p>
<p>In the end, the limitations of our model and the collected datasets
(including the daytime and nighttime scenes) are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Sickness Modeling with Visual Vertical Estimation and Its Application to Autonomous Personal Mobility Vehicles. (arXiv:2202.06299v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06299">
<div class="article-summary-box-inner">
<span><p>Passengers (drivers) of level 3-5 autonomous personal mobility vehicles
(APMV) and cars can perform non-driving tasks, such as reading books and
smartphones, while driving. It has been pointed out that such activities may
increase motion sickness. Many studies have been conducted to build
countermeasures, of which various computational motion sickness models have
been developed. Many of these are based on subjective vertical conflict (SVC)
theory, which describes vertical changes in direction sensed by human sensory
organs vs. those expected by the central nervous system. Such models are
expected to be applied to autonomous driving scenarios. However, no current
computational model can integrate visual vertical information with vestibular
sensations.
</p>
<p>We proposed a 6 DoF SVC-VV model which add a visually perceived vertical
block into a conventional six-degrees-of-freedom SVC model to predict VV
directions from image data simulating the visual input of a human. Hence, a
simple image-based VV estimation method is proposed.
</p>
<p>As the validation of the proposed model, this paper focuses on describing the
fact that the motion sickness increases as a passenger reads a book while using
an AMPV, assuming that visual vertical (VV) plays an important role. In the
static experiment, it is demonstrated that the estimated VV by the proposed
method accurately described the gravitational acceleration direction with a low
mean absolute deviation. In addition, the results of the driving experiment
using an APMV demonstrated that the proposed 6 DoF SVC-VV model could describe
that the increased motion sickness experienced when the VV and gravitational
acceleration directions were different.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction. (arXiv:2202.06300v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06300">
<div class="article-summary-box-inner">
<span><p>Lighting prediction from a single image is becoming increasingly important in
many vision and augmented reality (AR) applications in which shading and shadow
consistency between virtual and real objects should be guaranteed. However,
this is a notoriously ill-posed problem, especially for indoor scenarios,
because of the complexity of indoor luminaires and the limited information
involved in 2D images. In this paper, we propose a graph learning-based
framework for indoor lighting estimation. At its core is a new lighting model
(dubbed DSGLight) based on depth-augmented Spherical Gaussians (SG) and a Graph
Convolutional Network (GCN) that infers the new lighting representation from a
single LDR image of limited field-of-view. Our lighting model builds 128 evenly
distributed SGs over the indoor panorama, where each SG encoding the lighting
and the depth around that node. The proposed GCN then learns the mapping from
the input image to DSGLight. Compared with existing lighting models, our
DSGLight encodes both direct lighting and indirect environmental lighting more
faithfully and compactly. It also makes network training and inference more
stable. The estimated depth distribution enables temporally stable shading and
shadows under spatially-varying lighting. Through thorough experiments, we show
that our method obviously outperforms existing methods both qualitatively and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Fine-tuning for Backdoor Defense: Connect Adversarial Examples to Triggered Samples. (arXiv:2202.06312v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06312">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are known to be vulnerable to backdoor attacks,
i.e., a backdoor trigger planted at training time, the infected DNN model would
misclassify any testing sample embedded with the trigger as target label. Due
to the stealthiness of backdoor attacks, it is hard either to detect or erase
the backdoor from infected models. In this paper, we propose a new Adversarial
Fine-Tuning (AFT) approach to erase backdoor triggers by leveraging adversarial
examples of the infected model. For an infected model, we observe that its
adversarial examples have similar behaviors as its triggered samples. Based on
such observation, we design the AFT to break the foundation of the backdoor
attack (i.e., the strong correlation between a trigger and a target label). We
empirically show that, against 5 state-of-the-art backdoor attacks, AFT can
effectively erase the backdoor triggers without obvious performance degradation
on clean samples, which significantly outperforms existing defense methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation. (arXiv:2202.06344v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06344">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of glioma and its subregions is of great significance
for diagnosis, treatment and monitoring of disease. In this paper, an
augmentation method, called TensorMixup, was proposed and applied to the three
dimensional U-Net architecture for brain tumor segmentation. The main ideas
included that first, two image patches with size of 128 in three dimensions
were selected according to glioma information of ground truth labels from the
magnetic resonance imaging data of any two patients with the same modality.
Next, a tensor in which all elements were independently sampled from Beta
distribution was used to mix the image patches. Then the tensor was mapped to a
matrix which was used to mix the one-hot encoded labels of the above image
patches. Therefore, a new image and its one-hot encoded label were synthesized.
Finally, the new data was used to train the model which could be used to
segment glioma. The experimental results show that the mean accuracy of Dice
scores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor
core, and enhancing tumor segmentation, which proves that the proposed
TensorMixup is feasible and effective for brain tumor segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse facial inpainting guided by exemplars. (arXiv:2202.06358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06358">
<div class="article-summary-box-inner">
<span><p>Facial image inpainting is a task of filling visually realistic and
semantically meaningful contents for missing or masked pixels in a face image.
Although existing methods have made significant progress in achieving high
visual quality, the controllable diversity of facial image inpainting remains
an open problem in this field. This paper introduces EXE-GAN, a novel diverse
and interactive facial inpainting framework, which can not only preserve the
high-quality visual effect of the whole image but also complete the face image
with exemplar-like facial attributes. The proposed facial inpainting is
achieved based on generative adversarial networks by leveraging the global
style of input image, the stochastic style, and the exemplar style of example
image. A novel attribute similarity metric is introduced to encourage networks
to learn the style of facial attributes from the exemplar in a self-supervised
way. To guarantee the natural transition across the boundary of inpainted
regions, a novel spatial variant gradient backpropagation technique is designed
to adjust the loss gradients based on the spatial location. A variety of
experimental results and comparisons on public CelebA-HQ and FFHQ datasets are
presented to demonstrate the superiority of the proposed method in terms of
both the quality and diversity in facial inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Perspective Deformation in X-Ray Transmission Imaging. (arXiv:2202.06366v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06366">
<div class="article-summary-box-inner">
<span><p>In cone-beam X-ray transmission imaging, due to the divergence of X-rays,
imaged structures with different depths have different magnification factors on
an X-ray detector, which results in perspective deformation. Perspective
deformation causes difficulty in direct, accurate geometric assessments of
anatomical structures. In this work, to reduce perspective deformation in X-ray
images acquired from regular cone-beam computed tomography (CBCT) systems, we
investigate on learning perspective deformation, i.e., converting perspective
projections into orthogonal projections. Directly converting a single
perspective projection image into an orthogonal projection image is extremely
challenging due to the lack of depth information. Therefore, we propose to
utilize one additional perspective projection, a complementary (180-degree) or
orthogonal (90-degree) view, to provide a certain degree of depth information.
Furthermore, learning perspective deformation in different spatial domains is
investigated. Our proposed method is evaluated on numerical spherical bead
phantoms as well as patients' chest and head X-ray data. The experiments on
numerical bead phantom data demonstrate that learning perspective deformation
in polar coordinates has significant advantages over learning in Cartesian
coordinates, as root-mean-square error (RMSE) decreases from 5.31 to 1.40,
while learning in log-polar coordinates has no further considerable improvement
(RMSE = 1.85). In addition, using a complementary view (RMSE = 1.40) is better
than an orthogonal view (RMSE = 3.87). The experiments on patients' chest and
head data demonstrate that learning perspective deformation using dual
complementary views is also applicable in anatomical X-ray data, allowing
accurate cardiothoracic ratio measurements in chest X-ray images and
cephalometric analysis in synthetic cephalograms from cone-beam X-ray
projections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omnifont Persian OCR System Using Primitives. (arXiv:2202.06371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06371">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a model-based omnifont Persian OCR system. The
system uses a set of 8 primitive elements as structural features for
recognition. First, the scanned document is preprocessed. After normalizing the
preprocessed image, text rows and sub-words are separated and then thinned.
After recognition of dots in sub-words, strokes are extracted and primitive
elements of each sub-word are recognized using the strokes. Finally, the
primitives are compared with a predefined set of character identification
vectors in order to identify sub-word characters. The separation and
recognition steps of the system are concurrent, eliminating unavoidable errors
of independent separation of letters. The system has been tested on documents
with 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron. (arXiv:2202.06372v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06372">
<div class="article-summary-box-inner">
<span><p>The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing
threat to humans worldwide, creating a health crisis that infected millions of
lives, as well as devastating the global economy. Deep learning (DL) techniques
have proved helpful in analysis and delineation of infectious regions in
radiological images in a timely manner. This paper makes an in-depth survey of
DL techniques and draws a taxonomy based on diagnostic strategies and learning
approaches. DL techniques are systematically categorized into classification,
segmentation, and multi-stage approaches for COVID-19 diagnosis at image and
region level analysis. Each category includes pre-trained and custom-made
Convolutional Neural Network architectures for detecting COVID-19 infection in
radiographic imaging modalities; X-Ray, and Computer Tomography (CT).
Furthermore, a discussion is made on challenges in developing diagnostic
techniques in pandemic, cross-platform interoperability, and examining imaging
modality, in addition to reviewing methodologies and performance measures used
in these techniques. This survey provides an insight into promising areas of
research in DL for analyzing radiographic images and thus, may further
accelerate the research in designing of customized DL based diagnostic tools
for effectively dealing with new variants of COVID-19 and emerging challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scheduling Techniques for Liver Segmentation: ReduceLRonPlateau Vs OneCycleLR. (arXiv:2202.06373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06373">
<div class="article-summary-box-inner">
<span><p>Machine learning and computer vision techniques have influenced many fields
including the biomedical one. The aim of this paper is to investigate the
important concept of schedulers in manipulating the learning rate (LR), for the
liver segmentation task, throughout the training process, focusing on the newly
devised OneCycleLR against the ReduceLRonPlateau. A dataset, published in 2018
and produced by the Medical Segmentation Decathlon Challenge organizers, called
Task 8 Hepatic Vessel (MSDC-T8) has been used for testing and validation. The
reported results that have the same number of maximum epochs (75), and are the
average of 5-fold cross-validation, indicate that ReduceLRonPlateau converges
faster while maintaining a similar or even better loss score on the validation
set when compared to OneCycleLR. The epoch at which the peak LR occurs perhaps
should be made early for the OneCycleLR such that the super-convergence feature
can be observed. Moreover, the overall results outperform the state-of-the-art
results from the researchers who published the liver masks for this dataset. To
conclude, both schedulers are suitable for medical segmentation challenges,
especially the MSDC-T8 dataset, and can be used confidently in rapidly
converging the validation loss with a minimal number of epochs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Sound Localization in the Wild by Cross-Modal Interference Erasing. (arXiv:2202.06406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06406">
<div class="article-summary-box-inner">
<span><p>The task of audio-visual sound source localization has been well studied
under constrained scenes, where the audio recordings are clean. However, in
real-world scenarios, audios are usually contaminated by off-screen sound and
background noise. They will interfere with the procedure of identifying desired
sources and building visual-sound connections, making previous studies
non-applicable. In this work, we propose the Interference Eraser (IEr)
framework, which tackles the problem of audio-visual sound source localization
in the wild. The key idea is to eliminate the interference by redefining and
carving discriminative audio representations. Specifically, we observe that the
previous practice of learning only a single audio representation is
insufficient due to the additive nature of audio signals. We thus extend the
audio representation with our Audio-Instance-Identifier module, which clearly
distinguishes sounding instances when audio signals of different volumes are
unevenly mixed. Then we erase the influence of the audible but off-screen
sounds and the silent but visible objects by a Cross-modal Referrer module with
cross-modality distillation. Quantitative and qualitative evaluations
demonstrate that our proposed framework achieves superior results on sound
localization tasks, especially under real-world scenarios. Code is available at
https://github.com/alvinliu0/Visual-Sound-Localization-in-the-Wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Point Cloud Encoding and Decoding with Lightweight Self-Attention based Model. (arXiv:2202.06407v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06407">
<div class="article-summary-box-inner">
<span><p>In this paper we present SA-CNN, a hierarchical and lightweight
self-attention based encoding and decoding architecture for representation
learning of point cloud data. The proposed SA-CNN introduces convolution and
transposed convolution stacks to capture and generate contextual information
among unordered 3D points. Following conventional hierarchical pipeline, the
encoding process extracts feature in local-to-global manner, while the decoding
process generates feature and point cloud in coarse-to-fine, multi-resolution
stages. We demonstrate that SA-CNN is capable of a wide range of applications,
namely classification, part segmentation, reconstruction, shape retrieval, and
unsupervised classification. While achieving the state-of-the-art or comparable
performance in the benchmarks, SA-CNN maintains its model complexity several
order of magnitude lower than the others. In term of qualitative results, we
visualize the multi-stage point cloud reconstructions and latent walks on rigid
objects as well as deformable non-rigid human and robot models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Action Recognition and Prediction: A Survey. (arXiv:1806.11230v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1806.11230">
<div class="article-summary-box-inner">
<span><p>Derived from rapid advances in computer vision and machine learning, video
analysis tasks have been moving from inferring the present state to predicting
the future state. Vision-based action recognition and prediction from videos
are such tasks, where action recognition is to infer human actions (present
state) based upon complete action executions, and action prediction to predict
human actions (future state) based upon incomplete action executions. These two
tasks have become particularly prevalent topics recently because of their
explosively emerging real-world applications, such as visual surveillance,
autonomous driving vehicle, entertainment, and video retrieval, etc. Many
attempts have been devoted in the last a few decades in order to build a robust
and effective framework for action recognition and prediction. In this paper,
we survey the complete state-of-the-art techniques in action recognition and
prediction. Existing models, popular algorithms, technical difficulties,
popular action databases, evaluation protocols, and promising future directions
are also provided with systematic discussions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Distillation for RGB-Depth Person Re-Identification. (arXiv:1810.11641v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1810.11641">
<div class="article-summary-box-inner">
<span><p>Person re-identification is a key challenge for surveillance across multiple
sensors. Prompted by the advent of powerful deep learning models for visual
recognition, and inexpensive RGB-D cameras and sensor-rich mobile robotic
platforms, e.g. self-driving vehicles, we investigate the relatively unexplored
problem of cross-modal re-identification of persons between RGB (color) and
depth images. The considerable divergence in data distributions across
different sensor modalities introduces additional challenges to the typical
difficulties like distinct viewpoints, occlusions, and pose and illumination
variation. While some work has investigated re-identification across RGB and
infrared, we take inspiration from successes in transfer learning from RGB to
depth in object detection tasks. Our main contribution is a novel method for
cross-modal distillation for robust person re-identification, which learns a
shared feature representation space of person's appearance in both RGB and
depth images. In addition, we propose a cross-modal attention mechanism where
the gating signal from one modality can dynamically activate the most
discriminant CNN filters of the other modality. The proposed distillation
method is compared to conventional and deep learning approaches proposed for
other cross-domain re-identification tasks. Results obtained on the public BIWI
and RobotPKU datasets indicate that the proposed method can significantly
outperform the state-of-the-art approaches by up to 16.1% in mean Average
Precision (mAP), demonstrating the benefit of the distillation paradigm. The
experimental results also indicate that using cross-modal attention allows to
improve recognition accuracy considerably with respect to the proposed
distillation method and relevant state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concise and Effective Network for 3D Human Modeling from Orthogonal Silhouettes. (arXiv:1912.11616v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.11616">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit the problem of 3D human modeling from two
orthogonal silhouettes of individuals (i.e., front and side views). Different
from our prior work, a supervised learning approach based on convolutional
neural network (CNN) is investigated to solve the problem by establishing a
mapping function that can effectively extract features from two silhouettes and
fuse them into coefficients in the shape space of human bodies. A new CNN
structure is proposed in our work to exact not only the discriminative features
of front and side views and also their mixed features for the mapping function.
3D human models with high accuracy are synthesized from coefficients generated
by the mapping function. Existing CNN approaches for 3D human modeling usually
learn a large number of parameters (from 8.5M to 355.4M) from two binary
images. Differently, we investigate a new network architecture and conduct the
samples on silhouettes as input. As a consequence, more accurate models can be
generated by our network with only 2.4M coefficients. The training of our
network is conducted on samples obtained by augmenting a publicly accessible
dataset. Learning transfer by using datasets with a smaller number of scanned
models is applied to our network to enable the function of generating results
with gender-oriented (or geographical) patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards High Performance Low Complexity Calibration in Appearance Based Gaze Estimation. (arXiv:2001.09284v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.09284">
<div class="article-summary-box-inner">
<span><p>Appearance-based gaze estimation from RGB images provides relatively
unconstrained gaze tracking. We have previously proposed a gaze decomposition
method that decomposes the gaze angle into the sum of a subject-independent
gaze estimate from the image and a subject-dependent bias. This paper extends
that work with a more complete characterization of the interplay between the
complexity of the calibration dataset and estimation accuracy. We analyze the
effect of the number of gaze targets, the number of images used per gaze target
and the number of head positions in calibration data using a new NISLGaze
dataset, which is well suited for analyzing these effects as it includes more
diversity in head positions and orientations for each subject than other
datasets. A better understanding of these factors enables low complexity high
performance calibration. Our results indicate that using only a single gaze
target and single head position is sufficient to achieve high quality
calibration, outperforming state-of-the-art methods by more than 6.3%. One of
the surprising findings is that the same estimator yields the best performance
both with and without calibration. To better understand the reasons, we provide
a new theoretical analysis that specifies the conditions under which this can
be expected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyclic Differentiable Architecture Search. (arXiv:2006.10724v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10724">
<div class="article-summary-box-inner">
<span><p>Differentiable ARchiTecture Search, i.e., DARTS, has drawn great attention in
neural architecture search. It tries to find the optimal architecture in a
shallow search network and then measures its performance in a deep evaluation
network. The independent optimization of the search and evaluation networks,
however, leaves room for potential improvement by allowing interaction between
the two networks. To address the problematic optimization issue, we propose new
joint optimization objectives and a novel Cyclic Differentiable ARchiTecture
Search framework, dubbed CDARTS. Considering the structure difference, CDARTS
builds a cyclic feedback mechanism between the search and evaluation networks
with introspective distillation. First, the search network generates an initial
architecture for evaluation, and the weights of the evaluation network are
optimized. Second, the architecture weights in the search network are further
optimized by the label supervision in classification, as well as the
regularization from the evaluation network through feature distillation.
Repeating the above cycle results in joint optimization of the search and
evaluation networks and thus enables the evolution of the architecture to fit
the final evaluation network. The experiments and analysis on CIFAR, ImageNet
and NAS-Bench-201 demonstrate the effectiveness of the proposed approach over
the state-of-the-art ones. Specifically, in the DARTS search space, we achieve
97.52% top-1 accuracy on CIFAR10 and 76.3% top-1 accuracy on ImageNet. In the
chain-structured search space, we achieve 78.2% top-1 accuracy on ImageNet,
which is 1.1% higher than EfficientNet-B0. Our code and models are publicly
available at https://github.com/researchmm/CDARTS.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision and Normalizing Flow-Based Defect Detection. (arXiv:2012.06737v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06737">
<div class="article-summary-box-inner">
<span><p>Visual defect detection is critical to ensure the quality of most products.
However, the majority of small and medium-sized manufacturing enterprises still
rely on tedious and error-prone human manual inspection. The main reasons
include: 1) the existing automated visual defect detection systems require
altering production assembly lines, which is time consuming and expensive 2)
the existing systems require manually collecting defective samples and labeling
them for a comparison-based algorithm or training a machine learning model.
This introduces a heavy burden for small and medium-sized manufacturing
enterprises as defects do not happen often and are difficult and time-consuming
to collect. Furthermore, we cannot exhaustively collect or define all defect
types as any new deviation from acceptable products are defects. In this paper,
we overcome these challenges and design a three-stage plug-and-play fully
automated unsupervised 360-degree defect detection system. In our system,
products are freely placed on an unaltered assembly line and receive 360 degree
visual inspection with multiple cameras from different angles. As such, the
images collected from real-world product assembly lines contain lots of
background noise. The products face different angles. The product sizes vary
due to the distance to cameras. All these make defect detection much more
difficult. Our system use object detection, background subtraction and
unsupervised normalizing flow-based defect detection techniques to tackle these
difficulties. Experiments show our system can achieve 0.90 AUROC in a
real-world non-altered drinkware production assembly line.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Facial Image Inpainting Based on an Encoder-Generator Architecture. (arXiv:2101.07036v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07036">
<div class="article-summary-box-inner">
<span><p>Facial image inpainting is a challenging problem as it requires generating
new pixels that include semantic information for masked key components in a
face, e.g., eyes and nose. Recently, remarkable methods have been proposed in
this field. Most of these approaches use encoder-decoder architectures and have
different limitations such as allowing unique results for a given image and a
particular mask. Alternatively, some optimization-based approaches generate
promising results using different masks with generator networks. However, these
approaches are computationally more expensive. In this paper, we propose an
efficient solution to the facial image inpainting problem using the Cyclic
Reverse Generator (CRG) architecture, which provides an encoder-generator
model. We use the encoder to embed a given image to the generator space and
incrementally inpaint the masked regions until a plausible image is generated;
we trained a discriminator model to assess the quality of the generated images
during the iterations and determine the convergence. After the generation
process, for the post-processing, we utilize a Unet model that we trained
specifically for this task to remedy the artifacts close to the mask
boundaries. We empirically observed that only a few iterations are sufficient
to generate realistic images with the proposed model. Since the models are not
trained for particular mask types, our method allows applying sketch-based
inpaintings, using a variety of mask types, and producing multiple and diverse
results. We compared our method with the state-of-the-art models both
quantitatively and qualitatively, and observed that our method can compete with
the other models in all mask types; it is particularly better in images where
larger masks are utilized. Our code, dataset and models are available at:
https://github.com/yahyadogan72/iterative facial image inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-source Pseudo-label Learning of Semantic Segmentation for the Scene Recognition of Agricultural Mobile Robots. (arXiv:2102.06386v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06386">
<div class="article-summary-box-inner">
<span><p>This paper describes a novel method of training a semantic segmentation model
for scene recognition of agricultural mobile robots exploiting publicly
available datasets of outdoor scenes that are different from the target
greenhouse environments. Semantic segmentation models require abundant labels
given by tedious manual annotation. A method to work around it is unsupervised
domain adaptation (UDA) that transfers knowledge from labeled source datasets
to unlabeled target datasets. However, the effectiveness of existing methods is
not well studied in adaptation between heterogeneous environments, such as
urban scenes and greenhouses. In this paper, we propose a method to train a
semantic segmentation model for greenhouse images without manually labeled
datasets of greenhouse images. The core of our idea is to use multiple rich
image datasets of different environments with segmentation labels to generate
pseudo-labels for the target images to effectively transfer the knowledge from
multiple sources and realize a precise training of semantic segmentation. Along
with the pseudo-label generation, we introduce state-of-the-art methods to deal
with noise in the pseudo-labels to further improve the performance. We
demonstrate in experiments with multiple greenhouse datasets that our proposed
method improves the performance compared to the single-source baselines and an
existing approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Gait Recognition: A Survey. (arXiv:2102.09546v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09546">
<div class="article-summary-box-inner">
<span><p>Gait recognition is an appealing biometric modality which aims to identify
individuals based on the way they walk. Deep learning has reshaped the research
landscape in this area since 2015 through the ability to automatically learn
discriminative representations. Gait recognition methods based on deep learning
now dominate the state-of-the-art in the field and have fostered real-world
applications. In this paper, we present a comprehensive overview of
breakthroughs and recent developments in gait recognition with deep learning,
and cover broad topics including datasets, test protocols, state-of-the-art
solutions, challenges, and future research directions. We first review the
commonly used gait datasets along with the principles designed for evaluating
them. We then propose a novel taxonomy made up of four separate dimensions
namely body representation, temporal representation, feature representation,
and neural architecture, to help characterize and organize the research
landscape and literature in this area. Following our proposed taxonomy, a
comprehensive survey of gait recognition methods using deep learning is
presented with discussions on their performances, characteristics, advantages,
and limitations. We conclude this survey with a discussion on current
challenges and mention a number of promising directions for future research in
gait recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View. (arXiv:2102.10543v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10543">
<div class="article-summary-box-inner">
<span><p>From the intuitive notion of disentanglement, the image variations
corresponding to different factors should be distinct from each other, and the
disentangled representation should reflect those variations with separate
dimensions. To discover the factors and learn disentangled representation,
previous methods typically leverage an extra regularization term when learning
to generate realistic images. However, the term usually results in a trade-off
between disentanglement and generation quality. For the generative models
pretrained without any disentanglement term, the generated images show
semantically meaningful variations when traversing along different directions
in the latent space. Based on this observation, we argue that it is possible to
mitigate the trade-off by $(i)$ leveraging the pretrained generative models
with high generation quality, $(ii)$ focusing on discovering the traversal
directions as factors for disentangled representation learning. To achieve
this, we propose Disentaglement via Contrast (DisCo) as a framework to model
the variations based on the target disentangled representations, and contrast
the variations to jointly discover disentangled directions and learn
disentangled representations. DisCo achieves the state-of-the-art disentangled
representation learning and distinct direction discovering, given pretrained
non-disentangled generative models including GAN, VAE, and Flow. Source code is
at https://github.com/xrenaa/DisCo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Representational Invariances for Data-Efficient Action Recognition. (arXiv:2103.16565v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16565">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a ubiquitous technique for improving image
classification when labeled data is scarce. Constraining the model predictions
to be invariant to diverse data augmentations effectively injects the desired
representational invariances to the model (e.g., invariance to photometric
variations) and helps improve accuracy. Compared to image data, the appearance
variations in videos are far more complex due to the additional temporal
dimension. Yet, data augmentation methods for videos remain under-explored.
This paper investigates various data augmentation strategies that capture
different video invariances, including photometric, geometric, temporal, and
actor/scene augmentations. When integrated with existing semi-supervised
learning frameworks, we show that our data augmentation strategy leads to
promising performance on the Kinetics-100/400, Mini-Something-v2, UCF-101, and
HMDB-51 datasets in the low-label regime. We also validate our data
augmentation strategy in the fully supervised setting and demonstrate improved
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Convolutional Neural Networks for Stalled Brain Capillary Detection. (arXiv:2104.01687v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01687">
<div class="article-summary-box-inner">
<span><p>Adequate blood supply is critical for normal brain function. Brain
vasculature dysfunctions such as stalled blood flow in cerebral capillaries are
associated with cognitive decline and pathogenesis in Alzheimer's disease.
Recent advances in imaging technology enabled generation of high-quality 3D
images that can be used to visualize stalled blood vessels. However,
localization of stalled vessels in 3D images is often required as the first
step for downstream analysis, which can be tedious, time-consuming and
error-prone, when done manually. Here, we describe a deep learning-based
approach for automatic detection of stalled capillaries in brain images based
on 3D convolutional neural networks. Our networks employed custom 3D data
augmentations and were used weight transfer from pre-trained 2D models for
initialization. We used an ensemble of several 3D models to produce the winning
submission to the Clog Loss: Advance Alzheimer's Research with Stall Catchers
machine learning competition that challenged the participants with classifying
blood vessels in 3D image stacks as stalled or flowing. In this setting, our
approach outperformed other methods and demonstrated state-of-the-art results,
achieving 0.85 Matthews correlation coefficient, 85% sensitivity, and 99.3%
specificity. The source code for our solution is made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lung Cancer Diagnosis Using Deep Attention Based on Multiple Instance Learning and Radiomics. (arXiv:2104.14655v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14655">
<div class="article-summary-box-inner">
<span><p>Early diagnosis of lung cancer is a key intervention for the treatment of
lung cancer computer aided diagnosis (CAD) can play a crucial role. However,
most published CAD methods treat lung cancer diagnosis as a lung nodule
classification problem, which does not reflect clinical practice, where
clinicians diagnose a patient based on a set of images of nodules, instead of
one specific nodule. Besides, the low interpretability of the output provided
by these methods presents an important barrier for their adoption. In this
article, we treat lung cancer diagnosis as a multiple instance learning (MIL)
problem in order to better reflect the diagnosis process in the clinical
setting and for the higher interpretability of the output. We chose radiomics
as the source of input features and deep attention-based MIL as the
classification algorithm.The attention mechanism provides higher
interpretability by estimating the importance of each instance in the set for
the final diagnosis.In order to improve the model's performance in a small
imbalanced dataset, we introduce a new bag simulation method for MIL.The
results show that our method can achieve a mean accuracy of 0.807 with a
standard error of the mean (SEM) of 0.069, a recall of 0.870 (SEM 0.061), a
positive predictive value of 0.928 (SEM 0.078), a negative predictive value of
0.591 (SEM 0.155) and an area under the curve (AUC) of 0.842 (SEM 0.074),
outperforming other MIL methods.Additional experiments show that the proposed
oversampling strategy significantly improves the model's performance. In
addition, our experiments show that our method provides an indication of the
importance of each nodule in determining the diagnosis, which combined with the
well-defined radiomic features, make the results more interpretable and
acceptable for doctors and patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12639">
<div class="article-summary-box-inner">
<span><p>Neural network ensembles, such as Bayesian neural networks (BNNs), have shown
success in the areas of uncertainty estimation and robustness. However, a
crucial challenge prohibits their use in practice. BNNs require a large number
of predictions to produce reliable results, leading to a significant increase
in computational cost. To alleviate this issue, we propose spatial smoothing, a
method that spatially ensembles neighboring feature map points of convolutional
neural networks. By simply adding a few blur layers to the models, we
empirically show that spatial smoothing improves accuracy, uncertainty
estimation, and robustness of BNNs across a whole range of ensemble sizes. In
particular, BNNs incorporating spatial smoothing achieve high predictive
performance merely with a handful of ensembles. Moreover, this method also can
be applied to canonical deterministic neural networks to improve the
performances. A number of evidences suggest that the improvements can be
attributed to the stabilized feature maps and the smoothing of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing them
as special cases of spatial smoothing. These not only enhance accuracy, but
also improve uncertainty estimation and robustness by making the loss landscape
smoother in the same manner as spatial smoothing. The code is available at
https://github.com/xxxnell/spatial-smoothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Real-time and Light-weight Line Segment Detection. (arXiv:2106.00186v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00186">
<div class="article-summary-box-inner">
<span><p>Previous deep learning-based line segment detection (LSD) suffers from the
immense model size and high computational cost for line prediction. This
constrains them from real-time inference on computationally restricted
environments. In this paper, we propose a real-time and light-weight line
segment detector for resource-constrained environments named Mobile LSD
(M-LSD). We design an extremely efficient LSD architecture by minimizing the
backbone network and removing the typical multi-module process for line
prediction found in previous methods. To maintain competitive performance with
a light-weight network, we present novel training schemes: Segments of Line
segment (SoL) augmentation, matching and geometric loss. SoL augmentation
splits a line segment into multiple subparts, which are used to provide
auxiliary line data during the training process. Moreover, the matching and
geometric loss allow a model to capture additional geometric cues. Compared
with TP-LSD-Lite, previously the best real-time LSD method, our model
(M-LSD-tiny) achieves competitive performance with 2.5% of model size and an
increase of 130.5% in inference speed on GPU. Furthermore, our model runs at
56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices,
respectively. To the best of our knowledge, this is the first real-time deep
LSD available on mobile devices. Our code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P2T: Pyramid Pooling Transformer for Scene Understanding. (arXiv:2106.12011v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12011">
<div class="article-summary-box-inner">
<span><p>Recently, the vision transformer has achieved great successes by pushing the
state-of-the-arts of various vision tasks. One of the most challenging problems
in the vision transformer is that the large sequence length of image tokens
leads to high computational cost (quadratic complexity). A popular solution to
this problem is to use a single pooling operation to reduce the sequence
length. This paper considers how to improve existing vision transformers where
the pooled feature extracted by a single pooling operation seems less powerful.
To this end, we note that pyramid pooling has been demonstrated to be effective
in various vision tasks owing to its powerful ability in context abstraction.
However, pyramid pooling has not been explored in backbone network design. To
bridge this gap, we propose to adapt pyramid pooling to Multi-Head
Self-Attention (MHSA) in the vision transformer, simultaneously reducing the
sequence length and capturing powerful contextual features. Plugged with our
pooling-based MHSA, we build a universal vision transformer backbone, dubbed
Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when
applied P2T as the backbone network, it shows substantial superiority in
various vision tasks such as image classification, semantic segmentation,
object detection, and instance segmentation, compared to previous CNN- and
transformer-based networks. The code will be released at
https://github.com/yuhuan-wu/P2T.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No-Reference Quality Assessment for 3D Colored Point Cloud and Mesh Models. (arXiv:2107.02041v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02041">
<div class="article-summary-box-inner">
<span><p>To improve the viewer's Quality of Experience (QoE) and optimize computer
graphics applications, 3D model quality assessment (3D-QA) has become an
important task in the multimedia area. Point cloud and mesh are the two most
widely used digital representation formats of 3D models, the visual quality of
which is quite sensitive to lossy operations like simplification and
compression. Therefore, many related studies such as point cloud quality
assessment (PCQA) and mesh quality assessment (MQA) have been carried out to
measure the visual quality degradations of 3D models. However, a large part of
previous studies utilize full-reference (FR) metrics, which indicates they can
not predict the quality level with the absence of the reference 3D model.
Furthermore, few 3D-QA metrics consider color information, which significantly
restricts their effectiveness and scope of application. In this paper, we
propose a no-reference (NR) quality assessment metric for colored 3D models
represented by both point cloud and mesh. First, we project the 3D models from
3D space into quality-related geometry and color feature domains. Then, the 3D
natural scene statistics (3D-NSS) and entropy are utilized to extract
quality-aware features. Finally, machine learning is employed to regress the
quality-aware features into visual quality scores. Our method is validated on
the colored point cloud quality assessment database (SJTU-PCQA), the Waterloo
point cloud assessment database (WPC), and the colored mesh quality assessment
database (CMDM). The experimental results show that the proposed method
outperforms most compared NR 3D-QA metrics with competitive computational
resources and greatly reduces the performance gap with the state-of-the-art FR
3D-QA metrics. The code of the proposed model is publicly available now to
facilitate further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. (arXiv:2107.08221v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08221">
<div class="article-summary-box-inner">
<span><p>An important component for generalization in machine learning is to uncover
underlying latent factors of variation as well as the mechanism through which
each factor acts in the world. In this paper, we test whether 17 unsupervised,
weakly supervised, and fully supervised representation learning approaches
correctly infer the generative factors of variation in simple datasets
(dSprites, Shapes3D, MPI3D) from controlled environments, and on our
contributed CelebGlow dataset. In contrast to prior robustness work that
introduces novel factors of variation during test time, such as blur or other
(un)structured noise, we here recompose, interpolate, or extrapolate only
existing factors of variation from the training data set (e.g., small and
medium-sized objects during training and large objects during testing). Models
that learn the correct mechanism should be able to generalize to this
benchmark. In total, we train and test 2000+ models and observe that all of
them struggle to learn the underlying mechanism regardless of supervision
signal and architectural bias. Moreover, the generalization capabilities of all
tested models drop significantly as we move from artificial datasets towards
more realistic real-world datasets. Despite their inability to identify the
correct mechanism, the models are quite modular as their ability to infer other
in-distribution factors remains fairly stable, providing only a single factor
is out-of-distribution. These results point to an important yet understudied
problem of learning mechanistic models of observations that can facilitate
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCDAL: Maximum Classifier Discrepancy for Active Learning. (arXiv:2107.11049v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11049">
<div class="article-summary-box-inner">
<span><p>Recent state-of-the-art active learning methods have mostly leveraged
Generative Adversarial Networks (GAN) for sample acquisition; however, GAN is
usually known to suffer from instability and sensitivity to hyper-parameters.
In contrast to these methods, we propose in this paper a novel active learning
framework that we call Maximum Classifier Discrepancy for Active Learning
(MCDAL) which takes the prediction discrepancies between multiple classifiers.
In particular, we utilize two auxiliary classification layers that learn
tighter decision boundaries by maximizing the discrepancies among them.
Intuitively, the discrepancies in the auxiliary classification layers'
predictions indicate the uncertainty in the prediction. In this regard, we
propose a novel method to leverage the classifier discrepancies for the
acquisition function for active learning. We also provide an interpretation of
our idea in relation to existing GAN based active learning methods and domain
adaptation frameworks. Moreover, we empirically demonstrate the utility of our
approach where the performance of our approach exceeds the state-of-the-art
methods on several image classification and semantic segmentation datasets in
active learning setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NODEO: A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration. (arXiv:2108.03443v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03443">
<div class="article-summary-box-inner">
<span><p>Deformable image registration (DIR), aiming to find spatial correspondence
between images, is one of the most critical problems in the domain of medical
image analysis. In this paper, we present a novel, generic, and accurate
diffeomorphic image registration framework that utilizes neural ordinary
differential equations (NODEs). We model each voxel as a moving particle and
consider the set of all voxels in a 3D image as a high-dimensional dynamical
system whose trajectory determines the targeted deformation field. Our method
leverages deep neural networks for their expressive power in modeling dynamical
systems, and simultaneously optimizes for a dynamical system between the image
pairs and the corresponding transformation. Our formulation allows various
constraints to be imposed along the transformation to maintain desired
regularities. Our experiment results show that our method outperforms the
benchmarks under various metrics. Additionally, we demonstrate the feasibility
to expand our framework to register multiple image sets using a unified form of
transformation,which could possibly serve a wider range of applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06932">
<div class="article-summary-box-inner">
<span><p>Most polyp segmentation methods use CNNs as their backbone, leading to two
key issues when exchanging information between the encoder and decoder: 1)
taking into account the differences in contribution between different-level
features; and 2) designing an effective mechanism for fusing these features.
Different from existing CNN-based methods, we adopt a transformer encoder,
which learns more powerful and robust representations. In addition, considering
the image acquisition influence and elusive properties of polyps, we introduce
three novel modules, including a cascaded fusion module (CFM), a camouflage
identification module (CIM), a and similarity aggregation module (SAM). Among
these, the CFM is used to collect the semantic and location information of
polyps from high-level features, while the CIM is applied to capture polyp
information disguised in low-level features. With the help of the SAM, we
extend the pixel features of the polyp area with high-level semantic position
information to the entire polyp area, thereby effectively fusing cross-level
features. The proposed model, named \ourmodel, effectively suppresses noises in
the features and significantly improves their expressive capabilities.
Extensive experiments on five widely adopted datasets show that the proposed
model is more robust to various challenging situations (e.g. appearance
changes, small objects) than existing methods, and achieves the new
state-of-the-art performance. The proposed model is available at
https://github.com/DengPingFan/Polyp-PVT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Attentive Deep Neural Network for Exposing GAN-generated Faces. (arXiv:2109.02167v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02167">
<div class="article-summary-box-inner">
<span><p>GAN-based techniques that generate and synthesize realistic faces have caused
severe social concerns and security problems. Existing methods for detecting
GAN-generated faces can perform well on limited public datasets. However,
images from existing public datasets do not represent real-world scenarios well
enough in terms of view variations and data distributions (where real faces
largely outnumber synthetic faces). The state-of-the-art methods do not
generalize well in real-world problems and lack the interpretability of
detection results. Performance of existing GAN-face detection models degrades
significantly when facing imbalanced data distributions. To address these
shortcomings, we propose a robust, attentive, end-to-end network that can spot
GAN-generated faces by analyzing their eye inconsistencies. Specifically, our
model learns to identify inconsistent eye components by localizing and
comparing the iris artifacts between the two eyes automatically. Our deep
network addresses the imbalance learning issues by considering the AUC loss and
the traditional cross-entropy loss jointly. Comprehensive evaluations of the
FFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the
superiority of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04468">
<div class="article-summary-box-inner">
<span><p>Image-to-image (i2i) networks struggle to capture local changes because they
do not affect the global scene structure. For example, translating from highway
scenes to offroad, i2i networks easily focus on global color features but
ignore obvious traits for humans like the absence of lane markings. In this
paper, we leverage human knowledge about spatial domain characteristics which
we refer to as 'local domains' and demonstrate its benefit for image-to-image
translation. Relying on a simple geometrical guidance, we train a patch-based
GAN on few source data and hallucinate a new unseen domain which subsequently
eases transfer learning to target. We experiment on three tasks ranging from
unstructured environments to adverse weather. Our comprehensive evaluation
setting shows we are able to generate realistic translations, with minimal
priors, and training only on a few images. Furthermore, when trained on our
translations images we show that all tested proxy tasks are significantly
improved, without ever seeing target domain at training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey on deep learning approaches for breast cancer diagnosis. (arXiv:2109.08853v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08853">
<div class="article-summary-box-inner">
<span><p>Deep learning has introduced several learning-based methods to recognize
breast tumours and presents high applicability in breast cancer diagnostics. It
has presented itself as a practical installment in Computer-Aided Diagnostic
(CAD) systems to further assist radiologists in diagnostics for different
modalities. A deep learning network trained on images provided by hospitals or
public databases can perform classification, detection, and segmentation of
lesion types. Significant progress has been made in recognizing tumours on 2D
images but recognizing 3D images remains a frontier so far. The interconnection
of deep learning networks between different fields of study help propels
discoveries for more efficient, accurate, and robust networks. In this review
paper, the following topics will be explored: (i) theory and application of
deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast
tumour recognition from a performance metric perspective, and (iii) challenges
faced in CNN approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Step Denoiser for convergent Plug-and-Play. (arXiv:2110.03220v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03220">
<div class="article-summary-box-inner">
<span><p>Plug-and-Play methods constitute a class of iterative algorithms for imaging
problems where regularization is performed by an off-the-shelf denoiser.
Although Plug-and-Play methods can lead to tremendous visual performance for
various image problems, the few existing convergence guarantees are based on
unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly
convex data terms. In this work, we propose a new type of Plug-and-Play
methods, based on half-quadratic splitting, for which the denoiser is realized
as a gradient descent step on a functional parameterized by a deep neural
network. Exploiting convergence results for proximal gradient descent
algorithms in the non-convex setting, we show that the proposed Plug-and-Play
algorithm is a convergent iterative scheme that targets stationary points of an
explicit global functional. Besides, experiments show that it is possible to
learn such a deep denoiser while not compromising the performance in comparison
to other state-of-the-art deep denoisers used in Plug-and-Play schemes. We
apply our proximal gradient algorithm to various ill-posed inverse problems,
e.g. deblurring, super-resolution and inpainting. For all these applications,
numerical results empirically confirm the convergence results. Experiments also
show that this new algorithm reaches state-of-the-art performance, both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoDeRNN: Towards Fine-grained Motion Details for Spatiotemporal Predictive Learning. (arXiv:2110.12978v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12978">
<div class="article-summary-box-inner">
<span><p>Spatiotemporal predictive learning (ST-PL) aims at predicting the subsequent
frames via limited observed sequences, and it has broad applications in the
real world. However, learning representative spatiotemporal features for
prediction is challenging. Moreover, chaotic uncertainty among consecutive
frames exacerbates the difficulty in long-term prediction. This paper
concentrates on improving prediction quality by enhancing the correspondence
between the previous context and the current state. We carefully design Detail
Context Block (DCB) to extract fine-grained details and improve the isolated
correlation between upper context state and current input state. We integrate
DCB with standard ConvLSTM and introduce Motion Details RNN (MoDeRNN) to
capture fine-grained spatiotemporal features and improve the expression of
latent states of RNNs to achieve significant quality. Experiments on Moving
MNIST and Typhoon datasets demonstrate the effectiveness of the proposed
method. MoDeRNN outperforms existing state-of-the-art techniques qualitatively
and quantitatively with lower computation loads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Integrated Pipeline of Segmentation Guided Classification of Breast Cancer from Ultrasound Images. (arXiv:2110.14013v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14013">
<div class="article-summary-box-inner">
<span><p>Breast cancer has become a symbol of tremendous concern in the modern world,
as it is one of the major causes of cancer mortality worldwide. In this regard,
breast ultrasonography images are frequently utilized by doctors to diagnose
breast cancer at an early stage. However, the complex artifacts and heavily
noised breast ultrasonography images make diagnosis a great challenge.
Furthermore, the ever-increasing number of patients being screened for breast
cancer necessitates the use of automated end-to-end technology for highly
accurate diagnosis at a low cost and in a short time. In this concern, to
develop an end-to-end integrated pipeline for breast ultrasonography image
classification, we conducted an exhaustive analysis of image preprocessing
methods such as K Means++ and SLIC, as well as four transfer learning models
such as VGG16, VGG19, DenseNet121, and ResNet50. With a Dice-coefficient score
of 63.4 in the segmentation stage and accuracy and an F1-Score (Benign) of
73.72 percent and 78.92 percent in the classification stage, the combination of
SLIC, UNET, and VGG16 outperformed all other integrated combinations. Finally,
we have proposed an end to end integrated automated pipelining framework which
includes preprocessing with SLIC to capture super-pixel features from the
complex artifact of ultrasonography images, complementing semantic segmentation
with modified U-Net, leading to breast tumor classification using a transfer
learning approach with a pre-trained VGG16 and a densely connected neural
network. The proposed automated pipeline can be effectively implemented to
assist medical practitioners in making more accurate and timely diagnoses of
breast cancer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Quantity in Percentage of Factory Machines Using Computer Vision and Mathematical Methods. (arXiv:2111.05080v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05080">
<div class="article-summary-box-inner">
<span><p>Computer vision has been thriving since AI development was gaining thrust.
Using deep learning techniques has been the most popular way which computer
scientists thought the solution of. However, deep learning techniques tend to
show lower performance than manual processing. Using deep learning is not
always the answer to a problem related to computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Robust Unsupervised Video Person Re-identification. (arXiv:2111.05170v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05170">
<div class="article-summary-box-inner">
<span><p>Unsupervised video person re-identification (reID) methods usually depend on
global-level features. And many supervised reID methods employed local-level
features and achieved significant performance improvements. However, applying
local-level features to unsupervised methods may introduce an unstable
performance. To improve the performance stability for unsupervised video reID,
this paper introduces a general scheme fusing part models and unsupervised
learning. In this scheme, the global-level feature is divided into equal
local-level feature. A local-aware module is employed to explore the poentials
of local-level feature for unsupervised learning. A global-aware module is
proposed to overcome the disadvantages of local-level features. Features from
these two modules are fused to form a robust feature representation for each
input image. This feature representation has the advantages of local-level
feature without suffering from its disadvantages. Comprehensive experiments are
conducted on three benchmarks, including PRID2011, iLIDS-VID, and
DukeMTMC-VideoReID, and the results demonstrate that the proposed approach
achieves state-of-the-art performance. Extensive ablation studies demonstrate
the effectiveness and robustness of proposed scheme, local-aware module and
global-aware module. The code and generated features are available at
https://github.com/deropty/uPMnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Probability Estimation. (arXiv:2111.10734v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10734">
<div class="article-summary-box-inner">
<span><p>Reliable probability estimation is of crucial importance in many real-world
applications where there is inherent uncertainty, such as weather forecasting,
medical prognosis, or collision avoidance in autonomous vehicles.
Probability-estimation models are trained on observed outcomes (e.g. whether it
has rained or not, or whether a patient has died or not), because the
ground-truth probabilities of the events of interest are typically unknown. The
problem is therefore analogous to binary classification, with the important
difference that the objective is to estimate probabilities rather than
predicting the specific outcome. The goal of this work is to investigate
probability estimation from high-dimensional data using deep neural networks.
There exist several methods to improve the probabilities generated by these
models but they mostly focus on classification problems where the probabilities
are related to model uncertainty. In the case of problems with inherent
uncertainty, it is challenging to evaluate performance without access to
ground-truth probabilities. To address this, we build a synthetic dataset to
study and compare different computable metrics. We evaluate existing methods on
the synthetic data as well as on three real-world probability estimation tasks,
all of which involve inherent uncertainty. We also give a theoretical analysis
of a model for high-dimensional probability estimation which reproduces several
of the phenomena evinced in our experiments. Finally, we propose a new method
for probability estimation using neural networks, which modifies the training
process to promote output probabilities that are consistent with empirical
probabilities computed from the data. The method outperforms existing
approaches on most metrics on the simulated as well as real-world data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handwritten Mathematical Expression Recognition via Attention Aggregation based Bi-directional Mutual Learning. (arXiv:2112.03603v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03603">
<div class="article-summary-box-inner">
<span><p>Handwritten mathematical expression recognition aims to automatically
generate LaTeX sequences from given images. Currently, attention-based
encoder-decoder models are widely used in this task. They typically generate
target sequences in a left-to-right (L2R) manner, leaving the right-to-left
(R2L) contexts unexploited. In this paper, we propose an Attention aggregation
based Bi-directional Mutual learning Network (ABM) which consists of one shared
encoder and two parallel inverse decoders (L2R and R2L). The two decoders are
enhanced via mutual distillation, which involves one-to-one knowledge transfer
at each training step, making full use of the complementary information from
two inverse directions. Moreover, in order to deal with mathematical symbols in
diverse scales, an Attention Aggregation Module (AAM) is proposed to
effectively integrate multi-scale coverage attentions. Notably, in the
inference phase, given that the model already learns knowledge from two inverse
directions, we only use the L2R branch for inference, keeping the original
parameter size and inference speed. Extensive experiments demonstrate that our
proposed approach achieves the recognition accuracy of 56.85 % on CROHME 2014,
52.92 % on CROHME 2016, and 53.96 % on CROHME 2019 without data augmentation
and model ensembling, substantially outperforming the state-of-the-art methods.
The source code is available in https://github.com/XH-B/ABM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adverse Weather Image Translation with Asymmetric and Uncertainty-aware GAN. (arXiv:2112.04283v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04283">
<div class="article-summary-box-inner">
<span><p>Adverse weather image translation belongs to the unsupervised image-to-image
(I2I) translation task which aims to transfer adverse condition domain (eg,
rainy night) to standard domain (eg, day). It is a challenging task because
images from adverse domains have some artifacts and insufficient information.
Recently, many studies employing Generative Adversarial Networks (GANs) have
achieved notable success in I2I translation but there are still limitations in
applying them to adverse weather enhancement. Symmetric architecture based on
bidirectional cycle-consistency loss is adopted as a standard framework for
unsupervised domain transfer methods. However, it can lead to inferior
translation result if the two domains have imbalanced information. To address
this issue, we propose a novel GAN model, i.e., AU-GAN, which has an asymmetric
architecture for adverse domain translation. We insert a proposed feature
transfer network (${T}$-net) in only a normal domain generator (i.e., rainy
night-&gt; day) to enhance encoded features of the adverse domain image. In
addition, we introduce asymmetric feature matching for disentanglement of
encoded features. Finally, we propose uncertainty-aware cycle-consistency loss
to address the regional uncertainty of a cyclic reconstructed image. We
demonstrate the effectiveness of our method by qualitative and quantitative
comparisons with state-of-the-art models. Codes are available at
https://github.com/jgkwak95/AU-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Atlas Building with Deep Registration Priors. (arXiv:2112.06406v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06406">
<div class="article-summary-box-inner">
<span><p>Registration-based atlas building often poses computational challenges in
high-dimensional image spaces. In this paper, we introduce a novel hybrid atlas
building algorithm that fast estimates atlas from large-scale image datasets
with much reduced computational cost. In contrast to previous approaches that
iteratively perform registration tasks between an estimated atlas and
individual images, we propose to use learned priors of registration from
pre-trained neural networks. This newly developed hybrid framework features
several advantages of (i) providing an efficient way of atlas building without
losing the quality of results, and (ii) offering flexibility in utilizing a
wide variety of deep learning based registration methods. We demonstrate the
effectiveness of this proposed model on 3D brain magnetic resonance imaging
(MRI) scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04756">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the solution of roadside LiDAR object detection
using a combination of two unsupervised learning algorithms. The 3D point
clouds data are firstly converted into spherical coordinates and filled into
the elevation-azimuth matrix using a hash function. After that, the raw LiDAR
data were rearranged into new data structures to store the information of
range, azimuth, and intensity. Then, the Dynamic Mode Decomposition method is
applied for decomposing the LiDAR data into low-rank backgrounds and sparse
foregrounds based on intensity channel pattern recognition. The Coarse Fine
Triangle Algorithm (CFTA) automatically finds the dividing value to separate
the moving targets from static background according to range information. After
intensity and range background subtraction, the foreground moving objects will
be detected using a density-based detector and encoded into the state-space
model for tracking. The output of the proposed model includes vehicle
trajectories that can enable many mobility and safety applications. The method
was validated at both path and point levels against a commercial traffic data
collection platform and outperformed the state-of-the-art. In contrast to the
previous methods that process directly on the scattered and discrete point
clouds, the dynamic classification method can establish the less sophisticated
linear relationship of the 3D measurement data, which captures the
spatial-temporal structure that we often desire.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Detection in Extreme Conditions: A Machine-learning Approach. (arXiv:2201.06220v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06220">
<div class="article-summary-box-inner">
<span><p>Face detection in unrestricted conditions has been a trouble for years due to
various expressions, brightness, and coloration fringing. Recent studies show
that deep learning knowledge of strategies can acquire spectacular performance
inside the identification of different gadgets and patterns. This face
detection in unconstrained surroundings is difficult due to various poses,
illuminations, and occlusions. Figuring out someone with a picture has been
popularized through the mass media. However, it's miles less sturdy to
fingerprint or retina scanning. The latest research shows that deep mastering
techniques can gain mind-blowing performance on those two responsibilities. In
this paper, I recommend a deep cascaded multi-venture framework that exploits
the inherent correlation among them to boost up their performance. In
particular, my framework adopts a cascaded shape with 3 layers of cautiously
designed deep convolutional networks that expect face and landmark region in a
coarse-to-fine way. Besides, within the gaining knowledge of the procedure, I
propose a new online tough sample mining method that can enhance the
performance robotically without manual pattern choice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reading-strategy Inspired Visual Representation Learning for Text-to-Video Retrieval. (arXiv:2201.09168v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09168">
<div class="article-summary-box-inner">
<span><p>This paper aims for the task of text-to-video retrieval, where given a query
in the form of a natural-language sentence, it is asked to retrieve videos
which are semantically relevant to the given query, from a great number of
unlabeled videos. The success of this task depends on cross-modal
representation learning that projects both videos and sentences into common
spaces for semantic similarity computation. In this work, we concentrate on
video representation learning, an essential component for text-to-video
retrieval. Inspired by the reading strategy of humans, we propose a
Reading-strategy Inspired Visual Representation Learning (RIVRL) to represent
videos, which consists of two branches: a previewing branch and an
intensive-reading branch. The previewing branch is designed to briefly capture
the overview information of videos, while the intensive-reading branch is
designed to obtain more in-depth information. Moreover, the intensive-reading
branch is aware of the video overview captured by the previewing branch. Such
holistic information is found to be useful for the intensive-reading branch to
extract more fine-grained features. Extensive experiments on three datasets are
conducted, where our model RIVRL achieves a new state-of-the-art on TGIF and
VATEX. Moreover, on MSR-VTT, our model using two video features shows
comparable performance to the state-of-the-art using seven video features and
even outperforms models pre-trained on the large-scale HowTo100M dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POTHER: Patch-Voted Deep Learning-based Chest X-ray Bias Analysis for COVID-19 Detection. (arXiv:2201.09360v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09360">
<div class="article-summary-box-inner">
<span><p>A critical step in the fight against COVID-19, which continues to have a
catastrophic impact on peoples lives, is the effective screening of patients
presented in the clinics with severe COVID-19 symptoms. Chest radiography is
one of the promising screening approaches. Many studies reported detecting
COVID-19 in chest X-rays accurately using deep learning. A serious limitation
of many published approaches is insufficient attention paid to explaining
decisions made by deep learning models. Using explainable artificial
intelligence methods, we demonstrate that model decisions may rely on
confounding factors rather than medical pathology. After an analysis of
potential confounding factors found on chest X-ray images, we propose a novel
method to minimise their negative impact. We show that our proposed method is
more robust than previous attempts to counter confounding factors such as ECG
leads in chest X-rays that often influence model classification decisions. In
addition to being robust, our method achieves results comparable to the
state-of-the-art. The source code and pre-trained weights are publicly
available (https://github.com/tomek1911/POTHER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beam Stack Search-based reconstruction of unhealthy coronary artery wall segmentations in CCTA-CPR scans. (arXiv:2201.10424v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10424">
<div class="article-summary-box-inner">
<span><p>The estimation of the coronary artery wall boundaries in CCTA scans is a
costly but essential task in the diagnosis of cardiac diseases. To automatize
this task, deep learning-based image segmentation methods are commonly used.
However, in the case of coronary artery wall, even state-of-the-art
segmentation methods fail to produce an accurate boundary in the presence of
plaques and bifurcations. Post-processing reconstruction methods have been
proposed to further refine segmentation results, but when applying
general-purpose reconstruction to artery wall segmentations, they fail to
reproduce the wide variety of boundary shapes. In this paper, we propose a
novel method for reconstructing coronary artery wall segmentations, the Tube
Beam Stack Search (TBSS). By leveraging the voxel shape of adjacent slices in a
CPR volume, our TBSS is capable of finding the most plausible path of the
artery wall. Similarly to the original Beam Stack Search, TBSS navigates along
the voxel probabilities output by the segmentation method, reconstructing the
inner and outer artery walls. Finally, skeletonization is applied on the TBSS
reconstructions to eliminate noise and produce more refined segmentations.
Also, since our method does not require learning a model, the lack of annotated
data is not a limitation. We evaluated our method on a dataset of coronary CT
angiography with curved planar reconstruction (CCTA-CPR) of 92 arteries.
Experimental results show that our method outperforms the state-of-the-art work
in reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Low Can We Go? Pixel Annotation for Semantic Segmentation. (arXiv:2201.10448v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10448">
<div class="article-summary-box-inner">
<span><p>How many labeled pixels are needed to segment an image, without any prior
knowledge? We conduct an experiment to answer this question.
</p>
<p>In our experiment, an Oracle is using Active Learning to train a network from
scratch. The Oracle has access to the entire label map of the image, but the
goal is to reveal as little pixel labels to the network as possible. We find
that, on average, the Oracle needs to reveal (i.e., annotate) less than 0.1% of
the pixels in order to train a network. The network can then label all pixels
in the image at an accuracy of more than 98%.
</p>
<p>Based on this single-image-annotation experiment, we design an experiment to
quickly annotate an entire data set. In the data set level experiment the
Oracle trains a new network for each image from scratch. The network can then
be used to create pseudo-labels, which are the network predicted labels of the
unlabeled pixels, for the entire image. Only then, a data set level network is
trained from scratch on all the pseudo-labeled images at once.
</p>
<p>We repeat both image level and data set level experiments on two, very
different, real-world data sets, and find that it is possible to reach the
performance of a fully annotated data set using a fraction of the annotation
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-Based Framework for Camera Calibration with Distortion Correction and High Precision Feature Detection. (arXiv:2202.00158v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00158">
<div class="article-summary-box-inner">
<span><p>Camera calibration is a crucial technique which significantly influences the
performance of many robotic systems. Robustness and high precision have always
been the pursuit of diverse calibration methods. State-of-the-art calibration
techniques based on classical Zhang's method, however, still suffer from
environmental noise, radial lens distortion and sub-optimal parameter
estimation. Therefore, in this paper, we propose a hybrid camera calibration
framework which combines learning-based approaches with traditional methods to
handle these bottlenecks. In particular, this framework leverages
learning-based approaches to perform efficient distortion correction and robust
chessboard corner coordinate encoding. For sub-pixel accuracy of corner
detection, a specially-designed coordinate decoding algorithm with embed
outlier rejection mechanism is proposed. To avoid sub-optimal estimation
results, we improve the traditional parameter estimation by RANSAC algorithm
and achieve stable results. Compared with two widely-used camera calibration
toolboxes, experiment results on both real and synthetic datasets manifest the
better robustness and higher precision of the proposed framework. The massive
synthetic dataset is the basis of our framework's decent performance and will
be publicly available along with the code at
https://github.com/Easonyesheng/CCS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Online Meta-Learning Without Task Boundaries. (arXiv:2202.00263v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00263">
<div class="article-summary-box-inner">
<span><p>While deep networks can learn complex functions such as classifiers,
detectors, and trackers, many applications require models that continually
adapt to changing input distributions, changing tasks, and changing
environmental conditions. Indeed, this ability to continuously accrue knowledge
and use past experience to learn new tasks quickly in continual settings is one
of the key properties of an intelligent system. For complex and
high-dimensional problems, simply updating the model continually with standard
learning algorithms such as gradient descent may result in slow adaptation.
Meta-learning can provide a powerful tool to accelerate adaptation yet is
conventionally studied in batch settings. In this paper, we study how
meta-learning can be applied to tackle online problems of this nature,
simultaneously adapting to changing tasks and input distributions and
meta-training the model in order to adapt more quickly in the future. Extending
meta-learning into the online setting presents its own challenges, and although
several prior methods have studied related problems, they generally require a
discrete notion of tasks, with known ground-truth task boundaries. Such methods
typically adapt to each task in sequence, resetting the model between tasks,
rather than adapting continuously across tasks. In many real-world settings,
such discrete boundaries are unavailable, and may not even exist. To address
these settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which
does not require any ground truth knowledge about the task boundaries and stays
fully online without resetting back to pre-trained weights. Our experiments
show that FOML was able to learn new tasks faster than the state-of-the-art
online learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRBF-Fusion: Accurate 3D reconstruction from RGB-D data using on-the-fly implicits. (arXiv:2202.01829v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01829">
<div class="article-summary-box-inner">
<span><p>Reconstruction of high-fidelity 3D objects or scenes is a fundamental
research problem. Recent advances in RGB-D fusion have demonstrated the
potential of producing 3D models from consumer-level RGB-D cameras. However,
due to the discrete nature and limited resolution of their surface
representations (e.g., point- or voxel-based), existing approaches suffer from
the accumulation of errors in camera tracking and distortion in the
reconstruction, which leads to an unsatisfactory 3D reconstruction. In this
paper, we present a method using on-the-fly implicits of Hermite Radial Basis
Functions (HRBFs) as a continuous surface representation for camera tracking in
an existing RGB-D fusion framework. Furthermore, curvature estimation and
confidence evaluation are coherently derived from the inherent surface
properties of the on-the-fly HRBF implicits, which devote to a data fusion with
better quality. We argue that our continuous but on-the-fly surface
representation can effectively mitigate the impact of noise with its robustness
and constrain the reconstruction with inherent surface smoothness when being
compared with discrete representations. Experimental results on various
real-world and synthetic datasets demonstrate that our HRBF-fusion outperforms
the state-of-the-art approaches in terms of tracking robustness and
reconstruction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Object Detection from Images for Autonomous Driving: A Survey. (arXiv:2202.02980v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02980">
<div class="article-summary-box-inner">
<span><p>3D object detection from images, one of the fundamental and challenging
problems in autonomous driving, has received increasing attention from both
industry and academia in recent years. Benefiting from the rapid development of
deep learning technologies, image-based 3D detection has achieved remarkable
progress. Particularly, more than 200 works have studied this problem from 2015
to 2021, encompassing a broad spectrum of theories, algorithms, and
applications. However, to date no recent survey exists to collect and organize
this knowledge. In this paper, we fill this gap in the literature and provide
the first comprehensive survey of this novel and continuously growing research
field, summarizing the most commonly used pipelines for image-based 3D
detection and deeply analyzing each of their components. Additionally, we also
propose two new taxonomies to organize the state-of-the-art methods into
different categories, with the intent of providing a more systematic review of
existing methods and facilitating fair comparisons with future works. In
retrospect of what has been achieved so far, we also analyze the current
challenges in the field and discuss future directions for image-based 3D
detection research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning for Complex Data through Ensemble-based Self-Supervised Learning. (arXiv:2202.03126v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03126">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning deals with problems that have little or no available
labeled data. Recent work has shown impressive results when underlying classes
have significant semantic differences. One important dataset in which this
technique thrives is ImageNet, as intra-class distances are substantially lower
than inter-class distances. However, this is not the case for several critical
tasks, and general self-supervised learning methods fail to learn
discriminative features when classes have closer semantics, thus requiring more
robust strategies. We propose a strategy to tackle this problem, and to enable
learning from unlabeled data even when samples from different classes are not
prominently diverse. We approach the problem by leveraging a novel
ensemble-based clustering strategy where clusters derived from different
configurations are combined to generate a better grouping for the data samples
in a fully-unsupervised way. This strategy allows clusters with different
densities and higher variability to emerge, which in turn reduces intra-class
discrepancies, without requiring the burden of finding an optimal configuration
per dataset. We also consider different Convolutional Neural Networks to
compute distances between samples. We refine these distances by performing
context analysis and group them to capture complementary information. We
consider two applications to validate our pipeline: Person Re-Identification
and Text Authorship Verification. These are challenging applications
considering that classes are semantically close to each other and that training
and test sets have disjoint identities. Our method is robust across different
modalities and outperforms state-of-the-art results with a fully-unsupervised
solution without any labeling or human intervention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phase-Stretch Adaptive Gradient-Field Extractor (PAGE). (arXiv:2202.03570v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03570">
<div class="article-summary-box-inner">
<span><p>Phase-Stretch Adaptive Gradient-Field Extractor (PAGE) is an edge detection
algorithm that is inspired by physics of electromagnetic diffraction and
dispersion. A computational imaging algorithm, it identifies edges, their
orientations and sharpness in a digital image where the image brightness
changes abruptly. Edge detection is a basic operation performed by the eye and
is crucial to visual perception. PAGE embeds an original image into a set of
feature maps that can be used for object representation and classification. The
algorithm performs exceptionally well as an edge and texture extractor in low
light level and low contrast images. This manuscript is prepared to support the
open-source code which is being simultaneously made available within the GitHub
repository
https://github.com/JalaliLabUCLA/Phase-Stretch-Adaptive-Gradient-field-Extractor/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation. (arXiv:2202.04074v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04074">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL), which aims at leveraging a few labeled images
and a large number of unlabeled images for network training, is beneficial for
relieving the burden of data annotation in medical image segmentation.
According to the experience of medical imaging experts, local attributes such
as texture, luster and smoothness are very important factors for identifying
target objects like lesions and polyps in medical images. Motivated by this, we
propose a cross-level contrastive learning scheme to enhance representation
capacity for local features in semi-supervised medical image segmentation.
Compared to existing image-wise, patch-wise and point-wise contrastive learning
algorithms, our devised method is capable of exploring more complex similarity
cues, namely the relational characteristics between global and local patch-wise
representations. Additionally, for fully making use of cross-level semantic
relations, we devise a novel consistency constraint that compares the
predictions of patches against those of the full image. With the help of the
cross-level contrastive learning and consistency constraint, the unlabelled
data can be effectively explored to improve segmentation performance on two
medical image datasets for polyp and skin lesion segmentation respectively.
Code of our approach is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04901">
<div class="article-summary-box-inner">
<span><p>We present a frame interpolation algorithm that synthesizes multiple
intermediate frames from two input images with large in-between motion. Recent
methods use multiple networks to estimate optical flow or depth and a separate
network dedicated to frame synthesis. This is often complex and requires scarce
optical flow or depth ground-truth. In this work, we present a single unified
network, distinguished by a multi-scale feature extractor that shares weights
at all scales, and is trainable from frames alone. To synthesize crisp and
pleasing frames, we propose to optimize our network with the Gram matrix loss
that measures the correlation difference between feature maps. Our approach
outperforms state-of-the-art methods on the Xiph large motion benchmark. We
also achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing
to methods that use perceptual losses. We study the effect of weight sharing
and of training with datasets of increasing motion range. Finally, we
demonstrate our model's effectiveness in synthesizing high quality and
temporally coherent videos on a challenging near-duplicate photos dataset.
Codes and pre-trained models are available at
https://github.com/google-research/frame-interpolation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OWL (Observe, Watch, Listen): Localizing Actions in Egocentric Video via Audiovisual Temporal Context. (arXiv:2202.04947v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04947">
<div class="article-summary-box-inner">
<span><p>Temporal action localization (TAL) is an important task extensively explored
and improved for third-person videos in recent years. Recent efforts have been
made to perform fine-grained temporal localization on first-person videos.
However, current TAL methods only use visual signals, neglecting the audio
modality that exists in most videos and that shows meaningful action
information in egocentric videos. In this work, we take a deep look into the
effectiveness of audio in detecting actions in egocentric videos and introduce
a simple-yet-effective approach via Observing, Watching, and Listening (OWL) to
leverage audio-visual information and context for egocentric TAL. For doing
that, we: 1) compare and study different strategies for where and how to fuse
the two modalities; 2) propose a transformer-based model to incorporate
temporal audio-visual context. Our experiments show that our approach achieves
state-of-the-art performance on EPIC-KITCHENS-100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariance Regularization for Image Reconstruction. (arXiv:2202.05062v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05062">
<div class="article-summary-box-inner">
<span><p>In this work, we propose Regularization-by-Equivariance (REV), a novel
structure-adaptive regularization scheme for solving imaging inverse problems
under incomplete measurements. This regularization scheme utilizes the
equivariant structure in the physics of the measurements -- which is prevalent
in many inverse problems such as tomographic image reconstruction -- to
mitigate the ill-poseness of the inverse problem. Our proposed scheme can be
applied in a plug-and-play manner alongside with any classic first-order
optimization algorithm such as the accelerated gradient descent/FISTA for
simplicity and fast convergence. The numerical experiments in sparse-view X-ray
CT image reconstruction tasks demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Learning of Structured Memory via Closed-Loop Transcription. (arXiv:2202.05411v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05411">
<div class="article-summary-box-inner">
<span><p>This work proposes a minimal computational model for learning a structured
memory of multiple object classes in an incremental setting. Our approach is
based on establishing a closed-loop transcription between multiple classes and
their corresponding subspaces, known as a linear discriminative representation,
in a low-dimensional feature space. Our method is both simpler and more
efficient than existing approaches to incremental learning, in terms of model
size, storage, and computation: it requires only a single, fixed-capacity
autoencoding network with a feature space that is used for both discriminative
and generative purposes. All network parameters are optimized simultaneously
without architectural manipulations, by solving a constrained minimax game
between the encoding and decoding maps over a single rate reduction-based
objective. Experimental results show that our method can effectively alleviate
catastrophic forgetting, achieving significantly better performance than prior
work for both generative and discriminative purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer. (arXiv:2202.05508v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05508">
<div class="article-summary-box-inner">
<span><p>Text spotting end-to-end methods have recently gained attention in the
literature due to the benefits of jointly optimizing the text detection and
recognition components. Existing methods usually have a distinct separation
between the detection and recognition branches, requiring exact annotations for
the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach
for text spotting and the first text spotting framework which may be trained
with both fully- and weakly-supervised settings. By learning a single latent
representation per word detection, and using a novel loss function based on the
Hungarian loss, our method alleviates the need for expensive localization
annotations. Trained with only text transcription annotations on real data, our
weakly-supervised method achieves competitive performance with previous
state-of-the-art fully-supervised methods. When trained in a fully-supervised
manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-driven Neural Physically-based Facial Asset for Production. (arXiv:2202.05592v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05592">
<div class="article-summary-box-inner">
<span><p>Production-level workflows for producing convincing 3D dynamic human faces
have long relied on a disarray of labor-intensive tools for geometry and
texture generation, motion capture and rigging, and expression synthesis.
Recent neural approaches automate individual components but the corresponding
latent representations cannot provide artists explicit controls as in
conventional tools. In this paper, we present a new learning-based,
video-driven approach for generating dynamic facial geometries with
high-quality physically-based assets. Two key components are well-structured
latent spaces due to dense temporal samplings from videos and explicit facial
expression controls to regulate the latent spaces. For data collection, we
construct a hybrid multiview-photometric capture stage, coupling with an
ultra-fast video camera to obtain raw 3D facial assets. We then model the
facial expression, geometry and physically-based textures using separate VAEs
with a global MLP-based expression mapping across the latent spaces, to
preserve characteristics across respective attributes while maintaining
explicit controls over geometry and texture. We also introduce to model the
delta information as wrinkle maps for physically-base textures, achieving
high-quality rendering of dynamic textures. We demonstrate our approach in
high-fidelity performer-specific facial capture and cross-identity facial
motion retargeting. In addition, our neural asset along with fast adaptation
schemes can also be deployed to handle in-the-wild videos. Besides, we motivate
the utility of our explicit facial disentangle strategy by providing promising
physically-based editing results like geometry and material editing or winkle
transfer with high realism. Comprehensive experiments show that our technique
provides higher accuracy and visual fidelity than previous video-driven facial
reconstruction and animation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector Quantized Bayesian Neural Network Inference for Data Streams. (arXiv:1907.05911v3 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.05911">
<div class="article-summary-box-inner">
<span><p>Bayesian neural networks (BNN) can estimate the uncertainty in predictions,
as opposed to non-Bayesian neural networks (NNs). However, BNNs have been far
less widely used than non-Bayesian NNs in practice since they need iterative NN
executions to predict a result for one data, and it gives rise to prohibitive
computational cost. This computational burden is a critical problem when
processing data streams with low-latency. To address this problem, we propose a
novel model VQ-BNN, which approximates BNN inference for data streams. In order
to reduce the computational burden, VQ-BNN inference predicts NN only once and
compensates the result with previously memorized predictions. To be specific,
VQ-BNN inference for data streams is given by temporal exponential smoothing of
recent predictions. The computational cost of this model is almost the same as
that of non-Bayesian NNs. Experiments including semantic segmentation on
real-world data show that this model performs significantly faster than BNNs
while estimating predictive results comparable to or superior to the results of
BNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coded ResNeXt: a network for designing disentangled information paths. (arXiv:2202.05343v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05343">
<div class="article-summary-box-inner">
<span><p>To avoid treating neural networks as highly complex black boxes, the deep
learning research community has tried to build interpretable models allowing
humans to understand the decisions taken by the model. Unfortunately, the focus
is mostly on manipulating only the very high-level features associated with the
last layers. In this work, we look at neural network architectures for
classification in a more general way and introduce an algorithm which defines
before the training the paths of the network through which the per-class
information flows. We show that using our algorithm we can extract a lighter
single-purpose binary classifier for a particular class by removing the
parameters that do not participate in the predefined information path of that
class, which is approximately 60% of the total parameters. Notably, leveraging
coding theory to design the information paths enables us to use intermediate
network layers for making early predictions without having to evaluate the full
network. We demonstrate that a slightly modified ResNeXt model, trained with
our algorithm, can achieve higher classification accuracy on CIFAR-10/100 and
ImageNet than the original ResNeXt, while having all the aforementioned
properties.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-15 23:07:23.795210003 UTC">2022-02-15 23:07:23 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>