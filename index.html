<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-28T01:30:00Z">04-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Bootstrapping Approaches to Improve Low Resource Abusive Language Detection for Indic Languages. (arXiv:2204.12543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12543">
<div class="article-summary-box-inner">
<span><p>Abusive language is a growing concern in many social media platforms.
Repeated exposure to abusive speech has created physiological effects on the
target users. Thus, the problem of abusive language should be addressed in all
forms for online peace and safety. While extensive research exists in abusive
speech detection, most studies focus on English. Recently, many smearing
incidents have occurred in India, which provoked diverse forms of abusive
speech in online space in various languages based on the geographic location.
Therefore it is essential to deal with such malicious content. In this paper,
to bridge the gap, we demonstrate a large-scale analysis of multilingual
abusive speech in Indic languages. We examine different interlingual transfer
mechanisms and observe the performance of various multilingual models for
abusive speech detection for eight different Indic languages. We also
experiment to show how robust these models are on adversarial attacks. Finally,
we conduct an in-depth error analysis by looking into the models' misclassified
posts across various settings. We have made our code and models public for
other researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parkinson's disease diagnostics using AI and natural language knowledge transfer. (arXiv:2204.12559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12559">
<div class="article-summary-box-inner">
<span><p>In this work, the issue of Parkinson's disease (PD) diagnostics using
non-invasive antemortem techniques was tackled. A deep learning approach for
classification of raw speech recordings in patients with diagnosed PD was
proposed. The core of proposed method is an audio classifier using knowledge
transfer from a pretrained natural language model, namely \textit{wav2vec 2.0}.
Method was tested on a group of 38 PD patients and 10 healthy persons above the
age of 50. A dataset of speech recordings acquired using a smartphone recorder
was constructed and the recordings were label as PD/non-PD with severity of the
disease additionally rated using Hoehn-Yahr scale. The audio recordings were
cut into 2141 samples that include sentences, syllables, vowels and sustained
phonation. The classifier scores up to 97.92\% of cross-validated accuracy.
Additionally, paper presents results of a human-level performance assessment
questionnaire, which was consulted with the neurology professionals
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for Tamil TrollMeme Classification. (arXiv:2204.12587v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12587">
<div class="article-summary-box-inner">
<span><p>Social media platforms often act as breeding grounds for various forms of
trolling or malicious content targeting users or communities. One way of
trolling users is by creating memes, which in most cases unites an image with a
short piece of text embedded on top of it. The situation is more complex for
multilingual(e.g., Tamil) memes due to the lack of benchmark datasets and
models. We explore several models to detect Troll memes in Tamil based on the
shared task, "Troll Meme Classification in DravidianLangTech2022" at ACL-2022.
We observe while the text-based model MURIL performs better for Non-troll meme
classification, the image-based model VGG16 performs better for Troll-meme
classification. Further fusing these two modalities help us achieve stable
outcomes in both classes. Our fusion model achieved a 0.561 weighted average F1
score and ranked second in this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Machine Translation Domain Adaptation. (arXiv:2204.12608v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12608">
<div class="article-summary-box-inner">
<span><p>Machine translation models struggle when translating out-of-domain text,
which makes domain adaptation a topic of critical importance. However, most
domain adaptation methods focus on fine-tuning or training the entire or part
of the model on every new domain, which can be costly. On the other hand,
semi-parametric models have been shown to successfully perform domain
adaptation by retrieving examples from an in-domain datastore (Khandelwal et
al., 2021). A drawback of these retrieval-augmented models, however, is that
they tend to be substantially slower. In this paper, we explore several
approaches to speed up nearest neighbor machine translation. We adapt the
methods recently proposed by He et al. (2021) for language modeling, and
introduce a simple but effective caching strategy that avoids performing
retrieval when similar contexts have been seen before. Translation quality and
runtimes for several domains show the effectiveness of the proposed solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Testing the Ability of Language Models to Interpret Figurative Language. (arXiv:2204.12632v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12632">
<div class="article-summary-box-inner">
<span><p>Figurative and metaphorical language are commonplace in discourse, and
figurative expressions play an important role in communication and cognition.
However, figurative language has been a relatively under-studied area in NLP,
and it remains an open question to what extent modern language models can
interpret nonliteral phrases. To address this question, we introduce Fig-QA, a
Winograd-style nonliteral language understanding task consisting of correctly
interpreting paired figurative phrases with divergent meanings. We evaluate the
performance of several state-of-the-art language models on this task, and find
that although language models achieve performance significantly over chance,
they still fall short of human performance, particularly in zero- or few-shot
settings. This suggests that further work is needed to improve the nonliteral
reasoning capabilities of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing Universal Dependency Treebanks for Magahi and Braj. (arXiv:2204.12633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12633">
<div class="article-summary-box-inner">
<span><p>In this paper, we discuss the development of treebanks for two low-resourced
Indian languages - Magahi and Braj based on the Universal Dependencies
framework. The Magahi treebank contains 945 sentences and Braj treebank around
500 sentences marked with their lemmas, part-of-speech, morphological features
and universal dependencies. This paper gives a description of the different
dependency relationship found in the two languages and give some statistics of
the two treebanks. The dataset will be made publicly available on Universal
Dependency (UD) repository
(https://github.com/UniversalDependencies/UD_Magahi-MGTB/tree/master) in the
next(v2.10) release.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Executive Function: A Contrastive Value Policy for Resampling and Relabeling Perceptions via Hindsight Summarization?. (arXiv:2204.12639v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12639">
<div class="article-summary-box-inner">
<span><p>We develop the few-shot continual learning task from first principles and
hypothesize an evolutionary motivation and mechanism of action for executive
function as a contrastive value policy which resamples and relabels perception
data via hindsight summarization to minimize attended prediction error, similar
to an online prompt engineering problem. This is made feasible by the use of a
memory policy and a pretrained network with inductive biases for a grammar of
learning and is trained to maximize evolutionary survival. We show how this
model of executive function can be used to implement hypothesis testing as a
stream of consciousness and may explain observations of human few-shot learning
and neuroanatomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Query Graph Selection for Knowledge Base Question Answering. (arXiv:2204.12662v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12662">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel approach based on semantic parsing to improve the
performance of Knowledge Base Question Answering (KBQA). Specifically, we focus
on how to select an optimal query graph from a candidate set so as to retrieve
the answer from knowledge base (KB). In our approach, we first propose to
linearize the query graph into a sequence, which is used to form a sequence
pair with the question. It allows us to use mature sequence modeling, such as
BERT, to encode the sequence pair. Then we use a ranking method to sort
candidate query graphs. In contrast to the previous studies, our approach can
efficiently model semantic interactions between the graph and the question as
well as rank the candidate graphs from a global view. The experimental results
show that our system achieves the top performance on ComplexQuestions and the
second best performance on WebQuestions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptable Text Matching via Meta-Weight Regulator. (arXiv:2204.12668v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12668">
<div class="article-summary-box-inner">
<span><p>Neural text matching models have been used in a range of applications such as
question answering and natural language inference, and have yielded a good
performance. However, these neural models are of a limited adaptability,
resulting in a decline in performance when encountering test examples from a
different dataset or even a different task. The adaptability is particularly
important in the few-shot setting: in many cases, there is only a limited
amount of labeled data available for a target dataset or task, while we may
have access to a richly labeled source dataset or task. However, adapting a
model trained on the abundant source data to a few-shot target dataset or task
is challenging. To tackle this challenge, we propose a Meta-Weight Regulator
(MWR), which is a meta-learning approach that learns to assign weights to the
source examples based on their relevance to the target loss. Specifically, MWR
first trains the model on the uniformly weighted source examples, and measures
the efficacy of the model on the target examples via a loss function. By
iteratively performing a (meta) gradient descent, high-order gradients are
propagated to the source examples. These gradients are then used to update the
weights of source examples, in a way that is relevant to the target
performance. As MWR is model-agnostic, it can be applied to any backbone neural
model. Extensive experiments are conducted with various backbone text matching
models, on four widely used datasets and two tasks. The results demonstrate
that our proposed approach significantly outperforms a number of existing
adaptation methods and effectively improves the cross-dataset and cross-task
adaptability of the neural text matching models in the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Driven Adaptive Simultaneous Machine Translation. (arXiv:2204.12672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12672">
<div class="article-summary-box-inner">
<span><p>In simultaneous translation (SimulMT), the most widely used strategy is the
wait-k policy thanks to its simplicity and effectiveness in balancing
translation quality and latency. However, wait-k suffers from two major
limitations: (a) it is a fixed policy that can not adaptively adjust latency
given context, and (b) its training is much slower than full-sentence
translation. To alleviate these issues, we propose a novel and efficient
training scheme for adaptive SimulMT by augmenting the training corpus with
adaptive prefix-to-prefix pairs, while the training complexity remains the same
as that of training full-sentence translation models. Experiments on two
language pairs show that our method outperforms all strong baselines in terms
of translation quality and latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Span-level Bidirectional Cross-attention Framework for Aspect Sentiment Triplet Extraction. (arXiv:2204.12674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12674">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment
analysis task that aims to extract triplets of aspect terms, sentiments, and
opinion terms from review sentences. Recently, span-level models achieve
gratifying results on ASTE task by taking advantage of whole span predictions.
However, all the spans generated by these methods inevitably share at least one
token with some others, and these method suffer from the similarity of these
spans due to their similar distributions. Moreover, since either the aspect
term or opinion term can trigger a sentiment triplet, it is challenging to make
use of the information more comprehensively and adequately. To address these
concerns, we propose a span-level bidirectional cross-attention framework.
Specifically, we design a similar span separation loss to detach the spans with
shared tokens and a bidirectional cross-attention structure that consists of
aspect and opinion decoders to decode the span-level representations in both
aspect-to-opinion and opinion-to-aspect directions. With differentiated span
representations and bidirectional decoding structure, our model can extract
sentiment triplets more precisely and efficiently. Experimental results show
that our framework significantly outperforms state-of-the-art methods,
achieving better performance in predicting triplets with multi-token entities
and extracting triplets in sentences with multi-triplets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-Level Relation Extraction with Sentences Importance Estimation and Focusing. (arXiv:2204.12679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12679">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (DocRE) aims to determine the relation
between two entities from a document of multiple sentences. Recent studies
typically represent the entire document by sequence- or graph-based models to
predict the relations of all entity pairs. However, we find that such a model
is not robust and exhibits bizarre behaviors: it predicts correctly when an
entire test document is fed as input, but errs when non-evidence sentences are
removed. To this end, we propose a Sentence Importance Estimation and Focusing
(SIEF) framework for DocRE, where we design a sentence importance score and a
sentence focusing loss, encouraging DocRE models to focus on evidence
sentences. Experimental results on two domains show that our SIEF not only
improves overall performance, but also makes DocRE models more robust.
Moreover, SIEF is a general framework, shown to be effective when combined with
a variety of base DocRE models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$G^2$: Enhance Knowledge Grounded Dialogue via Ground Graph. (arXiv:2204.12681v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12681">
<div class="article-summary-box-inner">
<span><p>Knowledge grounded dialogue system is designed to generate responses that
convey information from given knowledge documents. However, it's a challenge
for the current Seq2Seq model to acquire knowledge from complex documents and
integrate it to perform correct responses without the aid of an explicit
semantic structure. To address these issues, we present a novel graph
structure, Ground Graph ($G^2$), which models the semantic structure of both
dialogue contexts and knowledge documents to facilitate knowledge selection and
integration for the task. Besides, a Ground Graph Aware Transformer ($G^2AT$)
is proposed to enhance knowledge grounded response generation. Empirical
results show that our proposed model outperforms previous state-of-the-art
methods with more than 10\% and 20\% gains on response generation and factual
consistency. Furthermore, our structure-aware approach shows excellent
generalization ability in resource-limited situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distant finetuning with discourse relations for stance classification. (arXiv:2204.12693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12693">
<div class="article-summary-box-inner">
<span><p>Approaches for the stance classification task, an important task for
understanding argumentation in debates and detecting fake news, have been
relying on models which deal with individual debate topics. In this paper, in
order to train a system independent from topics, we propose a new method to
extract data with silver labels from raw text to finetune a model for stance
classification. The extraction relies on specific discourse relation
information, which is shown as a reliable and accurate source for providing
stance information. We also propose a 3-stage training framework where the
noisy level in the data used for finetuning decreases over different stages
going from the most noisy to the least noisy. Detailed experiments show that
the automatically annotated dataset as well as the 3-stage training help
improve model performance in stance classification. Our approach ranks 1st
among 26 competing teams in the stance classification track of the NLPCC 2021
shared task Argumentative Text Understanding for AI Debater, which confirms the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations. (arXiv:2204.12708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12708">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that deep learning models in NLP are highly sensitive
to low-level correlations between simple features and specific output labels,
leading to overfitting and lack of generalization. To mitigate this problem, a
common practice is to balance datasets by adding new instances or by filtering
out "easy" instances (Sakaguchi et al., 2020), culminating in a recent proposal
to eliminate single-word correlations altogether (Gardner et al., 2021). In
this opinion paper, we identify that despite these efforts,
increasingly-powerful models keep exploiting ever-smaller spurious
correlations, and as a result even balancing all single-word features is
insufficient for mitigating all of these correlations. In parallel, a truly
balanced dataset may be bound to "throw the baby out with the bathwater" and
miss important signal encoding common sense and world knowledge. We highlight
several alternatives to dataset balancing, focusing on enhancing datasets with
richer contexts, allowing models to abstain and interact with users, and
turning from large-scale fine-tuning to zero- or few-shot setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREER: A Large-Scale Corpus for Relation Extraction and Entity Recognition. (arXiv:2204.12710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12710">
<div class="article-summary-box-inner">
<span><p>We describe the design and use of the CREER dataset, a large corpus annotated
with rich English grammar and semantic attributes. The CREER dataset uses the
Stanford CoreNLP Annotator to capture rich language structures from Wikipedia
plain text. This dataset follows widely used linguistic and semantic
annotations so that it can be used for not only most natural language
processing tasks but also scaling the dataset. This large supervised dataset
can serve as the basis for improving the performance of NLP tasks in the
future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UBERT: A Novel Language Model for Synonymy Prediction at Scale in the UMLS Metathesaurus. (arXiv:2204.12716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12716">
<div class="article-summary-box-inner">
<span><p>The UMLS Metathesaurus integrates more than 200 biomedical source
vocabularies. During the Metathesaurus construction process, synonymous terms
are clustered into concepts by human editors, assisted by lexical similarity
algorithms. This process is error-prone and time-consuming. Recently, a deep
learning model (LexLM) has been developed for the UMLS Vocabulary Alignment
(UVA) task. This work introduces UBERT, a BERT-based language model, pretrained
on UMLS terms via a supervised Synonymy Prediction (SP) task replacing the
original Next Sentence Prediction (NSP) task. The effectiveness of UBERT for
UMLS Metathesaurus construction process is evaluated using the UMLS Vocabulary
Alignment (UVA) task. We show that UBERT outperforms the LexLM, as well as
biomedical BERT-based models. Key to the performance of UBERT are the synonymy
prediction task specifically developed for UBERT, the tight alignment of
training data to the UVA task, and the similarity of the models used for
pretrained UBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition. (arXiv:2204.12732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12732">
<div class="article-summary-box-inner">
<span><p>Nested named entity recognition (nested NER) is a fundamental task in natural
language processing. Various span-based methods have been proposed to detect
nested entities with span representations. However, span-based methods do not
consider the relationship between a span and other entities or phrases, which
is helpful in the NER task. Besides, span-based methods have trouble predicting
long entities due to limited span enumeration length. To mitigate these issues,
we present the Propose-and-Refine Network (PnRNet), a two-stage set prediction
network for nested NER. In the propose stage, we use a span-based predictor to
generate some coarse entity predictions as entity proposals. In the refine
stage, proposals interact with each other, and richer contextual information is
incorporated into the proposal representations. The refined proposal
representations are used to re-predict entity boundaries and classes. In this
way, errors in coarse proposals can be eliminated, and the boundary prediction
is no longer constrained by the span enumeration length limitation.
Additionally, we build multi-scale sentence representations, which better model
the hierarchical structure of sentences and provide richer contextual
information than token-level representations. Experiments show that PnRNet
achieves state-of-the-art performance on four nested NER datasets and one flat
NER dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Control Globally, Understand Locally: A Global-to-Local Hierarchical Graph Network for Emotional Support Conversation. (arXiv:2204.12749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12749">
<div class="article-summary-box-inner">
<span><p>Emotional support conversation aims at reducing the emotional distress of the
help-seeker, which is a new and challenging task. It requires the system to
explore the cause of help-seeker's emotional distress and understand their
psychological intention to provide supportive responses. However, existing
methods mainly focus on the sequential contextual information, ignoring the
hierarchical relationships with the global cause and local psychological
intention behind conversations, thus leads to a weak ability of emotional
support. In this paper, we propose a Global-to-Local Hierarchical Graph Network
to capture the multi-source information (global cause, local intentions and
dialog history) and model hierarchical relationships between them, which
consists of a multi-source encoder, a hierarchical graph reasoner, and a
global-guide decoder. Furthermore, a novel training objective is designed to
monitor semantic information of the global cause. Experimental results on the
emotional support conversation dataset, ESConv, confirm that the proposed GLHG
has achieved the state-of-the-art performance on the automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Understanding of Code-mixed Language Semantics using Hierarchical Transformer. (arXiv:2204.12753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12753">
<div class="article-summary-box-inner">
<span><p>Being a popular mode of text-based communication in multilingual communities,
code-mixing in online social media has became an important subject to study.
Learning the semantics and morphology of code-mixed language remains a key
challenge, due to scarcity of data and unavailability of robust and
language-invariant representation learning technique. Any morphologically-rich
language can benefit from character, subword, and word-level embeddings, aiding
in learning meaningful correlations. In this paper, we explore a hierarchical
transformer-based architecture (HIT) to learn the semantics of code-mixed
languages. HIT consists of multi-headed self-attention and outer product
attention components to simultaneously comprehend the semantic and syntactic
structures of code-mixed texts. We evaluate the proposed method across 6 Indian
languages (Bengali, Gujarati, Hindi, Tamil, Telugu and Malayalam) and Spanish
for 9 NLP tasks on 17 datasets. The HIT model outperforms state-of-the-art
code-mixed representation learning and multilingual language models in all
tasks. We further demonstrate the generalizability of the HIT architecture
using masked language modeling-based pre-training, zero-shot learning, and
transfer learning approaches. Our empirical results show that the pre-training
objectives significantly improve the performance on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Thorough Examination on Zero-shot Dense Retrieval. (arXiv:2204.12755v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12755">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the significant advance in dense retrieval (DR)
based on powerful pre-trained language models (PLM). DR models have achieved
excellent performance in several benchmark datasets, while they are shown to be
not as competitive as traditional sparse retrieval models (e.g., BM25) in a
zero-shot retrieval setting. However, in the related literature, there still
lacks a detailed and comprehensive study on zero-shot retrieval. In this paper,
we present the first thorough examination of the zero-shot capability of DR
models. We aim to identify the key factors and analyze how they affect
zero-shot retrieval performance. In particular, we discuss the effect of
several key factors related to source training set, analyze the potential bias
from the target dataset, and review and compare existing zero-shot DR models.
Our findings provide important evidence to better understand and develop
zero-shot DR models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?. (arXiv:2204.12765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12765">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised learning (SSL) has demonstrated strong performance
in speaker recognition, even if the pre-training objective is designed for
speech recognition. In this paper, we study which factor leads to the success
of self-supervised learning on speaker-related tasks, e.g. speaker verification
(SV), through a series of carefully designed experiments. Our empirical results
on the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a
combination of mask speech prediction loss, data scale, and model size, while
the SSL quantizer has a minor impact. We further employ the integrated
gradients attribution method and loss landscape visualization to understand the
effectiveness of self-supervised learning for speaker recognition performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultra Fast Speech Separation Model with Teacher Student Learning. (arXiv:2204.12777v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12777">
<div class="article-summary-box-inner">
<span><p>Transformer has been successfully applied to speech separation recently with
its strong long-dependency modeling capacity using a self-attention mechanism.
However, Transformer tends to have heavy run-time costs due to the deep encoder
layers, which hinders its deployment on edge devices. A small Transformer model
with fewer encoder layers is preferred for computational efficiency, but it is
prone to performance degradation. In this paper, an ultra fast speech
separation Transformer model is proposed to achieve both better performance and
efficiency with teacher student learning (T-S learning). We introduce
layer-wise T-S learning and objective shifting mechanisms to guide the small
student model to learn intermediate representations from the large teacher
model. Compared with the small Transformer model trained from scratch, the
proposed T-S learning method reduces the word error rate (WER) by more than 5%
for both multi-channel and single-channel speech separation on LibriCSS
dataset. Utilizing more unlabeled speech data, our ultra fast speech separation
models achieve more than 10% relative WER reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks. (arXiv:2204.12784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12784">
<div class="article-summary-box-inner">
<span><p>Aspect-level sentiment analysis aims to determine the sentiment polarity
towards a specific target in a sentence. The main challenge of this task is to
effectively model the relation between targets and sentiments so as to filter
out noisy opinion words from irrelevant targets. Most recent efforts capture
relations through target-sentiment pairs or opinion spans from a word-level or
phrase-level perspective. Based on the observation that targets and sentiments
essentially establish relations following the grammatical hierarchy of
phrase-clause-sentence structure, it is hopeful to exploit comprehensive
syntactic information for better guiding the learning process. Therefore, we
introduce the concept of Scope, which outlines a structural text region related
to a specific target. To jointly learn structural Scope and predict the
sentiment polarity, we propose a hybrid graph convolutional network (HGCN) to
synthesize information from constituency tree and dependency tree, exploring
the potential of linking two syntax parsing methods to enrich the
representation. Experimental results on four public datasets illustrate that
our HGCN model outperforms current state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug-and-Play Adaptation for Continuously-updated QA. (arXiv:2204.12785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12785">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have shown great potential as implicit knowledge bases
(KBs). And for their practical use, knowledge in LMs need to be updated
periodically. However, existing tasks to assess LMs' efficacy as KBs do not
adequately consider multiple large-scale updates. To this end, we first propose
a novel task--Continuously-updated QA (CuQA)--in which multiple large-scale
updates are made to LMs, and the performance is measured with respect to the
success in adding and updating knowledge while retaining existing knowledge. We
then present LMs with plug-in modules that effectively handle the updates.
Experiments conducted on zsRE QA and NQ datasets show that our method
outperforms existing approaches. We find that our method is 4x more effective
in terms of updates/forgets ratio, compared to a fine-tuning baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12793">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on the task of generating SPARQL queries from natural
language questions, which can then be executed on Knowledge Graphs (KGs). We
assume that gold entity and relations have been provided, and the remaining
task is to arrange them in the right order along with SPARQL vocabulary, and
input tokens to produce the correct SPARQL query. Pre-trained Language Models
(PLMs) have not been explored in depth on this task so far, so we experiment
with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings,
looking for new baselines in the PLM era for this task, on DBpedia and Wikidata
KGs. We show that T5 requires special input tokenisation, but produces state of
the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms
task-specific models from previous works. Moreover, the methods enable semantic
parsing for questions where a part of the input needs to be copied to the
output query, thus enabling a new paradigm in KG semantic parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Simile Knowledge from Pre-trained Language Models. (arXiv:2204.12807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12807">
<div class="article-summary-box-inner">
<span><p>Simile interpretation (SI) and simile generation (SG) are challenging tasks
for NLP because models require adequate world knowledge to produce predictions.
Previous works have employed many hand-crafted resources to bring
knowledge-related into models, which is time-consuming and labor-intensive. In
recent years, pre-trained language models (PLMs) based approaches have become
the de-facto standard in NLP since they learn generic knowledge from a large
corpus. The knowledge embedded in PLMs may be useful for SI and SG tasks.
Nevertheless, there are few works to explore it. In this paper, we probe simile
knowledge from PLMs to solve the SI and SG tasks in the unified framework of
simile triple completion for the first time. The backbone of our framework is
to construct masked sentences with manual patterns and then predict the
candidate words in the masked position. In this framework, we adopt a secondary
training process (Adjective-Noun mask Training) with the masked language model
(MLM) loss to enhance the prediction diversity of candidate words in the masked
position. Moreover, pattern ensemble (PE) and pattern search (PS) are applied
to improve the quality of predicted words. Finally, automatic and human
evaluations demonstrate the effectiveness of our framework in both SI and SG
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Method of Query Graph Reranking for Knowledge Base Question Answering. (arXiv:2204.12808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12808">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel reranking method to better choose the optimal
query graph, a sub-graph of knowledge graph, to retrieve the answer for an
input question in Knowledge Base Question Answering (KBQA). Existing methods
suffer from a severe problem that there is a significant gap between top-1
performance and the oracle score of top-n results. To address this problem, our
method divides the choosing procedure into two steps: query graph ranking and
query graph reranking. In the first step, we provide top-n query graphs for
each question. Then we propose to rerank the top-n query graphs by combining
with the information of answer type. Experimental results on two widely used
datasets show that our proposed method achieves the best results on the
WebQuestions dataset and the second best on the ComplexQuestions dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkillSpan: Hard and Soft Skill Extraction from English Job Postings. (arXiv:2204.12811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12811">
<div class="article-summary-box-inner">
<span><p>Skill Extraction (SE) is an important and widely-studied task useful to gain
insights into labor market dynamics. However, there is a lacuna of datasets and
annotation guidelines; available datasets are few and contain crowd-sourced
labels on the span-level or labels from a predefined skill inventory. To
address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of
14.5K sentences and over 12.5K annotated spans. We release its respective
guidelines created over three different sources annotated for hard and soft
skills by domain experts. We introduce a BERT baseline (Devlin et al., 2019).
To improve upon this baseline, we experiment with language models that are
optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous
pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et
al., 2020), and multi-task learning (Caruana, 1997). Our results show that the
domain-adapted models significantly outperform their non-adapted counterparts,
and single-task outperforms multi-task learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LyS_ACoru\~na at SemEval-2022 Task 10: Repurposing Off-the-Shelf Tools for Sentiment Analysis as Semantic Dependency Parsing. (arXiv:2204.12820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12820">
<div class="article-summary-box-inner">
<span><p>This paper addressed the problem of structured sentiment analysis using a
bi-affine semantic dependency parser, large pre-trained language models, and
publicly available translation models. For the monolingual setup, we
considered: (i) training on a single treebank, and (ii) relaxing the setup by
training on treebanks coming from different languages that can be adequately
processed by cross-lingual language models. For the zero-shot setup and a given
target treebank, we relied on: (i) a word-level translation of available
treebanks in other languages to get noisy, unlikely-grammatical, but annotated
data (we release as much of it as licenses allow), and (ii) merging those
translated treebanks to obtain training data. In the post-evaluation phase, we
also trained cross-lingual models that simply merged all the English treebanks
and did not use word-level translations, and yet obtained better results.
According to the official results, we ranked 8th and 9th in the monolingual and
cross-lingual setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Parallelize in a Shared-Memory Environment with Transformers. (arXiv:2204.12835v1 [cs.DC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12835">
<div class="article-summary-box-inner">
<span><p>In past years, the world has switched to many-core and multi-core shared
memory architectures.
</p>
<p>As a result, there is a growing need to utilize these architectures by
introducing shared memory parallelization schemes to software applications.
OpenMP is the most comprehensive API that implements such schemes,
characterized by a readable interface. Nevertheless, introducing OpenMP into
code is challenging due to pervasive pitfalls in management of parallel shared
memory. To facilitate the performance of this task, many source-to-source (S2S)
compilers have been created over the years, tasked with inserting OpenMP
directives into code automatically.
</p>
<p>In addition to having limited robustness to their input format, these
compilers still do not achieve satisfactory coverage and precision in locating
parallelizable code and generating appropriate directives.
</p>
<p>In this work, we propose leveraging recent advances in ML techniques,
specifically in natural language processing (NLP), to replace S2S compilers
altogether.
</p>
<p>We create a database (corpus), Open-OMP, specifically for this goal. Open-OMP
contains over 28,000 code snippets, half of which contain OpenMP directives
while the other half do not need parallelization at all with high probability.
</p>
<p>We use the corpus to train systems to automatically classify code segments in
need of parallelization, as well as suggest individual OpenMP clauses.
</p>
<p>We train several transformer models, named PragFormer, for these tasks, and
show that they outperform statistically-trained baselines and automatic S2S
parallelization compilers in both classifying the overall need for an OpenMP
directive and the introduction of private and reduction clauses.
</p>
<p>Our source code and database are available at:
https://github.com/Scientific-Computing-Lab-NRCN/PragFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query2Particles: Knowledge Graph Reasoning with Particle Embeddings. (arXiv:2204.12847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12847">
<div class="article-summary-box-inner">
<span><p>Answering complex logical queries on incomplete knowledge graphs (KGs) with
missing edges is a fundamental and important task for knowledge graph
reasoning. The query embedding method is proposed to answer these queries by
jointly encoding queries and entities to the same embedding space. Then the
answer entities are selected according to the similarities between the entity
embeddings and the query embedding. As the answers to a complex query are
obtained from a combination of logical operations over sub-queries, the
embeddings of the answer entities may not always follow a uni-modal
distribution in the embedding space. Thus, it is challenging to simultaneously
retrieve a set of diverse answers from the embedding space using a single and
concentrated query representation such as a vector or a hyper-rectangle. To
better cope with queries with diversified answers, we propose Query2Particles
(Q2P), a complex KG query answering method. Q2P encodes each query into
multiple vectors, named particle embeddings. By doing so, the candidate answers
can be retrieved from different areas over the embedding space using the
maximal similarities between the entity embeddings and any of the particle
embeddings. Meanwhile, the corresponding neural logic operations are defined to
support its reasoning over arbitrary first-order logic queries. The experiments
show that Query2Particles achieves state-of-the-art performance on the complex
query answering tasks on FB15k, FB15K-237, and NELL knowledge graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaCoach: A Virtual Coach for Training Customer Service Agents. (arXiv:2204.12935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12935">
<div class="article-summary-box-inner">
<span><p>With the development of online business, customer service agents gradually
play a crucial role as an interface between the companies and their customers.
Most companies spend a lot of time and effort on hiring and training customer
service agents. To this end, we propose AdaCoach: A Virtual Coach for Training
Customer Service Agents, to promote the ability of newly hired service agents
before they get to work. AdaCoach is designed to simulate real customers who
seek help and actively initiate the dialogue with the customer service agents.
Besides, AdaCoach uses an automated dialogue evaluation model to score the
performance of the customer agent in the training process, which can provide
necessary assistance when the newly hired customer service agent encounters
problems. We apply recent NLP technologies to ensure efficient run-time
performance in the deployed system. To the best of our knowledge, this is the
first system that trains the customer service agent through human-computer
interaction. Until now, the system has already supported more than 500,000
simulation training and cultivated over 1000 qualified customer service agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Dialogue Summarization System for Sales Calls. (arXiv:2204.12951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12951">
<div class="article-summary-box-inner">
<span><p>Summarizing sales calls is a routine task performed manually by salespeople.
We present a production system which combines generative models fine-tuned for
customer-agent setting, with a human-in-the-loop user experience for an
interactive summary curation process. We address challenging aspects of
dialogue summarization task in a real-world setting including long input
dialogues, content validation, lack of labeled data and quality evaluation. We
show how GPT-3 can be leveraged as an offline data labeler to handle training
data scarcity and accommodate privacy constraints in an industrial setting.
Experiments show significant improvements by our models in tackling the
summarization and content validation tasks on public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extremal GloVe: Theoretically Accurate Distributed Word Embedding by Tail Inference. (arXiv:2204.13009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13009">
<div class="article-summary-box-inner">
<span><p>Distributed word embeddings such as Word2Vec and GloVe have been widely
adopted in industrial context settings. Major technical applications of GloVe
include recommender systems and natural language processing. The fundamental
theory behind GloVe relies on the selection of a weighting function in the
weighted least squres formulation that computes the powered ratio of word
occurrence count and the maximum word count in the corpus. However, the initial
formulation of GloVe is not theoretically sound in two aspects, namely the
selection of the weighting function and its power exponent is ad-hoc. In this
paper, we utilize the theory of extreme value analysis and propose a
theoretically accurate version of GloVe. By reformulating the weighted least
squares loss function as the expected loss function and accurately choosing the
power exponent, we create a theoretically accurate version of GloVe. We
demonstrate the competitiveness of our algorithm and show that the initial
formulation of GloVe with the suggested optimal parameter can be viewed as a
special case of our paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2204.13021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13021">
<div class="article-summary-box-inner">
<span><p>We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. \textbf{1)} NLU++ provides fine-grained
domain ontologies with a large set of challenging \textit{multi-intent}
sentences, introducing and validating the idea of \textit{intent modules} that
can be combined into complex intents that convey complex user goals, combined
with finer-grained and thus more challenging slot sets. \textbf{2)} The
ontology is divided into \textit{domain-specific} and \textit{generic} (i.e.,
domain-universal) intent modules that overlap across domains, promoting
cross-domain reusability of annotated examples. \textbf{3)} The dataset design
has been inspired by the problems observed in industrial ToD systems, and
\textbf{4)} it has been collected, filtered and carefully annotated by dialogue
NLU experts, yielding high-quality annotated data. Finally, we benchmark a
series of current state-of-the-art NLU models on NLU++; the results demonstrate
the challenging nature of the dataset, especially in low-data regimes, the
validity of `intent modularisation', and call for further research on ToD NLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation. (arXiv:2204.13031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13031">
<div class="article-summary-box-inner">
<span><p>Dialog response generation in open domain is an important research topic
where the main challenge is to generate relevant and diverse responses. In this
paper, we propose a new dialog pre-training framework called DialogVED, which
introduces continuous latent variables into the enhanced encoder-decoder
pre-training framework to increase the relevance and diversity of responses.
With the help of a large dialog corpus (Reddit), we pre-train the model using
the following 4 tasks, used in training language models (LMs) and Variational
Autoencoders (VAEs) literature: 1) masked language model; 2) response
generation; 3) bag-of-words prediction; and 4) KL divergence reduction. We also
add additional parameters to model the turn structure in dialogs to improve the
performance of the pre-trained model. We conduct experiments on PersonaChat,
DailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental
results show that our model achieves the new state-of-the-art results on all
these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TimeBERT: Enhancing Pre-Trained Language Representations with Temporal Information. (arXiv:2204.13032v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13032">
<div class="article-summary-box-inner">
<span><p>Time is an important aspect of text documents, which has been widely
exploited in natural language processing and has strong influence, for example,
in temporal information retrieval, where the temporal information of queries or
documents need to be identified for relevance estimation. Event-related tasks
like event ordering, which aims to order events by their occurrence time, also
need to determine the temporal information of events. In this work, we
investigate methods for incorporating temporal information during pre-training,
to further improve the performance on time-related tasks. Compared with BERT
which utilizes synchronic document collections (BooksCorpus and English
Wikipedia) as the training corpora, we use long-span temporal news collection
for building word representations, since temporal information constitutes one
of the most significant features of news articles. We then introduce TimeBERT,
a novel language representation model trained on a temporal collection of news
articles via two new pre-training tasks, which harness two distinct temporal
signals to construct time-aware language representation. The experimental
results show that TimeBERT consistently outperforms BERT and other existing
pre-trained models, with substantial gains on different downstream NLP tasks or
applications for which time is of importance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Teachable Reasoning Systems. (arXiv:2204.13074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13074">
<div class="article-summary-box-inner">
<span><p>Our goal is a teachable reasoning system for question-answering (QA), where a
user can interact with faithful answer explanations, and correct errors so that
the system improves over time. Our approach is three-fold: First, generated
chains of reasoning show how answers are implied by the system's own internal
beliefs. Second, users can interact with the explanations to identify erroneous
model beliefs and provide corrections. Third, we augment the model with a
dynamic memory of such corrections. Retrievals from memory are used as
additional context for QA, to help avoid previous mistakes in similar new
situations - a novel type of memory-based continuous learning. To our
knowledge, this is the first system to generate chains that are both faithful
(the answer follows from the reasoning) and truthful (the chain reflects the
system's own beliefs, as ascertained by self-querying). In evaluation, users
judge that a majority (65%+) of generated chains clearly show how an answer
follows from a set of facts - substantially better than a high-performance
baseline. We also find that using simulated feedback, our system (called
EntailmentWriter) continually improves with time, requiring feedback on only
25% of training examples to reach within 1% of the upper-bound (feedback on all
examples). We observe a similar trend with real users. This suggests new
opportunities for using language models in an interactive setting where users
can inspect, debug, correct, and improve a system's performance over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Borrow -- Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion. (arXiv:2204.13097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13097">
<div class="article-summary-box-inner">
<span><p>Prior work on integrating text corpora with knowledge graphs (KGs) to improve
Knowledge Graph Embedding (KGE) have obtained good performance for entities
that co-occur in sentences in text corpora. Such sentences (textual mentions of
entity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between
two entities. However, it is not possible to represent relations between
entities that do not co-occur in a single sentence using LDPs. In this paper,
we propose and evaluate several methods to address this problem, where we
borrow LDPs from the entity pairs that co-occur in sentences in the corpus
(i.e. with mention entity pairs) to represent entity pairs that do not co-occur
in any sentence in the corpus (i.e. without mention entity pairs). We propose a
supervised borrowing method, SuperBorrow, that learns to score the suitability
of an LDP to represent a without-mention entity pair using pre-trained entity
embeddings and contextualised LDP representations. Experimental results show
that SuperBorrow improves the link prediction performance of multiple
widely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural String Edit Distance. (arXiv:2104.08388v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08388">
<div class="article-summary-box-inner">
<span><p>We propose the neural string edit distance model for string-pair matching and
string transduction based on learnable string edit distance. We modify the
original expectation-maximization learned edit distance algorithm into a
differentiable loss function, allowing us to integrate it into a neural network
providing a contextual representation of the input. We evaluate on cognate
detection, transliteration, and grapheme-to-phoneme conversion, and show that
we can trade off between performance and interpretability in a single
framework. Using contextual representations, which are difficult to interpret,
we match the performance of state-of-the-art string-pair matching models. Using
static embeddings and a slightly different loss function, we force
interpretability, at the expense of an accuracy drop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shades of confusion: Lexical uncertainty modulates ad hoc coordination in an interactive communication task. (arXiv:2105.06546v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06546">
<div class="article-summary-box-inner">
<span><p>There is substantial variability in the expectations that communication
partners bring into interactions, creating the potential for misunderstandings.
To directly probe these gaps and our ability to overcome them, we propose a
communication task based on color-concept associations. In Experiment 1, we
establish several key properties of the mental representations of these
expectations, or lexical priors, based on recent probabilistic theories.
Associations are more variable for abstract concepts, variability is
represented as uncertainty within each individual, and uncertainty enables
accurate predictions about whether others are likely to share the same
association. In Experiment 2, we then examine the downstream consequences of
these representations for communication. Accuracy is initially low when
communicating about concepts with more variable associations, but rapidly
increases as participants form ad hoc conventions. Together, our findings
suggest that people cope with variability by maintaining well-calibrated
uncertainty about their partner and appropriately adaptable representations of
their own.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Pre-trained Auto-regressive Language Models for Named Entity Typing and Recognition. (arXiv:2108.11857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11857">
<div class="article-summary-box-inner">
<span><p>Despite impressive results of language models for named entity recognition
(NER), their generalization to varied textual genres, a growing entity type
set, and new entities remains a challenge. Collecting thousands of annotations
in each new case for training or fine-tuning is expensive and time-consuming.
In contrast, humans can easily identify named entities given some simple
instructions. Inspired by this, we challenge the reliance on large datasets and
study pre-trained language models for NER in a meta-learning setup. First, we
test named entity typing (NET) in a zero-shot transfer scenario. Then, we
perform NER by giving few examples at inference. We propose a method to select
seen and rare / unseen names when having access only to the pre-trained model
and report results on these groups. The results show: auto-regressive language
models as meta-learners can perform NET and NER fairly well especially for
regular or seen names; name irregularity when often present for a certain
entity type can become an effective exploitable cue; names with words foreign
to the model have the most negative impact on results; the model seems to rely
more on name than context cues in few-shot NER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations. (arXiv:2109.13087v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13087">
<div class="article-summary-box-inner">
<span><p>We study the problem of coarse-grained response selection in retrieval-based
dialogue systems. The problem is equally important with fine-grained response
selection, but is less explored in existing literature. In this paper, we
propose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained
response selection in open-domain conversations. In our CFC model, dense
representations of query, candidate response and corresponding context is
learned based on the multi-tower architecture, and more expressive knowledge
learned from the one-tower architecture (fine-grained) is distilled into the
multi-tower architecture (coarse-grained) to enhance the performance of the
retriever. To evaluate the performance of our proposed model, we construct two
new datasets based on the Reddit comments dump and Twitter corpus. Extensive
experimental results on the two datasets show that the proposed methods achieve
a significant improvement over all evaluation metrics compared with traditional
baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why don't people use character-level machine translation?. (arXiv:2110.08191v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08191">
<div class="article-summary-box-inner">
<span><p>We present a literature and empirical survey that critically assesses the
state of the art in character-level modeling for machine translation (MT).
Despite evidence in the literature that character-level systems are comparable
with subword systems, they are virtually never used in competitive setups in
WMT competitions. We empirically show that even with recent modeling
innovations in character-level natural language processing, character-level MT
systems still struggle to match their subword-based counterparts.
Character-level MT systems show neither better domain robustness, nor better
morphological generalization, despite being often so motivated. However, we are
able to show robustness towards source side noise and that translation quality
does not degrade with increasing beam size at decoding time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Cross-Utterance Language Modeling for Conversational Speech Recognition. (arXiv:2111.03333v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03333">
<div class="article-summary-box-inner">
<span><p>Conversational speech normally is embodied with loose syntactic structures at
the utterance level but simultaneously exhibits topical coherence relations
across consecutive utterances. Prior work has shown that capturing longer
context information with a recurrent neural network or long short-term memory
language model (LM) may suffer from the recent bias while excluding the
long-range context. In order to capture the long-term semantic interactions
among words and across utterances, we put forward disparate conversation
history fusion methods for language modeling in automatic speech recognition
(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is
introduced, which manages to fuse and utilize the acoustic embeddings of a
current utterance and the semantic content of its corresponding conversation
history in a cooperative way. To flesh out our ideas, we frame the ASR N-best
hypothesis rescoring task as a prediction problem, leveraging BERT, an iconic
pre-trained LM, as the ingredient vehicle to facilitate selection of the oracle
hypothesis from a given N-best hypothesis list. Empirical experiments conducted
on the AMI benchmark dataset seem to demonstrate the feasibility and efficacy
of our methods in relation to some current top-of-line methods. The proposed
methods not only achieve significant inference time reduction but also improve
the ASR performance for conversational speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Transferability of Prompt Tuning for Natural Language Processing. (arXiv:2111.06719v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06719">
<div class="article-summary-box-inner">
<span><p>Prompt tuning (PT) is a promising parameter-efficient method to utilize
extremely large pre-trained language models (PLMs), which can achieve
comparable performance to full-parameter fine-tuning by only tuning a few soft
prompts. However, PT requires much more training time than fine-tuning.
Intuitively, knowledge transfer can help to improve the efficiency. To explore
whether we can improve PT via prompt transfer, we empirically investigate the
transferability of soft prompts across different downstream tasks and PLMs in
this work. We find that (1) in zero-shot setting, trained soft prompts can
effectively transfer to similar tasks on the same PLM and also to other PLMs
with a cross-model projector trained on similar tasks; (2) when used as
initialization, trained soft prompts of similar tasks and projected prompts of
other PLMs can significantly accelerate training and also improve the
performance of PT. Moreover, to explore what decides prompt transferability, we
investigate various transferability indicators and find that the overlapping
rate of activated neurons strongly reflects the transferability, which suggests
how the prompts stimulate PLMs is essential. Our findings show that prompt
transfer is promising for improving PT, and further research shall focus more
on prompts' stimulation to PLMs. The source code can be obtained from
https://github.com/thunlp/Prompt-Transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated scholarly paper review: Technologies and challenges. (arXiv:2111.07533v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07533">
<div class="article-summary-box-inner">
<span><p>Peer review is a widely accepted mechanism for research evaluation, playing a
pivotal role in scholarly publishing. However, criticisms have long been
leveled on this mechanism, mostly because of its inefficiency and subjectivity.
Recent years have seen the application of artificial intelligence (AI) in
assisting the peer review process. Nonetheless, with the involvement of humans,
such limitations remain inevitable. In this review paper, we propose the
concept and pipeline of automated scholarly paper review (ASPR) and review the
relevant literature and technologies of achieving a full-scale computerized
review process. On the basis of the review and discussion, we conclude that
there is already corresponding research and implementation at each stage of
ASPR. We further look into the challenges in ASPR with the existing
technologies. The major difficulties lie in imperfect document parsing and
representation, inadequate data, defective human-computer interaction and
flawed deep logical reasoning. Moreover, we discuss the possible moral &amp;
ethical issues and point out the future directions of ASPR. In the foreseeable
future, ASPR and peer review will coexist in a reinforcing manner before ASPR
is able to fully undertake the reviewing workload from humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Triggerless Backdoor Attack for NLP Tasks with Clean Labels. (arXiv:2111.07970v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07970">
<div class="article-summary-box-inner">
<span><p>Backdoor attacks pose a new threat to NLP models. A standard strategy to
construct poisoned data in backdoor attacks is to insert triggers (e.g., rare
words) into selected sentences and alter the original label to a target label.
This strategy comes with a severe flaw of being easily detected from both the
trigger and the label perspectives: the trigger injected, which is usually a
rare word, leads to an abnormal natural language expression, and thus can be
easily detected by a defense model; the changed target label leads the example
to be mistakenly labeled and thus can be easily detected by manual inspections.
To deal with this issue, in this paper, we propose a new strategy to perform
textual backdoor attacks which do not require an external trigger, and the
poisoned samples are correctly labeled. The core idea of the proposed strategy
is to construct clean-labeled examples, whose labels are correct but can lead
to test label changes when fused with the training set. To generate poisoned
clean-labeled examples, we propose a sentence generation model based on the
genetic algorithm to cater to the non-differentiable characteristic of text
data. Extensive experiments demonstrate that the proposed attacking strategy is
not only effective, but more importantly, hard to defend due to its triggerless
and clean-labeled nature. Our work marks the first step towards developing
triggerless attacking strategies in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Literature-Augmented Clinical Outcome Prediction. (arXiv:2111.08374v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08374">
<div class="article-summary-box-inner">
<span><p>We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach
for clinical outcome prediction that retrieves patient-specific medical
literature and incorporates it into predictive models. Based on each individual
patient's clinical notes, we train language models (LMs) to find relevant
papers and fuse them with information from notes to predict outcomes such as
in-hospital mortality. We develop methods to retrieve literature based on
noisy, information-dense patient notes, and to augment existing outcome
prediction models with retrieved papers in a manner that maximizes predictive
accuracy. Our approach boosts predictive performance on three important
clinical tasks in comparison to strong recent LM baselines, increasing F1 by up
to 5 points and precision@Top-K by a large margin of over 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Learning for Unsupervised Knowledge Grounded Dialogs. (arXiv:2112.00653v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00653">
<div class="article-summary-box-inner">
<span><p>Recent methods for knowledge grounded dialogs generate responses by
incorporating information from an external textual document. These methods do
not require the exact document to be known during training and rely on the use
of a retrieval system to fetch relevant documents from a large index. The
documents used to generate the responses are modeled as latent variables whose
prior probabilities need to be estimated. Models such as RAG and REALM,
marginalize the document probabilities over the documents retrieved from the
index to define the log likelihood loss function which is optimized end-to-end.
</p>
<p>In this paper, we develop a variational approach to the above technique
wherein, we instead maximize the Evidence Lower bound (ELBO). Using a
collection of three publicly available open-conversation datasets, we
demonstrate how the posterior distribution, that has information from the
ground-truth response, allows for a better approximation of the objective
function during training. To overcome the challenges associated with sampling
over a large knowledge collection, we develop an efficient approach to
approximate the ELBO. To the best of our knowledge we are the first to apply
variational training for open-scale unsupervised knowledge grounded dialog
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Text-to-SQL Parsing through Question Decomposition. (arXiv:2112.06311v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06311">
<div class="article-summary-box-inner">
<span><p>Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query
relational data. Training such parsers, by contrast, generally requires
expertise in annotating natural language (NL) utterances with corresponding SQL
queries. In this work, we propose a weak supervision approach for training
text-to-SQL parsers. We take advantage of the recently proposed question
meaning representation called QDMR, an intermediate between NL and formal query
languages. Given questions, their QDMR structures (annotated by non-experts or
automatically predicted), and the answers, we are able to automatically
synthesize SQL queries that are used to train text-to-SQL models. We test our
approach by experimenting on five benchmark datasets. Our results show that the
weakly supervised models perform competitively with those trained on annotated
NL-SQL data. Overall, we effectively train text-to-SQL parsers, while using
zero SQL annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Neural Models for Query-Focused Summarization. (arXiv:2112.07637v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07637">
<div class="article-summary-box-inner">
<span><p>Query-focused summarization (QFS) aims to produce summaries that answer
particular questions of interest, enabling greater user control and
personalization. While recently released datasets, such as QMSum or AQuaMuSe,
facilitate research efforts in QFS, the field lacks a comprehensive study of
the broad space of applicable modeling methods. In this paper we conduct a
systematic exploration of neural approaches to QFS, considering two general
classes of methods: two-stage extractive-abstractive solutions and end-to-end
models. Within those categories, we investigate existing models and explore
strategies for transfer learning. We also present two modeling extensions that
achieve state-of-the-art performance on the QMSum dataset, up to a margin of
3.38 ROUGE-1, 3.72 ROUGE2, and 3.28 ROUGE-L when combined with transfer
learning strategies. Results from human evaluation suggest that the best models
produce more comprehensive and factually consistent summaries compared to a
baseline model. Code and checkpoints are made publicly available:
https://github.com/salesforce/query-focused-sum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sublinear Time Approximation of Text Similarity Matrices. (arXiv:2112.09631v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09631">
<div class="article-summary-box-inner">
<span><p>We study algorithms for approximating pairwise similarity matrices that arise
in natural language processing. Generally, computing a similarity matrix for
$n$ data points requires $\Omega(n^2)$ similarity computations. This quadratic
scaling is a significant bottleneck, especially when similarities are computed
via expensive functions, e.g., via transformer models. Approximation methods
reduce this quadratic complexity, often by using a small subset of exactly
computed similarities to approximate the remainder of the complete pairwise
similarity matrix.
</p>
<p>Significant work focuses on the efficient approximation of positive
semidefinite (PSD) similarity matrices, which arise e.g., in kernel methods.
However, much less is understood about indefinite (non-PSD) similarity
matrices, which often arise in NLP. Motivated by the observation that many of
these matrices are still somewhat close to PSD, we introduce a generalization
of the popular Nystr\"{o}m method to the indefinite setting. Our algorithm can
be applied to any similarity matrix and runs in sublinear time in the size of
the matrix, producing a rank-$s$ approximation with just $O(ns)$ similarity
computations.
</p>
<p>We show that our method, along with a simple variant of CUR decomposition,
performs very well in approximating a variety of similarity matrices arising in
NLP tasks. We demonstrate high accuracy of the approximated similarity matrices
in the downstream tasks of document classification, sentence similarity, and
cross-document coreference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asyncval: A Toolkit for Asynchronously Validating Dense Retriever Checkpoints during Training. (arXiv:2202.12510v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12510">
<div class="article-summary-box-inner">
<span><p>The process of model checkpoint validation refers to the evaluation of the
performance of a model checkpoint executed on a held-out portion of the
training data while learning the hyperparameters of the model, and is used to
avoid over-fitting and determine when the model has converged so as to stop
training. A simple and efficient strategy to validate deep learning checkpoints
is the addition of validation loops to execute during training. However, the
validation of dense retrievers (DR) checkpoints is not as trivial -- and the
addition of validation loops is not efficient. This is because, in order to
accurately evaluate the performance of a DR checkpoint, the whole document
corpus needs to be encoded into vectors using the current checkpoint before any
actual retrieval operation for checkpoint validation can be performed. This
corpus encoding process can be very time-consuming if the document corpus
contains millions of documents (e.g., 8.8m for MS MARCO and 21m for Natural
Questions). Thus, a naive use of validation loops during training will
significantly increase training time. To address this issue, in this demo
paper, we propose Asyncval: a Python-based toolkit for efficiently validating
DR checkpoints during training. Instead of pausing the training loop for
validating DR checkpoints, Asyncval decouples the validation loop from the
training loop, uses another GPU to automatically validate new DR checkpoints
and thus permits to perform validation asynchronously from training. Asyncval
also implements a range of different corpus subset sampling strategies for
validating DR checkpoints; these strategies allow to further speed up the
validation process. We provide an investigation of these methods in terms of
their impact on validation time and validation fidelity. Asyncval is made
available as an open-source project at https://github.com/ielab/asyncval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval. (arXiv:2203.06169v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06169">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever
that does not require any supervised data for training. Specifically, we first
present Iterative Contrastive Learning (ICoL) that iteratively trains the query
and document encoders with a cache mechanism. ICoL not only enlarges the number
of negative instances but also keeps representations of cached examples in the
same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a
simple yet effective way to enhance dense retrieval with lexical matching. We
evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18
datasets of 9 zero-shot text retrieval tasks. Experimental results show that
LaPraDoR achieves state-of-the-art performance compared with supervised dense
retrieval models, and further analysis reveals the effectiveness of our
training strategy and objectives. Compared to re-ranking, our lexicon-enhanced
approach can be run in milliseconds (22.5x faster) while achieving superior
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08075">
<div class="article-summary-box-inner">
<span><p>Spatial commonsense, the knowledge about spatial position and relationship
between objects (like the relative size of a lion and a girl, and the position
of a boy relative to a bicycle when cycling), is an important part of
commonsense knowledge. Although pretrained language models (PLMs) succeed in
many NLP tasks, they are shown to be ineffective in spatial commonsense
reasoning. Starting from the observation that images are more likely to exhibit
spatial commonsense than texts, we explore whether models with visual signals
learn more spatial commonsense than text-based PLMs. We propose a spatial
commonsense benchmark that focuses on the relative scales of objects, and the
positional relationship between people and objects under different actions. We
probe PLMs and models with visual signals, including vision-language pretrained
models and image synthesis models, on this benchmark, and find that image
synthesis models are more capable of learning accurate and consistent spatial
knowledge than other models. The spatial knowledge from image synthesis models
also helps in natural language understanding tasks that require spatial
commonsense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plagiarism Detection in the Bengali Language: A Text Similarity-Based Approach. (arXiv:2203.13430v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13430">
<div class="article-summary-box-inner">
<span><p>Plagiarism means taking another person's work and not giving any credit to
them for it. Plagiarism is one of the most serious problems in academia and
among researchers. Even though there are multiple tools available to detect
plagiarism in a document but most of them are domain-specific and designed to
work in English texts, but plagiarism is not limited to a single language only.
Bengali is the most widely spoken language of Bangladesh and the second most
spoken language in India with 300 million native speakers and 37 million
second-language speakers. Plagiarism detection requires a large corpus for
comparison. Bengali Literature has a history of 1300 years. Hence most Bengali
Literature books are not yet digitalized properly. As there was no such corpus
present for our purpose so we have collected Bengali Literature books from the
National Digital Library of India and with a comprehensive methodology
extracted texts from it and constructed our corpus. Our experimental results
find out average accuracy between 72.10 % - 79.89 % in text extraction using
OCR. Levenshtein Distance algorithm is used for determining Plagiarism. We have
built a web application for end-user and successfully tested it for Plagiarism
detection in Bengali texts. In future, we aim to construct a corpus with more
books for more accurate detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Indian Language Datasets used for Text Summarization. (arXiv:2203.16127v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16127">
<div class="article-summary-box-inner">
<span><p>In this paper, we survey Text Summarization (TS) datasets in Indian Languages
(ILs), which are also low-resource languages (LRLs). We seek to answer one
primary question: is the pool of Indian Language Text Summarization (ILTS)
dataset growing or is there a resource poverty? To an-swer the primary
question, we pose two sub-questions that we seek about ILTS datasets: first,
what characteristics: format and domain do ILTS datasets have? Second, how
different are those characteristics of ILTS datasets from high-resource
languages (HRLs) particularly English. We focus on datasets reported in
published ILTS research works during 2012-2022. The survey of ILTS and English
datasets reveals two similarities and one contrast. The two similarities are:
first, the domain of dataset commonly is news (Hermann et al., 2015). The
second similarity is the format of the dataset which is both extractive and
abstractive. The contrast is in how the research in dataset development has
progressed. ILs face a slow speed of development and public release of datasets
as compared with English. We argue that the relatively lower number of ILTS
datasets is because of two reasons: first, absence of a dedicated forum for
developing TS tools and resources; and second, lack of shareable standard
datasets in the public domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperBox: A Supervised Approach for Hypernym Discovery using Box Embeddings. (arXiv:2204.02058v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02058">
<div class="article-summary-box-inner">
<span><p>Hypernymy plays a fundamental role in many AI tasks like taxonomy learning,
ontology learning, etc. This has motivated the development of many automatic
identification methods for extracting this relation, most of which rely on word
distribution. We present a novel model HyperBox to learn box embeddings for
hypernym discovery. Given an input term, HyperBox retrieves its suitable
hypernym from a target corpus. For this task, we use the dataset published for
SemEval 2018 Shared Task on Hypernym Discovery. We compare the performance of
our model on two specific domains of knowledge: medical and music.
Experimentally, we show that our model outperforms existing methods on the
majority of the evaluation metrics. Moreover, our model generalize well over
unseen hypernymy pairs using only a small set of training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weakly Supervised Propagation Model for Rumor Verification and Stance Detection with Multiple Instance Learning. (arXiv:2204.02626v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02626">
<div class="article-summary-box-inner">
<span><p>The diffusion of rumors on microblogs generally follows a propagation tree
structure, that provides valuable clues on how an original message is
transmitted and responded by users over time. Recent studies reveal that rumor
detection and stance detection are two different but relevant tasks which can
jointly enhance each other, e.g., rumors can be debunked by cross-checking the
stances conveyed by their relevant microblog posts, and stances are also
conditioned on the nature of the rumor. However, most stance detection methods
require enormous post-level stance labels for training, which are
labor-intensive given a large number of posts. Enlightened by Multiple Instance
Learning (MIL) scheme, we first represent the diffusion of claims with
bottom-up and top-down trees, then propose two tree-structured weakly
supervised frameworks to jointly classify rumors and stances, where only the
bag-level labels concerning claim's veracity are needed. Specifically, we
convert the multi-class problem into a multiple MIL-based binary classification
problem where each binary model focuses on differentiating a target stance or
rumor type and other types. Finally, we propose a hierarchical attention
mechanism to aggregate the binary predictions, including (1) a bottom-up or
top-down tree attention layer to aggregate binary stances into binary veracity;
and (2) a discriminative attention layer to aggregate the binary class into
finer-grained classes. Extensive experiments conducted on three Twitter-based
datasets demonstrate promising performance of our model on both claim-level
rumor detection and post-level stance classification compared with
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tweet Emotion Dynamics: Emotion Word Usage in Tweets from US and Canada. (arXiv:2204.04862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04862">
<div class="article-summary-box-inner">
<span><p>Over the last decade, Twitter has emerged as one of the most influential
forums for social, political, and health discourse. In this paper, we introduce
a massive dataset of more than 45 million geo-located tweets posted between
2015 and 2021 from US and Canada (TUSC), especially curated for natural
language analysis. We also introduce Tweet Emotion Dynamics (TED) -- metrics to
capture patterns of emotions associated with tweets over time. We use TED and
TUSC to explore the use of emotion-associated words across US and Canada;
across 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the
second year of the pandemic); and across individual tweeters. We show that
Canadian tweets tend to have higher valence, lower arousal, and higher
dominance than the US tweets. Further, we show that the COVID-19 pandemic had a
marked impact on the emotional signature of tweets posted in 2020, when
compared to the adjoining years. Finally, we determine metrics of TED for
170,000 tweeters to benchmark characteristics of TED metrics at an aggregate
level. TUSC and the metrics for TED will enable a wide variety of research on
studying how we use language to express ourselves, persuade, communicate, and
influence, with particularly promising applications in public health, affective
science, social science, and psychology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image. (arXiv:2204.05533v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05533">
<div class="article-summary-box-inner">
<span><p>This study investigates how fake news uses a thumbnail for a news article
with a focus on whether a news article's thumbnail represents the news content
correctly. A news article shared with an irrelevant thumbnail can mislead
readers into having a wrong impression of the issue, especially in social media
environments where users are less likely to click the link and consume the
entire content. We propose to capture the degree of semantic incongruity in the
multimodal relation by using the pretrained CLIP representation. From a
source-level analysis, we found that fake news employs a more incongruous image
to the main content than general news. Going further, we attempted to detect
news articles with image-text incongruity. Evaluation experiments suggest that
CLIP-based methods can successfully detect news articles in which the thumbnail
is semantically irrelevant to news text. This study contributes to the research
by providing a novel view on tackling online fake news and misinformation. Code
and datasets are available at
https://github.com/ssu-humane/fake-news-thumbnail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets. (arXiv:2204.08997v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08997">
<div class="article-summary-box-inner">
<span><p>In recent years, interest has arisen in using machine learning to improve the
efficiency of automatic medical consultation and enhance patient experience. In
this paper, we propose two frameworks to support automatic medical
consultation, namely doctor-patient dialogue understanding and task-oriented
interaction. A new large medical dialogue dataset with multi-level fine-grained
annotations is introduced and five independent tasks are established, including
named entity recognition, dialogue act classification, symptom label inference,
medical report generation and diagnosis-oriented dialogue policy. We report a
set of benchmark results for each task, which shows the usability of the
dataset and sets a baseline for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11406">
<div class="article-summary-box-inner">
<span><p>Self-augmentation has been received increasing research interest recently to
improve named entity recognition (NER) performance in low-resource scenarios.
Token substitution and mixup are two feasible heterogeneous self-augmentation
techniques for NER that can achieve effective performance with certain
specialized efforts. Noticeably, self-augmentation may introduce potentially
noisy augmented data. Prior research has mainly resorted to heuristic rule
based constraints to reduce the noise for specific self-augmentation
individually. In this paper, we revisit the two self-augmentation methods for
NER, and propose a unified meta-reweighting strategy for these heterogeneous
methods to achieve a natural integration. Our method is easily extensible,
imposing little effort on a specific self-augmentation method. Experiments on
different Chinese and English NER benchmarks demonstrate that our token
substitution and mixup method, as well as their integration, can obtain
effective performance improvement. Based on the meta-reweighting mechanism, we
can enhance the advantages of the self-augmentation techniques without extra
efforts.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions. (arXiv:2204.12511v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12511">
<div class="article-summary-box-inner">
<span><p>Cross-entropy loss and focal loss are the most common choices when training
deep neural networks for classification problems. Generally speaking, however,
a good loss function can take on much more flexible forms, and should be
tailored for different tasks and datasets. Motivated by how functions can be
approximated via Taylor expansion, we propose a simple framework, named
PolyLoss, to view and design loss functions as a linear combination of
polynomial functions. Our PolyLoss allows the importance of different
polynomial bases to be easily adjusted depending on the targeting tasks and
datasets, while naturally subsuming the aforementioned cross-entropy loss and
focal loss as special cases. Extensive experimental results show that the
optimal choice within the PolyLoss is indeed dependent on the task and dataset.
Simply by introducing one extra hyperparameter and adding one line of code, our
Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D
image classification, instance segmentation, object detection, and 3D object
detection tasks, sometimes by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coupled Iterative Refinement for 6D Multi-Object Pose Estimation. (arXiv:2204.12516v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12516">
<div class="article-summary-box-inner">
<span><p>We address the task of 6D multi-object pose: given a set of known 3D objects
and an RGB or RGB-D input image, we detect and estimate the 6D pose of each
object. We propose a new approach to 6D object pose estimation which consists
of an end-to-end differentiable architecture that makes use of geometric
knowledge. Our approach iteratively refines both pose and correspondence in a
tightly coupled manner, allowing us to dynamically remove outliers to improve
accuracy. We use a novel differentiable layer to perform pose refinement by
solving an optimization problem we refer to as Bidirectional Depth-Augmented
Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on
standard 6D Object Pose benchmarks. Code is available at
https://github.com/princeton-vl/Coupled-Iterative-Refinement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Unlabeled Data for Sketch-based Understanding. (arXiv:2204.12522v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12522">
<div class="article-summary-box-inner">
<span><p>Sketch-based understanding is a critical component of human cognitive
learning and is a primitive communication means between humans. This topic has
recently attracted the interest of the computer vision community as sketching
represents a powerful tool to express static objects and dynamic scenes.
Unfortunately, despite its broad application domains, the current sketch-based
models strongly rely on labels for supervised training, ignoring knowledge from
unlabeled data, thus limiting the underlying generalization and the
applicability. Therefore, we present a study about the use of unlabeled data to
improve a sketch-based model. To this end, we evaluate variations of VAE and
semi-supervised VAE, and present an extension of BYOL to deal with sketches.
Our results show the superiority of sketch-BYOL, which outperforms other
self-supervised approaches increasing the retrieval performance for known and
unknown categories. Furthermore, we show how other tasks can benefit from our
proposal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expanding the Latent Space of StyleGAN for Real Face Editing. (arXiv:2204.12530v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12530">
<div class="article-summary-box-inner">
<span><p>Recently, a surge of face editing techniques have been proposed to employ the
pretrained StyleGAN for semantic manipulation. To successfully edit a real
image, one must first convert the input image into StyleGAN's latent variables.
However, it is still challenging to find latent variables, which have the
capacity for preserving the appearance of the input subject (e.g., identity,
lighting, hairstyles) as well as enabling meaningful manipulations. In this
paper, we present a method to expand the latent space of StyleGAN with
additional content features to break down the trade-off between low-distortion
and high-editability. Specifically, we proposed a two-branch model, where the
style branch first tackles the entanglement issue by the sparse manipulation of
latent codes, and the content branch then mitigates the distortion issue by
leveraging the content and appearance details from the input image. We confirm
the effectiveness of our method using extensive qualitative and quantitative
experiments on real face editing and reconstruction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AccMPEG: Optimizing Video Encoding for Video Analytics. (arXiv:2204.12534v1 [cs.NI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12534">
<div class="article-summary-box-inner">
<span><p>With more videos being recorded by edge sensors (cameras) and analyzed by
computer-vision deep neural nets (DNNs), a new breed of video streaming systems
has emerged, with the goal to compress and stream videos to remote servers in
real time while preserving enough information to allow highly accurate
inference by the server-side DNNs. An ideal design of the video streaming
system should simultaneously meet three key requirements: (1) low latency of
encoding and streaming, (2) high accuracy of server-side DNNs, and (3) low
compute overheads on the camera. Unfortunately, despite many recent efforts,
such video streaming system has hitherto been elusive, especially when serving
advanced vision tasks such as object detection or semantic segmentation. This
paper presents AccMPEG, a new video encoding and streaming system that meets
all the three requirements. The key is to learn how much the encoding quality
at each (16x16) macroblock can influence the server-side DNN accuracy, which we
call accuracy gradient. Our insight is that these macroblock-level accuracy
gradient can be inferred with sufficient precision by feeding the video frames
through a cheap model. AccMPEG provides a suite of techniques that, given a new
server-side DNN, can quickly create a cheap model to infer the accuracy
gradient on any new frame in near realtime. Our extensive evaluation of AccMPEG
on two types of edge devices (one Intel Xeon Silver 4100 CPU or NVIDIA Jetson
Nano) and three vision tasks (six recent pre-trained DNNs) shows that AccMPEG
(with the same camera-side compute resources) can reduce the end-to-end
inference delay by 10-43% without hurting accuracy compared to the
state-of-the-art baselines
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Change Detection using Multi-Temporal Airborne LiDAR Data. (arXiv:2204.12535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12535">
<div class="article-summary-box-inner">
<span><p>Building change detection is essential for monitoring urbanization, disaster
assessment, urban planning and frequently updating the maps. 3D structure
information from airborne light detection and ranging (LiDAR) is very effective
for detecting urban changes. But the 3D point cloud from airborne LiDAR(ALS)
holds an enormous amount of unordered and irregularly sparse information.
Handling such data is tricky and consumes large memory for processing. Most of
this information is not necessary when we are looking for a particular type of
urban change. In this study, we propose an automatic method that reduces the 3D
point clouds into a much smaller representation without losing the necessary
information required for detecting Building changes. The method utilizes the
Deep Learning(DL) model U-Net for segmenting the buildings from the background.
Produced segmentation maps are then processed further for detecting changes and
the results are refined using morphological methods. For the change detection
task, we used multi-temporal airborne LiDAR data. The data is acquired over
Stockholm in the years 2017 and 2019. The changes in buildings are classified
into four types: 'newly built', 'demolished', 'taller' and 'shorter'. The
detected changes are visualized in one map for better interpretation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi stain graph fusion for multimodal integration in pathology. (arXiv:2204.12541v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12541">
<div class="article-summary-box-inner">
<span><p>In pathology, tissue samples are assessed using multiple staining techniques
to enhance contrast in unique histologic features. In this paper, we introduce
a multimodal CNN-GNN based graph fusion approach that leverages complementary
information from multiple non-registered histopathology images to predict
pathologic scores. We demonstrate this approach in nonalcoholic steatohepatitis
(NASH) by predicting CRN fibrosis stage and NAFLD Activity Score (NAS). Primary
assessment of NASH typically requires liver biopsy evaluation on two
histological stains: Trichrome (TC) and hematoxylin and eosin (H&amp;E). Our
multimodal approach learns to extract complementary information from TC and H&amp;E
graphs corresponding to each stain while simultaneously learning an optimal
policy to combine this information. We report up to 20% improvement in
predicting fibrosis stage and NAS component grades over single-stain modeling
approaches, measured by computing linearly weighted Cohen's kappa between
machine-derived vs. pathologist consensus scores. Broadly, this paper
demonstrates the value of leveraging diverse pathology images for improved
ML-powered histologic assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for Tamil TrollMeme Classification. (arXiv:2204.12587v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12587">
<div class="article-summary-box-inner">
<span><p>Social media platforms often act as breeding grounds for various forms of
trolling or malicious content targeting users or communities. One way of
trolling users is by creating memes, which in most cases unites an image with a
short piece of text embedded on top of it. The situation is more complex for
multilingual(e.g., Tamil) memes due to the lack of benchmark datasets and
models. We explore several models to detect Troll memes in Tamil based on the
shared task, "Troll Meme Classification in DravidianLangTech2022" at ACL-2022.
We observe while the text-based model MURIL performs better for Non-troll meme
classification, the image-based model VGG16 performs better for Troll-meme
classification. Further fusing these two modalities help us achieve stable
outcomes in both classes. Our fusion model achieved a 0.561 weighted average F1
score and ranked second in this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Influence of the Other-Race Effect on Susceptibility to Face Morphing Attacks. (arXiv:2204.12591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12591">
<div class="article-summary-box-inner">
<span><p>Facial morphs created between two identities resemble both of the faces used
to create the morph. Consequently, humans and machines are prone to mistake
morphs made from two identities for either of the faces used to create the
morph. This vulnerability has been exploited in "morph attacks" in security
scenarios. Here, we asked whether the "other-race effect" (ORE) -- the human
advantage for identifying own- vs. other-race faces -- exacerbates morph attack
susceptibility for humans. We also asked whether face-identification
performance in a deep convolutional neural network (DCNN) is affected by the
race of morphed faces. Caucasian (CA) and East-Asian (EA) participants
performed a face-identity matching task on pairs of CA and EA face images in
two conditions. In the morph condition, different-identity pairs consisted of
an image of identity "A" and a 50/50 morph between images of identity "A" and
"B". In the baseline condition, morphs of different identities never appeared.
As expected, morphs were identified mistakenly more often than original face
images. Moreover, CA participants showed an advantage for CA faces in
comparison to EA faces (a partial ORE). Of primary interest, morph
identification was substantially worse for cross-race faces than for own-race
faces. Similar to humans, the DCNN performed more accurately for original face
images than for morphed image pairs. Notably, the deep network proved
substantially more accurate than humans in both cases. The results point to the
possibility that DCNNs might be useful for improving face identification
accuracy when morphed faces are presented. They also indicate the significance
of the ORE in morph attack susceptibility in applied settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Self-taught Learning-based Representations for Facial Emotion Recognition. (arXiv:2204.12624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12624">
<div class="article-summary-box-inner">
<span><p>This work describes different strategies to generate unsupervised
representations obtained through the concept of self-taught learning for facial
emotion recognition (FER). The idea is to create complementary representations
promoting diversity by varying the autoencoders' initialization, architecture,
and training data. SVM, Bagging, Random Forest, and a dynamic ensemble
selection method are evaluated as final classification methods. Experimental
results on Jaffe and Cohn-Kanade datasets using a leave-one-subject-out
protocol show that FER methods based on the proposed diverse representations
compare favorably against state-of-the-art approaches that also explore
unsupervised feature learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCGC : Self-Supervised Contrastive Graph Clustering. (arXiv:2204.12656v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12656">
<div class="article-summary-box-inner">
<span><p>Graph clustering discovers groups or communities within networks. Deep
learning methods such as autoencoders (AE) extract effective clustering and
downstream representations but cannot incorporate rich structural information.
While Graph Neural Networks (GNN) have shown great success in encoding graph
structure, typical GNNs based on convolution or attention variants suffer from
over-smoothing, noise, heterophily, are computationally expensive and typically
require the complete graph being present. Instead, we propose Self-Supervised
Contrastive Graph Clustering (SCGC), which imposes graph-structure via
contrastive loss signals to learn discriminative node representations and
iteratively refined soft cluster labels. We also propose SCGC*, with a more
effective, novel, Influence Augmented Contrastive (IAC) loss to fuse richer
structural information, and half the original model parameters. SCGC(*) is
faster with simple linear units, completely eliminate convolutions and
attention of traditional GNNs, yet efficiently incorporates structure. It is
impervious to layer depth and robust to over-smoothing, incorrect edges and
heterophily. It is scalable by batching, a limitation in many prior GNN models,
and trivially parallelizable. We obtain significant improvements over
state-of-the-art on a wide range of benchmark graph datasets, including images,
sensor data, text, and citation networks efficiently. Specifically, 20% on ARI
and 18% on NMI for DBLP; overall 55% reduction in training time and overall,
81% reduction on inference time. Our code is available at :
https://github.com/gayanku/SCGC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation. (arXiv:2204.12667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12667">
<div class="article-summary-box-inner">
<span><p>Test-time adaptation approaches have recently emerged as a practical solution
for handling domain shift without access to the source domain data. In this
paper, we propose and explore a new multi-modal extension of test-time
adaptation for 3D semantic segmentation. We find that directly applying
existing methods usually results in performance instability at test time
because multi-modal input is not considered jointly. To design a framework that
can take full advantage of multi-modality, where each modality provides
regularized self-supervisory signals to other modalities, we propose two
complementary modules within and across the modalities. First, Intra-modal
Pseudolabel Generation (Intra-PG) is introduced to obtain reliable pseudo
labels within each modality by aggregating information from two models that are
both pre-trained on source data but updated with target data at different
paces. Second, Inter-modal Pseudo-label Refinement (Inter-PR) adaptively
selects more reliable pseudo labels from different modalities based on a
proposed consistency scheme. Experiments demonstrate that our regularized
pseudo labels produce stable self-learning signals in numerous multi-modal
test-time adaptation scenarios for 3D semantic segmentation. Visit our project
website at https://www.nec-labs.com/~mas/MM-TTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimized latent-code selection for explainable conditional text-to-image GANs. (arXiv:2204.12678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12678">
<div class="article-summary-box-inner">
<span><p>The task of text-to-image generation has achieved remarkable progress due to
the advances in the conditional generative adversarial networks (GANs).
However, existing conditional text-to-image GANs approaches mostly concentrate
on improving both image quality and semantic relevance but ignore the
explainability of the model which plays a vital role in real-world
applications. In this paper, we present a variety of techniques to take a deep
look into the latent space and semantic space of the conditional text-to-image
GANs model. We introduce pairwise linear interpolation of latent codes and
`linguistic' linear interpolation to study what the model has learned within
the latent space and `linguistic' embeddings. Subsequently, we extend linear
interpolation to triangular interpolation conditioned on three corners to
further analyze the model. After that, we build a Good/Bad data set containing
unsuccessfully and successfully synthetic samples and corresponding latent
codes for the image-quality research. Based on this data set, we propose a
framework for finding good latent codes by utilizing a linear SVM. Experimental
results on the recent DiverGAN generator trained on two benchmark data sets
qualitatively prove the effectiveness of our presented techniques, with a
better than 94\% accuracy in predicting ${Good}$/${Bad}$ classes for latent
vectors. The Good/Bad data set is publicly available at
https://zenodo.org/record/5850224#.YeGMwP7MKUk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Transferability of Adversarial Examples with Restructure Embedded Patches. (arXiv:2204.12680v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12680">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have demonstrated impressive performance in
various computer vision tasks. However, the adversarial examples generated by
ViTs are challenging to transfer to other networks with different structures.
Recent attack methods do not consider the specificity of ViTs architecture and
self-attention mechanism, which leads to poor transferability of the generated
adversarial samples by ViTs. We attack the unique self-attention mechanism in
ViTs by restructuring the embedded patches of the input. The restructured
embedded patches enable the self-attention mechanism to obtain more diverse
patches connections and help ViTs keep regions of interest on the object.
Therefore, we propose an attack method against the unique self-attention
mechanism in ViTs, called Self-Attention Patches Restructure (SAPR). Our method
is simple to implement yet efficient and applicable to any self-attention based
network and gradient transferability-based attack methods. We evaluate attack
transferability on black-box models with different structures. The result show
that our method generates adversarial examples on white-box ViTs with higher
transferability and higher image quality. Our research advances the development
of black-box transfer attacks on ViTs and demonstrates the feasibility of using
white-box ViTs to attack other black-box models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Density-preserving Deep Point Cloud Compression. (arXiv:2204.12684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12684">
<div class="article-summary-box-inner">
<span><p>Local density of point clouds is crucial for representing local details, but
has been overlooked by existing point cloud compression methods. To address
this, we propose a novel deep point cloud compression method that preserves
local density information. Our method works in an auto-encoder fashion: the
encoder downsamples the points and learns point-wise features, while the
decoder upsamples the points using these features. Specifically, we propose to
encode local geometry and density with three embeddings: density embedding,
local position embedding and ancestor embedding. During the decoding, we
explicitly predict the upsampling factor for each point, and the directions and
scales of the upsampled points. To mitigate the clustered points issue in
existing methods, we design a novel sub-point convolution layer, and an
upsampling block with adaptive scale. Furthermore, our method can also compress
point-wise attributes, such as normal. Extensive qualitative and quantitative
results on SemanticKITTI and ShapeNet demonstrate that our method achieves the
state-of-the-art rate-distortion trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Face Anti-Spoofing with Dual Probabilistic Modeling. (arXiv:2204.12685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12685">
<div class="article-summary-box-inner">
<span><p>The field of face anti-spoofing (FAS) has witnessed great progress with the
surge of deep learning. Due to its data-driven nature, existing FAS methods are
sensitive to the noise in the dataset, which will hurdle the learning process.
However, very few works consider noise modeling in FAS. In this work, we
attempt to fill this gap by automatically addressing the noise problem from
both label and data perspectives in a probabilistic manner. Specifically, we
propose a unified framework called Dual Probabilistic Modeling (DPM), with two
dedicated modules, DPM-LQ (Label Quality aware learning) and DPM-DQ (Data
Quality aware learning). Both modules are designed based on the assumption that
data and label should form coherent probabilistic distributions. DPM-LQ is able
to produce robust feature representations without overfitting to the
distribution of noisy semantic labels. DPM-DQ can eliminate data noise from
`False Reject' and `False Accept' during inference by correcting the prediction
confidence of noisy data based on its quality distribution. Both modules can be
incorporated into existing deep networks seamlessly and efficiently.
Furthermore, we propose the generalized DPM to address the noise problem in
practical usage without the need of semantic annotations. Extensive experiments
demonstrate that this probabilistic modeling can 1) significantly improve the
accuracy, and 2) make the model robust to the noise in real-world datasets.
Without bells and whistles, our proposed DPM achieves state-of-the-art
performance on multiple standard FAS benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grasping the Arrow of Time from the Singularity: Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN. (arXiv:2204.12696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12696">
<div class="article-summary-box-inner">
<span><p>The disentanglement of StyleGAN latent space has paved the way for realistic
and controllable image editing, but does StyleGAN know anything about temporal
motion, as it was only trained on static images? To study the motion features
in the latent space of StyleGAN, in this paper, we hypothesize and demonstrate
that a series of meaningful, natural, and versatile small, local movements
(referred to as "micromotion", such as expression, head movement, and aging
effect) can be represented in low-rank spaces extracted from the latent space
of a conventionally pre-trained StyleGAN-v2 model for face generation, with the
guidance of proper "anchors" in the form of either short text or video clips.
Starting from one target face image, with the editing direction decoded from
the low-rank space, its micromotion features can be represented as simple as an
affine transformation over its latent feature. Perhaps more surprisingly, such
micromotion subspace, even learned from just single target face, can be
painlessly transferred to other unseen face images, even those from vastly
different domains (such as oil painting, cartoon, and sculpture faces). It
demonstrates that the local feature geometry corresponding to one type of
micromotion is aligned across different face subjects, and hence that
StyleGAN-v2 is indeed "secretly" aware of the subject-disentangled feature
variations caused by that micromotion. We present various successful examples
of applying our low-dimensional micromotion subspace technique to directly and
effortlessly manipulate faces, showing high robustness, low computational
overhead, and impressive domain transferability. Our codes are available at
https://github.com/wuqiuche/micromotion-StyleGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping suburban bicycle lanes using street scene images and deep learning. (arXiv:2204.12701v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12701">
<div class="article-summary-box-inner">
<span><p>On-road bicycle lanes improve safety for cyclists, and encourage
participation in cycling for active transport and recreation. With many local
authorities responsible for portions of the infrastructure, official maps and
datasets of bicycle lanes may be out-of-date and incomplete. Even
"crowdsourced" databases may have significant gaps, especially outside popular
metropolitan areas. This thesis presents a method to create a map of bicycle
lanes in a survey area by taking sample street scene images from each road, and
then applying a deep learning model that has been trained to recognise bicycle
lane symbols. The list of coordinates where bicycle lane markings are detected
is then correlated to geospatial data about the road network to record bicycle
lane routes. The method was applied to successfully build a map for a survey
area in the outer suburbs of Melbourne. It was able to identify bicycle lanes
not previously recorded in the official state government dataset,
OpenStreetMap, or the "biking" layer of Google Maps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset for Robust and Accurate Leading Vehicle Velocity Recognition. (arXiv:2204.12717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12717">
<div class="article-summary-box-inner">
<span><p>Recognition of the surrounding environment using a camera is an important
technology in Advanced Driver-Assistance Systems and Autonomous Driving, and
recognition technology is often solved by machine learning approaches such as
deep learning in recent years. Machine learning requires datasets for learning
and evaluation. To develop robust recognition technology in the real world, in
addition to normal driving environment, data in environments that are difficult
for cameras such as rainy weather or nighttime are essential. We have
constructed a dataset that one can benchmark the technology, targeting the
velocity recognition of the leading vehicle. This task is an important one for
the Advanced Driver-Assistance Systems and Autonomous Driving. The dataset is
available at https://signate.jp/competitions/657
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRE-NAS: Predictor-assisted Evolutionary Neural Architecture Search. (arXiv:2204.12726v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12726">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) aims to automate architecture engineering in
neural networks. This often requires a high computational overhead to evaluate
a number of candidate networks from the set of all possible networks in the
search space during the search. Prediction of the networks' performance can
alleviate this high computational overhead by mitigating the need for
evaluating every candidate network. Developing such a predictor typically
requires a large number of evaluated architectures which may be difficult to
obtain. We address this challenge by proposing a novel evolutionary-based NAS
strategy, Predictor-assisted E-NAS (PRE-NAS), which can perform well even with
an extremely small number of evaluated architectures. PRE-NAS leverages new
evolutionary search strategies and integrates high-fidelity weight inheritance
over generations. Unlike one-shot strategies, which may suffer from bias in the
evaluation due to weight sharing, offspring candidates in PRE-NAS are
topologically homogeneous, which circumvents bias and leads to more accurate
predictions. Extensive experiments on NAS-Bench-201 and DARTS search spaces
show that PRE-NAS can outperform state-of-the-art NAS methods. With only a
single GPU searching for 0.6 days, competitive architecture can be found by
PRE-NAS which achieves 2.40% and 24% test error rates on CIFAR-10 and ImageNet
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Centered Prior-Guided and Task-Dependent Multi-Task Representation Learning for Action Recognition Pre-Training. (arXiv:2204.12729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12729">
<div class="article-summary-box-inner">
<span><p>Recently, much progress has been made for self-supervised action recognition.
Most existing approaches emphasize the contrastive relations among videos,
including appearance and motion consistency. However, two main issues remain
for existing pre-training methods: 1) the learned representation is neutral and
not informative for a specific task; 2) multi-task learning-based pre-training
sometimes leads to sub-optimal solutions due to inconsistent domains of
different tasks. To address the above issues, we propose a novel action
recognition pre-training framework, which exploits human-centered prior
knowledge that generates more informative representation, and avoids the
conflict between multiple tasks by using task-dependent representations.
Specifically, we distill knowledge from a human parsing model to enrich the
semantic capability of representation. In addition, we combine knowledge
distillation with contrastive learning to constitute a task-dependent
multi-task framework. We achieve state-of-the-art performance on two popular
benchmarks for action recognition task, i.e., UCF101 and HMDB51, verifying the
effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Head Convolutional Neural Network With Multi-path Attention improves Image Denoising. (arXiv:2204.12736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12736">
<div class="article-summary-box-inner">
<span><p>Recently, convolutional neural networks (CNNs) and attention mechanisms have
been widely used in image denoising and achieved satisfactory performance.
However, the previous works mostly use a single head to receive the noisy
image, limiting the richness of extracted features. Therefore, a novel CNN with
multiple heads (MH) named MHCNN is proposed in this paper, whose heads will
receive the input images rotated by different rotation angles. MH makes MHCNN
simultaneously utilize features of rotated images to remove noise. We also
present a novel multi-path attention mechanism (MPA) to integrate these
features effectively. Unlike previous attention mechanisms that handle
pixel-level, channel-level, and patch-level features, MPA focuses on features
at the image level. Experiments show MHCNN surpasses other state-of-the-art CNN
models on additive white Gaussian noise (AWGN) denoising and real-world image
denoising. Its peak signal-to-noise ratio (PSNR) results are higher than other
networks, such as DnCNN, BRDNet, RIDNet, PAN-Net, and CSANN. It is also
demonstrated that the proposed MH with MPA mechanism can be used as a pluggable
component.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Text Erasing with Controllable Image Synthesis. (arXiv:2204.12743v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12743">
<div class="article-summary-box-inner">
<span><p>Recent efforts on scene text erasing have shown promising results. However,
existing methods require rich yet costly label annotations to obtain robust
models, which limits the use for practical applications. To this end, we study
an unsupervised scenario by proposing a novel Self-supervised Text Erasing
(STE) framework that jointly learns to synthesize training images with erasure
ground-truth and accurately erase texts in the real world. We first design a
style-aware image synthesis function to generate synthetic images with diverse
styled texts based on two synthetic mechanisms. To bridge the text style gap
between the synthetic and real-world data, a policy network is constructed to
control the synthetic mechanisms by picking style parameters with the guidance
of two specifically designed rewards. The synthetic training images with
erasure ground-truth are then fed to train a coarse-to-fine erasing network. To
produce better erasing outputs, a triplet erasure loss is designed to enforce
the refinement stage to recover background textures. Moreover, we provide a new
dataset (called PosterErase), which contains 60K high-resolution posters with
texts and is more challenging for the text erasing task. The proposed method
has been extensively evaluated with both PosterErase and the widely-used
SCUT-Enstext dataset. Notably, on PosterErase, our unsupervised method achieves
5.07 in terms of FID, with a relative performance of 20.9% over existing
supervised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Driving Car Steering Angle Prediction: Let Transformer Be a Car Again. (arXiv:2204.12748v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12748">
<div class="article-summary-box-inner">
<span><p>Self-driving vehicles are expected to be a massive economic influence over
the coming decades. Udacity https://www.udacity.com/ has been working on a
completely open-source self driving car. Thus, it regularly organizes various
competitions, one of which was dedicated to steering angle prediction task. In
this work, we perform an extensive study on this particular task by exploring
the Udacity Self-driving Car Challenge 2. We provide insights on the previous
teams' solutions. Moreover, we propose our new architecture that is inspired by
some of the teams. We report our performance and compare it with multiple
baseline architectures as well as other teams' solutions. We make our work
available on GitHub and hope it is useful for the Udacity community and brings
insights for future works https://github.com/chingisooinar/AI_self-driving-car
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion. (arXiv:2204.12756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12756">
<div class="article-summary-box-inner">
<span><p>Talking head generation is to synthesize a lip-synchronized talking head
video by inputting an arbitrary face image and corresponding audio clips.
Existing methods ignore not only the interaction and relationship of
cross-modal information, but also the local driving information of the mouth
muscles. In this study, we propose a novel generative framework that contains a
dilated non-causal temporal convolutional self-attention network as a
multimodal fusion module to promote the relationship learning of cross-modal
features. In addition, our proposed method uses both audio- and speech-related
facial action units (AUs) as driving information. Speech-related AU information
can guide mouth movements more accurately. Because speech is highly correlated
with speech-related AUs, we propose an audio-to-AU module to predict
speech-related AU information. We utilize pre-trained AU classifier to ensure
that the generated images contain correct AU information. We verify the
effectiveness of the proposed model on the GRID and TCD-TIMIT datasets. An
ablation study is also conducted to verify the contribution of each component.
The results of quantitative and qualitative experiments demonstrate that our
method outperforms existing methods in terms of both image quality and lip-sync
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching. (arXiv:2204.12805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12805">
<div class="article-summary-box-inner">
<span><p>We present a scalable combinatorial algorithm for globally optimizing over
the space of geometrically consistent mappings between 3D shapes. We use the
mathematically elegant formalism proposed by Windheuser et al. (ICCV 2011)
where 3D shape matching was formulated as an integer linear program over the
space of orientation-preserving diffeomorphisms. Until now, the resulting
formulation had limited practical applicability due to its complicated
constraint structure and its large size. We propose a novel primal heuristic
coupled with a Lagrange dual problem that is several orders of magnitudes
faster compared to previous solvers. This allows us to handle shapes with
substantially more triangles than previously solvable. We demonstrate
compelling results on diverse datasets, and, even showcase that we can address
the challenging setting of matching two partial shapes without availability of
complete shapes. Our code is publicly available at
<a href="http://github.com/paul0noah/sm-comb">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The MeVer DeepFake Detection Service: Lessons Learnt from Developing and Deploying in the Wild. (arXiv:2204.12816v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12816">
<div class="article-summary-box-inner">
<span><p>Enabled by recent improvements in generation methodologies, DeepFakes have
become mainstream due to their increasingly better visual quality, the increase
in easy-to-use generation tools and the rapid dissemination through social
media. This fact poses a severe threat to our societies with the potential to
erode social cohesion and influence our democracies. To mitigate the threat,
numerous DeepFake detection schemes have been introduced in the literature but
very few provide a web service that can be used in the wild. In this paper, we
introduce the MeVer DeepFake detection service, a web service detecting deep
learning manipulations in images and video. We present the design and
implementation of the proposed processing pipeline that involves a model
ensemble scheme, and we endow the service with a model card for transparency.
Experimental results show that our service performs robustly on the three
benchmark datasets while being vulnerable to Adversarial Attacks. Finally, we
outline our experience and lessons learned when deploying a research system
into production in the hopes that it will be useful to other academic and
industry teams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CATrans: Context and Affinity Transformer for Few-Shot Segmentation. (arXiv:2204.12817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12817">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation (FSS) aims to segment novel categories given scarce
annotated support images. The crux of FSS is how to aggregate dense
correlations between support and query images for query segmentation while
being robust to the large variations in appearance and context. To this end,
previous Transformer-based methods explore global consensus either on context
similarity or affinity map between support-query pairs. In this work, we
effectively integrate the context and affinity information via the proposed
novel Context and Affinity Transformer (CATrans) in a hierarchical
architecture. Specifically, the Relation-guided Context Transformer (RCT)
propagates context information from support to query images conditioned on more
informative support features. Based on the observation that a huge feature
distinction between support and query pairs brings barriers for context
knowledge transfer, the Relation-guided Affinity Transformer (RAT) measures
attention-aware affinity as auxiliary information for FSS, in which the
self-affinity is responsible for more reliable cross-affinity. We conduct
experiments to demonstrate the effectiveness of the proposed model,
outperforming the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conformer and Blind Noisy Students for Improved Image Quality Assessment. (arXiv:2204.12819v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12819">
<div class="article-summary-box-inner">
<span><p>Generative models for image restoration, enhancement, and generation have
significantly improved the quality of the generated images. Surprisingly, these
models produce more pleasant images to the human eye than other methods, yet,
they may get a lower perceptual quality score using traditional perceptual
quality metrics such as PSNR or SSIM. Therefore, it is necessary to develop a
quantitative metric to reflect the performance of new algorithms, which should
be well-aligned with the person's mean opinion score (MOS). Learning-based
approaches for perceptual image quality assessment (IQA) usually require both
the distorted and reference image for measuring the perceptual quality
accurately. However, commonly only the distorted or generated image is
available. In this work, we explore the performance of transformer-based
full-reference IQA models. We also propose a method for IQA based on
semi-supervised knowledge distillation from full-reference teacher models into
blind student models using noisy pseudo-labeled data. Our approaches achieved
competitive results on the NTIRE 2022 Perceptual Image Quality Assessment
Challenge: our full-reference model was ranked 4th, and our blind noisy student
was ranked 3rd among 70 participants, each in their respective track.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Power Bundle Adjustment for Large-Scale 3D Reconstruction. (arXiv:2204.12834v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12834">
<div class="article-summary-box-inner">
<span><p>We present the design and the implementation of a new expansion type
algorithm to solve large-scale bundle adjustment problems. Our approach --
called Power Bundle Adjustment -- is based on the power series expansion of the
inverse Schur complement. This initiates a new family of solvers that we call
inverse expansion methods. We show with the real-world BAL dataset that the
proposed solver challenges the traditional direct and iterative methods. The
solution of the normal equation is significantly accelerated, even for reaching
a very high accuracy. Last but not least, our solver can also complement a
recently presented distributed bundle adjustment framework. We demonstrate that
employing the proposed Power Bundle Adjustment as a sub-problem solver greatly
improves speed and accuracy of the distributed optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BBBD: Bounding Box Based Detector for Occlusion Detection and Order Recovery. (arXiv:2204.12841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12841">
<div class="article-summary-box-inner">
<span><p>Occlusion handling is one of the challenges of object detection and
segmentation, and scene understanding. Because objects appear differently when
they are occluded in varying degree, angle, and locations. Therefore,
determining the existence of occlusion between objects and their order in a
scene is a fundamental requirement for semantic understanding. Existing works
mostly use deep learning based models to retrieve the order of the instances in
an image or for occlusion detection. This requires labelled occluded data and
it is time consuming. In this paper, we propose a simpler and faster method
that can perform both operations without any training and only requires the
modal segmentation masks. For occlusion detection, instead of scanning the two
objects entirely, we only focus on the intersected area between their bounding
boxes. Similarly, we use the segmentation mask inside the same area to recover
the depth-ordering. When tested on COCOA dataset, our method achieves +8% and
+5% more accuracy than the baselines in order recovery and occlusion detection
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forecasting Urban Development from Satellite Images. (arXiv:2204.12875v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12875">
<div class="article-summary-box-inner">
<span><p>Forecasting where and when new buildings will emerge is a rather unexplored
niche topic, but relevant in disciplines such as urban planning, agriculture,
resource management, and even autonomous flight. In this work, we present a
method that accomplishes this task using satellite images and a custom neural
network training procedure. In stage A, a DeepLapv3+ backbone is pretrained
through a Siamese network architecture aimed at solving a building change
detection task. In stage B, we transfer the backbone into a change forecasting
model that relies solely on the initial input image. We also transfer the
backbone into a forecasting model predicting the correct time range of the
future change. For our experiments, we use the SpaceNet7 dataset with 960 km2
spatial extension and 24 monthly frames. We found that our training strategy
consistently outperforms the traditional pretraining on the ImageNet dataset.
Especially with longer forecasting ranges of 24 months, we observe F1 scores of
24% instead of 16%. Furthermore, we found that our method performed well in
forecasting the times of future building constructions. Hereby, the strengths
of our custom pretraining become especially apparent when we increase the
difficulty of the task by predicting finer time windows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-rank Meets Sparseness: An Integrated Spatial-Spectral Total Variation Approach to Hyperspectral Denoising. (arXiv:2204.12879v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12879">
<div class="article-summary-box-inner">
<span><p>Spatial-Spectral Total Variation (SSTV) can quantify local smoothness of
image structures, so it is widely used in hyperspectral image (HSI) processing
tasks. Essentially, SSTV assumes a sparse structure of gradient maps calculated
along the spatial and spectral directions. In fact, these gradient tensors are
not only sparse, but also (approximately) low-rank under FFT, which we have
verified by numerical tests and theoretical analysis. Based on this fact, we
propose a novel TV regularization to simultaneously characterize the sparsity
and low-rank priors of the gradient map (LRSTV). The new regularization not
only imposes sparsity on the gradient map itself, but also penalize the rank on
the gradient map after Fourier transform along the spectral dimension. It
naturally encodes the sparsity and lowrank priors of the gradient map, and thus
is expected to reflect the inherent structure of the original image more
faithfully. Further, we use LRSTV to replace conventional SSTV and embed it in
the HSI processing model to improve its performance. Experimental results on
multiple public data-sets with heavy mixed noise show that the proposed model
can get 1.5dB improvement of PSNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gleo-Det: Deep Convolution Feature-Guided Detector with Local Entropy Optimization for Salient Points. (arXiv:2204.12884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12884">
<div class="article-summary-box-inner">
<span><p>Feature detection is an important procedure for image matching, where
unsupervised feature detection methods are the detection approaches that have
been mostly studied recently, including the ones that are based on
repeatability requirement to define loss functions, and the ones that attempt
to use descriptor matching to drive the optimization of the pipelines. For the
former type, mean square error (MSE) is usually used which cannot provide
strong constraint for training and can make the model easy to be stuck into the
collapsed solution. For the later one, due to the down sampling operation and
the expansion of receptive fields, the details can be lost for local
descriptors can be lost, making the constraint not fine enough. Considering the
issues above, we propose to combine both ideas, which including three aspects.
1) We propose to achieve fine constraint based on the requirement of
repeatability while coarse constraint with guidance of deep convolution
features. 2) To address the issue that optimization with MSE is limited,
entropy-based cost function is utilized, both soft cross-entropy and
self-information. 3) With the guidance of convolution features, we define the
cost function from both positive and negative sides. Finally, we study the
effect of each modification proposed and experiments demonstrate that our
method achieves competitive results over the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Trajectory Helps Person Retrieval in a Camera Network. (arXiv:2204.12900v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12900">
<div class="article-summary-box-inner">
<span><p>We are concerned about retrieving a query person from the videos taken by a
non-overlapping camera network. Existing methods often rely on pure visual
matching or consider temporal constraint, but ignore the spatial information of
the camera network. To address this problem, we propose a framework of person
retrieval based on cross-camera trajectory generation which integrates both
temporal and spatial information. To obtain the pedestrian trajectories, we
propose a new cross-camera spatio-temporal model that integrates the walking
habits of pedestrians and the path layout between cameras, forming a joint
probability distribution. Such a spatio-temporal model among a camera network
can be specified using sparsely sampled pedestrian data. Based on the
spatio-temporal model, the cross-camera trajectories of a specific pedestrian
can be extracted by the conditional random field model, and further optimized
by the restricted nonnegative matrix factorization. Finally, a trajectory
re-ranking technology is proposed to improve the person retrieval results. To
verify the effectiveness of our approach, we build the first dataset of
cross-camera pedestrian trajectories over an actual monitoring scenario, namely
the Person Trajectory Dataset. Extensive experiments have verified the
effectiveness and robustness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Epicardial Adipose Tissue Segmentation from CT Images with A Semi-3D Neural Network. (arXiv:2204.12904v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12904">
<div class="article-summary-box-inner">
<span><p>Epicardial adipose tissue is a type of adipose tissue located between the
heart wall and a protective layer around the heart called the pericardium. The
volume and thickness of epicardial adipose tissue are linked to various
cardiovascular diseases. It is shown to be an independent cardiovascular
disease risk factor. Fully automatic and reliable measurements of epicardial
adipose tissue from CT scans could provide better disease risk assessment and
enable the processing of large CT image data sets for a systemic epicardial
adipose tissue study. This paper proposes a method for fully automatic semantic
segmentation of epicardial adipose tissue from CT images using a deep neural
network. The proposed network uses a U-Net-based architecture with slice depth
information embedded in the input image to segment a pericardium region of
interest, which is used to obtain an epicardial adipose tissue segmentation.
Image augmentation is used to increase model robustness. Cross-validation of
the proposed method yields a Dice score of 0.86 on the CT scans of 20 patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Iterative Labeling Method for Annotating Fisheries Imagery. (arXiv:2204.12934v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12934">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a methodology for fisheries-related data that
allows us to converge on a labeled image dataset by iterating over the dataset
with multiple training and production loops that can exploit crowdsourcing
interfaces. We present our algorithm and its results on two separate sets of
image data collected using the Seabed autonomous underwater vehicle. The first
dataset comprises of 2,026 completely unlabeled images, while the second
consists of 21,968 images that were point annotated by experts. Our results
indicate that training with a small subset and iterating on that to build a
larger set of labeled data allows us to converge to a fully annotated dataset
with a small number of iterations. Even in the case of a dataset labeled by
experts, a single iteration of the methodology improves the labels by
discovering additional complicated examples of labels associated with fish that
overlap, are very small, or obscured by the contrast limitations associated
with underwater imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of Unbiased Visual Representations. (arXiv:2204.12941v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12941">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are known for their inability to learn robust
representations when biases exist in the dataset. This results in a poor
generalization to unbiased datasets, as the predictions strongly rely on
peripheral and confounding factors, which are erroneously learned by the
network. Many existing works deal with this issue by either employing an
explicit supervision on the bias attributes, or assuming prior knowledge about
the bias. In this work we study this problem in a more difficult scenario, in
which no explicit annotation about the bias is available, and without any prior
knowledge about its nature. We propose a fully unsupervised debiasing
framework, consisting of three steps: first, we exploit the natural preference
for learning malignant biases, obtaining a bias-capturing model; then, we
perform a pseudo-labelling step to obtain bias labels; finally we employ
state-of-the-art supervised debiasing techniques to obtain an unbiased model.
We also propose a theoretical framework to assess the biasness of a model, and
provide a detailed analysis on how biases affect the training of neural
networks. We perform experiments on synthetic and real-world datasets, showing
that our method achieves state-of-the-art performance in a variety of settings,
sometimes even higher than fully supervised debiasing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAPLE-Edge: A Runtime Latency Predictor for Edge Devices. (arXiv:2204.12950v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12950">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) has enabled automatic discovery of more
efficient neural network architectures, especially for mobile and embedded
vision applications. Although recent research has proposed ways of quickly
estimating latency on unseen hardware devices with just a few samples, little
focus has been given to the challenges of estimating latency on runtimes using
optimized graphs, such as TensorRT and specifically for edge devices. In this
work, we propose MAPLE-Edge, an edge device-oriented extension of MAPLE, the
state-of-the-art latency predictor for general purpose hardware, where we train
a regression network on architecture-latency pairs in conjunction with a
hardware-runtime descriptor to effectively estimate latency on a diverse pool
of edge devices. Compared to MAPLE, MAPLE-Edge can describe the runtime and
target device platform using a much smaller set of CPU performance counters
that are widely available on all Linux kernels, while still achieving up to
+49.6% accuracy gains against previous state-of-the-art baseline methods on
optimized edge device runtimes, using just 10 measurements from an unseen
target device. We also demonstrate that unlike MAPLE which performs best when
trained on a pool of devices sharing a common runtime, MAPLE-Edge can
effectively generalize across runtimes by applying a trick of normalizing
performance counters by the operator latency, in the measured hardware-runtime
descriptor. Lastly, we show that for runtimes exhibiting lower than desired
accuracy, performance can be boosted by collecting additional samples from the
target device, with an extra 90 samples translating to gains of nearly +40%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards assessing agricultural land suitability with causal machine learning. (arXiv:2204.12956v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12956">
<div class="article-summary-box-inner">
<span><p>Understanding the suitability of agricultural land for applying specific
management practices is of great importance for sustainable and resilient
agriculture against climate change. Recent developments in the field of causal
machine learning enable the estimation of intervention impacts on an outcome of
interest, for samples described by a set of observed characteristics. We
introduce an extensible data-driven framework that leverages earth observations
and frames agricultural land suitability as a geospatial impact assessment
problem, where the estimated effects of agricultural practices on
agroecosystems serve as a land suitability score and guide decision making. We
formulate this as a causal machine learning task and discuss how this approach
can be used for agricultural planning in a changing climate. Specifically, we
extract the agricultural management practices of "crop rotation" and "landscape
crop diversity" from crop type maps, account for climate and land use data, and
use double machine learning to estimate their heterogeneous effect on Net
Primary Productivity (NPP), within the Flanders region of Belgium from 2010 to
2020. We find that the effect of crop rotation was insignificant, while
landscape crop diversity had a small negative effect on NPP. Finally, we
observe considerable effect heterogeneity in space for both practices and
analyze it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CapOnImage: Context-driven Dense-Captioning on Image. (arXiv:2204.12974v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12974">
<div class="article-summary-box-inner">
<span><p>Existing image captioning systems are dedicated to generating narrative
captions for images, which are spatially detached from the image in
presentation. However, texts can also be used as decorations on the image to
highlight the key points and increase the attractiveness of images. In this
work, we introduce a new task called captioning on image (CapOnImage), which
aims to generate dense captions at different locations of the image based on
contextual information. To fully exploit the surrounding visual context to
generate the most suitable caption for each location, we propose a multi-modal
pre-training model with multi-level pre-training tasks that progressively learn
the correspondence between texts and image locations from easy to difficult.
Since the model may generate redundant captions for nearby locations, we
further enhance the location embedding with neighbor locations as context. For
this new task, we also introduce a large-scale benchmark called CapOnImage2M,
which contains 2.1 million product images, each with an average of 4.8
spatially localized captions. Compared with other image captioning model
variants, our model achieves the best results in both captioning accuracy and
diversity aspects. We will make code and datasets public to facilitate future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers. (arXiv:2204.12997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12997">
<div class="article-summary-box-inner">
<span><p>Transformers are successfully applied to computer vision due to their
powerful modeling capacity with self-attention. However, the excellent
performance of transformers heavily depends on enormous training images. Thus,
a data-efficient transformer solution is urgently needed. In this work, we
propose an early knowledge distillation framework, which is termed as DearKD,
to improve the data efficiency required by transformers. Our DearKD is a
two-stage framework that first distills the inductive biases from the early
intermediate layers of a CNN and then gives the transformer full play by
training without distillation. Further, our DearKD can be readily applied to
the extreme data-free case where no real images are available. In this case, we
propose a boundary-preserving intra-divergence loss based on DeepInversion to
further close the performance gap against the full-data counterpart. Extensive
experiments on ImageNet, partial ImageNet, data-free setting and other
downstream tasks prove the superiority of DearKD over its baselines and
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relevance-based Margin for Contrastively-trained Video Retrieval Models. (arXiv:2204.13001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13001">
<div class="article-summary-box-inner">
<span><p>Video retrieval using natural language queries has attracted increasing
interest due to its relevance in real-world applications, from intelligent
access in private media galleries to web-scale video search. Learning the
cross-similarity of video and text in a joint embedding space is the dominant
approach. To do so, a contrastive loss is usually employed because it organizes
the embedding space by putting similar items close and dissimilar items far.
This framework leads to competitive recall rates, as they solely focus on the
rank of the groundtruth items. Yet, assessing the quality of the ranking list
is of utmost importance when considering intelligent retrieval systems, since
multiple items may share similar semantics, hence a high relevance. Moreover,
the aforementioned framework uses a fixed margin to separate similar and
dissimilar items, treating all non-groundtruth items as equally irrelevant. In
this paper we propose to use a variable margin: we argue that varying the
margin used during training based on how much relevant an item is to a given
query, i.e. a relevance-based margin, easily improves the quality of the
ranking lists measured through nDCG and mAP. We demonstrate the advantages of
our technique using different models on EPIC-Kitchens-100 and YouCook2. We show
that even if we carefully tuned the fixed margin, our technique (which does not
have the margin as a hyper-parameter) would still achieve better performance.
Finally, extensive ablation studies and qualitative analysis support the
robustness of our approach. Code will be released at
\url{https://github.com/aranciokov/RelevanceMargin-ICMR22}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Against Person Hiding Adversarial Patch Attack with a Universal White Frame. (arXiv:2204.13004v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13004">
<div class="article-summary-box-inner">
<span><p>Object detection has attracted great attention in the computer vision area
and has emerged as an indispensable component in many vision systems. In the
era of deep learning, many high-performance object detection networks have been
proposed. Although these detection networks show high performance, they are
vulnerable to adversarial patch attacks. Changing the pixels in a restricted
region can easily fool the detection network in the physical world. In
particular, person-hiding attacks are emerging as a serious problem in many
safety-critical applications such as autonomous driving and surveillance
systems. Although it is necessary to defend against an adversarial patch
attack, very few efforts have been dedicated to defending against person-hiding
attacks. To tackle the problem, in this paper, we propose a novel defense
strategy that mitigates a person-hiding attack by optimizing defense patterns,
while previous methods optimize the model. In the proposed method, a
frame-shaped pattern called a 'universal white frame' (UWF) is optimized and
placed on the outside of the image. To defend against adversarial patch
attacks, UWF should have three properties (i) suppressing the effect of the
adversarial patch, (ii) maintaining its original prediction, and (iii)
applicable regardless of images. To satisfy the aforementioned properties, we
propose a novel pattern optimization algorithm that can defend against the
adversarial patch. Through comprehensive experiments, we demonstrate that the
proposed method effectively defends against the adversarial patch attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ollivier-Ricci Curvature For Head Pose Estimation From a Single Image. (arXiv:2204.13006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13006">
<div class="article-summary-box-inner">
<span><p>Head pose estimation is a crucial challenge for many real-world applications,
such as attention and human behavior analysis. This paper aims to estimate head
pose from a single image by applying notions of network curvature. In the real
world, many complex networks have groups of nodes that are well connected to
each other with significant functional roles. Similarly, the interactions of
facial landmarks can be represented as complex dynamic systems modeled by
weighted graphs. The functionalities of such systems are therefore
intrinsically linked to the topology and geometry of the underlying graph. In
this work, using the geometric notion of Ollivier-Ricci curvature (ORC) on
weighted graphs as input to the XGBoost regression model, we show that the
intrinsic geometric basis of ORC offers a natural approach to discovering
underlying common structure within a pool of poses. Experiments on the BIWI,
AFLW2000 and Pointing'04 datasets show that the ORC_XGB method performs well
compared to state-of-the-art methods, both landmark-based and image-only.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dropout Inference with Non-Uniform Weight Scaling. (arXiv:2204.13047v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13047">
<div class="article-summary-box-inner">
<span><p>Dropout as regularization has been used extensively to prevent overfitting
for training neural networks. During training, units and their connections are
randomly dropped, which could be considered as sampling many different
submodels from the original model. At test time, weight scaling and Monte Carlo
approximation are two widely applied approaches to approximate the outputs.
Both approaches work well practically when all submodels are low-bias complex
learners. However, in this work, we demonstrate scenarios where some submodels
behave closer to high-bias models and a non-uniform weight scaling is a better
approximation for inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution. (arXiv:2204.13062v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13062">
<div class="article-summary-box-inner">
<span><p>Estimating the pose and shape of hands and objects under interaction finds
numerous applications including augmented and virtual reality. Existing
approaches for hand and object reconstruction require explicitly defined
physical constraints and known objects, which limits its application domains.
Our algorithm is agnostic to object models, and it learns the physical rules
governing hand-object interaction. This requires automatically inferring the
shapes and physical interaction of hands and (potentially unknown) objects. We
seek to approach this challenging problem by proposing a collaborative learning
strategy where two-branches of deep networks are learning from each other.
Specifically, we transfer hand mesh information to the object branch and vice
versa for the hand branch. The resulting optimisation (training) problem can be
unstable, and we address this via two strategies: (i) attention-guided graph
convolution which helps identify and focus on mutual occlusion and (ii)
unsupervised associative loss which facilitates the transfer of information
between the branches. Experiments using four widely-used benchmarks show that
our framework achieves beyond state-of-the-art accuracy in 3D pose estimation,
as well as recovers dense 3D hand and object shapes. Each technical component
above contributes meaningfully in the ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Consistency on Visual Corruptions for Single-Source Domain Generalization. (arXiv:2204.13091v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13091">
<div class="article-summary-box-inner">
<span><p>Generalizing visual recognition models trained on a single distribution to
unseen input distributions (i.e. domains) requires making them robust to
superfluous correlations in the training set. In this work, we achieve this
goal by altering the training images to simulate new domains and imposing
consistent visual attention across the different views of the same sample. We
discover that the first objective can be simply and effectively met through
visual corruptions. Specifically, we alter the content of the training images
using the nineteen corruptions of the ImageNet-C benchmark and three additional
transformations based on Fourier transform. Since these corruptions preserve
object locations, we propose an attention consistency loss to ensure that class
activation maps across original and corrupted versions of the same training
sample are aligned. We name our model Attention Consistency on Visual
Corruptions (ACVC). We show that ACVC consistently achieves the state of the
art on three single-source domain generalization benchmarks, PACS, COCO, and
the large-scale DomainNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Magic Mirror: Clothing Reconstruction from a Single Image via a Causal Perspective. (arXiv:2204.13096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13096">
<div class="article-summary-box-inner">
<span><p>This research aims to study a self-supervised 3D clothing reconstruction
method, which recovers the geometry shape, and texture of human clothing from a
single 2D image. Compared with existing methods, we observe that three primary
challenges remain: (1) the conventional template-based methods are limited to
modeling non-rigid clothing objects, e.g., handbags and dresses, which are
common in fashion images; (2) 3D ground-truth meshes of clothing are usually
inaccessible due to annotation difficulties and time costs. (3) It remains
challenging to simultaneously optimize four reconstruction factors, i.e.,
camera viewpoint, shape, texture, and illumination. The inherent ambiguity
compromises the model training, such as the dilemma between a large shape with
a remote camera or a small shape with a close camera.
</p>
<p>In an attempt to address the above limitations, we propose a causality-aware
self-supervised learning method to adaptively reconstruct 3D non-rigid objects
from 2D images without 3D annotations. In particular, to solve the inherent
ambiguity among four implicit variables, i.e., camera position, shape, texture,
and illumination, we study existing works and introduce an explainable
structural causal map (SCM) to build our model. The proposed model structure
follows the spirit of the causal map, which explicitly considers the prior
template in the camera estimation and shape prediction. When optimization, the
causality intervention tool, i.e., two expectation-maximization loops, is
deeply embedded in our algorithm to (1) disentangle four encoders and (2) help
the prior template update. Extensive experiments on two 2D fashion benchmarks,
e.g., ATR, and Market-HQ, show that the proposed method could yield
high-fidelity 3D reconstruction. Furthermore, we also verify the scalability of
the proposed method on a fine-grained bird dataset, i.e., CUB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Head Swapping in the Wild. (arXiv:2204.13100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13100">
<div class="article-summary-box-inner">
<span><p>The head swapping task aims at flawlessly placing a source head onto a target
body, which is of great importance to various entertainment scenarios. While
face swapping has drawn much attention, the task of head swapping has rarely
been explored, particularly under the few-shot setting. It is inherently
challenging due to its unique needs in head modeling and background blending.
In this paper, we present the Head Swapper (HeSer), which achieves few-shot
head swapping in the wild through two delicately designed modules. Firstly, a
Head2Head Aligner is devised to holistically migrate pose and expression
information from the target to the source head by examining multi-scale
information. Secondly, to tackle the challenges of skin color variations and
head-background mismatches in the swapping procedure, a Head2Scene Blender is
introduced to simultaneously modify facial skin color and fill mismatched gaps
in the background around the head. Particularly, seamless blending is achieved
with the help of a Semantic-Guided Color Reference Creation procedure and a
Blending UNet. Extensive experiments demonstrate that the proposed method
produces superior head swapping results in a variety of scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning of Object Parts for Semantic Segmentation. (arXiv:2204.13101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13101">
<div class="article-summary-box-inner">
<span><p>Progress in self-supervised learning has brought strong general image
representation learning methods. Yet so far, it has mostly focused on
image-level learning. In turn, tasks such as unsupervised image segmentation
have not benefited from this trend as they require spatially-diverse
representations. However, learning dense representations is challenging, as in
the unsupervised context it is not clear how to guide the model to learn
representations that correspond to various potential object categories. In this
paper, we argue that self-supervised learning of object parts is a solution to
this issue. Object parts are generalizable: they are a priori independent of an
object definition, but can be grouped to form objects a posteriori. To this
end, we leverage the recently proposed Vision Transformer's capability of
attending to objects and combine it with a spatially dense clustering task for
fine-tuning the spatial tokens. Our method surpasses the state-of-the-art on
three semantic segmentation benchmarks by 17%-3%, showing that our
representations are versatile under various object definitions. Finally, we
extend this to fully unsupervised segmentation - which refrains completely from
using label information even at test-time - and demonstrate that a simple
method for automatically merging discovered object parts based on community
detection yields substantial gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Chiral Domain of a Camera Arrangement. (arXiv:2003.09265v4 [math.AG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.09265">
<div class="article-summary-box-inner">
<span><p>We introduce the chiral domain of an arrangement of cameras $\mathcal{A} =
\{A_1,\dots, A_m\}$ which is the subset of $\mathbb{P}^3$ visible in
$\mathcal{A}$. It generalizes the classical definition of chirality to include
all of $\mathbb{P}^3$ and offers a unifying framework for studying multiview
chirality. We give an algebraic description of the chiral domain which allows
us to define and describe a chiral version of Triggs' joint image. We then use
the chiral domain to re-derive and extend prior results on chirality due to
Hartley.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A closed-form solution to estimate uncertainty in non-rigid structure from motion. (arXiv:2005.04810v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04810">
<div class="article-summary-box-inner">
<span><p>Semi-Definite Programming (SDP) with low-rank prior has been widely applied
in Non-Rigid Structure from Motion (NRSfM). Based on a low-rank constraint, it
avoids the inherent ambiguity of basis number selection in conventional
base-shape or base-trajectory methods. Despite the efficiency in deformable
shape reconstruction, it remains unclear how to assess the uncertainty of the
recovered shape from the SDP process. In this paper, we present a statistical
inference on the element-wise uncertainty quantification of the estimated
deforming 3D shape points in the case of the exact low-rank SDP problem. A
closed-form uncertainty quantification method is proposed and tested. Moreover,
we extend the exact low-rank uncertainty quantification to the approximate
low-rank scenario with a numerical optimal rank selection method, which enables
solving practical application in SDP based NRSfM scenario. The proposed method
provides an independent module to the SDP method and only requires the
statistic information of the input 2D tracked points. Extensive experiments
prove that the output 3D points have identical normal distribution to the 2D
trackings, the proposed method and quantify the uncertainty accurately, and
supports that it has desirable effects on routinely SDP low-rank based NRSfM
solver.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic Representation Space. (arXiv:2008.00397v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.00397">
<div class="article-summary-box-inner">
<span><p>In this work, we formulate a visual dialog as an information flow in which
each piece of information is encoded with the joint visual-linguistic
representation of a single dialog round. Based on this formulation, we consider
the visual dialog task as a sequence problem consisting of ordered
visual-linguistic vectors. For featurization, we use a Dense Symmetric
Co-Attention network as a lightweight vison-language joint representation
generator to fuse multimodal features (i.e., image and text), yielding better
computation and data efficiencies. For inference, we propose two Sequential
Dialog Networks (SeqDialN): the first uses LSTM for information propagation
(IP) and the second uses a modified Transformer for multi-step reasoning (MR).
Our architecture separates the complexity of multimodal feature fusion from
that of inference, which allows simpler design of the inference engine. IP
based SeqDialN is our baseline with a simple 2-layer LSTM design that achieves
decent performance. MR based SeqDialN, on the other hand, recurrently refines
the semantic question/history representations through the self-attention stack
of Transformer and produces promising results on the visual dialog task. On
VisDial v1.0 test-std dataset, our best single generative SeqDialN achieves
62.54% NDCG and 48.63% MRR; our ensemble generative SeqDialN achieves 63.78%
NDCG and 49.98% MRR, which set a new state-of-the-art generative visual dialog
model. We fine-tune discriminative SeqDialN with dense annotations and boost
the performance up to 72.41% NDCG and 55.11% MRR. In this work, we discuss the
extensive experiments we have conducted to demonstrate the effectiveness of our
model components. We also provide visualization for the reasoning process from
the relevant conversation rounds and discuss our fine-tuning methods. Our code
is available at https://github.com/xiaoxiaoheimei/SeqDialN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation. (arXiv:2011.14672v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14672">
<div class="article-summary-box-inner">
<span><p>Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh
for the human body by estimating several parameters. However, learning the
abstract parameters is a highly non-linear process and suffers from image-model
misalignment, leading to mediocre model performance. In contrast, 3D keypoint
estimation methods combine deep CNN network with the volumetric representation
to achieve pixel-level localization accuracy but may predict unrealistic body
structure. In this paper, we address the above issues by bridging the gap
between body mesh estimation and 3D keypoint estimation. We propose a novel
hybrid inverse kinematics solution (HybrIK). HybrIK directly transforms
accurate 3D joints to relative body-part rotations for 3D body mesh
reconstruction, via the twist-and-swing decomposition. The swing rotation is
analytically solved with 3D joints, and the twist rotation is derived from the
visual cues through the neural network. We show that HybrIK preserves both the
accuracy of 3D pose and the realistic body structure of the parametric human
model, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than
the pure 3D keypoint estimation methods. Without bells and whistles, the
proposed method surpasses the state-of-the-art methods by a large margin on
various 3D human pose and shape benchmarks. As an illustrative example, HybrIK
outperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW
dataset. Our code is available at https://github.com/Jeff-sjtu/HybrIK.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperspectral Image Classification-Traditional to Deep Models: A Survey for Future Prospects. (arXiv:2101.06116v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06116">
<div class="article-summary-box-inner">
<span><p>Hyperspectral Imaging (HSI) has been extensively utilized in many real-life
applications because it benefits from the detailed spectral information
contained in each pixel. Notably, the complex characteristics i.e., the
nonlinear relation among the captured spectral information and the
corresponding object of HSI data make accurate classification challenging for
traditional methods. In the last few years, Deep Learning (DL) has been
substantiated as a powerful feature extractor that effectively addresses the
nonlinear problems that appeared in a number of computer vision tasks. This
prompts the deployment of DL for HSI classification (HSIC) which revealed good
performance. This survey enlists a systematic overview of DL for HSIC and
compared state-of-the-art strategies on the said topic. Primarily, we will
encapsulate the main challenges of traditional machine learning for HSIC and
then we will acquaint the superiority of DL to address these problems. This
survey breakdown the state-of-the-art DL frameworks into spectral features,
spatial features, and together spatial-spectral features to systematically
analyze the achievements (future research directions as well) of these
frameworks for HSIC. Moreover, we will consider the fact that DL requires a
large number of labeled training examples whereas acquiring such a number for
HSIC is challenging in terms of time and cost. Therefore, this survey discusses
some strategies to improve the generalization performance of DL strategies
which can provide some future guidelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAI Estimation of Cucumber Crop Based on Improved Fully Convolutional Network. (arXiv:2104.07955v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07955">
<div class="article-summary-box-inner">
<span><p>LAI (Leaf Area Index) is of great importance for crop yield estimation in
agronomy. It is directly related to plant growth status, net assimilation rate,
plant photosynthesis, and carbon dioxide in the environment. How to measure LAI
accurately and efficiently is the key to the crop yield estimation problem.
Manual measurement consumes a lot of human resources and material resources.
Remote sensing technology is not suitable for near-Earth LAI measurement.
Besides, methods based on traditional digital image processing are greatly
affected by environmental noise and image exposure. Nowadays, deep learning is
widely used in many fields. The improved FCN (Fully Convolutional Network) is
proposed in our study for LAI measure task. Eighty-two cucumber images
collected from our greenhouse are labeled to fine-tuning the pre-trained model.
The result shows that the improved FCN model performs well on our dataset. Our
method's mean IoU can reach 0.908, which is 11% better than conventional
methods and 4.7% better than the basic FCN model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Anchors by the Detector Itself. (arXiv:2105.14086v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14086">
<div class="article-summary-box-inner">
<span><p>Usually, it is difficult to determine the scale and aspect ratio of anchors
for anchor-based object detection methods. Current state-of-the-art object
detectors either determine anchor parameters according to objects' shape and
scale in a dataset, or avoid this problem by utilizing anchor-free methods,
however, the former scheme is dataset-specific and the latter methods could not
get better performance than the former ones. In this paper, we propose a novel
anchor augmentation method named AADI, which means Augmenting Anchors by the
Detector Itself. AADI is not an anchor-free method, instead, it can convert the
scale and aspect ratio of anchors from a continuous space to a discrete space,
which greatly alleviates the problem of anchors' designation. Furthermore, AADI
is a learning-based anchor augmentation method, but it does not add any
parameters or hyper-parameters, which is beneficial for research and downstream
tasks. Extensive experiments on COCO dataset demonstrate the effectiveness of
AADI, specifically, AADI achieves significant performance boosts on many
state-of-the-art object detectors (eg. at least +2.4 box AP on Faster R-CNN,
+2.2 box AP on Mask R-CNN, and +0.9 box AP on Cascade Mask R-CNN). We hope that
this simple and cost-efficient method can be widely used in object detection.
Code and models are available at https://github.com/WanXiaopei/aadi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Contrastive Learning for Image Reconstruction: Learning Transferable Representations from Noisy Images. (arXiv:2106.10070v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10070">
<div class="article-summary-box-inner">
<span><p>This paper is concerned with contrastive learning (CL) for low-level image
restoration and enhancement tasks. We propose a new label-efficient learning
paradigm based on residuals, residual contrastive learning (RCL), and derive an
unsupervised visual representation learning framework, suitable for low-level
vision tasks with noisy inputs. While supervised image reconstruction aims to
minimize residual terms directly, RCL alternatively builds a connection between
residuals and CL by defining a novel instance discrimination pretext task,
using residuals as the discriminative feature. Our formulation mitigates the
severe task misalignment between instance discrimination pretext tasks and
downstream image reconstruction tasks, present in existing CL frameworks.
Experimentally, we find that RCL can learn robust and transferable
representations that improve the performance of various downstream tasks, such
as denoising and super resolution, in comparison with recent self-supervised
methods designed specifically for noisy inputs. Additionally, our unsupervised
pre-training can significantly reduce annotation costs whilst maintaining
performance competitive with fully-supervised image reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Aware Sampling Layer for Point-Wise Analysis. (arXiv:2107.04291v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04291">
<div class="article-summary-box-inner">
<span><p>Sampling, grouping, and aggregation are three important components in the
multi-scale analysis of point clouds. In this paper, we present a novel
data-driven sampler learning strategy for point-wise analysis tasks. Unlike the
widely used sampling technique, Farthest Point Sampling (FPS), we propose to
learn sampling and downstream applications jointly. Our key insight is that
uniform sampling methods like FPS are not always optimal for different tasks:
sampling more points around boundary areas can make the point-wise
classification easier for segmentation. Towards this end, we propose a novel
sampler learning strategy that learns sampling point displacement supervised by
task-related ground truth information and can be trained jointly with the
underlying tasks. We further demonstrate our methods in various point-wise
analysis tasks, including semantic part segmentation, point cloud completion,
and keypoint detection. Our experiments show that jointly learning of the
sampler and task brings better performance than using FPS in various
point-based networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning for Image-Based Camera Localization. (arXiv:2108.09112v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09112">
<div class="article-summary-box-inner">
<span><p>For several emerging technologies such as augmented reality, autonomous
driving and robotics, visual localization is a critical component. Directly
regressing camera pose/3D scene coordinates from the input image using deep
neural networks has shown great potential. However, such methods assume a
stationary data distribution with all scenes simultaneously available during
training. In this paper, we approach the problem of visual localization in a
continual learning setup -- whereby the model is trained on scenes in an
incremental manner. Our results show that similar to the classification domain,
non-stationary data induces catastrophic forgetting in deep networks for visual
localization. To address this issue, a strong baseline based on storing and
replaying images from a fixed buffer is proposed. Furthermore, we propose a new
sampling method based on coverage score (Buff-CS) that adapts the existing
sampling strategies in the buffering process to the problem of visual
localization. Results demonstrate consistent improvements over standard
buffering methods on two challenging datasets -- 7Scenes, 12Scenes, and also
19Scenes by combining the former scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bio-Inspired Audio-Visual Cues Integration for Visual Attention Prediction. (arXiv:2109.08371v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08371">
<div class="article-summary-box-inner">
<span><p>Visual Attention Prediction (VAP) methods simulates the human selective
attention mechanism to perceive the scene, which is significant and imperative
in many vision tasks. Most existing methods only consider visual cues, while
neglect the accompanied audio information, which can provide complementary
information for the scene understanding. In fact, there exists a strong
relation between auditory and visual cues, and humans generally perceive the
surrounding scene by simultaneously sensing these cues. Motivated by this, a
bio-inspired audio-visual cues integration method is proposed for the VAP task,
which explores the audio modality to better predict the visual attention map by
assisting vision modality. The proposed method consists of three parts: 1)
audio-visual encoding, 2) audio-visual location, and 3) multi-cues aggregation
parts. Firstly, a refined SoundNet architecture is adopted to encode audio
modality for obtaining corresponding features, and a modified 3D ResNet-50
architecture is employed to learn visual features, containing both spatial
location and temporal motion information. Secondly, an audio-visual location
part is devised to locate the sound source in the visual scene by learning the
correspondence between audio-visual information. Thirdly, a multi-cues
aggregation part is devised to adaptively aggregate audio-visual information
and center-bias prior to generate the final visual attention map. Extensive
experiments are conducted on six challenging audiovisual eye-tracking datasets,
including DIEM, AVAD, Coutrot1, Coutrot2, SumMe, and ETMD, which shows
significant superiority over state-of-the-art visual attention models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BINAS: Bilinear Interpretable Neural Architecture Search. (arXiv:2110.12399v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12399">
<div class="article-summary-box-inner">
<span><p>Practical use of neural networks often involves requirements on latency,
energy and memory among others. A popular approach to find networks under such
requirements is through constrained Neural Architecture Search (NAS). However,
previous methods use complicated predictors for the accuracy of the network.
Those predictors are hard to interpret and sensitive to many hyperparameters to
be tuned, hence, the resulting accuracy of the generated models is often
harmed. In this work we resolve this by introducing Bilinear Interpretable
Neural Architecture Search (BINAS), that is based on an accurate and simple
bilinear formulation of both an accuracy estimator and the expected resource
requirement, together with a scalable search method with theoretical
guarantees. The simplicity of our proposed estimator together with the
intuitive way it is constructed bring interpretability through many insights
about the contribution of different design choices. For example, we find that
in the examined search space, adding depth and width is more effective at
deeper stages of the network and at the beginning of each resolution stage. Our
experiments show that BINAS generates comparable to or better architectures
than other state-of-the-art NAS methods within a reduced marginal search cost,
while strictly satisfying the resource constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Detect Open Carry and Concealed Object with 77GHz Radar. (arXiv:2111.00551v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00551">
<div class="article-summary-box-inner">
<span><p>Detecting harmful carried objects plays a key role in intelligent
surveillance systems and has widespread applications, for example, in airport
security. In this paper, we focus on the relatively unexplored area of using
low-cost 77GHz mmWave radar for the carried objects detection problem. The
proposed system is capable of real-time detecting three classes of objects -
laptop, phone, and knife - under open carry and concealed cases where objects
are hidden with clothes or bags. This capability is achieved by the initial
signal processing for localization and generating range-azimuth-elevation image
cubes, followed by a deep learning-based prediction network and a multi-shot
post-processing module for detecting objects. Extensive experiments for
validating the system performance on detecting open carry and concealed objects
have been presented with a self-built radar-camera testbed and collected
dataset. Additionally, the influence of different input formats, factors, and
parameters on system performance is analyzed, providing an intuitive
understanding of the system. This system would be the very first baseline for
other future works aiming to detect carried objects using 77GHz radar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-pass Object-adaptive Data Undersampling and Reconstruction for MRI. (arXiv:2111.09212v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09212">
<div class="article-summary-box-inner">
<span><p>There is much recent interest in techniques to accelerate the data
acquisition process in MRI by acquiring limited measurements. Often
sophisticated reconstruction algorithms are deployed to maintain high image
quality in such settings. In this work, we propose a data-driven sampler using
a convolutional neural network, MNet, to provide object-specific sampling
patterns adaptive to each scanned object. The network observes very limited
low-frequency k-space data for each object and rapidly predicts the desired
undersampling pattern in one go that achieves high image reconstruction
quality.
</p>
<p>We propose an accompanying alternating-type training framework with a
mask-backward procedure that efficiently generates training labels for the
sampler network and jointly trains an image reconstruction network.
Experimental results on the fastMRI knee dataset demonstrate the ability of the
proposed learned undersampling network to generate object-specific masks at
fourfold and eightfold acceleration that achieve superior image reconstruction
performance than several existing schemes. The source code for the proposed
joint sampling and reconstruction learning framework is available at
https://github.com/zhishenhuang/mri.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis. (arXiv:2111.15186v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15186">
<div class="article-summary-box-inner">
<span><p>Obtaining annotations for large training sets is expensive, especially in
settings where domain knowledge is required, such as behavior analysis. Weak
supervision has been studied to reduce annotation costs by using weak labels
from task-specific labeling functions (LFs) to augment ground truth labels.
However, domain experts still need to hand-craft different LFs for different
tasks, limiting scalability. To reduce expert effort, we present AutoSWAP: a
framework for automatically synthesizing data-efficient task-level LFs. The key
to our approach is to efficiently represent expert knowledge in a reusable
domain-specific language and more general domain-level LFs, with which we use
state-of-the-art program synthesis techniques and a small labeled dataset to
generate task-level LFs. Additionally, we propose a novel structural diversity
cost that allows for efficient synthesis of diverse sets of LFs, further
improving AutoSWAP's performance. We evaluate AutoSWAP in three behavior
analysis domains and demonstrate that AutoSWAP outperforms existing approaches
using only a fraction of the data. Our results suggest that AutoSWAP is an
effective way to automatically generate LFs that can significantly reduce
expert effort for behavior analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Keypoint Discovery in Behavioral Videos. (arXiv:2112.05121v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05121">
<div class="article-summary-box-inner">
<span><p>We propose a method for learning the posture and structure of agents from
unlabelled behavioral videos. Starting from the observation that behaving
agents are generally the main sources of movement in behavioral videos, our
method, Behavioral Keypoint Discovery (B-KinD), uses an encoder-decoder
architecture with a geometric bottleneck to reconstruct the spatiotemporal
difference between video frames. By focusing only on regions of movement, our
approach works directly on input videos without requiring manual annotations.
Experiments on a variety of agent types (mouse, fly, human, jellyfish, and
trees) demonstrate the generality of our approach and reveal that our
discovered keypoints represent semantically meaningful body parts, which
achieve state-of-the-art performance on keypoint regression among
self-supervised methods. Additionally, B-KinD achieve comparable performance to
supervised keypoints on downstream tasks, such as behavior classification,
suggesting that our method can dramatically reduce model training costs
vis-a-vis supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07309">
<div class="article-summary-box-inner">
<span><p>Real-time object pose estimation is necessary for many robot manipulation
algorithms. However, state-of-the-art methods for object pose estimation are
trained for a specific set of objects; these methods thus need to be retrained
to estimate the pose of each new object, often requiring tens of GPU-days of
training for optimal performance. In this paper, we propose the OSSID
framework, leveraging a slow zero-shot pose estimator to self-supervise the
training of a fast detection algorithm. This fast detector can then be used to
filter the input to the pose estimator, drastically improving its inference
speed. We show that this self-supervised training exceeds the performance of
existing zero-shot detection methods on two widely used object pose estimation
and detection datasets, without requiring any human annotations. Further, we
show that the resulting method for pose estimation has a significantly faster
inference speed, due to the ability to filter out large parts of the image.
Thus, our method for self-supervised online learning of a detector (trained
using pseudo-labels from a slow pose estimator) leads to accurate pose
estimation at real-time speeds, without requiring human annotations.
Supplementary materials and code can be found at
https://georgegu1997.github.io/OSSID/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation. (arXiv:2201.07927v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07927">
<div class="article-summary-box-inner">
<span><p>Despite recent advances in appearance-based gaze estimation techniques, the
need for training data that covers the target head pose and gaze distribution
remains a crucial challenge for practical deployment. This work examines a
novel approach for synthesizing gaze estimation training data based on
monocular 3D face reconstruction. Unlike prior works using multi-view
reconstruction, photo-realistic CG models, or generative neural networks, our
approach can manipulate and extend the head pose range of existing training
data without any additional requirements. We introduce a projective matching
procedure to align the reconstructed 3D facial mesh with the camera coordinate
system and synthesize face images with accurate gaze labels. We also propose a
mask-guided gaze estimation model and data augmentation strategies to further
improve the estimation accuracy by taking advantage of synthetic training data.
Experiments using multiple public datasets show that our approach significantly
improves the estimation performance on challenging cross-dataset settings with
non-overlapping gaze distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-Time Rendering Method for Light Field Display. (arXiv:2201.08266v4 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08266">
<div class="article-summary-box-inner">
<span><p>A real-time elemental image array (EIA) generation method which does not
sacrifice accuracy nor rely on high-performance hardware is developed, through
raytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both
offline and online working flow, experiments will verified the effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On scale-invariant properties in natural images and their simulations. (arXiv:2201.13312v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13312">
<div class="article-summary-box-inner">
<span><p>We study samples of natural images for which a set of statistical
characteristics is computed and scale-invariant properties of samples are
demonstrated computationally. Computations of the power spectrum are carried
out and a power-law decaying power spectrum is observed on samples taken from
van Hateren images of natural scenes. We propose a dynamic model to reproduce
the observed slope in the power spectrum qualitatively. For two types of
sources for this model the behaviour of power spectrum is investigated and
scale-invariance confirmed numerically. We then discuss potential applications
of scale-invariant properties of natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISNet: Costless and Implicit Image Segmentation for Deep Classifiers, with Application in COVID-19 Detection. (arXiv:2202.00232v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00232">
<div class="article-summary-box-inner">
<span><p>This work proposes a novel deep neural network (DNN) architecture, Implicit
Segmentation Neural Network (ISNet), to solve the task of image segmentation
followed by classification. It substitutes the common pipeline of two DNNs with
a single model. We designed the ISNet for high flexibility and performance: it
allows virtually any classification neural network architecture to analyze a
common image as if it had been previously segmented. Furthermore, in relation
to the unmodified classifier, the ISNet does not cause any increment in
computational cost at run-time. We test the architecture with two applications:
COVID-19 detection in chest X-rays, and facial attribute estimation. We
implement an ISNet based on a DenseNet121 classifier, and compare the model to
a U-net (performing lung/face segmentation) followed by a DenseNet121, and to a
standalone DenseNet121. The new architecture matched the other DNNs in facial
attribute estimation. Moreover, it strongly surpassed them in COVID-19
detection, according to an external test dataset. The ISNet precisely ignored
the image regions outside of the lungs or faces. Therefore, in COVID-19
detection it reduced the effects of background bias and shortcut learning, and
it improved security in facial attribute estimation. ISNet presents an
accurate, fast, and light methodology. The successful implicit segmentation,
considering two largely diverse fields, highlights the architecture's general
applicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROMNet: Renovate the Old Memories. (arXiv:2202.02606v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02606">
<div class="article-summary-box-inner">
<span><p>Renovating the memories in old photos is an intriguing research topic in
computer vision fields. These legacy images often suffer from severe and
commingled degradations such as cracks, noise, and color-fading, while lack of
large-scale paired old photo datasets makes this restoration task very
challenging. In this work, we present a novel reference-based end-to-end
learning framework that can jointly repair and colorize the degraded legacy
pictures. Specifically, the proposed framework consists of three modules: a
restoration sub-network for degradation restoration, a similarity sub-network
for color histogram matching and transfer, and a colorization subnet that
learns to predict the chroma elements of the images conditioned on chromatic
reference signals. The whole system takes advantage of the color histogram
priors in a given reference image, which vastly reduces the dependency on
large-scale training data. Apart from the proposed method, we also create, to
our knowledge, the first public and real-world old photo dataset with paired
ground truth for evaluating old photo restoration models, wherein each old
photo is paired with a manually restored pristine image by PhotoShop experts.
Our extensive experiments conducted on both synthetic and real-world datasets
demonstrate that our method significantly outperforms state-of-the-arts both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Perspective Deformation in X-Ray Transmission Imaging. (arXiv:2202.06366v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06366">
<div class="article-summary-box-inner">
<span><p>In cone-beam X-ray transmission imaging, due to the divergence of X-rays,
imaged structures with different depths have different magnification factors on
an X-ray detector, which results in perspective deformation. Perspective
deformation causes difficulty in direct, accurate geometric assessments of
anatomical structures. In this work, to reduce perspective deformation in X-ray
images acquired from regular cone-beam computed tomography (CBCT) systems, we
investigate on learning perspective deformation, i.e., converting perspective
projections into orthogonal projections. Directly converting a single
perspective projection image into an orthogonal projection image is extremely
challenging due to the lack of depth information. Therefore, we propose to
utilize one additional perspective projection, a complementary (180-degree) or
orthogonal (90-degree) view, to provide a certain degree of depth information.
Furthermore, learning perspective deformation in different spatial domains is
investigated. Our proposed method is evaluated on numerical spherical bead
phantoms as well as patients' chest and head X-ray data. The experiments on
numerical bead phantom data demonstrate that learning perspective deformation
in polar coordinates has significant advantages over learning in Cartesian
coordinates, as root-mean-square error (RMSE) decreases from 5.31 to 1.40,
while learning in log-polar coordinates has no further considerable improvement
(RMSE = 1.85). In addition, using a complementary view (RMSE = 1.40) is better
than an orthogonal view (RMSE = 3.87). The experiments on patients' chest and
head data demonstrate that learning perspective deformation using dual
complementary views is also applicable in anatomical X-ray data, allowing
accurate cardiothoracic ratio measurements in chest X-ray images and
cephalometric analysis in synthetic cephalograms from cone-beam X-ray
projections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Wavelet-based Dual-stream Network for Underwater Image Enhancement. (arXiv:2202.08758v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08758">
<div class="article-summary-box-inner">
<span><p>We present a wavelet-based dual-stream network that addresses color cast and
blurry details in underwater images. We handle these artifacts separately by
decomposing an input image into multiple frequency bands using discrete wavelet
transform, which generates the downsampled structure image and detail images.
These sub-band images are used as input to our dual-stream network that
incorporates two sub-networks: the multi-color space fusion network and the
detail enhancement network. The multi-color space fusion network takes the
decomposed structure image as input and estimates the color corrected output by
employing the feature representations from diverse color spaces of the input.
The detail enhancement network addresses the blurriness of the original
underwater image by improving the image details from high-frequency sub-bands.
We validate the proposed method on both real-world and synthetic underwater
datasets and show the effectiveness of our model in color correction and blur
removal with low computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields. (arXiv:2203.01913v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01913">
<div class="article-summary-box-inner">
<span><p>Thin, reflective objects such as forks and whisks are common in our daily
lives, but they are particularly challenging for robot perception because it is
hard to reconstruct them using commodity RGB-D cameras or multi-view stereo
techniques. While traditional pipelines struggle with objects like these,
Neural Radiance Fields (NeRFs) have recently been shown to be remarkably
effective for performing view synthesis on objects with thin structures or
reflective materials. In this paper we explore the use of NeRF as a new source
of supervision for robust robot vision systems. In particular, we demonstrate
that a NeRF representation of a scene can be used to train dense object
descriptors. We use an optimized NeRF to extract dense correspondences between
multiple views of an object, and then use these correspondences as training
data for learning a view-invariant representation of the object. NeRF's usage
of a density field allows us to reformulate the correspondence problem with a
novel distribution-of-depths formulation, as opposed to the conventional
approach of using a depth map. Dense correspondence models supervised with our
method significantly outperform off-the-shelf learned descriptors by 106%
(PCK@3px metric, more than doubling performance) and outperform our baseline
supervised with multi-view stereo by 29%. Furthermore, we demonstrate the
learned dense descriptors enable robots to perform accurate 6-degree of freedom
(6-DoF) pick and place of thin and reflective objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08075">
<div class="article-summary-box-inner">
<span><p>Spatial commonsense, the knowledge about spatial position and relationship
between objects (like the relative size of a lion and a girl, and the position
of a boy relative to a bicycle when cycling), is an important part of
commonsense knowledge. Although pretrained language models (PLMs) succeed in
many NLP tasks, they are shown to be ineffective in spatial commonsense
reasoning. Starting from the observation that images are more likely to exhibit
spatial commonsense than texts, we explore whether models with visual signals
learn more spatial commonsense than text-based PLMs. We propose a spatial
commonsense benchmark that focuses on the relative scales of objects, and the
positional relationship between people and objects under different actions. We
probe PLMs and models with visual signals, including vision-language pretrained
models and image synthesis models, on this benchmark, and find that image
synthesis models are more capable of learning accurate and consistent spatial
knowledge than other models. The spatial knowledge from image synthesis models
also helps in natural language understanding tasks that require spatial
commonsense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating natural images with direct Patch Distributions Matching. (arXiv:2203.11862v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11862">
<div class="article-summary-box-inner">
<span><p>Many traditional computer vision algorithms generate realistic images by
requiring that each patch in the generated image be similar to a patch in a
training image and vice versa. Recently, this classical approach has been
replaced by adversarial training with a patch discriminator. The adversarial
approach avoids the computational burden of finding nearest neighbors of
patches but often requires very long training times and may fail to match the
distribution of patches. In this paper we leverage the recently developed
Sliced Wasserstein Distance and develop an algorithm that explicitly and
efficiently minimizes the distance between patch distributions in two images.
Our method is conceptually simple, requires no training and can be implemented
in a few lines of codes. On a number of image generation tasks we show that our
results are often superior to single-image-GANs, require no training, and can
generate high quality images in a few seconds. Our implementation is available
at https://github.com/ariel415el/GPDM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14542">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning methods require a large repository of annotated
data; hence, label noise is inevitable. Training with such noisy data
negatively impacts the generalization performance of deep neural networks. To
combat label noise, recent state-of-the-art methods employ some sort of sample
selection mechanism to select a possibly clean subset of data. Next, an
off-the-shelf semi-supervised learning method is used for training where
rejected samples are treated as unlabeled data. Our comprehensive analysis
shows that current selection methods disproportionately select samples from
easy (fast learnable) classes while rejecting those from relatively harder
ones. This creates class imbalance in the selected clean set and in turn,
deteriorates performance under high label noise. In this work, we propose
UNICON, a simple yet effective sample selection method which is robust to high
label noise. To address the disproportionate selection of easy and hard
samples, we introduce a Jensen-Shannon divergence based uniform selection
mechanism which does not require any probabilistic modeling and hyperparameter
tuning. We complement our selection method with contrastive learning to further
combat the memorization of noisy labels. Extensive experimentation on multiple
benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%
improvement over the current state-of-the-art on CIFAR100 dataset with a 90%
noise rate. Our code is publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrative Few-Shot Learning for Classification and Segmentation. (arXiv:2203.15712v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15712">
<div class="article-summary-box-inner">
<span><p>We introduce the integrative task of few-shot classification and segmentation
(FS-CS) that aims to both classify and segment target objects in a query image
when the target classes are given with a few examples. This task combines two
conventional few-shot learning problems, few-shot classification and
segmentation. FS-CS generalizes them to more realistic episodes with arbitrary
image pairs, where each target class may or may not be present in the query. To
address the task, we propose the integrative few-shot learning (iFSL) framework
for FS-CS, which trains a learner to construct class-wise foreground maps for
multi-label classification and pixel-wise segmentation. We also develop an
effective iFSL model, attentive squeeze network (ASNet), that leverages deep
semantic correlation and global self-attention to produce reliable foreground
maps. In experiments, the proposed method shows promising performance on the
FS-CS task and also achieves the state of the art on standard few-shot
segmentation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiEarth 2022 -- Multimodal Learning for Earth and Environment Workshop and Challenge. (arXiv:2204.07649v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07649">
<div class="article-summary-box-inner">
<span><p>The Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022)
will be the first competition aimed at the monitoring and analysis of
deforestation in the Amazon rainforest at any time and in any weather
conditions. The goal of the Challenge is to provide a common benchmark for
multimodal information processing and to bring together the earth and
environmental science communities as well as multimodal representation learning
communities to compare the relative merits of the various multimodal learning
methods to deforestation estimation under well-defined and strictly comparable
conditions. MultiEarth 2022 will have three sub-challenges: 1) matrix
completion, 2) deforestation estimation, and 3) image-to-image translation.
This paper presents the challenge guidelines, datasets, and evaluation metrics
for the three sub-challenges. Our challenge website is available at
https://sites.google.com/view/rainforest-challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network. (arXiv:2204.08084v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08084">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework to learn 3D point cloud semantics from 2D
multi-view image observations containing pose error. On the one hand, directly
learning from the massive, unstructured and unordered 3D point cloud is
computationally and algorithmically more difficult than learning from
compactly-organized and context-rich 2D RGB images. On the other hand, both
LiDAR point cloud and RGB images are captured in standard automated-driving
datasets. This motivates us to conduct a "task transfer" paradigm so that 3D
semantic segmentation benefits from aggregating 2D semantic cues, albeit pose
noises are contained in 2D image observations. Among all difficulties, pose
noise and erroneous prediction from 2D semantic segmentation approaches are the
main challenges for the task transfer. To alleviate the influence of those
factor, we perceive each 3D point using multi-view images and for each single
image a patch observation is associated. Moreover, the semantic labels of a
block of neighboring 3D points are predicted simultaneously, enabling us to
exploit the point structure prior to further improve the performance. A
hierarchical full attention network~(HiFANet) is designed to sequentially
aggregates patch, bag-of-frames and inter-point semantic cues, with
hierarchical attention mechanism tailored for different level of semantic cues.
Also, each preceding attention block largely reduces the feature size before
feeding to the next attention block, making our framework slim. Experiment
results on Semantic-KITTI show that the proposed framework outperforms existing
3D point cloud based methods significantly, it requires much less training data
and exhibits tolerance to pose noise. The code is available at
https://github.com/yuhanghe01/HiFANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09914">
<div class="article-summary-box-inner">
<span><p>LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic Images in Facial Capture Pipelines. (arXiv:2204.10746v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10746">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end pipeline for both building and tracking 3D facial
models from personalized in-the-wild (cellphone, webcam, youtube clips, etc.)
video data. First, we present a method for automatic data curation and
retrieval based on a hierarchical clustering framework typical of collision
detection algorithms in traditional computer graphics pipelines. Subsequently,
we utilize synthetic turntables and leverage deepfake technology in order to
build a synthetic multi-view stereo pipeline for appearance capture that is
robust to imperfect synthetic geometry and image misalignment. The resulting
model is fit with an animation rig, which is then used to track facial
performances. Notably, our novel use of deepfake technology enables us to
perform robust tracking of in-the-wild data using differentiable renderers
despite a significant synthetic-to-real domain gap. Finally, we outline how we
train a motion capture regressor, leveraging the aforementioned techniques to
avoid the need for real-world ground truth data and/or a high-end calibrated
camera capture setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identity Preserving Loss for Learned Image Compression. (arXiv:2204.10869v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10869">
<div class="article-summary-box-inner">
<span><p>Deep learning model inference on embedded devices is challenging due to the
limited availability of computation resources. A popular alternative is to
perform model inference on the cloud, which requires transmitting images from
the embedded device to the cloud. Image compression techniques are commonly
employed in such cloud-based architectures to reduce transmission latency over
low bandwidth networks. This work proposes an end-to-end image compression
framework that learns domain-specific features to achieve higher compression
ratios than standard HEVC/JPEG compression techniques while maintaining
accuracy on downstream tasks (e.g., recognition). Our framework does not
require fine-tuning of the downstream task, which allows us to drop-in any
off-the-shelf downstream task model without retraining. We choose faces as an
application domain due to the ready availability of datasets and off-the-shelf
recognition models as representative downstream tasks. We present a novel
Identity Preserving Reconstruction (IPR) loss function which achieves
Bits-Per-Pixel (BPP) values that are ~38% and ~42% of CRF-23 HEVC compression
for LFW (low-resolution) and CelebA-HQ (high-resolution) datasets,
respectively, while maintaining parity in recognition accuracy. The superior
compression ratio is achieved as the model learns to retain the domain-specific
features (e.g., facial features) while sacrificing details in the background.
Furthermore, images reconstructed by our proposed compression model are robust
to changes in downstream model architectures. We show at-par recognition
performance on the LFW dataset with an unseen recognition model while retaining
a lower BPP value of ~38% of CRF-23 HEVC compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11479">
<div class="article-summary-box-inner">
<span><p>While efficient architectures and a plethora of augmentations for end-to-end
image classification tasks have been suggested and heavily investigated,
state-of-the-art techniques for audio classifications still rely on numerous
representations of the audio signal together with large architectures,
fine-tuned from large datasets. By utilizing the inherited lightweight nature
of audio and novel audio augmentations, we were able to present an efficient
end-to-end network with strong generalization ability. Experiments on a variety
of sound classification sets demonstrate the effectiveness and robustness of
our approach, by achieving state-of-the-art results in various settings. Public
code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Monocular 3D Object Detection via Self-Training. (arXiv:2204.11590v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11590">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection (Mono3D) has achieved unprecedented success
with the advent of deep learning techniques and emerging large-scale autonomous
driving datasets. However, drastic performance degradation remains an
unwell-studied challenge for practical cross-domain deployment as the lack of
labels on the target domain. In this paper, we first comprehensively
investigate the significant underlying factor of the domain gap in Mono3D,
where the critical observation is a depth-shift issue caused by the geometric
misalignment of domains. Then, we propose STMono3D, a new self-teaching
framework for unsupervised domain adaptation on Mono3D. To mitigate the
depth-shift, we introduce the geometry-aligned multi-scale training strategy to
disentangle the camera parameters and guarantee the geometry consistency of
domains. Based on this, we develop a teacher-student paradigm to generate
adaptive pseudo labels on the target domain. Benefiting from the end-to-end
framework that provides richer information of the pseudo labels, we propose the
quality-aware supervision strategy to take instance-level pseudo confidences
into account and improve the effectiveness of the target-domain training
process. Moreover, the positive focusing training strategy and dynamic
threshold are proposed to handle tremendous FN and FP pseudo samples. STMono3D
achieves remarkable performance on all evaluated datasets and even surpasses
fully supervised results on the KITTI 3D object detection dataset. To the best
of our knowledge, this is the first study to explore effective UDA methods for
Mono3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performer: A Novel PPG to ECG Reconstruction Transformer For a Digital Biomarker of Cardiovascular Disease Detection. (arXiv:2204.11795v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11795">
<div class="article-summary-box-inner">
<span><p>Cardiovascular diseases (CVDs) have become the top one cause of death;
three-quarters of these deaths occur in lower-income communities.
Electrocardiography (ECG), an electrical measurement capturing the cardiac
activities, is a gold-standard to diagnose CVDs. However, ECG is infeasible for
continuous cardiac monitoring due to its requirement for user participation.
Meanwhile, photoplethysmography (PPG) is easy to collect, but the limited
accuracy constrains its clinical usage. In this research, a novel
Transformer-based architecture, Performer, is invented to reconstruct ECG from
PPG and to create a novel digital biomarker, PPG along with its reconstructed
ECG, as multiple modalities for CVD detection. This architecture, for the first
time, performs Transformer sequence to sequence translation on biomedical
waveforms, while also utilizing the advantages of the easily accessible PPG and
the well-studied base of ECG. Shifted Patch-based Attention (SPA) is created to
maximize the signal features by fetching the various sequence lengths as
hierarchical stages into the training while also capturing cross-patch
connections through the shifted patch mechanism. This architecture generates a
state-of-the-art performance of 0.29 RMSE for reconstructing ECG from PPG,
achieving an average of 95.9% diagnosis for CVDs on the MIMIC III dataset and
75.9% for diabetes on the PPG-BP dataset. Performer, along with its novel
digital biomarker, offers a low-cost and non-invasive solution for continuous
cardiac monitoring, only requiring the easily extractable PPG data to
reconstruct the not-as-accessible ECG data. As a prove of concept, an earring
wearable, named PEARL (prototype), is designed to scale up the point-of-care
(POC) healthcare system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive learning-based computational histopathology predict differential expression of cancer driver genes. (arXiv:2204.11994v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11994">
<div class="article-summary-box-inner">
<span><p>Digital pathological analysis is run as the main examination used for cancer
diagnosis. Recently, deep learning-driven feature extraction from pathology
images is able to detect genetic variations and tumor environment, but few
studies focus on differential gene expression in tumor cells. In this paper, we
propose a self-supervised contrastive learning framework, HistCode, to infer
differential gene expressions from whole slide images (WSIs). We leveraged
contrastive learning on large-scale unannotated WSIs to derive slide-level
histopathological feature in latent space, and then transfer it to tumor
diagnosis and prediction of differentially expressed cancer driver genes. Our
extensive experiments showed that our method outperformed other
state-of-the-art models in tumor diagnosis tasks, and also effectively
predicted differential gene expressions. Interestingly, we found the higher
fold-changed genes can be more precisely predicted. To intuitively illustrate
the ability to extract informative features from pathological images, we
spatially visualized the WSIs colored by the attentive scores of image tiles.
We found that the tumor and necrosis areas were highly consistent with the
annotations of experienced pathologists. Moreover, the spatial heatmap
generated by lymphocyte-specific gene expression patterns was also consistent
with the manually labeled WSI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the ability of generative adversarial networks to learn canonical medical image statistics. (arXiv:2204.12007v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12007">
<div class="article-summary-box-inner">
<span><p>In recent years, generative adversarial networks (GANs) have gained
tremendous popularity for potential applications in medical imaging, such as
medical image synthesis, restoration, reconstruction, translation, as well as
objective image quality assessment. Despite the impressive progress in
generating high-resolution, perceptually realistic images, it is not clear if
modern GANs reliably learn the statistics that are meaningful to a downstream
medical imaging application. In this work, the ability of a state-of-the-art
GAN to learn the statistics of canonical stochastic image models (SIMs) that
are relevant to objective assessment of image quality is investigated. It is
shown that although the employed GAN successfully learned several basic first-
and second-order statistics of the specific medical SIMs under consideration
and generated images with high perceptual quality, it failed to correctly learn
several per-image statistics pertinent to the these SIMs, highlighting the
urgent need to assess medical image GANs in terms of objective measures of
image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment. (arXiv:2204.12283v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12283">
<div class="article-summary-box-inner">
<span><p>Understanding the impact of tumor biology on the composition of nearby cells
often requires characterizing the impact of biologically distinct tumor
regions. Biomarkers have been developed to label biologically distinct tumor
regions, but challenges arise because of differences in the spatial extent and
distribution of differentially labeled regions. In this work, we present a
framework for systematically investigating the impact of distinct tumor regions
on cells near the tumor borders, accounting their cross spatial distributions.
We apply the framework to multiplex immunohistochemistry (mIHC) studies of
pancreatic cancer and show its efficacy in demonstrating how biologically
different tumor regions impact the immune response in the tumor
microenvironment. Furthermore, we show that the proposed framework can be
extended to largescale whole slide image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Quality of a Synthesized Motion with the Fr\'echet Motion Distance. (arXiv:2204.12318v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12318">
<div class="article-summary-box-inner">
<span><p>Evaluating the Quality of a Synthesized Motion with the Fr\'echet Motion
Distance
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding The Robustness in Vision Transformers. (arXiv:2204.12451v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12451">
<div class="article-summary-box-inner">
<span><p>Recent studies show that Vision Transformers(ViTs) exhibit strong robustness
against various corruptions. Although this property is partly attributed to the
self-attention mechanism, there is still a lack of systematic understanding. In
this paper, we examine the role of self-attention in learning robust
representations. Our study is motivated by the intriguing properties of the
emerging visual grouping in Vision Transformers, which indicates that
self-attention may promote robustness through improved mid-level
representations. We further propose a family of fully attentional networks
(FANs) that strengthen this capability by incorporating an attentional channel
processing design. We validate the design comprehensively on various
hierarchical backbones. Our model achieves a state of-the-art 87.1% accuracy
and 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also
demonstrate state-of-the-art accuracy and robustness in two downstream tasks:
semantic segmentation and object detection. Code will be available at
https://github.com/NVlabs/FAN.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-28 23:08:34.991871632 UTC">2022-04-28 23:08:34 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>