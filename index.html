<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-25T01:30:00Z">02-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A gentle introduction to Quantum Natural Language Processing. (arXiv:2202.11766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11766">
<div class="article-summary-box-inner">
<span><p>The main goal of this master's thesis is to introduce Quantum Natural
Language Processing (QNLP) in a way understandable by both the NLP engineer and
the quantum computing practitioner. QNLP is a recent application of quantum
computing that aims at representing sentences' meaning as vectors encoded into
quantum computers. To achieve this, the distributional meaning of words is
extended by the compositional meaning of sentences (DisCoCat model) : the
vectors representing words' meanings are composed through the syntactic
structure of the sentence. This is done using an algorithm based on tensor
products. We see that this algorithm is inefficient on classical computers but
scales well using quantum circuits. After exposing the practical details of its
implementation, we go through three use-cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Unstructured Text to Causal Knowledge Graphs: A Transformer-Based Approach. (arXiv:2202.11768v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11768">
<div class="article-summary-box-inner">
<span><p>Qualitative causal relationships compactly express the direction, dependency,
temporal constraints, and monotonicity constraints of discrete or continuous
interactions in the world. In everyday or academic language, we may express
interactions between quantities (e.g., sleep decreases stress), between
discrete events or entities (e.g., a protein inhibits another protein's
transcription), or between intentional or functional factors (e.g., hospital
patients pray to relieve their pain). Extracting and representing these diverse
causal relations are critical for cognitive systems that operate in domains
spanning from scientific discovery to social science. This paper presents a
transformer-based NLP architecture that jointly extracts knowledge graphs
including (1) variables or factors described in language, (2) qualitative
causal relationships over these variables, (3) qualifiers and magnitudes that
constrain these causal relationships, and (4) word senses to localize each
extracted node within a large ontology. We do not claim that our
transformer-based architecture is itself a cognitive system; however, we
provide evidence of its accurate knowledge graph extraction in real-world
domains and the practicality of its resulting knowledge graphs for cognitive
systems that perform graph-based reasoning. We demonstrate this approach and
include promising results in two use cases, processing textual inputs from
academic publications, news articles, and social media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using natural language prompts for machine translation. (arXiv:2202.11822v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11822">
<div class="article-summary-box-inner">
<span><p>We explore the use of natural language prompts for controlling various
aspects of the outputs generated by machine translation models. We demonstrate
that natural language prompts allow us to influence properties like formality
or specific dialect of the output. We show that using language names to control
the output language of multilingual translation models enables positive
transfer for unseen language pairs. This unlocks the ability to translate into
languages not seen during fine-tuning by using their English names. We
investigate how scale, number of pre-training steps, number of languages in
fine-tuning, and language similarity affect this phenomenon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">First is Better Than Last for Training Data Influence. (arXiv:2202.11844v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11844">
<div class="article-summary-box-inner">
<span><p>The ability to identify influential training examples enables us to debug
training data and explain model behavior. Existing techniques are based on the
flow of influence through the model parameters. For large models in NLP
applications, it is often computationally infeasible to study this flow through
all model parameters, therefore techniques usually pick the last layer of
weights. Our first observation is that for classification problems, the last
layer is reductive and does not encode sufficient input level information.
Deleting influential examples, according to this measure, typically does not
change the model's behavior much. We propose a technique called TracIn-WE that
modifies a method called TracIn to operate on the word embedding layer instead
of the last layer. This could potentially have the opposite concern, that the
word embedding layer does not encode sufficient high level information.
However, we find that gradients (unlike embeddings) do not suffer from this,
possibly because they chain through higher layers. We show that TracIn-WE
significantly outperforms other data influence methods applied on the last
layer by 4-10 times on the case deletion evaluation on three language
classification tasks. In addition, TracIn-WE can produce scores not just at the
training data level, but at the word training data level, a further aid in
debugging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAISE: Conversational Agent for Image Search and Editing. (arXiv:2202.11847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11847">
<div class="article-summary-box-inner">
<span><p>Demand for image editing has been increasing as users' desire for expression
is also increasing. However, for most users, image editing tools are not easy
to use since the tools require certain expertise in photo effects and have
complex interfaces. Hence, users might need someone to help edit their images,
but having a personal dedicated human assistant for every user is impossible to
scale. For that reason, an automated assistant system for image editing is
desirable. Additionally, users want more image sources for diverse image
editing works, and integrating an image search functionality into the editing
tool is a potential remedy for this demand. Thus, we propose a dataset of an
automated Conversational Agent for Image Search and Editing (CAISE). To our
knowledge, this is the first dataset that provides conversational image search
and editing annotations, where the agent holds a grounded conversation with
users and helps them to search and edit images according to their requests. To
build such a system, we first collect image search and editing conversations
between pairs of annotators. The assistant-annotators are equipped with a
customized image search and editing tool to address the requests from the
user-annotators. The functions that the assistant-annotators conduct with the
tool are recorded as executable commands, allowing the trained system to be
useful for real-world application execution. We also introduce a
generator-extractor baseline model for this task, which can adaptively select
the source of the next token (i.e., from the vocabulary or from textual/visual
contexts) for the executable command. This serves as a strong starting point
while still leaving a large human-machine performance gap for useful future
work. Our code and dataset are publicly available at:
https://github.com/hyounghk/CAISE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Some Stylometric Remarks on Ovid's Heroides and the Epistula Sapphus. (arXiv:2202.11864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11864">
<div class="article-summary-box-inner">
<span><p>This article aims to contribute to two well-worn areas of debate in classical
Latin philology, relating to Ovid's Heroides. The first is the question of the
authenticity (and, to a lesser extent the correct position) of the letter
placed fifteenth by almost every editor -- the so-called Epistula Sapphus
(henceforth ES). The secondary question, although perhaps now less fervently
debated, is the authenticity of the 'Double Heroides', placed by those who
accept them as letters 16-21. I employ a variety of methods drawn from the
domain of computational stylometry to consider the poetics and the
lexico-grammatical features of these elegiac poems in the broader context of a
corpus of 'shorter' (from 20 to 546 lines) elegiac works from five authors (266
poems in all) comprising more or less all of the non-fragmentary classical
corpus. Based on a variety of techniques, every measure gives clear indication
that the poetic style of the Heroides is Ovidian, but distinctive; they can be
accurately isolated from Ovid more broadly. The Single and Double Heroides
split into two clear groups, with the ES grouped consistently with the single
letters. Furthermore, by comparing the style of the letters with the 'early'
(although there are complications in this label) works of the Amores and the
late works of the Ex Ponto, the evidence supports sequential composition --
meaning that the ES is correctly placed -- and, further, supports the growing
consensus that the double letters were composed significantly later, in exile.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using calibrator to improve robustness in Machine Reading Comprehension. (arXiv:2202.11865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11865">
<div class="article-summary-box-inner">
<span><p>Machine Reading Comprehension(MRC) has achieved a remarkable result since
some powerful models, such as BERT, are proposed. However, these models are not
robust enough and vulnerable to adversarial input perturbation and
generalization examples. Some works tried to improve the performance on
specific types of data by adding some related examples into training data while
it leads to degradation on the original dataset, because the shift of data
distribution makes the answer ranking based on the softmax probability of model
unreliable. In this paper, we propose a method to improve the robustness by
using a calibrator as the post-hoc reranker, which is implemented based on
XGBoost model. The calibrator combines both manual features and representation
learning features to rerank candidate results. Experimental results on
adversarial datasets show that our model can achieve performance improvement by
more than 10\% and also make improvement on the original and generalization
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phase Continuity: Learning Derivatives of Phase Spectrum for Speech Enhancement. (arXiv:2202.11918v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11918">
<div class="article-summary-box-inner">
<span><p>Modern neural speech enhancement models usually include various forms of
phase information in their training loss terms, either explicitly or
implicitly. However, these loss terms are typically designed to reduce the
distortion of phase spectrum values at specific frequencies, which ensures they
do not significantly affect the quality of the enhanced speech. In this paper,
we propose an effective phase reconstruction strategy for neural speech
enhancement that can operate in noisy environments. Specifically, we introduce
a phase continuity loss that considers relative phase variations across the
time and frequency axes. By including this phase continuity loss in a
state-of-the-art neural speech enhancement system trained with reconstruction
loss and a number of magnitude spectral losses, we show that our proposed
method further improves the quality of enhanced speech signals over the
baseline, especially when training is done jointly with a magnitude spectrum
loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Welcome to the Modern World of Pronouns: Identity-Inclusive Natural Language Processing beyond Gender. (arXiv:2202.11923v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11923">
<div class="article-summary-box-inner">
<span><p>The world of pronouns is changing. From a closed class of words with few
members to a much more open set of terms to reflect identities. However,
Natural Language Processing (NLP) is barely reflecting this linguistic shift,
even though recent work outlined the harms of gender-exclusive language
technology. Particularly problematic is the current modeling 3rd person
pronouns, as it largely ignores various phenomena like neopronouns, i.e.,
pronoun sets that are novel and not (yet) widely established. This omission
contributes to the discrimination of marginalized and underrepresented groups,
e.g., non-binary individuals. However, other identity-expression phenomena
beyond gender are also ignored by current NLP technology. In this paper, we
provide an overview of 3rd person pronoun issues for NLP. Based on our
observations and ethical considerations, we define a series of desiderata for
modeling pronouns in language technology. We evaluate existing and novel
modeling approaches w.r.t. these desiderata qualitatively, and quantify the
impact of a more discrimination-free approach on established benchmark data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Segmentation on Discovered Phone Units with Dynamic Programming and Self-Supervised Scoring. (arXiv:2202.11929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11929">
<div class="article-summary-box-inner">
<span><p>Recent work on unsupervised speech segmentation has used self-supervised
models with a phone segmentation module and a word segmentation module that are
trained jointly. This paper compares this joint methodology with an older idea:
bottom-up phone-like unit discovery is performed first, and symbolic word
segmentation is then performed on top of the discovered units (without
influencing the lower level). I specifically describe a duration-penalized
dynamic programming (DPDP) procedure that can be used for either phone or word
segmentation by changing the self-supervised scoring network that gives segment
costs. For phone discovery, DPDP is applied with a contrastive predictive
coding clustering model, while for word segmentation it is used with an
autoencoding recurrent neural network. The two models are chained in order to
segment speech. This approach gives comparable word segmentation results to
state-of-the-art joint self-supervised models on an English benchmark. On
French and Mandarin data, it outperforms previous systems on the ZeroSpeech
benchmarks. Analysis shows that the chained DPDP system segments shorter filler
words well, but longer words might require an external top-down signal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Generalization Requires Compositional Parsers. (arXiv:2202.11937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11937">
<div class="article-summary-box-inner">
<span><p>A rapidly growing body of research on compositional generalization
investigates the ability of a semantic parser to dynamically recombine
linguistic elements seen in training into unseen sequences. We present a
systematic comparison of sequence-to-sequence models and models guided by
compositional principles on the recent COGS corpus (Kim and Linzen, 2020).
Though seq2seq models can perform well on lexical tasks, they perform with
near-zero accuracy on structural generalization tasks that require novel
syntactic structures; this holds true even when they are trained to predict
syntax instead of semantics. In contrast, compositional models achieve
near-perfect accuracy on structural generalization; we present new results
confirming this from the AM parser (Groschwitz et al., 2021). Our findings show
structural generalization is a key measure of compositional generalization and
requires models that are aware of complex structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better. (arXiv:2202.12024v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12024">
<div class="article-summary-box-inner">
<span><p>Effectively finetuning pretrained language models (PLMs) is critical for
their success in downstream tasks. However, PLMs may have risks in overfitting
pretraining signals, and there are some gaps between downstream tasks and the
pretraining tasks. It can be difficult for vanilla finetuning methods to
overcome the barrier between pretraining and downstream tasks, which leads to
suboptimal performance. In this paper, we propose a very simple yet effective
method named NoisyTune which can help better finetune PLMs in downstream tasks
by adding some noise to the parameters of PLMs before finetuning. More
specifically, we propose a matrix-wise perturbing method by adding different
uniform noises according to the standard deviations of different parameter
matrices, which can consider the varied characteristics of different types of
parameters in PLMs. Extensive experiments on the GLUE English benchmark and the
XTREME multilingual benchmark show that NoisyTune can consistently improve the
performance of different PLMs in many downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Unfairness of DP-SGD Across Settings. (arXiv:2202.12058v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12058">
<div class="article-summary-box-inner">
<span><p>End users and regulators require private and fair artificial intelligence
models, but previous work suggests these objectives may be at odds. We use the
CivilComments to evaluate the impact of applying the {\em de facto} standard
approach to privacy, DP-SGD, across several fairness metrics. We evaluate three
implementations of DP-SGD: for dimensionality reduction (PCA), linear
classification (logistic regression), and robust deep learning (Group-DRO). We
establish a negative, logarithmic correlation between privacy and fairness in
the case of linear classification and robust deep learning. DP-SGD had no
significant impact on fairness for PCA, but upon inspection, also did not seem
to lead to private representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KESA: A Knowledge Enhanced Approach For Sentiment Analysis. (arXiv:2202.12093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12093">
<div class="article-summary-box-inner">
<span><p>Though some recent works focus on injecting sentiment knowledge into
pre-trained language models, they usually design mask and reconstruction tasks
in the post-training phase. In this paper, we aim to benefit from sentiment
knowledge in a lighter way. To achieve this goal, we study sentence-level
sentiment analysis and, correspondingly, propose two sentiment-aware auxiliary
tasks named sentiment word cloze and conditional sentiment prediction. The
first task learns to select the correct sentiment words within the input, given
the overall sentiment polarity as prior knowledge. On the contrary, the second
task predicts the overall sentiment polarity given the sentiment polarity of
the word as prior knowledge. In addition, two kinds of label combination
methods are investigated to unify multiple types of labels in each task. We
argue that more information can promote the models to learn more profound
semantic representation. We implement it in a straightforward way to verify
this hypothesis. The experimental results demonstrate that our approach
consistently outperforms pre-trained models and is additive to existing
knowledge-enhanced post-trained models. The code and data are released at
https://github.com/lshowway/KESA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems. (arXiv:2202.12107v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12107">
<div class="article-summary-box-inner">
<span><p>Our work is the first attempt to apply Natural Language Processing to
automate the development of simulation models of logistics systems. We
demonstrated that the framework built on top of the fine-tuned
Transdormer-based language model could produce functionally valid simulations
of queuing and inventory control systems given the verbal description. The
proposed framework has the potential to remove the tedium of programming and
allow experts to focus on the high-level consideration of the problem and
holistic thinking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction. (arXiv:2202.12109v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12109">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an effective yet efficient model PAIE for both
sentence-level and document-level Event Argument Extraction (EAE), which also
generalizes well when there is a lack of training data. On the one hand, PAIE
utilizes prompt tuning for extractive objectives to take the best advantages of
Pre-trained Language Models (PLMs). It introduces two span selectors based on
prompt to select start/end tokens among input texts for each role. On the other
hand, we capture argument interactions via multi-role prompts, and conduct
joint optimization with optimal span assignments via a bipartite matching loss.
Also, with flexible prompt design, PAIE can extract multiple arguments with the
same role, instead of conventional heuristic threshold tuning. We have
conducted extensive experiments on three benchmarks, including both sentence-
and document-level EAE. The results present a promising improvements from PAIE
(1.1% and 3.8% F1 gains on average in sentence-level and document-level
respectively). Further analysis demonstrates the efficiency, generalization to
few-shot settings and effectiveness of different extractive prompt tuning
strategies. We will release our codes upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"splink" is happy and "phrouth" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12132">
<div class="article-summary-box-inner">
<span><p>People associate affective meanings to words -- "death" is scary and sad
while "party" is connotated with surprise and joy. This raises the question if
the association is purely a product of the learned affective imports inherent
to semantic meanings, or is also an effect of other features of words, e.g.,
morphological and phonological patterns. We approach this question with an
annotation-based analysis leveraging nonsense words. Specifically, we conduct a
best-worst scaling crowdsourcing study in which participants assign intensity
scores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense
words and, for comparison of the results to previous work, to 68 real words.
Based on this resource, we develop character-level and phonology-based
intensity regressors and evaluate them on real and nonsense words, and across
these categories (making use of the NRC emotion intensity lexicon of 7493
words). The data analysis reveals that some phonetic patterns show clear
differences between emotion intensities. For instance, s as a first phoneme
contributes to joy, sh to surprise, p as last phoneme more to disgust than to
anger and fear. In the modelling experiments, a regressor trained on real words
from the NRC emotion intensity lexicon shows a higher performance (r = 0.17)
than regressors that aim at learning the emotion connotation purely from
nonsense words. We conclude that humans do associate affective meaning to words
based on surface patterns, but also based on similarities to existing words
("juy" to "joy", or "flike" to "like").
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How reparametrization trick broke differentially-private text representation leaning. (arXiv:2202.12138v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12138">
<div class="article-summary-box-inner">
<span><p>As privacy gains traction in the NLP community, researchers have started
adopting various approaches to privacy-preserving methods. One of the favorite
privacy frameworks, differential privacy (DP), is perhaps the most compelling
thanks to its fundamental theoretical guarantees. Despite the apparent
simplicity of the general concept of differential privacy, it seems non-trivial
to get it right when applying it to NLP. In this short paper, we formally
analyze several recent NLP papers proposing text representation learning using
DPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and
reveal their false claims of being differentially private. Furthermore, we also
show a simple yet general empirical sanity check to determine whether a given
implementation of a DP mechanism almost certainly violates the privacy loss
guarantees. Our main goal is to raise awareness and help the community
understand potential pitfalls of applying differential privacy to text
representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining without Wordpieces: Learning Over a Vocabulary of Millions of Words. (arXiv:2202.12142v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12142">
<div class="article-summary-box-inner">
<span><p>The standard BERT adopts subword-based tokenization, which may break a word
into two or more wordpieces (e.g., converting "lossless" to "loss" and "less").
This will bring inconvenience in following situations: (1) what is the best way
to obtain the contextual vector of a word that is divided into multiple
wordpieces? (2) how to predict a word via cloze test without knowing the number
of wordpieces in advance? In this work, we explore the possibility of
developing BERT-style pretrained model over a vocabulary of words instead of
wordpieces. We call such word-level BERT model as WordBERT. We train models
with different vocabulary sizes, initialization configurations and languages.
Results show that, compared to standard wordpiece-based BERT, WordBERT makes
significant improvements on cloze test and machine reading comprehension. On
many other natural language understanding tasks, including POS tagging,
chunking and NER, WordBERT consistently performs better than BERT. Model
analysis indicates that the major advantage of WordBERT over BERT lies in the
understanding for low-frequency words and rare words. Furthermore, since the
pipeline is language-independent, we train WordBERT for Chinese language and
obtain significant gains on five natural language understanding datasets.
Lastly, the analyse on inference speed illustrates WordBERT has comparable time
cost to BERT in natural language understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An NLP Solution to Foster the Use of Information in Electronic Health Records for Efficiency in Decision-Making in Hospital Care. (arXiv:2202.12159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12159">
<div class="article-summary-box-inner">
<span><p>The project aimed to define the rules and develop a technological solution to
automatically identify a set of attributes within free-text clinical records
written in Portuguese. The first application developed and implemented on this
basis was a structured summary of a patient's clinical history, including
previous diagnoses and procedures, usual medication, and relevant
characteristics or conditions for clinical decisions, such as allergies, being
under anticoagulant therapy, etc. The project's goal was achieved by a
multidisciplinary team that included clinicians, epidemiologists, computational
linguists, machine learning researchers and software engineers, bringing
together the expertise and perspectives of a public hospital, the university
and the private sector. Relevant benefits to users and patients are related
with facilitated access to the patient's history, which translates into
exhaustiveness in apprehending the patient's clinical past and efficiency due
to time saving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-attention for incomplete utterance rewriting. (arXiv:2202.12160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12160">
<div class="article-summary-box-inner">
<span><p>Incomplete utterance rewriting (IUR) has recently become an essential task in
NLP, aiming to complement the incomplete utterance with sufficient context
information for comprehension. In this paper, we propose a novel method by
directly extracting the coreference and omission relationship from the
self-attention weight matrix of the transformer instead of word embeddings and
edit the original text accordingly to generate the complete utterance.
Benefiting from the rich information in the self-attention weight matrix, our
method achieved competitive results on public IUR datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12163">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel language identification system based on
conformer layers. We propose an attentive temporal pooling mechanism to allow
the model to carry information in long-form audio via a recurrent form, such
that the inference can be performed in a streaming fashion. Additionally, a
simple domain adaptation mechanism is introduced to allow adapting an existing
language identification model to a new domain where the prior language
distribution is different. We perform a comparative study of different model
topologies under different constraints of model size, and find that
conformer-base models outperform LSTM and transformer based models. Our
experiments also show that attentive temporal pooling and domain adaptation
significantly improve the model accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming a Theoretical Limitation of Self-Attention. (arXiv:2202.12172v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12172">
<div class="article-summary-box-inner">
<span><p>Although transformers are remarkably effective for many tasks, there are some
surprisingly easy-looking regular languages that they struggle with. Hahn shows
that for languages where acceptance depends on a single input symbol, a
transformer's classification decisions become less and less confident (that is,
with cross-entropy approaching 1 bit per string) as input strings get longer
and longer. We examine this limitation using two languages: PARITY, the
language of bit strings with an odd number of 1s, and FIRST, the language of
bit strings starting with a 1. We demonstrate three ways of overcoming the
limitation suggested by Hahn's lemma. First, we settle an open question by
constructing a transformer that recognizes PARITY with perfect accuracy, and
similarly for FIRST. Second, we use layer normalization to bring the
cross-entropy of both models arbitrarily close to zero. Third, when
transformers need to focus on a single position, as for FIRST, we find that
they can fail to generalize to longer strings; we offer a simple remedy to this
problem that also improves length generalization in machine translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review. (arXiv:2202.12205v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12205">
<div class="article-summary-box-inner">
<span><p>Advocates for Neuro-Symbolic AI (NeSy) assert that combining deep learning
with symbolic reasoning will lead to stronger AI than either paradigm on its
own. As successful as deep learning has been, it is generally accepted that
even our best deep learning systems are not very good at abstract reasoning.
And since reasoning is inextricably linked to language, it makes intuitive
sense that Natural Language Processing (NLP), would be a particularly
well-suited candidate for NeSy. We conduct a structured review of studies
implementing NeSy for NLP, challenges and future directions, and aim to answer
the question of whether NeSy is indeed meeting its promises: reasoning,
out-of-distribution generalization, interpretability, learning and reasoning
from small data, and transferability to new domains. We examine the impact of
knowledge representation, such as rules and semantic networks, language
structure and relational structure, and whether implicit or explicit reasoning
contributes to higher promise scores. We find that knowledge encoded in
relational structures and explicit reasoning tend to lead to more NeSy goals
being satisfied. We also advocate for a more methodical approach to the
application of theories of reasoning, which we hope can reduce some of the
friction between the symbolic and sub-symbolic schools of AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTVision -- A Parameter-Efficient Approach for Question Answering. (arXiv:2202.12210v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12210">
<div class="article-summary-box-inner">
<span><p>We present a highly parameter efficient approach for Question Answering that
significantly reduces the need for extended BERT fine-tuning. Our method uses
information from the hidden state activations of each BERT transformer layer,
which is discarded during typical BERT inference. Our best model achieves
maximal BERT performance at a fraction of the training time and GPU or TPU
expense. Performance is further improved by ensembling our model with BERTs
predictions. Furthermore, we find that near optimal performance can be achieved
for QA span annotation using less training data. Our experiments show that this
approach works well not only for span annotation, but also for classification,
suggesting that it may be extensible to a wider range of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing BERT's priors with serial reproduction chains. (arXiv:2202.12226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12226">
<div class="article-summary-box-inner">
<span><p>We can learn as much about language models from what they say as we learn
from their performance on targeted benchmarks. Sampling is a promising
bottom-up method for probing, but generating samples from successful models
like BERT remains challenging. Taking inspiration from theories of iterated
learning in cognitive science, we explore the use of serial reproduction chains
to probe BERT's priors. Although the masked language modeling objective does
not guarantee a consistent joint distribution, we observe that a unique and
consistent estimator of the ground-truth joint distribution may be obtained by
a GSN sampler, which randomly selects which word to mask and reconstruct on
each step. We compare the lexical and syntactic statistics of sentences from
the resulting prior distribution against those of the ground-truth corpus
distribution and elicit a large empirical sample of naturalness judgments to
investigate how, exactly, the model deviates from human speakers. Our findings
suggest the need to move beyond top-down evaluation methods toward bottom-up
probing to capture the full richness of what has been learned about language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural reality of argument structure constructions. (arXiv:2202.12246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12246">
<div class="article-summary-box-inner">
<span><p>In lexicalist linguistic theories, argument structure is assumed to be
predictable from the meaning of verbs. As a result, the verb is the primary
determinant of the meaning of a clause. In contrast, construction grammarians
propose that argument structure is encoded in constructions (or form-meaning
pairs) that are distinct from verbs. Decades of psycholinguistic research have
produced substantial empirical evidence in favor of the construction view. Here
we adapt several psycholinguistic studies to probe for the existence of
argument structure constructions (ASCs) in Transformer-based language models
(LMs). First, using a sentence sorting experiment, we find that sentences
sharing the same construction are closer in embedding space than sentences
sharing the same verb. Furthermore, LMs increasingly prefer grouping by
construction with more input data, mirroring the behaviour of non-native
language learners. Second, in a "Jabberwocky" priming-based experiment, we find
that LMs associate ASCs with meaning, even in semantically nonsensical
sentences. Our work offers the first evidence for ASCs in LMs and highlights
the potential to devise novel probing methods grounded in psycholinguistic
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward More Meaningful Resources for Lower-resourced Languages. (arXiv:2202.12288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12288">
<div class="article-summary-box-inner">
<span><p>In this position paper, we describe our perspective on how meaningful
resources for lower-resourced languages should be developed in connection with
the speakers of those languages. We first examine two massively multilingual
resources in detail. We explore the contents of the names stored in Wikidata
for a few lower-resourced languages and find that many of them are not in fact
in the languages they claim to be and require non-trivial effort to correct. We
discuss quality issues present in WikiAnn and evaluate whether it is a useful
supplement to hand annotated data. We then discuss the importance of creating
annotation for lower-resourced languages in a thoughtful and ethical way that
includes the languages' speakers as part of the development process. We
conclude with recommended guidelines for resource development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Failures of Large Language Models via Human Cognitive Biases. (arXiv:2202.12299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12299">
<div class="article-summary-box-inner">
<span><p>Large language models generate complex, open-ended outputs: instead of
outputting a single class, they can write summaries, generate dialogue, and
produce working code. In order to study the reliability of these open-ended
systems, we must understand not just when they fail, but also how they fail. To
approach this, we draw inspiration from human cognitive biases -- systematic
patterns of deviation from rational judgement. Specifically, we use cognitive
biases to (i) identify inputs that models are likely to err on, and (ii)
develop tests to qualitatively characterize their errors on these inputs. Using
code generation as a case study, we find that OpenAI's Codex errs predictably
based on how the input prompt is framed, adjusts outputs towards anchors, and
is biased towards outputs that mimic frequent training examples. We then use
our framework to uncover high-impact errors such as incorrectly deleting files.
Our experiments suggest that cognitive science can be a useful jumping-off
point to better understand how contemporary machine learning systems behave.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embeddings-Based Clustering for Target Specific Stances: The Case of a Polarized Turkey. (arXiv:2005.09649v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.09649">
<div class="article-summary-box-inner">
<span><p>On June 24, 2018, Turkey conducted a highly consequential election in which
the Turkish people elected their president and parliament in the first election
under a new presidential system. During the election period, the Turkish people
extensively shared their political opinions on Twitter. One aspect of
polarization among the electorate was support for or opposition to the
reelection of Recep Tayyip Erdo\u{g}an. In this paper, we present an
unsupervised method for target-specific stance detection in a polarized
setting, specifically Turkish politics, achieving 90% precision in identifying
user stances, while maintaining more than 80% recall. The method involves
representing users in an embedding space using Google's Convolutional Neural
Network (CNN) based multilingual universal sentence encoder. The
representations are then projected onto a lower dimensional space in a manner
that reflects similarities and are consequently clustered. We show the
effectiveness of our method in properly clustering users of divergent groups
across multiple targets that include political figures, different groups, and
parties. We perform our analysis on a large dataset of 108M Turkish
election-related tweets along with the timeline tweets of 168k Turkish users,
who authored 213M tweets. Given the resultant user stances, we are able to
observe correlations between topics and compute topic polarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum circuit design for universal distribution using a superposition of classical automata. (arXiv:2006.00987v2 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.00987">
<div class="article-summary-box-inner">
<span><p>In this research, we present a quantum circuit design and implementation for
a parallel universal linear bounded automata. This circuit is able to
accelerate the inference of algorithmic structures in data for discovering
causal generative models. The computation model is practically restricted in
time and space resources. A classical exhaustive enumeration of all possible
programs on the automata is shown for a couple of example cases. The precise
quantum circuit design that allows executing a superposition of programs, along
with a superposition of inputs as in the standard quantum Turing machine
formulation, is presented. This is the first time, a superposition of classical
automata is implemented on the circuit model of quantum computation, having the
corresponding mechanistic parts of a classical Turing machine. The
superposition of programs allows our model to be used for experimenting with
the space of program-output behaviors in algorithmic information theory. Our
implementations on OpenQL and Qiskit quantum programming language is copy-left
and is publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Self-supervised Representation Learning of Sentence Structure for Authorship Attribution. (arXiv:2010.06786v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06786">
<div class="article-summary-box-inner">
<span><p>Syntactic structure of sentences in a document substantially informs about
its authorial writing style. Sentence representation learning has been widely
explored in recent years and it has been shown that it improves the
generalization of different downstream tasks across many domains. Even though
utilizing probing methods in several studies suggests that these learned
contextual representations implicitly encode some amount of syntax, explicit
syntactic information further improves the performance of deep neural models in
the domain of authorship attribution. These observations have motivated us to
investigate the explicit representation learning of syntactic structure of
sentences. In this paper, we propose a self-supervised framework for learning
structural representations of sentences. The self-supervised network contains
two components; a lexical sub-network and a syntactic sub-network which take
the sequence of words and their corresponding structural labels as the input,
respectively. Due to the n-to-1 mapping of words to their structural labels,
each word will be embedded into a vector representation which mainly carries
structural information. We evaluate the learned structural representations of
sentences using different probing tasks, and subsequently utilize them in the
authorship attribution task. Our experimental results indicate that the
structural embeddings significantly improve the classification tasks when
concatenated with the existing pre-trained word embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetGen: A Joint framework for Retrieval and Grounded Text Generation Modeling. (arXiv:2105.06597v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06597">
<div class="article-summary-box-inner">
<span><p>Recent advances in large-scale pre-training such as GPT-3 allow seemingly
high quality text to be generated from a given prompt. However, such generation
systems often suffer from problems of hallucinated facts, and are not
inherently designed to incorporate useful external information. Grounded
generation models appear to offer remedies, but their training typically relies
on rarely-available parallel data where information-relevant documents are
provided for context. We propose a framework that alleviates this data
constraint by jointly training a grounded generator and document retriever on
the language model signal. The model learns to reward retrieval of the
documents with the highest utility in generation, and attentively combines them
using a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We
demonstrate that both generator and retriever can take advantage of this joint
training and work synergistically to produce more informative and relevant text
in both prose and dialogue generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing. (arXiv:2110.07205v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07205">
<div class="article-summary-box-inner">
<span><p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in
pre-trained natural language processing models, we propose a unified-modal
SpeechT5 framework that explores the encoder-decoder pre-training for
self-supervised speech/text representation learning. The SpeechT5 framework
consists of a shared encoder-decoder network and six modal-specific
(speech/text) pre/post-nets. After preprocessing the input speech/text through
the pre-nets, the shared encoder-decoder network models the
sequence-to-sequence transformation, and then the post-nets generate the output
in the speech/text modality based on the output of the decoder. Leveraging
large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a
unified-modal representation, hoping to improve the modeling capability for
both speech and text. To align the textual and speech information into this
unified semantic space, we propose a cross-modal vector quantization approach
that randomly mixes up speech/text states with latent units as the interface
between encoder and decoder. Extensive evaluations show the superiority of the
proposed SpeechT5 framework on a wide variety of spoken language processing
tasks, including automatic speech recognition, speech synthesis, speech
translation, voice conversion, speech enhancement, and speaker identification.
We will release our code and model at https://github.com/microsoft/SpeechT5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying causal relations in tweets using deep learning: Use case on diabetes-related tweets from 2017-2021. (arXiv:2111.01225v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01225">
<div class="article-summary-box-inner">
<span><p>Objective: Leveraging machine learning methods, we aim to extract both
explicit and implicit cause-effect associations in patient-reported,
diabetes-related tweets and provide a tool to better understand opinion,
feelings and observations shared within the diabetes online community from a
causality perspective. Materials and Methods: More than 30 million
diabetes-related tweets in English were collected between April 2017 and
January 2021. Deep learning and natural language processing methods were
applied to focus on tweets with personal and emotional content. A
cause-effect-tweet dataset was manually labeled and used to train 1) a
fine-tuned Bertweet model to detect causal sentences containing a causal
association 2) a CRF model with BERT based features to extract possible
cause-effect associations. Causes and effects were clustered in a
semi-supervised approach and visualised in an interactive cause-effect-network.
Results: Causal sentences were detected with a recall of 68% in an imbalanced
dataset. A CRF model with BERT based features outperformed a fine-tuned BERT
model for cause-effect detection with a macro recall of 68%. This led to 96,676
sentences with cause-effect associations. "Diabetes" was identified as the
central cluster followed by "Death" and "Insulin". Insulin pricing related
causes were frequently associated with "Death". Conclusions: A novel
methodology was developed to detect causal sentences and identify both explicit
and implicit, single and multi-word cause and corresponding effect as expressed
in diabetes-related tweets leveraging BERT-based architectures and visualised
as cause-effect-network. Extracting causal associations on real-life, patient
reported outcomes in social media data provides a useful complementary source
of information in diabetes research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Step-unrolled Denoising Autoencoders for Text Generation. (arXiv:2112.06749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06749">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a new generative model of text, Step-unrolled
Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models.
Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a
sequence of tokens, starting from random inputs and improving them each time
until convergence. We present a simple new improvement operator that converges
in fewer iterations than diffusion methods, while qualitatively producing
better samples on natural language datasets. SUNDAE achieves state-of-the-art
results (among non-autoregressive methods) on the WMT'14 English-to-German
translation task and good qualitative results on unconditional language
modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python
code from GitHub. The non-autoregressive nature of SUNDAE opens up
possibilities beyond left-to-right prompted generation, by filling in arbitrary
blank patterns in a template.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Memorization Across Neural Language Models. (arXiv:2202.07646v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07646">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) have been shown to memorize parts of their
training data, and when prompted appropriately, they will emit the memorized
training data verbatim. This is undesirable because memorization violates
privacy (exposing user data), degrades utility (repeated easy-to-memorize text
is often low quality), and hurts fairness (some texts are memorized over
others).
</p>
<p>We describe three log-linear relationships that quantify the degree to which
LMs emit memorized training data. Memorization significantly grows as we
increase (1) the capacity of a model, (2) the number of times an example has
been duplicated, and (3) the number of tokens of context used to prompt the
model. Surprisingly, we find the situation becomes complicated when
generalizing these results across model families. On the whole, we find that
memorization in LMs is more prevalent than previously believed and will likely
get worse as models continues to scale, at least without active mitigations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09662">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models are able to generate fluent text and be
efficiently adapted across various natural language generation tasks. However,
language models that are pretrained on large unlabeled web text corpora have
been shown to suffer from degenerating toxic content and social bias behaviors,
consequently hindering their safe deployment. Various detoxification methods
were proposed to mitigate the language model's toxicity; however, these methods
struggled to detoxify language models when conditioned on prompts that contain
specific social identities related to gender, race, or religion. In this study,
we propose Reinforce-Detoxify; A reinforcement learning-based method for
mitigating toxicity in language models. We address the challenge of safety in
language models and propose a new reward model that is able to detect toxic
content and mitigate unintended bias towards social identities in toxicity
prediction. The experiments demonstrate that the Reinforce-Detoxify method for
language model detoxification outperforms existing detoxification approaches in
automatic evaluation metrics, indicating the ability of our approach in
language model detoxification and less prone to unintended bias toward social
identities in generated content.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation. (arXiv:2202.11742v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11742">
<div class="article-summary-box-inner">
<span><p>Following language instructions to navigate in unseen environments is a
challenging problem for autonomous embodied agents. The agent not only needs to
ground languages in visual scenes, but also should explore the environment to
reach its target. In this work, we propose a dual-scale graph transformer
(DUET) for joint long-term action planning and fine-grained cross-modal
understanding. We build a topological map on-the-fly to enable efficient
exploration in global action space. To balance the complexity of large action
space reasoning and fine-grained language grounding, we dynamically combine a
fine-scale encoding over local observations and a coarse-scale encoding on a
global map via graph transformers. The proposed approach, DUET, significantly
outperforms state-of-the-art methods on goal-oriented vision-and-language
navigation (VLN) benchmarks REVERIE and SOON. It also improves the success rate
on the fine-grained VLN benchmark R2R.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Image Super-Resolution Methods: A Survey. (arXiv:2202.11763v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11763">
<div class="article-summary-box-inner">
<span><p>Super-resolution (SR), the process of obtaining high-resolution images from
one or more low-resolution observations of the same scene, has been a very
popular topic of research in the last few decades in both signal processing and
image processing areas. Due to the recent developments in Convolutional Neural
Networks, the popularity of SR algorithms has skyrocketed as the barrier of
entry has been lowered significantly. Recently, this popularity has spread into
video processing areas to the lengths of developing SR models that work in
real-time. In this paper, we compare different SR models that specialize in
single image processing and will take a glance at how they evolved to take on
many different objectives and shapes over the years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When do GANs replicate? On the choice of dataset size. (arXiv:2202.11765v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11765">
<div class="article-summary-box-inner">
<span><p>Do GANs replicate training images? Previous studies have shown that GANs do
not seem to replicate training data without significant change in the training
procedure. This leads to a series of research on the exact condition needed for
GANs to overfit to the training data. Although a number of factors has been
theoretically or empirically identified, the effect of dataset size and
complexity on GANs replication is still unknown. With empirical evidence from
BigGAN and StyleGAN2, on datasets CelebA, Flower and LSUN-bedroom, we show that
dataset size and its complexity play an important role in GANs replication and
perceptual quality of the generated images. We further quantify this
relationship, discovering that replication percentage decays exponentially with
respect to dataset size and complexity, with a shared decaying factor across
GAN-dataset combinations. Meanwhile, the perceptual image quality follows a
U-shape trend w.r.t dataset size. This finding leads to a practical tool for
one-shot estimation on minimal dataset size to prevent GAN replication which
can be used to guide datasets construction and selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Multiple and Diverse Directions for Cognitive Image Properties. (arXiv:2202.11772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11772">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that it is possible to find interpretable
directions in the latent spaces of pre-trained GANs. These directions enable
controllable generation and support a variety of semantic editing operations.
While previous work has focused on discovering a single direction that performs
a desired editing operation such as zoom-in, limited work has been done on the
discovery of multiple and diverse directions that can achieve the desired edit.
In this work, we propose a novel framework that discovers multiple and diverse
directions for a given property of interest. In particular, we focus on the
manipulation of cognitive properties such as Memorability, Emotional Valence
and Aesthetics. We show with extensive experiments that our method successfully
manipulates these properties while producing diverse outputs. Our project page
and source code can be found at <a href="http://catlab-team.github.io/latentcognitive.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Art Creation with Multi-Conditional StyleGANs. (arXiv:2202.11777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11777">
<div class="article-summary-box-inner">
<span><p>Creating meaningful art is often viewed as a uniquely human endeavor. A human
artist needs a combination of unique skills, understanding, and genuine
intention to create artworks that evoke deep feelings and emotions. In this
paper, we introduce a multi-conditional Generative Adversarial Network (GAN)
approach trained on large amounts of human paintings to synthesize
realistic-looking paintings that emulate human art. Our approach is based on
the StyleGAN neural network architecture, but incorporates a custom
multi-conditional control mechanism that provides fine-granular control over
characteristics of the generated paintings, e.g., with regard to the perceived
emotion evoked in a spectator. For better control, we introduce the conditional
truncation trick, which adapts the standard truncation trick for the
conditional setting and diverse datasets. Finally, we develop a diverse set of
evaluation techniques tailored to multi-conditional generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification. (arXiv:2202.11781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11781">
<div class="article-summary-box-inner">
<span><p>In this work, we present RadioTransformer, a novel visual attention-driven
transformer framework, that leverages radiologists' gaze patterns and models
their visuo-cognitive behavior for disease diagnosis on chest radiographs.
Domain experts, such as radiologists, rely on visual information for medical
image interpretation. On the other hand, deep neural networks have demonstrated
significant promise in similar tasks even where visual interpretation is
challenging. Eye-gaze tracking has been used to capture the viewing behavior of
domain experts, lending insights into the complexity of visual search. However,
deep learning frameworks, even those that rely on attention mechanisms, do not
leverage this rich domain information. RadioTransformer fills this critical gap
by learning from radiologists' visual search patterns, encoded as 'human visual
attention regions' in a cascaded global-focal transformer framework. The
overall 'global' image characteristics and the more detailed 'local' features
are captured by the proposed global and focal modules, respectively. We
experimentally validate the efficacy of our student-teacher approach for 8
datasets involving different disease classification tasks where eye-gaze data
is not available during the inference phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nuclei panoptic segmentation and composition regression with multi-task deep neural networks. (arXiv:2202.11804v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11804">
<div class="article-summary-box-inner">
<span><p>Nuclear segmentation, classification and quantification within Haematoxylin &amp;
Eosin stained histology images enables the extraction of interpretable
cell-based features that can be used in downstream explainable models in
computational pathology. The Colon Nuclei Identification and Counting (CoNIC)
Challenge is held to help drive forward research and innovation for automatic
nuclei recognition in computational pathology. This report describes our
proposed method submitted to the CoNIC challenge. Our method employs a
multi-task learning framework, which performs a panoptic segmentation task and
a regression task. For the panoptic segmentation task, we use encoder-decoder
type deep neural networks predicting a direction map in addition to a
segmentation map in order to separate neighboring nuclei into different
instances
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A modification of the conjugate direction method for motion estimation. (arXiv:2202.11831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11831">
<div class="article-summary-box-inner">
<span><p>A comparative study of different block matching alternatives for motion
estimation is presented. The study is focused on computational burden and
objective measures on the accuracy of prediction. Together with existing
algorithms several new variations have been tested. An interesting modification
of the conjugate direction method previously related in literature is reported.
This new algorithm shows a good trade-off between computational complexity and
accuracy of motion vector estimation. Computational complexity is evaluated
using a sequence of artificial images designed to incorporate a great variety
of motion vectors. The performance of block matching methods has been measured
in terms of the entropy in the error signal between the motion compensated and
the original frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Near Perfect GAN Inversion. (arXiv:2202.11833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11833">
<div class="article-summary-box-inner">
<span><p>To edit a real photo using Generative Adversarial Networks (GANs), we need a
GAN inversion algorithm to identify the latent vector that perfectly reproduces
it. Unfortunately, whereas existing inversion algorithms can synthesize images
similar to real photos, they cannot generate the identical clones needed in
most applications. Here, we derive an algorithm that achieves near perfect
reconstructions of photos. Rather than relying on encoder- or
optimization-based methods to find an inverse mapping on a fixed generator
$G(\cdot)$, we derive an approach to locally adjust $G(\cdot)$ to more
optimally represent the photos we wish to synthesize. This is done by locally
tweaking the learned mapping $G(\cdot)$ s.t. $\| {\bf x} - G({\bf z})
\|&lt;\epsilon$, with ${\bf x}$ the photo we wish to reproduce, ${\bf z}$ the
latent vector, $\|\cdot\|$ an appropriate metric, and $\epsilon &gt; 0$ a small
scalar. We show that this approach can not only produce synthetic images that
are indistinguishable from the real photos we wish to replicate, but that these
images are readily editable. We demonstrate the effectiveness of the derived
algorithm on a variety of datasets including human faces, animals, and cars,
and discuss its importance for diversity and inclusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explanatory Paradigms in Neural Networks. (arXiv:2202.11838v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11838">
<div class="article-summary-box-inner">
<span><p>In this article, we present a leap-forward expansion to the study of
explainability in neural networks by considering explanations as answers to
abstract reasoning-based questions. With $P$ as the prediction from a neural
network, these questions are `Why P?', `What if not P?', and `Why P, rather
than Q?' for a given contrast prediction $Q$. The answers to these questions
are observed correlations, observed counterfactuals, and observed contrastive
explanations respectively. Together, these explanations constitute the
abductive reasoning scheme. We term the three explanatory schemes as observed
explanatory paradigms. The term observed refers to the specific case of
post-hoc explainability, when an explanatory technique explains the decision
$P$ after a trained neural network has made the decision $P$. The primary
advantage of viewing explanations through the lens of abductive reasoning-based
questions is that explanations can be used as reasons while making decisions.
The post-hoc field of explainability, that previously only justified decisions,
becomes active by being involved in the decision making process and providing
limited, but relevant and contextual interventions. The contributions of this
article are: ($i$) realizing explanations as reasoning paradigms, ($ii$)
providing a probabilistic definition of observed explanations and their
completeness, ($iii$) creating a taxonomy for evaluation of explanations, and
($iv$) positioning gradient-based complete explanainability's replicability and
reproducibility across multiple applications and data modalities, ($v$) code
repositories, publicly available at
https://github.com/olivesgatech/Explanatory-Paradigms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAISE: Conversational Agent for Image Search and Editing. (arXiv:2202.11847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11847">
<div class="article-summary-box-inner">
<span><p>Demand for image editing has been increasing as users' desire for expression
is also increasing. However, for most users, image editing tools are not easy
to use since the tools require certain expertise in photo effects and have
complex interfaces. Hence, users might need someone to help edit their images,
but having a personal dedicated human assistant for every user is impossible to
scale. For that reason, an automated assistant system for image editing is
desirable. Additionally, users want more image sources for diverse image
editing works, and integrating an image search functionality into the editing
tool is a potential remedy for this demand. Thus, we propose a dataset of an
automated Conversational Agent for Image Search and Editing (CAISE). To our
knowledge, this is the first dataset that provides conversational image search
and editing annotations, where the agent holds a grounded conversation with
users and helps them to search and edit images according to their requests. To
build such a system, we first collect image search and editing conversations
between pairs of annotators. The assistant-annotators are equipped with a
customized image search and editing tool to address the requests from the
user-annotators. The functions that the assistant-annotators conduct with the
tool are recorded as executable commands, allowing the trained system to be
useful for real-world application execution. We also introduce a
generator-extractor baseline model for this task, which can adaptively select
the source of the next token (i.e., from the vocabulary or from textual/visual
contexts) for the executable command. This serves as a strong starting point
while still leaving a large human-machine performance gap for useful future
work. Our code and dataset are publicly available at:
https://github.com/hyounghk/CAISE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11855">
<div class="article-summary-box-inner">
<span><p>We present a method to learn compositional predictive models from image
observations based on implicit object encoders, Neural Radiance Fields (NeRFs),
and graph neural networks. A central question in learning dynamic models from
sensor observations is on which representations predictions should be
performed. NeRFs have become a popular choice for representing scenes due to
their strong 3D prior. However, most NeRF approaches are trained on a single
scene, representing the whole scene with a global model, making generalization
to novel scenes, containing different numbers of objects, challenging. Instead,
we present a compositional, object-centric auto-encoder framework that maps
multiple views of the scene to a \emph{set} of latent vectors representing each
object separately. The latent vectors parameterize individual NeRF models from
which the scene can be reconstructed and rendered from novel viewpoints. We
train a graph neural network dynamics model in the latent space to achieve
compositionality for dynamics prediction. A key feature of our approach is that
the learned 3D information of the scene through the NeRF model enables us to
incorporate structural priors in learning the dynamics models, making long-term
predictions more stable. The model can further be used to synthesize new scenes
from individual object observations. For planning, we utilize RRTs in the
learned latent space, where we can exploit our model and the implicit object
encoder to make sampling the latent space informative and more efficient. In
the experiments, we show that the model outperforms several baselines on a
pushing task containing many objects. Video:
https://dannydriess.github.io/compnerfdyn/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CG-SSD: Corner Guided Single Stage 3D Object Detection from LiDAR Point Cloud. (arXiv:2202.11868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11868">
<div class="article-summary-box-inner">
<span><p>At present, the anchor-based or anchor-free models that use LiDAR point
clouds for 3D object detection use the center assigner strategy to infer the 3D
bounding boxes. However, in a real world scene, the LiDAR can only acquire a
limited object surface point clouds, but the center point of the object does
not exist. Obtaining the object by aggregating the incomplete surface point
clouds will bring a loss of accuracy in direction and dimension estimation. To
address this problem, we propose a corner-guided anchor-free single-stage 3D
object detection model (CG-SSD ).Firstly, 3D sparse convolution backbone
network composed of residual layers and sub-manifold sparse convolutional
layers are used to construct bird's eye view (BEV) features for further deeper
feature mining by a lite U-shaped network; Secondly, a novel corner-guided
auxiliary module (CGAM) is proposed to incorporate corner supervision signals
into the neural network. CGAM is explicitly designed and trained to detect
partially visible and invisible corners to obtains a more accurate object
feature representation, especially for small or partial occluded objects;
Finally, the deep features from both the backbone networks and CGAM module are
concatenated and fed into the head module to predict the classification and 3D
bounding boxes of the objects in the scene. The experiments demonstrate CG-SSD
achieves the state-of-art performance on the ONCE benchmark for supervised 3D
object detection using single frame point cloud data, with 62.77%mAP.
Additionally, the experiments on ONCE and Waymo Open Dataset show that CGAM can
be extended to most anchor-based models which use the BEV feature to detect
objects, as a plug-in and bring +1.17%-+14.27%AP improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">New Benchmark for Household Garbage Image Recognition. (arXiv:2202.11878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11878">
<div class="article-summary-box-inner">
<span><p>Household garbage images are usually faced with complex backgrounds, variable
illuminations, diverse angles, and changeable shapes, which bring a great
difficulty in garbage image classification. Due to the ability to discover
problem-specific features, deep learning and especially convolutional neural
networks (CNNs) have been successfully and widely used for image representation
learning. However, available and stable household garbage datasets are
insufficient, which seriously limits the development of research and
application. Besides, the state of the art in the field of garbage image
classification is not entirely clear. To solve this problem, in this study, we
built a new open benchmark dataset for household garbage image classification
by simulating different lightings, backgrounds, angles, and shapes. This
dataset is named 30 Classes of Household Garbage Images (HGI-30), which
contains 18,000 images of 30 household garbage classes. The publicly available
HGI-30 dataset allows researchers to develop accurate and robust methods for
household garbage recognition. We also conducted experiments and performance
analysis of the state-of-the-art deep CNN methods on HGI-30, which serves as
baseline results on this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Note on Machine Learning Approach for Computational Imaging. (arXiv:2202.11883v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11883">
<div class="article-summary-box-inner">
<span><p>Computational imaging has been playing a vital role in the development of
natural sciences. Advances in sensory, information, and computer technologies
have further extended the scope of influence of imaging, making digital images
an essential component of our daily lives. For the past three decades, we have
witnessed phenomenal developments of mathematical and machine learning methods
in computational imaging. In this note, we will review some of the recent
developments of the machine learning approach for computational imaging and
discuss its differences and relations to the mathematical approach. We will
demonstrate how we may combine the wisdom from both approaches, discuss the
merits and potentials of such a combination and present some of the new
computational and theoretical challenges it brings about.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2I: From Factored Marginal Trajectory Prediction to Interactive Prediction. (arXiv:2202.11884v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11884">
<div class="article-summary-box-inner">
<span><p>Predicting future motions of road participants is an important task for
driving autonomously in urban scenes. Existing models excel at predicting
marginal trajectories for single agents, yet it remains an open question to
jointly predict scene compliant trajectories over multiple agents. The
challenge is due to exponentially increasing prediction space as a function of
the number of agents. In this work, we exploit the underlying relations between
interacting agents and decouple the joint prediction problem into marginal
prediction problems. Our proposed approach M2I first classifies interacting
agents as pairs of influencers and reactors, and then leverages a marginal
prediction model and a conditional prediction model to predict trajectories for
the influencers and reactors, respectively. The predictions from interacting
agents are combined and selected according to their joint likelihoods.
Experiments show that our simple but effective approach achieves
state-of-the-art performance on the Waymo Open Motion Dataset interactive
prediction benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A spectral-spatial fusion anomaly detection method for hyperspectral imagery. (arXiv:2202.11889v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11889">
<div class="article-summary-box-inner">
<span><p>In hyperspectral, high-quality spectral signals convey subtle spectral
differences to distinguish similar materials, thereby providing unique
advantage for anomaly detection. Hence fine spectra of anomalous pixels can be
effectively screened out from heterogeneous background pixels. Since the same
materials have similar characteristics in spatial and spectral dimension,
detection performance can be significantly enhanced by jointing spatial and
spectral information. In this paper, a spectralspatial fusion anomaly detection
(SSFAD) method is proposed for hyperspectral imagery. First, original spectral
signals are mapped to a local linear background space composed of median and
mean with high confidence, where saliency weight and feature enhancement
strategies are implemented to obtain an initial detection map in spectral
domain. Futhermore, to make full use of similarity information of local
background around testing pixel, a new detector is designed to extract the
local similarity spatial features of patch images in spatial domain. Finally,
anomalies are detected by adaptively combining the spectral and spatial
detection maps. The experimental results demonstrate that our proposed method
has superior detection performance than traditional methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HMD-EgoPose: Head-Mounted Display-Based Egocentric Marker-Less Tool and Hand Pose Estimation for Augmented Surgical Guidance. (arXiv:2202.11891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11891">
<div class="article-summary-box-inner">
<span><p>The success or failure of modern computer-assisted surgery procedures hinges
on the precise six-degree-of-freedom (6DoF) position and orientation (pose)
estimation of tracked instruments and tissue. In this paper, we present
HMD-EgoPose, a single-shot learning-based approach to hand and object pose
estimation and demonstrate state-of-the-art performance on a benchmark dataset
for monocular red-green-blue (RGB) 6DoF marker-less hand and surgical
instrument pose tracking. Further, we reveal the capacity of our HMD-EgoPose
framework for 6DoF near real-time pose estimation on a commercially available
optical see-through head-mounted display (OST-HMD) through a low-latency
streaming approach. Our framework utilized an efficient convolutional neural
network (CNN) backbone for multi-scale feature extraction and a set of
subnetworks to jointly learn the 6DoF pose representation of the rigid surgical
drill instrument and the grasping orientation of the hand of a user. To make
our approach accessible to a commercially available OST-HMD, the Microsoft
HoloLens 2, we created a pipeline for low-latency video and data communication
with a high-performance computing workstation capable of optimized network
inference. HMD-EgoPose outperformed current state-of-the-art approaches on a
benchmark dataset for surgical tool pose estimation, achieving an average tool
3D vertex error of 11.0 mm on real data and furthering the progress towards a
clinically viable marker-free tracking strategy. Through our low-latency
streaming approach, we achieved a round trip latency of 202.5 ms for pose
estimation and augmented visualization of the tracked model when integrated
with the OST-HMD. Our single-shot learned approach was robust to occlusion and
complex surfaces and improved on current state-of-the-art approaches to
marker-less tool and hand pose estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Memorability of Face Images. (arXiv:2202.11896v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11896">
<div class="article-summary-box-inner">
<span><p>Everyday, we are bombarded with many photographs of faces, whether on social
media, television, or smartphones. From an evolutionary perspective, faces are
intended to be remembered, mainly due to survival and personal relevance.
However, all these faces do not have the equal opportunity to stick in our
minds. It has been shown that memorability is an intrinsic feature of an image
but yet, it is largely unknown what attributes make an image more memorable. In
this work, we aimed to address this question by proposing a fast approach to
modify and control the memorability of face images. In our proposed method, we
first found a hyperplane in the latent space of StyleGAN to separate high and
low memorable images. We then modified the image memorability (while
maintaining the identity and other facial features such as age, emotion, etc.)
by moving in the positive or negative direction of this hyperplane normal
vector. We further analyzed how different layers of the StyleGAN augmented
latent space contribute to face memorability. These analyses showed how each
individual face attribute makes an image more or less memorable. Most
importantly, we evaluated our proposed method for both real and synthesized
face images. The proposed method successfully modifies and controls the
memorability of real human faces as well as unreal synthesized faces. Our
proposed method can be employed in photograph editing applications for social
media, learning aids, or advertisement purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Robustness of Convolutional Neural Networks Using Element-Wise Activation Scaling. (arXiv:2202.11898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11898">
<div class="article-summary-box-inner">
<span><p>Recent works reveal that re-calibrating the intermediate activation of
adversarial examples can improve the adversarial robustness of a CNN model. The
state of the arts [Baiet al., 2021] and [Yanet al., 2021] explores this feature
at the channel level, i.e. the activation of a channel is uniformly scaled by a
factor. In this paper, we investigate the intermediate activation manipulation
at a more fine-grained level. Instead of uniformly scaling the activation, we
individually adjust each element within an activation and thus propose
Element-Wise Activation Scaling, dubbed EWAS, to improve CNNs' adversarial
robustness. Experimental results on ResNet-18 and WideResNet with CIFAR10 and
SVHN show that EWAS significantly improves the robustness accuracy. Especially
for ResNet18 on CIFAR10, EWAS increases the adversarial accuracy by 37.65% to
82.35% against C&amp;W attack. EWAS is simple yet very effective in terms of
improving robustness. The codes are anonymously available at
https://anonymous.4open.science/r/EWAS-DD64.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SLRNet: Semi-Supervised Semantic Segmentation Via Label Reuse for Human Decomposition Images. (arXiv:2202.11900v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11900">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a challenging computer vision task demanding a
significant amount of pixel-level annotated data. Producing such data is a
time-consuming and costly process, especially for domains with a scarcity of
experts, such as medicine or forensic anthropology. While numerous
semi-supervised approaches have been developed to make the most from the
limited labeled data and ample amount of unlabeled data, domain-specific
real-world datasets often have characteristics that both reduce the
effectiveness of off-the-shelf state-of-the-art methods and also provide
opportunities to create new methods that exploit these characteristics. We
propose and evaluate a semi-supervised method that reuses available labels for
unlabeled images of a dataset by exploiting existing similarities, while
dynamically weighting the impact of these reused labels in the training
process. We evaluate our method on a large dataset of human decomposition
images and find that our method, while conceptually simple, outperforms
state-of-the-art consistency and pseudo-labeling-based methods for the
segmentation of this dataset. This paper includes graphic content of human
decomposition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-driven Planner for Exploration and Navigation. (arXiv:2202.11907v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11907">
<div class="article-summary-box-inner">
<span><p>We consider the problems of exploration and point-goal navigation in
previously unseen environments, where the spatial complexity of indoor scenes
and partial observability constitute these tasks challenging. We argue that
learning occupancy priors over indoor maps provides significant advantages
towards addressing these problems. To this end, we present a novel planning
framework that first learns to generate occupancy maps beyond the field-of-view
of the agent, and second leverages the model uncertainty over the generated
areas to formulate path selection policies for each task of interest. For
point-goal navigation the policy chooses paths with an upper confidence bound
policy for efficient and traversable paths, while for exploration the policy
maximizes model uncertainty over candidate paths. We perform experiments in the
visually realistic environments of Matterport3D using the Habitat simulator and
demonstrate: 1) Improved results on exploration and map quality metrics over
competitive methods, and 2) The effectiveness of our planning module when
paired with the state-of-the-art DD-PPO method for the point-goal navigation
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Transformer Meets Robotic Grasping: Exploits Context for Efficient Grasp Detection. (arXiv:2202.11911v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11911">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a transformer-based architecture, namely TF-Grasp,
for robotic grasp detection. The developed TF-Grasp framework has two elaborate
designs making it well suitable for visual grasping tasks. The first key design
is that we adopt the local window attention to capture local contextual
information and detailed features of graspable objects. Then, we apply the
cross window attention to model the long-term dependencies between distant
pixels. Object knowledge, environmental configuration, and relationships
between different visual entities are aggregated for subsequent grasp
detection. The second key design is that we build a hierarchical
encoder-decoder architecture with skip-connections, delivering shallow features
from encoder to decoder to enable a multi-scale feature fusion. Due to the
powerful attention mechanism, the TF-Grasp can simultaneously obtain the local
information (i.e., the contours of objects), and model long-term connections
such as the relationships between distinct visual concepts in clutter.
Extensive computational experiments demonstrate that the TF-Grasp achieves
superior results versus state-of-art grasping convolutional models and attain a
higher accuracy of 97.99% and 94.6% on Cornell and Jacquard grasping datasets,
respectively. Real-world experiments using a 7DoF Franka Emika Panda robot also
demonstrate its capability of grasping unseen objects in a variety of
scenarios. The code and pre-trained models will be available at
https://github.com/WangShaoSUN/grasp-transformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpolation-based Contrastive Learning for Few-Label Semi-Supervised Learning. (arXiv:2202.11915v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11915">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) has long been proved to be an effective
technique to construct powerful models with limited labels. In the existing
literature, consistency regularization-based methods, which force the perturbed
samples to have similar predictions with the original ones have attracted much
attention for their promising accuracy. However, we observe that, the
performance of such methods decreases drastically when the labels get extremely
limited, e.g., 2 or 3 labels for each category. Our empirical study finds that
the main problem lies with the drifting of semantic information in the
procedure of data augmentation. The problem can be alleviated when enough
supervision is provided. However, when little guidance is available, the
incorrect regularization would mislead the network and undermine the
performance of the algorithm. To tackle the problem, we (1) propose an
interpolation-based method to construct more reliable positive sample pairs;
(2) design a novel contrastive loss to guide the embedding of the learned
network to change linearly between samples so as to improve the discriminative
capability of the network by enlarging the margin decision boundaries. Since no
destructive regularization is introduced, the performance of our proposed
algorithm is largely improved. Specifically, the proposed algorithm outperforms
the second best algorithm (Comatch) with 5.3% by achieving 88.73%
classification accuracy when only two labels are available for each class on
the CIFAR-10 dataset. Moreover, we further prove the generality of the proposed
method by improving the performance of the existing state-of-the-art algorithms
considerably with our proposed strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-scaling Vision Transformers without Training. (arXiv:2202.11921v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11921">
<div class="article-summary-box-inner">
<span><p>This work targets automated designing and scaling of Vision Transformers
(ViTs). The motivation comes from two pain spots: 1) the lack of efficient and
principled methods for designing and scaling ViTs; 2) the tremendous
computational cost of training ViT that is much heavier than its convolution
counterpart. To tackle these issues, we propose As-ViT, an auto-scaling
framework for ViTs without training, which automatically discovers and scales
up ViTs in an efficient and principled manner. Specifically, we first design a
"seed" ViT topology by leveraging a training-free search process. This
extremely fast search is fulfilled by a comprehensive study of ViT's network
complexity, yielding a strong Kendall-tau correlation with ground-truth
accuracies. Second, starting from the "seed" topology, we automate the scaling
rule for ViTs by growing widths/depths to different ViT layers. This results in
a series of architectures with different numbers of parameters in a single run.
Finally, based on the observation that ViTs can tolerate coarse tokenization in
early training stages, we propose a progressive tokenization strategy to train
ViTs faster and cheaper. As a unified framework, As-ViT achieves strong
performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7%
mAP on COCO) without any manual crafting nor scaling of ViT architectures: the
end-to-end model design and scaling process cost only 12 hours on one V100 GPU.
Our code is available at https://github.com/VITA-Group/AsViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Aided Diagnosis and Out-of-Distribution Detection in Glaucoma Screening Using Color Fundus Photography. (arXiv:2202.11944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11944">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence for RObust Glaucoma Screening (AIROGS) Challenge is
held for developing solutions for glaucoma screening from color fundus
photography that are robust to real-world scenarios. This report describes our
method submitted to the AIROGS challenge. Our method employs convolutional
neural networks to classify input images to "referable glaucoma" or "no
referable glaucoma". In addition, we introduce an inference-time
out-of-distribution (OOD) detection method to identify ungradable images. Our
OOD detection is based on an energy-based method combined with activation
rectification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Disentangled Generative Adversarial Network for Zero-Shot Sketch-Based 3D Shape Retrieval. (arXiv:2202.11948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11948">
<div class="article-summary-box-inner">
<span><p>Sketch-based 3D shape retrieval is a challenging task due to the large domain
discrepancy between sketches and 3D shapes. Since existing methods are trained
and evaluated on the same categories, they cannot effectively recognize the
categories that have not been used during training. In this paper, we propose a
novel domain disentangled generative adversarial network (DD-GAN) for zero-shot
sketch-based 3D retrieval, which can retrieve the unseen categories that are
not accessed during training. Specifically, we first generate domain-invariant
features and domain-specific features by disentangling the learned features of
sketches and 3D shapes, where the domain-invariant features are used to align
with the corresponding word embeddings. Then, we develop a generative
adversarial network that combines the domainspecific features of the seen
categories with the aligned domain-invariant features to synthesize samples,
where the synthesized samples of the unseen categories are generated by using
the corresponding word embeddings. Finally, we use the synthesized samples of
the unseen categories combined with the real samples of the seen categories to
train the network for retrieval, so that the unseen categories can be
recognized. In order to reduce the domain shift between the synthesized domain
and the real domain, we adopt the transductive setting to reduce the gap
between the distributions of the synthesized unseen categories and real unseen
categories. Extensive experiments on the SHREC'13 and SHREC'14 datasets show
that our method significantly improves the retrieval performance of the unseen
categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMILE: Sequence-to-Sequence Domain Adaption with Minimizing Latent Entropy for Text Image Recognition. (arXiv:2202.11949v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11949">
<div class="article-summary-box-inner">
<span><p>Training recognition models with synthetic images have achieved remarkable
results in text recognition. However, recognizing text from real-world images
still faces challenges due to the domain shift between synthetic and real-world
text images. One of the strategies to eliminate the domain difference without
manual annotation is unsupervised domain adaptation (UDA). Due to the
characteristic of sequential labeling tasks, most popular UDA methods cannot be
directly applied to text recognition. To tackle this problem, we proposed a UDA
method with minimizing latent entropy on sequence-to-sequence attention-based
models with classbalanced self-paced learning. Our experiments show that our
proposed framework achieves better recognition results than the existing
methods on most UDA text recognition benchmarks. All codes are publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Self-Supervised Learning for Semantic Segmentation. (arXiv:2202.11981v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11981">
<div class="article-summary-box-inner">
<span><p>In this work, we present a fully self-supervised framework for semantic
segmentation(FS^4). A fully bootstrapped strategy for semantic segmentation,
which saves efforts for the huge amount of annotation, is crucial for building
customized models from end-to-end for open-world domains. This application is
eagerly needed in realistic scenarios. Even though recent self-supervised
semantic segmentation methods have gained great progress, these works however
heavily depend on the fully-supervised pretrained model and make it impossible
a fully self-supervised pipeline. To solve this problem, we proposed a
bootstrapped training scheme for semantic segmentation, which fully leveraged
the global semantic knowledge for self-supervision with our proposed PGG
strategy and CAE module. In particular, we perform pixel clustering and
assignments for segmentation supervision. Preventing it from clustering a mess,
we proposed 1) a pyramid-global-guided (PGG) training strategy to supervise the
learning with pyramid image/patch-level pseudo labels, which are generated by
grouping the unsupervised features. The stable global and pyramid semantic
pseudo labels can prevent the segmentation from learning too many clutter
regions or degrading to one background region; 2) in addition, we proposed
context-aware embedding (CAE) module to generate global feature embedding in
view of its neighbors close both in space and appearance in a non-trivial way.
We evaluate our method on the large-scale COCO-Stuff dataset and achieved 7.19
mIoU improvements on both things and stuff objects
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-QGN: Navigation Map from a Monocular Camera using Quadtree Generating Networks. (arXiv:2202.11982v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11982">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation has been a popular area of research for several
years, especially since self-supervised networks have shown increasingly good
results in bridging the gap with supervised and stereo methods. However, these
approaches focus their interest on dense 3D reconstruction and sometimes on
tiny details that are superfluous for autonomous navigation. In this paper, we
propose to address this issue by estimating the navigation map under a quadtree
representation. The objective is to create an adaptive depth map prediction
that only extract details that are essential for the obstacle avoidance. Other
3D space which leaves large room for navigation will be provided with
approximate distance. Experiment on KITTI dataset shows that our method can
significantly reduce the number of output information without major loss of
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIAOTracker: A comprehensive framework for MCMOT with global information and optimizing strategies in VisDrone 2021. (arXiv:2202.11983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11983">
<div class="article-summary-box-inner">
<span><p>In recent years, algorithms for multiple object tracking tasks have benefited
from great progresses in deep models and video quality. However, in challenging
scenarios like drone videos, they still suffer from problems, such as small
objects, camera movements and view changes. In this paper, we propose a new
multiple object tracker, which employs Global Information And some Optimizing
strategies, named GIAOTracker. It consists of three stages, i.e., online
tracking, global link and post-processing. Given detections in every frame, the
first stage generates reliable tracklets using information of camera motion,
object motion and object appearance. Then they are associated into trajectories
by exploiting global clues and refined through four post-processing methods.
With the effectiveness of the three stages, GIAOTracker achieves
state-of-the-art performance on the VisDrone MOT dataset and wins the 3rd place
in the VisDrone2021 MOT Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Actor-centric Human-object Interaction Detection. (arXiv:2202.11998v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11998">
<div class="article-summary-box-inner">
<span><p>While Human-Object Interaction(HOI) Detection has achieved tremendous
advances in recent, it still remains challenging due to complex interactions
with multiple humans and objects occurring in images, which would inevitably
lead to ambiguities. Most existing methods either generate all human-object
pair candidates and infer their relationships by cropped local features
successively in a two-stage manner, or directly predict interaction points in a
one-stage procedure. However, the lack of spatial configurations or reasoning
steps of two- or one- stage methods respectively limits their performance in
such complex scenes. To avoid this ambiguity, we propose a novel actor-centric
framework. The main ideas are that when inferring interactions: 1) the
non-local features of the entire image guided by actor position are obtained to
model the relationship between the actor and context, and then 2) we use an
object branch to generate pixel-wise interaction area prediction, where the
interaction area denotes the object central area. Moreover, we also use an
actor branch to get interaction prediction of the actor and propose a novel
composition strategy based on center-point indexing to generate the final HOI
prediction. Thanks to the usage of the non-local features and the
partly-coupled property of the human-objects composition strategy, our proposed
framework can detect HOI more accurately especially for complex images.
Extensive experimental results show that our method achieves the
state-of-the-art on the challenging V-COCO and HICO-DET benchmarks and is more
robust especially in multiple persons and/or objects scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rare Gems: Finding Lottery Tickets at Initialization. (arXiv:2202.12002v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12002">
<div class="article-summary-box-inner">
<span><p>It has been widely observed that large neural networks can be pruned to a
small fraction of their original size, with little loss in accuracy, by
typically following a time-consuming "train, prune, re-train" approach. Frankle
&amp; Carbin (2018) conjecture that we can avoid this by training lottery tickets,
i.e., special sparse subnetworks found at initialization, that can be trained
to high accuracy. However, a subsequent line of work presents concrete evidence
that current algorithms for finding trainable networks at initialization, fail
simple baseline comparisons, e.g., against training random sparse subnetworks.
Finding lottery tickets that train to better accuracy compared to simple
baselines remains an open problem. In this work, we partially resolve this open
problem by discovering rare gems: subnetworks at initialization that attain
considerable accuracy, even before training. Refining these rare gems - "by
means of fine-tuning" - beats current baselines and leads to accuracy
competitive or better than magnitude pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Merge Tokens in Vision Transformers. (arXiv:2202.12015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12015">
<div class="article-summary-box-inner">
<span><p>Transformers are widely applied to solve natural language understanding and
computer vision tasks. While scaling up these architectures leads to improved
performance, it often comes at the expense of much higher computational costs.
In order for large-scale models to remain practical in real-world systems,
there is a need for reducing their computational overhead. In this work, we
present the PatchMerger, a simple module that reduces the number of patches or
tokens the network has to process by merging them between two consecutive
intermediate layers. We show that the PatchMerger achieves a significant
speedup across various model sizes while matching the original performance both
upstream and downstream after fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing generalisability of deep learning-based polyp detection and segmentation methods through a computer vision challenge. (arXiv:2202.12031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12031">
<div class="article-summary-box-inner">
<span><p>Polyps are well-known cancer precursors identified by colonoscopy. However,
variability in their size, location, and surface largely affect identification,
localisation, and characterisation. Moreover, colonoscopic surveillance and
removal of polyps (referred to as polypectomy ) are highly operator-dependent
procedures. There exist a high missed detection rate and incomplete removal of
colonic polyps due to their variable nature, the difficulties to delineate the
abnormality, the high recurrence rates, and the anatomical topography of the
colon. There have been several developments in realising automated methods for
both detection and segmentation of these polyps using machine learning.
However, the major drawback in most of these methods is their ability to
generalise to out-of-sample unseen datasets that come from different centres,
modalities and acquisition systems. To test this hypothesis rigorously we
curated a multi-centre and multi-population dataset acquired from multiple
colonoscopy systems and challenged teams comprising machine learning experts to
develop robust automated detection and segmentation methods as part of our
crowd-sourcing Endoscopic computer vision challenge (EndoCV) 2021. In this
paper, we analyse the detection results of the four top (among seven) teams and
the segmentation results of the five top teams (among 16). Our analyses
demonstrate that the top-ranking teams concentrated on accuracy (i.e., accuracy
&gt; 80% on overall Dice score on different validation sets) over real-time
performance required for clinical applicability. We further dissect the methods
and provide an experiment-based hypothesis that reveals the need for improved
generalisability to tackle diversity present in multi-centre datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFFDEX 2.0: A Real-Time Facial Expression Analysis Toolkit. (arXiv:2202.12059v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12059">
<div class="article-summary-box-inner">
<span><p>In this paper we introduce AFFDEX 2.0 - a toolkit for analyzing facial
expressions in the wild, that is, it is intended for users aiming to; a)
estimate the 3D head pose, b) detect facial Action Units (AUs), c) recognize
basic emotions and 2 new emotional states (sentimentality and confusion), and
d) detect high-level expressive metrics like blink and attention. AFFDEX 2.0
models are mainly based on Deep Learning, and are trained using a large-scale
naturalistic dataset consisting of thousands of participants from different
demographic groups. AFFDEX 2.0 is an enhanced version of our previous toolkit
[1], that is capable of tracking efficiently faces at more challenging
conditions, detecting more accurately facial expressions, and recognizing new
emotional states (sentimentality and confusion). AFFDEX 2.0 can process
multiple faces in real time, and is working across the Windows and Linux
platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase-Based Affordance Detection via Cyclic Bilateral Interaction. (arXiv:2202.12076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12076">
<div class="article-summary-box-inner">
<span><p>Affordance detection, which refers to perceiving objects with potential
action possibilities in images, is a challenging task since the possible
affordance depends on the person's purpose in real-world application scenarios.
The existing works mainly extract the inherent human-object dependencies from
image/video to accommodate affordance properties that change dynamically. In
this paper, we explore to perceive affordance from a vision-language
perspective and consider the challenging phrase-based affordance detection
problem,i.e., given a set of phrases describing the action purposes, all the
object regions in a scene with the same affordance should be detected. To this
end, we propose a cyclic bilateral consistency enhancement network (CBCE-Net)
to align language and vision features progressively. Specifically, the
presented CBCE-Net consists of a mutual guided vision-language module that
updates the common features of vision and language in a progressive manner, and
a cyclic interaction module (CIM) that facilitates the perception of possible
interaction with objects in a cyclic manner. In addition, we extend the public
Purpose-driven Affordance Dataset (PAD) by annotating affordance categories
with short phrases. The contrastive experimental results demonstrate the
superiority of our method over nine typical methods from four relevant fields
in terms of both objective metrics and visual quality. The related code and
dataset will be released at \url{https://github.com/lulsheng/CBCE-Net}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data variation-aware medical image segmentation. (arXiv:2202.12099v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12099">
<div class="article-summary-box-inner">
<span><p>Deep learning algorithms have become the golden standard for segmentation of
medical imaging data. In most works, the variability and heterogeneity of real
clinical data is acknowledged to still be a problem. One way to automatically
overcome this is to capture and exploit this variation explicitly. Here, we
propose an approach that improves on our previous work in this area and explain
how it potentially can improve clinical acceptance of (semi-)automatic
segmentation methods. In contrast to a standard neural network that produces
one segmentation, we propose to use a multi-pathUnet network that produces
multiple segmentation variants, presumably corresponding to the variations that
reside in the dataset. Different paths of the network are trained on disjoint
data subsets. Because a priori it may be unclear what variations exist in the
data, the subsets should be automatically determined. This is achieved by
searching for the best data partitioning with an evolutionary optimization
algorithm. Because each network path can become more specialized when trained
on a more homogeneous data subset, better segmentation quality can be achieved.
In practical usage, various automatically produced segmentations can be
presented to a medical expert, from which the preferred segmentation can be
selected. In experiments with a real clinical dataset of CT scans with prostate
segmentations, our approach provides an improvement of several percentage
points in terms of Dice and surface Dice coefficients compared to when all
network paths are trained on all training data. Noticeably, the largest
improvement occurs in the upper part of the prostate that is known to be most
prone to inter-observer segmentation variation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepFusionMOT: A 3D Multi-Object Tracking Framework Based on Camera-LiDAR Fusion with Deep Association. (arXiv:2202.12100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12100">
<div class="article-summary-box-inner">
<span><p>In the recent literature, on the one hand, many 3D multi-object tracking
(MOT) works have focused on tracking accuracy and neglected computation speed,
commonly by designing rather complex cost functions and feature extractors. On
the other hand, some methods have focused too much on computation speed at the
expense of tracking accuracy. In view of these issues, this paper proposes a
robust and fast camera-LiDAR fusion-based MOT method that achieves a good
trade-off between accuracy and speed. Relying on the characteristics of camera
and LiDAR sensors, an effective deep association mechanism is designed and
embedded in the proposed MOT method. This association mechanism realizes
tracking of an object in a 2D domain when the object is far away and only
detected by the camera, and updating of the 2D trajectory with 3D information
obtained when the object appears in the LiDAR field of view to achieve a smooth
fusion of 2D and 3D trajectories. Extensive experiments based on the KITTI
dataset indicate that our proposed method presents obvious advantages over the
state-of-the-art MOT methods in terms of both tracking accuracy and processing
speed. Our code is made publicly available for the benefit of the community
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-based Network for Deformable Medical Image Registration. (arXiv:2202.12104v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12104">
<div class="article-summary-box-inner">
<span><p>Deformable medical image registration plays an important role in clinical
diagnosis and treatment. Recently, the deep learning (DL) based image
registration methods have been widely investigated and showed excellent
performance in computational speed. However, these methods cannot provide
enough registration accuracy because of insufficient ability in representing
both the global and local features of the moving and fixed images. To address
this issue, this paper has proposed the transformer based image registration
method. This method uses the distinctive transformer to extract the global and
local image features for generating the deformation fields, based on which the
registered image is produced in an unsupervised way. Our method can improve the
registration accuracy effectively by means of self-attention mechanism and
bi-level information flow. Experimental results on such brain MR image datasets
as LPBA40 and OASIS-1 demonstrate that compared with several traditional and DL
based registration methods, our method provides higher registration accuracy in
terms of dice values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Robust Monocular Depth Estimation For Outdoor Environment Via Monochrome And Color Camera Fusion. (arXiv:2202.12108v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12108">
<div class="article-summary-box-inner">
<span><p>Depth estimation plays a important role in SLAM, odometry, and autonomous
driving. Especially, monocular depth estimation is profitable technology
because of its low cost, memory, and computation. However, it is not a
sufficiently predicting depth map due to a camera often failing to get a clean
image because of light conditions. To solve this problem, various sensor fusion
method has been proposed. Even though it is a powerful method, sensor fusion
requires expensive sensors, additional memory, and high computational
performance.
</p>
<p>In this paper, we present color image and monochrome image pixel-level fusion
and stereo matching with partially enhanced correlation coefficient
maximization. Our methods not only outperform the state-of-the-art works across
all metrics but also efficient in terms of cost, memory, and computation. We
also validate the effectiveness of our design with an ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slow-Fast Visual Tempo Learning for Video-based Action Recognition. (arXiv:2202.12116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12116">
<div class="article-summary-box-inner">
<span><p>Action visual tempo characterizes the dynamics and the temporal scale of an
action, which is helpful to distinguish human actions that share high
similarities in visual dynamics and appearance. Previous methods capture the
visual tempo either by sampling raw videos with multiple rates, which requires
a costly multi-layer network to handle each rate, or by hierarchically sampling
backbone features, which relies heavily on high-level features that miss
fine-grained temporal dynamics. In this work, we propose a Temporal Correlation
Module (TCM), which can be easily embedded into the current action recognition
backbones in a plug-in-and-play manner, to extract action visual tempo from
low-level backbone features at single-layer remarkably. Specifically, our TCM
contains two main components: a Multi-scale Temporal Dynamics Module (MTDM) and
a Temporal Attention Module (TAM). MTDM applies a correlation operation to
learn pixel-wise fine-grained temporal dynamics for both fast-tempo and
slow-tempo. TAM adaptively emphasizes expressive features and suppresses
inessential ones via analyzing the global information across various tempos.
Extensive experiments conducted on several action recognition benchmarks, e.g.
Something-Something V1 &amp; V2, Kinetics-400, UCF-101, and HMDB-51, have
demonstrated that the proposed TCM is effective to promote the performance of
the existing video-based action recognition models for a large margin. The
source code is publicly released at https://github.com/zphyix/TCM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel unsupervised covid lung lesion segmentation based on the lung tissue identification. (arXiv:2202.12148v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12148">
<div class="article-summary-box-inner">
<span><p>This study aimed to evaluate the performance of a novel unsupervised deep
learning-based framework for automated infections lesion segmentation from CT
images of Covid patients. In the first step, two residual networks were
independently trained to identify the lung tissue for normal and Covid patients
in a supervised manner. These two models, referred to as DL-Covid and DL-Norm
for Covid-19 and normal patients, respectively, generate the voxel-wise
probability maps for lung tissue identification. To detect Covid lesions, the
CT image of the Covid patient is processed by the DL-Covid and DL-Norm models
to obtain two lung probability maps. Since the DL-Norm model is not familiar
with Covid infections within the lung, this model would assign lower
probabilities to the lesions than the DL-Covid. Hence, the probability maps of
the Covid infections could be generated through the subtraction of the two lung
probability maps obtained from the DL-Covid and DL-Norm models. Manual lesion
segmentation of 50 Covid-19 CT images was used to assess the accuracy of the
unsupervised lesion segmentation approach. The Dice coefficients of 0.985 and
0.978 were achieved for the lung segmentation of normal and Covid patients in
the external validation dataset, respectively. Quantitative results of
infection segmentation by the proposed unsupervised method showed the Dice
coefficient and Jaccard index of 0.67 and 0.60, respectively. Quantitative
evaluation of the proposed unsupervised approach for Covid-19 infectious lesion
segmentation showed relatively satisfactory results. Since this framework does
not require any annotated dataset, it could be used to generate very large
training samples for the supervised machine learning algorithms dedicated to
noisy and/or weakly annotated datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12154">
<div class="article-summary-box-inner">
<span><p>Trojan attacks on deep neural networks are both dangerous and surreptitious.
Over the past few years, Trojan attacks have advanced from using only a simple
trigger and targeting only one class to using many sophisticated triggers and
targeting multiple classes. However, Trojan defenses have not caught up with
this development. Most defense methods still make out-of-date assumptions about
Trojan triggers and target classes, thus, can be easily circumvented by modern
Trojan attacks. In this paper, we advocate general defenses that are effective
and robust against various Trojan attacks and propose two novel "filtering"
defenses with these characteristics called Variational Input Filtering (VIF)
and Adversarial Input Filtering (AIF). VIF and AIF leverage variational
inference and adversarial training respectively to purify all potential Trojan
triggers in the input at run time without making any assumption about their
numbers and forms. We further extend "filtering" to
"filtering-then-contrasting" - a new defense mechanism that helps avoid the
drop in classification accuracy on clean data caused by filtering. Extensive
experimental results show that our proposed defenses significantly outperform 4
well-known defenses in mitigating 5 different Trojan attacks including the two
state-of-the-art which defeat many strong defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring CLEVRness: Blackbox testing of Visual Reasoning Models. (arXiv:2202.12162v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12162">
<div class="article-summary-box-inner">
<span><p>How can we measure the reasoning capabilities of intelligence systems? Visual
question answering provides a convenient framework for testing the model's
abilities by interrogating the model through questions about the scene.
However, despite scores of various visual QA datasets and architectures, which
sometimes yield even a super-human performance, the question of whether those
architectures can actually reason remains open to debate. To answer this, we
extend the visual question answering framework and propose the following
behavioral test in the form of a two-player game. We consider black-box neural
models of CLEVR. These models are trained on a diagnostic dataset benchmarking
reasoning. Next, we train an adversarial player that re-configures the scene to
fool the CLEVR model. We show that CLEVR models, which otherwise could perform
at a human level, can easily be fooled by our agent. Our results put in doubt
whether data-driven approaches can do reasoning without exploiting the numerous
biases that are often present in those datasets. Finally, we also propose a
controlled experiment measuring the efficiency of such models to learn and
perform reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in Medical Image Analysis: A Review. (arXiv:2202.12165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12165">
<div class="article-summary-box-inner">
<span><p>Transformers have dominated the field of natural language processing, and
recently impacted the computer vision area. In the field of medical image
analysis, Transformers have also been successfully applied to full-stack
clinical applications, including image synthesis/reconstruction, registration,
segmentation, detection, and diagnosis. Our paper presents both a position
paper and a primer, promoting awareness and application of Transformers in the
field of medical image analysis. Specifically, we first overview the core
concepts of the attention mechanism built into Transformers and other basic
components. Second, we give a new taxonomy of various Transformer architectures
tailored for medical image applications and discuss their limitations. Within
this review, we investigate key challenges revolving around the use of
Transformers in different learning paradigms, improving the model efficiency,
and their coupling with other techniques. We hope this review can give a
comprehensive picture of Transformers to the readers in the field of medical
image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FreeSOLO: Learning to Segment Objects without Annotations. (arXiv:2202.12181v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12181">
<div class="article-summary-box-inner">
<span><p>Instance segmentation is a fundamental vision task that aims to recognize and
segment each object in an image. However, it requires costly annotations such
as bounding boxes and segmentation masks for learning. In this work, we propose
a fully unsupervised learning method that learns class-agnostic instance
segmentation without any annotations. We present FreeSOLO, a self-supervised
instance segmentation framework built on top of the simple instance
segmentation method SOLO. Our method also presents a novel localization-aware
pre-training framework, where objects can be discovered from complicated scenes
in an unsupervised manner. FreeSOLO achieves 9.8% AP_{50} on the challenging
COCO dataset, which even outperforms several segmentation proposal methods that
use manual annotations. For the first time, we demonstrate unsupervised
class-agnostic instance segmentation successfully. FreeSOLO's box localization
significantly outperforms state-of-the-art unsupervised object
detection/discovery methods, with about 100% relative improvements in COCO AP.
FreeSOLO further demonstrates superiority as a strong pre-training method,
outperforming state-of-the-art self-supervised pre-training methods by +9.8% AP
when fine-tuning instance segmentation with only 5% COCO masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Distilled StyleGAN: Towards Generation from Internet Photos. (arXiv:2202.12211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12211">
<div class="article-summary-box-inner">
<span><p>StyleGAN is known to produce high-fidelity images, while also offering
unprecedented semantic editing. However, these fascinating abilities have been
demonstrated only on a limited set of datasets, which are usually structurally
aligned and well curated. In this paper, we show how StyleGAN can be adapted to
work on raw uncurated images collected from the Internet. Such image
collections impose two main challenges to StyleGAN: they contain many outlier
images, and are characterized by a multi-modal distribution. Training StyleGAN
on such raw image collections results in degraded image synthesis quality. To
meet these challenges, we proposed a StyleGAN-based self-distillation approach,
which consists of two main components: (i) A generative-based self-filtering of
the dataset to eliminate outlier images, in order to generate an adequate
training set, and (ii) Perceptual clustering of the generated images to detect
the inherent data modalities, which are then employed to improve StyleGAN's
"truncation trick" in the image synthesis process. The presented technique
enables the generation of high-quality images, while minimizing the loss in
diversity of the data. Through qualitative and quantitative evaluation, we
demonstrate the power of our approach to new challenging and diverse domains
collected from the Internet. New datasets and pre-trained models are available
at https://self-distilled-stylegan.github.io/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparative study of in-air trajectories at short and long distances in online handwriting. (arXiv:2202.12237v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12237">
<div class="article-summary-box-inner">
<span><p>Introduction Existing literature about online handwriting analysis to support
pathology diagnosis has taken advantage of in-air trajectories. A similar
situation occurred in biometric security applications where the goal is to
identify or verify an individual using his signature or handwriting. These
studies do not consider the distance of the pen tip to the writing surface.
This is due to the fact that current acquisition devices do not provide height
formation. However, it is quite straightforward to differentiate movements at
two different heights: a) short distance: height lower or equal to 1 cm above a
surface of digitizer, the digitizer provides x and y coordinates. b) long
distance: height exceeding 1 cm, the only information available is a time stamp
that indicates the time that a specific stroke has spent at long distance.
Although short distance has been used in several papers, long distances have
been ignored and will be investigated in this paper. Methods In this paper, we
will analyze a large set of databases (BIOSECURID, EMOTHAW, PaHaW,
Oxygen-Therapy and SALT), which contain a total amount of 663 users and 17951
files. We have specifically studied: a) the percentage of time spent
on-surface, in-air at short distance, and in-air at long distance for different
user profiles (pathological and healthy users) and different tasks; b) The
potential use of these signals to improve classification rates. Results and
conclusions Our experimental results reveal that long-distance movements
represent a very small portion of the total execution time (0.5 % in the case
of signatures and 10.4% for uppercase words of BIOSECUR-ID, which is the
largest database). In addition, significant differences have been found in the
comparison of pathological versus control group for letter l in PaHaW database
(p=0.0157) and crossed pentagons in SALT database (p=0.0122)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-line signature verification system with failure to enroll managing. (arXiv:2202.12242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12242">
<div class="article-summary-box-inner">
<span><p>In this paper we simulate a real biometric verification system based on
on-line signatures. For this purpose we have split the MCYT signature database
in three subsets: one for classifier training, another for system adjustment
and a third one for system testing simulating enrollment and verification. This
context corresponds to a real operation, where a new user tries to enroll an
existing system and must be automatically guided by the system in order to
detect the failure to enroll situations. The main contribution of this work is
the management of failure to enroll situations by means of a new proposal,
called intelligent enrollment, which consists of consistency checking in order
to automatically reject low quality samples. This strategy lets to enhance the
verification errors up to 22% when leaving out 8% of the users. In this
situation 8% of the people cannot be enrolled in the system and must be
verified by other biometrics or by human abilities. These people are identified
with intelligent enrollment and the situation can be thus managed. In addition
we also propose a DCT-based feature extractor with threshold coding and
discriminability criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMOTHAW: A novel database for emotional state recognition from handwriting. (arXiv:2202.12245v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12245">
<div class="article-summary-box-inner">
<span><p>The detection of negative emotions through daily activities such as
handwriting is useful for promoting well-being. The spread of human-machine
interfaces such as tablets makes the collection of handwriting samples easier.
In this context, we present a first publicly available handwriting database
which relates emotional states to handwriting, that we call EMOTHAW. This
database includes samples of 129 participants whose emotional states, namely
anxiety, depression and stress, are assessed by the Depression Anxiety Stress
Scales (DASS) questionnaire. Seven tasks are recorded through a digitizing
tablet: pentagons and house drawing, words copied in handprint, circles and
clock drawing, and one sentence copied in cursive writing. Records consist in
pen positions, on-paper and in-air, time stamp, pressure, pen azimuth and
altitude. We report our analysis on this database. From collected data, we
first compute measurements related to timing and ductus. We compute separate
measurements according to the position of the writing device: on paper or
in-air. We analyse and classify this set of measurements (referred to as
features) using a random forest approach. This latter is a machine learning
method [2], based on an ensemble of decision trees, which includes a feature
ranking process. We use this ranking process to identify the features which
best reveal a targeted emotional state.
</p>
<p>We then build random forest classifiers associated to each emotional state.
Our results, obtained from cross-validation experiments, show that the targeted
emotional states can be identified with accuracies ranging from 60% to 71%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLPnet: A new DNN model and Bengali OCR engine for Automatic License Plate Recognition. (arXiv:2202.12250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12250">
<div class="article-summary-box-inner">
<span><p>The development of the Automatic License Plate Recognition (ALPR) system has
received much attention for the English license plate. However, despite being
the sixth largest population around the world, no significant progress can be
tracked in the Bengali language countries or states for the ALPR system
addressing their more alarming traffic management with inadequate road-safety
measures. This paper reports a computationally efficient and reasonably
accurate Automatic License Plate Recognition (ALPR) system for Bengali
characters with a new end-to-end DNN model that we call Bengali License Plate
Network(BLPnet). The cascaded architecture for detecting vehicle regions prior
to vehicle license plate (VLP) in the model is proposed to eliminate false
positives resulting in higher detection accuracy of VLP. Besides, a lower set
of trainable parameters is considered for reducing the computational cost
making the system faster and more compatible for a real-time application. With
a Computational Neural Network (CNN)based new Bengali OCR engine and
word-mapping process, the model is characters rotation invariant, and can
readily extract, detect and output the complete license plate number of a
vehicle. The model feeding with17 frames per second (fps) on real-time video
footage can detect a vehicle with the Mean Squared Error (MSE) of 0.0152, and
the mean license plate character recognition accuracy of 95%. While compared to
the other models, an improvement of 5% and 20% were recorded for the BLPnetover
the prominent YOLO-based ALPR model and the Tesseract model for the
number-plate detection accuracy and time requirement, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISDA: Position-Aware Instance Segmentation with Deformable Attention. (arXiv:2202.12251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12251">
<div class="article-summary-box-inner">
<span><p>Most instance segmentation models are not end-to-end trainable due to either
the incorporation of proposal estimation (RPN) as a pre-processing or
non-maximum suppression (NMS) as a post-processing. Here we propose a novel
end-to-end instance segmentation method termed ISDA. It reshapes the task into
predicting a set of object masks, which are generated via traditional
convolution operation with learned position-aware kernels and features of
objects. Such kernels and features are learned by leveraging a deformable
attention network with multi-scale representation. Thanks to the introduced
set-prediction mechanism, the proposed method is NMS-free. Empirically, ISDA
outperforms Mask R-CNN (the strong baseline) by 2.6 points on MS-COCO, and
achieves leading performance compared with recent models. Code will be
available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Method for Waste Segregation using Convolutional Neural Networks. (arXiv:2202.12258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12258">
<div class="article-summary-box-inner">
<span><p>Segregation of garbage is a primary concern in many nations across the world.
Even though we are in the modern era, many people still do not know how to
distinguish between organic and recyclable waste. It is because of this that
the world is facing a major crisis of waste disposal. In this paper, we try to
use deep learning algorithms to help solve this problem of waste
classification. The waste is classified into two categories like organic and
recyclable. Our proposed model achieves an accuracy of 94.9%. Although the
other two models also show promising results, the Proposed Model stands out
with the greatest accuracy. With the help of deep learning, one of the greatest
obstacles to efficient waste management can finally be removed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from the Pros: Extracting Professional Goalkeeper Technique from Broadcast Footage. (arXiv:2202.12259v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12259">
<div class="article-summary-box-inner">
<span><p>As an amateur goalkeeper playing grassroots soccer, who better to learn from
than top professional goalkeepers? In this paper, we harness computer vision
and machine learning models to appraise the save technique of professionals in
a way those at lower levels can learn from. We train an unsupervised machine
learning model using 3D body pose data extracted from broadcast footage to
learn professional goalkeeper technique. Then, an "expected saves" model is
developed, from which we can identify the optimal goalkeeper technique in
different match contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inflation of test accuracy due to data leakage in deep learning-based classification of OCT images. (arXiv:2202.12267v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12267">
<div class="article-summary-box-inner">
<span><p>In the application of deep learning on optical coherence tomography (OCT)
data, it is common to train classification networks using 2D images originating
from volumetric data. Given the micrometer resolution of OCT systems,
consecutive images are often very similar in both visible structures and noise.
Thus, an inappropriate data split can result in overlap between the training
and testing sets, with a large portion of the literature overlooking this
aspect. In this study, the effect of improper dataset splitting on model
evaluation is demonstrated for two classification tasks using two OCT
open-access datasets extensively used in the literature, Kermany's
ophthalmology dataset and AIIMS breast tissue dataset. Our results show that
the classification accuracy is inflated by 3.9 to 26 percentage units for
models tested on a dataset with improper splitting, highlighting the
considerable effect of dataset handling on model evaluation. This study intends
to raise awareness on the importance of dataset splitting for research on deep
learning using OCT data and volumetric data in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Feature Attribution Methods in the Image Domain. (arXiv:2202.12270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12270">
<div class="article-summary-box-inner">
<span><p>Feature attribution maps are a popular approach to highlight the most
important pixels in an image for a given prediction of a model. Despite a
recent growth in popularity and available methods, little attention is given to
the objective evaluation of such attribution maps. Building on previous work in
this domain, we investigate existing metrics and propose new variants of
metrics for the evaluation of attribution maps. We confirm a recent finding
that different attribution metrics seem to measure different underlying
concepts of attribution maps, and extend this finding to a larger selection of
attribution metrics. We also find that metric results on one dataset do not
necessarily generalize to other datasets, and methods with desirable
theoretical properties such as DeepSHAP do not necessarily outperform
computationally cheaper alternatives. Based on these findings, we propose a
general benchmarking approach to identify the ideal feature attribution method
for a given use case. Implementations of attribution metrics and our
experiments are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorizer: A Scalable Interpretable Approach to Context Modeling for Medical Image Segmentation. (arXiv:2202.12295v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12295">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) with U-shaped architectures have
dominated medical image segmentation, which is crucial for various clinical
purposes. However, the inherent locality of convolution makes CNNs fail to
fully exploit global context, essential for better recognition of some
structures, e.g., brain lesions. Transformers have recently proved promising
performance on vision tasks, including semantic segmentation, mainly due to
their capability of modeling long-range dependencies. Nevertheless, the
quadratic complexity of attention makes existing Transformer-based models use
self-attention layers only after somehow reducing the image resolution, which
limits the ability to capture global contexts present at higher resolutions.
Therefore, this work introduces a family of models, dubbed Factorizer, which
leverages the power of low-rank matrix factorization for constructing an
end-to-end segmentation model. Specifically, we propose a linearly scalable
approach to context modeling, formulating Nonnegative Matrix Factorization
(NMF) as a differentiable layer integrated into a U-shaped architecture. The
shifted window technique is also utilized in combination with NMF to
effectively aggregate local information. Factorizers compete favorably with
CNNs and Transformers in terms of accuracy, scalability, and interpretability,
achieving state-of-the-art results on the BraTS dataset for brain tumor
segmentation, with Dice scores of 79.33%, 83.14%, and 90.16% for enhancing
tumor, tumor core, and whole tumor, respectively. Highly meaningful NMF
components give an additional interpretability advantage to Factorizers over
CNNs and Transformers. Moreover, our ablation studies reveal a distinctive
feature of Factorizers that enables a significant speed-up in inference for a
trained Factorizer without any extra steps and without sacrificing much
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSGCNet: A Pyramidal Scale and Global Context Guided Network for Dense Object Counting in Remote Sensing Images. (arXiv:2012.03597v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03597">
<div class="article-summary-box-inner">
<span><p>Object counting, which aims to count the accurate number of object instances
in images, has been attracting more and more attention. However, challenges
such as large scale variation, complex background interference, and non-uniform
density distribution greatly limit the counting accuracy, particularly striking
in remote sensing imagery. To mitigate the above issues, this paper proposes a
novel framework for dense object counting in remote sensing images, which
incorporates a pyramidal scale module (PSM) and a global context module (GCM),
dubbed PSGCNet, where PSM is used to adaptively capture multi-scale information
and GCM is to guide the model to select suitable scales generated from PSM.
Moreover, a reliable supervision manner improved from Bayesian and Counting
loss (BCL) is utilized to learn the density probability and then compute the
count expectation at each annotation. It can relieve non-uniform density
distribution to a certain extent. Extensive experiments on four remote sensing
counting datasets demonstrate the effectiveness of the proposed method and the
superiority of it compared with state-of-the-arts. Additionally, experiments
extended on four commonly used crowd counting datasets further validate the
generalization ability of the model. Code is available at
https://github.com/gaoguangshuai/PSGCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLAVR: Flow-Agnostic Video Representations for Fast Frame Interpolation. (arXiv:2012.08512v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08512">
<div class="article-summary-box-inner">
<span><p>A majority of methods for video frame interpolation compute bidirectional
optical flow between adjacent frames of a video, followed by a suitable warping
algorithm to generate the output frames. However, approaches relying on optical
flow often fail to model occlusions and complex non-linear motions directly
from the video and introduce additional bottlenecks unsuitable for widespread
deployment. We address these limitations with FLAVR, a flexible and efficient
architecture that uses 3D space-time convolutions to enable end-to-end learning
and inference for video frame interpolation. Our method efficiently learns to
reason about non-linear motions, complex occlusions and temporal abstractions,
resulting in improved performance on video interpolation, while requiring no
additional inputs in the form of optical flow or depth maps. Due to its
simplicity, FLAVR can deliver 3x faster inference speed compared to the current
most accurate method on multi-frame interpolation without losing interpolation
accuracy. In addition, we evaluate FLAVR on a wide range of challenging
settings and consistently demonstrate superior qualitative and quantitative
results compared with prior methods on various popular benchmarks including
Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR
for video frame interpolation can serve as a useful self-supervised pretext
task for action recognition, optical flow estimation, and motion magnification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09716">
<div class="article-summary-box-inner">
<span><p>Identifying the status of individual network units is critical for
understanding the mechanism of convolutional neural networks (CNNs). However,
it is still challenging to reliably give a general indication of unit status,
especially for units in different network models. To this end, we propose a
novel method for quantitatively clarifying the status of single unit in CNN
using algebraic topological tools. Unit status is indicated via the calculation
of a defined topological-based entropy, called feature entropy, which measures
the degree of chaos of the global spatial pattern hidden in the unit for a
category. In this way, feature entropy could provide an accurate indication of
status for units in different networks with diverse situations like
weight-rescaling operation. Further, we show that feature entropy decreases as
the layer goes deeper and shares almost simultaneous trend with loss during
training. We show that by investigating the feature entropy of units on only
training data, it could give discrimination between networks with different
generalization ability from the view of the effectiveness of feature
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Unsupervised Diversity Denoising and Artefact Removal. (arXiv:2104.01374v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01374">
<div class="article-summary-box-inner">
<span><p>Image denoising and artefact removal are complex inverse problems admitting
multiple valid solutions. Unsupervised diversity restoration, that is,
obtaining a diverse set of possible restorations given a corrupted image, is
important for ambiguity removal in many applications such as microscopy where
paired data for supervised training are often unobtainable. In real world
applications, imaging noise and artefacts are typically hard to model, leading
to unsatisfactory performance of existing unsupervised approaches. This work
presents an interpretable approach for unsupervised and diverse image
restoration. To this end, we introduce a capable architecture called
Hierarchical DivNoising (HDN) based on hierarchical Variational Autoencoder. We
show that HDN learns an interpretable multi-scale representation of artefacts
and we leverage this interpretability to remove imaging artefacts commonly
occurring in microscopy data. Our method achieves state-of-the-art results on
twelve benchmark image denoising datasets while providing access to a whole
distribution of sensibly restored solutions. Additionally, we demonstrate on
three real microscopy datasets that HDN removes artefacts without supervision,
being the first method capable of doing so while generating multiple plausible
restorations all consistent with the given corrupted image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error. (arXiv:2105.13343v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13343">
<div class="article-summary-box-inner">
<span><p>In computer vision, it is standard practice to draw a single sample from the
data augmentation procedure for each unique image in the mini-batch. However
recent work has suggested drawing multiple samples can achieve higher test
accuracies. In this work, we provide a detailed empirical evaluation of how the
number of augmentation samples per unique image influences model performance on
held out data when training deep ResNets. We demonstrate drawing multiple
samples per image consistently enhances the test accuracy achieved for both
small and large batch training. Crucially, this benefit arises even if
different numbers of augmentations per image perform the same number of
parameter updates and gradient evaluations (requiring the same total compute).
Although prior work has found variance in the gradient estimate arising from
subsampling the dataset has an implicit regularization benefit, our experiments
suggest variance which arises from the data augmentation process harms
generalization. We apply these insights to the highly performant NFNet-F5,
achieving 86.8$\%$ top-1 w/o extra data on ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid mmWave and Camera System for Long-Range Depth Imaging. (arXiv:2106.07856v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07856">
<div class="article-summary-box-inner">
<span><p>mmWave radars offer excellent depth resolution even at very long ranges owing
to their high bandwidth. But their angular resolution is at least an
order-of-magnitude worse than camera and lidar systems. Hence, mmWave radar is
not a capable 3-D imaging solution in isolation. We propose Metamoran, a system
that combines the complimentary strengths of radar and camera to obtain
accurate, high resolution depth images over long ranges even in high clutter
environments, all from a single fixed vantage point. Metamoran enables rich
long-range depth imaging with applications in security and surveillance,
roadside safety infrastructure and wide-area mapping. Our approach leverages
the high angular resolution from cameras using computer vision techniques,
including image segmentation and monocular depth estimation, to obtain object
shape. Our core contribution is a method to convert this object shape into an
RF I/Q equivalent, which we use in a novel radar processing pipeline to help
declutter the scene and capture extremely weak reflections from objects at long
distances. We perform a detailed evaluation of Metamoran's depth imaging
capabilities in 400 diverse scenes. Our evaluation shows that Metamoran
estimates the depth of static objects up to 90 m and moving objects up to 305 m
and with a median error of 28 cm, an improvement of 13$\times$ compared to a
naive radar+camera baseline and 23$\times$ compared to monocular depth
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics perception in sloshing scenes with guaranteed thermodynamic consistency. (arXiv:2106.13301v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13301">
<div class="article-summary-box-inner">
<span><p>Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Importance-aware Transferable Adversarial Attacks. (arXiv:2107.14185v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14185">
<div class="article-summary-box-inner">
<span><p>Transferability of adversarial examples is of central importance for
attacking an unknown model, which facilitates adversarial attacks in more
practical scenarios, e.g., black-box attacks. Existing transferable attacks
tend to craft adversarial examples by indiscriminately distorting features to
degrade prediction accuracy in a source model without aware of intrinsic
features of objects in the images. We argue that such brute-force degradation
would introduce model-specific local optimum into adversarial examples, thus
limiting the transferability. By contrast, we propose the Feature
Importance-aware Attack (FIA), which disrupts important object-aware features
that dominate model decisions consistently. More specifically, we obtain
feature importance by introducing the aggregate gradient, which averages the
gradients with respect to feature maps of the source model, computed on a batch
of random transforms of the original clean image. The gradients will be highly
correlated to objects of interest, and such correlation presents invariance
across different models. Besides, the random transforms will preserve intrinsic
features of objects and suppress model-specific information. Finally, the
feature importance guides to search for adversarial examples towards disrupting
critical features, achieving stronger transferability. Extensive experimental
evaluation demonstrates the effectiveness and superior performance of the
proposed FIA, i.e., improving the success rate by 9.5% against normally trained
models and 12.8% against defense models as compared to the state-of-the-art
transferable attacks. Code is available at: https://github.com/hcguoO0/FIA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v4 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05539">
<div class="article-summary-box-inner">
<span><p>Brain-inspired computation and information processing alongside compatibility
with neuromorphic hardware have made spiking neural networks (SNN) a promising
method for solving learning tasks in machine learning (ML). However, the mere
use of spiking neurons is not sufficient for building models that mimic the
learning mechanisms in the biological brain; network architecture and learning
rules are important factors to consider when developing such artificial agents.
In this work, inspired by the human visual pathway and the role of dopamine in
learning, we propose a reward-modulated locally connected spiking neural
network, BioLCNet, for visual learning tasks. To extract visual features from
Poisson-distributed spike trains, we used local filters that are more analogous
to the biological visual system compared to convolutional filters with weight
sharing. In the decoding layer, we applied a spike population-based voting
scheme to determine the decision of the network. We employed
Spike-timing-dependent plasticity (STDP) for learning the visual features, and
its reward-modulated variant (R-STDP) for training the decoder based on the
reward or punishment feedback signal. For evaluation, we first assessed the
robustness of our rewarding mechanism to varying target responses in a
classical conditioning experiment. Afterwards, we evaluated the performance of
our network on image classification tasks of MNIST and XOR MNIST datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised domain adaptation for cross-modality liver segmentation via joint adversarial learning and self-learning. (arXiv:2109.05664v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05664">
<div class="article-summary-box-inner">
<span><p>Liver segmentation on images acquired using computed tomography (CT) and
magnetic resonance imaging (MRI) plays an important role in clinical management
of liver diseases. Compared to MRI, CT images of liver are more abundant and
readily available. However, MRI can provide richer quantitative information of
the liver compared to CT. Thus, it is desirable to achieve unsupervised domain
adaptation for transferring the learned knowledge from the source domain
containing labeled CT images to the target domain containing unlabeled MR
images. In this work, we report a novel unsupervised domain adaptation
framework for cross-modality liver segmentation via joint adversarial learning
and self-learning. We propose joint semantic-aware and shape-entropy-aware
adversarial learning with post-situ identification manner to implicitly align
the distribution of task-related features extracted from the target domain with
those from the source domain. In proposed framework, a network is trained with
the above two adversarial losses in an unsupervised manner, and then a mean
completer of pseudo-label generation is employed to produce pseudo-labels to
train the next network (desired model). Additionally, semantic-aware
adversarial learning and two self-learning methods, including pixel-adaptive
mask refinement and student-to-partner learning, are proposed to train the
desired model. To improve the robustness of the desired model, a low-signal
augmentation function is proposed to transform MRI images as the input of the
desired model to handle hard samples. Using the public data sets, our
experiments demonstrated the proposed unsupervised domain adaptation framework
reached four supervised learning methods with a Dice score 0.912 plus or minus
0.037 (mean plus or minus standard deviation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CT-ICP: Real-time Elastic LiDAR Odometry with Loop Closure. (arXiv:2109.12979v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12979">
<div class="article-summary-box-inner">
<span><p>Multi-beam LiDAR sensors are increasingly used in robotics, particularly with
autonomous cars for localization and perception tasks, both relying on the
ability to build a precise map of the environment. For this, we propose a new
real-time LiDAR-only odometry method called CT-ICP (for Continuous-Time ICP),
completed into a full SLAM with a novel loop detection procedure. The core of
this method, is the introduction of the combined continuity in the scan
matching, and discontinuity between scans. It allows both the elastic
distortion of the scan during the registration for increased precision, and the
increased robustness to high frequency motions from the discontinuity.
</p>
<p>We build a complete SLAM on top of this odometry, using a fast pure LiDAR
loop detection based on elevation image 2D matching, providing a pose graph
with loop constraints. To show the robustness of the method, we tested it on
seven datasets: KITTI, KITTI-raw, KITTI-360, KITTI-CARLA, ParisLuco, Newer
College, and NCLT in driving and high-frequency motion scenarios. Both the
CT-ICP odometry and the loop detection are made available online. CT-ICP is
currently first, among those giving access to a public code, on the KITTI
odometry leaderboard, with an average Relative Translation Error (RTE) of 0.59%
and an average time per scan of 60ms on a CPU with a single thread.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Score-based diffusion models for accelerated MRI. (arXiv:2110.05243v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05243">
<div class="article-summary-box-inner">
<span><p>Score-based diffusion models provide a powerful way to model images using the
gradient of the data distribution. Leveraging the learned score function as a
prior, here we introduce a way to sample data from a conditional distribution
given the measurements, such that the model can be readily used for solving
inverse problems in imaging, especially for accelerated MRI. In short, we train
a continuous time-dependent score function with denoising score matching. Then,
at the inference stage, we iterate between numerical SDE solver and data
consistency projection step to achieve reconstruction. Our model requires
magnitude images only for training, and yet is able to reconstruct
complex-valued data, and even extends to parallel imaging. The proposed method
is agnostic to sub-sampling patterns, and can be used with any sampling
schemes. Also, due to its generative nature, our approach can quantify
uncertainty, which is not possible with standard regression settings. On top of
all the advantages, our method also has very strong performance, even beating
the models trained with full supervision. With extensive experiments, we verify
the superiority of our method in terms of quality and practicality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2020 CATARACTS Semantic Segmentation Challenge. (arXiv:2110.10965v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10965">
<div class="article-summary-box-inner">
<span><p>Surgical scene segmentation is essential for anatomy and instrument
localization which can be further used to assess tissue-instrument interactions
during a surgical procedure. In 2017, the Challenge on Automatic Tool
Annotation for cataRACT Surgery (CATARACTS) released 50 cataract surgery videos
accompanied by instrument usage annotations. These annotations included
frame-level instrument presence information. In 2020, we released pixel-wise
semantic annotations for anatomy and instruments for 4670 images sampled from
25 videos of the CATARACTS training set. The 2020 CATARACTS Semantic
Segmentation Challenge, which was a sub-challenge of the 2020 MICCAI Endoscopic
Vision (EndoVis) Challenge, presented three sub-tasks to assess participating
solutions on anatomical structure and instrument segmentation. Their
performance was assessed on a hidden test set of 531 images from 10 videos of
the CATARACTS test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INTERN: A New Learning Paradigm Towards General Vision. (arXiv:2111.08687v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08687">
<div class="article-summary-box-inner">
<span><p>Enormous waves of technological innovations over the past several years,
marked by the advances in AI technologies, are profoundly reshaping the
industry and the society. However, down the road, a key challenge awaits us,
that is, our capability of meeting rapidly-growing scenario-specific demands is
severely limited by the cost of acquiring a commensurate amount of training
data. This difficult situation is in essence due to limitations of the
mainstream learning paradigm: we need to train a new model for each new
scenario, based on a large quantity of well-annotated data and commonly from
scratch. In tackling this fundamental problem, we move beyond and develop a new
learning paradigm named INTERN. By learning with supervisory signals from
multiple sources in multiple stages, the model being trained will develop
strong generalizability. We evaluate our model on 26 well-known datasets that
cover four categories of tasks in computer vision. In most cases, our models,
adapted with only 10% of the training data in the target domain, outperform the
counterparts trained with the full set of data, often by a significant margin.
This is an important step towards a promising prospect where such a model with
general vision capability can dramatically reduce our reliance on data, thus
expediting the adoption of AI technologies. Furthermore, revolving around our
new paradigm, we also introduce a new data system, a new architecture, and a
new benchmark, which, together, form a general vision ecosystem to support its
future development in an open and inclusive manner. See project website at
https://opengvlab.shlab.org.cn .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lebanon Solar Rooftop Potential Assessment using Buildings Segmentation from Aerial Images. (arXiv:2111.11397v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11397">
<div class="article-summary-box-inner">
<span><p>Estimating the solar rooftop potential of buildings' rooftops at a large
scale is a fundamental step for every country to utilize its solar power
efficiently. However, such estimation becomes time-consuming and costly if done
through on-site measurements. This paper uses deep learning-based multi-class
instance segmentation to extract buildings' footprints from satellite images.
Hence, we introduce Lebanon's first complete and comprehensive buildings'
footprints map. Furthermore, we propose a photovoltaic panels placement
algorithm to estimate the solar potential of every rooftop, which results in
Lebanon's first buildings' solar rooftop potential map too. Finally, we report
total and average solar rooftop potential per district and localize regions
corresponding to the highest solar rooftop potential yield. Conducted analysis
reveal solar rooftop potential urban patterns and provide policymakers and key
stakeholders with tangible insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-aware Monocular Depth Prediction with Instance Convolutions. (arXiv:2112.01521v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01521">
<div class="article-summary-box-inner">
<span><p>With the advent of deep learning, estimating depth from a single RGB image
has recently received a lot of attention, being capable of empowering many
different applications ranging from path planning for robotics to computational
cinematography. Nevertheless, while the depth maps are in their entirety fairly
reliable, the estimates around object discontinuities are still far from
satisfactory. This can be contributed to the fact that the convolutional
operator naturally aggregates features across object discontinuities, resulting
in smooth transitions rather than clear boundaries. Therefore, in order to
circumvent this issue, we propose a novel convolutional operator which is
explicitly tailored to avoid feature aggregation of different object parts. In
particular, our method is based on estimating per-part depth values by means of
superpixels. The proposed convolutional operator, which we dub "Instance
Convolution", then only considers each object part individually on the basis of
the estimated superpixels. Our evaluation with respect to the NYUv2 as well as
the iBims dataset clearly demonstrates the superiority of Instance Convolutions
over the classical convolution at estimating depth around occlusion boundaries,
while producing comparable results elsewhere. Code will be made publicly
available upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGM3D: Stereo Guided Monocular 3D Object Detection. (arXiv:2112.01914v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01914">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection aims to predict the object location, dimension
and orientation in 3D space alongside the object category given only a
monocular image. It poses a great challenge due to its ill-posed property which
is critically lack of depth information in the 2D image plane. While there
exist approaches leveraging off-the-shelve depth estimation or relying on LiDAR
sensors to mitigate this problem, the dependence on the additional depth model
or expensive equipment severely limits their scalability to generic 3D
perception. In this paper, we propose a stereo-guided monocular 3D object
detection framework, dubbed SGM3D, adapting the robust 3D features learned from
stereo inputs to enhance the feature for monocular detection. We innovatively
present a multi-granularity domain adaptation (MG-DA) mechanism to exploit the
network's ability to generate stereo-mimicking features given only on monocular
cues. Coarse BEV feature-level, as well as the fine anchor-level domain
adaptation, are both leveraged for guidance in the monocular domain.In
addition, we introduce an IoU matching-based alignment (IoU-MA) method for
object-level domain adaptation between the stereo and monocular predictions to
alleviate the mismatches while adopting the MG-DA. Extensive experiments
demonstrate state-of-the-art results on KITTI and Lyft datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization. (arXiv:2201.13256v3 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13256">
<div class="article-summary-box-inner">
<span><p>Plug-and-Play (PnP) methods solve ill-posed inverse problems through
iterative proximal algorithms by replacing a proximal operator by a denoising
operation. When applied with deep neural network denoisers, these methods have
shown state-of-the-art visual performance for image restoration problems.
However, their theoretical convergence analysis is still incomplete. Most of
the existing convergence results consider nonexpansive denoisers, which is
non-realistic, or limit their analysis to strongly convex data-fidelity terms
in the inverse problem to solve. Recently, it was proposed to train the
denoiser as a gradient descent step on a functional parameterized by a deep
neural network. Using such a denoiser guarantees the convergence of the PnP
version of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this
paper, we show that this gradient denoiser can actually correspond to the
proximal operator of another scalar function. Given this new result, we exploit
the convergence theory of proximal algorithms in the nonconvex setting to
obtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM
(Alternating Direction Method of Multipliers). When built on top of a smooth
gradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target
stationary points of an explicit functional. These convergence results are
confirmed with numerical experiments on deblurring, super-resolution and
inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISNet: Costless and Implicit Image Segmentation for Deep Classifiers, with Application in COVID-19 Detection. (arXiv:2202.00232v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00232">
<div class="article-summary-box-inner">
<span><p>This work proposes a novel deep neural network (DNN) architecture, Implicit
Segmentation Neural Network (ISNet), to solve the task of image segmentation
followed by classification. It substitutes the common pipeline of two DNNs with
a single model. We designed the ISNet for high flexibility and performance: it
allows virtually any classification neural network architecture to analyze a
common image as if it had been previously segmented. Furthermore, in relation
to the original classifier, the ISNet does not cause any increment in
computational cost at run-time. We implement an ISNet based on a DenseNet121
classifier to solve the task of COVID-19 detection in chest X-rays. The ISNet
precisely ignored the image regions outside of the lungs; it achieved 94.5
+/-4.1% mean accuracy with an external test database, surpassing the
performances of state-of-the-art DNNs (U-Net segmenter followed by DenseNet121
and standalone DenseNet121) by 6 to 7.9%. ISNet presents an accurate, fast, and
light methodology to perform classification preceded by segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04595">
<div class="article-summary-box-inner">
<span><p>Neural image compression have reached or out-performed traditional methods
(such as JPEG, BPG, WebP). However,their sophisticated network structures with
cascaded convolution layers bring heavy computational burden for practical
deployment. In this paper, we explore the structural sparsity in neural image
compression network to obtain real-time acceleration without any specialized
hardware design or algorithm. We propose a simple plug-in adaptive binary
channel masking(ABCM) to judge the importance of each convolution channel and
introduce sparsity during training. During inference, the unimportant channels
are pruned to obtain slimmer network and less computation. We implement our
method into three neural image compression networks with different entropy
models to verify its effectiveness and generalization, the experiment results
show that up to 7x computation reduction and 3x acceleration can be achieved
with negligible performance drop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REFUGE2 Challenge: Treasure for Multi-Domain Learning in Glaucoma Assessment. (arXiv:2202.08994v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08994">
<div class="article-summary-box-inner">
<span><p>Glaucoma is the second leading cause of blindness and is the leading cause of
irreversible blindness disease in the world. Early screening for glaucoma in
the population is significant. Color fundus photography is the most cost
effective imaging modality to screen for ocular diseases. Deep learning network
is often used in color fundus image analysis due to its powful feature
extraction capability. However, the model training of deep learning method
needs a large amount of data, and the distribution of data should be abundant
for the robustness of model performance. To promote the research of deep
learning in color fundus photography and help researchers further explore the
clinical application signification of AI technology, we held a REFUGE2
challenge. This challenge released 2,000 color fundus images of four models,
including Zeiss, Canon, Kowa and Topcon, which can validate the stabilization
and generalization of algorithms on multi-domain. Moreover, three sub-tasks
were designed in the challenge, including glaucoma classification, cup/optic
disc segmentation, and macular fovea localization. These sub-tasks technically
cover the three main problems of computer vision and clinicly cover the main
researchs of glaucoma diagnosis. Over 1,300 international competitors joined
the REFUGE2 challenge, 134 teams submitted more than 3,000 valid preliminary
results, and 22 teams reached the final. This article summarizes the methods of
some of the finalists and analyzes their results. In particular, we observed
that the teams using domain adaptation strategies had high and robust
performance on the dataset with multi-domain. This indicates that UDA and other
multi-domain related researches will be the trend of deep learning field in the
future, and our REFUGE2 datasets will play an important role in these
researches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiDAR-guided Stereo Matching with a Spatial Consistency Constraint. (arXiv:2202.09953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09953">
<div class="article-summary-box-inner">
<span><p>The complementary fusion of light detection and ranging (LiDAR) data and
image data is a promising but challenging task for generating high-precision
and high-density point clouds. This study proposes an innovative LiDAR-guided
stereo matching approach called LiDAR-guided stereo matching (LGSM), which
considers the spatial consistency represented by continuous disparity or depth
changes in the homogeneous region of an image. The LGSM first detects the
homogeneous pixels of each LiDAR projection point based on their color or
intensity similarity. Next, we propose a riverbed enhancement function to
optimize the cost volume of the LiDAR projection points and their homogeneous
pixels to improve the matching robustness. Our formulation expands the
constraint scopes of sparse LiDAR projection points with the guidance of image
information to optimize the cost volume of pixels as much as possible. We
applied LGSM to semi-global matching and AD-Census on both simulated and real
datasets. When the percentage of LiDAR points in the simulated datasets was
0.16%, the matching accuracy of our method achieved a subpixel level, while
that of the original stereo matching algorithm was 3.4 pixels. The experimental
results show that LGSM is suitable for indoor, street, aerial, and satellite
image datasets and provides good transferability across semi-global matching
and AD-Census. Furthermore, the qualitative and quantitative evaluations
demonstrate that LGSM is superior to two state-of-the-art optimizing cost
volume methods, especially in reducing mismatches in difficult matching areas
and refining the boundaries of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Teacher Knowledge Distillation for Incremental Implicitly-Refined Classification. (arXiv:2202.11384v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11384">
<div class="article-summary-box-inner">
<span><p>Incremental learning methods can learn new classes continually by distilling
knowledge from the last model (as a teacher model) to the current model (as a
student model) in the sequentially learning process. However, these methods
cannot work for Incremental Implicitly-Refined Classification (IIRC), an
incremental learning extension where the incoming classes could have two
granularity levels, a superclass label and a subclass label. This is because
the previously learned superclass knowledge may be occupied by the subclass
knowledge learned sequentially. To solve this problem, we propose a novel
Multi-Teacher Knowledge Distillation (MTKD) strategy. To preserve the subclass
knowledge, we use the last model as a general teacher to distill the previous
knowledge for the student model. To preserve the superclass knowledge, we use
the initial model as a superclass teacher to distill the superclass knowledge
as the initial model contains abundant superclass knowledge. However,
distilling knowledge from two teacher models could result in the student model
making some redundant predictions. We further propose a post-processing
mechanism, called as Top-k prediction restriction to reduce the redundant
predictions. Our experimental results on IIRC-ImageNet120 and IIRC-CIFAR100
show that the proposed method can achieve better classification accuracy
compared with existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-27 23:07:32.101604008 UTC">2022-02-27 23:07:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>