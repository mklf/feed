<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-27T01:30:00Z">05-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12986">
<div class="article-summary-box-inner">
<span><p>Sentence scoring aims at measuring the likelihood score of a sentence and is
widely used in many natural language processing scenarios, like reranking,
which is to select the best sentence from multiple candidates. Previous works
on sentence scoring mainly adopted either causal language modeling (CLM) like
GPT or masked language modeling (MLM) like BERT, which have some limitations:
1) CLM only utilizes unidirectional information for the probability estimation
of a sentence without considering bidirectional context, which affects the
scoring quality; 2) MLM can only estimate the probability of partial tokens at
a time and thus requires multiple forward passes to estimate the probability of
the whole sentence, which incurs large computation and time cost. In this
paper, we propose \textit{Transcormer} -- a Transformer model with a novel
\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,
our SLM adopts a triple-stream self-attention mechanism to estimate the
probability of all tokens in a sentence with bidirectional context and only
requires a single forward pass. SLM can avoid the limitations of CLM (only
unidirectional context) and MLM (multiple forward passes) and inherit their
advantages, and thus achieve high effectiveness and efficiency in scoring.
Experimental results on multiple tasks demonstrate that our method achieves
better performance than other language modelings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiT: Robustly Binarized Multi-distilled Transformer. (arXiv:2205.13016v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13016">
<div class="article-summary-box-inner">
<span><p>Modern pre-trained transformers have rapidly advanced the state-of-the-art in
machine learning, but have also grown in parameters and computational
complexity, making them increasingly difficult to deploy in
resource-constrained environments. Binarization of the weights and activations
of the network can significantly alleviate these issues, however is technically
challenging from an optimization perspective. In this work, we identify a
series of improvements which enables binary transformers at a much higher
accuracy than what was possible previously. These include a two-set
binarization scheme, a novel elastic binary activation function with learned
parameters, and a method to quantize a network to its limit by successively
distilling higher precision models into lower precision students. These
approaches allow for the first time, fully binarized transformer models that
are at a practical level of accuracy, approaching a full-precision BERT
baseline on the GLUE language understanding benchmark within as little as 5.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion. (arXiv:2205.13108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13108">
<div class="article-summary-box-inner">
<span><p>We advance the state-of-the-art in unsupervised abstractive dialogue
summarization by utilizing multi-sentence compression graphs. Starting from
well-founded assumptions about word graphs, we present simple but reliable
path-reranking and topic segmentation schemes. Robustness of our method is
demonstrated on datasets across multiple domains, including meetings,
interviews, movie scripts, and day-to-day conversations. We also identify
possible avenues to augment our heuristic-based system with deep learning. We
open-source our code, to provide a strong, reproducible baseline for future
research into unsupervised dialogue summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13115">
<div class="article-summary-box-inner">
<span><p>Modern image captioning models are usually trained with text similarity
objectives. However, since reference captions in public datasets often describe
the most salient common objects, models trained with text similarity objectives
tend to ignore specific and detailed aspects of an image that distinguish it
from others. Toward more descriptive and distinctive caption generation, we
propose using CLIP, a multimodal encoder trained on huge image-text pairs from
web, to calculate multimodal similarity and use it as a reward function. We
also propose a simple finetuning strategy of the CLIP text encoder to improve
grammar that does not require extra text annotation. This completely eliminates
the need for reference captions during the reward computation. To
comprehensively evaluate descriptive captions, we introduce FineCapEval, a new
dataset for caption evaluation with fine-grained criteria: overall, background,
object, relations. In our experiments on text-to-image retrieval and
FineCapEval, the proposed CLIP-guided model generates more distinctive captions
than the CIDEr-optimized model. We also show that our unsupervised grammar
finetuning of the CLIP text encoder alleviates the degeneration problem of the
naive CLIP reward. Lastly, we show human analysis where the annotators strongly
prefer the CLIP reward to the CIDEr and MLE objectives according to various
criteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Metrics for Paraphrasing. (arXiv:2205.13119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13119">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation is a difficult problem. This is not only because of the
limitations in text generation capabilities but also due that to the lack of a
proper definition of what qualifies as a paraphrase and corresponding metrics
to measure how good it is. Metrics for evaluation of paraphrasing quality is an
on going research problem. Most of the existing metrics in use having been
borrowed from other tasks do not capture the complete essence of a good
paraphrase, and often fail at borderline-cases. In this work, we propose a
novel metric $ROUGE_P$ to measure the quality of paraphrases along the
dimensions of adequacy, novelty and fluency. We also provide empirical evidence
to show that the current natural language generation metrics are insufficient
to measure these desired properties of a good paraphrase. We look at paraphrase
model fine-tuning and generation from the lens of metrics to gain a deeper
understanding of what it takes to generate and evaluate a good paraphrase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Reinforcement Adaptation for Class-Imbalanced Text Classification. (arXiv:2205.13139v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13139">
<div class="article-summary-box-inner">
<span><p>Class imbalance naturally exists when train and test models in different
domains. Unsupervised domain adaptation (UDA) augments model performance with
only accessible annotations from the source domain and unlabeled data from the
target domain. However, existing state-of-the-art UDA models learn
domain-invariant representations and evaluate primarily on class-balanced data
across domains. In this work, we propose an unsupervised domain adaptation
approach via reinforcement learning that jointly leverages feature variants and
imbalanced labels across domains. We experiment with the text classification
task for its easily accessible datasets and compare the proposed method with
five baselines. Experiments on three datasets prove that our proposed method
can effectively learn robust domain-invariant representations and successfully
adapt text classifiers on imbalanced classes over domains. The code is
available at https://github.com/woqingdoua/ImbalanceClass.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar Detection for Sentiment Analysis through Improved Viterbi Algorithm. (arXiv:2205.13148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13148">
<div class="article-summary-box-inner">
<span><p>Grammar Detection, also referred to as Parts of Speech Tagging of raw text,
is considered an underlying building block of the various Natural Language
Processing pipelines like named entity recognition, question answering, and
sentiment analysis. In short, forgiven a sentence, Parts of Speech tagging is
the task of specifying and tagging each word of a sentence with nouns, verbs,
adjectives, adverbs, and more. Sentiment Analysis may well be a procedure
accustomed to determining if a given sentence's emotional tone is neutral,
positive or negative. To assign polarity scores to the thesis or entities
within phrase, in-text analysis and analytics, machine learning and natural
language processing, approaches are incorporated. This Sentiment Analysis using
POS tagger helps us urge a summary of the broader public over a specific topic.
For this, we are using the Viterbi algorithm, Hidden Markov Model, Constraint
based Viterbi algorithm for POS tagging. By comparing the accuracies, we select
the foremost accurate result of the model for Sentiment Analysis for
determining the character of the sentence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Dependency Grammar for Fine-Grained Offensive Language Detection using Graph Convolutional Networks. (arXiv:2205.13164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13164">
<div class="article-summary-box-inner">
<span><p>The last few years have witnessed an exponential rise in the propagation of
offensive text on social media. Identification of this text with high precision
is crucial for the well-being of society. Most of the existing approaches tend
to give high toxicity scores to innocuous statements (e.g., "I am a gay man").
These false positives result from over-generalization on the training data
where specific terms in the statement may have been used in a pejorative sense
(e.g., "gay"). Emphasis on such words alone can lead to discrimination against
the classes these systems are designed to protect. In this paper, we address
the problem of offensive language detection on Twitter, while also detecting
the type and the target of the offence. We propose a novel approach called
SyLSTM, which integrates syntactic features in the form of the dependency parse
tree of a sentence and semantic features in the form of word embeddings into a
deep learning architecture using a Graph Convolutional Network. Results show
that the proposed approach significantly outperforms the state-of-the-art BERT
model with orders of magnitude fewer number of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach. (arXiv:2205.13183v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13183">
<div class="article-summary-box-inner">
<span><p>Pre-trained models (PTMs) have lead to great improvements in natural language
generation (NLG). However, it is still unclear how much commonsense knowledge
they possess. With the goal of evaluating commonsense knowledge of NLG models,
recent work has proposed the problem of generative commonsense reasoning, e.g.,
to compose a logical sentence given a set of unordered concepts. Existing
approaches to this problem hypothesize that PTMs lack sufficient parametric
knowledge for this task, which can be overcome by introducing external
knowledge or task-specific pre-training objectives. Different from this trend,
we argue that PTM's inherent ability for generative commonsense reasoning is
underestimated due to the order-agnostic property of its input. In particular,
we hypothesize that the order of the input concepts can affect the PTM's
ability to utilize its commonsense knowledge. To this end, we propose a
pre-ordering approach to elaborately manipulate the order of the given concepts
before generation. Experiments show that our approach can outperform the more
sophisticated models that have access to a lot of external data and resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions. (arXiv:2205.13190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13190">
<div class="article-summary-box-inner">
<span><p>Role-oriented dialogue summarization is to generate summaries for different
roles in the dialogue, e.g., merchants and consumers. Existing methods handle
this task by summarizing each role's content separately and thus are prone to
ignore the information from other roles. However, we believe that other roles'
content could benefit the quality of summaries, such as the omitted information
mentioned by other roles. Therefore, we propose a novel role interaction
enhanced method for role-oriented dialogue summarization. It adopts cross
attention and decoder self-attention interactions to interactively acquire
other roles' critical information. The cross attention interaction aims to
select other roles' critical dialogue utterances, while the decoder
self-attention interaction aims to obtain key information from other roles'
summaries. Experimental results have shown that our proposed method
significantly outperforms strong baselines on two public role-oriented dialogue
summarization datasets. Extensive analyses have demonstrated that other roles'
content could help generate summaries with more complete semantics and correct
topic structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbiotic Child Emotional Support with Social Robots and Temporal Knowledge Graphs. (arXiv:2205.13229v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13229">
<div class="article-summary-box-inner">
<span><p>In current youth-care programs, children with needs (mental health, family
issues, learning disabilities, and autism) receive support from youth and
family experts as one-to-one assistance at schools or hospitals. Occasionally,
social robots have featured in such settings as support roles in a one-to-one
interaction with the child. In this paper, we suggest the development of a
symbiotic framework for real-time Emotional Support (ES) with social robots
Knowledge Graphs (KG). By augmenting a domain-specific corpus from the
literature on ES for children (between the age of 8 and 12) and providing
scenario-driven context including the history of events, we suggest developing
an experimental knowledge-aware ES framework. The framework both guides the
social robot in providing ES statements to the child and assists the expert in
tracking and interpreting the child's emotional state and related events over
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Split BERT for Heterogeneous Text Classification. (arXiv:2205.13299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13299">
<div class="article-summary-box-inner">
<span><p>Pre-trained BERT models have achieved impressive performance in many natural
language processing (NLP) tasks. However, in many real-world situations,
textual data are usually decentralized over many clients and unable to be
uploaded to a central server due to privacy protection and regulations.
Federated learning (FL) enables multiple clients collaboratively to train a
global model while keeping the local data privacy. A few researches have
investigated BERT in federated learning setting, but the problem of performance
loss caused by heterogeneous (e.g., non-IID) data over clients remain
under-explored. To address this issue, we propose a framework, FedSplitBERT,
which handles heterogeneous data and decreases the communication cost by
splitting the BERT encoder layers into local part and global part. The local
part parameters are trained by the local client only while the global part
parameters are trained by aggregating gradients of multiple clients. Due to the
sheer size of BERT, we explore a quantization method to further reduce the
communication cost with minimal performance loss. Our framework is ready-to-use
and compatible to many existing federated learning algorithms, including
FedAvg, FedProx and FedAdam. Our experiments verify the effectiveness of the
proposed framework, which outperforms baseline methods by a significant margin,
while FedSplitBERT with quantization can reduce the communication cost by
$11.9\times$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Non-negative Matrix Factorization for Short Texts Topic Modeling with Mutual Information. (arXiv:2205.13300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13300">
<div class="article-summary-box-inner">
<span><p>Non-negative matrix factorization (NMF) based topic modeling is widely used
in natural language processing (NLP) to uncover hidden topics of short text
documents. Usually, training a high-quality topic model requires large amount
of textual data. In many real-world scenarios, customer textual data should be
private and sensitive, precluding uploading to data centers. This paper
proposes a Federated NMF (FedNMF) framework, which allows multiple clients to
collaboratively train a high-quality NMF based topic model with locally stored
data. However, standard federated learning will significantly undermine the
performance of topic models in downstream tasks (e.g., text classification)
when the data distribution over clients is heterogeneous. To alleviate this
issue, we further propose FedNMF+MI, which simultaneously maximizes the mutual
information (MI) between the count features of local texts and their topic
weight vectors to mitigate the performance degradation. Experimental results
show that our FedNMF+MI methods outperform Federated Latent Dirichlet
Allocation (FedLDA) and the FedNMF without MI methods for short texts by a
significant margin on both coherence score and classification F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target-aware Abstractive Related Work Generation with Contrastive Learning. (arXiv:2205.13339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13339">
<div class="article-summary-box-inner">
<span><p>The related work section is an important component of a scientific paper,
which highlights the contribution of the target paper in the context of the
reference papers. Authors can save their time and effort by using the
automatically generated related work section as a draft to complete the final
related work. Most of the existing related work section generation methods rely
on extracting off-the-shelf sentences to make a comparative discussion about
the target work and the reference papers. However, such sentences need to be
written in advance and are hard to obtain in practice. Hence, in this paper, we
propose an abstractive target-aware related work generator (TAG), which can
generate related work sections consisting of new sentences. Concretely, we
first propose a target-aware graph encoder, which models the relationships
between reference papers and the target paper with target-centered attention
mechanisms. In the decoding process, we propose a hierarchical decoder that
attends to the nodes of different levels in the graph with keyphrases as
semantic indicators. Finally, to generate a more informative related work, we
propose multi-level contrastive optimization objectives, which aim to maximize
the mutual information between the generated related work with the references
and minimize that with non-references. Extensive experiments on two public
scholar datasets show that the proposed model brings substantial improvements
over several strong baselines in terms of automatic and tailored human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Causal Inference for Explainable Automatic Program Repair. (arXiv:2205.13342v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13342">
<div class="article-summary-box-inner">
<span><p>Deep learning models have made significant progress in automatic program
repair. However, the black-box nature of these methods has restricted their
practical applications. To address this challenge, this paper presents an
interpretable approach for program repair based on sequence-to-sequence models
with causal inference and our method is called CPR, short for causal program
repair. Our CPR can generate explanations in the process of decision making,
which consists of groups of causally related input-output tokens. Firstly, our
method infers these relations by querying the model with inputs disturbed by
data augmentation. Secondly, it generates a graph over tokens from the
responses and solves a partitioning problem to select the most relevant
components. The experiments on four programming languages (Java, C, Python, and
JavaScript) show that CPR can generate causal graphs for reasonable
interpretations and boost the performance of bug fixing in automatic program
repair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation. (arXiv:2205.13346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13346">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has achieved impressive success in generation tasks to
militate the "exposure bias" problem and discriminatively exploit the different
quality of references. Existing works mostly focus on contrastive learning on
the instance-level without discriminating the contribution of each word, while
keywords are the gist of the text and dominant the constrained mapping
relationships. Hence, in this work, we propose a hierarchical contrastive
learning mechanism, which can unify hybrid granularities semantic meaning in
the input text. Concretely, we first propose a keyword graph via contrastive
correlations of positive-negative pairs to iteratively polish the keyword
representations. Then, we construct intra-contrasts within instance-level and
keyword-level, where we assume words are sampled nodes from a sentence
distribution. Finally, to bridge the gap between independent contrast levels
and tackle the common contrast vanishing problem, we propose an inter-contrast
mechanism that measures the discrepancy between contrastive keyword nodes
respectively to the instance distribution. Experiments demonstrate that our
model outperforms competitive baselines on paraphrasing, dialogue generation,
and storytelling tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Document Vectors Using Cosine Similarity Revisited. (arXiv:2205.13357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13357">
<div class="article-summary-box-inner">
<span><p>The current state-of-the-art test accuracy (97.42\%) on the IMDB movie
reviews dataset was reported by \citet{thongtan-phienthrakul-2019-sentiment}
and achieved by the logistic regression classifier trained on the Document
Vectors using Cosine Similarity (DV-ngrams-cosine) proposed in their paper and
the Bag-of-N-grams (BON) vectors scaled by Naive Bayesian weights. While large
pre-trained Transformer-based models have shown SOTA results across many
datasets and tasks, the aforementioned model has not been surpassed by them,
despite being much simpler and pre-trained on the IMDB dataset only.
</p>
<p>In this paper, we describe an error in the evaluation procedure of this
model, which was found when we were trying to analyze its excellent performance
on the IMDB dataset. We further show that the previously reported test accuracy
of 97.42\% is invalid and should be corrected to 93.68\%. We also analyze the
model performance with different amounts of training data (subsets of the IMDB
dataset) and compare it to the Transformer-based RoBERTa model. The results
show that while RoBERTa has a clear advantage for larger training sets, the
DV-ngrams-cosine performs better than RoBERTa when the labelled training set is
very small (10 or 20 documents). Finally, we introduce a sub-sampling scheme
based on Naive Bayesian weights for the training process of the
DV-ngrams-cosine, which leads to faster training and better quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Your Transformer May Not be as Powerful as You Expect. (arXiv:2205.13401v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13401">
<div class="article-summary-box-inner">
<span><p>Relative Positional Encoding (RPE), which encodes the relative distance
between any pair of tokens, is one of the most successful modifications to the
original Transformer. As far as we know, theoretical understanding of the
RPE-based Transformers is largely unexplored. In this work, we mathematically
analyze the power of RPE-based Transformers regarding whether the model is
capable of approximating any continuous sequence-to-sequence functions. One may
naturally assume the answer is in the affirmative -- RPE-based Transformers are
universal function approximators. However, we present a negative result by
showing there exist continuous sequence-to-sequence functions that RPE-based
Transformers cannot approximate no matter how deep and wide the neural network
is. One key reason lies in that most RPEs are placed in the softmax attention
that always generates a right stochastic matrix. This restricts the network
from capturing positional information in the RPEs and limits its capacity. To
overcome the problem and make the model more powerful, we first present
sufficient conditions for RPE-based Transformers to achieve universal function
approximation. With the theoretical guidance, we develop a novel attention
module, called Universal RPE-based (URPE) Attention, which satisfies the
conditions. Therefore, the corresponding URPE-based Transformers become
universal function approximators. Extensive experiments covering typical
architectures and tasks demonstrate that our model is parameter-efficient and
can achieve superior performance to strong baselines in a wide range of
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Learning Span Extraction and Sequence Labeling for Information Extraction from Business Documents. (arXiv:2205.13434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13434">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new information extraction model for business
documents. Different from prior studies which only base on span extraction or
sequence labeling, the model takes into account advantage of both span
extraction and sequence labeling. The combination allows the model to deal with
long documents with sparse information (the small amount of extracted
information). The model is trained end-to-end to jointly optimize the two tasks
in a unified manner. Experimental results on four business datasets in English
and Japanese show that the model achieves promising results and is
significantly faster than the normal span-based extraction method. The code is
also available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. (arXiv:2205.13445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13445">
<div class="article-summary-box-inner">
<span><p>Text-to-image generation and image captioning are recently emerged as a new
experimental paradigm to assess machine intelligence. They predict continuous
quantity accompanied by their sampling techniques in the generation, making
evaluation complicated and intractable to get marginal distributions. Based on
a recent trend that multimodal generative evaluations exploit a
vison-and-language pre-trained model, we propose the negative Gaussian
cross-mutual information using the CLIP features as a unified metric, coined by
Mutual Information Divergence (MID). To validate, we extensively compare it
with competing metrics using carefully-generated or human-annotated judgments
in text-to-image generation and image captioning tasks. The proposed MID
significantly outperforms the competitive methods by having consistency across
benchmarks, sample parsimony, and robustness toward the exploited CLIP model.
We look forward to seeing the underrepresented implications of the Gaussian
cross-mutual information in multimodal representation learning and the future
works based on this novel proposition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Parsing of Interpage Relations. (arXiv:2205.13530v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13530">
<div class="article-summary-box-inner">
<span><p>Page-level analysis of documents has been a topic of interest in digitization
efforts, and multimodal approaches have been applied to both classification and
page stream segmentation. In this work, we focus on capturing finer semantic
relations between pages of a multi-page document. To this end, we formalize the
task as semantic parsing of interpage relations and we propose an end-to-end
approach for interpage dependency extraction, inspired by the dependency
parsing literature. We further design a multi-task training approach to jointly
optimize for page embeddings to be used in segmentation, classification, and
parsing of the page dependencies using textual and visual features extracted
from the pages. Moreover, we also combine the features from two modalities to
obtain multimodal page embeddings. To the best of our knowledge, this is the
first study to extract rich semantic interpage relations from multi-page
documents. Our experimental results show that the proposed method increased LAS
by 41 percentage points for semantic parsing, increased accuracy by 33
percentage points for page stream segmentation, and 45 percentage points for
page classification over a naive baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCoLM: COmplex COmmonsense Enhanced Language Model with Discourse Relations. (arXiv:2012.15643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15643">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have demonstrated strong knowledge
representation ability. However, recent studies suggest that even though these
giant models contains rich simple commonsense knowledge (e.g., bird can fly and
fish can swim.), they often struggle with the complex commonsense knowledge
that involves multiple eventualities (verb-centric phrases, e.g., identifying
the relationship between ``Jim yells at Bob'' and ``Bob is upset'').To address
this problem, in this paper, we propose to help pre-trained language models
better incorporate complex commonsense knowledge. Different from existing
fine-tuning approaches, we do not focus on a specific task and propose a
general language model named CoCoLM. Through the careful training over a
large-scale eventuality knowledge graphs ASER, we successfully teach
pre-trained language models (i.e., BERT and RoBERTa) rich complex commonsense
knowledge among eventualities. Experiments on multiple downstream commonsense
tasks that requires the correct understanding of eventualities demonstrate the
effectiveness of CoCoLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Resource Multi-Dialectal Arabic Natural Language Understanding. (arXiv:2104.06591v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06591">
<div class="article-summary-box-inner">
<span><p>A reasonable amount of annotated data is required for fine-tuning pre-trained
language models (PLM) on downstream tasks. However, obtaining labeled examples
for different language varieties can be costly. In this paper, we investigate
the zero-shot performance on Dialectal Arabic (DA) when fine-tuning a PLM on
modern standard Arabic (MSA) data only -- identifying a significant performance
drop when evaluating such models on DA. To remedy such performance drop, we
propose self-training with unlabeled DA data and apply it in the context of
named entity recognition (NER), part-of-speech (POS) tagging, and sarcasm
detection (SRD) on several DA varieties. Our results demonstrate the
effectiveness of self-training with unlabeled DA data: improving zero-shot
MSA-to-DA transfer by as large as $\sim$10\% F$_1$ (NER), 2\% accuracy (POS
tagging), and 4.5\% F$_1$ (SRD). We conduct an ablation experiment and show
that the performance boost observed directly results from the unlabeled DA
examples used for self-training. Our work opens up opportunities for leveraging
the relatively abundant labeled MSA datasets to develop DA models for zero and
low-resource dialects. We also report new state-of-the-art performance on all
three tasks and open-source our fine-tuned models for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems. (arXiv:2104.08570v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08570">
<div class="article-summary-box-inner">
<span><p>In task-oriented dialogue (ToD), a user holds a conversation with an
artificial agent to complete a concrete task. Although this technology
represents one of the central objectives of AI and has been the focus of ever
more intense research and development efforts, it is currently limited to a few
narrow domains (e.g., food ordering, ticket booking) and a handful of languages
(e.g., English, Chinese). This work provides an extensive overview of existing
methods and resources in multilingual ToD as an entry point to this exciting
and emerging field. We find that the most critical factor preventing the
creation of truly multilingual ToD systems is the lack of datasets in most
languages for both training and evaluation. In fact, acquiring annotations or
human feedback for each component of modular systems or for data-hungry
end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches
to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer
from resource-rich languages (almost exclusively English), either by means of
machine translation or multilingual representations. These approaches are
currently viable only for typologically similar languages and languages with
parallel / monolingual corpora available. On the other hand, their
effectiveness beyond these boundaries is doubtful or hard to assess due to the
lack of linguistically diverse benchmarks (especially for natural language
generation and end-to-end evaluation). To overcome this limitation, we draw
parallels between components of the ToD pipeline and other NLP tasks, which can
inspire solutions for learning in low-resource scenarios. Finally, we list
additional challenges that multilinguality poses for related areas (such as
speech and human-centred evaluation), and indicate future directions that hold
promise to further expand language coverage and dialogue capabilities of
current ToD systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarization, Simplification, and Generation: The Case of Patents. (arXiv:2104.14860v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14860">
<div class="article-summary-box-inner">
<span><p>We survey Natural Language Processing (NLP) approaches to summarizing,
simplifying, and generating patents' text. While solving these tasks has
important practical applications - given patents' centrality in the R&amp;D process
- patents' idiosyncrasies open peculiar challenges to the current NLP state of
the art. This survey aims at a) describing patents' characteristics and the
questions they raise to the current NLP systems, b) critically presenting
previous work and its evolution, and c) drawing attention to directions of
research in which further work is needed. To the best of our knowledge, this is
the first survey of generative approaches in the patent domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Dense Information Retrieval with Contrastive Learning. (arXiv:2112.09118v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09118">
<div class="article-summary-box-inner">
<span><p>Recently, information retrieval has seen the emergence of dense retrievers,
based on neural networks, as an alternative to classical sparse methods based
on term-frequency. These models have obtained state-of-the-art results on
datasets and tasks where large training sets are available. However, they do
not transfer well to new applications with no training data, and are
outperformed by unsupervised term-frequency methods such as BM25. In this work,
we explore the limits of contrastive learning as a way to train unsupervised
dense retrievers and show that it leads to strong performance in various
retrieval settings. On the BEIR benchmark our unsupervised model outperforms
BM25 on 11 out of 15 datasets for the Recall@100 metric. When used as
pre-training before fine-tuning, either on a few thousands in-domain examples
or on the large MS MARCO dataset, our contrastive model leads to improvements
on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual
retrieval, where training data is even scarcer than for English, and show that
our approach leads to strong unsupervised performance. Our model also exhibits
strong cross-lingual transfer when fine-tuned on supervised English data only
and evaluated on low resources language such as Swahili. We show that our
unsupervised models can perform cross-lingual retrieval between different
scripts, such as retrieving English documents from Arabic queries, which would
not be possible with term matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation. (arXiv:2201.08054v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08054">
<div class="article-summary-box-inner">
<span><p>Existing multimodal machine translation (MMT) datasets consist of images and
video captions or general subtitles, which rarely contain linguistic ambiguity,
making visual information not so effective to generate appropriate
translations. We introduce VISA, a new dataset that consists of 40k
Japanese-English parallel sentence pairs and corresponding video clips with the
following key features: (1) the parallel sentences are subtitles from movies
and TV episodes; (2) the source subtitles are ambiguous, which means they have
multiple possible translations with different meanings; (3) we divide the
dataset into Polysemy and Omission according to the cause of ambiguity. We show
that VISA is challenging for the latest MMT system, and we hope that the
dataset can facilitate MMT research. The VISA dataset is available at:
https://github.com/ku-nlp/VISA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation. (arXiv:2205.06457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06457">
<div class="article-summary-box-inner">
<span><p>We present ViT5, a pretrained Transformer-based encoder-decoder model for the
Vietnamese language. With T5-style self-supervised pretraining, ViT5 is trained
on a large corpus of high-quality and diverse Vietnamese texts. We benchmark
ViT5 on two downstream text generation tasks, Abstractive Text Summarization
and Named Entity Recognition. Although Abstractive Text Summarization has been
widely studied for the English language thanks to its rich and large source of
data, there has been minimal research into the same task in Vietnamese, a much
lower resource language. In this work, we perform exhaustive experiments on
both Vietnamese Abstractive Summarization and Named Entity Recognition,
validating the performance of ViT5 against many other pretrained
Transformer-based encoder-decoder models. Our experiments show that ViT5
significantly outperforms existing models and achieves state-of-the-art results
on Vietnamese Text Summarization. On the task of Named Entity Recognition, ViT5
is competitive against previous best results from pretrained encoder-based
Transformer models. Further analysis shows the importance of context length
during the self-supervised pretraining on downstream performance across
different settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach for Automatic Construction of an Algorithmic Knowledge Graph from Textual Resources. (arXiv:2205.06854v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06854">
<div class="article-summary-box-inner">
<span><p>There is enormous growth in various fields of research. This development is
accompanied by new problems. To solve these problems efficiently and in an
optimized manner, algorithms are created and described by researchers in the
scientific literature. Scientific algorithms are vital for understanding and
reusing existing work in numerous domains. However, algorithms are generally
challenging to find. Also, the comparison among similar algorithms is difficult
because of the disconnected documentation. Information about algorithms is
mostly present in websites, code comments, and so on. There is an absence of
structured metadata to portray algorithms. As a result, sometimes redundant or
similar algorithms are published, and the researchers build them from scratch
instead of reusing or expanding upon the already existing algorithm. In this
paper, we introduce an approach for automatically developing a knowledge graph
(KG) for algorithmic problems from unstructured data. Because it captures
information more clearly and extensively, an algorithm KG will give additional
context and explainability to the algorithm metadata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization. (arXiv:2205.07208v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07208">
<div class="article-summary-box-inner">
<span><p>It is challenging to train a good intent classifier for a task-oriented
dialogue system with only a few annotations. Recent studies have shown that
fine-tuning pre-trained language models with a small amount of labeled
utterances from public benchmarks in a supervised manner is extremely helpful.
However, we find that supervised pre-training yields an anisotropic feature
space, which may suppress the expressive power of the semantic representations.
Inspired by recent research in isotropization, we propose to improve supervised
pre-training by regularizing the feature space towards isotropy. We propose two
regularizers based on contrastive learning and correlation matrix respectively,
and demonstrate their effectiveness through extensive experiments. Our main
finding is that it is promising to regularize supervised pre-training with
isotropization to further improve the performance of few-shot intent detection.
The source code can be found at https://github.com/fanolabs/isoIntentBert-main.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese. (arXiv:2205.10517v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10517">
<div class="article-summary-box-inner">
<span><p>Multilingual language models such as mBERT have seen impressive cross-lingual
transfer to a variety of languages, but many languages remain excluded from
these models. In this paper, we analyse the effect of pre-training with
monolingual data for a low-resource language that is not included in mBERT --
Maltese -- with a range of pre-training set ups. We conduct evaluations with
the newly pre-trained models on three morphosyntactic tasks -- dependency
parsing, part-of-speech tagging, and named-entity recognition -- and one
semantic classification task -- sentiment analysis. We also present a newly
created corpus for Maltese, and determine the effect that the pre-training data
size and domain have on the downstream performance. Our results show that using
a mixture of pre-training domains is often superior to using Wikipedia text
only. We also find that a fraction of this corpus is enough to make significant
leaps in performance over Wikipedia-trained models. We pre-train and compare
two models on the new corpus: a monolingual BERT model trained from scratch
(BERTu), and a further pre-trained multilingual BERT (mBERTu). The models
achieve state-of-the-art performance on these tasks, despite the new corpus
being considerably smaller than typically used corpora for high-resourced
languages. On average, BERTu outperforms or performs competitively with mBERTu,
and the largest gains are observed for higher-level tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Certified Robustness Against Natural Language Attacks by Causal Intervention. (arXiv:2205.12331v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12331">
<div class="article-summary-box-inner">
<span><p>Deep learning models have achieved great success in many fields, yet they are
vulnerable to adversarial examples. This paper follows a causal perspective to
look into the adversarial vulnerability and proposes Causal Intervention by
Semantic Smoothing (CISS), a novel framework towards robustness against natural
language attacks. Instead of merely fitting observational data, CISS learns
causal effects p(y|do(x)) by smoothing in the latent semantic space to make
robust predictions, which scales to deep architectures and avoids tedious
construction of noise customized for specific attacks. CISS is provably robust
against word substitution attacks, as well as empirically robust even when
perturbations are strengthened by unknown attack algorithms. For example, on
YELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness
against word substitutions, and achieves 79.4% empirical robustness when
syntactic attacks are integrated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QAMPARI: : An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. (arXiv:2205.12665v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12665">
<div class="article-summary-box-inner">
<span><p>Existing benchmarks for open-domain question answering (ODQA) typically focus
on questions whose answers can be extracted from a single paragraph. By
contrast, many natural questions, such as "What players were drafted by the
Brooklyn Nets?" have a list of answers. Answering such questions requires
retrieving and reading from many passages, in a large corpus. We introduce
QAMPARI, an ODQA benchmark, where question answers are lists of entities,
spread across many paragraphs. We created QAMPARI by (a) generating questions
with multiple answers from Wikipedia's knowledge graph and tables, (b)
automatically pairing answers with supporting evidence in Wikipedia paragraphs,
and (c) manually paraphrasing questions and validating each answer. We train
ODQA models from the retrieve-and-read family and find that QAMPARI is
challenging in terms of both passage retrieval and answer generation, reaching
an F1 score of 26.6 at best. Our results highlight the need for developing ODQA
models that handle a broad range of question types, including single and
multi-answer questions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Diverse and Natural Scene-aware 3D Human Motion Synthesis. (arXiv:2205.13001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13001">
<div class="article-summary-box-inner">
<span><p>The ability to synthesize long-term human motion sequences in real-world
scenes can facilitate numerous applications. Previous approaches for
scene-aware motion synthesis are constrained by pre-defined target objects or
positions and thus limit the diversity of human-scene interactions for
synthesized motions. In this paper, we focus on the problem of synthesizing
diverse scene-aware human motions under the guidance of target action
sequences. To achieve this, we first decompose the diversity of scene-aware
human motions into three aspects, namely interaction diversity (e.g. sitting on
different objects with different poses in the given scenes), path diversity
(e.g. moving to the target locations following different paths), and the motion
diversity (e.g. having various body movements during moving). Based on this
factorized scheme, a hierarchical framework is proposed, with each sub-module
responsible for modeling one aspect. We assess the effectiveness of our
framework on two challenging datasets for scene-aware human motion synthesis.
The experiment results show that the proposed framework remarkably outperforms
previous methods in terms of diversity and naturalness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">People counting system for retail analytics using edge AI. (arXiv:2205.13020v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13020">
<div class="article-summary-box-inner">
<span><p>Developments in IoT applications are playing an important role in our
day-to-day life, starting from business predictions to self driving cars. One
of the area, most influenced by the field of AI and IoT is retail analytics. In
Retail Analytics, Conversion Rates - a metric which is most often used by
retail stores to measure how many people have visited the store and how many
purchases has happened. This retail conversion rate assess the marketing
operations, increasing stock, store outlet and running promotions ..etc. Our
project intends to build a cost-effective people counting system with AI at
Edge, where it calculates Conversion rates using total number of people counted
by the system and number of transactions for the day, which helps in providing
analytical insights for retail store optimization with a very minimum hardware
requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How explainable are adversarially-robust CNNs?. (arXiv:2205.13042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13042">
<div class="article-summary-box-inner">
<span><p>Three important criteria of existing convolutional neural networks (CNNs) are
(1) test-set accuracy; (2) out-of-distribution accuracy; and (3)
explainability. While these criteria have been studied independently, their
relationship is unknown. For example, do CNNs that have a stronger
out-of-distribution performance have also stronger explainability? Furthermore,
most prior feature-importance studies only evaluate methods on 2-3 common
vanilla ImageNet-trained CNNs, leaving it unknown how these methods generalize
to CNNs of other architectures and training algorithms. Here, we perform the
first, large-scale evaluation of the relations of the three criteria using 9
feature-importance methods and 12 ImageNet-trained CNNs that are of 3 training
algorithms and 5 CNN architectures. We find several important insights and
recommendations for ML practitioners. First, adversarially robust CNNs have a
higher explainability score on gradient-based attribution methods (but not
CAM-based or perturbation-based methods). Second, AdvProp models, despite being
highly accurate more than both vanilla and robust models alone, are not
superior in explainability. Third, among 9 feature attribution methods tested,
GradCAM and RISE are consistently the best methods. Fourth, Insertion and
Deletion are biased towards vanilla and robust models respectively, due to
their strong correlation with the confidence score distributions of a CNN.
Fifth, we did not find a single CNN to be the best in all three criteria, which
interestingly suggests that CNNs are harder to interpret as they become more
accurate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Deep Equilibrium Learning for Regularization by Denoising. (arXiv:2205.13051v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13051">
<div class="article-summary-box-inner">
<span><p>Plug-and-Play Priors (PnP) and Regularization by Denoising (RED) are
widely-used frameworks for solving imaging inverse problems by computing
fixed-points of operators combining physical measurement models and learned
image priors. While traditional PnP/RED formulations have focused on priors
specified using image denoisers, there is a growing interest in learning
PnP/RED priors that are end-to-end optimal. The recent Deep Equilibrium Models
(DEQ) framework has enabled memory-efficient end-to-end learning of PnP/RED
priors by implicitly differentiating through the fixed-point equations without
storing intermediate activation values. However, the dependence of the
computational/memory complexity of the measurement models in PnP/RED on the
total number of measurements leaves DEQ impractical for many imaging
applications. We propose ODER as a new strategy for improving the efficiency of
DEQ through stochastic approximations of the measurement models. We
theoretically analyze ODER giving insights into its convergence and ability to
approximate the traditional DEQ approach. Our numerical results suggest the
potential improvements in training/testing complexity due to ODER on three
distinct imaging applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing an Efficient End-to-end Machine Learning Pipeline for Real-time Empty-shelf Detection. (arXiv:2205.13060v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13060">
<div class="article-summary-box-inner">
<span><p>On-Shelf Availability (OSA) of products in retail stores is a critical
business criterion in the fast moving consumer goods and retails sector. When a
product is out-of-stock (OOS) and a customer cannot find it on its designed
shelf, this causes a negative impact on the customer's behaviors and future
demands. Several methods are being adopted by retailers today to detect empty
shelves and ensure high OSA of products; however, such methods are generally
ineffective and infeasible since they are either manual, expensive or less
accurate. Recently machine learning based solutions have been proposed, but
they suffer from high computation cost and low accuracy problem due to lack of
large annotated datasets of on-shelf products. Here, we present an elegant
approach for designing an end-to-end machine learning (ML) pipeline for
real-time empty shelf detection. Considering the strong dependency between the
quality of ML models and the quality of data, we focus on the importance of
proper data collection, cleaning and correct data annotation before delving
into modeling. Since an empty-shelf detection solution should be
computationally-efficient for real-time predictions, we explore different
run-time optimizations to improve the model performance. Our dataset contains
1000 images, collected and annotated by following well-defined guidelines. Our
low-latency model achieves a mean average F1-score of 68.5%, and can process up
to 67 images/s on Intel Xeon Gold and up to 860 images/s on an A100 GPU. Our
annotated dataset is publicly available along with our optimized models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Map-based Features for Efficient Attention-based Vehicle Motion Prediction. (arXiv:2205.13071v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13071">
<div class="article-summary-box-inner">
<span><p>Motion prediction (MP) of multiple agents is a crucial task in arbitrarily
complex environments, from social robots to self-driving cars. Current
approaches tackle this problem using end-to-end networks, where the input data
is usually a rendered top-view of the scene and the past trajectories of all
the agents; leveraging this information is a must to obtain optimal
performance. In that sense, a reliable Autonomous Driving (AD) system must
produce reasonable predictions on time, however, despite many of these
approaches use simple ConvNets and LSTMs, models might not be efficient enough
for real-time applications when using both sources of information (map and
trajectory history). Moreover, the performance of these models highly depends
on the amount of training data, which can be expensive (particularly the
annotated HD maps). In this work, we explore how to achieve competitive
performance on the Argoverse 1.0 Benchmark using efficient attention-based
models, which take as input the past trajectories and map-based features from
minimal map information to ensure efficient and reliable MP. These features
represent interpretable information as the driveable area and plausible goal
points, in opposition to black-box CNN-based methods for map processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Representation Blending for Multi-Label Image Recognition with Partial Labels. (arXiv:2205.13092v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13092">
<div class="article-summary-box-inner">
<span><p>Despite achieving impressive progress, current multi-label image recognition
(MLR) algorithms heavily depend on large-scale datasets with complete labels,
making collecting large-scale datasets extremely time-consuming and
labor-intensive. Training the multi-label image recognition models with partial
labels (MLR-PL) is an alternative way to address this issue, in which merely
some labels are known while others are unknown for each image (see Figure 1).
However, current MLP-PL algorithms mainly rely on the pre-trained image
classification or similarity models to generate pseudo labels for the unknown
labels. Thus, they depend on a certain amount of data annotations and
inevitably suffer from obvious performance drops, especially when the known
label proportion is low. To address this dilemma, we propose a unified
semantic-aware representation blending (SARB) that consists of two crucial
modules to blend multi-granularity category-specific semantic representation
across different images to transfer information of known labels to complement
unknown labels. Extensive experiments on the MS-COCO, Visual Genome, and Pascal
VOC 2007 datasets show that the proposed SARB consistently outperforms current
state-of-the-art algorithms on all known label proportion settings. Concretely,
it obtain the average mAP improvement of 1.9%, 4.5%, 1.0% on the three
benchmark datasets compared with the second-best algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VizInspect Pro -- Automated Optical Inspection (AOI) solution. (arXiv:2205.13095v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13095">
<div class="article-summary-box-inner">
<span><p>Traditional vision based Automated Optical Inspection (referred to as AOI in
paper) systems present multiple challenges in factory settings including
inability to scale across multiple product lines, requirement of vendor
programming expertise, little tolerance to variations and lack of cloud
connectivity for aggregated insights. The lack of flexibility in these systems
presents a unique opportunity for a deep learning based AOI system specifically
for factory automation. The proposed solution, VizInspect pro is a generic
computer vision based AOI solution built on top of Leo - An edge AI platform.
Innovative features that overcome challenges of traditional vision systems
include deep learning based image analysis which combines the power of
self-learning with high speed and accuracy, an intuitive user interface to
configure inspection profiles in minutes without ML or vision expertise and the
ability to solve complex inspection challenges while being tolerant to
deviations and unpredictable defects. This solution has been validated by
multiple external enterprise customers with confirmed value propositions. In
this paper we show you how this solution and platform solved problems around
model development, deployment, scaling multiple inferences and visualizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to segment with limited annotations: Self-supervised pretraining with regression and contrastive loss in MRI. (arXiv:2205.13109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13109">
<div class="article-summary-box-inner">
<span><p>Obtaining manual annotations for large datasets for supervised training of
deep learning (DL) models is challenging. The availability of large unlabeled
datasets compared to labeled ones motivate the use of self-supervised
pretraining to initialize DL models for subsequent segmentation tasks. In this
work, we consider two pre-training approaches for driving a DL model to learn
different representations using: a) regression loss that exploits spatial
dependencies within an image and b) contrastive loss that exploits semantic
similarity between pairs of images. The effect of pretraining techniques is
evaluated in two downstream segmentation applications using Magnetic Resonance
(MR) images: a) liver segmentation in abdominal T2-weighted MR images and b)
prostate segmentation in T2-weighted MR images of the prostate. We observed
that DL models pretrained using self-supervision can be finetuned for
comparable performance with fewer labeled datasets. Additionally, we also
observed that initializing the DL model using contrastive loss based
pretraining performed better than the regression loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13115">
<div class="article-summary-box-inner">
<span><p>Modern image captioning models are usually trained with text similarity
objectives. However, since reference captions in public datasets often describe
the most salient common objects, models trained with text similarity objectives
tend to ignore specific and detailed aspects of an image that distinguish it
from others. Toward more descriptive and distinctive caption generation, we
propose using CLIP, a multimodal encoder trained on huge image-text pairs from
web, to calculate multimodal similarity and use it as a reward function. We
also propose a simple finetuning strategy of the CLIP text encoder to improve
grammar that does not require extra text annotation. This completely eliminates
the need for reference captions during the reward computation. To
comprehensively evaluate descriptive captions, we introduce FineCapEval, a new
dataset for caption evaluation with fine-grained criteria: overall, background,
object, relations. In our experiments on text-to-image retrieval and
FineCapEval, the proposed CLIP-guided model generates more distinctive captions
than the CIDEr-optimized model. We also show that our unsupervised grammar
finetuning of the CLIP text encoder alleviates the degeneration problem of the
naive CLIP reward. Lastly, we show human analysis where the annotators strongly
prefer the CLIP reward to the CIDEr and MLE objectives according to various
criteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Cluster Faces via Pairwise Classification. (arXiv:2205.13117v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13117">
<div class="article-summary-box-inner">
<span><p>Face clustering plays an essential role in exploiting massive unlabeled face
data. Recently, graph-based face clustering methods are getting popular for
their satisfying performances. However, they usually suffer from excessive
memory consumption especially on large-scale graphs, and rely on empirical
thresholds to determine the connectivities between samples in inference, which
restricts their applications in various real-world scenes. To address such
problems, in this paper, we explore face clustering from the pairwise angle.
Specifically, we formulate the face clustering task as a pairwise relationship
classification task, avoiding the memory-consuming learning on large-scale
graphs. The classifier can directly determine the relationship between samples
and is enhanced by taking advantage of the contextual information. Moreover, to
further facilitate the efficiency of our method, we propose a rank-weighted
density to guide the selection of pairs sent to the classifier. Experimental
results demonstrate that our method achieves state-of-the-art performances on
several public clustering benchmarks at the fastest speed and shows a great
advantage in comparison with graph-based clustering methods on memory
consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Learned Source-Channel Coding for High-Fidelity Image Semantic Transmission. (arXiv:2205.13120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13120">
<div class="article-summary-box-inner">
<span><p>As one novel approach to realize end-to-end wireless image semantic
transmission, deep learning-based joint source-channel coding (deep JSCC)
method is emerging in both deep learning and communication communities.
However, current deep JSCC image transmission systems are typically optimized
for traditional distortion metrics such as peak signal-to-noise ratio (PSNR) or
multi-scale structural similarity (MS-SSIM). But for low transmission rates,
due to the imperfect wireless channel, these distortion metrics lose
significance as they favor pixel-wise preservation. To account for human visual
perception in semantic communications, it is of great importance to develop new
deep JSCC systems optimized beyond traditional PSNR and MS-SSIM metrics. In
this paper, we introduce adversarial losses to optimize deep JSCC, which tends
to preserve global semantic information and local texture. Our new deep JSCC
architecture combines encoder, wireless channel, decoder/generator, and
discriminator, which are jointly learned under both perceptual and adversarial
losses. Our method yields human visually much more pleasing results than
state-of-the-art engineered image coded transmission systems and traditional
deep JSCC systems. A user study confirms that achieving the perceptually
similar end-to-end image transmission quality, the proposed method can save
about 50\% wireless channel bandwidth cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To image, or not to image: Class-specific diffractive cameras with all-optical erasure of undesired objects. (arXiv:2205.13122v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13122">
<div class="article-summary-box-inner">
<span><p>Privacy protection is a growing concern in the digital era, with machine
vision techniques widely used throughout public and private settings. Existing
methods address this growing problem by, e.g., encrypting camera images or
obscuring/blurring the imaged information through digital algorithms. Here, we
demonstrate a camera design that performs class-specific imaging of target
objects with instantaneous all-optical erasure of other classes of objects.
This diffractive camera consists of transmissive surfaces structured using deep
learning to perform selective imaging of target classes of objects positioned
at its input field-of-view. After their fabrication, the thin diffractive
layers collectively perform optical mode filtering to accurately form images of
the objects that belong to a target data class or group of classes, while
instantaneously erasing objects of the other data classes at the output
field-of-view. Using the same framework, we also demonstrate the design of
class-specific permutation cameras, where the objects of a target data class
are pixel-wise permuted for all-optical class-specific encryption, while the
other objects are irreversibly erased from the output image. The success of
class-specific diffractive cameras was experimentally demonstrated using
terahertz (THz) waves and 3D-printed diffractive layers that selectively imaged
only one class of the MNIST handwritten digit dataset, all-optically erasing
the other handwritten digits. This diffractive camera design can be scaled to
different parts of the electromagnetic spectrum, including, e.g., the visible
and infrared wavelengths, to provide transformative opportunities for
privacy-preserving digital cameras and task-specific data-efficient imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PixelGame: Infrared small target segmentation as a Nash equilibrium. (arXiv:2205.13124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13124">
<div class="article-summary-box-inner">
<span><p>A key challenge of infrared small target segmentation (ISTS) is to balance
false negative pixels (FNs) and false positive pixels (FPs). Traditional
methods combine FNs and FPs into a single objective by weighted sum, and the
optimization process is decided by one actor. Minimizing FNs and FPs with the
same strategy leads to antagonistic decisions. To address this problem, we
propose a competitive game framework (pixelGame) from a novel perspective for
ISTS. In pixelGame, FNs and FPs are controlled by different player whose goal
is to minimize their own utility function. FNs-player and FPs-player are
designed with different strategies: One is to minimize FNs and the other is to
minimize FPs. The utility function drives the evolution of the two participants
in competition. We consider the Nash equilibrium of pixelGame as the optimal
solution. In addition, we propose maximum information modulation (MIM) to
highlight the tar-get information. MIM effectively focuses on the salient
region including small targets. Extensive experiments on two standard public
datasets prove the effectiveness of our method. Compared with other
state-of-the-art methods, our method achieves better performance in terms of
F1-measure (F1) and the intersection of union (IoU).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Learning for Unpaired Image Captioning. (arXiv:2205.13125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13125">
<div class="article-summary-box-inner">
<span><p>Unpaired Image Captioning (UIC) has been developed to learn image
descriptions from unaligned vision-language sample pairs. Existing schemes
usually adopt the visual concept reward of reinforcement learning to obtain the
alignment between visual concepts and images. However, the cross-domain
alignment is usually weak that severely constrains the overall performance of
these existing schemes. Recent successes of Vision-Language Pre-Trained Models
(VL-PTMs) have triggered the development of prompt-based learning from VL-PTMs.
We present in this paper a novel scheme based on prompt to train the UIC model,
making best use of the powerful generalization ability and abundant
vision-language prior knowledge learned under VL-PTMs. We adopt the CLIP model
for this research in unpaired image captioning. Specifically, the visual images
are taken as input to the prompt generation module, which contains the
pre-trained model as well as one feed-forward layer for prompt extraction.
Then, the input images and generated prompts are aggregated for unpaired
adversarial captioning learning. To further enhance the potential performance
of the captioning, we designed a high-quality pseudo caption filter guided by
the CLIP logits to measure correlations between predicted captions and the
corresponding images. This allows us to improve the captioning model in a
supervised learning manner. Extensive experiments on the COCO and Flickr30K
datasets have been carried out to validate the superiority of the proposed
model. We have achieved the state-of-the-art performance on the COCO dataset,
which outperforms the best UIC model by 1.9% on the BLEU-4 metric. We expect
that the proposed prompt-based UIC model will inspire a new line of research
for the VL-PTMs based captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wireless Deep Video Semantic Transmission. (arXiv:2205.13129v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13129">
<div class="article-summary-box-inner">
<span><p>In this paper, we design a new class of high-efficiency deep joint
source-channel coding methods to achieve end-to-end video transmission over
wireless channels. The proposed methods exploit nonlinear transform and
conditional coding architecture to adaptively extract semantic features across
video frames, and transmit semantic feature domain representations over
wireless channels via deep joint source-channel coding. Our framework is
collected under the name deep video semantic transmission (DVST). In
particular, benefiting from the strong temporal prior provided by the feature
domain context, the learned nonlinear transform function becomes temporally
adaptive, resulting in a richer and more accurate entropy model guiding the
transmission of current frame. Accordingly, a novel rate adaptive transmission
mechanism is developed to customize deep joint source-channel coding for video
sources. It learns to allocate the limited channel bandwidth within and among
video frames to maximize the overall transmission performance. The whole DVST
design is formulated as an optimization problem whose goal is to minimize the
end-to-end transmission rate-distortion performance under perceptual quality
metrics or machine vision task performance metrics. Across standard video
source test sequences and various communication scenarios, experiments show
that our DVST can generally surpass traditional wireless video coded
transmission schemes. The proposed DVST framework can well support future
semantic communications due to its video content-aware and machine vision task
integration abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning. (arXiv:2205.13137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13137">
<div class="article-summary-box-inner">
<span><p>In this study, we propose Mixed and Masked Image Modeling (MixMIM), a simple
but efficient MIM method that is applicable to various hierarchical Vision
Transformers. Existing MIM methods replace a random subset of input tokens with
a special MASK symbol and aim at reconstructing original image tokens from the
corrupted image. However, we find that using the MASK symbol greatly slows down
the training and causes training-finetuning inconsistency, due to the large
masking ratio (e.g., 40% in BEiT). In contrast, we replace the masked tokens of
one image with visible tokens of another image, i.e., creating a mixed image.
We then conduct dual reconstruction to reconstruct the original two images from
the mixed input, which significantly improves efficiency. While MixMIM can be
applied to various architectures, this paper explores a simpler but stronger
hierarchical Transformer, and scales with MixMIM-B, -L, and -H. Empirical
results demonstrate that MixMIM can learn high-quality visual representations
efficiently. Notably, MixMIM-B with 88M parameters achieves 85.1% top-1
accuracy on ImageNet-1K by pretraining for 600 epochs, setting a new record for
neural networks with comparable model sizes (e.g., ViT-B) among MIM methods.
Besides, its transferring performances on the other 6 datasets show MixMIM has
better FLOPs / performance tradeoff than previous MIM methods. Code is
available at https://github.com/Sense-X/MixMIM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matryoshka Representations for Adaptive Deployment. (arXiv:2205.13147v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13147">
<div class="article-summary-box-inner">
<span><p>Learned representations are a central component in modern ML systems, serving
a multitude of downstream tasks. When training such representations, it is
often the case that computational and statistical constraints for each
downstream task are unknown. In this context rigid, fixed capacity
representations can be either over or under-accommodating to the task at hand.
This leads us to ask: can we design a flexible representation that can adapt to
multiple downstream tasks with varying computational resources? Our main
contribution is Matryoshka Representation Learning (MRL) which encodes
information at different granularities and allows a single embedding to adapt
to the computational constraints of downstream tasks. MRL minimally modifies
existing representation learning pipelines and imposes no additional cost
during inference and deployment. MRL learns coarse-to-fine representations that
are at least as accurate and rich as independently trained low-dimensional
representations. The flexibility within the learned Matryoshka Representations
offer: (a) up to 14x smaller embedding size for ImageNet-1K classification at
the same level of accuracy; (b) up to 14x real-world speed-ups for large-scale
retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for
long-tail few-shot classification, all while being as robust as the original
representations. Finally, we show that MRL extends seamlessly to web-scale
datasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),
vision + language (ALIGN) and language (BERT). MRL code and pretrained models
are open-sourced at https://github.com/RAIVNLab/MRL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferable Adversarial Attack based on Integrated Gradients. (arXiv:2205.13152v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13152">
<div class="article-summary-box-inner">
<span><p>The vulnerability of deep neural networks to adversarial examples has drawn
tremendous attention from the community. Three approaches, optimizing standard
objective functions, exploiting attention maps, and smoothing decision
surfaces, are commonly used to craft adversarial examples. By tightly
integrating the three approaches, we propose a new and simple algorithm named
Transferable Attack based on Integrated Gradients (TAIG) in this paper, which
can find highly transferable adversarial examples for black-box attacks. Unlike
previous methods using multiple computational terms or combining with other
methods, TAIG integrates the three approaches into one single term. Two
versions of TAIG that compute their integrated gradients on a straight-line
path and a random piecewise linear path are studied. Both versions offer strong
transferability and can seamlessly work together with the previous methods.
Experimental results demonstrate that TAIG outperforms the state-of-the-art
methods. The code will available at https://github.com/yihuang2016/TAIG
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation. (arXiv:2205.13158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13158">
<div class="article-summary-box-inner">
<span><p>Data-driven approaches for medium-range weather forecasting are recently
shown extraordinarily promising for ensemble forecasting for their fast
inference speed compared to traditional numerical weather prediction (NWP)
models, but their forecast accuracy can hardly match the state-of-the-art
operational ECMWF Integrated Forecasting System (IFS) model. Previous
data-driven attempts achieve ensemble forecast using some simple perturbation
methods, like initial condition perturbation and Monte Carlo dropout. However,
they mostly suffer unsatisfactory ensemble performance, which is arguably
attributed to the sub-optimal ways of applying perturbation. We propose a Swin
Transformer-based Variational Recurrent Neural Network (SwinVRNN), which is a
stochastic weather forecasting model combining a SwinRNN predictor with a
perturbation module. SwinRNN is designed as a Swin Transformer-based recurrent
neural network, which predicts future states deterministically. Furthermore, to
model the stochasticity in prediction, we design a perturbation module
following the Variational Auto-Encoder paradigm to learn multivariate Gaussian
distributions of a time-variant stochastic latent variable from data. Ensemble
forecasting can be easily achieved by perturbing the model features leveraging
noise sampled from the learned distribution. We also compare four categories of
perturbation methods for ensemble forecasting, i.e. fixed distribution
perturbation, learned distribution perturbation, MC dropout, and multi model
ensemble. Comparisons on WeatherBench dataset show the learned distribution
perturbation method using our SwinVRNN model achieves superior forecast
accuracy and reasonable ensemble spread due to joint optimization of the two
targets. More notably, SwinVRNN surpasses operational IFS on surface variables
of 2-m temperature and 6-hourly total precipitation at all lead times up to
five days.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIRL: A General Framework for Hierarchical Image Representation Learning. (arXiv:2205.13159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13159">
<div class="article-summary-box-inner">
<span><p>Learning self-supervised image representations has been broadly studied to
boost various visual understanding tasks. Existing methods typically learn a
single level of image semantics like pairwise semantic similarity or image
clustering patterns. However, these methods can hardly capture multiple levels
of semantic information that naturally exists in an image dataset, e.g., the
semantic hierarchy of "Persian cat to cat to mammal" encoded in an image
database for species. It is thus unknown whether an arbitrary image
self-supervised learning (SSL) approach can benefit from learning such
hierarchical semantics. To answer this question, we propose a general framework
for Hierarchical Image Representation Learning (HIRL). This framework aims to
learn multiple semantic representations for each image, and these
representations are structured to encode image semantics from fine-grained to
coarse-grained. Based on a probabilistic factorization, HIRL learns the most
fine-grained semantics by an off-the-shelf image SSL approach and learns
multiple coarse-grained semantics by a novel semantic path discrimination
scheme. We adopt six representative image SSL methods as baselines and study
how they perform under HIRL. By rigorous fair comparison, performance gain is
observed on all the six methods for diverse downstream tasks, which, for the
first time, verifies the general effectiveness of learning hierarchical image
semantics. All source code and model weights are available at
https://github.com/hirl-team/HIRL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Field Raindrop Removal via 4D Re-sampling. (arXiv:2205.13165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13165">
<div class="article-summary-box-inner">
<span><p>The Light Field Raindrop Removal (LFRR) aims to restore the background areas
obscured by raindrops in the Light Field (LF). Compared with single image, the
LF provides more abundant information by regularly and densely sampling the
scene. Since raindrops have larger disparities than the background in the LF,
the majority of texture details occluded by raindrops are visible in other
views. In this paper, we propose a novel LFRR network by directly utilizing the
complementary pixel information of raindrop-free areas in the input raindrop
LF, which consists of the re-sampling module and the refinement module.
Specifically, the re-sampling module generates a new LF which is less polluted
by raindrops through re-sampling position predictions and the proposed 4D
interpolation. The refinement module improves the restoration of the completely
occluded background areas and corrects the pixel error caused by 4D
interpolation. Furthermore, we carefully build the first real scene LFRR
dataset for model training and validation. Experiments demonstrate that the
proposed method can effectively remove raindrops and achieves state-of-the-art
performance in both background restoration and view consistency maintenance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Latent Space of GAN through Local Dimension Estimation. (arXiv:2205.13182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13182">
<div class="article-summary-box-inner">
<span><p>The impressive success of style-based GANs (StyleGANs) in high-fidelity image
synthesis has motivated research to understand the semantic properties of their
latent spaces. Recently, a close relationship was observed between the
semantically disentangled local perturbations and the local PCA components in
the learned latent space $\mathcal{W}$. However, understanding the number of
disentangled perturbations remains challenging. Building upon this observation,
we propose a local dimension estimation algorithm for an arbitrary intermediate
layer in a pre-trained GAN model. The estimated intrinsic dimension corresponds
to the number of disentangled local perturbations. In this perspective, we
analyze the intermediate layers of the mapping network in StyleGANs. Our
analysis clarifies the success of $\mathcal{W}$-space in StyleGAN and suggests
an alternative. Moreover, the intrinsic dimension estimation opens the
possibility of unsupervised evaluation of global-basis-compatibility and
disentanglement for a latent space. Our proposed metric, called Distortion,
measures an inconsistency of intrinsic tangent space on the learned latent
space. The metric is purely geometric and does not require any additional
attribute information. Nevertheless, the metric shows a high correlation with
the global-basis-compatibility and supervised disentanglement score. Our
findings pave the way towards an unsupervised selection of globally
disentangled latent space among the intermediate latent spaces in a GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI for Porosity and Permeability Prediction from Geologic Core X-Ray Micro-Tomography. (arXiv:2205.13189v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13189">
<div class="article-summary-box-inner">
<span><p>Geologic cores are rock samples that are extracted from deep under the ground
during the well drilling process. They are used for petroleum reservoirs'
performance characterization. Traditionally, physical studies of cores are
carried out by the means of manual time-consuming experiments. With the
development of deep learning, scientists actively started working on developing
machine-learning-based approaches to identify physical properties without any
manual experiments. Several previous works used machine learning to determine
the porosity and permeability of the rocks, but either method was inaccurate or
computationally expensive. We are proposing to use self-supervised pretraining
of the very small CNN-transformer-based model to predict the physical
properties of the rocks with high accuracy in a time-efficient manner. We show
that this technique prevents overfitting even for extremely small datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree Reconstruction using Topology Optimisation. (arXiv:2205.13192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13192">
<div class="article-summary-box-inner">
<span><p>Generating accurate digital tree models from scanned environments is
invaluable for forestry, agriculture, and other outdoor industries in tasks
such as identifying biomass, fall hazards and traversability, as well as
digital applications such as animation and gaming. Existing methods for tree
reconstruction rely on feature identification (trunk, crown, etc) to
heuristically segment a forest into individual trees and generate a branch
structure graph, limiting their application to sparse trees and uniform
forests. However, the natural world is a messy place in which trees present
with significant heterogeneity and are frequently encroached upon by the
surrounding environment. We present a general method for extracting the branch
structure of trees from point cloud data, which estimates the structure of
trees by adapting the methods of structural topology optimisation to find the
optimal material distribution to support wind-loading. We present the results
of this optimisation over a wide variety of scans, and discuss the benefits and
drawbacks of this novel approach to tree structure reconstruction. Despite the
high variability of datasets containing trees, and the high rate of occlusions,
our method generates detailed and accurate tree structures in most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupled Pyramid Correlation Network for Liver Tumor Segmentation from CT images. (arXiv:2205.13199v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13199">
<div class="article-summary-box-inner">
<span><p>Purpose: Automated liver tumor segmentation from Computed Tomography (CT)
images is a necessary prerequisite in the interventions of hepatic
abnormalities and surgery planning. However, accurate liver tumor segmentation
remains challenging due to the large variability of tumor sizes and
inhomogeneous texture. Recent advances based on Fully Convolutional Network
(FCN) for medical image segmentation drew on the success of learning
discriminative pyramid features. In this paper, we propose a Decoupled Pyramid
Correlation Network (DPC-Net) that exploits attention mechanisms to fully
leverage both low- and high-level features embedded in FCN to segment liver
tumor. Methods: We first design a powerful Pyramid Feature Encoder (PFE) to
extract multi-level features from input images. Then we decouple the
characteristics of features concerning spatial dimension (i.e., height, width,
depth) and semantic dimension (i.e., channel). On top of that, we present two
types of attention modules, Spatial Correlation (SpaCor) and Semantic
Correlation (SemCor) modules, to recursively measure the correlation of
multi-level features. The former selectively emphasizes global semantic
information in low-level features with the guidance of high-level ones. The
latter adaptively enhance spatial details in high-level features with the
guidance of low-level ones. Results: We evaluate the DPC-Net on MICCAI 2017
LiTS Liver Tumor Segmentation (LiTS) challenge dataset. Dice Similarity
Coefficient (DSC) and Average Symmetric Surface Distance (ASSD) are employed
for evaluation. The proposed method obtains a DSC of 76.4% and an ASSD of 0.838
mm for liver tumor segmentation, outperforming the state-of-the-art methods. It
also achieves a competitive results with a DSC of 96.0% and an ASSD of 1.636 mm
for liver segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13213">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) have triggered the most recent and significant
breakthroughs in computer vision. Their efficient designs are mostly guided by
the indirect metric of computational complexity, i.e., FLOPs, which however has
a clear gap with the direct metric such as throughput. Thus, we propose to use
the direct speed evaluation on the target platform as the design principle for
efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT
which performs favourably against the existing state-of-the-art methods across
a spectrum of different model sizes with faster speed. At the core of LITv2 is
a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the
insight that high frequencies in an image capture local fine details and low
frequencies focus on global structures, whereas a multi-head self-attention
layer neglects the characteristic of different frequencies. Therefore, we
propose to disentangle the high/low frequency patterns in an attention layer by
separating the heads into two groups, where one group encodes high frequencies
via self-attention within each local window, and another group performs the
attention to model the global relationship between the average-pooled
low-frequency keys from each window and each query position in the input
feature map. Benefit from the efficient design for both groups, we show that
HiLo is superior to the existing attention mechanisms by comprehensively
benchmarking on FLOPs, speed and memory consumption on GPUs. Powered by HiLo,
LITv2 serves as a strong backbone for mainstream vision tasks including image
classification, dense detection and segmentation. Code is available at
https://github.com/zip-group/LITv2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. (arXiv:2205.13218v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13218">
<div class="article-summary-box-inner">
<span><p>Real-world applications require the classification model to adapt to new
classes without forgetting old ones. Correspondingly, Class-Incremental
Learning (CIL) aims to train a model with limited memory size to meet this
requirement. Typical CIL methods tend to save representative exemplars from
former classes to resist forgetting, while recent works find that storing
models from history can substantially boost the performance. However, the
stored models are not counted into the memory budget, which implicitly results
in unfair comparisons. We find that when counting the model size into the total
budget and comparing methods with aligned memory size, saving models do not
consistently work, especially for the case with limited memory budgets. As a
result, we need to holistically evaluate different CIL methods at different
memory scales and simultaneously consider accuracy and memory size for
measurement. On the other hand, we dive deeply into the construction of the
memory buffer for memory efficiency. By analyzing the effect of different
layers in the network, we find that shallow and deep layers have different
characteristics in CIL. Motivated by this, we propose a simple yet effective
baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends
specialized layers based on the shared generalized representations, efficiently
extracting diverse representations with modest cost and maintaining
representative exemplars. Extensive experiments on benchmark datasets validate
MEMO's competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13219">
<div class="article-summary-box-inner">
<span><p>Obtaining gold standard annotated data for object detection is often costly,
involving human-level effort. Semi-supervised object detection algorithms solve
the problem with a small amount of gold-standard labels and a large unlabelled
dataset used to generate silver-standard labels. But training on the silver
standard labels does not produce good results, because they are
machine-generated annotations. In this work, we design a modified loss function
to train on large silver standard annotated sets generated by a weak annotator.
We include a confidence metric associated with the annotation as an additional
term in the loss function, signifying the quality of the annotation. We test
the effectiveness of our approach on various test sets and use numerous
variations to compare the results with some of the current approaches to object
detection. In comparison with the baseline where no confidence metric is used,
we achieved a 4\% gain in mAP with 25\% labeled data and 10\% gain in mAP with
50\% labeled data by using the proposed confidence metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DGSVis: Visual Analysis of Hierarchical Snapshots in Dynamic Graph. (arXiv:2205.13220v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13220">
<div class="article-summary-box-inner">
<span><p>Dynamic graph visualization attracts researchers' concentration as it
represents time-varying relationships between entities in multiple domains
(e.g., social media analysis, academic cooperation analysis, team sports
analysis). Integrating visual analytic methods is consequential in presenting,
comparing, and reviewing dynamic graphs. Even though dynamic graph
visualization is developed for many years, how to effectively visualize
large-scale and time-intensive dynamic graph data with subtle changes is still
challenging for researchers. To provide an effective analysis method for this
type of dynamic graph data, we propose a snapshot generation algorithm
involving Human-In-Loop to help users divide the dynamic graphs into
multi-granularity and hierarchical snapshots for further analysis. In addition,
we design a visual analysis prototype system (DGSVis) to assist users in
accessing the dynamic graph insights effectively. DGSVis integrates a graphical
operation interface to help users generate snapshots visually and
interactively. It is equipped with the overview and details for visualizing
hierarchical snapshots of the dynamic graph data. To illustrate the usability
and efficiency of our proposed methods for this type of dynamic graph data, we
introduce two case studies based on basketball player networks in a
competition. In addition, we conduct an evaluation and receive exciting
feedback from experienced visualization experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images. (arXiv:2205.13226v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13226">
<div class="article-summary-box-inner">
<span><p>Survival time prediction from medical images is important for treatment
planning, where accurate estimations can improve healthcare quality. One issue
affecting the training of survival models is censored data. Most of the current
survival prediction approaches are based on Cox models that can deal with
censored data, but their application scope is limited because they output a
hazard function instead of a survival time. On the other hand, methods that
predict survival time usually ignore censored data, resulting in an
under-utilization of the training set. In this work, we propose a new training
method that predicts survival time using all censored and uncensored data. We
propose to treat censored data as samples with a lower-bound time to death and
estimate pseudo labels to semi-supervise a censor-aware survival time
regressor. We evaluate our method on pathology and x-ray images from the
TCGA-GM and NLST datasets. Our results establish the state-of-the-art survival
prediction accuracy on both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denial-of-Service Attacks on Learned Image Compression. (arXiv:2205.13253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13253">
<div class="article-summary-box-inner">
<span><p>Deep learning techniques have shown promising results in image compression,
with competitive bitrate and image reconstruction quality from compressed
latent. However, while image compression has progressed towards higher peak
signal-to-noise ratio (PSNR) and fewer bits per pixel (bpp), their robustness
to corner-case images has never received deliberation. In this work, we, for
the first time, investigate the robustness of image compression systems where
imperceptible perturbation of input images can precipitate a significant
increase in the bitrate of their compressed latent. To characterize the
robustness of state-of-the-art learned image compression, we mount white and
black-box attacks. Our results on several image compression models with various
bitrate qualities show that they are surprisingly fragile, where the white-box
attack achieves up to 56.326x and black-box 1.947x bpp change. To improve
robustness, we propose a novel model which incorporates attention modules and a
basic factorized entropy model, resulting in a promising trade-off between the
PSNR/bpp ratio and robustness to adversarial attacks that surpasses existing
learned image compressors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Customized Self-Supervised Pre-training with Scalable Dynamic Routing. (arXiv:2205.13267v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13267">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL), especially contrastive methods, has raised
attraction recently as it learns effective transferable representations without
semantic annotations. A common practice for self-supervised pre-training is to
use as much data as possible. For a specific downstream task, however,
involving irrelevant data in pre-training may degenerate the downstream
performance, observed from our extensive experiments. On the other hand, for
existing SSL methods, it is burdensome and infeasible to use different
downstream-task-customized datasets in pre-training for different tasks. To
address this issue, we propose a novel SSL paradigm called Scalable Dynamic
Routing (SDR), which can be trained once and deployed efficiently to different
downstream tasks with task-customized pre-trained models. Specifically, we
construct the SDRnet with various sub-nets and train each sub-net with only one
subset of the data by data-aware progressive training. When a downstream task
arrives, we route among all the pre-trained sub-nets to get the best along with
its corresponding weights. Experiment results show that our SDR can train 256
sub-nets on ImageNet simultaneously, which provides better transfer performance
than a unified model trained on the full ImageNet, achieving state-of-the-art
(SOTA) averaged accuracy over 11 downstream classification tasks and AP on
PASCAL VOC detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MemeTector: Enforcing deep focus for meme detection. (arXiv:2205.13268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13268">
<div class="article-summary-box-inner">
<span><p>Image memes and specifically their widely-known variation image macros, is a
special new media type that combines text with images and is used in social
media to playfully or subtly express humour, irony, sarcasm and even hate. It
is important to accurately retrieve image memes from social media to better
capture the cultural and social aspects of online phenomena and detect
potential issues (hate-speech, disinformation). Essentially, the background
image of an image macro is a regular image easily recognized as such by humans
but cumbersome for the machine to do so due to feature map similarity with the
complete image macro. Hence, accumulating suitable feature maps in such cases
can lead to deep understanding of the notion of image memes. To this end, we
propose a methodology that utilizes the visual part of image memes as instances
of the regular image class and the initial image memes as instances of the
image meme class to force the model to concentrate on the critical parts that
characterize an image meme. Additionally, we employ a trainable attention
mechanism on top of a standard ViT architecture to enhance the model's ability
to focus on these critical parts and make the predictions interpretable.
Several training and test scenarios involving web-scraped regular images of
controlled text presence are considered in terms of model robustness and
accuracy. The findings indicate that light visual part utilization combined
with sufficient text presence during training provides the best and most robust
model, surpassing state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multi-object Segmentation Using Attention and Soft-argmax. (arXiv:2205.13271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13271">
<div class="article-summary-box-inner">
<span><p>We introduce a new architecture for unsupervised object-centric
representation learning and multi-object detection and segmentation, which uses
an attention mechanism to associate a feature vector to each object present in
the scene and to predict the coordinates of these objects using soft-argmax. A
transformer encoder handles occlusions and redundant detections, and a separate
pre-trained background model is in charge of background reconstruction. We show
that this architecture significantly outperforms the state of the art on
complex synthetic benchmarks and provide examples of applications to real-world
traffic videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCN-Pose: A Pruned and Quantized CNN for Robot Pose Estimation for Constrained Devices. (arXiv:2205.13272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13272">
<div class="article-summary-box-inner">
<span><p>IoT devices suffer from resource limitations, such as processor, RAM, and
disc storage. These limitations become more evident when handling demanding
applications, such as deep learning, well-known for their heavy computational
requirements. A case in point is robot pose estimation, an application that
predicts the critical points of the desired image object. One way to mitigate
processing and storage problems is compressing that deep learning application.
This paper proposes a new CNN for the pose estimation while applying the
compression techniques of pruning and quantization to reduce his demands and
improve the response time. While the pruning process reduces the total number
of parameters required for inference, quantization decreases the precision of
the floating-point. We run the approach using a pose estimation task for a
robotic arm and compare the results in a high-end device and a constrained
device. As metrics, we consider the number of Floating-point Operations Per
Second(FLOPS), the total of mathematical computations, the calculation of
parameters, the inference time, and the number of video frames processed per
second. In addition, we undertake a qualitative evaluation where we compare the
output image predicted for each pruned network with the corresponding original
one. We reduce the originally proposed network to a 70% pruning rate, implying
an 88.86% reduction in parameters, 94.45% reduction in FLOPS, and for the disc
storage, we reduced the requirement in 70% while increasing error by a mere
$1\%$. With regard input image processing, this metric increases from 11.71 FPS
to 41.9 FPS for the Desktop case. When using the constrained device, image
processing augmented from 2.86 FPS to 10.04 FPS. The higher processing rate of
image frames achieved by the proposed approach allows a much shorter response
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks. (arXiv:2205.13273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13273">
<div class="article-summary-box-inner">
<span><p>This paper features convolutional neural networks defined on hypercomplex
algebras applied to classify lymphocytes in blood smear digital microscopic
images. Such classification is helpful for the diagnosis of acute lymphoblast
leukemia (ALL), a type of blood cancer. We perform the classification task
using eight hypercomplex-valued convolutional neural networks (HvCNNs) along
with real-valued convolutional networks. Our results show that HvCNNs perform
better than the real-valued model, showcasing higher accuracy with a much
smaller number of parameters. Moreover, we found that HvCNNs based on Clifford
algebras processing HSV-encoded images attained the highest observed
accuracies. Precisely, our HvCNN yielded an average accuracy rate of 96.6%
using the ALL-IDB2 dataset with a 50% train-test split, a value extremely close
to the state-of-the-art models but using a much simpler architecture with
significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VIDI: A Video Dataset of Incidents. (arXiv:2205.13277v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13277">
<div class="article-summary-box-inner">
<span><p>Automatic detection of natural disasters and incidents has become more
important as a tool for fast response. There have been many studies to detect
incidents using still images and text. However, the number of approaches that
exploit temporal information is rather limited. One of the main reasons for
this is that a diverse video dataset with various incident types does not
exist. To address this need, in this paper we present a video dataset, Video
Dataset of Incidents, VIDI, that contains 4,534 video clips corresponding to 43
incident categories. Each incident class has around 100 videos with a duration
of ten seconds on average. To increase diversity, the videos have been searched
in several languages. To assess the performance of the recent state-of-the-art
approaches, Vision Transformer and TimeSformer, as well as to explore the
contribution of video-based information for incident classification, we
performed benchmark experiments on the VIDI and Incidents Dataset. We have
shown that the recent methods improve the incident classification accuracy. We
have found that employing video data is very beneficial for the task. By using
the video data, the top-1 accuracy is increased to 76.56% from 67.37%, which
was obtained using a single frame. VIDI will be made publicly available.
Additional materials can be found at the following link:
https://github.com/vididataset/VIDI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Segmentation for Thermal Images: A Comparative Survey. (arXiv:2205.13278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13278">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a challenging task since it requires excessively
more low-level spatial information of the image compared to other computer
vision problems. The accuracy of pixel-level classification can be affected by
many factors, such as imaging limitations and the ambiguity of object
boundaries in an image. Conventional methods exploit three-channel RGB images
captured in the visible spectrum with deep neural networks (DNN). Thermal
images can significantly contribute during the segmentation since thermal
imaging cameras are capable of capturing details despite the weather and
illumination conditions. Using infrared spectrum in semantic segmentation has
many real-world use cases, such as autonomous driving, medical imaging,
agriculture, defense industry, etc. Due to this wide range of use cases,
designing accurate semantic segmentation algorithms with the help of infrared
spectrum is an important challenge. One approach is to use both visible and
infrared spectrum images as inputs. These methods can accomplish higher
accuracy due to enriched input information, with the cost of extra effort for
the alignment and processing of multiple inputs. Another approach is to use
only thermal images, enabling less hardware cost for smaller use cases. Even
though there are multiple surveys on semantic segmentation methods, the
literature lacks a comprehensive survey centered explicitly around semantic
segmentation using infrared spectrum. This work aims to fill this gap by
presenting algorithms in the literature and categorizing them by their input
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Objects Matter: Learning Object Relation Graph for Robust Camera Relocalization. (arXiv:2205.13280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13280">
<div class="article-summary-box-inner">
<span><p>Visual relocalization aims to estimate the pose of a camera from one or more
images. In recent years deep learning based pose regression methods have
attracted many attentions. They feature predicting the absolute poses without
relying on any prior built maps or stored images, making the relocalization
very efficient. However, robust relocalization under environments with complex
appearance changes and real dynamics remains very challenging. In this paper,
we propose to enhance the distinctiveness of the image features by extracting
the deep relationship among objects. In particular, we extract objects in the
image and construct a deep object relation graph (ORG) to incorporate the
semantic connections and relative spatial clues of the objects. We integrate
our ORG module into several popular pose regression models. Extensive
experiments on various public indoor and outdoor datasets demonstrate that our
method improves the performance significantly and outperforms the previous
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surround-view Fisheye Camera Perception for Automated Driving: Overview, Survey and Challenges. (arXiv:2205.13281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13281">
<div class="article-summary-box-inner">
<span><p>Surround-view fisheye cameras are commonly used for near-field sensing in
automated driving. Four fisheye cameras on four sides of the vehicle are
sufficient to cover 360{\deg} around the vehicle capturing the entire
near-field region. Some primary use cases are automated parking, traffic jam
assist, and urban driving. There are limited datasets and very little work on
near-field perception tasks as the main focus in automotive perception is on
far-field perception. In contrast to far-field, surround-view perception poses
additional challenges due to high precision object detection requirements of
10cm and partial visibility of objects. Due to the large radial distortion of
fisheye cameras, standard algorithms can not be extended easily to the
surround-view use case. Thus we are motivated to provide a self-contained
reference for automotive fisheye camera perception for researchers and
practitioners. Firstly, we provide a unified and taxonomic treatment of
commonly used fisheye camera models. Secondly, we discuss various perception
tasks and existing literature. Finally, we discuss the challenges and future
direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Eigenvalues of Global Covariance Pooling for Fine-grained Visual Recognition. (arXiv:2205.13282v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13282">
<div class="article-summary-box-inner">
<span><p>The Fine-Grained Visual Categorization (FGVC) is challenging because the
subtle inter-class variations are difficult to be captured. One notable
research line uses the Global Covariance Pooling (GCP) layer to learn powerful
representations with second-order statistics, which can effectively model
inter-class differences. In our previous conference paper, we show that
truncating small eigenvalues of the GCP covariance can attain smoother gradient
and improve the performance on large-scale benchmarks. However, on fine-grained
datasets, truncating the small eigenvalues would make the model fail to
converge. This observation contradicts the common assumption that the small
eigenvalues merely correspond to the noisy and unimportant information.
Consequently, ignoring them should have little influence on the performance. To
diagnose this peculiar behavior, we propose two attribution methods whose
visualizations demonstrate that the seemingly unimportant small eigenvalues are
crucial as they are in charge of extracting the discriminative class-specific
features. Inspired by this observation, we propose a network branch dedicated
to magnifying the importance of small eigenvalues. Without introducing any
additional parameters, this branch simply amplifies the small eigenvalues and
achieves state-of-the-art performances of GCP methods on three fine-grained
benchmarks. Furthermore, the performance is also competitive against other FGVC
approaches on larger datasets. Code is available at
\href{https://github.com/KingJamesSong/DifferentiableSVD}{https://github.com/KingJamesSong/DifferentiableSVD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analytical Interpretation of Latent Codes in InfoGAN with SAR Images. (arXiv:2205.13294v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13294">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) can synthesize abundant
photo-realistic synthetic aperture radar (SAR) images. Some recent GANs (e.g.,
InfoGAN), are even able to edit specific properties of the synthesized images
by introducing latent codes. It is crucial for SAR image synthesis since the
targets in real SAR images are with different properties due to the imaging
mechanism. Despite the success of InfoGAN in manipulating properties, there
still lacks a clear explanation of how these latent codes affect synthesized
properties, thus editing specific properties usually relies on empirical
trials, unreliable and time-consuming. In this paper, we show that latent codes
are disentangled to affect the properties of SAR images in a non-linear manner.
By introducing some property estimators for latent codes, we are able to
provide a completely analytical nonlinear model to decompose the entangled
causality between latent codes and different properties. The qualitative and
quantitative experimental results further reveal that the properties can be
calculated by latent codes, inversely, the satisfying latent codes can be
estimated given desired properties. In this case, properties can be manipulated
by latent codes as we expect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Interpretable Tree for Pedestrian Trajectory Prediction. (arXiv:2205.13296v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13296">
<div class="article-summary-box-inner">
<span><p>Understanding the multiple socially-acceptable future behaviors is an
essential task for many vision applications. In this paper, we propose a
tree-based method, termed as Social Interpretable Tree (SIT), to address this
multi-modal prediction task, where a hand-crafted tree is built depending on
the prior information of observed trajectory to model multiple future
trajectories. Specifically, a path in the tree from the root to leaf represents
an individual possible future trajectory. SIT employs a coarse-to-fine
optimization strategy, in which the tree is first built by high-order velocity
to balance the complexity and coverage of the tree and then optimized greedily
to encourage multimodality. Finally, a teacher-forcing refining operation is
used to predict the final fine trajectory. Compared with prior methods which
leverage implicit latent variables to represent possible future trajectories,
the path in the tree can explicitly explain the rough moving behaviors (e.g.,
go straight and then turn right), and thus provides better interpretability.
Despite the hand-crafted tree, the experimental results on ETH-UCY and Stanford
Drone datasets demonstrate that our method is capable of matching or exceeding
the performance of state-of-the-art methods. Interestingly, the experiments
show that the raw built tree without training outperforms many prior deep
neural network based approaches. Meanwhile, our method presents sufficient
flexibility in long-term prediction and different best-of-$K$ predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment of CT Images. (arXiv:2205.13297v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13297">
<div class="article-summary-box-inner">
<span><p>Reliably detecting diseases using relevant biological information is crucial
for real-world applicability of deep learning techniques in medical imaging. We
debias deep learning models during training against unknown bias - without
preprocessing/filtering the input beforehand or assuming specific knowledge
about its distribution or precise nature in the dataset. We use control regions
as surrogates that carry information regarding the bias, employ the classifier
model to extract features, and suppress biased intermediate features with our
custom, modular DecorreLayer. We evaluate our method on a dataset of 952 lung
computed tomography scans by introducing simulated biases w.r.t. reconstruction
kernel and noise level and propose including an adversarial test set in
evaluations of bias reduction techniques. In a moderately sized model
architecture, applying the proposed method to learn from data exhibiting a
strong bias, it near-perfectly recovers the classification performance observed
when training with corresponding unbiased data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SARS-CoV-2 Result Interpretation based on Image Analysis of Lateral Flow Devices. (arXiv:2205.13311v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13311">
<div class="article-summary-box-inner">
<span><p>The widely used gene quantisation technique, Lateral Flow Device (LFD), is
now commonly used to detect the presence of SARS-CoV-2. It is enabling the
control and prevention of the spread of the virus. Depending on the viral load,
LFD have different sensitivity and self-test for normal user present additional
challenge to interpret the result. With the evolution of machine learning
algorithms, image processing and analysis has seen unprecedented growth. In
this interdisciplinary study, we employ novel image analysis methods of
computer vision and machine learning field to study visual features of the
control region of LFD. Here, we automatically derive results for any image
containing LFD into positive, negative or inconclusive. This will reduce the
burden of human involvement of health workers and perception bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Architecture Self-supervised Video Representation Learning. (arXiv:2205.13313v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13313">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new cross-architecture contrastive learning
(CACL) framework for self-supervised video representation learning. CACL
consists of a 3D CNN and a video transformer which are used in parallel to
generate diverse positive pairs for contrastive learning. This allows the model
to learn strong representations from such diverse yet meaningful pairs.
Furthermore, we introduce a temporal self-supervised learning module able to
predict an Edit distance explicitly between two video sequences in the temporal
order. This enables the model to learn a rich temporal representation that
compensates strongly to the video-level representation learned by the CACL. We
evaluate our method on the tasks of video retrieval and action recognition on
UCF101 and HMDB51 datasets, where our method achieves excellent performance,
surpassing the state-of-the-art methods such as VideoMoCo and MoCo+BE by a
large margin. The code is made available at https://github.com/guoshengcv/CACL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13326">
<div class="article-summary-box-inner">
<span><p>This paper describes the methods submitted for evaluation to the SHREC 2022
track on pothole and crack detection in the road pavement. A total of 7
different runs for the semantic segmentation of the road surface are compared,
6 from the participants plus a baseline method. All methods exploit Deep
Learning techniques and their performance is tested using the same environment
(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic
segmentation image/mask pairs and 797 RGB-D video clips collected with the
latest depth cameras was made available to the participants. The methods are
then evaluated on the 496 image/mask pairs in the validation set, on the 504
pairs in the test set and finally on 8 video clips. The analysis of the results
is based on quantitative metrics for image segmentation and qualitative
analysis of the video clips. The participation and the results show that the
scenario is of great interest and that the use of RGB-D data is still
challenging in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13331">
<div class="article-summary-box-inner">
<span><p>This paper deals with deep transductive learning, and proposes TransBoost as
a procedure for fine-tuning any deep neural model to improve its performance on
any (unlabeled) test set provided at training time. TransBoost is inspired by a
large margin principle and is efficient and simple to use. The ImageNet
classification performance is consistently and significantly improved with
TransBoost on many architectures such as ResNets, MobileNetV3-L,
EfficientNetB0, ViT-S, and ConvNext-T. Additionally we show that TransBoost is
effective on a wide variety of image classification datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning What and Where -- Unsupervised Disentangling Location and Identity Tracking. (arXiv:2205.13349v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13349">
<div class="article-summary-box-inner">
<span><p>Our brain can almost effortlessly decompose visual data streams into
background and salient objects. Moreover, it can track the objects and
anticipate their motion and interactions. In contrast, recent object reasoning
datasets, such as CATER, have revealed fundamental shortcomings of current
vision-based AI systems, particularly when targeting explicit object encodings,
object permanence, and object reasoning. We introduce an unsupervised
disentangled LOCation and Identity tracking system (Loci), which excels on the
CATER tracking challenge. Inspired by the dorsal-ventral pathways in the brain,
Loci tackles the what-and-where binding problem by means of a self-supervised
segregation mechanism. Our autoregressive neural network partitions and
distributes the visual input stream across separate, identically-parameterized
and autonomously recruited neural network modules. Each module binds what with
where, that is, compressed Gestalt encodings with locations. On the deep latent
encoding levels interaction dynamics are processed. Besides exhibiting superior
performance in current benchmarks, we propose that Loci may set the stage for
deeper, explanation-oriented video processing -- akin to some deeper networked
processes in the brain that appear to integrate individual entity and
spatiotemporal interaction dynamics into event structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Shot Face Reenactment on Megapixels. (arXiv:2205.13368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13368">
<div class="article-summary-box-inner">
<span><p>The goal of face reenactment is to transfer a target expression and head pose
to a source face while preserving the source identity. With the popularity of
face-related applications, there has been much research on this topic. However,
the results of existing methods are still limited to low-resolution and lack
photorealism. In this work, we present a one-shot and high-resolution face
reenactment method called MegaFR. To be precise, we leverage StyleGAN by using
3DMM-based rendering images and overcome the lack of high-quality video
datasets by designing a loss function that works without high-quality videos.
Also, we apply iterative refinement to deal with extreme poses and/or
expressions. Since the proposed method controls source images through 3DMM
parameters, we can explicitly manipulate source images. We apply MegaFR to
various applications such as face frontalization, eye in-painting, and talking
head generation. Experimental results show that our method successfully
disentangles identity from expression and head pose, and outperforms
conventional methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning. (arXiv:2205.13383v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13383">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to Trojan attacks. Existing attacks use
visible patterns (e.g., a patch or image transformations) as triggers, which
are vulnerable to human inspection. In this paper, we propose stealthy and
efficient Trojan attacks, BppAttack. Based on existing biology literature on
human visual systems, we propose to use image quantization and dithering as the
Trojan trigger, making imperceptible changes. It is a stealthy and efficient
attack without training auxiliary models. Due to the small changes made to
images, it is hard to inject such triggers during training. To alleviate this
problem, we propose a contrastive learning based approach that leverages
adversarial attacks to generate negative sample pairs so that the learned
trigger is precise and accurate. The proposed method achieves high attack
success rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and
CelebA. It also effectively bypasses existing Trojan defenses and human
inspection. Our code can be found in
https://github.com/RU-System-Software-and-Security/BppAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning for Visual Search with Backward Consistent Feature Embedding. (arXiv:2205.13384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13384">
<div class="article-summary-box-inner">
<span><p>In visual search, the gallery set could be incrementally growing and added to
the database in practice. However, existing methods rely on the model trained
on the entire dataset, ignoring the continual updating of the model. Besides,
as the model updates, the new model must re-extract features for the entire
gallery set to maintain compatible feature space, imposing a high computational
cost for a large gallery set. To address the issues of long-term visual search,
we introduce a continual learning (CL) approach that can handle the
incrementally growing gallery set with backward embedding consistency. We
enforce the losses of inter-session data coherence, neighbor-session model
coherence, and intra-session discrimination to conduct a continual learner. In
addition to the disjoint setup, our CL solution also tackles the situation of
increasingly adding new classes for the blurry boundary without assuming all
categories known in the beginning and during model update. To our knowledge,
this is the first CL method both tackling the issue of backward-consistent
feature embedding and allowing novel classes to occur in the new sessions.
Extensive experiments on various benchmarks show the efficacy of our approach
under a wide range of setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Physical-World Adversarial Attack Against 3D Face Recognition. (arXiv:2205.13412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13412">
<div class="article-summary-box-inner">
<span><p>3D face recognition systems have been widely employed in intelligent
terminals, among which structured light imaging is a common method to measure
the 3D shape. However, this method could be easily attacked, leading to
inaccurate 3D face recognition. In this paper, we propose a novel,
physically-achievable attack on the fringe structured light system, named
structured light attack. The attack utilizes a projector to project optical
adversarial fringes on faces to generate point clouds with well-designed
noises. We firstly propose a 3D transform-invariant loss function to enhance
the robustness of 3D adversarial examples in the physical-world attack. Then we
reverse the 3D adversarial examples to the projector's input to place noises on
phase-shift images, which models the process of structured light imaging. A
real-world structured light system is constructed for the attack and several
state-of-the-art 3D face recognition neural networks are tested. Experiments
show that our method can attack the physical system successfully and only needs
minor modifications of projected images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation. (arXiv:2205.13425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13425">
<div class="article-summary-box-inner">
<span><p>Action classification has made great progress, but segmenting and recognizing
actions from long untrimmed videos remains a challenging problem. Most
state-of-the-art methods focus on designing temporal convolution-based models,
but the limitations on modeling long-term temporal dependencies and
inflexibility of temporal convolutions limit the potential of these models.
Recently, Transformer-based models with flexible and strong sequence modeling
ability have been applied in various tasks. However, the lack of inductive bias
and the inefficiency of handling long video sequences limit the application of
Transformer in action segmentation. In this paper, we design a pure
Transformer-based model without temporal convolutions by incorporating the
U-Net architecture. The U-Transformer architecture reduces complexity while
introducing an inductive bias that adjacent frames are more likely to belong to
the same class, but the introduction of coarse resolutions results in the
misclassification of boundaries. We observe that the similarity distribution
between a boundary frame and its neighboring frames depends on whether the
boundary frame is the start or end of an action segment. Therefore, we further
propose a boundary-aware loss based on the distribution of similarity scores
between frames from attention modules to enhance the ability to recognize
boundaries. Extensive experiments show the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Information Divergence: A Unified Metric for Multimodal Generative Models. (arXiv:2205.13445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13445">
<div class="article-summary-box-inner">
<span><p>Text-to-image generation and image captioning are recently emerged as a new
experimental paradigm to assess machine intelligence. They predict continuous
quantity accompanied by their sampling techniques in the generation, making
evaluation complicated and intractable to get marginal distributions. Based on
a recent trend that multimodal generative evaluations exploit a
vison-and-language pre-trained model, we propose the negative Gaussian
cross-mutual information using the CLIP features as a unified metric, coined by
Mutual Information Divergence (MID). To validate, we extensively compare it
with competing metrics using carefully-generated or human-annotated judgments
in text-to-image generation and image captioning tasks. The proposed MID
significantly outperforms the competitive methods by having consistency across
benchmarks, sample parsimony, and robustness toward the exploited CLIP model.
We look forward to seeing the underrepresented implications of the Gaussian
cross-mutual information in multimodal representation learning and the future
works based on this novel proposition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual evaluation for lifelong learning: Identifying the stability gap. (arXiv:2205.13452v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13452">
<div class="article-summary-box-inner">
<span><p>Introducing a time dependency on the data generating distribution has proven
to be difficult for gradient-based training of neural networks, as the greedy
updates result in catastrophic forgetting of previous timesteps. Continual
learning aims to overcome the greedy optimization to enable continuous
accumulation of knowledge over time. The data stream is typically divided into
locally stationary distributions, called tasks, allowing task-based evaluation
on held-out data from the training tasks. Contemporary evaluation protocols and
metrics in continual learning are task-based and quantify the trade-off between
stability and plasticity only at task transitions. However, our empirical
evidence suggests that between task transitions significant, temporary
forgetting can occur, remaining unidentified in task-based evaluation.
Therefore, we propose a framework for continual evaluation that establishes
per-iteration evaluation and define a new set of metrics that enables
identifying the worst-case performance of the learner over its lifetime.
Performing continual evaluation, we empirically identify that replay suffers
from a stability gap: upon learning a new task, there is a substantial but
transient decrease in performance on past tasks. Further conceptual and
empirical analysis suggests not only replay-based, but also
regularization-based continual learning methods are prone to the stability gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2D versus 3D Convolutional Spiking Neural Networks Trained with Unsupervised STDP for Human Action Recognition. (arXiv:2205.13474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13474">
<div class="article-summary-box-inner">
<span><p>Current advances in technology have highlighted the importance of video
analysis in the domain of computer vision. However, video analysis has
considerably high computational costs with traditional artificial neural
networks (ANNs). Spiking neural networks (SNNs) are third generation
biologically plausible models that process the information in the form of
spikes. Unsupervised learning with SNNs using the spike timing dependent
plasticity (STDP) rule has the potential to overcome some bottlenecks of
regular artificial neural networks, but STDP-based SNNs are still immature and
their performance is far behind that of ANNs. In this work, we study the
performance of SNNs when challenged with the task of human action recognition,
because this task has many real-time applications in computer vision, such as
video surveillance. In this paper we introduce a multi-layered 3D convolutional
SNN model trained with unsupervised STDP. We compare the performance of this
model to those of a 2D STDP-based SNN when challenged with the KTH and Weizmann
datasets. We also compare single-layer and multi-layer versions of these models
in order to get an accurate assessment of their performance. We show that
STDP-based convolutional SNNs can learn motion patterns using 3D kernels, thus
enabling motion-based recognition from videos. Finally, we give evidence that
3D convolution is superior to 2D convolution with STDP-based SNNs, especially
when dealing with long video sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Perceptual Color Differences of Smartphone Photography. (arXiv:2205.13489v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13489">
<div class="article-summary-box-inner">
<span><p>Measuring perceptual color differences (CDs) is of great importance in modern
smartphone photography. Despite the long history, most CD measures have been
constrained by psychophysical data of homogeneous color patches or a limited
number of simplistic natural images. It is thus questionable whether existing
CD measures generalize in the age of smartphone photography characterized by
greater content complexities and learning-based image signal processors. In
this paper, we put together so far the largest image dataset for perceptual CD
assessment, in which the natural images are 1) captured by six flagship
smartphones, 2) altered by Photoshop, 3) post-processed by built-in filters of
the smartphones, and 4) reproduced with incorrect color profiles. We then
conduct a large-scale psychophysical experiment to gather perceptual CDs of
30,000 image pairs in a carefully controlled laboratory environment. Based on
the newly established dataset, we make one of the first attempts to construct
an end-to-end learnable CD formula based on a lightweight neural network, as a
generalization of several previous metrics. Extensive experiments demonstrate
that the optimized formula outperforms 28 existing CD measures by a large
margin, offers reasonable local CD maps without the use of dense supervision,
generalizes well to color patch data, and empirically behaves as a proper
metric in the mathematical sense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemAffiNet: Semantic-Affine Transformation for Point Cloud Segmentation. (arXiv:2205.13490v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13490">
<div class="article-summary-box-inner">
<span><p>Conventional point cloud semantic segmentation methods usually employ an
encoder-decoder architecture, where mid-level features are locally aggregated
to extract geometric information. However, the over-reliance on these
class-agnostic local geometric representations may raise confusion between
local parts from different categories that are similar in appearance or
spatially adjacent. To address this issue, we argue that mid-level features can
be further enhanced with semantic information, and propose semantic-affine
transformation that transforms features of mid-level points belonging to
different categories with class-specific affine parameters. Based on this
technique, we propose SemAffiNet for point cloud semantic segmentation, which
utilizes the attention mechanism in the Transformer module to implicitly and
explicitly capture global structural knowledge within local parts for overall
comprehension of each category. We conduct extensive experiments on the
ScanNetV2 and NYUv2 datasets, and evaluate semantic-affine transformation on
various 3D point cloud and 2D image segmentation baselines, where both
qualitative and quantitative results demonstrate the superiority and
generalization ability of our proposed approach. Code is available at
https://github.com/wangzy22/SemAffiNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Green Hierarchical Vision Transformer for Masked Image Modeling. (arXiv:2205.13515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13515">
<div class="article-summary-box-inner">
<span><p>We present an efficient approach for Masked Image Modeling (MIM) with
hierarchical Vision Transformers (ViTs), e.g., Swin Transformer, allowing the
hierarchical ViTs to discard masked patches and operate only on the visible
ones. Our approach consists of two key components. First, for the window
attention, we design a Group Window Attention scheme following the
Divide-and-Conquer strategy. To mitigate the quadratic complexity of the
self-attention w.r.t. the number of patches, group attention encourages a
uniform partition that visible patches within each local window of arbitrary
size can be grouped with equal size, where masked self-attention is then
performed within each group. Second, we further improve the grouping strategy
via the Dynamic Programming algorithm to minimize the overall computation cost
of the attention on the grouped patches. As a result, MIM now can work on
hierarchical ViTs in a green and efficient way. For example, we can train the
hierarchical ViTs about 2.7$\times$ faster and reduce the GPU memory usage by
70%, while still enjoying competitive performance on ImageNet classification
and the superiority on downstream COCO object detection benchmarks. Code and
pre-trained models have been made publicly available at
https://github.com/LayneH/GreenMIM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PREF: Phasorial Embedding Fields for Compact Neural Representations. (arXiv:2205.13524v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13524">
<div class="article-summary-box-inner">
<span><p>We present a phasorial embedding field \emph{PREF} as a compact
representation to facilitate neural signal modeling and reconstruction tasks.
Pure multi-layer perceptron (MLP) based neural techniques are biased towards
low frequency signals and have relied on deep layers or Fourier encoding to
avoid losing details. PREF instead employs a compact and physically explainable
encoding field based on the phasor formulation of the Fourier embedding space.
We conduct a comprehensive theoretical analysis to demonstrate the advantages
of PREF over the latest spatial embedding techniques. We then develop a highly
efficient frequency learning framework using an approximated inverse Fourier
transform scheme for PREF along with a novel Parseval regularizer. Extensive
experiments show our compact PREF-based neural signal processing technique is
on par with the state-of-the-art in 2D image completion, 3D SDF surface
regression, and 5D radiance field reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. (arXiv:2205.13535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13535">
<div class="article-summary-box-inner">
<span><p>Although the pre-trained Vision Transformers (ViTs) achieved great success in
computer vision, adapting a ViT to various image and video tasks is challenging
because of its heavy computation and storage burdens, where each model needs to
be independently and comprehensively fine-tuned to different tasks, limiting
its transferability in different domains. To address this challenge, we propose
an effective adaptation approach for Transformer, namely AdaptFormer, which can
adapt the pre-trained ViTs into many different image and video tasks
efficiently. It possesses several benefits more appealing than prior arts.
Firstly, AdaptFormer introduces lightweight modules that only add less than 2%
extra parameters to a ViT, while it is able to increase the ViT's
transferability without updating its original pre-trained parameters,
significantly outperforming the existing 100% fully fine-tuned models on action
recognition benchmarks. Secondly, it can be plug-and-play in different
Transformers and scalable to many visual tasks. Thirdly, extensive experiments
on five image and video datasets show that AdaptFormer largely improves ViTs in
the target domains. For example, when updating just 1.5% extra parameters, it
achieves about 10% and 19% relative improvement compared to the fully
fine-tuned models on Something-Something~v2 and HMDB51, respectively. Project
page: <a href="http://www.shoufachen.com/adaptformer-page.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation. (arXiv:2205.13542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13542">
<div class="article-summary-box-inner">
<span><p>Multi-sensor fusion is essential for an accurate and reliable autonomous
driving system. Recent approaches are based on point-level fusion: augmenting
the LiDAR point cloud with camera features. However, the camera-to-LiDAR
projection throws away the semantic density of camera features, hindering the
effectiveness of such methods, especially for semantic-oriented tasks (such as
3D scene segmentation). In this paper, we break this deeply-rooted convention
with BEVFusion, an efficient and generic multi-task multi-sensor fusion
framework. It unifies multi-modal features in the shared bird's-eye view (BEV)
representation space, which nicely preserves both geometric and semantic
information. To achieve this, we diagnose and lift key efficiency bottlenecks
in the view transformation with optimized BEV pooling, reducing latency by more
than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports
different 3D perception tasks with almost no architectural changes. It
establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and
NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with
1.9x lower computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing the Dark Secrets of Masked Image Modeling. (arXiv:2205.13543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13543">
<div class="article-summary-box-inner">
<span><p>Masked image modeling (MIM) as pre-training is shown to be effective for
numerous vision downstream tasks, but how and where MIM works remain unclear.
In this paper, we compare MIM with the long-dominant supervised pre-trained
models from two perspectives, the visualizations and the experiments, to
uncover their key representational differences. From the visualizations, we
find that MIM brings locality inductive bias to all layers of the trained
models, but supervised models tend to focus locally at lower layers but more
globally at higher layers. That may be the reason why MIM helps Vision
Transformers that have a very large receptive field to optimize. Using MIM, the
model can maintain a large diversity on attention heads in all layers. But for
supervised models, the diversity on attention heads almost disappears from the
last three layers and less diversity harms the fine-tuning performance. From
the experiments, we find that MIM models can perform significantly better on
geometric and motion tasks with weak semantics or fine-grained classification
tasks, than their supervised counterparts. Without bells and whistles, a
standard MIM pre-trained SwinV2-L could achieve state-of-the-art performance on
pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth
estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object
tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the
categories are sufficiently covered by the supervised pre-training, MIM models
can still achieve highly competitive transfer performance. With a deeper
understanding of MIM, we hope that our work can inspire new and solid research
in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural Networks Acceleration. (arXiv:2006.12963v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12963">
<div class="article-summary-box-inner">
<span><p>Deep learning has achieved impressive results in many areas, but the
deployment of edge intelligent devices is still very slow. To solve this
problem, we propose a novel compression and acceleration method based on data
distribution characteristics for deep neural networks, namely Pruning Filter
via Gaussian Distribution Feature (PFGDF). Compared with previous advanced
pruning methods, PFGDF compresses the model by filters with insignificance in
distribution, regardless of the contribution and sensitivity information of the
convolution filter. PFGDF is significantly different from weight sparsification
pruning because it does not require the special accelerated library to process
the sparse weight matrix and introduces no more extra parameters. The pruning
process of PFGDF is automated. Furthermore, the model compressed by PFGDF can
restore the same performance as the uncompressed model. We evaluate PFGDF
through extensive experiments, on CIFAR-10, PFGDF compresses the convolution
filter on VGG-16 by 66.62% with more than 90% parameter reduced, while the
inference time is accelerated by 83.73% on Huawei MATE 10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polygon-free: Unconstrained Scene Text Detection with Box Annotations. (arXiv:2011.13307v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13307">
<div class="article-summary-box-inner">
<span><p>Although a polygon is a more accurate representation than an upright bounding
box for text detection, the annotations of polygons are extremely expensive and
challenging. Unlike existing works that employ fully-supervised training with
polygon annotations, this study proposes an unconstrained text detection system
termed Polygon-free (PF), in which most existing polygon-based text detectors
(e.g., PSENet [33],DB [16]) are trained with only upright bounding box
annotations. Our core idea is to transfer knowledge from synthetic data to real
data to enhance the supervision information of upright bounding boxes. This is
made possible with a simple segmentation network, namely Skeleton Attention
Segmentation Network (SASN), that includes three vital components (i.e.,
channel attention, spatial attention and skeleton attention map) and one soft
cross-entropy loss. Experiments demonstrate that the proposed Polygonfree
system can combine general detectors (e.g., EAST, PSENet, DB) to yield
surprisingly high-quality pixel-level results with only upright bounding box
annotations on a variety of datasets (e.g., ICDAR2019-Art, TotalText,
ICDAR2015). For example, without using polygon annotations, PSENet achieves an
80.5% F-score on TotalText [3] (vs. 80.9% of fully supervised counterpart),
31.1% better than training directly with upright bounding box annotations, and
saves 80%+ labeling costs. We hope that PF can provide a new perspective for
text detection to reduce the labeling costs. The code can be found at
https://github.com/weijiawu/Unconstrained-Text-Detection-with-Box-Supervisionand-Dynamic-Self-Training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.15081">
<div class="article-summary-box-inner">
<span><p>We propose Deep Estimators of Features (DEFs), a learning-based framework for
predicting sharp geometric features in sampled 3D shapes. Differently from
existing data-driven methods, which reduce this problem to feature
classification, we propose to regress a scalar field representing the distance
from point samples to the closest feature line on local patches. Our approach
is the first that scales to massive point clouds by fusing distance-to-feature
estimates obtained on individual patches. We extensively evaluate our approach
against related state-of-the-art methods on newly proposed synthetic and
real-world 3D CAD model benchmarks. Our approach not only outperforms these
(with improvements in Recall and False Positives Rates), but generalizes to
real-world scans after training our model on synthetic data and fine-tuning it
on a small dataset of scanned data. We demonstrate a downstream application,
where we reconstruct an explicit representation of straight and curved sharp
feature lines from range scan data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning for Blind Image Quality Assessment. (arXiv:2102.09717v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09717">
<div class="article-summary-box-inner">
<span><p>The explosive growth of image data facilitates the fast development of image
processing and computer vision methods for emerging visual applications,
meanwhile introducing novel distortions to the processed images. This poses a
grand challenge to existing blind image quality assessment (BIQA) models,
failing to continually adapt to such subpopulation shift. Recent work suggests
training BIQA methods on the combination of all available human-rated IQA
datasets. However, this type of approach is not scalable to a large number of
datasets, and is cumbersome to incorporate a newly created dataset as well. In
this paper, we formulate continual learning for BIQA, where a model learns
continually from a stream of IQA datasets, building on what was learned from
previously seen data. We first identify five desiderata in the new setting with
a measure to quantify the plasticity-stability trade-off. We then propose a
simple yet effective method for learning BIQA models continually. Specifically,
based on a shared backbone network, we add a prediction head for a new dataset,
and enforce a regularizer to allow all prediction heads to evolve with new data
while being resistant to catastrophic forgetting of old data. We compute the
quality score by an adaptive weighted summation of estimates from all
prediction heads. Extensive experiments demonstrate the promise of the proposed
continual learning method in comparison to standard training techniques for
BIQA. We made the code publicly available at
https://github.com/zwx8981/BIQA_CL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Little Energy Goes a Long Way: Build an Energy-Efficient, Accurate Spiking Neural Network from Convolutional Neural Network. (arXiv:2103.00944v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00944">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks (SNNs) offer an inherent ability to process
spatial-temporal data, or in other words, realworld sensory data, but suffer
from the difficulty of training high accuracy models. A major thread of
research on SNNs is on converting a pre-trained convolutional neural network
(CNN) to an SNN of the same structure. State-of-the-art conversion methods are
approaching the accuracy limit, i.e., the near-zero accuracy loss of SNN
against the original CNN. However, we note that this is made possible only when
significantly more energy is consumed to process an input. In this paper, we
argue that this trend of "energy for accuracy" is not necessary -- a little
energy can go a long way to achieve the near-zero accuracy loss. Specifically,
we propose a novel CNN-to-SNN conversion method that is able to use a
reasonably short spike train (e.g., 256 timesteps for CIFAR10 images) to
achieve the near-zero accuracy loss. The new conversion method, named as
explicit current control (ECC), contains three techniques (current
normalisation, thresholding for residual elimination, and consistency
maintenance for batch-normalisation), in order to explicitly control the
currents flowing through the SNN when processing inputs. We implement ECC into
a tool nicknamed SpKeras, which can conveniently import Keras CNN models and
convert them into SNNs. We conduct an extensive set of experiments with the
tool -- working with VGG16 and various datasets such as CIFAR10 and CIFAR100 --
and compare with state-of-the-art conversion methods. Results show that ECC is
a promising method that can optimise over energy consumption and accuracy loss
simultaneously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steerable 3D Spherical Neurons. (arXiv:2106.13863v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13863">
<div class="article-summary-box-inner">
<span><p>Emerging from low-level vision theory, steerable filters found their
counterpart in prior work on steerable convolutional neural networks
equivariant to rigid transformations. In our work, we propose a steerable
feed-forward learning-based approach that consists of neurons with spherical
decision surfaces and operates on point clouds. Such spherical neurons are
obtained by conformal embedding of Euclidean space and have recently been
revisited in the context of learning representations of point sets. Focusing on
3D geometry, we exploit the isometry property of spherical neurons and derive a
3D steerability constraint. After training spherical neurons to classify point
clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate
the neurons and construct rotation-equivariant spherical filter banks. We then
apply the derived constraint to interpolate the filter bank outputs and, thus,
obtain a rotation-invariant network. Finally, we use a synthetic point set and
real-world 3D skeleton data to verify our theoretical findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers. (arXiv:2107.03996v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03996">
<div class="article-summary-box-inner">
<span><p>We propose to address quadrupedal locomotion tasks using Reinforcement
Learning (RL) with a Transformer-based model that learns to combine
proprioceptive information and high-dimensional depth sensor inputs. While
learning-based locomotion has made great advances using RL, most methods still
rely on domain randomization for training blind agents that generalize to
challenging terrains. Our key insight is that proprioceptive states only offer
contact measurements for immediate reaction, whereas an agent equipped with
visual sensory observations can learn to proactively maneuver environments with
obstacles and uneven terrain by anticipating changes in the environment many
steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL
method that leverages both proprioceptive states and visual observations for
locomotion control. We evaluate our method in challenging simulated
environments with different obstacles and uneven terrain. We transfer our
learned policy from simulation to a real robot by running it indoors and in the
wild with unseen obstacles and terrain. Our method not only significantly
improves over baselines, but also achieves far better generalization
performance, especially when transferred to the real robot. Our project page
with videos is at https://rchalyang.github.io/LocoTransformer/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Perceptual Locomotion on Uneven Terrains using Sparse Visual Observations. (arXiv:2109.14026v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14026">
<div class="article-summary-box-inner">
<span><p>To proactively navigate and traverse various terrains, active use of visual
perception becomes indispensable. We aim to investigate the feasibility and
performance of using sparse visual observations to achieve perceptual
locomotion over a range of common terrains (steps, ramps, gaps, and stairs) in
human-centered environments. We formulate a selection of sparse visual inputs
suitable for locomotion over the terrains of interest, and propose a learning
framework to integrate exteroceptive and proprioceptive states. We specifically
design the state observations and a training curriculum to learn feedback
control policies effectively over a range of different terrains. We extensively
validate and benchmark the learned policy in various tasks: omnidirectional
walking on flat ground, and forward locomotion over various obstacles, showing
high success rate of traversability. Furthermore, we study exteroceptive
ablations and evaluate policy generalization by adding various levels of noise
and testing on new unseen terrains. We demonstrate the capabilities of
autonomous perceptual locomotion that can be achieved by only using sparse
visual observations from direct depth measurements, which are easily available
from a Lidar or RGB-D sensor, showing robust ascent and descent over high
stairs of 20 cm height, i.e., 50% leg length, and robustness against noise and
unseen terrains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Memorization of Noisy Labels via Regularization between Representations. (arXiv:2110.09022v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09022">
<div class="article-summary-box-inner">
<span><p>Designing robust loss functions is popular in learning with noisy labels
while existing designs did not explicitly consider the overfitting property of
deep neural networks (DNNs). As a result, applying these losses may still
suffer from overfitting/memorizing noisy labels as training proceeds. In this
paper, we first theoretically analyze the memorization effect and show that a
lower-capacity model may perform better on noisy datasets. However, it is
non-trivial to design a neural network with the best capacity given an
arbitrary task. To circumvent this dilemma, instead of changing the model
architecture, we decouple DNNs into an encoder followed by a linear classifier
and propose to restrict the function space of a DNN by a representation
regularizer. Particularly, we require the distance between two self-supervised
features to be positively related to the distance between the corresponding two
supervised model outputs. Our proposed framework is easily extendable and can
incorporate many other robust loss functions to further improve performance.
Extensive experiments and theoretical analyses support our claims. Code is
available at github.com/UCSC-REAL/SelfSup_NoisyLabel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Generalization of Contrastive Self-Supervised Learning. (arXiv:2111.00743v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00743">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised learning has attracted great attention, since it
only requires unlabeled data for training. Contrastive learning is one popular
method for self-supervised learning and has achieved promising empirical
performance. However, the theoretical understanding of its generalization
ability is still limited. To this end, we define a kind of
$(\sigma,\delta)$-measure to mathematically quantify the data augmentation, and
then provide an upper bound of the downstream classification error based on the
measure. We show that the generalization ability of contrastive self-supervised
learning depends on three key factors: alignment of positive samples,
divergence of class centers, and concentration of augmented data. The first two
factors can be optimized by contrastive algorithms, while the third one is
priorly determined by pre-defined data augmentation. With the above theoretical
findings, we further study two canonical contrastive losses, InfoNCE and
cross-correlation loss, and prove that both of them are indeed able to satisfy
the first two factors. Moreover, we empirically verify the third factor by
conducting various experiments on the real-world dataset, and show that our
theoretical inferences on the relationship between the data augmentation and
the generalization of contrastive self-supervised learning agree with the
empirical observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated pharyngeal phase detection and bolus localization in videofluoroscopic swallowing study: Killing two birds with one stone?. (arXiv:2111.04699v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04699">
<div class="article-summary-box-inner">
<span><p>The videofluoroscopic swallowing study (VFSS) is a gold-standard imaging
technique for assessing swallowing, but analysis and rating of VFSS recordings
is time consuming and requires specialized training and expertise. Researchers
have recently demonstrated that it is possible to automatically detect the
pharyngeal phase of swallowing and to localize the bolus in VFSS recordings via
computer vision, fostering the development of novel techniques for automatic
VFSS analysis. However, training of algorithms to perform these tasks requires
large amounts of annotated data that are seldom available. We demonstrate that
the challenges of pharyngeal phase detection and bolus localization can be
solved together using a single approach. We propose a deep-learning framework
that jointly tackles pharyngeal phase detection and bolus localization in a
weakly-supervised manner, requiring only the initial and final frames of the
pharyngeal phase as ground truth annotations for the training. Our approach
stems from the observation that bolus presence in the pharynx is the most
prominent visual feature upon which to infer whether individual VFSS frames
belong to the pharyngeal phase. We conducted extensive experiments with
multiple convolutional neural networks (CNNs) on a dataset of 1245 bolus-level
clips from 59 healthy subjects. We demonstrated that the pharyngeal phase can
be detected with an F1-score higher than 0.9. Moreover, by processing the class
activation maps of the CNNs, we were able to localize the bolus with promising
results, obtaining correlations with ground truth trajectories higher than 0.9,
without any manual annotations of bolus location used for training purposes.
Once validated on a larger sample of participants with swallowing disorders,
our framework will pave the way for the development of intelligent tools for
VFSS analysis to support clinicians in swallowing assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Fine-Tuning by Better Leveraging Pre-Training Data. (arXiv:2111.12292v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12292">
<div class="article-summary-box-inner">
<span><p>As a dominant paradigm, fine-tuning a pre-trained model on the target data is
widely used in many deep learning applications, especially for small data sets.
However, recent studies have empirically shown that training from scratch has
the final performance that is no worse than this pre-training strategy once the
number of training samples is increased in some vision tasks. In this work, we
revisit this phenomenon from the perspective of generalization analysis by
using excess risk bound which is popular in learning theory. The result reveals
that the excess risk bound may have a weak dependency on the pre-trained model.
The observation inspires us to leverage pre-training data for fine-tuning,
since this data is also available for fine-tuning. The generalization result of
using pre-training data shows that the excess risk bound on a target task can
be improved when the appropriate pre-training data is included in fine-tuning.
With the theoretical motivation, we propose a novel selection strategy to
select a subset from pre-training data to help improve the generalization on
the target task. Extensive experimental results for image classification tasks
on 8 benchmark data sets verify the effectiveness of the proposed data
selection based fine-tuning pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02478">
<div class="article-summary-box-inner">
<span><p>Background and Objective: Artificial intelligence (AI) methods coupled with
biomedical analysis has a critical role during pandemics as it helps to release
the overwhelming pressure from healthcare systems and physicians. As the
ongoing COVID-19 crisis worsens in countries having dense populations and
inadequate testing kits like Brazil and India, radiological imaging can act as
an important diagnostic tool to accurately classify covid-19 patients and
prescribe the necessary treatment in due time. With this motivation, we present
our study based on deep learning architecture for detecting covid-19 infected
lungs using chest X-rays. Dataset: We collected a total of 2470 images for
three different class labels, namely, healthy lungs, ordinary pneumonia, and
covid-19 infected pneumonia, out of which 470 X-ray images belong to the
covid-19 category. Methods: We first pre-process all the images using histogram
equalization techniques and segment them using U-net architecture. VGG-16
network is then used for feature extraction from the pre-processed images which
is further sampled by SMOTE oversampling technique to achieve a balanced
dataset. Finally, the class-balanced features are classified using a support
vector machine (SVM) classifier with 10-fold cross-validation and the accuracy
is evaluated. Result and Conclusion: Our novel approach combining well-known
pre-processing techniques, feature extraction methods, and dataset balancing
method, lead us to an outstanding rate of recognition of 98% for COVID-19
images over a dataset of 2470 X-ray images. Our model is therefore fit to be
utilized in healthcare facilities for screening purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trajectory-Constrained Deep Latent Visual Attention for Improved Local Planning in Presence of Heterogeneous Terrain. (arXiv:2112.04684v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04684">
<div class="article-summary-box-inner">
<span><p>We present a reward-predictive, model-based deep learning method featuring
trajectory-constrained visual attention for local planning in visual navigation
tasks. Our method learns to place visual attention at locations in latent image
space which follow trajectories caused by vehicle control actions to enhance
predictive accuracy during planning. The attention model is jointly optimized
by the task-specific loss and an additional trajectory-constraint loss,
allowing adaptability yet encouraging a regularized structure for improved
generalization and reliability. Importantly, visual attention is applied in
latent feature map space instead of raw image space to promote efficient
planning. We validated our model in visual navigation tasks of planning low
turbulence, collision-free trajectories in off-road settings and hill climbing
with locking differentials in the presence of slippery terrain. Experiments
involved randomized procedural generated simulation and real-world
environments. We found our method improved generalization and learning
efficiency when compared to no-attention and self-attention alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonlinear Transform Source-Channel Coding for Semantic Communications. (arXiv:2112.10961v2 [cs.IT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10961">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a class of high-efficiency deep joint
source-channel coding methods that can closely adapt to the source distribution
under the nonlinear transform, it can be collected under the name nonlinear
transform source-channel coding (NTSCC). In the considered model, the
transmitter first learns a nonlinear analysis transform to map the source data
into latent space, then transmits the latent representation to the receiver via
deep joint source-channel coding. Our model incorporates the nonlinear
transform as a strong prior to effectively extract the source semantic features
and provide side information for source-channel coding. Unlike existing
conventional deep joint source-channel coding methods, the proposed NTSCC
essentially learns both the source latent representation and an entropy model
as the prior on the latent representation. Accordingly, novel adaptive rate
transmission and hyperprior-aided codec refinement mechanisms are developed to
upgrade deep joint source-channel coding. The whole system design is formulated
as an optimization problem whose goal is to minimize the end-to-end
transmission rate-distortion performance under established perceptual quality
metrics. Across test image sources with various resolutions, we find that the
proposed NTSCC transmission method generally outperforms both the analog
transmission using the standard deep joint source-channel coding and the
classical separation-based digital transmission. Notably, the proposed NTSCC
method can potentially support future semantic communications due to its
content-aware ability and perceptual optimization goal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Extraction, Classification and Prediction for Hand Hygiene Gestures with KNN Algorithm. (arXiv:2112.15085v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15085">
<div class="article-summary-box-inner">
<span><p>There are six, well-structured hand gestures for washing hands as provided by
World Health Organisation guidelines. In this paper, hand features such as
contours of the hands, the centroid of the hands, and extreme hand points along
the largest contour are extracted for specific hand-washing gestures with the
use of a computer vision library, OpenCV. For this project, a robust dataset of
hand hygiene video recordings is built with the help of 30 research
participants. In this work, a subset of the dataset was used as a pilot study
to demonstrate the effectiveness of the KNN algorithm. Extracted hand features
saved in a CSV file are passed to a KNN model with a cross-fold validation
technique for the classification and prediction of the unlabelled data. A mean
accuracy score of &gt;95% is achieved and proves that the KNN algorithm with an
appropriate input value of K=3 is efficient for hand hygiene gestures
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01080">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are threatened by adversarial examples.
Adversarial detection, which distinguishes adversarial images from benign
images, is fundamental for robust DNN-based services. Image transformation is
one of the most effective approaches to detect adversarial examples. During the
last few years, a variety of image transformations have been studied and
discussed to design reliable adversarial detectors. In this paper, we
systematically synthesize the recent progress on adversarial detection via
image transformations with a novel classification method. Then, we conduct
extensive experiments to test the detection performance of image
transformations against state-of-the-art adversarial attacks. Furthermore, we
reveal that each individual transformation is not capable of detecting
adversarial examples in a robust way, and propose a DNN-based approach referred
to as \emph{AdvJudge}, which combines scores of 9 image transformations.
Without knowing which individual scores are misleading or not misleading,
AdvJudge can make the right judgment, and achieve a significant improvement in
detection rate. Finally, we utilize an explainable AI tool to show the
contribution of each image transformation to adversarial detection.
Experimental results show that the contribution of image transformations to
adversarial detection is significantly different, the combination of them can
significantly improve the generic detection ability against state-of-the-art
adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy. (arXiv:2201.02867v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02867">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in high-resolution imaging of biomolecules in solution
with cryo-electron microscopy (cryo-EM) have unlocked new doors for the
reconstruction of molecular volumes, thereby promising further advances in
biology, chemistry, and pharmacological research. Recent next-generation volume
reconstruction algorithms that combine generative modeling with end-to-end
unsupervised deep learning techniques have shown promising preliminary results,
but still face considerable technical and theoretical hurdles when applied to
experimental cryo-EM images. In light of the proliferation of such methods, we
propose here a critical review of recent advances in the field of deep
generative modeling for cryo-EM volume reconstruction. The present review aims
to (i) unify and compare these new methods using a consistent statistical
framework, (ii) present them using a terminology familiar to machine learning
researchers and computational biologists with no specific background in
cryo-EM, and (iii) provide the necessary perspective on current advances to
highlight their relative strengths and weaknesses, along with outstanding
bottlenecks and avenues for improvements in the field. This review might also
raise the interest of computer vision practitioners, as it highlights
significant limits of deep generative models in low signal-to-noise regimes --
therefore emphasizing a need for new theoretical and methodological
developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RDP-Net: Region Detail Preserving Network for Change Detection. (arXiv:2202.09745v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09745">
<div class="article-summary-box-inner">
<span><p>Change detection (CD) is an essential earth observation technique. It
captures the dynamic information of land objects. With the rise of deep
learning, neural networks (NN) have shown great potential in CD. However,
current NN models introduce backbone architectures that lose detailed
information during learning. Moreover, current NN models are heavy in
parameters, which prevents their deployment on edge devices such as UAVs. In
this work, we tackle this issue by proposing RDP-Net: a region detail
preserving network for CD. We propose an efficient training strategy that
constructs the training tasks during the warmup period of NN training and lets
the NN learn from easy to hard. The training strategy enables NN to learn more
powerful features with fewer FLOPs and achieve better performance. Next, we
propose an effective edge loss that increases the penalty for errors on details
and improves the network's attention to details such as boundary regions and
small areas. Furthermore, we provide a NN model with a brand new backbone that
achieves the state-of-the-art empirical performance in CD with only 1.70M
parameters. We hope our RDP-Net would benefit the practical CD applications on
compact devices and could inspire more people to bring change detection to a
new level with the efficient training strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-driven Visual Tempo Learning for Video-based Action Recognition. (arXiv:2202.12116v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12116">
<div class="article-summary-box-inner">
<span><p>Action visual tempo characterizes the dynamics and the temporal scale of an
action, which is helpful to distinguish human actions that share high
similarities in visual dynamics and appearance. Previous methods capture the
visual tempo either by sampling raw videos with multiple rates, which require a
costly multi-layer network to handle each rate, or by hierarchically sampling
backbone features, which rely heavily on high-level features that miss
fine-grained temporal dynamics. In this work, we propose a Temporal Correlation
Module (TCM), which can be easily embedded into the current action recognition
backbones in a plug-in-and-play manner, to extract action visual tempo from
low-level backbone features at single-layer remarkably. Specifically, our TCM
contains two main components: a Multi-scale Temporal Dynamics Module (MTDM) and
a Temporal Attention Module (TAM). MTDM applies a correlation operation to
learn pixel-wise fine-grained temporal dynamics for both fast-tempo and
slow-tempo. TAM adaptively emphasizes expressive features and suppresses
inessential ones via analyzing the global information across various tempos.
Extensive experiments conducted on several action recognition benchmarks, e.g.
Something-Something V1 $\&amp;$ V2, Kinetics-400, UCF-101, and HMDB-51, have
demonstrated that the proposed TCM is effective to promote the performance of
the existing video-based action recognition models for a large margin. The
source code is publicly released at https://github.com/yzfly/TCM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Creativity Characterization of Generative Models via Group-based Subset Scanning. (arXiv:2203.00523v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00523">
<div class="article-summary-box-inner">
<span><p>Deep generative models, such as Variational Autoencoders (VAEs) and
Generative Adversarial Networks (GANs), have been employed widely in
computational creativity research. However, such models discourage
out-of-distribution generation to avoid spurious sample generation, thereby
limiting their creativity. Thus, incorporating research on human creativity
into generative deep learning techniques presents an opportunity to make their
outputs more compelling and human-like. As we see the emergence of generative
models directed toward creativity research, a need for machine learning-based
surrogate metrics to characterize creative output from these models is
imperative. We propose group-based subset scanning to identify, quantify, and
characterize creative processes by detecting a subset of anomalous
node-activations in the hidden layers of the generative models. Our experiments
on the standard image benchmarks, and their "creatively generated" variants,
reveal that the proposed subset scores distribution is more useful for
detecting creative processes in the activation space rather than the pixel
space. Further, we found that creative samples generate larger subsets of
anomalies than normal or non-creative samples across datasets. The node
activations highlighted during the creative decoding process are different from
those responsible for the normal sample generation. Lastly, we assess if the
images from the subsets selected by our method were also found creative by
human evaluators, presenting a link between creativity perception in humans and
node activations within deep neural nets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decontextualized I3D ConvNet for ultra-distance runners performance analysis at a glance. (arXiv:2203.06749v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06749">
<div class="article-summary-box-inner">
<span><p>In May 2021, the site runnersworld.com published that participation in
ultra-distance races has increased by 1,676% in the last 23 years. Moreover,
nearly 41% of those runners participate in more than one race per year. The
development of wearable devices has undoubtedly contributed to motivating
participants by providing performance measures in real-time. However, we
believe there is room for improvement, particularly from the organizers point
of view. This work aims to determine how the runners performance can be
quantified and predicted by considering a non-invasive technique focusing on
the ultra-running scenario. In this sense, participants are captured when they
pass through a set of locations placed along the race track. Each footage is
considered an input to an I3D ConvNet to extract the participant's running gait
in our work. Furthermore, weather and illumination capture conditions or
occlusions may affect these footages due to the race staff and other runners.
To address this challenging task, we have tracked and codified the
participant's running gait at some RPs and removed the context intending to
ensure a runner-of-interest proper evaluation. The evaluation suggests that the
features extracted by an I3D ConvNet provide enough information to estimate the
participant's performance along the different race tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Stage Duplex Fusion ConvNet for Aerial Scene Classification. (arXiv:2203.16325v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16325">
<div class="article-summary-box-inner">
<span><p>Existing deep learning based methods effectively prompt the performance of
aerial scene classification. However, due to the large amount of parameters and
computational cost, it is rather difficult to apply these methods to multiple
real-time remote sensing applications such as on-board data preception on
drones and satellites. In this paper, we address this task by developing a
light-weight ConvNet named multi-stage duplex fusion network (MSDF-Net). The
key idea is to use parameters as little as possible while obtaining as strong
as possible scene representation capability. To this end, a residual-dense
duplex fusion strategy is developed to enhance the feature propagation while
re-using parameters as much as possible, and is realized by our duplex fusion
block (DFblock). Specifically, our MSDF-Net consists of multi-stage structures
with DFblock. Moreover, duplex semantic aggregation (DSA) module is developed
to mine the remote sensing scene information from extracted convolutional
features, which also contains two parallel branches for semantic description.
Extensive experiments are conducted on three widely-used aerial scene
classification benchmarks, and reflect that our MSDF-Net can achieve a
competitive performance against the recent state-of-art while reducing up to
80% parameter numbers. Particularly, an accuracy of 92.96% is achieved on AID
with only 0.49M parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions. (arXiv:2204.00746v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00746">
<div class="article-summary-box-inner">
<span><p>We propose a novel one-stage Transformer-based semantic and spatial refined
transformer (SSRT) to solve the Human-Object Interaction detection task, which
requires to localize humans and objects, and predicts their interactions.
Differently from previous Transformer-based HOI approaches, which mostly focus
at improving the design of the decoder outputs for the final detection, SSRT
introduces two new modules to help select the most relevant object-action pairs
within an image and refine the queries' representation using rich semantic and
spatial features. These enhancements lead to state-of-the-art results on the
two most popular HOI benchmarks: V-COCO and HICO-DET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Structured State-Evolution for Vision-Language Navigation. (arXiv:2204.09280v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09280">
<div class="article-summary-box-inner">
<span><p>Vision-and-language Navigation (VLN) task requires an embodied agent to
navigate to a remote location following a natural language instruction.
Previous methods usually adopt a sequence model (e.g., Transformer and LSTM) as
the navigator. In such a paradigm, the sequence model predicts action at each
step through a maintained navigation state, which is generally represented as a
one-dimensional vector. However, the crucial navigation clues (i.e.,
object-level environment layout) for embodied navigation task is discarded
since the maintained vector is essentially unstructured. In this paper, we
propose a novel Structured state-Evolution (SEvol) model to effectively
maintain the environment layout clues for VLN. Specifically, we utilise the
graph-based feature to represent the navigation state instead of the
vector-based state. Accordingly, we devise a Reinforced Layout clues Miner
(RLM) to mine and detect the most crucial layout graph for long-term navigation
via a customised reinforcement learning strategy. Moreover, the Structured
Evolving Module (SEM) is proposed to maintain the structured graph-based state
during navigation, where the state is gradually evolved to learn the
object-level spatial-temporal relationship. The experiments on the R2R and R4R
datasets show that the proposed SEvol model improves VLN models' performance by
large margins, e.g., +3% absolute SPL accuracy for NvEM and +8% for EnvDrop on
the R2R test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Noisy Prediction to True Label: Noisy Prediction Calibration via Generative Model. (arXiv:2205.00690v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00690">
<div class="article-summary-box-inner">
<span><p>Noisy labels are inevitable yet problematic in machine learning society. It
ruins the generalization of a classifier by making the classifier over-fitted
to noisy labels. Existing methods on noisy label have focused on modifying the
classifier during the training procedure. It has two potential problems. First,
these methods are not applicable to a pre-trained classifier without further
access to training. Second, it is not easy to train a classifier and regularize
all negative effects from noisy labels, simultaneously. We suggest a new branch
of method, Noisy Prediction Calibration (NPC) in learning with noisy labels.
Through the introduction and estimation of a new type of transition matrix via
generative model, NPC corrects the noisy prediction from the pre-trained
classifier to the true label as a post-processing scheme. We prove that NPC
theoretically aligns with the transition matrix based methods. Yet, NPC
empirically provides more accurate pathway to estimate true label, even without
involvement in classifier learning. Also, NPC is applicable to any classifier
trained with noisy label methods, if training instances and its predictions are
available. Our method, NPC, boosts the classification performances of all
baseline models on both synthetic and real-world datasets. The implemented code
is available at https://github.com/BaeHeeSun/NPC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization. (arXiv:2205.04464v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04464">
<div class="article-summary-box-inner">
<span><p>We propose a new microscopy simulation system that can depict atomistic
models in a micrograph visual style, similar to results of physical electron
microscopy imaging. This system is scalable, able to represent simulation of
electron microscopy of tens of viral particles and synthesizes the image faster
than previous methods. On top of that, the simulator is differentiable, both
its deterministic as well as stochastic stages that form signal and noise
representations in the micrograph. This notable property has the capability for
solving inverse problems by means of optimization and thus allows for
generation of microscopy simulations using the parameter settings estimated
from real data. We demonstrate this learning capability through two
applications: (1) estimating the parameters of the modulation transfer function
defining the detector properties of the simulated and real micrographs, and (2)
denoising the real data based on parameters trained from the simulated
examples. While current simulators do not support any parameter estimation due
to their forward design, we show that the results obtained using estimated
parameters are very similar to the results of real micrographs. Additionally,
we evaluate the denoising capabilities of our approach and show that the
results showed an improvement over state-of-the-art methods. Denoised
micrographs exhibit less noise in the tilt-series tomography reconstructions,
ultimately reducing the visual dominance of noise in direct volume rendering of
microscopy tomograms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When does dough become a bagel? Analyzing the remaining mistakes on ImageNet. (arXiv:2205.04596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04596">
<div class="article-summary-box-inner">
<span><p>Image classification accuracy on the ImageNet dataset has been a barometer
for progress in computer vision over the last decade. Several recent papers
have questioned the degree to which the benchmark remains useful to the
community, yet innovations continue to contribute gains to performance, with
today's largest models achieving 90%+ top-1 accuracy. To help contextualize
progress on ImageNet and provide a more meaningful evaluation for today's
state-of-the-art models, we manually review and categorize every remaining
mistake that a few top models make in order to provide insight into the
long-tail of errors on one of the most benchmarked datasets in computer vision.
We focus on the multi-label subset evaluation of ImageNet, where today's best
models achieve upwards of 97% top-1 accuracy. Our analysis reveals that nearly
half of the supposed mistakes are not mistakes at all, and we uncover new valid
multi-labels, demonstrating that, without careful review, we are significantly
underestimating the performance of these models. On the other hand, we also
find that today's best models still make a significant number of mistakes (40%)
that are obviously wrong to human reviewers. To calibrate future progress on
ImageNet, we provide an updated multi-label evaluation set, and we curate
ImageNet-Major: a 68-example "major error" slice of the obvious mistakes made
by today's top models -- a slice where models should achieve near perfection,
but today are far from doing so.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Visual Words for Interpretable Image Recognition. (arXiv:2205.10724v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10724">
<div class="article-summary-box-inner">
<span><p>To interpret deep models' predictions, attention-based visual cues are widely
used in addressing \textit{why} deep models make such predictions. Beyond that,
the current research community becomes more interested in reasoning
\textit{how} deep models make predictions, where some prototype-based methods
employ interpretable representations with their corresponding visual cues to
reveal the black-box mechanism of deep model behaviors. However, these
pioneering attempts only either learn the category-specific prototypes and
deteriorate their generalizing capacities, or demonstrate several illustrative
examples without a quantitative evaluation of visual-based interpretability
with further limitations on their practical usages. In this paper, we revisit
the concept of visual words and propose the Learnable Visual Words (LVW) to
interpret the model prediction behaviors with two novel modules: semantic
visual words learning and dual fidelity preservation. The semantic visual words
learning relaxes the category-specific constraint, enabling the general visual
words shared across different categories. Beyond employing the visual words for
prediction to align visual words with the base model, our dual fidelity
preservation also includes the attention guided semantic alignment that
encourages the learned visual words to focus on the same conceptual regions for
prediction. Experiments on six visual benchmarks demonstrate the superior
effectiveness of our proposed LVW in both accuracy and model interpretation
over the state-of-the-art methods. Moreover, we elaborate on various in-depth
analyses to further explore the learned visual words and the generalizability
of our method for unseen categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNNs are Myopic. (arXiv:2205.10760v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10760">
<div class="article-summary-box-inner">
<span><p>We claim that Convolutional Neural Networks (CNNs) learn to classify images
using only small seemingly unrecognizable tiles. We show experimentally that
CNNs trained only using such tiles can match or even surpass the performance of
CNNs trained on full images. Conversely, CNNs trained on full images show
similar predictions on small tiles. We also propose the first a priori
theoretical model for convolutional data sets that seems to explain this
behavior. This gives additional support to the long standing suspicion that
CNNs do not need to understand the global structure of images to achieve
state-of-the-art accuracies. Surprisingly it also suggests that over-fitting is
not needed either.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super Vision Transformer. (arXiv:2205.11397v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11397">
<div class="article-summary-box-inner">
<span><p>We attempt to reduce the computational costs in vision transformers (ViTs),
which increase quadratically in the token number. We present a novel training
paradigm that trains only one ViT model at a time, but is capable of providing
improved image recognition performance with various computational costs. Here,
the trained ViT model, termed super vision transformer (SuperViT), is empowered
with the versatile ability to solve incoming patches of multiple sizes as well
as preserve informative tokens with multiple keeping rates (the ratio of
keeping tokens) to achieve good hardware efficiency for inference, given that
the available hardware resources often change from time to time. Experimental
results on ImageNet demonstrate that our SuperViT can considerably reduce the
computational costs of ViT models with even performance increase. For example,
we reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2% and
0.7% for 1.5x reduction. Also, our SuperViT significantly outperforms existing
studies on efficient vision transformers. For example, when consuming the same
amount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SoTA) EViT
by 1.1% when using DeiT-S as their backbones. The project of this work is made
publicly available at https://github.com/lmbxmu/SuperViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods. (arXiv:2205.11508v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11508">
<div class="article-summary-box-inner">
<span><p>Self-Supervised Learning (SSL) surmises that inputs and pairwise positive
relationships are enough to learn meaningful representations. Although SSL has
recently reached a milestone: outperforming supervised methods in many
modalities\dots the theoretical foundations are limited, method-specific, and
fail to provide principled design guidelines to practitioners. In this paper,
we propose a unifying framework under the helm of spectral manifold learning to
address those limitations. Through the course of this study, we will rigorously
demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous
spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al.
</p>
<p>This unification will then allow us to obtain (i) the closed-form optimal
representation for each method, (ii) the closed-form optimal network parameters
in the linear regime for each method, (iii) the impact of the pairwise
relations used during training on each of those quantities and on downstream
task performances, and most importantly, (iv) the first theoretical bridge
between contrastive and non-contrastive methods towards global and local
spectral embedding methods respectively, hinting at the benefits and
limitations of each. For example, (i) if the pairwise relation is aligned with
the downstream task, any SSL method can be employed successfully and will
recover the supervised method, but in the low data regime, VICReg's invariance
hyper-parameter should be high; (ii) if the pairwise relation is misaligned
with the downstream task, VICReg with small invariance hyper-parameter should
be preferred over SimCLR or BarlowTwins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance. (arXiv:2205.12022v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12022">
<div class="article-summary-box-inner">
<span><p>With the rapid development of the Metaverse, virtual humans have emerged, and
human image synthesis and editing techniques, such as pose transfer, have
recently become popular. Most of the existing techniques rely on GANs, which
can generate good human images even with large variants and occlusions. But
from our best knowledge, the existing state-of-the-art method still has the
following problems: the first is that the rendering effect of the synthetic
image is not realistic, such as poor rendering of some regions. And the second
is that the training of GAN is unstable and slow to converge, such as model
collapse. Based on the above two problems, we propose several methods to solve
them. To improve the rendering effect, we use the Residual Fast Fourier
Transform Block to replace the traditional Residual Block. Then, spectral
normalization and Wasserstein distance are used to improve the speed and
stability of GAN training. Experiments demonstrate that the methods we offer
are effective at solving the problems listed above, and we get state-of-the-art
scores in LPIPS and PSNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCoViT: Mobile Convolutional Vision Transformer. (arXiv:2205.12635v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12635">
<div class="article-summary-box-inner">
<span><p>Recently, Transformer networks have achieved impressive results on a variety
of vision tasks. However, most of them are computationally expensive and not
suitable for real-world mobile applications. In this work, we present Mobile
Convolutional Vision Transformer (MoCoViT), which improves in performance and
efficiency by introducing transformer into mobile convolutional networks to
leverage the benefits of both architectures. Different from recent works on
vision transformer, the mobile transformer block in MoCoViT is carefully
designed for mobile devices and is very lightweight, accomplished through two
primary modifications: the Mobile Self-Attention (MoSA) module and the Mobile
Feed Forward Network (MoFFN). MoSA simplifies the calculation of the attention
map through Branch Sharing scheme while MoFFN serves as a mobile version of MLP
in the transformer, further reducing the computation by a large margin.
Comprehensive experiments verify that our proposed MoCoViT family outperform
state-of-the-art portable CNNs and transformer neural architectures on various
vision tasks. On ImageNet classification, it achieves 74.5% top-1 accuracy at
147M FLOPs, gaining 1.2% over MobileNetV3 with less computations. And on the
COCO object detection task, MoCoViT outperforms GhostNet by 2.1 AP in RetinaNet
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniInst: Unique Representation for End-to-End Instance Segmentation. (arXiv:2205.12646v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12646">
<div class="article-summary-box-inner">
<span><p>Existing instance segmentation methods have achieved impressive performance
but still suffer from a common dilemma: redundant representations (e.g.,
multiple boxes, grids, and anchor points) are inferred for one instance, which
leads to multiple duplicated predictions. Thus, mainstream methods usually rely
on a hand-designed non-maximum suppression (NMS) post-processing step to select
the optimal prediction result, which hinders end-to-end training. To address
this issue, we propose a box-free and NMS-free end-to-end instance segmentation
framework, termed UniInst, that yields only one unique representation for each
instance. Specifically, we design an instance-aware one-to-one assignment
scheme, namely Only Yield One Representation (OYOR), which dynamically assigns
one unique representation to each instance according to the matching quality
between predictions and ground truths. Then, a novel prediction re-ranking
strategy is elegantly integrated into the framework to address the misalignment
between the classification score and the mask quality, enabling the learned
representation to be more discriminative. With these techniques, our UniInst,
the first FCN-based end-to-end instance segmentation framework, achieves
competitive performance, e.g., 39.0 mask AP using ResNet-50-FPN and 40.2 mask
AP using ResNet-101-FPN, against mainstream methods on COCO test-dev. Moreover,
the proposed instance-aware method is robust to occlusion scenes, outperforming
common baselines by remarkable mask AP on the heavily-occluded OCHuman
benchmark. Our codes will be available upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure Unbiased Adversarial Model for Medical Image Segmentation. (arXiv:2205.12857v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12857">
<div class="article-summary-box-inner">
<span><p>Generative models have been widely proposed in image recognition to generate
more images where the distribution is similar to that of the real images. It
often introduces a discriminator network to discriminate original real data and
generated data.
</p>
<p>However, such discriminator often considers the distribution of the data and
did not pay enough attention to the intrinsic gap due to structure.
</p>
<p>In this paper, we reformulate a new image to image translation problem to
reduce structural gap, in addition to the typical intensity distribution gap.
We further propose a simple yet important Structure Unbiased Adversarial Model
for Medical Image Segmentation (SUAM) with learnable inverse structural
deformation for medical image segmentation. It consists of a structure
extractor, an attention diffeomorphic registration and a structure \&amp; intensity
distribution rendering module. The structure extractor aims to extract the
dominant structure of the input image. The attention diffeomorphic registration
is proposed to reduce the structure gap with an inverse deformation field to
warp the prediction masks back to their original form. The structure rendering
module is to render the deformed structure to an image with targeted intensity
distribution. We apply the proposed SUAM on both optical coherence tomography
(OCT), magnetic resonance imaging (MRI) and computerized tomography (CT) data.
Experimental results show that the proposed method has the capability to
transfer both intensity and structure distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inception Transformer. (arXiv:2205.12956v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12956">
<div class="article-summary-box-inner">
<span><p>Recent studies show that Transformer has strong capability of building
long-range dependencies, yet is incompetent in capturing high frequencies that
predominantly convey local information. To tackle this issue, we present a
novel and general-purpose Inception Transformer, or iFormer for short, that
effectively learns comprehensive features with both high- and low-frequency
information in visual data. Specifically, we design an Inception mixer to
explicitly graft the advantages of convolution and max-pooling for capturing
the high-frequency information to Transformers. Different from recent hybrid
frameworks, the Inception mixer brings greater efficiency through a channel
splitting mechanism to adopt parallel convolution/max-pooling path and
self-attention path as high- and low-frequency mixers, while having the
flexibility to model discriminative information scattered within a wide
frequency range. Considering that bottom layers play more roles in capturing
high-frequency details while top layers more in modeling low-frequency global
information, we further introduce a frequency ramp structure, i.e. gradually
decreasing the dimensions fed to the high-frequency mixer and increasing those
to the low-frequency mixer, which can effectively trade-off high- and
low-frequency components across different layers. We benchmark the iFormer on a
series of vision tasks, and showcase that it achieves impressive performance on
image classification, COCO detection and ADE20K segmentation. For example, our
iFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than
DeiT-S by 3.6%, and even slightly better than much bigger model Swin-B (83.3%)
with only 1/4 parameters and 1/3 FLOPs. Code and models will be released at
https://github.com/sail-sg/iFormer.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-27 23:08:47.518809572 UTC">2022-05-27 23:08:47 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>