<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-18T01:30:00Z">02-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction. (arXiv:2202.08316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08316">
<div class="article-summary-box-inner">
<span><p>This paper presents FAMIE, a comprehensive and efficient active learning (AL)
toolkit for multilingual information extraction. FAMIE is designed to address a
fundamental problem in existing AL frameworks where annotators need to wait for
a long time between annotation batches due to the time-consuming nature of
model training and data selection at each AL iteration. This hinders the
engagement, productivity, and efficiency of annotators. Based on the idea of
using a small proxy network for fast data selection, we introduce a novel
knowledge distillation mechanism to synchronize the proxy network with the main
large model (i.e., BERT-based) to ensure the appropriateness of the selected
annotation examples for the main model. Our AL framework can support multiple
languages. The experiments demonstrate the advantages of FAMIE in terms of
competitive performance and time efficiency for sequence labeling with AL. We
publicly release our code (\url{https://github.com/nlp-uoregon/famie}) and demo
website (\url{<a href="http://nlp.uoregon.edu:9000/">this http URL</a>}). A demo video for FAMIE is
provided at: \url{https://youtu.be/I2i8n_jAyrY}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Based Action-Model Acquisition for Planning. (arXiv:2202.08373v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08373">
<div class="article-summary-box-inner">
<span><p>Although there have been approaches that are capable of learning action
models from plan traces, there is no work on learning action models from
textual observations, which is pervasive and much easier to collect from
real-world applications compared to plan traces. In this paper we propose a
novel approach to learning action models from natural language texts by
integrating Constraint Satisfaction and Natural Language Processing techniques.
Specifically, we first build a novel language model to extract plan traces from
texts, and then build a set of constraints to generate action models based on
the extracted plan traces. After that, we iteratively improve the language
model and constraints until we achieve the convergent language model and action
models. We empirically exhibit that our approach is both effective and
efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Training of Both Translation Models in the Back-Translation Framework. (arXiv:2202.08465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08465">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning algorithms in neural machine translation (NMT) have
significantly improved translation quality compared to the supervised learning
algorithms by using additional monolingual corpora. Among them,
back-translation is a theoretically well-structured and cutting-edge method.
Given two pre-trained NMT models between source and target languages, one
translates a monolingual sentence as a latent sentence, and the other
reconstructs the monolingual input sentence given the latent sentence.
Therefore, previous works tried to apply the variational auto-encoder's (VAE)
training framework to the back-translation framework. However, the discrete
property of the latent sentence made it impossible to use backpropagation in
the framework. This paper proposes a categorical reparameterization trick that
generates a differentiable sentence, with which we practically implement the
VAE's training framework for the back-translation and train it by end-to-end
backpropagation. In addition, we propose several regularization techniques that
are especially advantageous to this framework. In our experiments, we
demonstrate that our method makes backpropagation available through the latent
sentences and improves the BLEU scores on the datasets of the WMT18 translation
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Evaluation Metrics of Paraphrase Generation. (arXiv:2202.08479v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08479">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation is an important NLP task that has achieved significant
progress recently. However, one crucial problem is overlooked, `how to evaluate
the quality of paraphrase?'. Most existing paraphrase generation models use
reference-based metrics (e.g., BLEU) from neural machine translation (NMT) to
evaluate their generated paraphrase. Such metrics' reliability is hardly
evaluated, and they are only plausible when there exists a standard reference.
Therefore, this paper first answers one fundamental question, `Are existing
metrics reliable for paraphrase generation?'. We present two conclusions that
disobey conventional wisdom in paraphrasing generation: (1) existing metrics
poorly align with human annotation in system-level and segment-level paraphrase
evaluation. (2) reference-free metrics outperform reference-based metrics,
indicating that the standard references are unnecessary to evaluate the
paraphrase's quality. Such empirical findings expose a lack of reliable
automatic evaluation metrics. Therefore, this paper proposes BBScore, a
reference-free metric that can reflect the generated paraphrase's quality.
BBScore consists of two sub-metrics: S3C score and SelfBLEU, which correspond
to two criteria for paraphrase evaluation: semantic preservation and diversity.
By connecting two sub-metrics, BBScore significantly outperforms existing
paraphrase evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AISHELL-NER: Named Entity Recognition from Chinese Speech. (arXiv:2202.08533v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08533">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) from speech is among Spoken Language
Understanding (SLU) tasks, aiming to extract semantic information from the
speech signal. NER from speech is usually made through a two-step pipeline that
consists of (1) processing the audio using an Automatic Speech Recognition
(ASR) system and (2) applying an NER tagger to the ASR outputs. Recent works
have shown the capability of the End-to-End (E2E) approach for NER from English
and French speech, which is essentially entity-aware ASR. However, due to the
many homophones and polyphones that exist in Chinese, NER from Chinese speech
is effectively a more challenging task. In this paper, we introduce a new
dataset AISEHLL-NER for NER from Chinese speech. Extensive experiments are
conducted to explore the performance of several state-of-the-art methods. The
results demonstrate that the performance could be improved by combining
entity-aware ASR and pretrained NER tagger, which can be easily applied to the
modern SLU pipeline. The dataset is publicly available at
github.com/Alibaba-NLP/AISHELL-NER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Term Rewriting Based On Set Automaton Matching. (arXiv:2202.08687v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08687">
<div class="article-summary-box-inner">
<span><p>In previous work we have proposed an efficient pattern matching algorithm
based on the notion of set automaton. In this article we investigate how set
automata can be exploited to implement efficient term rewriting procedures.
These procedures interleave pattern matching steps and rewriting steps and thus
smoothly integrate redex discovery and subterm replacement. Concretely, we
propose an optimised algorithm for outermost rewriting of left-linear term
rewriting systems, prove its correctness, and present the results of some
implementation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08712">
<div class="article-summary-box-inner">
<span><p>To date, there are no effective treatments for most neurodegenerative
diseases. Knowledge graphs can provide comprehensive and semantic
representation for heterogeneous data, and have been successfully leveraged in
many biomedical applications including drug repurposing. Our objective is to
construct a knowledge graph from literature to study relations between
Alzheimer's disease (AD) and chemicals, drugs and dietary supplements in order
to identify opportunities to prevent or delay neurodegenerative progression. We
collected biomedical annotations and extracted their relations using SemRep via
SemMedDB. We used both a BERT-based classifier and rule-based methods during
data preprocessing to exclude noise while preserving most AD-related semantic
triples. The 1,672,110 filtered triples were used to train with knowledge graph
completion algorithms (i.e., TransE, DistMult, and ComplEx) to predict
candidates that might be helpful for AD treatment or prevention. Among three
knowledge graph completion models, TransE outperformed the other two (MR =
13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further
evaluate the prediction results. We found supporting evidence for most highly
ranked candidates predicted by our model which indicates that our approach can
inform reliable new knowledge. This paper shows that our graph mining model can
predict reliable new relationships between AD and other entities (i.e., dietary
supplements, chemicals, and drugs). The knowledge graph constructed can
facilitate data-driven knowledge discoveries and the generation of novel
hypotheses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models. (arXiv:2202.08772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08772">
<div class="article-summary-box-inner">
<span><p>With the increasing of model capacity brought by pre-trained language models,
there emerges boosting needs for more knowledgeable natural language processing
(NLP) models with advanced functionalities including providing and making
flexible use of encyclopedic and commonsense knowledge. The mere pre-trained
language models, however, lack the capacity of handling such
knowledge-intensive NLP tasks alone. To address this challenge, large numbers
of pre-trained language models augmented with external knowledge sources are
proposed and in rapid development. In this paper, we aim to summarize the
current progress of pre-trained language model-based knowledge-enhanced models
(PLMKEs) by dissecting their three vital elements: knowledge sources,
knowledge-intensive NLP tasks, and knowledge fusion methods. Finally, we
present the challenges of PLMKEs based on the discussion regarding the three
elements and attempt to provide NLP practitioners with potential directions for
further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">cosFormer: Rethinking Softmax in Attention. (arXiv:2202.08791v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08791">
<div class="article-summary-box-inner">
<span><p>Transformer has shown great successes in natural language processing,
computer vision, and audio processing. As one of its core components, the
softmax attention helps to capture long-range dependencies yet prohibits its
scale-up due to the quadratic space and time complexity to the sequence length.
Kernel methods are often adopted to reduce the complexity by approximating the
softmax operator. Nevertheless, due to the approximation errors, their
performances vary in different tasks/corpus and suffer crucial performance
drops when compared with the vanilla softmax attention. In this paper, we
propose a linear transformer called cosFormer that can achieve comparable or
better accuracy to the vanilla transformer in both casual and cross attentions.
cosFormer is based on two key properties of softmax attention: i).
non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme
that can concentrate the distribution of the attention matrix. As its linear
substitute, cosFormer fulfills these properties with a linear operator and a
cosine-based distance re-weighting mechanism. Extensive experiments on language
modeling and text understanding tasks demonstrate the effectiveness of our
method. We further examine our method on long sequences and achieve
state-of-the-art performance on the Long-Range Arena benchmark. The source code
is available at https://github.com/OpenNLPLab/cosFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08806">
<div class="article-summary-box-inner">
<span><p>We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist
approach toward learning a compositional and grounded meaning representation of
language from grounded data, such as paired images and texts. At the core of
G2L2 is a collection of lexicon entries, which map each word to a tuple of a
syntactic type and a neuro-symbolic semantic program. For example, the word
shiny has a syntactic type of adjective; its neuro-symbolic semantic program
has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is
associated with a neural network embedding, which will be used to classify
shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries
associated with each token. It then derives the meaning of the sentence as an
executable neuro-symbolic program by composing lexical meanings based on
syntax. The recovered meaning programs can be executed on grounded inputs. To
facilitate learning in an exponentially-growing compositional space, we
introduce a joint parsing and expected execution algorithm, which does local
marginalization over derivations to reduce the training time. We evaluate G2L2
on two domains: visual reasoning and language-driven navigation. Results show
that G2L2 can generalize from small amounts of data to novel compositions of
words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Creative Inspiration with Fine-Grained Functional Aspects of Ideas. (arXiv:2102.09761v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09761">
<div class="article-summary-box-inner">
<span><p>Large repositories of products, patents and scientific papers offer an
opportunity for building systems that scour millions of ideas and help users
discover inspirations. However, idea descriptions are typically in the form of
unstructured text, lacking key structure that is required for supporting
creative innovation interactions. Prior work has explored idea representations
that were either limited in expressivity, required significant manual effort
from users, or dependent on curated knowledge bases with poor coverage. We
explore a novel representation that automatically breaks up products into
fine-grained functional aspects capturing the purposes and mechanisms of ideas,
and use it to support important creative innovation interactions: functional
search for ideas, and exploration of the design space around a focal problem by
viewing related problem perspectives pooled from across many products. In user
studies, our approach boosts the quality of creative search and inspirations,
substantially outperforming strong baselines by 50-60%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integration of Pre-trained Networks with Continuous Token Interface for End-to-End Spoken Language Understanding. (arXiv:2104.07253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07253">
<div class="article-summary-box-inner">
<span><p>Most End-to-End (E2E) SLU networks leverage the pre-trained ASR networks but
still lack the capability to understand the semantics of utterances, crucial
for the SLU task. To solve this, recently proposed studies use pre-trained NLU
networks. However, it is not trivial to fully utilize both pre-trained
networks; many solutions were proposed, such as Knowledge Distillation,
cross-modal shared embedding, and network integration with Interface. We
propose a simple and robust integration method for the E2E SLU network with
novel Interface, Continuous Token Interface (CTI), the junctional
representation of the ASR and NLU networks when both networks are pre-trained
with the same vocabulary. Because the only difference is the noise level, we
directly feed the ASR network's output to the NLU network. Thus, we can train
our SLU network in an E2E manner without additional modules, such as
Gumbel-Softmax. We evaluate our model using SLURP, a challenging SLU dataset
and achieve state-of-the-art scores on both intent classification and slot
filling tasks. We also verify the NLU network, pre-trained with Masked Language
Model, can utilize a noisy textual representation of CTI. Moreover, we show our
model can be trained with multi-task learning from heterogeneous data even
after integration with CTI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning. (arXiv:2110.02600v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02600">
<div class="article-summary-box-inner">
<span><p>Multilingual models jointly pretrained on multiple languages have achieved
remarkable performance on various multilingual downstream tasks. Moreover,
models finetuned on a single monolingual downstream task have shown to
generalize to unseen languages. In this paper, we first show that it is crucial
for those tasks to align gradients between them in order to maximize knowledge
transfer while minimizing negative transfer. Despite its importance, the
existing methods for gradient alignment either have a completely different
purpose, ignore inter-task alignment, or aim to solve continual learning
problems in rather inefficient ways. As a result of the misaligned gradients
between tasks, the model suffers from severe negative transfer in the form of
catastrophic forgetting of the knowledge acquired from the pretraining. To
overcome the limitations, we propose a simple yet effective method that can
efficiently align gradients between tasks. Specifically, we perform each
inner-optimization by sequentially sampling batches from all the tasks,
followed by a Reptile outer update. Thanks to the gradients aligned between
tasks by our method, the model becomes less vulnerable to negative transfer and
catastrophic forgetting. We extensively validate our method on various
multi-task learning and zero-shot cross-lingual transfer tasks, where our
method largely outperforms all the relevant baselines we consider.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07298">
<div class="article-summary-box-inner">
<span><p>Existing approaches to lifelong language learning rely on plenty of labeled
data for learning a new task, which is hard to obtain in most real scenarios.
Considering that humans can continually learn new tasks from a handful of
examples, we expect the models also to be able to generalize well on new
few-shot tasks without forgetting the previous ones. In this work, we define
this more challenging yet practical problem as Lifelong Few-shot Language
Learning (LFLL) and propose a unified framework for it based on prompt tuning
of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot
learning ability, and simultaneously trains the model as a task solver and a
data generator. Before learning a new domain of the same task type, LFPT5
generates pseudo (labeled) samples of previously learned domains, and later
gets trained on those samples to alleviate forgetting of previous knowledge as
it learns the new domain. In addition, a KL divergence loss is minimized to
achieve label consistency between the previous and the current model. While
adapting to a new task type, LFPT5 includes and tunes additional prompt
embeddings for the new task. With extensive experiments, we demonstrate that
LFPT5 can be applied to various different types of tasks and significantly
outperform previous methods in different LFLL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't speak too fast: The impact of data bias on self-supervised speech models. (arXiv:2110.07957v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07957">
<div class="article-summary-box-inner">
<span><p>Self-supervised Speech Models (S3Ms) have been proven successful in many
speech downstream tasks, like ASR. However, how pre-training data affects S3Ms'
downstream behavior remains an unexplored issue. In this paper, we study how
pre-training data affects S3Ms by pre-training models on biased datasets
targeting different factors of speech, including gender, content, and prosody,
and evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB
Benchmark. Our experiments show that S3Ms have tolerance toward gender bias.
Moreover, we find that the content of speech has little impact on the
performance of S3Ms across downstream tasks, but S3Ms do show a preference
toward a slower speech rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DataWords: Getting Contrarian with Text, Structured Data and Explanations. (arXiv:2111.05384v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05384">
<div class="article-summary-box-inner">
<span><p>Our goal is to build classification models using a combination of free-text
and structured data. To do this, we represent structured data by text
sentences, DataWords, so that similar data items are mapped into the same
sentence. This permits modeling a mixture of text and structured data by using
only text-modeling algorithms. Several examples illustrate that it is possible
to improve text classification performance by first running extraction tools
(named entity recognition), then converting the output to DataWords, and adding
the DataWords to the original text -- before model building and classification.
This approach also allows us to produce explanations for inferences in terms of
both free text and structured data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User Response and Sentiment Prediction for Automatic Dialogue Evaluation. (arXiv:2111.08808v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08808">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation is beneficial for open-domain dialog system development.
However, standard word-overlap metrics (BLEU, ROUGE) do not correlate well with
human judgements of open-domain dialog systems. In this work we propose to use
the sentiment of the next user utterance for turn or dialog level evaluation.
Specifically we propose three methods: one that predicts the next sentiment
directly, and two others that predict the next user utterance using an
utterance or a feedback generator model and then classify its sentiment.
Experiments show our model outperforming existing automatic evaluation metrics
on both written and spoken open-domain dialogue datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning music audio representations via weak language supervision. (arXiv:2112.04214v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04214">
<div class="article-summary-box-inner">
<span><p>Audio representations for music information retrieval are typically learned
via supervised learning in a task-specific fashion. Although effective at
producing state-of-the-art results, this scheme lacks flexibility with respect
to the range of applications a model can have and requires extensively
annotated datasets. In this work, we pose the question of whether it may be
possible to exploit weakly aligned text as the only supervisory signal to learn
general-purpose music audio representations. To address this question, we
design a multimodal architecture for music and language pre-training (MuLaP)
optimised via a set of proxy tasks. Weak supervision is provided in the form of
noisy natural language descriptions conveying the overall musical content of
the track. After pre-training, we transfer the audio backbone of the model to a
set of music audio classification and regression tasks. We demonstrate the
usefulness of our approach by comparing the performance of audio
representations produced by the same audio backbone with different training
strategies and show that our pre-training method consistently achieves
comparable or higher scores on all tasks and datasets considered. Our
experiments also confirm that MuLaP effectively leverages audio-caption pairs
to learn representations that are competitive with audio-only and cross-modal
self-supervised methods in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Isometric MT: Neural Machine Translation for Automatic Dubbing. (arXiv:2112.08682v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08682">
<div class="article-summary-box-inner">
<span><p>Automatic dubbing (AD) is among the machine translation (MT) use cases where
translations should match a given length to allow for synchronicity between
source and target speech. For neural MT, generating translations of length
close to the source length (e.g. within +-10% in character count), while
preserving quality is a challenging task. Controlling MT output length comes at
a cost to translation quality, which is usually mitigated with a two step
approach of generating N-best hypotheses and then re-ranking based on length
and quality. This work introduces a self-learning approach that allows a
transformer model to directly learn to generate outputs that closely match the
source length, in short Isometric MT. In particular, our approach does not
require to generate multiple hypotheses nor any auxiliary ranking function. We
report results on four language pairs (English - French, Italian, German,
Spanish) with a publicly available benchmark. Automatic and manual evaluations
show that our method for Isometric MT outperforms more complex approaches
proposed in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sublinear Time Approximation of Text Similarity Matrices. (arXiv:2112.09631v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09631">
<div class="article-summary-box-inner">
<span><p>We study algorithms for approximating pairwise similarity matrices that arise
in natural language processing. Generally, computing a similarity matrix for
$n$ data points requires $\Omega(n^2)$ similarity computations. This quadratic
scaling is a significant bottleneck, especially when similarities are computed
via expensive functions, e.g., via transformer models. Approximation methods
reduce this quadratic complexity, often by using a small subset of exactly
computed similarities to approximate the remainder of the complete pairwise
similarity matrix.
</p>
<p>Significant work focuses on the efficient approximation of positive
semidefinite (PSD) similarity matrices, which arise e.g., in kernel methods.
However, much less is understood about indefinite (non-PSD) similarity
matrices, which often arise in NLP. Motivated by the observation that many of
these matrices are still somewhat close to PSD, we introduce a generalization
of the popular Nystr\"{o}m method to the indefinite setting. Our algorithm can
be applied to any similarity matrix and runs in sublinear time in the size of
the matrix, producing a rank-$s$ approximation with just $O(ns)$ similarity
computations.
</p>
<p>We show that our method, along with a simple variant of CUR decomposition,
performs very well in approximating a variety of similarity matrices arising in
NLP tasks. We demonstrate high accuracy of the approximated similarity matrices
in the downstream tasks of document classification, sentence similarity, and
cross-document coreference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regional Differences in Information Privacy Concerns After the Facebook-Cambridge Analytica Data Scandal. (arXiv:2202.07075v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07075">
<div class="article-summary-box-inner">
<span><p>While there is increasing global attention to data privacy, most of their
current theoretical understanding is based on research conducted in a few
countries. Prior work argues that people's cultural backgrounds might shape
their privacy concerns; thus, we could expect people from different world
regions to conceptualize them in diverse ways. We collected and analyzed a
large-scale dataset of tweets about the #CambridgeAnalytica scandal in Spanish
and English to start exploring this hypothesis. We employed word embeddings and
qualitative analysis to identify which information privacy concerns are present
and characterize language and regional differences in emphasis on these
concerns. Our results suggest that related concepts, such as regulations, can
be added to current information privacy frameworks. We also observe a greater
emphasis on data collection in English than in Spanish. Additionally, data from
North America exhibits a narrower focus on awareness compared to other regions
under study. Our results call for more diverse sources of data and nuanced
analysis of data privacy concerns around the globe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Speech Recognition By Learning Conversation-level Characteristics. (arXiv:2202.07855v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07855">
<div class="article-summary-box-inner">
<span><p>Conversational automatic speech recognition (ASR) is a task to recognize
conversational speech including multiple speakers. Unlike sentence-level ASR,
conversational ASR can naturally take advantages from specific characteristics
of conversation, such as role preference and topical coherence. This paper
proposes a conversational ASR model which explicitly learns conversation-level
characteristics under the prevalent end-to-end neural framework. The highlights
of the proposed model are twofold. First, a latent variational module (LVM) is
attached to a conformer-based encoder-decoder ASR backbone to learn role
preference and topical coherence. Second, a topic model is specifically adopted
to bias the outputs of the decoder to words in the predicted topics.
Experiments on two Mandarin conversational ASR tasks show that the proposed
model achieves a maximum 12% relative character error rate (CER) reduction.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation and Analysis of Different Aggregation and Hyperparameter Selection Methods for Federated Brain Tumor Segmentation. (arXiv:2202.08261v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08261">
<div class="article-summary-box-inner">
<span><p>Availability of large, diverse, and multi-national datasets is crucial for
the development of effective and clinically applicable AI systems in the
medical imaging domain. However, forming a global model by bringing these
datasets together at a central location, comes along with various data privacy
and ownership problems. To alleviate these problems, several recent studies
focus on the federated learning paradigm, a distributed learning approach for
decentralized data. Federated learning leverages all the available data without
any need for sharing collaborators' data with each other or collecting them on
a central server. Studies show that federated learning can provide competitive
performance with conventional central training, while having a good
generalization capability. In this work, we have investigated several federated
learning approaches on the brain tumor segmentation problem. We explore
different strategies for faster convergence and better performance which can
also work on strong Non-IID cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phase Aberration Robust Beamformer for Planewave US Using Self-Supervised Learning. (arXiv:2202.08262v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08262">
<div class="article-summary-box-inner">
<span><p>Ultrasound (US) is widely used for clinical imaging applications thanks to
its real-time and non-invasive nature. However, its lesion detectability is
often limited in many applications due to the phase aberration artefact caused
by variations in the speed of sound (SoS) within body parts. To address this,
here we propose a novel self-supervised 3D CNN that enables phase aberration
robust plane-wave imaging. Instead of aiming at estimating the SoS distribution
as in conventional methods, our approach is unique in that the network is
trained in a self-supervised manner to robustly generate a high-quality image
from various phase aberrated images by modeling the variation in the speed of
sound as stochastic. Experimental results using real measurements from
tissue-mimicking phantom and \textit{in vivo} scans confirmed that the proposed
method can significantly reduce the phase aberration artifacts and improve the
visual quality of deep scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenKBP-Opt: An international and reproducible evaluation of 76 knowledge-based planning pipelines. (arXiv:2202.08303v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08303">
<div class="article-summary-box-inner">
<span><p>We establish an open framework for developing plan optimization models for
knowledge-based planning (KBP) in radiotherapy. Our framework includes
reference plans for 100 patients with head-and-neck cancer and high-quality
dose predictions from 19 KBP models that were developed by different research
groups during the OpenKBP Grand Challenge. The dose predictions were input to
four optimization models to form 76 unique KBP pipelines that generated 7600
plans. The predictions and plans were compared to the reference plans via: dose
score, which is the average mean absolute voxel-by-voxel difference in dose a
model achieved; the deviation in dose-volume histogram (DVH) criterion; and the
frequency of clinical planning criteria satisfaction. We also performed a
theoretical investigation to justify our dose mimicking models. The range in
rank order correlation of the dose score between predictions and their KBP
pipelines was 0.50 to 0.62, which indicates that the quality of the predictions
is generally positively correlated with the quality of the plans. Additionally,
compared to the input predictions, the KBP-generated plans performed
significantly better (P&lt;0.05; one-sided Wilcoxon test) on 18 of 23 DVH
criteria. Similarly, each optimization model generated plans that satisfied a
higher percentage of criteria than the reference plans. Lastly, our theoretical
investigation demonstrated that the dose mimicking models generated plans that
are also optimal for a conventional planning model. This was the largest
international effort to date for evaluating the combination of KBP prediction
and optimization models. In the interest of reproducibility, our data and code
is freely available at https://github.com/ababier/open-kbp-opt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualize differential privacy in image database: a lightweight image differential privacy approach based on principle component analysis inverse. (arXiv:2202.08309v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08309">
<div class="article-summary-box-inner">
<span><p>Differential privacy (DP) has been the de-facto standard to preserve
privacy-sensitive information in database. Nevertheless, there lacks a clear
and convincing contextualization of DP in image database, where individual
images' indistinguishable contribution to a certain analysis can be achieved
and observed when DP is exerted. As a result, the privacy-accuracy trade-off
due to integrating DP is insufficiently demonstrated in the context of
differentially-private image database. This work aims at contextualizing DP in
image database by an explicit and intuitive demonstration of integrating
conceptional differential privacy with images. To this end, we design a
lightweight approach dedicating to privatizing image database as a whole and
preserving the statistical semantics of the image database to an adjustable
level, while making individual images' contribution to such statistics
indistinguishable. The designed approach leverages principle component analysis
(PCA) to reduce the raw image with large amount of attributes to a lower
dimensional space whereby DP is performed, so as to decrease the DP load of
calculating sensitivity attribute-by-attribute. The DP-exerted image data,
which is not visible in its privatized format, is visualized through PCA
inverse such that both a human and machine inspector can evaluate the
privatization and quantify the privacy-accuracy trade-off in an analysis on the
privatized image database. Using the devised approach, we demonstrate the
contextualization of DP in images by two use cases based on deep learning
models, where we show the indistinguishability of individual images induced by
DP and the privatized images' retention of statistical semantics in deep
learning tasks, which is elaborated by quantitative analyses on the
privacy-accuracy trade-off under different privatization settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments. (arXiv:2202.08325v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08325">
<div class="article-summary-box-inner">
<span><p>Data-Augmentation (DA) is known to improve performance across tasks and
datasets. We propose a method to theoretically analyze the effect of DA and
study questions such as: how many augmented samples are needed to correctly
estimate the information encoded by that DA? How does the augmentation policy
impact the final parameters of a model? We derive several quantities in
close-form, such as the expectation and variance of an image, loss, and model's
output under a given DA distribution. Those derivations open new avenues to
quantify the benefits and limitations of DA. For example, we show that common
DAs require tens of thousands of samples for the loss at hand to be correctly
estimated and for the model training to converge. We show that for a training
loss to be stable under DA sampling, the model's saliency map (gradient of the
loss with respect to the model's input) must align with the smallest
eigenvector of the sample variance under the considered DA augmentation,
hinting at a possible explanation on why models tend to shift their focus from
edges to textures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CortexODE: Learning Cortical Surface Reconstruction by Neural ODEs. (arXiv:2202.08329v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08329">
<div class="article-summary-box-inner">
<span><p>We present CortexODE, a deep learning framework for cortical surface
reconstruction. CortexODE leverages neural ordinary different equations (ODEs)
to deform an input surface into a target shape by learning a diffeomorphic
flow. The trajectories of the points on the surface are modeled as ODEs, where
the derivatives of their coordinates are parameterized via a learnable
Lipschitz-continuous deformation network. This provides theoretical guarantees
for the prevention of self-intersections. CortexODE can be integrated to an
automatic learning-based pipeline, which reconstructs cortical surfaces
efficiently in less than 6 seconds. The pipeline utilizes a 3D U-Net to predict
a white matter segmentation from brain Magnetic Resonance Imaging (MRI) scans,
and further generates a signed distance function that represents an initial
surface. Fast topology correction is introduced to guarantee homeomorphism to a
sphere. Following the isosurface extraction step, two CortexODE models are
trained to deform the initial surface to white matter and pial surfaces
respectively. The proposed pipeline is evaluated on large-scale neuroimage
datasets in various age groups including neonates (25-45 weeks), young adults
(22-36 years) and elderly subjects (55-90 years). Our experiments demonstrate
that the CortexODE-based pipeline can achieve less than 0.2mm average geometric
error while being orders of magnitude faster compared to conventional
processing pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines. (arXiv:2202.08340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08340">
<div class="article-summary-box-inner">
<span><p>Early in development, children learn to extend novel category labels to
objects with the same shape, a phenomenon known as the shape bias. Inspired by
these findings, Geirhos et al. (2019) examined whether deep neural networks
show a shape or texture bias by constructing images with conflicting shape and
texture cues. They found that convolutional neural networks strongly preferred
to classify familiar objects based on texture as opposed to shape, suggesting a
texture bias. However, there are a number of differences between how the
networks were tested in this study versus how children are typically tested. In
this work, we re-examine the inductive biases of neural networks by adapting
the stimuli and procedure from Geirhos et al. (2019) to more closely follow the
developmental paradigm and test on a wide range of pre-trained neural networks.
Across three experiments, we find that deep neural networks exhibit a
preference for shape rather than texture when tested under conditions that more
closely replicate the developmental procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomalib: A Deep Learning Library for Anomaly Detection. (arXiv:2202.08341v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08341">
<div class="article-summary-box-inner">
<span><p>This paper introduces anomalib, a novel library for unsupervised anomaly
detection and localization. With reproducibility and modularity in mind, this
open-source library provides algorithms from the literature and a set of tools
to design custom anomaly detection algorithms via a plug-and-play approach.
Anomalib comprises state-of-the-art anomaly detection algorithms that achieve
top performance on the benchmarks and that can be used off-the-shelf. In
addition, the library provides components to design custom algorithms that
could be tailored towards specific needs. Additional tools, including
experiment trackers, visualizers, and hyper-parameter optimizers, make it
simple to design and implement anomaly detection models. The library also
supports OpenVINO model optimization and quantization for real-time deployment.
Overall, anomalib is an extensive library for the design, implementation, and
deployment of unsupervised anomaly detection models from data to the edge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Smooth Neural Functions via Lipschitz Regularization. (arXiv:2202.08345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08345">
<div class="article-summary-box-inner">
<span><p>Neural implicit fields have recently emerged as a useful representation for
3D shapes. These fields are commonly represented as neural networks which map
latent descriptors and 3D coordinates to implicit function values. The latent
descriptor of a neural field acts as a deformation handle for the 3D shape it
represents. Thus, smoothness with respect to this descriptor is paramount for
performing shape-editing operations. In this work, we introduce a novel
regularization designed to encourage smooth latent spaces in neural fields by
penalizing the upper bound on the field's Lipschitz constant. Compared with
prior Lipschitz regularized networks, ours is computationally fast, can be
implemented in four lines of code, and requires minimal hyperparameter tuning
for geometric applications. We demonstrate the effectiveness of our approach on
shape interpolation and extrapolation as well as partial shape reconstruction
from 3D point clouds, showing both qualitative and quantitative improvements
over existing state-of-the-art and non-regularized baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision. (arXiv:2202.08360v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08360">
<div class="article-summary-box-inner">
<span><p>Discriminative self-supervised learning allows training models on any random
group of internet images, and possibly recover salient information that helps
differentiate between the images. Applied to ImageNet, this leads to object
centric features that perform on par with supervised features on most
object-centric downstream tasks. In this work, we question if using this
ability, we can learn any salient and more representative information present
in diverse unbounded set of images from across the globe. To do so, we train
models on billions of random images without any data pre-processing or prior
assumptions about what we want the model to learn. We scale our model size to
dense 10 billion parameters to avoid underfitting on a large data size. We
extensively study and validate our model performance on over 50 benchmarks
including fairness, robustness to distribution shift, geographical diversity,
fine grained recognition, image copy detection and many image classification
datasets. The resulting model, not only captures well semantic information, it
also captures information about artistic style and learns salient information
such as geolocations and multilingual word embeddings based on visual content
only. More importantly, we discover that such model is more robust, more fair,
less harmful and less biased than supervised models or models trained on object
centric datasets such as ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fuzzy Pooling. (arXiv:2202.08372v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08372">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) are artificial learning systems
typically based on two operations: convolution, which implements feature
extraction through filtering, and pooling, which implements dimensionality
reduction. The impact of pooling in the classification performance of the CNNs
has been highlighted in several previous works, and a variety of alternative
pooling operators have been proposed. However, only a few of them tackle with
the uncertainty that is naturally propagated from the input layer to the
feature maps of the hidden layers through convolutions. In this paper we
present a novel pooling operation based on (type-1) fuzzy sets to cope with the
local imprecision of the feature maps, and we investigate its performance in
the context of image classification. Fuzzy pooling is performed by
fuzzification, aggregation and defuzzification of feature map neighborhoods. It
is used for the construction of a fuzzy pooling layer that can be applied as a
drop-in replacement of the current, crisp, pooling layers of CNN architectures.
Several experiments using publicly available datasets show that the proposed
approach can enhance the classification performance of a CNN. A comparative
evaluation shows that it outperforms state-of-the-art pooling approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Fill the Optimum Set? Population Gradient Descent with Harmless Diversity. (arXiv:2202.08376v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08376">
<div class="article-summary-box-inner">
<span><p>Although traditional optimization methods focus on finding a single optimal
solution, most objective functions in modern machine learning problems,
especially those in deep learning, often have multiple or infinite numbers of
optima. Therefore, it is useful to consider the problem of finding a set of
diverse points in the optimum set of an objective function. In this work, we
frame this problem as a bi-level optimization problem of maximizing a diversity
score inside the optimum set of the main loss function, and solve it with a
simple population gradient descent framework that iteratively updates the
points to maximize the diversity score in a fashion that does not hurt the
optimization of the main loss. We demonstrate that our method can efficiently
generate diverse solutions on a variety of applications, including
text-to-image generation, text-to-mesh generation, molecular conformation
generation and ensemble neural network training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Limitations of Neural Collapse for Understanding Generalization in Deep Learning. (arXiv:2202.08384v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08384">
<div class="article-summary-box-inner">
<span><p>The recent work of Papyan, Han, &amp; Donoho (2020) presented an intriguing
"Neural Collapse" phenomenon, showing a structural property of interpolating
classifiers in the late stage of training. This opened a rich area of
exploration studying this phenomenon. Our motivation is to study the upper
limits of this research program: How far will understanding Neural Collapse
take us in understanding deep learning? First, we investigate its role in
generalization. We refine the Neural Collapse conjecture into two separate
conjectures: collapse on the train set (an optimization property) and collapse
on the test distribution (a generalization property). We find that while Neural
Collapse often occurs on the train set, it does not occur on the test set. We
thus conclude that Neural Collapse is primarily an optimization phenomenon,
with as-yet-unclear connections to generalization. Second, we investigate the
role of Neural Collapse in feature learning. We show simple, realistic
experiments where training longer leads to worse last-layer features, as
measured by transfer-performance on a downstream task. This suggests that
neural collapse is not always desirable for representation learning, as
previously claimed. Finally, we give preliminary evidence of a "cascading
collapse" phenomenon, wherein some form of Neural Collapse occurs not only for
the last layer, but in earlier layers as well. We hope our work encourages the
community to continue the rich line of Neural Collapse research, while also
considering its inherent limitations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shift-Memory Network for Temporal Scene Segmentation. (arXiv:2202.08399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08399">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation has achieved great accuracy in understanding spatial
layout. For real-time tasks based on dynamic scenes, we extend semantic
segmentation in temporal domain to enhance the spatial accuracy with motion. We
utilize a shift-mode network over streaming input to ensure zero-latency
output. For the data overlap under shifting network, this paper identifies
repeated computation in fixed periods across network layers. To avoid this
redundancy, we derive a Shift-Memory Network (SMN) from encoding-decoding
baseline to reuse the network values without accuracy loss. Trained in
patch-mode, the SMN extracts the network parameters for SMN to perform
inference promptly in compact memory. We segment dynamic scenes from 1D
scanning input and 2D video. The experiments of SMN achieve equivalent accuracy
as shift-mode but in faster inference speeds and much smaller memory. This will
facilitate semantic segmentation in real-time application on edge devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FPIC: A Novel Semantic Dataset for Optical PCB Assurance. (arXiv:2202.08414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08414">
<div class="article-summary-box-inner">
<span><p>The continued outsourcing of printed circuit board (PCB) fabrication to
overseas venues necessitates increased hardware assurance capabilities. Toward
this end, several automated optical inspection (AOI) techniques have been
proposed in the past exploring various aspects of PCB images acquired using
digital cameras. In this work, we review state-of-the-art AOI techniques and
observed the strong, rapid trend toward machine learning (ML) solutions. These
require significant amounts of labeled ground truth data, which is lacking in
the publicly available PCB data space. We propose the FICS PBC Image Collection
(FPIC) dataset to address this bottleneck in available large-volume, diverse,
semantic annotations. Additionally, this work covers the potential increase in
hardware security capabilities and observed methodological distinctions
highlighted during data collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Marionette: Unsupervised Learning of Motion Skeleton and Latent Dynamics from Volumetric Video. (arXiv:2202.08418v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08418">
<div class="article-summary-box-inner">
<span><p>We present Neural Marionette, an unsupervised approach that discovers the
skeletal structure from a dynamic sequence and learns to generate diverse
motions that are consistent with the observed motion dynamics. Given a video
stream of point cloud observation of an articulated body under arbitrary
motion, our approach discovers the unknown low-dimensional skeletal
relationship that can effectively represent the movement. Then the discovered
structure is utilized to encode the motion priors of dynamic sequences in a
latent structure, which can be decoded to the relative joint rotations to
represent the full skeletal motion. Our approach works without any prior
knowledge of the underlying motion or skeletal structure, and we demonstrate
that the discovered structure is even comparable to the hand-labeled ground
truth skeleton in representing a 4D sequence of motion. The skeletal structure
embeds the general semantics of possible motion space that can generate motions
for diverse scenarios. We verify that the learned motion prior is generalizable
to the multi-modal sequence generation, interpolation of two poses, and motion
retargeting to a different skeletal structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AKB-48: A Real-World Articulated Object Knowledge Base. (arXiv:2202.08432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08432">
<div class="article-summary-box-inner">
<span><p>Human life is populated with articulated objects. A comprehensive
understanding of articulated objects, namely appearance, structure, physics
property, and semantics, will benefit many research communities. As current
articulated object understanding solutions are usually based on synthetic
object dataset with CAD models without physics properties, which prevent
satisfied generalization from simulation to real-world applications in visual
and robotics tasks. To bridge the gap, we present AKB-48: a large-scale
Articulated object Knowledge Base which consists of 2,037 real-world 3D
articulated object models of 48 categories. Each object is described by a
knowledge graph ArtiKG. To build the AKB-48, we present a fast articulation
knowledge modeling (FArM) pipeline, which can fulfill the ArtiKG for an
articulated object within 10-15 minutes, and largely reduce the cost for object
modeling in the real world. Using our dataset, we propose AKBNet, a novel
integral pipeline for Category-level Visual Articulation Manipulation (C-VAM)
task, in which we benchmark three sub-tasks, namely pose estimation, object
reconstruction and manipulation. Dataset, codes, and models will be publicly
available at https://liuliu66.github.io/articulationobjects/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PENCIL: Deep Learning with Noisy Labels. (arXiv:2202.08436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08436">
<div class="article-summary-box-inner">
<span><p>Deep learning has achieved excellent performance in various computer vision
tasks, but requires a lot of training examples with clean labels. It is easy to
collect a dataset with noisy labels, but such noise makes networks overfit
seriously and accuracies drop dramatically. To address this problem, we propose
an end-to-end framework called PENCIL, which can update both network parameters
and label estimations as label distributions. PENCIL is independent of the
backbone network structure and does not need an auxiliary clean dataset or
prior information about noise, thus it is more general and robust than existing
methods and is easy to apply. PENCIL can even be used repeatedly to obtain
better performance. PENCIL outperforms previous state-of-the-art methods by
large margins on both synthetic and real-world datasets with different noise
types and noise rates. And PENCIL is also effective in multi-label
classification tasks through adding a simple attention structure on backbone
networks. Experiments show that PENCIL is robust on clean datasets, too.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual attention analysis of pathologists examining whole slide images of Prostate cancer. (arXiv:2202.08437v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08437">
<div class="article-summary-box-inner">
<span><p>We study the attention of pathologists as they examine whole-slide images
(WSIs) of prostate cancer tissue using a digital microscope. To the best of our
knowledge, our study is the first to report in detail how pathologists navigate
WSIs of prostate cancer as they accumulate information for their diagnoses. We
collected slide navigation data (i.e., viewport location, magnification level,
and time) from 13 pathologists in 2 groups (5 genitourinary (GU) specialists
and 8 general pathologists) and generated visual attention heatmaps and
scanpaths. Each pathologist examined five WSIs from the TCGA PRAD dataset,
which were selected by a GU pathology specialist. We examined and analyzed the
distributions of visual attention for each group of pathologists after each WSI
was examined. To quantify the relationship between a pathologist's attention
and evidence for cancer in the WSI, we obtained tumor annotations from a
genitourinary specialist. We used these annotations to compute the overlap
between the distribution of visual attention and annotated tumor region to
identify strong correlations. Motivated by this analysis, we trained a deep
learning model to predict visual attention on unseen WSIs. We find that the
attention heatmaps predicted by our model correlate quite well with the ground
truth attention heatmap and tumor annotations on a test set of 17 WSIs by using
various spatial and temporal evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V2X-Sim: A Virtual Collaborative Perception Dataset for Autonomous Driving. (arXiv:2202.08449v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08449">
<div class="article-summary-box-inner">
<span><p>Vehicle-to-everything (V2X), which denotes the collaboration between a
vehicle and any entity in its surrounding, can fundamentally improve the
perception in self-driving systems. As the individual perception rapidly
advances, collaborative perception has made little progress due to the shortage
of public V2X datasets. In this work, we present the V2X-Sim dataset, the first
public large-scale collaborative perception dataset in autonomous driving.
V2X-Sim provides: 1) well-synchronized recordings from roadside infrastructure
and multiple vehicles at the intersection to enable collaborative perception,
2) multi-modality sensor streams to facilitate multi-modality perception, 3)
diverse well-annotated ground truth to support various downstream tasks
including detection, tracking, and segmentation. We seek to inspire research on
multi-agent multi-modality multi-task perception, and our virtual dataset is
promising to promote the development of collaborative perception before
realistic datasets become widely available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCB Component Detection using Computer Vision for Hardware Assurance. (arXiv:2202.08452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08452">
<div class="article-summary-box-inner">
<span><p>Printed Circuit Board (PCB) assurance in the optical domain is a crucial
field of study. Though there are many existing PCB assurance methods using
image processing, computer vision (CV), and machine learning (ML), the PCB
field is complex and increasingly evolving so new techniques are required to
overcome the emerging problems. Existing ML-based methods outperform
traditional CV methods, however they often require more data, have low
explainability, and can be difficult to adapt when a new technology arises. To
overcome these challenges, CV methods can be used in tandem with ML methods. In
particular, human-interpretable CV algorithms such as those that extract color,
shape, and texture features increase PCB assurance explainability. This allows
for incorporation of prior knowledge, which effectively reduce the number of
trainable ML parameters and thus, the amount of data needed to achieve high
accuracy when training or retraining an ML model. Hence, this study explores
the benefits and limitations of a variety of common computer vision-based
features for the task of PCB component detection using semantic data. Results
of this study indicate that color features demonstrate promising performance
for PCB component detection. The purpose of this paper is to facilitate
collaboration between the hardware assurance, computer vision, and machine
learning communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery. (arXiv:2202.08453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08453">
<div class="article-summary-box-inner">
<span><p>Surgical instrument segmentation -- in general a pixel classification task --
is fundamentally crucial for promoting cognitive intelligence in robot-assisted
surgery (RAS). However, previous methods are struggling with discriminating
instrument types and instances. To address the above issues, we explore a mask
classification paradigm that produces per-segment predictions. We propose
TraSeTR, a novel Track-to-Segment Transformer that wisely exploits tracking
cues to assist surgical instrument segmentation. TraSeTR jointly reasons about
the instrument type, location, and identity with instance-level predictions
i.e., a set of class-bbox-mask pairs, by decoding query embeddings.
Specifically, we introduce the prior query that encoded with previous temporal
knowledge, to transfer tracking signals to current instances via identity
matching. A contrastive query learning strategy is further applied to reshape
the query feature space, which greatly alleviates the tracking difficulty
caused by large temporal variations. The effectiveness of our method is
demonstrated with state-of-the-art instrument type segmentation results on
three public datasets, including two RAS benchmarks from EndoVis Challenges and
one cataract surgery dataset CaDIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and Grasping. (arXiv:2202.08471v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08471">
<div class="article-summary-box-inner">
<span><p>Transparent objects are common in our daily life and frequently handled in
the automated production line. Robust vision-based robotic grasping and
manipulation for these objects would be beneficial for automation. However, the
majority of current grasping algorithms would fail in this case since they
heavily rely on the depth image, while ordinary depth sensors usually fail to
produce accurate depth information for transparent objects owing to the
reflection and refraction of light. In this work, we address this issue by
contributing a large-scale real-world dataset for transparent object depth
completion, which contains 57,715 RGB-D images from 130 different scenes. Our
dataset is the first large-scale real-world dataset and provides the most
comprehensive annotation. Cross-domain experiments show that our dataset has a
great generalization ability. Moreover, we propose an end-to-end depth
completion network, which takes the RGB image and the inaccurate depth map as
inputs and outputs a refined depth map. Experiments demonstrate superior
efficacy, efficiency and robustness of our method over previous works, and it
is able to process images of high resolutions under limited hardware resources.
Real robot experiment shows that our method can also be applied to novel object
grasping robustly. The full dataset and our method are publicly available at
www.graspnet.net/transcg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Object Comprehension: A Framework For Evaluating Artificial Visual Perception. (arXiv:2202.08490v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08490">
<div class="article-summary-box-inner">
<span><p>Augmented and Mixed Reality are emerging as likely successors to the mobile
internet. However, many technical challenges remain. One of the key
requirements of these systems is the ability to create a continuity between
physical and virtual worlds, with the user's visual perception as the primary
interface medium. Building this continuity requires the system to develop a
visual understanding of the physical world. While there has been significant
recent progress in computer vision and AI techniques such as image
classification and object detection, success in these areas has not yet led to
the visual perception required for these critical MR and AR applications. A
significant issue is that current evaluation criteria are insufficient for
these applications. To motivate and evaluate progress in this emerging area,
there is a need for new metrics. In this paper we outline limitations of
current evaluation criteria and propose new criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feels Bad Man: Dissecting Automated Hateful Meme Detection Through the Lens of Facebook's Challenge. (arXiv:2202.08492v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08492">
<div class="article-summary-box-inner">
<span><p>Internet memes have become a dominant method of communication; at the same
time, however, they are also increasingly being used to advocate extremism and
foster derogatory beliefs. Nonetheless, we do not have a firm understanding as
to which perceptual aspects of memes cause this phenomenon. In this work, we
assess the efficacy of current state-of-the-art multimodal machine learning
models toward hateful meme detection, and in particular with respect to their
generalizability across platforms. We use two benchmark datasets comprising
12,140 and 10,567 images from 4chan's "Politically Incorrect" board (/pol/) and
Facebook's Hateful Memes Challenge dataset to train the competition's
top-ranking machine learning models for the discovery of the most prominent
features that distinguish viral hateful memes from benign ones. We conduct
three experiments to determine the importance of multimodality on
classification performance, the influential capacity of fringe Web communities
on mainstream social platforms and vice versa, and the models' learning
transferability on 4chan memes. Our experiments show that memes' image
characteristics provide a greater wealth of information than its textual
content. We also find that current systems developed for online detection of
hate speech in memes necessitate further concentration on its visual elements
to improve their interpretation of underlying cultural connotations, implying
that multimodal models fail to adequately grasp the intricacies of hate speech
in memes and generalize across social media platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mirror-Yolo: An attention-based instance segmentation and detection model for mirrors. (arXiv:2202.08498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08498">
<div class="article-summary-box-inner">
<span><p>Mirrors can degrade the performance of computer vision models, however to
accurately detect mirrors in images remains challenging. YOLOv4 achieves
phenomenal results both in object detection accuracy and speed, nevertheless
the model often fails in detecting mirrors. In this paper, a novel mirror
detection method `Mirror-YOLO' is proposed, which mainly targets on mirror
detection. Based on YOLOv4, the proposed model embeds an attention mechanism
for better feature acquisition, and a hypercolumn-stairstep approach for
feature map fusion. Mirror-YOLO can also produce accurate bounding polygons for
instance segmentation. The effectiveness of our proposed model is demonstrated
by our experiments, compared to the existing mirror detection methods, the
proposed Mirror-YOLO achieves better performance in detection accuracy on the
mirror image dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLS: Cross Labeling Supervision for Semi-Supervised Learning. (arXiv:2202.08502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08502">
<div class="article-summary-box-inner">
<span><p>It is well known that the success of deep neural networks is greatly
attributed to large-scale labeled datasets. However, it can be extremely
time-consuming and laborious to collect sufficient high-quality labeled data in
most practical applications. Semi-supervised learning (SSL) provides an
effective solution to reduce the cost of labeling by simultaneously leveraging
both labeled and unlabeled data. In this work, we present Cross Labeling
Supervision (CLS), a framework that generalizes the typical pseudo-labeling
process. Based on FixMatch, where a pseudo label is generated from a
weakly-augmented sample to teach the prediction on a strong augmentation of the
same input sample, CLS allows the creation of both pseudo and complementary
labels to support both positive and negative learning. To mitigate the
confirmation bias of self-labeling and boost the tolerance to false labels, two
different initialized networks with the same structure are trained
simultaneously. Each network utilizes high-confidence labels from the other
network as additional supervision signals. During the label generation phase,
adaptive sample weights are assigned to artificial labels according to their
prediction confidence. The sample weight plays two roles: quantify the
generated labels' quality and reduce the disruption of inaccurate labels on
network training. Experimental results on the semi-supervised classification
task show that our framework outperforms existing approaches by large margins
on the CIFAR-10 and CIFAR-100 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSCNet: Contextual Semantic Consistency Network for Trajectory Prediction in Crowded Spaces. (arXiv:2202.08506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08506">
<div class="article-summary-box-inner">
<span><p>Trajectory prediction aims to predict the movement trend of the agents like
pedestrians, bikers, vehicles. It is helpful to analyze and understand human
activities in crowded spaces and widely applied in many areas such as
surveillance video analysis and autonomous driving systems. Thanks to the
success of deep learning, trajectory prediction has made significant progress.
The current methods are dedicated to studying the agents' future trajectories
under the social interaction and the sceneries' physical constraints. Moreover,
how to deal with these factors still catches researchers' attention. However,
they ignore the \textbf{Semantic Shift Phenomenon} when modeling these
interactions in various prediction sceneries. There exist several kinds of
semantic deviations inner or between social and physical interactions, which we
call the "\textbf{Gap}". In this paper, we propose a \textbf{C}ontextual
\textbf{S}emantic \textbf{C}onsistency \textbf{Net}work (\textbf{CSCNet}) to
predict agents' future activities with powerful and efficient context
constraints. We utilize a well-designed context-aware transfer to obtain the
intermediate representations from the scene images and trajectories. Then we
eliminate the differences between social and physical interactions by aligning
activity semantics and scene semantics to cross the Gap. Experiments
demonstrate that CSCNet performs better than most of the current methods
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Designing Compact Audio-Visual Wake Word Spotting System Based on Iterative Fine-Tuning in Neural Network Pruning. (arXiv:2202.08509v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08509">
<div class="article-summary-box-inner">
<span><p>Audio-only-based wake word spotting (WWS) is challenging under noisy
conditions due to environmental interference in signal transmission. In this
paper, we investigate on designing a compact audio-visual WWS system by
utilizing visual information to alleviate the degradation. Specifically, in
order to use visual information, we first encode the detected lips to
fixed-size vectors with MobileNet and concatenate them with acoustic features
followed by the fusion network for WWS. However, the audio-visual model based
on neural networks requires a large footprint and a high computational
complexity. To meet the application requirements, we introduce a neural network
pruning strategy via the lottery ticket hypothesis in an iterative fine-tuning
manner (LTH-IF), to the single-modal and multi-modal models, respectively.
Tested on our in-house corpus for audio-visual WWS in a home TV scene, the
proposed audio-visual system achieves significant performance improvements over
the single-modality (audio-only or video-only) system under different noisy
conditions. Moreover, LTH-IF pruning can largely reduce the network parameters
and computations with no degradation of WWS performance, leading to a potential
product solution for the TV wake-up scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A hybrid 2-stage vision transformer for AI-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies. (arXiv:2202.08510v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08510">
<div class="article-summary-box-inner">
<span><p>Gastric endoscopic screening is an effective way to decide appropriate
gastric cancer (GC) treatment at an early stage, reducing GC-associated
mortality rate. Although artificial intelligence (AI) has brought a great
promise to assist pathologist to screen digitalized whole slide images,
automatic classification systems for guiding proper GC treatment based on
clinical guideline are still lacking. Here, we propose an AI system classifying
5 classes of GC histology, which can be perfectly matched to general treatment
guidance. The AI system, mimicking the way pathologist understand slides
through multi-scale self-attention mechanism using a 2-stage Vision
Transformer, demonstrates clinical capability by achieving diagnostic
sensitivity of above 85% for both internal and external cohort analysis.
Furthermore, AI-assisted pathologists showed significantly improved diagnostic
sensitivity by 10% within 18% saved screening time compared to human
pathologists. Our AI system has a great potential for providing presumptive
pathologic opinion for deciding proper treatment for early GC patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Ground Truth Construction as Faceted Classification. (arXiv:2202.08512v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08512">
<div class="article-summary-box-inner">
<span><p>Recent work in Machine Learning and Computer Vision has provided evidence of
systematic design flaws in the development of major object recognition
benchmark datasets. One such example is ImageNet, wherein, for several
categories of images, there are incongruences between the objects they
represent and the labels used to annotate them. The consequences of this
problem are major, in particular considering the large number of machine
learning applications, not least those based on Deep Neural Networks, that have
been trained on these datasets. In this paper we posit the problem to be the
lack of a knowledge representation (KR) methodology providing the foundations
for the construction of these ground truth benchmark datasets. Accordingly, we
propose a solution articulated in three main steps: (i) deconstructing the
object recognition process in four ordered stages grounded in the philosophical
theory of teleosemantics; (ii) based on such stratification, proposing a novel
four-phased methodology for organizing objects in classification hierarchies
according to their visual properties; and (iii) performing such classification
according to the faceted classification paradigm. The key novelty of our
approach lies in the fact that we construct the classification hierarchies from
visual properties exploiting visual genus-differentiae, and not from
linguistically grounded properties. The proposed approach is validated by a set
of experiments on the ImageNet hierarchy of musical experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Self-supervised Representation Learning Using Image Transformations. (arXiv:2202.08514v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08514">
<div class="article-summary-box-inner">
<span><p>Deep neural networks need huge amount of training data, while in real world
there is a scarcity of data available for training purposes. To resolve these
issues, self-supervised learning (SSL) methods are used. SSL using geometric
transformations (GT) is a simple yet powerful technique used in unsupervised
representation learning. Although multiple survey papers have reviewed SSL
techniques, there is none that only focuses on those that use geometric
transformations. Furthermore, such methods have not been covered in depth in
papers where they are reviewed. Our motivation to present this work is that
geometric transformations have shown to be powerful supervisory signals in
unsupervised representation learning. Moreover, many such works have found
tremendous success, but have not gained much attention. We present a concise
survey of SSL approaches that use geometric transformations. We shortlist six
representative models that use image transformations including those based on
predicting and autoencoding transformations. We review their architecture as
well as learning methodologies. We also compare the performance of these models
in the object recognition task on CIFAR-10 and ImageNet datasets. Our analysis
indicates the AETv2 performs the best in most settings. Rotation with feature
decoupling also performed well in some settings. We then derive insights from
the observed results. Finally, we conclude with a summary of the results and
insights as well as highlighting open problems to be addressed and indicating
various future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAFNet: A Three-Stream Adaptive Fusion Network for RGB-T Crowd Counting. (arXiv:2202.08517v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08517">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a three-stream adaptive fusion network named
TAFNet, which uses paired RGB and thermal images for crowd counting.
Specifically, TAFNet is divided into one main stream and two auxiliary streams.
We combine a pair of RGB and thermal images to constitute the input of main
stream. Two auxiliary streams respectively exploit RGB image and thermal image
to extract modality-specific features. Besides, we propose an Information
Improvement Module (IIM) to fuse the modality-specific features into the main
stream adaptively. Experiment results on RGBT-CC dataset show that our method
achieves more than 20% improvement on mean average error and root mean squared
error compared with state-of-the-art method. The source code will be publicly
available at https://github.com/TANGHAIHAN/TAFNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Cloud Generation with Continuous Conditioning. (arXiv:2202.08526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08526">
<div class="article-summary-box-inner">
<span><p>Generative models can be used to synthesize 3D objects of high quality and
diversity. However, there is typically no control over the properties of the
generated object.This paper proposes a novel generative adversarial network
(GAN) setup that generates 3D point cloud shapes conditioned on a continuous
parameter. In an exemplary application, we use this to guide the generative
process to create a 3D object with a custom-fit shape. We formulate this
generation process in a multi-task setting by using the concept of auxiliary
classifier GANs. Further, we propose to sample the generator label input for
training from a kernel density estimation (KDE) of the dataset. Our ablations
show that this leads to significant performance increase in regions with few
samples. Extensive quantitative and qualitative experiments show that we gain
explicit control over the object dimensions while maintaining good generation
quality and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation for Underwater Image Enhancement via Content and Style Separation. (arXiv:2202.08537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08537">
<div class="article-summary-box-inner">
<span><p>Underwater image suffer from color cast, low contrast and hazy effect due to
light absorption, refraction and scattering, which degraded the high-level
application, e.g, object detection and object tracking. Recent learning-based
methods demonstrate astonishing performance on underwater image enhancement,
however, most of these works use synthesis pair data for supervised learning
and ignore the domain gap to real-world data. In this paper, we propose a
domain adaptation framework for underwater image enhancement via content and
style separation, we assume image could be disentangled to content and style
latent, and image could be clustered to the sub-domain of associated style in
latent space, the goal is to build up the mapping between underwater style
latent and clean one. Different from prior works of domain adaptation for
underwater image enhancement, which target to minimize the latent discrepancy
of synthesis and real-world data, we aim to distinguish style latent from
different sub-domains. To solve the problem of lacking pair real-world data, we
leverage synthesis to real image-to-image translation to obtain pseudo real
underwater image pairs for supervised learning, and enhancement can be achieved
by input content and clean style latent into generator. Our model provide a
user interact interface to adjust different enhanced level by latent
manipulation. Experiment on various public real-world underwater benchmarks
demonstrate that the proposed framework is capable to perform domain adaptation
for underwater image enhancement and outperform various state-of-the-art
underwater image enhancement algorithms in quantity and quality. The model and
source code are available at https://github.com/fordevoted/UIESS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An overview of deep learning in medical imaging. (arXiv:2202.08546v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08546">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) has seen enormous consideration during the most recent
decade. This success started in 2012 when an ML model accomplished a remarkable
triumph in the ImageNet Classification, the world's most famous competition for
computer vision. This model was a kind of convolutional neural system (CNN)
called deep learning (DL). Since then, researchers have started to participate
efficiently in DL's fastest developing area of research. These days, DL systems
are cutting-edge ML systems spanning a broad range of disciplines, from human
language processing to video analysis, and commonly used in the scholarly world
and enterprise sector. Recent advances can bring tremendous improvement to the
medical field. Improved and innovative methods for data processing, image
analysis and can significantly improve the diagnostic technologies and
medicinal services gradually. A quick review of current developments with
relevant problems in the field of DL used for medical imaging has been
provided. The primary purposes of the review are four: (i) provide a brief
prolog to DL by discussing different DL models, (ii) review of the DL usage for
medical image analysis (classification, detection, segmentation, and
registration), (iii) review seven main application fields of DL in medical
imaging, (iv) give an initial stage to those keen on adding to the research
area about DL in clinical imaging by providing links of some useful informative
assets, such as freely available DL codes, public datasets Table 7, and medical
imaging competition sources Table 8 and end our survey by outlining distinct
continuous difficulties, lessons learned and future of DL in the field of
medical science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EBHI:A New Enteroscope Biopsy Histopathological H&E Image Dataset for Image Classification Evaluation. (arXiv:2202.08552v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08552">
<div class="article-summary-box-inner">
<span><p>Background and purpose: Colorectal cancer has become the third most common
cancer worldwide, accounting for approximately 10% of cancer patients. Early
detection of the disease is important for the treatment of colorectal cancer
patients. Histopathological examination is the gold standard for screening
colorectal cancer. However, the current lack of histopathological image
datasets of colorectal cancer, especially enteroscope biopsies, hinders the
accurate evaluation of computer-aided diagnosis techniques. Methods: A new
publicly available Enteroscope Biopsy Histopathological H&amp;E Image Dataset
(EBHI) is published in this paper. To demonstrate the effectiveness of the EBHI
dataset, we have utilized several machine learning, convolutional neural
networks and novel transformer-based classifiers for experimentation and
evaluation, using an image with a magnification of 200x. Results: Experimental
results show that the deep learning method performs well on the EBHI dataset.
Traditional machine learning methods achieve maximum accuracy of 76.02% and
deep learning method achieves a maximum accuracy of 95.37%. Conclusion: To the
best of our knowledge, EBHI is the first publicly available colorectal
histopathology enteroscope biopsy dataset with four magnifications and five
types of images of tumor differentiation stages, totaling 5532 images. We
believe that EBHI could attract researchers to explore new classification
algorithms for the automated diagnosis of colorectal cancer, which could help
physicians and patients in clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-Aware Indoor Scene Synthesis with Depth Priors. (arXiv:2202.08553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08553">
<div class="article-summary-box-inner">
<span><p>Despite the recent advancement of Generative Adversarial Networks (GANs) in
learning 3D-aware image synthesis from 2D data, existing methods fail to model
indoor scenes due to the large diversity of room layouts and the objects
inside. We argue that indoor scenes do not have a shared intrinsic structure,
and hence only using 2D images cannot adequately guide the model with the 3D
geometry. In this work, we fill in this gap by introducing depth as a 3D prior.
Compared with other 3D data formats, depth better fits the convolution-based
generation mechanism and is more easily accessible in practice. Specifically,
we propose a dual-path generator, where one path is responsible for depth
generation, whose intermediate features are injected into the other path as the
condition for appearance rendering. Such a design eases the 3D-aware synthesis
with explicit geometry information. Meanwhile, we introduce a switchable
discriminator both to differentiate real v.s. fake domains and to predict the
depth from a given input. In this way, the discriminator can take the spatial
arrangement into account and advise the generator to learn an appropriate depth
condition. Extensive experimental results suggest that our approach is capable
of synthesizing indoor scenes with impressively good quality and 3D
consistency, significantly outperforming state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-based Autonomous Urban Driving. (arXiv:2202.08557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08557">
<div class="article-summary-box-inner">
<span><p>Vision-based autonomous urban driving in dense traffic is quite challenging
due to the complicated urban environment and the dynamics of the driving
behaviors. Widely-applied methods either heavily rely on hand-crafted rules or
learn from limited human experience, which makes them hard to generalize to
rare but critical scenarios. In this paper, we present a novel CAscade Deep
REinforcement learning framework, CADRE, to achieve model-free vision-based
autonomous urban driving. In CADRE, to derive representative latent features
from raw observations, we first offline train a Co-attention Perception Module
(CoPM) that leverages the co-attention mechanism to learn the
inter-relationships between the visual and control information from a
pre-collected driving dataset. Cascaded by the frozen CoPM, we then present an
efficient distributed proximal policy optimization framework to online learn
the driving policy under the guidance of particularly designed reward
functions. We perform a comprehensive empirical study with the CARLA NoCrash
benchmark as well as specific obstacle avoidance scenarios in autonomous urban
driving tasks. The experimental results well justify the effectiveness of CADRE
and its superiority over the state-of-the-art by a wide margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anatomically Parameterized Statistical Shape Model: Explaining Morphometry through Statistical Learning. (arXiv:2202.08580v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08580">
<div class="article-summary-box-inner">
<span><p>Statistical shape models (SSMs) are a popular tool to conduct morphological
analysis of anatomical structures which is a crucial step in clinical
practices. However, shape representations through SSMs are based on shape
coefficients and lack an explicit one-to-one relationship with anatomical
measures of clinical relevance. While a shape coefficient embeds a combination
of anatomical measures, a formalized approach to find the relationship between
them remains elusive in the literature. This limits the use of SSMs to
subjective evaluations in clinical practices. We propose a novel SSM controlled
by anatomical parameters derived from morphometric analysis. The proposed
anatomically parameterized SSM (ANAT-SSM) is based on learning a linear mapping
between shape coefficients and selected anatomical parameters. This mapping is
learned from a synthetic population generated by the standard SSM. Determining
the pseudo-inverse of the mapping allows us to build the ANAT-SSM. We further
impose orthogonality constraints to the anatomical parameterization to obtain
independent shape variation patterns. The proposed contribution was evaluated
on two skeletal databases of femoral and scapular bone shapes using clinically
relevant anatomical parameters. Anatomical measures of the synthetically
generated shapes exhibited realistic statistics. The learned matrices
corroborated well with the obtained statistical relationship, while the two
SSMs achieved moderate to excellent performance in predicting anatomical
parameters on unseen shapes. This study demonstrates the use of anatomical
representation for creating anatomically parameterized SSM and as a result,
removes the limited clinical interpretability of standard SSMs. The proposed
models could help analyze differences in relevant bone morphometry between
populations, and be integrated in patient-specific pre-surgery planning or
in-surgery assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point cloud completion on structured feature map with feedback network. (arXiv:2202.08583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08583">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the challenging problem of point cloud completion
from the perspective of feature learning. Our key observation is that to
recover the underlying structures as well as surface details given a partial
input, a fundamental component is a good feature representation that can
capture both global structure and local geometric details. Towards this end, we
first propose FSNet, a feature structuring module that can adaptively aggregate
point-wise features into a 2D structured feature map by learning multiple
latent patterns from local regions. We then integrate FSNet into a
coarse-to-fine pipeline for point cloud completion. Specifically, a 2D
convolutional neural network is adopted to decode feature maps from FSNet into
a coarse and complete point cloud. Next, a point cloud upsampling network is
used to generate dense point cloud from the partial input and the coarse
intermediate output. To efficiently exploit the local structures and enhance
the point distribution uniformity, we propose IFNet, a point upsampling module
with self-correction mechanism that can progressively refine details of the
generated dense point cloud. We conduct both qualitative and quantitative
experiments on ShapeNet, MVP, and KITTI datasets, which demonstrate that our
method outperforms state-of-the-art point cloud completion approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single UHD Image Dehazing via Interpretable Pyramid Network. (arXiv:2202.08589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08589">
<div class="article-summary-box-inner">
<span><p>Currently, most single image dehazing models cannot run an
ultra-high-resolution (UHD) image with a single GPU shader in real-time. To
address the problem, we introduce the principle of infinite approximation of
Taylor's theorem with the Laplace pyramid pattern to build a model which is
capable of handling 4K hazy images in real-time. The N branch networks of the
pyramid network correspond to the N constraint terms in Taylor's theorem.
Low-order polynomials reconstruct the low-frequency information of the image
(e.g. color, illumination). High-order polynomials regress the high-frequency
information of the image (e.g. texture). In addition, we propose a Tucker
reconstruction-based regularization term that acts on each branch network of
the pyramid model. It further constrains the generation of anomalous signals in
the feature space. Extensive experimental results demonstrate that our approach
can not only run 4K images with haze in real-time on a single GPU (80FPS) but
also has unparalleled interpretability.
</p>
<p>The developed method achieves state-of-the-art (SOTA) performance on two
benchmarks (O/I-HAZE) and our updated 4KID dataset while providing the reliable
groundwork for subsequent optimization schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Architectural Fine-Tuning with Neural Architecture Search using Early-Stopping in Image Classification. (arXiv:2202.08604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08604">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (NN) perform well in various tasks (e.g., computer
vision) because of the convolutional neural networks (CNN). However, the
difficulty of gathering quality data in the industry field hinders the
practical use of NN. To cope with this issue, the concept of transfer learning
(TL) has emerged, which leverages the fine-tuning of NNs trained on large-scale
datasets in data-scarce situations. Therefore, this paper suggests a two-stage
architectural fine-tuning method for image classification, inspired by the
concept of neural architecture search (NAS). One of the main ideas of our
proposed method is a mutation with base architectures, which reduces the search
cost by using given architectural information. Moreover, an early-stopping is
also considered which directly reduces NAS costs. Experimental results verify
that our proposed method reduces computational and searching costs by up to
28.2% and 22.3%, compared to existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time. (arXiv:2202.08614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08614">
<div class="article-summary-box-inner">
<span><p>Implicit neural representations such as Neural Radiance Field (NeRF) have
focused mainly on modeling static objects captured under multi-view settings
where real-time rendering can be achieved with smart data structures, e.g.,
PlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO)
technique to tackle efficient neural modeling and real-time rendering of
dynamic scenes captured under the free-view video (FVV) setting. The key idea
in our FPO is a novel combination of generalized NeRF, PlenOctree
representation, volumetric fusion and Fourier transform. To accelerate FPO
construction, we present a novel coarse-to-fine fusion scheme that leverages
the generalizable NeRF technique to generate the tree via spatial blending. To
tackle dynamic scenes, we tailor the implicit network to model the Fourier
coefficients of timevarying density and color attributes. Finally, we construct
the FPO and train the Fourier coefficients directly on the leaves of a union
PlenOctree structure of the dynamic sequence. We show that the resulting FPO
enables compact memory overload to handle dynamic objects and supports
efficient fine-tuning. Extensive experiments show that the proposed method is
3000 times faster than the original NeRF and achieves over an order of
magnitude acceleration over SOTA while preserving high visual quality for the
free-viewpoint rendering of unseen dynamic scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Proportional Patchmix for Few-Shot Learning. (arXiv:2202.08647v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08647">
<div class="article-summary-box-inner">
<span><p>Few-shot learning aims to classify unseen classes with only a limited number
of labeled data. Recent works have demonstrated that training models with a
simple transfer learning strategy can achieve competitive results in few-shot
classification. Although excelling at distinguishing training data, these
models are not well generalized to unseen data, probably due to insufficient
feature representations on evaluation. To tackle this issue, we propose
Semantically Proportional Patchmix (SePPMix), in which patches are cut and
pasted among training images and the ground truth labels are mixed
proportionally to the semantic information of the patches. In this way, we can
improve the generalization ability of the model by regional dropout effect
without introducing severe label noise. To learn more robust representations of
data, we further take rotate transformation on the mixed images and predict
rotations as a rule-based regularizer. Extensive experiments on prevalent
few-shot benchmarks have shown the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Randomization for Object Counting. (arXiv:2202.08670v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08670">
<div class="article-summary-box-inner">
<span><p>Recently, the use of synthetic datasets based on game engines has been shown
to improve the performance of several tasks in computer vision. However, these
datasets are typically only appropriate for the specific domains depicted in
computer games, such as urban scenes involving vehicles and people. In this
paper, we present an approach to generate synthetic datasets for object
counting for any domain without the need for photo-realistic techniques
manually generated by expensive teams of 3D artists. We introduce a domain
randomization approach for object counting based on synthetic datasets that are
quick and inexpensive to generate. We deliberately avoid photorealism and
drastically increase the variability of the dataset, producing images with
random textures and 3D transformations, which improves generalization.
Experiments show that our method facilitates good performance on various real
word object counting datasets for multiple domains: people, vehicles, penguins,
and fruit. The source code is available at: https://github.com/enric1994/dr4oc
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic data for unsupervised polyp segmentation. (arXiv:2202.08680v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08680">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown excellent performance in analysing medical images.
However, datasets are difficult to obtain due privacy issues, standardization
problems, and lack of annotations. We address these problems by producing
realistic synthetic images using a combination of 3D technologies and
generative adversarial networks. We use zero annotations from medical
professionals in our pipeline. Our fully unsupervised method achieves promising
results on five real polyp segmentation datasets. As a part of this study we
release Synth-Colon, an entirely synthetic dataset that includes 20000
realistic colon images and additional details about depth and 3D geometry:
https://enric1994.github.io/synth-colon
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Neuron Instance Segmentation based on Weakly Supervised Efficient UNet and Morphological Post-processing. (arXiv:2202.08682v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08682">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated the superiority of deep learning in medical
image analysis, especially in cell instance segmentation, a fundamental step
for many biological studies. However, the good performance of the neural
networks requires training on large unbiased dataset and annotations, which is
labor-intensive and expertise-demanding. In this paper, we present an
end-to-end weakly-supervised framework to automatically detect and segment NeuN
stained neuronal cells on histological images using only point annotations. We
integrate the state-of-the-art network, EfficientNet, into our U-Net-like
architecture. Validation results show the superiority of our model compared to
other recent methods. In addition, we investigated multiple post-processing
schemes and proposed an original strategy to convert the probability map into
segmented instances using ultimate erosion and dynamic reconstruction. This
approach is easy to configure and outperforms other classical post-processing
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A study of deep perceptual metrics for image quality assessment. (arXiv:2202.08692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08692">
<div class="article-summary-box-inner">
<span><p>Several metrics exist to quantify the similarity between images, but they are
inefficient when it comes to measure the similarity of highly distorted images.
In this work, we propose to empirically investigate perceptual metrics based on
deep neural networks for tackling the Image Quality Assessment (IQA) task. We
study deep perceptual metrics according to different hyperparameters like the
network's architecture or training procedure. Finally, we propose our
multi-resolution perceptual metric (MR-Perceptual), that allows us to aggregate
perceptual information at different resolutions and outperforms standard
perceptual metrics on IQA tasks with varying image deformations. Our code is
available at https://github.com/ENSTA-U2IS/MR_perceptual
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Learning the Unknown in Semantic Segmentation. (arXiv:2202.08700v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08700">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a crucial component for perception in automated
driving. Deep neural networks (DNNs) are commonly used for this task and they
are usually trained on a closed set of object classes appearing in a closed
operational domain. However, this is in contrast to the open world assumption
in automated driving that DNNs are deployed to. Therefore, DNNs necessarily
face data that they have never encountered previously, also known as anomalies,
which are extremely safety-critical to properly cope with. In this work, we
first give an overview about anomalies from an information-theoretic
perspective. Next, we review research in detecting semantically unknown objects
in semantic segmentation. We demonstrate that training for high entropy
responses on anomalous objects outperforms other recent methods, which is in
line with our theoretical findings. Moreover, we examine a method to assess the
occurrence frequency of anomalies in order to select anomaly types to include
into a model's set of semantic categories. We demonstrate that these anomalies
can then be learned in an unsupervised fashion, which is particularly suitable
in online applications based on deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Level set based particle filter driven by optical flow: an application to track the salt boundary from X-ray CT time-series. (arXiv:2202.08717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08717">
<div class="article-summary-box-inner">
<span><p>Image-based computational fluid dynamics have long played an important role
in leveraging knowledge and understanding of several physical phenomena. In
particular, probabilistic computational methods have opened the way to
modelling the complex dynamics of systems in purely random turbulent motion. In
the field of structural geology, a better understanding of the deformation and
stress state both within the salt and the surrounding rocks is of great
interest to characterize all kinds of subsurface long-terms energy-storage
systems. The objective of this research is to determine the non-linear
deformation of the salt boundary over time using a parallelized, stochastic
filtering approach from x-ray computed tomography (CT) image time series
depicting the evolution of salt structures triggered by gravity and under
differential loading. This work represents a first step towards bringing
together physical modeling and advanced stochastic image processing methods
where model uncertainty is taken into account.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colonoscopy polyp detection with massive endoscopic images. (arXiv:2202.08730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08730">
<div class="article-summary-box-inner">
<span><p>We improved an existing end-to-end polyp detection model with better average
precision validated by different data sets with trivial cost on detection
speed. Previous work on detecting polyps within colonoscopy \cite{Chen2018}
provided an efficient end-to-end solution to alleviate doctor's examination
overhead. However, our later experiments found this framework is not as robust
as before as the condition of polyp capturing varies. In this work, we
conducted several studies on data set, identifying main issues that causes low
precision rate in the task of polyp detection. We used an optimized anchor
generation methods to get better anchor box shape and more boxes are used for
detection as we believe this is necessary for small object detection. A
alternative backbone is used to compensate the heavy time cost introduced by
dense anchor box regression. With use of the attention gate module, our model
can achieve state-of-the-art polyp detection performance while still maintain
real-time detection speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniSyn: Synthesizing 360 Videos with Wide-baseline Panoramas. (arXiv:2202.08752v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08752">
<div class="article-summary-box-inner">
<span><p>Immersive maps such as Google Street View and Bing Streetside provide
true-to-life views with a massive collection of panoramas. However, these
panoramas are only available at sparse intervals along the path they are taken,
resulting in visual discontinuities during navigation. Prior art in view
synthesis is usually built upon a set of perspective images, a pair of
stereoscopic images, or a monocular image, but barely examines wide-baseline
panoramas, which are widely adopted in commercial platforms to optimize
bandwidth and storage usage. In this paper, we leverage the unique
characteristics of wide-baseline panoramas and present OmniSyn, a novel
pipeline for 360{\deg} view synthesis between wide-baseline panoramas. OmniSyn
predicts omnidirectional depth maps using a spherical cost volume and a
monocular skip connection, renders meshes in 360{\deg} images, and synthesizes
intermediate views with a fusion network. We demonstrate the effectiveness of
OmniSyn via comprehensive experimental results including comparison with the
state-of-the-art methods on CARLA and Matterport datasets, ablation studies,
and generalization studies on street views. We envision our work may inspire
future research for this unheeded real-world task and eventually produce a
smoother experience for navigating immersive maps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Wavelet-based Dual-stream Network for Underwater Image Enhancement. (arXiv:2202.08758v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08758">
<div class="article-summary-box-inner">
<span><p>We present a wavelet-based dual-stream network that addresses color cast and
blurry details in underwater images. We handle these artifacts separately by
decomposing an input image into multiple frequency bands using discrete wavelet
transform, which generates the downsampled structure image and detail images.
These sub-band images are used as input to our dual-stream network that
incorporates two sub-networks: the multi-color space fusion network and the
detail enhancement network. The multi-color space fusion network takes the
decomposed structure image as input and estimates the color corrected output by
employing the feature representations from diverse color spaces of the input.
The detail enhancement network addresses the blurriness of the original
underwater image by improving the image details from high-frequency sub-bands.
We validate the proposed method on both real-world and synthetic underwater
datasets and show the effectiveness of our model in color correction and blur
removal with low computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08771">
<div class="article-summary-box-inner">
<span><p>Training learning-based deblurring methods demands a significant amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and existing real-world blur datasets provide limited
diversity of scenes and camera settings. As a result, deblurring models trained
on them still suffer from the lack of generalization ability for handling real
blurred images. In this paper, we analyze various factors that introduce
differences between real and synthetic blurred images, and present a novel blur
synthesis pipeline that can synthesize more realistic blur. We also present
RSBlur, a novel dataset that contains real blurred images and the corresponding
sequences of sharp images. The RSBlur dataset can be used for generating
synthetic blurred images to enable detailed analysis on the differences between
real and synthetic blur. With our blur synthesis pipeline and RSBlur dataset,
we reveal the effects of different factors in the blur synthesis. We also show
that our synthesis method can improve the deblurring performance on real
blurred images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08806">
<div class="article-summary-box-inner">
<span><p>We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist
approach toward learning a compositional and grounded meaning representation of
language from grounded data, such as paired images and texts. At the core of
G2L2 is a collection of lexicon entries, which map each word to a tuple of a
syntactic type and a neuro-symbolic semantic program. For example, the word
shiny has a syntactic type of adjective; its neuro-symbolic semantic program
has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is
associated with a neural network embedding, which will be used to classify
shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries
associated with each token. It then derives the meaning of the sentence as an
executable neuro-symbolic program by composing lexical meanings based on
syntax. The recovered meaning programs can be executed on grounded inputs. To
facilitate learning in an exponentially-growing compositional space, we
introduce a joint parsing and expected execution algorithm, which does local
marginalization over derivations to reduce the training time. We evaluate G2L2
on two domains: visual reasoning and language-driven navigation. Results show
that G2L2 can generalize from small amounts of data to novel compositions of
words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Cyclical Training of Neural Networks. (arXiv:2202.08835v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08835">
<div class="article-summary-box-inner">
<span><p>This paper describes the principle of "General Cyclical Training" in machine
learning, where training starts and ends with "easy training" and the "hard
training" happens during the middle epochs. We propose several manifestations
for training neural networks, including algorithmic examples (via
hyper-parameters and loss functions), data-based examples, and model-based
examples. Specifically, we introduce several novel techniques: cyclical weight
decay, cyclical batch size, cyclical focal loss, cyclical softmax temperature,
cyclical data augmentation, cyclical gradient clipping, and cyclical
semi-supervised learning. In addition, we demonstrate that cyclical weight
decay, cyclical softmax temperature, and cyclical gradient clipping (as three
examples of this principle) are beneficial in the test accuracy performance of
a trained model. Furthermore, we discuss model-based examples (such as
pretraining and knowledge distillation) from the perspective of general
cyclical training and recommend some changes to the typical training
methodology. In summary, this paper defines the general cyclical training
concept and discusses several specific ways in which this concept can be
applied to training neural networks. In the spirit of reproducibility, the code
used in our experiments is available at \url{https://github.com/lnsmith54/CFL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adiabatic Quantum Computing for Multi Object Tracking. (arXiv:2202.08837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08837">
<div class="article-summary-box-inner">
<span><p>Multi-Object Tracking (MOT) is most often approached in the
tracking-by-detection paradigm, where object detections are associated through
time. The association step naturally leads to discrete optimization problems.
As these optimization problems are often NP-hard, they can only be solved
exactly for small instances on current hardware. Adiabatic quantum computing
(AQC) offers a solution for this, as it has the potential to provide a
considerable speedup on a range of NP-hard optimization problems in the near
future. However, current MOT formulations are unsuitable for quantum computing
due to their scaling properties. In this work, we therefore propose the first
MOT formulation designed to be solved with AQC. We employ an Ising model that
represents the quantum mechanical system implemented on the AQC. We show that
our approach is competitive compared with state-of-the-art optimization-based
approaches, even when using of-the-shelf integer programming solvers. Finally,
we demonstrate that our MOT problem is already solvable on the current
generation of real quantum computers for small examples, and analyze the
properties of the measured solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRI Reconstruction Using Deep Bayesian Estimation. (arXiv:1909.01127v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.01127">
<div class="article-summary-box-inner">
<span><p>Purpose: To develop a deep learning-based Bayesian inference for MRI
reconstruction. Methods: We modeled the MRI reconstruction problem with Bayes's
theorem, following the recently proposed PixelCNN++ method. The image
reconstruction from incomplete k-space measurement was obtained by maximizing
the posterior possibility. A generative network was utilized as the image
prior, which was computationally tractable, and the k-space data fidelity was
enforced by using an equality constraint. The stochastic backpropagation was
utilized to calculate the descent gradient in the process of maximum a
posterior, and a projected subgradient method was used to impose the equality
constraint. In contrast to the other deep learning reconstruction methods, the
proposed one used the likelihood of prior as the training loss and the
objective function in reconstruction to improve the image quality. Results: The
proposed method showed an improved performance in preserving image details and
reducing aliasing artifacts, compared with GRAPPA, $\ell_1$-ESPRiT, and MODL, a
state-of-the-art deep learning reconstruction method. The proposed method
generally achieved more than 5 dB peak signal-to-noise ratio improvement for
compressed sensing and parallel imaging reconstructions compared with the other
methods. Conclusion: The Bayesian inference significantly improved the
reconstruction performance, compared with the conventional $\ell_1$-sparsity
prior in compressed sensing reconstruction tasks. More importantly, the
proposed reconstruction framework can be generalized for most MRI
reconstruction scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Feature Fusion for Mitosis Counting. (arXiv:2002.03781v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03781">
<div class="article-summary-box-inner">
<span><p>Each woman living in the United States has about 1 in 8 chance of developing
invasive breast cancer. The mitotic cell count is one of the most common tests
to assess the aggressiveness or grade of breast cancer. In this prognosis,
histopathology images must be examined by a pathologist using high-resolution
microscopes to count the cells. Unfortunately, this can be an exhaustive task
with poor reproducibility, especially for non-experts. Deep learning networks
have recently been adapted to medical applications which are able to
automatically localize these regions of interest. However, these region-based
networks lack the ability to take advantage of the segmentation features
produced by a full image CNN which are often used as a sole method of
detection. Therefore, the proposed method leverages Faster RCNN for object
detection while fusing segmentation features generated by a UNet with RGB image
features to achieve an F-score of 0.508 on the MITOS-ATYPIA 2014 mitosis
counting challenge dataset, outperforming state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning. (arXiv:2003.05438v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.05438">
<div class="article-summary-box-inner">
<span><p>The recently advanced unsupervised learning approaches use the siamese-like
framework to compare two "views" from the same image for learning
representations. Making the two views distinctive is a core to guarantee that
unsupervised methods can learn meaningful information. However, such frameworks
are sometimes fragile on overfitting if the augmentations used for generating
two views are not strong enough, causing the over-confident issue on the
training data. This drawback hinders the model from learning subtle variance
and fine-grained information. To address this, in this work we aim to involve
the distance concept on label space in the unsupervised learning and let the
model be aware of the soft degree of similarity between positive or negative
pairs through mixing the input data space, to further work collaboratively for
the input and loss spaces. Despite its conceptual simplicity, we show
empirically that with the solution -- Unsupervised image mixtures (Un-Mix), we
can learn subtler, more robust and generalized representations from the
transformed input and corresponding new label space. Extensive experiments are
conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet
with popular unsupervised methods SimCLR, BYOL, MoCo V1&amp;V2, SwAV, etc. Our
proposed image mixture and label assignment strategy can obtain consistent
improvement by 1~3% following exactly the same hyperparameters and training
procedures of the base methods. Code is publicly available at
https://github.com/szq0214/Un-Mix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models. (arXiv:2012.01988v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01988">
<div class="article-summary-box-inner">
<span><p>Committee-based models (ensembles or cascades) construct models by combining
existing pre-trained ones. While ensembles and cascades are well-known
techniques that were proposed before deep learning, they are not considered a
core building block of deep model architectures and are rarely compared to in
recent literature on developing efficient models. In this work, we go back to
basics and conduct a comprehensive analysis of the efficiency of
committee-based models. We find that even the most simplistic method for
building committees from existing, independently pre-trained models can match
or exceed the accuracy of state-of-the-art models while being drastically more
efficient. These simple committee-based models also outperform sophisticated
neural architecture search methods (e.g., BigNAS). These findings hold true for
several tasks, including image classification, video classification, and
semantic segmentation, and various architecture families, such as ViT,
EfficientNet, ResNet, MobileNetV2, and X3D. Our results show that an
EfficientNet cascade can achieve a 5.4x speedup over B7 and a ViT cascade can
achieve a 2.3x speedup over ViT-L-384 while being equally accurate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-path Bit Sharing for Automatic Loss-aware Model Compression. (arXiv:2101.04935v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04935">
<div class="article-summary-box-inner">
<span><p>Network pruning and quantization are proven to be effective ways for deep
model compression. To obtain a highly compact model, most methods first perform
network pruning and then conduct network quantization based on the pruned
model. However, this strategy may ignore that they would affect each other and
thus performing them separately may lead to sub-optimal performance. To address
this, performing pruning and quantization jointly is essential. Nevertheless,
how to make a trade-off between pruning and quantization is non-trivial.
Moreover, existing compression methods often rely on some pre-defined
compression configurations. Some attempts have been made to search for optimal
configurations, which however may take unbearable optimization cost. To address
the above issues, we devise a simple yet effective method named Single-path Bit
Sharing (SBS). Specifically, we first consider network pruning as a special
case of quantization, which provides a unified view for pruning and
quantization. We then introduce a single-path model to encode all candidate
compression configurations. In this way, the configuration search problem is
transformed into a subset selection problem, which significantly reduces the
number of parameters, computational cost and optimization difficulty. Relying
on the single-path model, we further introduce learnable binary gates to encode
the choice of bitwidth. By jointly training the binary gates in conjunction
with network parameters, the compression configurations of each layer can be
automatically determined. Extensive experiments on both CIFAR-100 and ImageNet
show that SBS is able to significantly reduce computational cost while
achieving promising performance. For example, our SBS compressed MobileNetV2
achieves 22.6x Bit-Operation (BOP) reduction with only 0.1% drop in the Top-1
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Models as Distributions of Functions. (arXiv:2102.04776v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04776">
<div class="article-summary-box-inner">
<span><p>Generative models are typically trained on grid-like data such as images. As
a result, the size of these models usually scales directly with the underlying
grid resolution. In this paper, we abandon discretized grids and instead
parameterize individual data points by continuous functions. We then build
generative models by learning distributions over such functions. By treating
data points as functions, we can abstract away from the specific type of data
we train on and construct models that are agnostic to discretization. To train
our model, we use an adversarial approach with a discriminator that acts on
continuous signals. Through experiments on a wide variety of data modalities
including images, 3D shapes and climate data, we demonstrate that our model can
learn rich distributions of functions independently of data type and
resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09716">
<div class="article-summary-box-inner">
<span><p>Identifying the status of individual network units is critical for
understanding the mechanism of convolutional neural networks (CNNs). However,
it is still challenging to reliably give a general indication of unit status,
especially for units in different network models. To this end, we propose a
novel method for quantitatively clarifying the status of single unit in CNN
using algebraic topological tools. Unit status is indicated via the calculation
of a defined topological-based entropy, called feature entropy, which measures
the degree of chaos of the global spatial pattern hidden in the unit for a
category. In this way, feature entropy could provide an accurate indication of
status for units in different networks with diverse situations like
weight-rescaling operation. Further, we show that feature entropy decreases as
the layer goes deeper and shares almost simultaneous trend with loss during
training. We show that by investigating the feature entropy of units on only
training data, it could give discrimination between networks with different
generalization ability from the view of the effectiveness of feature
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Versatile Neural Architectures by Propagating Network Codes. (arXiv:2103.13253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13253">
<div class="article-summary-box-inner">
<span><p>This work explores how to design a single neural network capable of adapting
to multiple heterogeneous vision tasks, such as image segmentation, 3D
detection, and video recognition. This goal is challenging because both network
architecture search (NAS) spaces and methods in different tasks are
inconsistent. We solve this challenge from both sides. We first introduce a
unified design space for multiple tasks and build a multitask NAS benchmark
(NAS-Bench-MR) on many widely used datasets, including ImageNet, Cityscapes,
KITTI, and HMDB51. We further propose Network Coding Propagation (NCP), which
back-propagates gradients of neural predictors to directly update architecture
codes along the desired gradient directions to solve various tasks. In this
way, optimal architecture configurations can be found by NCP in our large
search space in seconds.
</p>
<p>Unlike prior arts of NAS that typically focus on a single task, NCP has
several unique benefits. (1) NCP transforms architecture optimization from
data-driven to architecture-driven, enabling joint search an architecture among
multitasks with different data distributions. (2) NCP learns from network codes
but not original data, enabling it to update the architecture efficiently
across datasets. (3) In addition to our NAS-Bench-MR, NCP performs well on
other NAS benchmarks, such as NAS-Bench-201. (4) Thorough studies of NCP on
inter-, cross-, and intra-tasks highlight the importance of cross-task neural
architecture design, i.e., multitask neural architectures and architecture
transferring between different tasks. Code is available at
https://github.com/dingmyu/NCP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling a Powerful Student Model via Online Knowledge Distillation. (arXiv:2103.14473v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14473">
<div class="article-summary-box-inner">
<span><p>Existing online knowledge distillation approaches either adopt the student
with the best performance or construct an ensemble model for better holistic
performance. However, the former strategy ignores other students' information,
while the latter increases the computational complexity during deployment. In
this paper, we propose a novel method for online knowledge distillation, termed
FFSD, which comprises two key components: Feature Fusion and Self-Distillation,
towards solving the above problems in a unified framework. Different from
previous works, where all students are treated equally, the proposed FFSD
splits them into a leader student and a common student set. Then, the feature
fusion module converts the concatenation of feature maps from all common
students into a fused feature map. The fused representation is used to assist
the learning of the leader student. To enable the leader student to absorb more
diverse information, we design an enhancement strategy to increase the
diversity among students. Besides, a self-distillation module is adopted to
convert the feature map of deeper layers into a shallower one. Then, the
shallower layers are encouraged to mimic the transformed feature maps of the
deeper layers, which helps the students to generalize better. After training,
we simply adopt the leader student, which achieves superior performance, over
the common students, without increasing the storage or inference cost.
Extensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of
our FFSD over existing works. The code is available at
https://github.com/SJLeo/FFSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification. (arXiv:2104.14528v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14528">
<div class="article-summary-box-inner">
<span><p>Existing deep learning methods for diagnosis of gastric cancer commonly use
convolutional neural network. Recently, the Visual Transformer has attracted
great attention because of its performance and efficiency, but its applications
are mostly in the field of computer vision. In this paper, a multi-scale visual
transformer model, referred to as GasHis-Transformer, is proposed for Gastric
Histopathological Image Classification (GHIC), which enables the automatic
classification of microscopic gastric images into abnormal and normal cases.
The GasHis-Transformer model consists of two key modules: A global information
module and a local information module to extract histopathological features
effectively. In our experiments, a public hematoxylin and eosin (H&amp;E) stained
gastric histopathological dataset with 280 abnormal and normal images are
divided into training, validation and test sets by a ratio of 1 : 1 : 2. The
GasHis-Transformer model is applied to estimate precision, recall, F1-score and
accuracy on the test set of gastric histopathological dataset as 98.0%, 100.0%,
96.0% and 98.0%, respectively. Furthermore, a critical study is conducted to
evaluate the robustness of GasHis-Transformer, where ten different noises
including four adversarial attack and six conventional image noises are added.
In addition, a clinically meaningful study is executed to test the
gastrointestinal cancer identification performance of GasHis-Transformer with
620 abnormal images and achieves 96.8% accuracy. Finally, a comparative study
is performed to test the generalizability with both H&amp;E and immunohistochemical
stained images on a lymphoma image dataset and a breast cancer dataset,
producing comparable F1-scores (85.6% and 82.8%) and accuracies (83.9% and
89.4%), respectively. In conclusion, GasHisTransformer demonstrates high
classification performance and shows its significant potential in the GHIC
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preservation of Global Knowledge by Not-True Distillation in Federated Learning. (arXiv:2106.03097v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03097">
<div class="article-summary-box-inner">
<span><p>In federated learning, a strong global model is collaboratively learned by
aggregating clients' locally trained models. Although this precludes the need
to access clients' data directly, the global model's convergence often suffers
from data heterogeneity. This study starts from an analogy to continual
learning and suggests that forgetting could be the bottleneck of federated
learning. We observe that the global model forgets the knowledge from previous
rounds, and the local training induces forgetting the knowledge outside of the
local distribution. Based on our findings, we hypothesize that tackling down
forgetting will relieve the data heterogeneity problem. To this end, we propose
a novel and effective algorithm, Federated Not-True Distillation (FedNTD),
which preserves the global perspective on locally available data only for the
not-true classes. In the experiments, FedNTD shows state-of-the-art performance
on various setups without compromising data privacy or incurring additional
communication costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoGG3D-Net: Locally Guided Global Descriptor Learning for 3D Place Recognition. (arXiv:2109.08336v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08336">
<div class="article-summary-box-inner">
<span><p>Retrieval-based place recognition is an efficient and effective solution for
re-localization within a pre-built map, or global data association for
Simultaneous Localization and Mapping (SLAM). The accuracy of such an approach
is heavily dependent on the quality of the extracted scene-level
representation. While end-to-end solutions - which learn a global descriptor
from input point clouds - have demonstrated promising results, such approaches
are limited in their ability to enforce desirable properties at the local
feature level. In this paper, we introduce a local consistency loss to guide
the network towards learning local features which are consistent across
revisits, hence leading to more repeatable global descriptors resulting in an
overall improvement in 3D place recognition performance. We formulate our
approach in an end-to-end trainable architecture called LoGG3D-Net. Experiments
on two large-scale public benchmarks (KITTI and MulRan) show that our method
achieves mean $F1_{max}$ scores of $0.939$ and $0.968$ on KITTI and MulRan
respectively, achieving state-of-the-art performance while operating in near
real-time. The open-source implementation is available at:
https://github.com/csiro-robotics/LoGG3D-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Multiple Instance Learning with Attention Mechanisms. (arXiv:2111.00947v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00947">
<div class="article-summary-box-inner">
<span><p>Strongly supervised learning requires detailed knowledge of truth labels at
instance levels, and in many machine learning applications this is a major
drawback. Multiple instance learning (MIL) is a popular weakly supervised
learning method where truth labels are not available at instance level, but
only at bag-of-instances level. However, sometimes the nature of the problem
requires a more complex description, where a nested architecture of bag-of-bags
at different levels can capture underlying relationships, like similar
instances grouped together. Predicting the latent labels of instances or
inner-bags might be as important as predicting the final bag-of-bags label but
is lost in a straightforward nested setting. We propose a Nested Multiple
Instance with Attention (NMIA) model architecture combining the concept of
nesting with attention mechanisms. We show that NMIA performs as conventional
MIL in simple scenarios and can grasp a complex scenario providing insights to
the latent labels at different levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Latent Space Directions For GAN-based Local Image Editing. (arXiv:2111.12583v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12583">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Network (GAN) based localized image editing can suffer
from ambiguity between semantic attributes. We thus present a novel objective
function to evaluate the locality of an image edit. By introducing the
supervision from a pre-trained segmentation network and optimizing the
objective function, our framework, called Locally Effective Latent Space
Direction (LELSD), is applicable to any dataset and GAN architecture. Our
method is also computationally fast and exhibits a high extent of
disentanglement, which allows users to interactively perform a sequence of
edits on an image. Our experiments on both GAN-generated and real images
qualitatively demonstrate the high quality and advantages of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting full Resolution Feature Context for Liver Tumor and Vessel Segmentation via Fusion Encoder: Application to Liver Tumor and Vessel 3D reconstruction. (arXiv:2111.13299v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13299">
<div class="article-summary-box-inner">
<span><p>Liver cancer is one of the most common malignant diseases in the world.
Segmentation and labeling of liver tumors and blood vessels in CT images can
provide convenience for doctors in liver tumor diagnosis and surgical
intervention. In the past decades, automatic CT segmentation methods based on
deep learning have received widespread attention in the medical field. Many
state-of-the-art segmentation algorithms appeared during this period. Yet, most
of the existing segmentation methods only care about the local feature context
and have a perception defect in the global relevance of medical images, which
significantly affects the segmentation effect of liver tumors and blood
vessels. We introduce a multi-scale feature context fusion network called
TransFusionNet based on Transformer and SEBottleNet. This network can
accurately detect and identify the details of the region of interest of the
liver vessel, meanwhile it can improve the recognition of morphologic margins
of liver tumors by exploiting the global information of CT images. Experiments
show that TransFusionNet is better than the state-of-the-art method on both the
public dataset LITS and 3Dircadb and our clinical dataset. Finally, we propose
an automatic 3D reconstruction algorithm based on the trained model. The
algorithm can complete the reconstruction quickly and accurately in 1 second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the HECKTOR Challenge at MICCAI 2021: Automatic Head and Neck Tumor Segmentation and Outcome Prediction in PET/CT Images. (arXiv:2201.04138v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04138">
<div class="article-summary-box-inner">
<span><p>This paper presents an overview of the second edition of the HEad and neCK
TumOR (HECKTOR) challenge, organized as a satellite event of the 24th
International Conference on Medical Image Computing and Computer Assisted
Intervention (MICCAI) 2021. The challenge is composed of three tasks related to
the automatic analysis of PET/CT images for patients with Head and Neck cancer
(H&amp;N), focusing on the oropharynx region. Task 1 is the automatic segmentation
of H&amp;N primary Gross Tumor Volume (GTVt) in FDG-PET/CT images. Task 2 is the
automatic prediction of Progression Free Survival (PFS) from the same
FDG-PET/CT. Finally, Task 3 is the same as Task 2 with ground truth GTVt
annotations provided to the participants. The data were collected from six
centers for a total of 325 images, split into 224 training and 101 testing
cases. The interest in the challenge was highlighted by the important
participation with 103 registered teams and 448 result submissions. The best
methods obtained a Dice Similarity Coefficient (DSC) of 0.7591 in the first
task, and a Concordance index (C-index) of 0.7196 and 0.6978 in Tasks 2 and 3,
respectively. In all tasks, simplicity of the approach was found to be key to
ensure generalization performance. The comparison of the PFS prediction
performance in Tasks 2 and 3 suggests that providing the GTVt contour was not
crucial to achieve best results, which indicates that fully automatic methods
can be used. This potentially obviates the need for GTVt contouring, opening
avenues for reproducible and large scale radiomics studies including thousands
potential subjects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-rank features based double transformation matrices learning for image classification. (arXiv:2201.12351v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12351">
<div class="article-summary-box-inner">
<span><p>Linear regression is a supervised method that has been widely used in
classification tasks. In order to apply linear regression to classification
tasks, a technique for relaxing regression targets was proposed. However,
methods based on this technique ignore the pressure on a single transformation
matrix due to the complex information contained in the data. A single
transformation matrix in this case is too strict to provide a flexible
projection, thus it is necessary to adopt relaxation on transformation matrix.
This paper proposes a double transformation matrices learning method based on
latent low-rank feature extraction. The core idea is to use double
transformation matrices for relaxation, and jointly projecting the learned
principal and salient features from two directions into the label space, which
can share the pressure of a single transformation matrix. Firstly, the low-rank
features are learned by the latent low rank representation (LatLRR) method
which processes the original data from two directions. In this process, sparse
noise is also separated, which alleviates its interference on projection
learning to some extent. Then, two transformation matrices are introduced to
process the two features separately, and the information useful for the
classification is extracted. Finally, the two transformation matrices can be
easily obtained by alternate optimization methods. Through such processing,
even when a large amount of redundant information is contained in samples, our
method can also obtain projection results that are easy to classify.
Experiments on multiple data sets demonstrate the effectiveness of our approach
for classification, especially for complex scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signing the Supermask: Keep, Hide, Invert. (arXiv:2201.13361v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13361">
<div class="article-summary-box-inner">
<span><p>The exponential growth in numbers of parameters of neural networks over the
past years has been accompanied by an increase in performance across several
fields. However, due to their sheer size, the networks not only became
difficult to interpret but also problematic to train and use in real-world
applications, since hardware requirements increased accordingly. Tackling both
issues, we present a novel approach that either drops a neural network's
initial weights or inverts their respective sign. Put simply, a network is
trained by weight selection and inversion without changing their absolute
values. Our contribution extends previous work on masking by additionally
sign-inverting the initial weights and follows the findings of the Lottery
Ticket Hypothesis. Through this extension and adaptations of initialization
methods, we achieve a pruning rate of up to 99%, while still matching or
exceeding the performance of various baseline and previous models. Our approach
has two main advantages. First, and most notable, signed Supermask models
drastically simplify a model's structure, while still performing well on given
tasks. Second, by reducing the neural network to its very foundation, we gain
insights into which weights matter for performance. The code is available on
GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decision boundaries and convex hulls in the feature space that deep learning functions learn from images. (arXiv:2202.04052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04052">
<div class="article-summary-box-inner">
<span><p>The success of deep neural networks in image classification and learning can
be partly attributed to the features they extract from images. It is often
speculated about the properties of a low-dimensional manifold that models
extract and learn from images. However, there is not sufficient understanding
about this low-dimensional space based on theory or empirical evidence. For
image classification models, their last hidden layer is the one where images of
each class is separated from other classes and it also has the least number of
features. Here, we develop methods and formulations to study that feature space
for any model. We study the partitioning of the domain in feature space,
identify regions guaranteed to have certain classifications, and investigate
its implications for the pixel space. We observe that geometric arrangements of
decision boundaries in feature space is significantly different compared to
pixel space, providing insights about adversarial vulnerabilities, image
morphing, extrapolation, ambiguity in classification, and the mathematical
understanding of image classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Box Supervised Video Segmentation Proposal Network. (arXiv:2202.07025v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07025">
<div class="article-summary-box-inner">
<span><p>Video Object Segmentation (VOS) has been targeted by various fully-supervised
and self-supervised approaches. While fully-supervised methods demonstrate
excellent results, self-supervised ones, which do not use pixel-level ground
truth, attract much attention. However, self-supervised approaches pose a
significant performance gap. Box-level annotations provide a balanced
compromise between labeling effort and result quality for image segmentation
but have not been exploited for the video domain. In this work, we propose a
box-supervised video object segmentation proposal network, which takes
advantage of intrinsic video properties. Our method incorporates object motion
in the following way: first, motion is computed using a bidirectional temporal
difference and a novel bounding box-guided motion compensation. Second, we
introduce a novel motion-aware affinity loss that encourages the network to
predict positive pixel pairs if they share similar motion and color. The
proposed method outperforms the state-of-the-art self-supervised benchmark by
16.4% and 6.9% $\mathcal{J}$ &amp;$\mathcal{F}$ score and the majority of fully
supervised methods on the DAVIS and Youtube-VOS dataset without imposing
network architectural specifications. We provide extensive tests and ablations
on the datasets, demonstrating the robustness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Natural Motion: Exploring Discontinuity for Video Frame Interpolation. (arXiv:2202.07291v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07291">
<div class="article-summary-box-inner">
<span><p>Video interpolation is the task that synthesizes the intermediate frame given
two consecutive frames. Most of the previous studies have focused on
appropriate frame warping operations and refinement modules for the warped
frames. These studies have been conducted on natural videos having only
continuous motions. However, many practical videos contain a lot of
discontinuous motions, such as chat windows, watermarks, GUI elements, or
subtitles. We propose three techniques to expand the concept of transition
between two consecutive frames to address these issues. First is a new
architecture that can separate continuous and discontinuous motion areas. We
also propose a novel data augmentation strategy called figure-text mixing (FTM)
to make our model learn more general scenarios. Finally, we propose loss
functions to give supervisions of the discontinuous motion areas with the data
augmentation. We collected a special dataset consisting of some mobile games
and chatting videos. We show that our method significantly improves the
interpolation qualities of the videos on the special dataset. Moreover, our
model outperforms the state-of-the-art methods for natural video datasets
containing only continuous motions, such as DAVIS and UCF101.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Semen Quality Evaluation in Microscopic Videos Using Computer Assisted Sperm Analysis. (arXiv:2202.07820v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07820">
<div class="article-summary-box-inner">
<span><p>The Computer Assisted Sperm Analysis (CASA) plays a crucial role in male
reproductive health diagnosis and Infertility treatment. With the development
of the computer industry in recent years, a great of accurate algorithms are
proposed. With the assistance of those novel algorithms, it is possible for
CASA to achieve a faster and higher quality result. Since image processing is
the technical basis of CASA, including pre-processing,feature extraction,
target detection and tracking, these methods are important technical steps in
dealing with CASA. The various works related to Computer Assisted Sperm
Analysis methods in the last 30 years (since 1988) are comprehensively
introduced and analysed in this survey. To facilitate understanding, the
methods involved are analysed in the sequence of general steps in sperm
analysis. In other words, the methods related to sperm detection (localization)
are first analysed, and then the methods of sperm tracking are analysed. Beside
this, we analyse and prospect the present situation and future of CASA.
According to our work, the feasible for applying in sperm microscopic video of
methods mentioned in this review is explained. Moreover, existing challenges of
object detection and tracking in microscope video are potential to be solved
inspired by this survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multi-reconstruction study of breast density estimation using Deep Learning. (arXiv:2202.08238v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08238">
<div class="article-summary-box-inner">
<span><p>Breast density estimation is one of the key tasks in recognizing individuals
predisposed to breast cancer. It is often challenging because of low contrast
and fluctuations in mammograms' fatty tissue background. Most of the time, the
breast density is estimated manually where a radiologist assigns one of the
four density categories decided by the Breast Imaging and Reporting Data
Systems (BI-RADS). There have been efforts in the direction of automating a
breast density classification pipeline.
</p>
<p>Breast density estimation is one of the key tasks performed during a
screening exam. Dense breasts are more susceptible to breast cancer. The
density estimation is challenging because of low contrast and fluctuations in
mammograms' fatty tissue background. Traditional mammograms are being replaced
by tomosynthesis and its other low radiation dose variants (for example
Hologic' Intelligent 2D and C-View). Because of the low-dose requirement,
increasingly more screening centers are favoring the Intelligent 2D view and
C-View. Deep-learning studies for breast density estimation use only a single
modality for training a neural network. However, doing so restricts the number
of images in the dataset. In this paper, we show that a neural network trained
on all the modalities at once performs better than a neural network trained on
any single modality. We discuss these results using the area under the receiver
operator characteristics curves.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-20 23:07:18.240442015 UTC">2022-02-20 23:07:18 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>