<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-12T01:30:00Z">01-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Likelihood Ratio based Domain Adaptation Method for E2E Models. (arXiv:2201.03655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03655">
<div class="article-summary-box-inner">
<span><p>End-to-end (E2E) automatic speech recognition models like Recurrent Neural
Networks Transducer (RNN-T) are becoming a popular choice for streaming ASR
applications like voice assistants. While E2E models are very effective at
learning representation of the training data they are trained on, their
accuracy on unseen domains remains a challenging problem. Additionally, these
models require paired audio and text training data, are computationally
expensive and are difficult to adapt towards the fast evolving nature of
conversational speech. In this work, we explore a contextual biasing approach
using likelihood-ratio that leverages text data sources to adapt RNN-T model to
new domains and entities. We show that this method is effective in improving
rare words recognition, and results in a relative improvement of 10% in 1-best
word error rate (WER) and 10% in n-best Oracle WER (n=8) on multiple
out-of-domain datasets without any degradation on a general dataset. We also
show that complementing the contextual biasing adaptation with adaptation of a
second-pass rescoring model gives additive WER improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Agnostic Website Embedding and Classification. (arXiv:2201.03677v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03677">
<div class="article-summary-box-inner">
<span><p>Currently, publicly available models for website classification do not offer
an embedding method and have limited support for languages beyond English. We
release a dataset with more than 1M websites in 92 languages with relative
labels collected from Curlie, the largest multilingual crowdsourced Web
directory. The dataset contains 14 website categories aligned across languages.
Alongside it, we introduce Homepage2Vec, a machine-learned pre-trained model
for classifying and embedding websites based on their homepage in a
language-agnostic way. Homepage2Vec, thanks to its feature set (textual
content, metadata tags, and visual attributes) and recent progress in natural
language representation, is language-independent by design and can generate
embeddings representation. We show that Homepage2Vec correctly classifies
websites with a macro-averaged F1-score of 0.90, with stable performance across
low- as well as high-resource languages. Feature analysis shows that a small
subset of efficiently computable features suffices to achieve high performance
even with limited computational resources. We make publicly available the
curated Curlie dataset aligned across languages, the pre-trained Homepage2Vec
model, and libraries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informal Persian Universal Dependency Treebank. (arXiv:2201.03679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03679">
<div class="article-summary-box-inner">
<span><p>This paper presents the phonological, morphological, and syntactic
distinctions between formal and informal Persian, showing that these two
variants have fundamental differences that cannot be attributed solely to
pronunciation discrepancies. Given that informal Persian exhibits particular
characteristics, any computational model trained on formal Persian is unlikely
to transfer well to informal Persian, necessitating the creation of dedicated
treebanks for this variety. We thus detail the development of the open-source
Informal Persian Universal Dependency Treebank, a new treebank annotated within
the Universal Dependencies scheme. We then investigate the parsing of informal
Persian by training two dependency parsers on existing formal treebanks and
evaluating them on out-of-domain data, i.e. the development set of our informal
treebank. Our results show that parsers experience a substantial performance
drop when we move across the two domains, as they face more unknown tokens and
structures and fail to generalize well. Furthermore, the dependency relations
whose performance deteriorates the most represent the unique properties of the
informal variant. The ultimate goal of this study that demonstrates a broader
impact is to provide a stepping-stone to reveal the significance of informal
variants of languages, which have been widely overlooked in natural language
processing tools across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. (arXiv:2201.03713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03713">
<div class="article-summary-box-inner">
<span><p>We introduce CVSS, a massively multilingual-to-English speech-to-speech
translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21
languages into English. CVSS is derived from the Common Voice speech corpus and
the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the
translation text from CoVoST 2 into speech using state-of-the-art TTS systems.
Two versions of translation speeches are provided: 1) CVSS-C: All the
translation speeches are in a single high-quality canonical voice; 2) CVSS-T:
The translation speeches are in voices transferred from the corresponding
source speeches. In addition, CVSS provides normalized translation text which
matches the pronunciation in the translation speech. On each version of CVSS,
we built baseline multilingual direct S2ST models and cascade S2ST models,
verifying the effectiveness of the corpus. To build strong cascade S2ST
baselines, we trained an ST model on CoVoST 2, which outperforms the previous
state-of-the-art trained on the corpus without extra data by 5.8 BLEU.
Nevertheless, the performance of the direct S2ST models approaches the strong
cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU
difference on ASR transcribed translation when initialized from matching ST
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Prediction Uncertainty of Pre-trained Language Models by Detecting Uncertain Words in Inputs. (arXiv:2201.03742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03742">
<div class="article-summary-box-inner">
<span><p>Estimating the predictive uncertainty of pre-trained language models is
important for increasing their trustworthiness in NLP. Although many previous
works focus on quantifying prediction uncertainty, there is little work on
explaining the uncertainty. This paper pushes a step further on explaining
uncertain predictions of post-calibrated pre-trained language models. We adapt
two perturbation-based post-hoc interpretation methods, Leave-one-out and
Sampling Shapley, to identify words in inputs that cause the uncertainty in
predictions. We test the proposed methods on BERT and RoBERTa with three tasks:
sentiment classification, natural language inference, and paraphrase
identification, in both in-domain and out-of-domain settings. Experiments show
that both methods consistently capture words in inputs that cause prediction
uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prior Knowledge Enhances Radiology Report Generation. (arXiv:2201.03761v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03761">
<div class="article-summary-box-inner">
<span><p>Radiology report generation aims to produce computer-aided diagnoses to
alleviate the workload of radiologists and has drawn increasing attention
recently. However, previous deep learning methods tend to neglect the mutual
influences between medical findings, which can be the bottleneck that limits
the quality of generated reports. In this work, we propose to mine and
represent the associations among medical findings in an informative knowledge
graph and incorporate this prior knowledge with radiology report generation to
help improve the quality of generated reports. Experiment results demonstrate
the superior performance of our proposed method on the IU X-ray dataset with a
ROUGE-L of 0.384$\pm$0.007 and CIDEr of 0.340$\pm$0.011. Compared with previous
works, our model achieves an average of 1.6% improvement (2.0% and 1.5%
improvements in CIDEr and ROUGE-L, respectively). The experiments suggest that
prior knowledge can bring performance gains to accurate radiology report
generation. We will make the code publicly available at
https://github.com/bionlplab/report_generation_amia2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command Recognition. (arXiv:2201.03804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03804">
<div class="article-summary-box-inner">
<span><p>With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Robustness to Adversarial Word Substitutions. (arXiv:2201.03829v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03829">
<div class="article-summary-box-inner">
<span><p>Deep-learning-based NLP models are found to be vulnerable to word
substitution perturbations. Before they are widely adopted, the fundamental
issues of robustness need to be addressed. Along this line, we propose a formal
framework to evaluate word-level robustness. First, to study safe regions for a
model, we introduce robustness radius which is the boundary where the model can
resist any perturbation. As calculating the maximum robustness radius is
computationally hard, we estimate its upper and lower bound. We repurpose
attack methods as ways of seeking upper bound and design a pseudo-dynamic
programming algorithm for a tighter upper bound. Then verification method is
utilized for a lower bound. Further, for evaluating the robustness of regions
outside a safe radius, we reexamine robustness from another view:
quantification. A robustness metric with a rigorous statistical guarantee is
introduced to measure the quantification of adversarial examples, which
indicates the model's susceptibility to perturbations outside the safe radius.
The metric helps us figure out why state-of-the-art models like BERT can be
easily fooled by a few word substitutions, but generalize well in the presence
of real-world noises.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turkish Sentiment Analysis Using Machine Learning Methods: Application on Online Food Order Site Reviews. (arXiv:2201.03848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03848">
<div class="article-summary-box-inner">
<span><p>Satisfaction measurement, which emerges in every sector today, is a very
important factor for many companies. In this study, it is aimed to reach the
highest accuracy rate with various machine learning algorithms by using the
data on Yemek Sepeti and variations of this data. The accuracy values of each
algorithm were calculated together with the various natural language processing
methods used. While calculating these accuracy values, the parameters of the
algorithms used were tried to be optimized. The models trained in this study on
labeled data can be used on unlabeled data and can give companies an idea in
measuring customer satisfaction. It was observed that 3 different natural
language processing methods applied resulted in approximately 5% accuracy
increase in most of the developed models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The GINCO Training Dataset for Web Genre Identification of Documents Out in the Wild. (arXiv:2201.03857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03857">
<div class="article-summary-box-inner">
<span><p>This paper presents a new training dataset for automatic genre identification
GINCO, which is based on 1,125 crawled Slovenian web documents that consist of
650 thousand words. Each document was manually annotated for genre with a new
annotation schema that builds upon existing schemata, having primarily clarity
of labels and inter-annotator agreement in mind. The dataset consists of
various challenges related to web-based data, such as machine translated
content, encoding errors, multiple contents presented in one document etc.,
enabling evaluation of classifiers in realistic conditions. The initial machine
learning experiments on the dataset show that (1) pre-Transformer models are
drastically less able to model the phenomena, with macro F1 metrics ranging
around 0.22, while Transformer-based models achieve scores of around 0.58, and
(2) multilingual Transformer models work as well on the task as the monolingual
models that were previously proven to be superior to multilingual models on
standard NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis with Deep Learning Models: A Comparative Study on a Decade of Sinhala Language Facebook Data. (arXiv:2201.03941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03941">
<div class="article-summary-box-inner">
<span><p>The relationship between Facebook posts and the corresponding reaction
feature is an interesting subject to explore and understand. To archive this
end, we test state-of-the-art Sinhala sentiment analysis models against a data
set containing a decade worth of Sinhala posts with millions of reactions. For
the purpose of establishing benchmarks and with the goal of identifying the
best model for Sinhala sentiment analysis, we also test, on the same data set
configuration, other deep learning models catered for sentiment analysis. In
this study we report that the 3 layer Bidirectional LSTM model achieves an F1
score of 84.58% for Sinhala sentiment analysis, surpassing the current
state-of-the-art model; Capsule B, which only manages to get an F1 score of
82.04%. Further, since all the deep learning models show F1 scores above 75% we
conclude that it is safe to claim that Facebook reactions are suitable to
predict the sentiment of a text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Intensity and its Control for Emotional Voice Conversion. (arXiv:2201.03967v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03967">
<div class="article-summary-box-inner">
<span><p>Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03969">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem
due to the heterogeneity gap between different modalities and the ambiguity of
human emotional expression. Although there have been many successful attempts
to construct multimodal representations for MSA, there are still two challenges
to be addressed: 1) A more robust multimodal representation needs to be
constructed to bridge the heterogeneity gap and cope with the complex
multimodal interactions, and 2) the contextual dynamics must be modeled
effectively throughout the information flow. In this work, we propose a
multimodal representation model based on Mutual information Maximization and
Minimization and Identity Embedding (MMMIE). We combine mutual information
maximization between modal pairs, and mutual information minimization between
input data and corresponding features to mine the modal-invariant and
task-related information. Furthermore, Identity Embedding is proposed to prompt
the downstream network to perceive the contextual information. Experimental
results on two public datasets demonstrate the effectiveness of the proposed
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training. (arXiv:2201.04026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04026">
<div class="article-summary-box-inner">
<span><p>Vision-language pre-training has been an emerging and fast-developing
research topic, which transfers multi-modal knowledge from rich-resource
pre-training task to limited-resource downstream tasks. Unlike existing works
that predominantly learn a single generic encoder, we present a pre-trainable
Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language
perception (e.g., visual question answering) and generation (e.g., image
captioning). Uni-EDEN is a two-stream Transformer based structure, consisting
of three modules: object and sentence encoders that separately learns the
representations of each modality, and sentence decoder that enables both
multi-modal reasoning and sentence generation via inter-modal interaction.
Considering that the linguistic representations of each image can span
different granularities in this hierarchy including, from simple to
comprehensive, individual label, a phrase, and a natural sentence, we pre-train
Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object
Classification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence
Matching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is
endowed with the power of both multi-modal representation extraction and
language modeling. Extensive experiments demonstrate the compelling
generalizability of Uni-EDEN by fine-tuning it to four vision-language
perception and generation downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rVAD: An Unsupervised Segment-Based Robust Voice Activity Detection Method. (arXiv:1906.03588v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.03588">
<div class="article-summary-box-inner">
<span><p>This paper presents an unsupervised segment-based method for robust voice
activity detection (rVAD). The method consists of two passes of denoising
followed by a voice activity detection (VAD) stage. In the first pass,
high-energy segments in a speech signal are detected by using a posteriori
signal-to-noise ratio (SNR) weighted energy difference and if no pitch is
detected within a segment, the segment is considered as a high-energy noise
segment and set to zero. In the second pass, the speech signal is denoised by a
speech enhancement method, for which several methods are explored. Next,
neighbouring frames with pitch are grouped together to form pitch segments, and
based on speech statistics, the pitch segments are further extended from both
ends in order to include both voiced and unvoiced sounds and likely non-speech
parts as well. In the end, a posteriori SNR weighted energy difference is
applied to the extended pitch segments of the denoised speech signal for
detecting voice activity. We evaluate the VAD performance of the proposed
method using two databases, RATS and Aurora-2, which contain a large variety of
noise conditions. The rVAD method is further evaluated, in terms of speaker
verification performance, on the RedDots 2016 challenge database and its
noise-corrupted versions. Experiment results show that rVAD is compared
favourably with a number of existing methods. In addition, we present a
modified version of rVAD where computationally intensive pitch extraction is
replaced by computationally efficient spectral flatness calculation. The
modified version significantly reduces the computational complexity at the cost
of moderately inferior VAD performance, which is an advantage when processing a
large amount of data and running on low resource devices. The source code of
rVAD is made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DensE: An Enhanced Non-commutative Representation for Knowledge Graph Embedding with Adaptive Semantic Hierarchy. (arXiv:2008.04548v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.04548">
<div class="article-summary-box-inner">
<span><p>Capturing the composition patterns of relations is a vital task in knowledge
graph completion. It also serves as a fundamental step towards multi-hop
reasoning over learned knowledge. Previously, several rotation-based
translational methods have been developed to model composite relations using
the product of a series of complex-valued diagonal matrices. However, these
methods tend to make several oversimplified assumptions on the composite
relations, e.g., forcing them to be commutative, independent from entities and
lacking semantic hierarchy. To systematically tackle these problems, we have
developed a novel knowledge graph embedding method, named DensE, to provide an
improved modeling scheme for the complex composition patterns of relations. In
particular, our method decomposes each relation into an SO(3) group-based
rotation operator and a scaling operator in the three dimensional (3-D)
Euclidean space. This design principle leads to several advantages of our
method: (1) For composite relations, the corresponding diagonal relation
matrices can be non-commutative, reflecting a predominant scenario in real
world applications; (2) Our model preserves the natural interaction between
relational operations and entity embeddings; (3) The scaling operation provides
the modeling power for the intrinsic semantic hierarchical structure of
entities; (4) The enhanced expressiveness of DensE is achieved with high
computational efficiency in terms of both parameter size and training time; and
(5) Modeling entities in Euclidean space instead of quaternion space keeps the
direct geometrical interpretations of relational patterns. Experimental results
on multiple benchmark knowledge graphs show that DensE outperforms the current
state-of-the-art models for missing link prediction, especially on composite
relations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularization for Long Named Entity Recognition. (arXiv:2104.07249v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07249">
<div class="article-summary-box-inner">
<span><p>When performing named entity recognition (NER), entity length is variable and
dependent on a specific domain or dataset. Pre-trained language models (PLMs)
are used to solve NER tasks and tend to be biased toward dataset patterns such
as length statistics, surface form, and skewed class distribution. These biases
hinder the generalization ability of PLMs, which is necessary to address many
unseen mentions in real-world situations. We propose a novel debiasing method
RegLER to improve predictions for entities of varying lengths. To close the gap
between evaluation and real-world situations, we evaluated PLMs on partitioned
benchmark datasets containing unseen mention sets. Here, RegLER shows
significant improvement over long-named entities that can predict through
debiasing on conjunction or special characters within entities. Furthermore,
there is a severe class imbalance in most NER datasets, causing easy-negative
examples to dominate during training, such as "The". Our approach alleviates
skewed class distribution by reducing the influence of easy-negative examples.
Extensive experiments on the biomedical and general domains demonstrated the
generalization capabilities of our method. To facilitate reproducibility and
future work, we release our code."https://github.com/minstar/RegLER"
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational AI Systems for Social Good: Opportunities and Challenges. (arXiv:2105.06457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06457">
<div class="article-summary-box-inner">
<span><p>Conversational artificial intelligence (ConvAI) systems have attracted much
academic and commercial attention recently, making significant progress on both
fronts. However, little existing work discusses how these systems can be
developed and deployed for social good in real-world applications, with
comprehensive case studies and analyses of pros and cons. In this paper, we
briefly review the progress the community has made towards better ConvAI
systems and reflect on how existing technologies can help advance social good
initiatives from various angles that are unique for ConvAI, or not yet become
common knowledge in the community. We further discuss about the challenges
ahead for ConvAI systems to better help us achieve these goals and highlight
the risks involved in their development and deployment in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransAug: Translate as Augmentation for Sentence Embeddings. (arXiv:2111.00157v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00157">
<div class="article-summary-box-inner">
<span><p>While contrastive learning greatly advances the representation of sentence
embeddings, it is still limited by the size of the existing sentence datasets.
In this paper, we present TransAug (Translate as Augmentation), which provide
the first exploration of utilizing translated sentence pairs as data
augmentation for text, and introduce a two-stage paradigm to advances the
state-of-the-art sentence embeddings. Instead of adopting an encoder trained in
other languages setting, we first distill a Chinese encoder from a SimCSE
encoder (pretrained in English), so that their embeddings are close in semantic
space, which can be regraded as implicit data augmentation. Then, we only
update the English encoder via cross-lingual contrastive learning and frozen
the distilled Chinese encoder. Our approach achieves a new state-of-art on
standard semantic textual similarity (STS), outperforming both SimCSE and
Sentence-T5, and the best performance in corresponding tracks on transfer tasks
evaluated by SentEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04022">
<div class="article-summary-box-inner">
<span><p>We study the problem of weakly supervised text classification, which aims to
classify text documents into a set of pre-defined categories with category
surface names only and without any annotated training document provided. Most
existing classifiers leverage textual information in each document. However, in
many domains, documents are accompanied by various types of metadata (e.g.,
authors, venue, and year of a research paper). These metadata and their
combinations may serve as strong category indicators in addition to textual
contents. In this paper, we explore the potential of using metadata to help
weakly supervised text classification. To be specific, we model the
relationships between documents and metadata via a heterogeneous information
network. To effectively capture higher-order structures in the network, we use
motifs to describe metadata combinations. We propose a novel framework, named
MotifClass, which (1) selects category-indicative motif instances, (2)
retrieves and generates pseudo-labeled training samples based on category names
and indicative motif instances, and (3) trains a text classifier using the
pseudo training data. Extensive experiments on real-world datasets demonstrate
the superior performance of MotifClass to existing weakly supervised text
classification approaches. Further analysis shows the benefit of considering
higher-order metadata information in our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Randomized Smoothing for Adversarially Robust Speech Recognition. (arXiv:2112.03000v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03000">
<div class="article-summary-box-inner">
<span><p>While Automatic Speech Recognition has been shown to be vulnerable to
adversarial attacks, defenses against these attacks are still lagging.
Existing, naive defenses can be partially broken with an adaptive attack. In
classification tasks, the Randomized Smoothing paradigm has been shown to be
effective at defending models. However, it is difficult to apply this paradigm
to ASR tasks, due to their complexity and the sequential nature of their
outputs. Our paper overcomes some of these challenges by leveraging
speech-specific tools like enhancement and ROVER voting to design an ASR model
that is robust to perturbations. We apply adaptive versions of state-of-the-art
attacks, such as the Imperceptible ASR attack, to our model, and show that our
strongest defense is robust to all attacks that use inaudible noise, and can
only be broken with very high distortion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving language models by retrieving from trillions of tokens. (arXiv:2112.04426v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04426">
<div class="article-summary-box-inner">
<span><p>We enhance auto-regressive language models by conditioning on document chunks
retrieved from a large corpus, based on local similarity with preceding tokens.
With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO)
obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite
using 25$\times$ fewer parameters. After fine-tuning, RETRO performance
translates to downstream knowledge-intensive tasks such as question answering.
RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked
cross-attention mechanism to predict tokens based on an order of magnitude more
data than what is typically consumed during training. We typically train RETRO
from scratch, yet can also rapidly RETROfit pre-trained transformers with
retrieval and still achieve good performance. Our work opens up new avenues for
improving language models through explicit memory at unprecedented scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The King is Naked: on the Notion of Robustness for Natural Language Processing. (arXiv:2112.07605v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07605">
<div class="article-summary-box-inner">
<span><p>There is growing evidence that the classical notion of adversarial robustness
originally introduced for images has been adopted as a de facto standard by a
large part of the NLP research community. We show that this notion is
problematic in the context of NLP as it considers a narrow spectrum of
linguistic phenomena. In this paper, we argue for semantic robustness, which is
better aligned with the human concept of linguistic fidelity. We characterize
semantic robustness in terms of biases that it is expected to induce in a
model. We study semantic robustness of a range of vanilla and robustly trained
architectures using a template-based generative test bed. We complement the
analysis with empirical evidence that, despite being harder to implement,
semantic robustness can improve performance %gives guarantees for on complex
linguistic phenomena where models robust in the classical sense fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling the Knowledge of Romanian BERTs Using Multiple Teachers. (arXiv:2112.12650v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12650">
<div class="article-summary-box-inner">
<span><p>Running large-scale pre-trained language models in computationally
constrained environments remains a challenging problem yet to be addressed,
while transfer learning from these models has become prevalent in Natural
Language Processing tasks. Several solutions, including knowledge distillation,
network quantization, or network pruning have been previously proposed;
however, these approaches focus mostly on the English language, thus widening
the gap when considering low-resource languages. In this work, we introduce
three light and fast versions of distilled BERT models for the Romanian
language: Distil-BERT-base-ro, Distil-RoBERT-base, and
DistilMulti-BERT-base-ro. The first two models resulted from the individual
distillation of knowledge from two base versions of Romanian BERTs available in
literature, while the last one was obtained by distilling their ensemble. To
our knowledge, this is the first attempt to create publicly available Romanian
distilled BERT models, which were thoroughly evaluated on five tasks:
part-of-speech tagging, named entity recognition, sentiment analysis, semantic
textual similarity, and dialect identification. Our experimental results argue
that the three distilled models maintain most performance in terms of accuracy
with their teachers, while being twice as fast on a GPU and ~35% smaller. In
addition, we further test the similarity between the predictions of our
students versus their teachers by measuring their label and probability
loyalty, together with regression loyalty - a new metric introduced in this
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Latent Structures in Natural Language Processing: A Survey. (arXiv:2201.00490v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00490">
<div class="article-summary-box-inner">
<span><p>While end-to-end learning with fully differentiable models has enabled
tremendous success in natural language process (NLP) and machine learning,
there have been significant recent interests in learning with latent discrete
structures to incorporate better inductive biases for improved end-task
performance and better interpretability. This paradigm, however, is not
straightforwardly amenable to the mainstream gradient-based optimization
methods. This work surveys three main families of methods to learn such models:
surrogate gradients, continuous relaxation, and marginal likelihood
maximization via sampling. We conclude with a review of applications of these
methods and an inspection of the learned latent structure that they induce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Stage Episodic Control for Strategic Exploration in Text Games. (arXiv:2201.01251v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01251">
<div class="article-summary-box-inner">
<span><p>Text adventure games present unique challenges to reinforcement learning
methods due to their combinatorially large action spaces and sparse rewards.
The interplay of these two factors is particularly demanding because large
action spaces require extensive exploration, while sparse rewards provide
limited feedback. This work proposes to tackle the explore-vs-exploit dilemma
using a multi-stage approach that explicitly disentangles these two strategies
within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins
each episode using an exploitation policy that imitates a set of promising
trajectories from the past, and then switches over to an exploration policy
aimed at discovering novel actions that lead to unseen state spaces. This
policy decomposition allows us to combine global decisions about which parts of
the game space to return to with curiosity-based local exploration in that
space, motivated by how a human may approach these games. Our method
significantly outperforms prior approaches by 27% and 11% average normalized
score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in
both deterministic and stochastic settings, respectively. On the game of Zork1,
in particular, XTX obtains a score of 103, more than a 2x improvement over
prior methods, and pushes past several known bottlenecks in the game that have
plagued previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuSpaCy: an industrial-strength Hungarian natural language processing toolkit. (arXiv:2201.01956v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01956">
<div class="article-summary-box-inner">
<span><p>Although there are a couple of open-source language processing pipelines
available for Hungarian, none of them satisfies the requirements of today's NLP
applications. A language processing pipeline should consist of close to
state-of-the-art lemmatization, morphosyntactic analysis, entity recognition
and word embeddings. Industrial text processing applications have to satisfy
non-functional software quality requirements, what is more, frameworks
supporting multiple languages are more and more favored. This paper introduces
HuSpaCy, an industry-ready Hungarian language processing toolkit. The presented
tool provides components for the most important basic linguistic analysis
tasks. It is open-source and is available under a permissive license. Our
system is built upon spaCy's NLP components resulting in an easily usable, fast
yet accurate application. Experiments confirm that HuSpaCy has high accuracy
while maintaining resource-efficient prediction capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot and Few-Shot Classification of Biomedical Articles in Context of the COVID-19 Pandemic. (arXiv:2201.03017v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03017">
<div class="article-summary-box-inner">
<span><p>MeSH (Medical Subject Headings) is a large thesaurus created by the National
Library of Medicine and used for fine-grained indexing of publications in the
biomedical domain. In the context of the COVID-19 pandemic, MeSH descriptors
have emerged in relation to articles published on the corresponding topic.
Zero-shot classification is an adequate response for timely labeling of the
stream of papers with MeSH categories. In this work, we hypothesise that rich
semantic information available in MeSH has potential to improve BioBERT
representations and make them more suitable for zero-shot/few-shot tasks. We
frame the problem as determining if MeSH term definitions, concatenated with
paper abstracts are valid instances or not, and leverage multi-task learning to
induce the MeSH hierarchy in the representations thanks to a seq2seq task.
Results establish a baseline on the MedLine and LitCovid datasets, and probing
shows that the resulting representations convey the hierarchical relations
present in MeSH.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducing BowNet: Learning Representations by Predicting Bags of Visual Words. (arXiv:2201.03556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03556">
<div class="article-summary-box-inner">
<span><p>This work aims to reproduce results from the CVPR 2020 paper by Gidaris et
al. Self-supervised learning (SSL) is used to learn feature representations of
an image using an unlabeled dataset. This work proposes to use bag-of-words
(BoW) deep feature descriptors as a self-supervised learning target to learn
robust, deep representations. BowNet is trained to reconstruct the histogram of
visual words (ie. the deep BoW descriptor) of a reference image when presented
a perturbed version of the image as input. Thus, this method aims to learn
perturbation-invariant and context-aware image features that can be useful for
few-shot tasks or supervised downstream tasks. In the paper, the author
describes BowNet as a network consisting of a convolutional feature extractor
$\Phi(\cdot)$ and a Dense-softmax layer $\Omega(\cdot)$ trained to predict BoW
features from images. After BoW training, the features of $\Phi$ are used in
downstream tasks. For this challenge we were trying to build and train a
network that could reproduce the CIFAR-100 accuracy improvements reported in
the original paper. However, we were unsuccessful in reproducing an accuracy
improvement comparable to what the authors mentioned.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demonstrating The Risk of Imbalanced Datasets in Chest X-ray Image-based Diagnostics by Prototypical Relevance Propagation. (arXiv:2201.03559v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03559">
<div class="article-summary-box-inner">
<span><p>The recent trend of integrating multi-source Chest X-Ray datasets to improve
automated diagnostics raises concerns that models learn to exploit
source-specific correlations to improve performance by recognizing the source
domain of an image rather than the medical pathology. We hypothesize that this
effect is enforced by and leverages label-imbalance across the source domains,
i.e, prevalence of a disease corresponding to a source. Therefore, in this
work, we perform a thorough study of the effect of label-imbalance in
multi-source training for the task of pneumonia detection on the widely used
ChestX-ray14 and CheXpert datasets. The results highlight and stress the
importance of using more faithful and transparent self-explaining models for
automated diagnosis, thus enabling the inherent detection of spurious learning.
They further illustrate that this undesirable effect of learning spurious
correlations can be reduced considerably when ensuring label-balanced source
domain datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative RAKI with Complex-Valued Convolution for Improved Image Reconstruction with Limited Scan-Specific Training Samples. (arXiv:2201.03560v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03560">
<div class="article-summary-box-inner">
<span><p>MRI scan time reduction is commonly achieved by Parallel Imaging methods,
typically based on uniform undersampling of the inverse image space (a.k.a.
k-space) and simultaneous signal reception with multiple receiver coils. The
GRAPPA method interpolates missing k-space signals by linear combination of
adjacent, acquired signals across all coils, and can be described by a
convolution in k-space. Recently, a more generalized method called RAKI was
introduced. RAKI is a deep-learning method that generalizes GRAPPA with
additional convolution layers, on which a non-linear activation function is
applied. This enables non-linear estimation of missing signals by convolutional
neural networks. In analogy to GRAPPA, the convolution kernels in RAKI are
trained using scan-specific training samples obtained from
auto-calibration-signals (ACS). RAKI provides superior reconstruction quality
compared to GRAPPA, however, often requires much more ACS due to its increased
number of unknown parameters. In order to overcome this limitation, this study
investigates the influence of training data on the reconstruction quality for
standard 2D imaging, with particular focus on its amount and contrast
information. Furthermore, an iterative k-space interpolation approach (iRAKI)
is evaluated, which includes training data augmentation via an initial GRAPPA
reconstruction, and refinement of convolution filters by iterative training.
Using only 18, 20 and 25 ACS lines (8%), iRAKI outperforms RAKI by suppressing
residual artefacts occurring at accelerations factors R=4 and R=5, and yields
strong noise suppression in comparison to GRAPPA, underlined by quantitative
quality metrics. Combination with a phase-constraint yields further
improvement. Additionally, iRAKI shows better performance than GRAPPA and RAKI
in case of pre-scan calibration and strongly varying contrast between training-
and undersampled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Sub-Image Retrieval using Contrastive Multimodal Image Representations. (arXiv:2201.03597v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03597">
<div class="article-summary-box-inner">
<span><p>In tissue characterization and cancer diagnostics, multimodal imaging has
emerged as a powerful technique. Thanks to computational advances, large
datasets can be exploited to improve diagnosis and discover patterns in
pathologies. However, this requires efficient and scalable image retrieval
methods. Cross-modality image retrieval is particularly demanding, as images of
the same content captured in different modalities may display little common
information. We propose a content-based image retrieval system (CBIR) for
reverse (sub-)image search to retrieve microscopy images in one modality given
a corresponding image captured by a different modality, where images are not
aligned and share only few structures. We propose to combine deep learning to
generate representations which embed both modalities in a common space, with
classic, fast, and robust feature extractors (SIFT, SURF) to create a
bag-of-words model for efficient and reliable retrieval. Our
application-independent approach shows promising results on a publicly
available dataset of brightfield and second harmonic generation microscopy
images. We obtain 75.4% and 83.6% top-10 retrieval success for retrieval in one
or the other direction. Our proposed method significantly outperforms both
direct retrieval of the original multimodal (sub-)images, as well as their
corresponding generative adversarial network (GAN)-based image-to-image
translations. We establish that the proposed method performs better in
comparison with a recent sub-image retrieval toolkit, GAN-based image-to-image
translations, and learnt feature extractors for the downstream task of
cross-modal image retrieval. We highlight the shortcomings of the latter
methods and observe the importance of equivariance and invariance properties of
the learnt representations and feature extractors in the CBIR pipeline. Code
will be available at github.com/MIDA-group.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-query Video Retrieval. (arXiv:2201.03639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03639">
<div class="article-summary-box-inner">
<span><p>Retrieving target videos based on text descriptions is a task of great
practical value and has received increasing attention over the past few years.
In this paper, we focus on the less-studied setting of multi-query video
retrieval, where multiple queries are provided to the model for searching over
the video archive. We first show that the multi-query retrieval task is more
pragmatic and representative of real-world use cases and better evaluates
retrieval capabilities of current models, thereby deserving of further
investigation alongside the more prevalent single-query retrieval setup. We
then propose several new methods for leveraging multiple queries at training
time to improve over simply combining similarity outputs of multiple queries
from regular single-query trained models. Our models consistently outperform
several competitive baselines over three different datasets. For instance,
Recall@1 can be improved by 4.7 points on MSR-VTT, 4.1 points on MSVD and 11.7
points on VATEX over a strong baseline built on the state-of-the-art CLIP4Clip
model. We believe further modeling efforts will bring new insights to this
direction and spark new systems that perform better in real-world video
retrieval applications. Code is available at
https://github.com/princetonvisualai/MQVR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Segmentation with Fully Trainable Gabor Kernels and Pearson's Correlation Coefficient. (arXiv:2201.03644v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03644">
<div class="article-summary-box-inner">
<span><p>The convolutional layer and loss function are two fundamental components in
deep learning. Because of the success of conventional deep learning kernels,
the less versatile Gabor kernels become less popular despite the fact that they
can provide abundant features at different frequencies, orientations, and
scales with much fewer parameters. For existing loss functions for multi-class
image segmentation, there is usually a tradeoff among accuracy, robustness to
hyperparameters, and manual weight selections for combining different losses.
Therefore, to gain the benefits of using Gabor kernels while keeping the
advantage of automatic feature generation in deep learning, we propose a fully
trainable Gabor-based convolutional layer where all Gabor parameters are
trainable through backpropagation. Furthermore, we propose a loss function
based on the Pearson's correlation coefficient, which is accurate, robust to
learning rates, and does not require manual weight selections. Experiments on
43 3D brain magnetic resonance images with 19 anatomical structures show that,
using the proposed loss function with a proper combination of conventional and
Gabor-based kernels, we can train a network with only 1.6 million parameters to
achieve an average Dice coefficient of 83%. This size is 44 times smaller than
the V-Net which has 71 million parameters. This paper demonstrates the
potentials of using learnable parametric kernels in deep learning for 3D
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Group Robustness in the presence of Partial Group Labels. (arXiv:2201.03668v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03668">
<div class="article-summary-box-inner">
<span><p>Learning invariant representations is an important requirement when training
machine learning models that are driven by spurious correlations in the
datasets. These spurious correlations, between input samples and the target
labels, wrongly direct the neural network predictions resulting in poor
performance on certain groups, especially the minority groups. Robust training
against these spurious correlations requires the knowledge of group membership
for every sample. Such a requirement is impractical in situations where the
data labeling efforts for minority or rare groups are significantly laborious
or where the individuals comprising the dataset choose to conceal sensitive
information. On the other hand, the presence of such data collection efforts
results in datasets that contain partially labeled group information. Recent
works have tackled the fully unsupervised scenario where no labels for groups
are available. Thus, we aim to fill the missing gap in the literature by
tackling a more realistic setting that can leverage partially available
sensitive or group information during training. First, we construct a
constraint set and derive a high probability bound for the group assignment to
belong to the set. Second, we propose an algorithm that optimizes for the
worst-off group assignments from the constraint set. Through experiments on
image and tabular datasets, we show improvements in the minority group's
performance while preserving overall aggregate accuracy across groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuroplastic graph attention networks for nuclei segmentation in histopathology images. (arXiv:2201.03669v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03669">
<div class="article-summary-box-inner">
<span><p>Modern histopathological image analysis relies on the segmentation of cell
structures to derive quantitative metrics required in biomedical research and
clinical diagnostics. State-of-the-art deep learning approaches predominantly
apply convolutional layers in segmentation and are typically highly customized
for a specific experimental configuration; often unable to generalize to
unknown data. As the model capacity of classical convolutional layers is
limited by a finite set of learned kernels, our approach uses a graph
representation of the image and focuses on the node transitions in multiple
magnifications. We propose a novel architecture for semantic segmentation of
cell nuclei robust to differences in experimental configuration such as
staining and variation of cell types. The architecture is comprised of a novel
neuroplastic graph attention network based on residual graph attention layers
and concurrent optimization of the graph structure representing multiple
magnification levels of the histopathological image. The modification of graph
structure, which generates the node features by projection, is as important to
the architecture as the graph neural network itself. It determines the possible
message flow and critical properties to optimize attention, graph structure,
and node updates in a balanced magnification loss. In experimental evaluation,
our framework outperforms ensembles of state-of-the-art neural networks, with a
fraction of the neurons typically required, and sets new standards for the
segmentation of new nuclei datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PrintsGAN: Synthetic Fingerprint Generator. (arXiv:2201.03674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03674">
<div class="article-summary-box-inner">
<span><p>A major impediment to researchers working in the area of fingerprint
recognition is the lack of publicly available, large-scale, fingerprint
datasets. The publicly available datasets that do exist contain very few
identities and impressions per finger. This limits research on a number of
topics, including e.g., using deep networks to learn fixed length fingerprint
embeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator
capable of generating unique fingerprints along with multiple impressions for a
given fingerprint. Using PrintsGAN, we synthesize a database of 525,000
fingerprints (35,000 distinct fingers, each with 15 impressions). Next, we show
the utility of the PrintsGAN generated dataset by training a deep network to
extract a fixed-length embedding from a fingerprint. In particular, an
embedding model trained on our synthetic fingerprints and fine-tuned on a small
number of publicly available real fingerprints (25,000 prints from NIST SD302)
obtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from
TAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint
generation methods do not enable such performance gains due to i) lack of
realism or ii) inability to generate multiple impressions per finger. We plan
to release our database of synthetic fingerprints to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NFANet: A Novel Method for Weakly Supervised Water Extraction from High-Resolution Remote Sensing Imagery. (arXiv:2201.03686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03686">
<div class="article-summary-box-inner">
<span><p>The use of deep learning for water extraction requires precise pixel-level
labels. However, it is very difficult to label high-resolution remote sensing
images at the pixel level. Therefore, we study how to utilize point labels to
extract water bodies and propose a novel method called the neighbor feature
aggregation network (NFANet). Compared with pixellevel labels, point labels are
much easier to obtain, but they will lose much information. In this paper, we
take advantage of the similarity between the adjacent pixels of a local
water-body, and propose a neighbor sampler to resample remote sensing images.
Then, the sampled images are sent to the network for feature aggregation. In
addition, we use an improved recursive training algorithm to further improve
the extraction accuracy, making the water boundary more natural. Furthermore,
our method utilizes neighboring features instead of global or local features to
learn more representative features. The experimental results show that the
proposed NFANet method not only outperforms other studied weakly supervised
approaches, but also obtains similar results as the state-of-the-art ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An analysis of reconstruction noise from undersampled 4D flow MRI. (arXiv:2201.03715v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03715">
<div class="article-summary-box-inner">
<span><p>Novel Magnetic Resonance (MR) imaging modalities can quantify hemodynamics
but require long acquisition times, precluding its widespread use for early
diagnosis of cardiovascular disease. To reduce the acquisition times,
reconstruction methods from undersampled measurements are routinely used, that
leverage representations designed to increase image compressibility.
</p>
<p>Reconstructed anatomical and hemodynamic images may present visual artifacts.
Although some of these artifact are essentially reconstruction errors, and thus
a consequence of undersampling, others may be due to measurement noise or the
random choice of the sampled frequencies. Said otherwise, a reconstructed image
becomes a random variable, and both its bias and its covariance can lead to
visual artifacts; the latter leads to spatial correlations that may be
misconstrued for visual information. Although the nature of the former has been
studied in the literature, the latter has not received as much attention.
</p>
<p>In this study, we investigate the theoretical properties of the random
perturbations arising from the reconstruction process, and perform a number of
numerical experiments on simulated and MR aortic flow. Our results show that
the correlation length remains limited to two to three pixels when a Gaussian
undersampling pattern is combined with recovery algorithms based on
$\ell_1$-norm minimization. However, the correlation length may increase
significantly for other undersampling patterns, higher undersampling factors
(i.e., 8x or 16x compression), and different reconstruction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSA-Net: Tube Self-Attention Network for Action Quality Assessment. (arXiv:2201.03746v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03746">
<div class="article-summary-box-inner">
<span><p>In recent years, assessing action quality from videos has attracted growing
attention in computer vision community and human computer interaction. Most
existing approaches usually tackle this problem by directly migrating the model
from action recognition tasks, which ignores the intrinsic differences within
the feature map such as foreground and background information. To address this
issue, we propose a Tube Self-Attention Network (TSA-Net) for action quality
assessment (AQA). Specifically, we introduce a single object tracker into AQA
and propose the Tube Self-Attention Module (TSA), which can efficiently
generate rich spatio-temporal contextual information by adopting sparse feature
interactions. The TSA module is embedded in existing video networks to form
TSA-Net. Overall, our TSA-Net is with the following merits: 1) High
computational efficiency, 2) High flexibility, and 3) The state-of-the art
performance. Extensive experiments are conducted on popular action quality
assessment datasets including AQA-7 and MTL-AQA. Besides, a dataset named Fall
Recognition in Figure Skating (FR-FS) is proposed to explore the basic action
assessment in the figure skating scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reciprocal Adversarial Learning for Brain Tumor Segmentation: A Solution to BraTS Challenge 2021 Segmentation Task. (arXiv:2201.03777v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03777">
<div class="article-summary-box-inner">
<span><p>This paper proposes an adversarial learning based training approach for brain
tumor segmentation task. In this concept, the 3D segmentation network learns
from dual reciprocal adversarial learning approaches. To enhance the
generalization across the segmentation predictions and to make the segmentation
network robust, we adhere to the Virtual Adversarial Training approach by
generating more adversarial examples via adding some noise on original patient
data. By incorporating a critic that acts as a quantitative subjective referee,
the segmentation network learns from the uncertainty information associated
with segmentation results. We trained and evaluated network architecture on the
RSNA-ASNR-MICCAI BraTS 2021 dataset. Our performance on the online validation
dataset is as follows: Dice Similarity Score of 81.38%, 90.77% and 85.39%;
Hausdorff Distance (95\%) of 21.83 mm, 5.37 mm, 8.56 mm for the enhancing
tumor, whole tumor and tumor core, respectively. Similarly, our approach
achieved a Dice Similarity Score of 84.55%, 90.46% and 85.30%, as well as
Hausdorff Distance (95\%) of 13.48 mm, 6.32 mm and 16.98 mm on the final test
dataset. Overall, our proposed approach yielded better performance in
segmentation accuracy for each tumor sub-region. Our code implementation is
publicly available at https://github.com/himashi92/vizviva_brats_2021
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drone Object Detection Using RGB/IR Fusion. (arXiv:2201.03786v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03786">
<div class="article-summary-box-inner">
<span><p>Object detection using aerial drone imagery has received a great deal of
attention in recent years. While visible light images are adequate for
detecting objects in most scenarios, thermal cameras can extend the
capabilities of object detection to night-time or occluded objects. As such,
RGB and Infrared (IR) fusion methods for object detection are useful and
important. One of the biggest challenges in applying deep learning methods to
RGB/IR object detection is the lack of available training data for drone IR
imagery, especially at night. In this paper, we develop several strategies for
creating synthetic IR images using the AIRSim simulation engine and CycleGAN.
Furthermore, we utilize an illumination-aware fusion framework to fuse RGB and
IR images for object detection on the ground. We characterize and test our
methods for both simulated and actual data. Our solution is implemented on an
NVIDIA Jetson Xavier running on an actual drone, requiring about 28
milliseconds of processing per RGB/IR image pair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Beer Bottles using Object Detection and Transfer Learning. (arXiv:2201.03791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03791">
<div class="article-summary-box-inner">
<span><p>Classification problems are common in Computer Vision. Despite this, there is
no dedicated work for the classification of beer bottles. As part of the
challenge of the master course Deep Learning, a dataset of 5207 beer bottle
images and brand labels was created. An image contains exactly one beer bottle.
In this paper we present a deep learning model which classifies pictures of
beer bottles in a two step approach. As the first step, a Faster-R-CNN detects
image sections relevant for classification independently of the brand. In the
second step, the relevant image sections are classified by a ResNet-18. The
image section with the highest confidence is returned as class label. We
propose a model, with which we surpass the classic one step transfer learning
approach and reached an accuracy of 99.86% during the challenge on the final
test dataset. We were able to achieve 100% accuracy after the challenge ended
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Non-Local Contrastive Attention for Image Super-Resolution. (arXiv:2201.03794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03794">
<div class="article-summary-box-inner">
<span><p>Non-Local Attention (NLA) brings significant improvement for Single Image
Super-Resolution (SISR) by leveraging intrinsic feature correlation in natural
images. However, NLA gives noisy information large weights and consumes
quadratic computation resources with respect to the input size, limiting its
performance and application. In this paper, we propose a novel Efficient
Non-Local Contrastive Attention (ENLCA) to perform long-range visual modeling
and leverage more relevant non-local features. Specifically, ENLCA consists of
two parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA
adopts the kernel method to approximate exponential function and obtains linear
computation complexity. For Sparse Aggregation, we multiply inputs by an
amplification factor to focus on informative features, yet the variance of
approximation increases exponentially. Therefore, contrastive learning is
applied to further separate relevant and irrelevant features. To demonstrate
the effectiveness of ENLCA, we build an architecture called Efficient Non-Local
Contrastive Network (ENLCN) by adding a few of our modules in a simple
backbone. Extensive experimental results show that ENLCN reaches superior
performance over state-of-the-art approaches on both quantitative and
qualitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COROLLA: An Efficient Multi-Modality Fusion Framework with Supervised Contrastive Learning for Glaucoma Grading. (arXiv:2201.03795v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03795">
<div class="article-summary-box-inner">
<span><p>Glaucoma is one of the ophthalmic diseases that may cause blindness, for
which early detection and treatment are very important. Fundus images and
optical coherence tomography (OCT) images are both widely-used modalities in
diagnosing glaucoma. However, existing glaucoma grading approaches mainly
utilize a single modality, ignoring the complementary information between
fundus and OCT. In this paper, we propose an efficient multi-modality
supervised contrastive learning framework, named COROLLA, for glaucoma grading.
Through layer segmentation as well as thickness calculation and projection,
retinal thickness maps are extracted from the original OCT volumes and used as
a replacing modality, resulting in more efficient calculations with less memory
usage. Given the high structure and distribution similarities across medical
image samples, we employ supervised contrastive learning to increase our
models' discriminative power with better convergence. Moreover, feature-level
fusion of paired fundus image and thickness map is conducted for enhanced
diagnosis accuracy. On the GAMMA dataset, our COROLLA framework achieves
overwhelming glaucoma grading performance compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptive Person Re-id with Local-enhance and Prototype Dictionary Learning. (arXiv:2201.03803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03803">
<div class="article-summary-box-inner">
<span><p>The unsupervised domain adaptive person re-identification (re-ID) task has
been a challenge because, unlike the general domain adaptive tasks, there is no
overlap between the classes of source and target domain data in the person
re-ID, which leads to a significant domain gap. State-of-the-art unsupervised
re-ID methods train the neural networks using a memory-based contrastive loss.
However, performing contrastive learning by treating each unlabeled instance as
a class will lead to the problem of class collision, and the updating intensity
is inconsistent due to the difference in the number of instances of different
categories when updating in the memory bank. To address such problems, we
propose Prototype Dictionary Learning for person re-ID which is able to utilize
both source domain data and target domain data by one training stage while
avoiding the problem of class collision and the problem of updating intensity
inconsistency by cluster-level prototype dictionary learning. In order to
reduce the interference of domain gap on the model, we propose a local-enhance
module to improve the domain adaptation of the model without increasing the
number of model parameters. Our experiments on two large datasets demonstrate
the effectiveness of the prototype dictionary learning. 71.5\% mAP is achieved
in the Market-to-Duke task, which is a 2.3\% improvement compared to the
state-of-the-art unsupervised domain adaptive re-ID methods. It achieves 83.9\%
mAP in the Duke-to-Market task, which improves by 4.4\% compared to the
state-of-the-art unsupervised adaptive re-ID methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobileFaceSwap: A Lightweight Framework for Video Face Swapping. (arXiv:2201.03808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03808">
<div class="article-summary-box-inner">
<span><p>Advanced face swapping methods have achieved appealing results. However, most
of these methods have many parameters and computations, which makes it
challenging to apply them in real-time applications or deploy them on edge
devices like mobile phones. In this work, we propose a lightweight
Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by
dynamically adjusting the model parameters according to the identity
information. In particular, we design an efficient Identity Injection Module
(IIM) by introducing two dynamic neural network techniques, including the
weights prediction and weights modulation. Once the IDN is updated, it can be
applied to swap faces given any target image or video. The presented IDN
contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it
capable for real-time video face swapping on mobile phones. In addition, we
introduce a knowledge distillation-based method for stable training, and a loss
reweighting module is employed to obtain better synthesized results. Finally,
our method achieves comparable results with the teacher models and other
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Exploring Pose Estimation as an Auxiliary Learning Task for Visible-Infrared Person Re-identification. (arXiv:2201.03859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03859">
<div class="article-summary-box-inner">
<span><p>Visible-infrared person re-identification (VI-ReID) has been challenging due
to the existence of large discrepancies between visible and infrared
modalities. Most pioneering approaches reduce intra-class variations and
inter-modality discrepancies by learning modality-shared and ID-related
features. However, an explicit modality-shared cue, i.e., body keypoints, has
not been fully exploited in VI-ReID. Additionally, existing feature learning
paradigms imposed constraints on either global features or partitioned feature
stripes, which neglect the prediction consistency of global and part features.
To address the above problems, we exploit Pose Estimation as an auxiliary
learning task to assist the VI-ReID task in an end-to-end framework. By jointly
training these two tasks in a mutually beneficial manner, our model learns
higher quality modality-shared and ID-related features. On top of it, the
learnings of global features and local features are seamlessly synchronized by
Hierarchical Feature Constraint (HFC), where the former supervises the latter
using the knowledge distillation strategy. Experimental results on two
benchmark VI-ReID datasets show that the proposed method consistently improves
state-of-the-art methods by significant margins. Specifically, our method
achieves nearly 20$\%$ mAP improvements against the state-of-the-art method on
the RegDB dataset. Our intriguing findings highlight the usage of auxiliary
task learning in VI-ReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Estimation from EEG -- A Dual Deep Learning Approach Combined with Saliency. (arXiv:2201.03891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03891">
<div class="article-summary-box-inner">
<span><p>Emotion estimation is an active field of research that has an important
impact on the interaction between human and computer. Among the different
modality to assess emotion, electroencephalogram (EEG) representing the
electrical brain activity presented motivating results during the last decade.
Emotion estimation from EEG could help in the diagnosis or rehabilitation of
certain diseases. In this paper, we propose a dual method considering the
physiological knowledge defined by specialists combined with novel deep
learning (DL) models initially dedicated to computer vision. The joint learning
has been enhanced with model saliency analysis. To present a global approach,
the model has been evaluated on four publicly available datasets and achieves
similar results to the state-of-theart approaches and outperforming results for
two of the proposed datasets with a lower standard deviation that reflects
higher stability. For sake of reproducibility, the codes and models proposed in
this paper are available at github.com/VDelv/Emotion-EEG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Is My Mind (looking at)? Predicting Visual Attention from Brain Activity. (arXiv:2201.03902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03902">
<div class="article-summary-box-inner">
<span><p>Visual attention estimation is an active field of research at the crossroads
of different disciplines: computer vision, artificial intelligence and
medicine. One of the most common approaches to estimate a saliency map
representing attention is based on the observed images. In this paper, we show
that visual attention can be retrieved from EEG acquisition. The results are
comparable to traditional predictions from observed images, which is of great
interest. For this purpose, a set of signals has been recorded and different
models have been developed to study the relationship between visual attention
and brain activity. The results are encouraging and comparable with other
approaches estimating attention with other modalities. The codes and dataset
considered in this paper have been made available at
\url{https://figshare.com/s/3e353bd1c621962888ad} to promote research in the
field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Extraction Framework based on Contrastive Learning with Adaptive Positive and Negative Samples. (arXiv:2201.03942v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03942">
<div class="article-summary-box-inner">
<span><p>In this study, we propose a feature extraction framework based on contrastive
learning with adaptive positive and negative samples (CL-FEFA) that is suitable
for unsupervised, supervised, and semi-supervised single-view feature
extraction. CL-FEFA constructs adaptively the positive and negative samples
from the results of feature extraction, which makes it more appropriate and
accurate. Thereafter, the discriminative features are re extracted to according
to InfoNCE loss based on previous positive and negative samples, which will
make the intra-class samples more compact and the inter-class samples more
dispersed. At the same time, using the potential structure information of
subspace samples to dynamically construct positive and negative samples can
make our framework more robust to noisy data. Furthermore, CL-FEFA considers
the mutual information between positive samples, that is, similar samples in
potential structures, which provides theoretical support for its advantages in
feature extraction. The final numerical experiments prove that the proposed
framework has a strong advantage over the traditional feature extraction
methods and contrastive learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering. (arXiv:2201.03965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03965">
<div class="article-summary-box-inner">
<span><p>In recent years, multi-modal transformers have shown significant progress in
Vision-Language tasks, such as Visual Question Answering (VQA), outperforming
previous architectures by a considerable margin. This improvement in VQA is
often attributed to the rich interactions between vision and language streams.
In this work, we investigate the efficacy of co-attention transformer layers in
helping the network focus on relevant regions while answering the question. We
generate visual attention maps using the question-conditioned image attention
scores in these co-attention layers. We evaluate the effect of the following
critical components on visual attention of a state-of-the-art VQA model: (i)
number of object region proposals, (ii) question part of speech (POS) tags,
(iii) question semantics, (iv) number of co-attention layers, and (v) answer
accuracy. We compare the neural network attention maps against human attention
maps both qualitatively and quantitatively. Our findings indicate that
co-attention transformer modules are crucial in attending to relevant regions
of the image given a question. Importantly, we observe that the semantic
meaning of the question is not what drives visual attention, but specific
keywords in the question do. Our work sheds light on the function and
interpretation of co-attention transformer layers, highlights gaps in current
networks, and can guide the development of future VQA models and networks that
simultaneously process visual and language streams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Representations Learning Based on Mutual Information Maximization and Minimization and Identity Embedding for Multimodal Sentiment Analysis. (arXiv:2201.03969v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03969">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis (MSA) is a fundamental complex research problem
due to the heterogeneity gap between different modalities and the ambiguity of
human emotional expression. Although there have been many successful attempts
to construct multimodal representations for MSA, there are still two challenges
to be addressed: 1) A more robust multimodal representation needs to be
constructed to bridge the heterogeneity gap and cope with the complex
multimodal interactions, and 2) the contextual dynamics must be modeled
effectively throughout the information flow. In this work, we propose a
multimodal representation model based on Mutual information Maximization and
Minimization and Identity Embedding (MMMIE). We combine mutual information
maximization between modal pairs, and mutual information minimization between
input data and corresponding features to mine the modal-invariant and
task-related information. Furthermore, Identity Embedding is proposed to prompt
the downstream network to perceive the contextual information. Experimental
results on two public datasets demonstrate the effectiveness of the proposed
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image quality measurements and denoising using Fourier Ring Correlations. (arXiv:2201.03992v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03992">
<div class="article-summary-box-inner">
<span><p>Image quality is a nebulous concept with different meanings to different
people. To quantify image quality a relative difference is typically calculated
between a corrupted image and a ground truth image. But what metric should we
use for measuring this difference? Ideally, the metric should perform well for
both natural and scientific images. The structural similarity index (SSIM) is a
good measure for how humans perceive image similarities, but is not sensitive
to differences that are scientifically meaningful in microscopy. In electron
and super-resolution microscopy, the Fourier Ring Correlation (FRC) is often
used, but is little known outside of these fields. Here we show that the FRC
can equally well be applied to natural images, e.g. the Google Open Images
dataset. We then define a loss function based on the FRC, show that it is
analytically differentiable, and use it to train a U-net for denoising of
images. This FRC-based loss function allows the network to train faster and
achieve similar or better results than when using L1- or L2- based losses. We
also investigate the properties and limitations of neural network denoising
with the FRC analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Home-Built Metrology to Analyze Oral Fluid Droplets and Quantify the Efficacy of Masks. (arXiv:2201.03993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03993">
<div class="article-summary-box-inner">
<span><p>Wearing masks is crucial to preventing the spread of potentially
pathogen-containing droplets, especially amidst the COVID-19 pandemic. However,
not all face coverings are equally effective and most experiments evaluating
mask efficacy are very expensive and complex to operate. In this work, a novel,
home-built, low-cost, and accurate metrology to visualize orally-generated
fluid droplets has been developed. The project includes setup optimization,
data collection, data analysis, and applications. The final materials chosen
were quinine-containing tonic water, 397-402 nm wavelength UV tube lights, an
iPhone and tripod, string, and a spray bottle. The experiment took place in a
dark closet with a dark background. During data collection, the test subject
first wets their mouth with an ingestible fluorescent liquid (tonic water) and
speaks, sneezes, or coughs under UV darklight. The fluorescence from the tonic
water droplets generated can be visualized, recorded by an iPhone 8+ camera in
slo-mo (240 fps), and analyzed. The software VLC is used for frame separation
and Fiji/ImageJ is used for image processing and analysis. The dependencies of
oral fluid droplet generation and propagation on different phonics, the
loudness of speech, and the type of expiratory event were studied in detail and
established using the metrology developed. The efficacy of different types of
masks was evaluated and correlated with fabric microstructures. All masks
blocked droplets to varying extent. Masks with smaller-sized pores and thicker
material were found to block the most droplets. This low-cost technique can be
easily constructed at home using materials that total to a cost of less than
$50. Despite the minimal cost, the method is very accurate and the data is
quantifiable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity-based Gray-box Adversarial Attack Against Deep Face Recognition. (arXiv:2201.04011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04011">
<div class="article-summary-box-inner">
<span><p>The majority of adversarial attack techniques perform well against deep face
recognition when the full knowledge of the system is revealed
(\emph{white-box}). However, such techniques act unsuccessfully in the gray-box
setting where the face templates are unknown to the attackers. In this work, we
propose a similarity-based gray-box adversarial attack (SGADV) technique with a
newly developed objective function. SGADV utilizes the dissimilarity score to
produce the optimized adversarial example, i.e., similarity-based adversarial
attack. This technique applies to both white-box and gray-box attacks against
authentication systems that determine genuine or imposter users using the
dissimilarity score. To validate the effectiveness of SGADV, we conduct
extensive experiments on face datasets of LFW, CelebA, and CelebA-HQ against
deep face recognition models of FaceNet and InsightFace in both white-box and
gray-box settings. The results suggest that the proposed method significantly
outperforms the existing adversarial attack techniques in the gray-box setting.
We hence summarize that the similarity-base approaches to develop the
adversarial example could satisfactorily cater to the gray-box attack scenarios
for de-authentication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Captcha Attack:Turning Captchas Against Humanity. (arXiv:2201.04014v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04014">
<div class="article-summary-box-inner">
<span><p>Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
</p>
<p>In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04019">
<div class="article-summary-box-inner">
<span><p>The recently proposed MaskFormer \cite{maskformer} gives a refreshed
perspective on the task of semantic segmentation: it shifts from the popular
pixel-level classification paradigm to a mask-level classification method. In
essence, it generates paired probabilities and masks corresponding to category
segments and combines them during inference for the segmentation maps. The
segmentation quality thus relies on how well the queries can capture the
semantic information for categories and their spatial locations within the
images. In our study, we find that per-mask classification decoder on top of a
single-scale feature is not effective enough to extract reliable probability or
mask. To mine for rich semantic information across the feature pyramid, we
propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask
approach semantic segmentation on top of multi-scale features. To efficiently
utilize image features of different resolutions without incurring too much
computational overheads, PFT uses a multi-scale transformer decoder with
cross-scale inter-query attention to exchange complimentary information.
Extensive experimental evaluations and ablations demonstrate the efficacy of
our framework. In particular, we achieve a 3.2 mIoU improvement on COCO-Stuff
10K dataset with ResNet-101c compared to MaskFormer. Besides, on ADE20K
validation set, our result with Swin-B backbone matches that of MaskFormer's
with a much larger Swin-L backbone in both single-scale and multi-scale
inference, achieving 54.1 mIoU and 55.3 mIoU respectively. Using a Swin-L
backbone, we achieve 56.0 mIoU single-scale result on the ADE20K validation set
and 57.2 multi-scale result, obtaining state-of-the-art performance on the
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimization Planning for 3D ConvNets. (arXiv:2201.04021v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04021">
<div class="article-summary-box-inner">
<span><p>It is not trivial to optimally learn a 3D Convolutional Neural Networks (3D
ConvNets) due to high complexity and various options of the training scheme.
The most common hand-tuning process starts from learning 3D ConvNets using
short video clips and then is followed by learning long-term temporal
dependency using lengthy clips, while gradually decaying the learning rate from
high to low as training progresses. The fact that such process comes along with
several heuristic settings motivates the study to seek an optimal "path" to
automate the entire training. In this paper, we decompose the path into a
series of training "states" and specify the hyper-parameters, e.g., learning
rate and the length of input clips, in each state. The estimation of the knee
point on the performance-epoch curve triggers the transition from one state to
another. We perform dynamic programming over all the candidate states to plan
the optimal permutation of states, i.e., optimization path. Furthermore, we
devise a new 3D ConvNets with a unique design of dual-head classifier to
improve spatial and temporal discrimination. Extensive experiments on seven
public video recognition benchmarks demonstrate the advantages of our proposal.
With the optimization planning, our 3D ConvNets achieves superior results when
comparing to the state-of-the-art recognition methods. More remarkably, we
obtain the top-1 accuracy of 80.5% and 82.7% on Kinetics-400 and Kinetics-600
datasets, respectively. Source code is available at
https://github.com/ZhaofanQiu/Optimization-Planning-for-3D-ConvNets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Condensing a Sequence to One Informative Frame for Video Recognition. (arXiv:2201.04022v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04022">
<div class="article-summary-box-inner">
<span><p>Video is complex due to large variations in motion and rich content in
fine-grained visual details. Abstracting useful information from such
information-intensive media requires exhaustive computing resources. This paper
studies a two-step alternative that first condenses the video sequence to an
informative "frame" and then exploits off-the-shelf image recognition system on
the synthetic frame. A valid question is how to define "useful information" and
then distill it from a video sequence down to one synthetic frame. This paper
presents a novel Informative Frame Synthesis (IFS) architecture that
incorporates three objective tasks, i.e., appearance reconstruction, video
categorization, motion estimation, and two regularizers, i.e., adversarial
learning, color consistency. Each task equips the synthetic frame with one
ability, while each regularizer enhances its visual quality. With these, by
jointly learning the frame synthesis in an end-to-end manner, the generated
frame is expected to encapsulate the required spatio-temporal information
useful for video analysis. Extensive experiments are conducted on the
large-scale Kinetics dataset. When comparing to baseline methods that map video
sequence to a single image, IFS shows superior performance. More remarkably,
IFS consistently demonstrates evident improvements on image-based 2D networks
and clip-based 3D networks, and achieves comparable performance with the
state-of-the-art methods with less computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Video Representation Learning with Multi-Faceted Integration. (arXiv:2201.04023v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04023">
<div class="article-summary-box-inner">
<span><p>Video content is multifaceted, consisting of objects, scenes, interactions or
actions. The existing datasets mostly label only one of the facets for model
training, resulting in the video representation that biases to only one facet
depending on the training dataset. There is no study yet on how to learn a
video representation from multifaceted labels, and whether multifaceted
information is helpful for video representation learning. In this paper, we
propose a new learning framework, MUlti-Faceted Integration (MUFI), to
aggregate facets from different datasets for learning a representation that
could reflect the full spectrum of video content. Technically, MUFI formulates
the problem as visual-semantic embedding learning, which explicitly maps video
representation into a rich semantic embedding space, and jointly optimizes
video representation from two perspectives. One is to capitalize on the
intra-facet supervision between each video and its own label descriptions, and
the second predicts the "semantic representation" of each video from the facets
of other datasets as the inter-facet supervision. Extensive experiments
demonstrate that learning 3D CNN via our MUFI framework on a union of four
large-scale video datasets plus two image datasets leads to superior capability
of video representation. The pre-learnt 3D CNN with MUFI also shows clear
improvements over other approaches on several downstream video applications.
More remarkably, MUFI achieves 98.1%/80.9% on UCF101/HMDB51 for action
recognition and 101.5% in terms of CIDEr-D score on MSVD for video captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Director: An Event-Driven Directing System for Live Broadcasting. (arXiv:2201.04024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04024">
<div class="article-summary-box-inner">
<span><p>Live video broadcasting normally requires a multitude of skills and expertise
with domain knowledge to enable multi-camera productions. As the number of
cameras keep increasing, directing a live sports broadcast has now become more
complicated and challenging than ever before. The broadcast directors need to
be much more concentrated, responsive, and knowledgeable, during the
production. To relieve the directors from their intensive efforts, we develop
an innovative automated sports broadcast directing system, called Smart
Director, which aims at mimicking the typical human-in-the-loop broadcasting
process to automatically create near-professional broadcasting programs in
real-time by using a set of advanced multi-view video analysis algorithms.
Inspired by the so-called "three-event" construction of sports broadcast, we
build our system with an event-driven pipeline consisting of three consecutive
novel components: 1) the Multi-view Event Localization to detect events by
modeling multi-view correlations, 2) the Multi-view Highlight Detection to rank
camera views by the visual importance for view selection, 3) the
Auto-Broadcasting Scheduler to control the production of broadcasting videos.
To our best knowledge, our system is the first end-to-end automated directing
system for multi-camera sports broadcasting, completely driven by the semantic
understanding of sports events. It is also the first system to solve the novel
problem of multi-view joint event detection by cross-view relation modeling. We
conduct both objective and subjective evaluations on a real-world multi-camera
soccer dataset, which demonstrate the quality of our auto-generated videos is
comparable to that of the human-directed. Thanks to its faster response, our
system is able to capture more fast-passing and short-duration events which are
usually missed by human directors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training. (arXiv:2201.04026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04026">
<div class="article-summary-box-inner">
<span><p>Vision-language pre-training has been an emerging and fast-developing
research topic, which transfers multi-modal knowledge from rich-resource
pre-training task to limited-resource downstream tasks. Unlike existing works
that predominantly learn a single generic encoder, we present a pre-trainable
Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language
perception (e.g., visual question answering) and generation (e.g., image
captioning). Uni-EDEN is a two-stream Transformer based structure, consisting
of three modules: object and sentence encoders that separately learns the
representations of each modality, and sentence decoder that enables both
multi-modal reasoning and sentence generation via inter-modal interaction.
Considering that the linguistic representations of each image can span
different granularities in this hierarchy including, from simple to
comprehensive, individual label, a phrase, and a natural sentence, we pre-train
Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object
Classification (MOC), Masked Region Phrase Generation (MRPG), Image-Sentence
Matching (ISM), and Masked Sentence Generation (MSG). In this way, Uni-EDEN is
endowed with the power of both multi-modal representation extraction and
language modeling. Extensive experiments demonstrate the compelling
generalizability of Uni-EDEN by fine-tuning it to four vision-language
perception and generation downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representing Videos as Discriminative Sub-graphs for Action Recognition. (arXiv:2201.04027v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04027">
<div class="article-summary-box-inner">
<span><p>Human actions are typically of combinatorial structures or patterns, i.e.,
subjects, objects, plus spatio-temporal interactions in between. Discovering
such structures is therefore a rewarding way to reason about the dynamics of
interactions and recognize the actions. In this paper, we introduce a new
design of sub-graphs to represent and encode the discriminative patterns of
each action in the videos. Specifically, we present MUlti-scale Sub-graph
LEarning (MUSLE) framework that novelly builds space-time graphs and clusters
the graphs into compact sub-graphs on each scale with respect to the number of
nodes. Technically, MUSLE produces 3D bounding boxes, i.e., tubelets, in each
video clip, as graph nodes and takes dense connectivity as graph edges between
tubelets. For each action category, we execute online clustering to decompose
the graph into sub-graphs on each scale through learning Gaussian Mixture Layer
and select the discriminative sub-graphs as action prototypes for recognition.
Extensive experiments are conducted on both Something-Something V1 &amp; V2 and
Kinetics-400 datasets, and superior results are reported when comparing to
state-of-the-art methods. More remarkably, our MUSLE achieves to-date the best
reported accuracy of 65.0% on Something-Something V2 validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-Focused Contrastive Learning of Video Representations. (arXiv:2201.04029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04029">
<div class="article-summary-box-inner">
<span><p>Motion, as the most distinct phenomenon in a video to involve the changes
over time, has been unique and critical to the development of video
representation learning. In this paper, we ask the question: how important is
the motion particularly for self-supervised video representation learning. To
this end, we compose a duet of exploiting the motion for data augmentation and
feature learning in the regime of contrastive learning. Specifically, we
present a Motion-focused Contrastive Learning (MCL) method that regards such
duet as the foundation. On one hand, MCL capitalizes on optical flow of each
frame in a video to temporally and spatially sample the tubelets (i.e.,
sequences of associated frame patches across time) as data augmentations. On
the other hand, MCL further aligns gradient maps of the convolutional layers to
optical flow maps from spatial, temporal and spatio-temporal perspectives, in
order to ground motion information in feature learning. Extensive experiments
conducted on R(2+1)D backbone demonstrate the effectiveness of our MCL. On
UCF101, the linear classifier trained on the representations learnt by MCL
achieves 81.91% top-1 accuracy, outperforming ImageNet supervised pre-training
by 6.78%. On Kinetics-400, MCL achieves 66.62% top-1 accuracy under the linear
protocol. Code is available at
https://github.com/YihengZhang-CV/MCL-Motion-Focused-Contrastive-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobilePhys: Personalized Mobile Camera-Based Contactless Physiological Sensing. (arXiv:2201.04039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04039">
<div class="article-summary-box-inner">
<span><p>Camera-based contactless photoplethysmography refers to a set of popular
techniques for contactless physiological measurement. The current
state-of-the-art neural models are typically trained in a supervised manner
using videos accompanied by gold standard physiological measurements. However,
they often generalize poorly out-of-domain examples (i.e., videos that are
unlike those in the training set). Personalizing models can help improve model
generalizability, but many personalization techniques still require some gold
standard data. To help alleviate this dependency, in this paper, we present a
novel mobile sensing system called MobilePhys, the first mobile personalized
remote physiological sensing system, that leverages both front and rear cameras
on a smartphone to generate high-quality self-supervised labels for training
personalized contactless camera-based PPG models. To evaluate the robustness of
MobilePhys, we conducted a user study with 39 participants who completed a set
of tasks under different mobile devices, lighting conditions/intensities,
motion tasks, and skin types. Our results show that MobilePhys significantly
outperforms the state-of-the-art on-device supervised training and few-shot
adaptation methods. Through extensive user studies, we further examine how does
MobilePhys perform in complex real-world settings. We envision that calibrated
or personalized camera-based contactless PPG models generated from our proposed
dual-camera mobile sensing system will open the door for numerous future
applications such as smart mirrors, fitness and mobile health applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lightweight Neural Animation : Exploration of Neural Network Pruning in Mixture of Experts-based Animation Models. (arXiv:2201.04042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04042">
<div class="article-summary-box-inner">
<span><p>In the past few years, neural character animation has emerged and offered an
automatic method for animating virtual characters. Their motion is synthesized
by a neural network. Controlling this movement in real time with a user-defined
control signal is also an important task in video games for example. Solutions
based on fully-connected layers (MLPs) and Mixture-of-Experts (MoE) have given
impressive results in generating and controlling various movements with
close-range interactions between the environment and the virtual character.
However, a major shortcoming of fully-connected layers is their computational
and memory cost which may lead to sub-optimized solution. In this work, we
apply pruning algorithms to compress an MLP- MoE neural network in the context
of interactive character animation, which reduces its number of parameters and
accelerates its computation time with a trade-off between this acceleration and
the synthesized motion quality. This work demonstrates that, with the same
number of experts and parameters, the pruned model produces less motion
artifacts than the dense model and the learned high-level motion features are
similar for both
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification of chicken egg fertility using SVM classifier based on first-order statistical feature extraction. (arXiv:2201.04063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04063">
<div class="article-summary-box-inner">
<span><p>This study aims to identify chicken eggs fertility using the support vector
machine (SVM) classifier method. The classification basis used the first-order
statistical (FOS) parameters as feature extraction in the identification
process. This research was developed based on the process's identification
process, which is still manual (conventional). Although currently there are
many technologies in the identification process, they still need development.
Thus, this research is one of the developments in the field of image processing
technology. The sample data uses datasets from previous studies with a total of
100 egg images. The egg object in the image is a single object. From these
data, the classification of each fertile and infertile egg is 50 image data.
Chicken egg image data became input in image processing, with the initial
process is segmentation. This initial segmentation aims to get the cropped
image according to the object. The cropped image is repaired using image
preprocessing with grayscaling and image enhancement methods. This method
(image enhancement) used two combination methods: contrast limited adaptive
histogram equalization (CLAHE) and histogram equalization (HE). The improved
image becomes the input for feature extraction using the FOS method. The FOS
uses five parameters, namely mean, entropy, variance, skewness, and kurtosis.
The five parameters entered into the SVM classifier method to identify the
fertility of chicken eggs. The results of these experiments, the method
proposed in the identification process has a success percentage of 84.57%.
Thus, the implementation of this method can be used as a reference for future
research improvements. In addition, it may be possible to use a second-order
feature extraction method to improve its accuracy and improve supervised
learning for classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Denoise Raw Mobile UI Layouts for ImprovingDatasets at Scale. (arXiv:2201.04100v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04100">
<div class="article-summary-box-inner">
<span><p>The layout of a mobile screen is a critical data source for UI designresearch
and semantic understanding of the screen. However, UIlayouts in existing
datasets are often noisy, have mismatches withtheir visual representation, or
consists of generic or app-specifictypes that are difficult to analyze and
model. In this paper, wepropose the CLAY pipeline that uses a deep learning
approach fordenoising UI layouts, allowing us to automatically improve
existingmobile UI layout datasets at scale. Our pipeline takes both
thescreenshot and the raw UI layout, and annotates the raw layout byremoving
incorrect nodes and assigning a semantically meaningfultype to each node. To
experiment with our data-cleaning pipeline,we create the CLAY dataset of 59,555
human-annotated screenlayouts, based on screenshots and raw layouts from Rico,
a publicmobile UI corpus. Our deep models achieve high accuracy withF1 scores
of 82.7% for detecting layout objects that do not have avalid visual
representation and 85.9% for recognizing object types,which significantly
outperforms a heuristic baseline. Our work laysa foundation for creating
large-scale high quality UI layout datasetsfor data-driven mobile UI research
and reduces the need of manuallabeling efforts that are prohibitively
expensive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DM-VIO: Delayed Marginalization Visual-Inertial Odometry. (arXiv:2201.04114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04114">
<div class="article-summary-box-inner">
<span><p>We present DM-VIO, a monocular visual-inertial odometry system based on two
novel techniques called delayed marginalization and pose graph bundle
adjustment. DM-VIO performs photometric bundle adjustment with a dynamic weight
for visual residuals. We adopt marginalization, which is a popular strategy to
keep the update time constrained, but it cannot easily be reversed, and
linearization points of connected variables have to be fixed. To overcome this
we propose delayed marginalization: The idea is to maintain a second factor
graph, where marginalization is delayed. This allows us to later readvance this
delayed graph, yielding an updated marginalization prior with new and
consistent linearization points. In addition, delayed marginalization enables
us to inject IMU information into already marginalized states. This is the
foundation of the proposed pose graph bundle adjustment, which we use for IMU
initialization. In contrast to prior works on IMU initialization, it is able to
capture the full photometric uncertainty, improving the scale estimation. In
order to cope with initially unobservable scale, we continue to optimize scale
and gravity direction in the main system after IMU initialization is complete.
We evaluate our system on the EuRoC, TUM-VI, and 4Seasons datasets, which
comprise flying drone, large-scale handheld, and automotive scenarios. Thanks
to the proposed IMU initialization, our system exceeds the state of the art in
visual-inertial odometry, even outperforming stereo-inertial methods while
using only a single camera and IMU. The code will be published at
<a href="http://vision.in.tum.de/dm-vio">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Defense of the Unitary Scalarization for Deep Multi-Task Learning. (arXiv:2201.04122v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04122">
<div class="article-summary-box-inner">
<span><p>Recent multi-task learning research argues against unitary scalarization,
where training simply minimizes the sum of the task losses. Several ad-hoc
multi-task optimization algorithms have instead been proposed, inspired by
various hypotheses about what makes multi-task settings difficult. The majority
of these optimizers require per-task gradients, and introduce significant
memory, runtime, and implementation overhead. We present a theoretical analysis
suggesting that many specialized multi-task optimizers can be interpreted as
forms of regularization. Moreover, we show that, when coupled with standard
regularization and stabilization techniques from single-task learning, unitary
scalarization matches or improves upon the performance of complex multi-task
optimizers in both supervised and reinforcement learning settings. We believe
our results call for a critical reevaluation of recent research in the area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">gDNA: Towards Generative Detailed Neural Avatars. (arXiv:2201.04123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04123">
<div class="article-summary-box-inner">
<span><p>To make 3D human avatars widely available, we must be able to generate a
variety of 3D virtual humans with varied identities and shapes in arbitrary
poses. This task is challenging due to the diversity of clothed body shapes,
their complex articulations, and the resulting rich, yet stochastic geometric
detail in clothing. Hence, current methods to represent 3D people do not
provide a full generative model of people in clothing. In this paper, we
propose a novel method that learns to generate detailed 3D shapes of people in
a variety of garments with corresponding skinning weights. Specifically, we
devise a multi-subject forward skinning module that is learned from only a few
posed, un-rigged scans per subject. To capture the stochastic nature of
high-frequency details in garments, we leverage an adversarial loss formulation
that encourages the model to capture the underlying statistics. We provide
empirical evidence that this leads to realistic generation of local details
such as wrinkles. We show that our model is able to generate natural human
avatars wearing diverse and detailed clothing. Furthermore, we show that our
method can be used on the task of fitting human models to raw scans,
outperforming the previous state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video. (arXiv:2201.04127v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04127">
<div class="article-summary-box-inner">
<span><p>We introduce a free-viewpoint rendering method -- HumanNeRF -- that works on
a given monocular video of a human performing complex body motions, e.g. a
video from YouTube. Our method enables pausing the video at any frame and
rendering the subject from arbitrary new camera viewpoints or even a full
360-degree camera path for that particular frame and body pose. This task is
particularly challenging, as it requires synthesizing photorealistic details of
the body, as seen from various camera angles that may not exist in the input
video, as well as synthesizing fine details such as cloth folds and facial
appearance. Our method optimizes for a volumetric representation of the person
in a canonical T-pose, in concert with a motion field that maps the estimated
canonical representation to every frame of the video via backward warps. The
motion field is decomposed into skeletal rigid and non-rigid motions, produced
by deep networks. We show significant performance improvements over prior work,
and compelling examples of free-viewpoint renderings from monocular video of
moving humans in challenging uncontrolled capture scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent 3D Attentional Networks for End-to-End Active Object Recognition. (arXiv:1610.04308v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1610.04308">
<div class="article-summary-box-inner">
<span><p>Active vision is inherently attention-driven: The agent actively selects
views to attend in order to fast achieve the vision task while improving its
internal representation of the scene being observed. Inspired by the recent
success of attention-based models in 2D vision tasks based on single RGB
images, we propose to address the multi-view depth-based active object
recognition using attention mechanism, through developing an end-to-end
recurrent 3D attentional network. The architecture takes advantage of a
recurrent neural network (RNN) to store and update an internal representation.
Our model, trained with 3D shape datasets, is able to iteratively attend to the
best views targeting an object of interest for recognizing it. To realize 3D
view selection, we derive a 3D spatial transformer network which is
differentiable for training with backpropagation, achieving much faster
convergence than the reinforcement learning employed by most existing
attention-based models. Experiments show that our method, with only depth
input, achieves state-of-the-art next-best-view performance in time efficiency
and recognition accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate. (arXiv:1910.04858v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.04858">
<div class="article-summary-box-inner">
<span><p>Uncertainty estimation is an essential step in the evaluation of the
robustness for deep learning models in computer vision, especially when applied
in risk-sensitive areas. However, most state-of-the-art deep learning models
either fail to obtain uncertainty estimation or need significant modification
(e.g., formulating a proper Bayesian treatment) to obtain it. Most previous
methods are not able to take an arbitrary model off the shelf and generate
uncertainty estimation without retraining or redesigning it. To address this
gap, we perform a systematic exploration into training-free uncertainty
estimation for dense regression, an unrecognized yet important problem, and
provide a theoretical construction justifying such estimations. We propose
three simple and scalable methods to analyze the variance of outputs from a
trained network under tolerable perturbations: infer-transformation,
infer-noise, and infer-dropout. They operate solely during the inference,
without the need to re-train, re-design, or fine-tune the models, as typically
required by state-of-the-art uncertainty estimation methods. Surprisingly, even
without involving such perturbations in training, our methods produce
comparable or even better uncertainty estimation when compared to
training-required state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition. (arXiv:2004.12349v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12349">
<div class="article-summary-box-inner">
<span><p>Recognizing objects and scenes are two challenging but essential tasks in
image understanding. In particular, the use of RGB-D sensors in handling these
tasks has emerged as an important area of focus for better visual
understanding. Meanwhile, deep neural networks, specifically convolutional
neural networks (CNNs), have become widespread and have been applied to many
visual tasks by replacing hand-crafted features with effective deep features.
However, it is an open problem how to exploit deep features from a multi-layer
CNN model effectively. In this paper, we propose a novel two-stage framework
that extracts discriminative feature representations from multi-modal RGB-D
images for object and scene recognition tasks. In the first stage, a pretrained
CNN model has been employed as a backbone to extract visual features at
multiple levels. The second stage maps these features into high level
representations with a fully randomized structure of recursive neural networks
(RNNs) efficiently. To cope with the high dimensionality of CNN activations, a
random weighted pooling scheme has been proposed by extending the idea of
randomness in RNNs. Multi-modal fusion has been performed through a soft voting
approach by computing weights based on individual recognition confidences (i.e.
SVM scores) of RGB and depth streams separately. This produces consistent class
label estimation in final RGB-D classification performance. Extensive
experiments verify that fully randomized structure in RNN stage encodes CNN
activations to discriminative solid features successfully. Comparative
experimental results on the popular Washington RGB-D Object and SUN RGB-D Scene
datasets show that the proposed approach achieves superior or on-par
performance compared to state-of-the-art methods both in object and scene
recognition tasks. Code is available at
https://github.com/acaglayan/CNN_randRNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MED-TEX: Transferring and Explaining Knowledge with Less Data from Pretrained Medical Imaging Models. (arXiv:2008.02593v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.02593">
<div class="article-summary-box-inner">
<span><p>Deep learning methods usually require a large amount of training data and
lack interpretability. In this paper, we propose a novel knowledge distillation
and model interpretation framework for medical image classification that
jointly solves the above two issues. Specifically, to address the data-hungry
issue, a small student model is learned with less data by distilling knowledge
from a cumbersome pretrained teacher model. To interpret the teacher model and
assist the learning of the student, an explainer module is introduced to
highlight the regions of an input that are important for the predictions of the
teacher model. Furthermore, the joint framework is trained by a principled way
derived from the information-theoretic perspective. Our framework outperforms
on the knowledge distillation and model interpretation tasks compared to
state-of-the-art methods on a fundus dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex-valued Iris Recognition Network. (arXiv:2011.11198v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11198">
<div class="article-summary-box-inner">
<span><p>In this work, we design a fully complex-valued neural network for the task of
iris recognition. Unlike the problem of general object recognition, where
real-valued neural networks can be used to extract pertinent features, iris
recognition depends on the extraction of both phase and magnitude information
from the input iris texture in order to better represent its biometric content.
This necessitates the extraction and processing of phase information that
cannot be effectively handled by a real-valued neural network. In this regard,
we design a fully complex-valued neural network that can better capture the
multi-scale, multi-resolution, and multi-orientation phase and amplitude
features of the iris texture. We show a strong correspondence of the proposed
complex-valued iris recognition network with Gabor wavelets that are used to
generate the classical IrisCode; however, the proposed method enables a new
capability of automatic complex-valued feature learning that is tailored for
iris recognition. We conduct experiments on three benchmark datasets -
ND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit
of the proposed network for the task of iris recognition. We exploit
visualization schemes to convey how the complex-valued network, when compared
to standard real-valued networks, extracts fundamentally different features
from the iris texture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie. (arXiv:2103.04715v5 [stat.ME] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04715">
<div class="article-summary-box-inner">
<span><p>Since the seminal work of Venkatakrishnan et al. (2013), Plug &amp; Play (PnP)
methods have become ubiquitous in Bayesian imaging. These methods derive
Minimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for
inverse problems in imaging by combining an explicit likelihood function with a
prior that is implicitly defined by an image denoising algorithm. The PnP
algorithms proposed in the literature mainly differ in the iterative schemes
they use for optimisation or for sampling. In the case of optimisation schemes,
some recent works guarantee the convergence to a fixed point, albeit not
necessarily a MAP estimate. In the case of sampling schemes, to the best of our
knowledge, there is no known proof of convergence. There also remain important
open questions regarding whether the underlying Bayesian models and estimators
are well defined, well-posed, and have the basic regularity properties required
to support these numerical schemes. To address these limitations, this paper
develops theory, methods, and provably convergent algorithms for performing
Bayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA
(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;
and 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent
results on the quantitative convergence of Markov chains, we establish detailed
convergence guarantees for these two algorithms under realistic assumptions on
the denoising operators used, with special attention to denoisers based on deep
neural networks. We also show that these algorithms approximately target a
decision-theoretically optimal Bayesian model that is well-posed. The proposed
algorithms are demonstrated on several canonical problems such as image
deblurring, inpainting, and denoising, where they are used for point estimation
as well as for uncertainty visualisation and quantification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation. (arXiv:2103.14304v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14304">
<div class="article-summary-box-inner">
<span><p>Despite the great progress in 3D human pose estimation from videos, it is
still an open problem to take full advantage of a redundant 2D pose sequence to
learn representative representations for generating one 3D pose. To this end,
we propose an improved Transformer-based architecture, called Strided
Transformer, which simply and effectively lifts a long sequence of 2D joint
locations to a single 3D pose. Specifically, a Vanilla Transformer Encoder
(VTE) is adopted to model long-range dependencies of 2D pose sequences. To
reduce the redundancy of the sequence, fully-connected layers in the
feed-forward network of VTE are replaced with strided convolutions to
progressively shrink the sequence length and aggregate information from local
contexts. The modified VTE is termed as Strided Transformer Encoder (STE),
which is built upon the outputs of VTE. STE not only effectively aggregates
long-range information to a single-vector representation in a hierarchical
global and local fashion, but also significantly reduces the computation cost.
Furthermore, a full-to-single supervision scheme is designed at both full
sequence and single target frame scales applied to the outputs of VTE and STE,
respectively. This scheme imposes extra temporal smoothness constraints in
conjunction with the single target frame supervision and hence helps produce
smoother and more accurate 3D poses. The proposed Strided Transformer is
evaluated on two challenging benchmark datasets, Human3.6M and HumanEva-I, and
achieves state-of-the-art results with fewer parameters. Code and models are
available at \url{https://github.com/Vegetebird/StridedTransformer-Pose3D}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device. (arXiv:2104.09949v2 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09949">
<div class="article-summary-box-inner">
<span><p>Recently, there has been an explosive growth of mobile and embedded
applications using convolutional neural networks(CNNs). To alleviate their
excessive computational demands, developers have traditionally resorted to
cloud offloading, inducing high infrastructure costs and a strong dependence on
networking conditions. On the other end, the emergence of powerful SoCs is
gradually enabling on-device execution. Nonetheless, low- and mid-tier
platforms still struggle to run state-of-the-art CNNs sufficiently. In this
paper, we present DynO, a distributed inference framework that combines the
best of both worlds to address several challenges, such as device
heterogeneity, varying bandwidth and multi-objective requirements. Key
components that enable this are its novel CNN-specific data packing method,
which exploits the variability of precision needs in different parts of the CNN
when onloading computation, and its novel scheduler that jointly tunes the
partition point and transferred data precision at run time to adapt inference
to its execution environment. Quantitative evaluation shows that DynO
outperforms the current state-of-the-art, improving throughput by over an order
of magnitude over device-only execution and up to 7.9x over competing CNN
offloading systems, with up to 60x less data transferred.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BADet: Boundary-Aware 3D Object Detection from Point Clouds. (arXiv:2104.10330v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10330">
<div class="article-summary-box-inner">
<span><p>Currently, existing state-of-the-art 3D object detectors are in two-stage
paradigm. These methods typically comprise two steps: 1) Utilize a region
proposal network to propose a handful of high-quality proposals in a bottom-up
fashion. 2) Resize and pool the semantic features from the proposed regions to
summarize RoI-wise representations for further refinement. Note that these
RoI-wise representations in step 2) are considered individually as uncorrelated
entries when fed to following detection headers. Nevertheless, we observe these
proposals generated by step 1) offset from ground truth somehow, emerging in
local neighborhood densely with an underlying probability. Challenges arise in
the case where a proposal largely forsakes its boundary information due to
coordinate offset while existing networks lack corresponding information
compensation mechanism. In this paper, we propose $BADet$ for 3D object
detection from point clouds. Specifically, instead of refining each proposal
independently as previous works do, we represent each proposal as a node for
graph construction within a given cut-off threshold, associating proposals in
the form of local neighborhood graph, with boundary correlations of an object
being explicitly exploited. Besides, we devise a lightweight Region Feature
Aggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise
features with expanding receptive fields for more informative RoI-wise
representations. We validate BADet both on widely used KITTI Dataset and highly
challenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par
performance on KITTI 3D detection leaderboard and ranks $1^{st}$ on $Moderate$
difficulty of $Car$ category on KITTI BEV detection leaderboard. The source
code is available at https://github.com/rui-qian/BADet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nnDetection: A Self-configuring Method for Medical Object Detection. (arXiv:2106.00817v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00817">
<div class="article-summary-box-inner">
<span><p>Simultaneous localisation and categorization of objects in medical images,
also referred to as medical object detection, is of high clinical relevance
because diagnostic decisions often depend on rating of objects rather than e.g.
pixels. For this task, the cumbersome and iterative process of method
configuration constitutes a major research bottleneck. Recently, nnU-Net has
tackled this challenge for the task of image segmentation with great success.
Following nnU-Net's agenda, in this work we systematize and automate the
configuration process for medical object detection. The resulting
self-configuring method, nnDetection, adapts itself without any manual
intervention to arbitrary medical detection problems while achieving results en
par with or superior to the state-of-the-art. We demonstrate the effectiveness
of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 11
further medical object detection tasks on public data sets for comprehensive
method evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-like Relational Models for Activity Recognition in Video. (arXiv:2107.05319v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05319">
<div class="article-summary-box-inner">
<span><p>Video activity recognition by deep neural networks is impressive for many
classes. However, it falls short of human performance, especially for
challenging to discriminate activities. Humans differentiate these complex
activities by recognising critical spatio-temporal relations among explicitly
recognised objects and parts, for example, an object entering the aperture of a
container. Deep neural networks can struggle to learn such critical
relationships effectively. Therefore we propose a more human-like approach to
activity recognition, which interprets a video in sequential temporal phases
and extracts specific relationships among objects and hands in those phases.
Random forest classifiers are learnt from these extracted relationships. We
apply the method to a challenging subset of the something-something dataset and
achieve a more robust performance against neural network baselines on
challenging activities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2Looking: A Satellite Side-Looking Dataset for Building Change Detection. (arXiv:2107.09244v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09244">
<div class="article-summary-box-inner">
<span><p>Building-change detection underpins many important applications, especially
in the military and crisis-management domains. Recent methods used for change
detection have shifted towards deep learning, which depends on the quality of
its training data. The assembly of large-scale annotated satellite imagery
datasets is therefore essential for global building-change surveillance.
Existing datasets almost exclusively offer near-nadir viewing angles. This
limits the range of changes that can be detected. By offering larger
observation ranges, the scroll imaging mode of optical satellites presents an
opportunity to overcome this restriction. This paper therefore introduces
S2Looking, a building-change-detection dataset that contains large-scale
side-looking satellite images captured at various off-nadir angles. The dataset
consists of 5000 bitemporal image pairs of rural areas and more than 65,920
annotated instances of changes throughout the world. The dataset can be used to
train deep-learning-based change-detection algorithms. It expands upon existing
datasets by providing (1) larger viewing angles; (2) large illumination
variances; and (3) the added complexity of rural images. To facilitate {the}
use of the dataset, a benchmark task has been established, and preliminary
tests suggest that deep-learning algorithms find the dataset significantly more
challenging than the closest-competing near-nadir dataset, LEVIR-CD+. S2Looking
may therefore promote important advances in existing building-change-detection
algorithms. The dataset is available at https://github.com/S2Looking/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphFPN: Graph Feature Pyramid Network for Object Detection. (arXiv:2108.00580v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00580">
<div class="article-summary-box-inner">
<span><p>Feature pyramids have been proven powerful in image understanding tasks that
require multi-scale features. State-of-the-art methods for multi-scale feature
learning focus on performing feature interactions across space and scales using
neural networks with a fixed topology. In this paper, we propose graph feature
pyramid networks that are capable of adapting their topological structures to
varying intrinsic image structures and supporting simultaneous feature
interactions across all scales. We first define an image-specific superpixel
hierarchy for each input image to represent its intrinsic image structures. The
graph feature pyramid network inherits its structure from this superpixel
hierarchy. Contextual and hierarchical layers are designed to achieve feature
interactions within the same scale and across different scales. To make these
layers more powerful, we introduce two types of local channel attention for
graph neural networks by generalizing global channel attention for
convolutional neural networks. The proposed graph feature pyramid network can
enhance the multiscale features from a convolutional feature pyramid network.
We evaluate our graph feature pyramid network in the object detection task by
integrating it into the Faster R-CNN algorithm. The modified algorithm
outperforms not only previous state-of-the-art feature pyramid-based methods
with a clear margin but also other popular detection methods on both MS-COCO
2017 validation and test datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey on IQA. (arXiv:2109.00347v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00347">
<div class="article-summary-box-inner">
<span><p>Image quality assessment(IQA) is of increasing importance for image-based
applications. Its purpose is to establish a model that can replace humans for
accurately evaluating image quality. According to whether the reference image
is complete and available, image quality evaluation can be divided into three
categories: full-reference(FR), reduced-reference(RR), and non-reference(NR)
image quality assessment. Due to the vigorous development of deep learning and
the widespread attention of researchers, several non-reference image quality
assessment methods based on deep learning have been proposed in recent years,
and some have exceeded the performance of reduced -reference or even
full-reference image quality assessment models. This article will review the
concepts and metrics of image quality assessment and also video quality
assessment, briefly introduce some methods of full-reference and semi-reference
image quality assessment, and focus on the non-reference image quality
assessment methods based on deep learning. Then introduce the commonly used
synthetic database and real-world database. Finally, summarize and present
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Wide-area, Low-latency, and Power-efficient 6-DoF Pose Tracking System for Rigid Objects. (arXiv:2109.07428v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07428">
<div class="article-summary-box-inner">
<span><p>Position sensitive detectors (PSDs) offer possibility to track single active
marker's two (or three) degrees of freedom (DoF) position with a high accuracy,
while having a fast response time with high update frequency and low latency,
all using a very simple signal processing circuit. However they are not
particularly suitable for 6-DoF object pose tracking system due to lack of
orientation measurement, limited tracking range, and sensitivity to
environmental variation. We propose a novel 6-DoF pose tracking system for a
rigid object tracking requiring a single active marker. The proposed system
uses a stereo-based PSD pair and multiple Inertial Measurement Units (IMUs).
This is done based on a practical approach to identify and control the power of
Infrared-Light Emitting Diode (IR-LED) active markers, with an aim to increase
the tracking work space and reduce the power consumption. Our proposed tracking
system is validated with three different work space sizes and for static and
dynamic positional accuracy using robotic arm manipulator with three different
dynamic motion patterns. The results show that the static position
root-mean-square (RMS) error is 0.6mm. The dynamic position RMS error is
0.7-0.9mm. The orientation RMS error is between 0.04 and 0.9 degree at varied
dynamic motion. Overall, our proposed tracking system is capable of tracking a
rigid object pose with sub-millimeter accuracy at the mid range of the work
space and sub-degree accuracy for all work space under a lab setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attacking Video Recognition Models with Bullet-Screen Comments. (arXiv:2110.15629v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15629">
<div class="article-summary-box-inner">
<span><p>Recent research has demonstrated that Deep Neural Networks (DNNs) are
vulnerable to adversarial patches which introduce perceptible but localized
changes to the input. Nevertheless, existing approaches have focused on
generating adversarial patches on images, their counterparts in videos have
been less explored. Compared with images, attacking videos is much more
challenging as it needs to consider not only spatial cues but also temporal
cues. To close this gap, we introduce a novel adversarial attack in this paper,
the bullet-screen comment (BSC) attack, which attacks video recognition models
with BSCs. Specifically, adversarial BSCs are generated with a Reinforcement
Learning (RL) framework, where the environment is set as the target model and
the agent plays the role of selecting the position and transparency of each
BSC. By continuously querying the target models and receiving feedback, the
agent gradually adjusts its selection strategies in order to achieve a high
fooling rate with non-overlapping BSCs. As BSCs can be regarded as a kind of
meaningful patch, adding it to a clean video will not affect people' s
understanding of the video content, nor will arouse people' s suspicion. We
conduct extensive experiments to verify the effectiveness of the proposed
method. On both UCF-101 and HMDB-51 datasets, our BSC attack method can achieve
about 90\% fooling rate when attacking three mainstream video recognition
models, while only occluding \textless 8\% areas in the video. Our code is
available at https://github.com/kay-ck/BSC-attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning of Time-Frequency Attention Mechanism for Automatic Modulation Recognition. (arXiv:2111.03258v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03258">
<div class="article-summary-box-inner">
<span><p>Recent learning-based image classification and speech recognition approaches
make extensive use of attention mechanisms to achieve state-of-the-art
recognition power, which demonstrates the effectiveness of attention
mechanisms. Motivated by the fact that the frequency and time information of
modulated radio signals are crucial for modulation mode recognition, this paper
proposes a time-frequency attention mechanism for a convolutional neural
network (CNN)-based modulation recognition framework. The proposed
time-frequency attention module is designed to learn which channel, frequency
and time information is more meaningful in CNN for modulation recognition. We
analyze the effectiveness of the proposed time-frequency attention mechanism
and compare the proposed method with two existing learning-based methods.
Experiments on an open-source modulation recognition dataset show that the
recognition performance of the proposed framework is better than those of the
framework without time-frequency attention and existing learning-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NarrationBot and InfoBot: A Hybrid System for Automated Video Description. (arXiv:2111.03994v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03994">
<div class="article-summary-box-inner">
<span><p>Video accessibility is crucial for blind and low vision users for equitable
engagements in education, employment, and entertainment. Despite the
availability of professional and amateur services and tools, most
human-generated descriptions are expensive and time consuming. Moreover, the
rate of human-generated descriptions cannot match the speed of video
production. To overcome the increasing gaps in video accessibility, we
developed a hybrid system of two tools to 1) automatically generate
descriptions for videos and 2) provide answers or additional descriptions in
response to user queries on a video. Results from a mixed-methods study with 26
blind and low vision individuals show that our system significantly improved
user comprehension and enjoyment of selected videos when both tools were used
in tandem. In addition, participants reported no significant difference in
their ability to understand videos when presented with autogenerated
descriptions versus human-revised autogenerated descriptions. Our results
demonstrate user enthusiasm about the developed system and its promise for
providing customized access to videos. We discuss the limitations of the
current work and provide recommendations for the future development of
automated video description tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Diverse Model Selection for Accessible Transfer Learning. (arXiv:2111.06977v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06977">
<div class="article-summary-box-inner">
<span><p>With the preponderance of pretrained deep learning models available
off-the-shelf from model banks today, finding the best weights to fine-tune to
your use-case can be a daunting task. Several methods have recently been
proposed to find good models for transfer learning, but they either don't scale
well to large model banks or don't perform well on the diversity of
off-the-shelf models. Ideally the question we want to answer is, "given some
data and a source model, can you quickly predict the model's accuracy after
fine-tuning?" In this paper, we formalize this setting as "Scalable Diverse
Model Selection" and propose several benchmarks for evaluating on this task. We
find that existing model selection and transferability estimation methods
perform poorly here and analyze why this is the case. We then introduce simple
techniques to improve the performance and speed of these algorithms. Finally,
we iterate on existing methods to create PARC, which outperforms all other
methods on diverse model selection. We have released the benchmarks and method
code in hope to inspire future work in model selection for accessible transfer
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aerial Images Meet Crowdsourced Trajectories: A New Approach to Robust Road Extraction. (arXiv:2111.15119v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15119">
<div class="article-summary-box-inner">
<span><p>Land remote sensing analysis is a crucial research in earth science. In this
work, we focus on a challenging task of land analysis, i.e., automatic
extraction of traffic roads from remote sensing data, which has widespread
applications in urban development and expansion estimation. Nevertheless,
conventional methods either only utilized the limited information of aerial
images, or simply fused multimodal information (e.g., vehicle trajectories),
thus cannot well recognize unconstrained roads. To facilitate this problem, we
introduce a novel neural network framework termed Cross-Modal Message
Propagation Network (CMMPNet), which fully benefits the complementary different
modal data (i.e., aerial images and crowdsourced trajectories). Specifically,
CMMPNet is composed of two deep Auto-Encoders for modality-specific
representation learning and a tailor-designed Dual Enhancement Module for
cross-modal representation refinement. In particular, the complementary
information of each modality is comprehensively extracted and dynamically
propagated to enhance the representation of another modality. Extensive
experiments on three real-world benchmarks demonstrate the effectiveness of our
CMMPNet for robust road extraction benefiting from blending different modal
data, either using image and trajectory data or image and Lidar data. From the
experimental results, we observe that the proposed approach outperforms current
state-of-the-art methods by large margins.Our source code is resealed on the
project page \url{<a href="http://lingboliu.com/multimodal">this http URL</a> road extraction.html}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIVE: Evaluating the Human Interpretability of Visual Explanations. (arXiv:2112.03184v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03184">
<div class="article-summary-box-inner">
<span><p>As machine learning is increasingly applied to high-impact, high-risk
domains, there have been a number of new methods aimed at making AI models more
human interpretable. Despite the recent growth of interpretability work, there
is a lack of systematic evaluation of proposed techniques. In this work, we
propose a novel human evaluation framework HIVE (Human Interpretability of
Visual Explanations) for diverse interpretability methods in computer vision;
to the best of our knowledge, this is the first work of its kind. We argue that
human studies should be the gold standard in properly evaluating how
interpretable a method is to human users. While human studies are often avoided
due to challenges associated with cost, study design, and cross-method
comparison, we describe how our framework mitigates these issues and conduct
IRB-approved studies of four methods that represent the diversity of
interpretability works: GradCAM, BagNet, ProtoPNet, and ProtoTree. Our results
suggest that explanations (regardless of if they are actually correct) engender
human trust, yet are not distinct enough for users to distinguish between
correct and incorrect predictions. Lastly, we also open-source our framework to
enable future studies and to encourage more human-centered approaches to
interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SalFBNet: Learning Pseudo-Saliency Distribution via Feedback Convolutional Networks. (arXiv:2112.03731v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03731">
<div class="article-summary-box-inner">
<span><p>Feed-forward only convolutional neural networks (CNNs) may ignore intrinsic
relationships and potential benefits of feedback connections in vision tasks
such as saliency detection, despite their significant representation
capabilities. In this work, we propose a feedback-recursive convolutional
framework (SalFBNet) for saliency detection. The proposed feedback model can
learn abundant contextual representations by bridging a recursive pathway from
higher-level feature blocks to low-level layer. Moreover, we create a
large-scale Pseudo-Saliency dataset to alleviate the problem of data deficiency
in saliency detection. We first use the proposed feedback model to learn
saliency distribution from pseudo-ground-truth. Afterwards, we fine-tune the
feedback model on existing eye-fixation datasets. Furthermore, we present a
novel Selective Fixation and Non-Fixation Error (sFNE) loss to make proposed
feedback model better learn distinguishable eye-fixation-based features.
Extensive experimental results show that our SalFBNet with fewer parameters
achieves competitive results on the public saliency detection benchmarks, which
demonstrate the effectiveness of proposed feedback model and Pseudo-Saliency
data. Source codes and Pseudo-Saliency dataset can be found at
https://github.com/gqding/SalFBNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPRAL: Collaborative Panoptic-Regional Active Learning for Semantic Segmentation. (arXiv:2112.05975v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05975">
<div class="article-summary-box-inner">
<span><p>Acquiring the most representative examples via active learning (AL) can
benefit many data-dependent computer vision tasks by minimizing efforts of
image-level or pixel-wise annotations. In this paper, we propose a novel
Collaborative Panoptic-Regional Active Learning framework (CPRAL) to address
the semantic segmentation task. For a small batch of images initially sampled
with pixel-wise annotations, we employ panoptic information to initially select
unlabeled samples. Considering the class imbalance in the segmentation dataset,
we import a Regional Gaussian Attention module (RGA) to achieve
semantics-biased selection. The subset is highlighted by vote entropy and then
attended by Gaussian kernels to maximize the biased regions. We also propose a
Contextual Labels Extension (CLE) to boost regional annotations with contextual
attention guidance. With the collaboration of semantics-agnostic panoptic
matching and regionbiased selection and extension, our CPRAL can strike a
balance between labeling efforts and performance and compromise the semantics
distribution. We perform extensive experiments on Cityscapes and BDD10K
datasets and show that CPRAL outperforms the cutting-edge methods with
impressive results and less labeling proportion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRIDA -- Generative Feature Replay for Incremental Domain Adaptation. (arXiv:2112.14316v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14316">
<div class="article-summary-box-inner">
<span><p>We tackle the novel problem of incremental unsupervised domain adaptation
(IDA) in this paper. We assume that a labeled source domain and different
unlabeled target domains are incrementally observed with the constraint that
data corresponding to the current domain is only available at a time. The goal
is to preserve the accuracies for all the past domains while generalizing well
for the current domain. The IDA setup suffers due to the abrupt differences
among the domains and the unavailability of past data including the source
domain. Inspired by the notion of generative feature replay, we propose a novel
framework called Feature Replay based Incremental Domain Adaptation (FRIDA)
which leverages a new incremental generative adversarial network (GAN) called
domain-generic auxiliary classification GAN (DGAC-GAN) for producing
domain-specific feature representations seamlessly. For domain alignment, we
propose a simple extension of the popular domain adversarial neural network
(DANN) called DANN-IB which encourages discriminative domain-invariant and
task-relevant feature learning. Experimental results on Office-Home,
Office-CalTech, and DomainNet datasets confirm that FRIDA maintains superior
stability-plasticity trade-off than the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01293">
<div class="article-summary-box-inner">
<span><p>This paper presents a transformer-based Siamese network architecture
(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of
co-registered remote sensing images. Different from recent CD frameworks, which
are based on fully convolutional networks (ConvNets), the proposed method
unifies hierarchically structured transformer encoder with Multi-Layer
Perception (MLP) decoder in a Siamese network architecture to efficiently
render multi-scale long-range details required for accurate CD. Experiments on
two CD datasets show that the proposed end-to-end trainable ChangeFormer
architecture achieves better CD performance than previous counterparts. Our
code is available at https://github.com/wgcban/ChangeFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02428">
<div class="article-summary-box-inner">
<span><p>Today, deep convolutional neural networks (CNNs) have demonstrated
state-of-the-art performance for medical image segmentation, on various imaging
modalities and tasks. Despite early success, segmentation networks may still
generate anatomically aberrant segmentations, with holes or inaccuracies near
the object boundaries. To enforce anatomical plausibility, recent research
studies have focused on incorporating prior knowledge such as object shape or
boundary, as constraints in the loss function. Prior integrated could be
low-level referring to reformulated representations extracted from the
ground-truth segmentations, or high-level representing external medical
information such as the organ's shape or size. Over the past few years,
prior-based losses exhibited a rising interest in the research field since they
allow integration of expert knowledge while still being architecture-agnostic.
However, given the diversity of prior-based losses on different medical imaging
challenges and tasks, it has become hard to identify what loss works best for
which dataset. In this paper, we establish a benchmark of recent prior-based
losses for medical image segmentation. The main objective is to provide
intuition onto which losses to choose given a particular task or dataset. To
this end, four low-level and high-level prior-based losses are selected. The
considered losses are validated on 8 different datasets from a variety of
medical image segmentation challenges including the Decathlon, the ISLES and
the WMH challenge. Results show that whereas low-level prior-based losses can
guarantee an increase in performance over the Dice loss baseline regardless of
the dataset characteristics, high-level prior-based losses can increase
anatomical plausibility as per data characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Generative Modeling for Volume Reconstruction in Cryo-Electron Microscopy. (arXiv:2201.02867v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02867">
<div class="article-summary-box-inner">
<span><p>Recent breakthroughs in high resolution imaging of biomolecules in solution
with cryo-electron microscopy (cryo-EM) have unlocked new doors for the
reconstruction of molecular volumes, thereby promising further advances in
biology, chemistry, and pharmacological research amongst others. Despite
significant headway, the immense challenges in cryo-EM data analysis remain
legion and intricately inter-disciplinary in nature, requiring insights from
physicists, structural biologists, computer scientists, statisticians, and
applied mathematicians. Meanwhile, recent next-generation volume reconstruction
algorithms that combine generative modeling with end-to-end unsupervised deep
learning techniques have shown promising results on simulated data, but still
face considerable hurdles when applied to experimental cryo-EM images. In light
of the proliferation of such methods and given the interdisciplinary nature of
the task, we propose here a critical review of recent advances in the field of
deep generative modeling for high resolution cryo-EM volume reconstruction. The
present review aims to (i) compare and contrast these new methods, while (ii)
presenting them from a perspective and using terminology familiar to scientists
in each of the five aforementioned fields with no specific background in
cryo-EM. The review begins with an introduction to the mathematical and
computational challenges of deep generative models for cryo-EM volume
reconstruction, along with an overview of the baseline methodology shared
across this class of algorithms. Having established the common thread weaving
through these different models, we provide a practical comparison of these
state-of-the-art algorithms, highlighting their relative strengths and
weaknesses, along with the assumptions that they rely on. This allows us to
identify bottlenecks in current methods and avenues for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agricultural Plant Cataloging and Establishment of a Data Framework from UAV-based Crop Images by Computer Vision. (arXiv:2201.02885v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02885">
<div class="article-summary-box-inner">
<span><p>UAV-based image retrieval in modern agriculture enables gathering large
amounts of spatially referenced crop image data. In large-scale experiments,
however, UAV images suffer from containing a multitudinous amount of crops in a
complex canopy architecture. Especially for the observation of temporal
effects, this complicates the recognition of individual plants over several
images and the extraction of relevant information tremendously. In this work,
we present a hands-on workflow for the automatized temporal and spatial
identification and individualization of crop images from UAVs abbreviated as
"cataloging" based on comprehensible computer vision methods. We evaluate the
workflow on two real-world datasets. One dataset is recorded for observation of
Cercospora leaf spot - a fungal disease - in sugar beet over an entire growing
cycle. The other one deals with harvest prediction of cauliflower plants. The
plant catalog is utilized for the extraction of single plant images seen over
multiple time points. This gathers large-scale spatio-temporal image dataset
that in turn can be applied to train further machine learning models including
various data layers. The presented approach improves analysis and
interpretation of UAV data in agriculture significantly. By validation with
some reference data, our method shows an accuracy that is similar to more
complex deep learning-based recognition techniques. Our workflow is able to
automatize plant cataloging and training image extraction, especially for large
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaskMTL: Attribute prediction in masked facial images with deep multitask learning. (arXiv:2201.03002v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03002">
<div class="article-summary-box-inner">
<span><p>Predicting attributes in the landmark free facial images is itself a
challenging task which gets further complicated when the face gets occluded due
to the usage of masks. Smart access control gates which utilize identity
verification or the secure login to personal electronic gadgets may utilize
face as a biometric trait. Particularly, the Covid-19 pandemic increasingly
validates the essentiality of hygienic and contactless identity verification.
In such cases, the usage of masks become more inevitable and performing
attribute prediction helps in segregating the target vulnerable groups from
community spread or ensuring social distancing for them in a collaborative
environment. We create a masked face dataset by efficiently overlaying masks of
different shape, size and textures to effectively model variability generated
by wearing mask. This paper presents a deep Multi-Task Learning (MTL) approach
to jointly estimate various heterogeneous attributes from a single masked
facial image. Experimental results on benchmark face attribute UTKFace dataset
demonstrate that the proposed approach supersedes in performance to other
competing techniques. The source code is available at
https://github.com/ritikajha/Attribute-prediction-in-masked-facial-images-with-deep-multitask-learning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Relation Graphs Enhanced Hierarchical Residual Network for Hierarchical Multi-Granularity Classification. (arXiv:2201.03194v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03194">
<div class="article-summary-box-inner">
<span><p>Hierarchical multi-granularity classification (HMC) assigns hierarchical
multi-granularity labels to each object and focuses on encoding the label
hierarchy, e.g., ["Albatross", "Laysan Albatross"] from coarse-to-fine levels.
However, the definition of what is fine-grained is subjective, and the image
quality may affect the identification. Thus, samples could be observed at any
level of the hierarchy, e.g., ["Albatross"] or ["Albatross", "Laysan
Albatross"], and examples discerned at coarse categories are often neglected in
the conventional setting of HMC. In this paper, we study the HMC problem in
which objects are labeled at any level of the hierarchy. The essential designs
of the proposed method are derived from two motivations: (1) learning with
objects labeled at various levels should transfer hierarchical knowledge
between levels; (2) lower-level classes should inherit attributes related to
upper-level superclasses. The proposed combinatorial loss maximizes the
marginal probability of the observed ground truth label by aggregating
information from related labels defined in the tree hierarchy. If the observed
label is at the leaf level, the combinatorial loss further imposes the
multi-class cross-entropy loss to increase the weight of fine-grained
classification loss. Considering the hierarchical feature interaction, we
propose a hierarchical residual network (HRN), in which granularity-specific
features from parent levels acting as residual connections are added to
features of children levels. Experiments on three commonly used datasets
demonstrate the effectiveness of our approach compared to the state-of-the-art
HMC approaches and fine-grained visual classification (FGVC) methods exploiting
the label hierarchy.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-12 23:07:13.994979707 UTC">2022-01-12 23:07:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>