<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-29T01:30:00Z">07-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The Leaf Clinical Trials Corpus: a new resource for query generation from clinical trial eligibility criteria. (arXiv:2207.13757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13757">
<div class="article-summary-box-inner">
<span><p>Identifying cohorts of patients based on eligibility criteria such as medical
conditions, procedures, and medication use is critical to recruitment for
clinical trials. Such criteria are often most naturally described in free-text,
using language familiar to clinicians and researchers. In order to identify
potential participants at scale, these criteria must first be translated into
queries on clinical databases, which can be labor-intensive and error-prone.
Natural language processing (NLP) methods offer a potential means of such
conversion into database queries automatically. However they must first be
trained and evaluated using corpora which capture clinical trials criteria in
sufficient detail. In this paper, we introduce the Leaf Clinical Trials (LCT)
corpus, a human-annotated corpus of over 1,000 clinical trial eligibility
criteria descriptions using highly granular structured labels capturing a range
of biomedical phenomena. We provide details of our schema, annotation process,
corpus quality, and statistics. Additionally, we present baseline information
extraction results on this corpus as benchmarks for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CompText: Visualizing, Comparing & Understanding Text Corpus. (arXiv:2207.13771v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13771">
<div class="article-summary-box-inner">
<span><p>A common practice in Natural Language Processing (NLP) is to visualize the
text corpus without reading through the entire literature, still grasping the
central idea and key points described. For a long time, researchers focused on
extracting topics from the text and visualizing them based on their relative
significance in the corpus. However, recently, researchers started coming up
with more complex systems that not only expose the topics of the corpus but
also word closely related to the topic to give users a holistic view. These
detailed visualizations spawned research on comparing text corpora based on
their visualization. Topics are often compared to idealize the difference
between corpora. However, to capture greater semantics from different corpora,
researchers have started to compare texts based on the sentiment of the topics
related to the text. Comparing the words carrying the most weightage, we can
get an idea about the important topics for corpus. There are multiple existing
texts comparing methods present that compare topics rather than sentiments but
we feel that focusing on sentiment-carrying words would better compare the two
corpora. Since only sentiments can explain the real feeling of the text and not
just the topic, topics without sentiments are just nouns. We aim to
differentiate the corpus with a focus on sentiment, as opposed to comparing all
the words appearing in the two corpora. The rationale behind this is, that the
two corpora do not many have identical words for side-by-side comparison, so
comparing the sentiment words gives us an idea of how the corpora are appealing
to the emotions of the reader. We can argue that the entropy or the
unexpectedness and divergence of topics should also be of importance and help
us to identify key pivot points and the importance of certain topics in the
corpus alongside relative sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods. (arXiv:2207.13919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13919">
<div class="article-summary-box-inner">
<span><p>Persona and Knowledge dual context open-domain chat is a novel dialogue
generation task introduced recently. While Persona and Knowledge is each
interesting context of open-domain dialogue, the combination of both has not
been well studied. We tackle Persona-Knowledge identification and response
generation tasks in this paper. We design an informed data augmentation
strategy that is compatible with neural Q&amp;A retrieval models. With the
augmented data, we perform permutative Persona-Knowledge evaluation and
successive Persona search fine-tuning. Furthermore, we perform dialogue
generation with various decoding techniques and illustrate crucial elements. We
achieve SOTA across official metrics with 93.99% Grounding accuracy average and
23.62 SacreBLEU score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLRIP: Pre-training a military language representation model with informative factual knowledge and professional knowledge base. (arXiv:2207.13929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13929">
<div class="article-summary-box-inner">
<span><p>Incorporating prior knowledge into pre-trained language models has proven to
be effective for knowledge-driven NLP tasks, such as entity typing and relation
extraction. Current pre-training procedures usually inject external knowledge
into models by using knowledge masking, knowledge fusion and knowledge
replacement. However, factual information contained in the input sentences have
not been fully mined, and the external knowledge for injecting have not been
strictly checked. As a result, the context information cannot be fully
exploited and extra noise will be introduced or the amount of knowledge
injected is limited. To address these issues, we propose MLRIP, which modifies
the knowledge masking strategies proposed by ERNIE-Baidu, and introduce a
two-stage entity replacement strategy. Extensive experiments with comprehensive
analyses illustrate the superiority of MLRIP over BERT-based models in military
knowledge-driven NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Interpretability Evaluation Benchmark for Pre-trained Language Models. (arXiv:2207.13948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13948">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (LMs) have brought great improvements in
many NLP tasks, there is increasing attention to explore capabilities of LMs
and interpret their predictions. However, existing works usually focus only on
a certain capability with some downstream tasks. There is a lack of datasets
for directly evaluating the masked word prediction performance and the
interpretability of pre-trained LMs. To fill in the gap, we propose a novel
evaluation benchmark providing with both English and Chinese annotated data. It
tests LMs abilities in multiple dimensions, i.e., grammar, semantics,
knowledge, reasoning and computation. In addition, it provides carefully
annotated token-level rationales that satisfy sufficiency and compactness. It
contains perturbed instances for each original instance, so as to use the
rationale consistency under perturbations as the metric for faithfulness, a
perspective of interpretability. We conduct experiments on several widely-used
pre-trained LMs. The results show that they perform very poorly on the
dimensions of knowledge and computation. And their plausibility in all
dimensions is far from satisfactory, especially when the rationale is short. In
addition, the pre-trained LMs we evaluated are not robust on syntax-aware data.
We will release this evaluation benchmark at \url{<a href="http://xyz">this http URL</a>}, and hope it can
facilitate the research progress of pre-trained LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Search on Efficient Transformers and Beyond. (arXiv:2207.13955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13955">
<div class="article-summary-box-inner">
<span><p>Recently, numerous efficient Transformers have been proposed to reduce the
quadratic computational complexity of standard Transformers caused by the
Softmax attention. However, most of them simply swap Softmax with an efficient
attention mechanism without considering the customized architectures specially
for the efficient attention. In this paper, we argue that the handcrafted
vanilla Transformer architectures for Softmax attention may not be suitable for
efficient Transformers. To address this issue, we propose a new framework to
find optimal architectures for efficient Transformers with the neural
architecture search (NAS) technique. The proposed method is validated on
popular machine translation and image classification tasks. We observe that the
optimal architecture of the efficient Transformer has the reduced computation
compared with that of the standard Transformer, but the general accuracy is
less comparable. It indicates that the Softmax attention and efficient
attention have their own distinctions but neither of them can simultaneously
balance the accuracy and efficiency well. This motivates us to mix the two
types of attention to reduce the performance imbalance. Besides the search
spaces that commonly used in existing NAS Transformer approaches, we propose a
new search space that allows the NAS algorithm to automatically search the
attention variants along with architectures. Extensive experiments on WMT' 14
En-De and CIFAR-10 demonstrate that our searched architecture maintains
comparable accuracy to the standard Transformer with notably improved
computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PHEMEPlus: Enriching Social Media Rumour Verification with External Evidence. (arXiv:2207.13970v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13970">
<div class="article-summary-box-inner">
<span><p>Work on social media rumour verification utilises signals from posts, their
propagation and users involved. Other lines of work target identifying and
fact-checking claims based on information from Wikipedia, or trustworthy news
articles without considering social media context. However works combining the
information from social media with external evidence from the wider web are
lacking. To facilitate research in this direction, we release a novel dataset,
PHEMEPlus, an extension of the PHEME benchmark, which contains social media
conversations as well as relevant external evidence for each rumour. We
demonstrate the effectiveness of incorporating such evidence in improving
rumour verification models. Additionally, as part of the evidence collection,
we evaluate various ways of query formulation to identify the most effective
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowing Where and What: Unified Word Block Pretraining for Document Understanding. (arXiv:2207.13979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13979">
<div class="article-summary-box-inner">
<span><p>Due to the complex layouts of documents, it is challenging to extract
information for documents. Most previous studies develop multimodal pre-trained
models in a self-supervised way. In this paper, we focus on the embedding
learning of word blocks containing text and layout information, and propose
UTel, a language model with Unified TExt and Layout pre-training. Specifically,
we propose two pre-training tasks: Surrounding Word Prediction (SWP) for the
layout learning, and Contrastive learning of Word Embeddings (CWE) for
identifying different word blocks. Moreover, we replace the commonly used 1D
position embedding with a 1D clipped relative position embedding. In this way,
the joint training of Masked Layout-Language Modeling (MLLM) and two newly
proposed tasks enables the interaction between semantic and spatial features in
a unified way. Additionally, the proposed UTel can process arbitrary-length
sequences by removing the 1D position embedding, while maintaining competitive
performance. Extensive experimental results show UTel learns better joint
representations and achieves superior performance than previous methods on
various downstream tasks, though requiring no image modality. Code is available
at \url{https://github.com/taosong2019/UTel}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence to sequence pretraining for a less-resourced Slovenian language. (arXiv:2207.13988v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13988">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models have recently conquered the area of natural
language processing. As an alternative to predominant masked language modelling
introduced in BERT, the T5 model has introduced a more general training
objective, namely sequence to sequence transformation, which includes masked
language model but more naturally fits text generation tasks such as machine
translation, summarization, open-domain question answering, text
simplification, dialogue systems, etc. The monolingual variants of T5 models
have been limited to well-resourced languages, while the massively multilingual
T5 model supports 101 languages. In contrast, we trained two different sized
T5-type sequence to sequence models for morphologically rich Slovene language
with much less resources and analyzed their behavior. Concerning classification
tasks, the SloT5 models mostly lag behind the monolingual Slovene SloBERTa
model but are to be considered for the generative tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation. (arXiv:2207.14000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14000">
<div class="article-summary-box-inner">
<span><p>Combining deep learning with symbolic logic reasoning aims to capitalize on
the success of both fields and is drawing increasing attention. Inspired by
DeepLogic, an end-to-end model trained to perform inference on logic programs,
we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step
reasoning expressed in natural language. In our model, reasoning is performed
using an iterative memory neural network based on RNN with a gate attention
mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES
V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention
can achieve higher test accuracy than DeepLogic and other RNN baseline models.
Our model achieves better out-of-distribution generalisation than RoBERTa-Large
when the rules have been shuffled. Furthermore, to address the issue of
unbalanced distribution of reasoning depths in the current multi-step reasoning
datasets, we develop PARARULE-Plus, a large dataset with more examples that
require deeper reasoning steps. Experimental results show that the addition of
PARARULE-Plus can increase the model's performance on examples requiring deeper
reasoning depths. The source code and data are available at
https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Raising Student Completion Rates with Adaptive Curriculum and Contextual Bandits. (arXiv:2207.14003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14003">
<div class="article-summary-box-inner">
<span><p>We present an adaptive learning Intelligent Tutoring System, which uses
model-based reinforcement learning in the form of contextual bandits to assign
learning activities to students. The model is trained on the trajectories of
thousands of students in order to maximize their exercise completion rates and
continues to learn online, automatically adjusting itself to new activities. A
randomized controlled trial with students shows that our model leads to
superior completion rates and significantly improved student engagement when
compared to other approaches. Our approach is fully-automated unlocking new
opportunities for learning experience personalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CubeMLP: A MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation. (arXiv:2207.14087v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14087">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis and depression estimation are two important
research topics that aim to predict human mental states using multimodal data.
Previous research has focused on developing effective fusion strategies for
exchanging and integrating mind-related information from different modalities.
Some MLP-based techniques have recently achieved considerable success in a
variety of computer vision tasks. Inspired by this, we explore multimodal
approaches with a feature-mixing perspective in this study. To this end, we
introduce CubeMLP, a multimodal feature processing framework based entirely on
MLP. CubeMLP consists of three independent MLP units, each of which has two
affine transformations. CubeMLP accepts all relevant modality features as input
and mixes them across three axes. After extracting the characteristics using
CubeMLP, the mixed multimodal features are flattened for task predictions. Our
experiments are conducted on sentiment analysis datasets: CMU-MOSI and
CMU-MOSEI, and depression estimation dataset: AVEC2019. The results show that
CubeMLP can achieve state-of-the-art performance with a much lower computing
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Type Prediction Leveraging Graph Walks and Entity Descriptions. (arXiv:2207.14094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14094">
<div class="article-summary-box-inner">
<span><p>The entity type information in Knowledge Graphs (KGs) such as DBpedia,
Freebase, etc. is often incomplete due to automated generation or human
curation. Entity typing is the task of assigning or inferring the semantic type
of an entity in a KG. This paper presents \textit{GRAND}, a novel approach for
entity typing leveraging different graph walk strategies in RDF2vec together
with textual entity descriptions. RDF2vec first generates graph walks and then
uses a language model to obtain embeddings for each node in the graph. This
study shows that the walk generation strategy and the embedding model have a
significant effect on the performance of the entity typing task. The proposed
approach outperforms the baseline approaches on the benchmark datasets DBpedia
and FIGER for entity typing in KGs for both fine-grained and coarse-grained
classes. The results show that the combination of order-aware RDF2vec variants
together with the contextual embeddings of the textual entity descriptions
achieve the best results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14116">
<div class="article-summary-box-inner">
<span><p>We present Claim-Dissector: a novel latent variable model for fact-checking
and fact-analysis, which given a claim and a set of retrieved provenances
allows learning jointly: (i) what are the relevant provenances to this claim
(ii) what is the veracity of this claim. We propose to disentangle the
per-provenance relevance probability and its contribution to the final veracity
probability in an interpretable way - the final veracity probability is
proportional to a linear ensemble of per-provenance relevance probabilities.
This way, it can be clearly identified the relevance of which sources
contributes to what extent towards the final probability. We show that our
system achieves state-of-the-art results on FEVER dataset comparable to
two-stage systems typically used in traditional fact-checking pipelines, while
it often uses significantly less parameters and computation.
</p>
<p>Our analysis shows that proposed approach further allows to learn not just
which provenances are relevant, but also which provenances lead to supporting
and which toward denying the claim, without direct supervision. This not only
adds interpretability, but also allows to detect claims with conflicting
evidence automatically. Furthermore, we study whether our model can learn
fine-grained relevance cues while using coarse-grained supervision. We show
that our model can achieve competitive sentence-recall while using only
paragraph-level relevance supervision. Finally, traversing towards the finest
granularity of relevance, we show that our framework is capable of identifying
relevance at the token-level. To do this, we present a new benchmark focusing
on token-level interpretability - humans annotate tokens in relevant
provenances they considered essential when making their judgement. Then we
measure how similar are these annotations to tokens our model is focusing on.
Our code, and dataset will be released online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions. (arXiv:2207.14251v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14251">
<div class="article-summary-box-inner">
<span><p>Large amounts of training data are one of the major reasons for the high
performance of state-of-the-art NLP models. But what exactly in the training
data causes a model to make a certain prediction? We seek to answer this
question by providing a language for describing how training data influences
predictions, through a causal framework. Importantly, our framework bypasses
the need to retrain expensive models and allows us to estimate causal effects
based on observational data alone. Addressing the problem of extracting factual
knowledge from pretrained language models (PLMs), we focus on simple data
statistics such as co-occurrence counts and show that these statistics do
influence the predictions of PLMs, suggesting that such models rely on shallow
heuristics. Our causal framework and our results demonstrate the importance of
studying datasets and the benefits of causality for understanding NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of Language Models to Fill in the Middle. (arXiv:2207.14255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14255">
<div class="article-summary-box-inner">
<span><p>We show that autoregressive language models can learn to infill text after we
apply a straightforward transformation to the dataset, which simply moves a
span of text from the middle of a document to its end. While this data
augmentation has garnered much interest in recent years, we provide extensive
evidence that training models with a large fraction of data transformed in this
way does not harm the original left-to-right generative capability, as measured
by perplexity and sampling evaluations across a wide range of scales. Given the
usefulness, simplicity, and efficiency of training models to fill-in-the-middle
(FIM), we suggest that future autoregressive language models be trained with
FIM by default. To this end, we run a series of ablations on key
hyperparameters, such as the data transformation frequency, the structure of
the transformation, and the method of selecting the infill span. We use these
ablations to prescribe strong default settings and best practices to train FIM
models. We have released our best infilling model trained with best practices
in our API, and release our infilling benchmarks to aid future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings. (arXiv:2109.08449v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08449">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models (PreLMs) are revolutionizing natural
language processing across all benchmarks. However, their sheer size is
prohibitive for small laboratories or for deployment on mobile devices.
Approaches like pruning and distillation reduce the model size but typically
retain the same model architecture. In contrast, we explore distilling PreLMs
into a different, more efficient architecture, Continual Multiplication of
Words (CMOW), which embeds each word as a matrix and uses matrix multiplication
to encode sequences. We extend the CMOW architecture and its CMOW/CBOW-Hybrid
variant with a bidirectional component for more expressive power, per-token
representations for a general (task-agnostic) distillation during pretraining,
and a two-sequence encoding scheme that facilitates downstream tasks on
sentence pairs, such as sentence similarity and natural language inference. Our
matrix-based bidirectional CMOW/CBOW-Hybrid model is competitive to DistilBERT
on question similarity and recognizing textual entailment, but uses only half
of the number of parameters and is three times faster in terms of inference
speed. We match or exceed the scores of ELMo for all tasks of the GLUE
benchmark except for the sentiment analysis task SST-2 and the linguistic
acceptability task CoLA. However, compared to previous cross-architecture
distillation approaches, we demonstrate a doubling of the scores on detecting
linguistic acceptability. This shows that matrix-based embeddings can be used
to distill large PreLM into competitive models and motivates further research
in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00693">
<div class="article-summary-box-inner">
<span><p>To enhance research on multimodal knowledge base and multimodal information
processing, we propose a new task called multimodal entity tagging (MET) with a
multimodal knowledge base (MKB). We also develop a dataset for the problem
using an existing MKB. In an MKB, there are entities and their associated texts
and images. In MET, given a text-image pair, one uses the information in the
MKB to automatically identify the related entity in the text-image pair. We
solve the task by using the information retrieval paradigm and implement
several baselines using state-of-the-art methods in NLP and CV. We conduct
extensive experiments and make analyses on the experimental results. The
results show that the task is challenging, but current technologies can achieve
relatively high performance. We will release the dataset, code, and models for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Klexikon: A German Dataset for Joint Summarization and Simplification. (arXiv:2201.07198v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07198">
<div class="article-summary-box-inner">
<span><p>Traditionally, Text Simplification is treated as a monolingual translation
task where sentences between source texts and their simplified counterparts are
aligned for training. However, especially for longer input documents,
summarizing the text (or dropping less relevant content altogether) plays an
important role in the simplification process, which is currently not reflected
in existing datasets. Simultaneously, resources for non-English languages are
scarce in general and prohibitive for training new solutions. To tackle this
problem, we pose core requirements for a system that can jointly summarize and
simplify long source documents. We further describe the creation of a new
dataset for joint Text Simplification and Summarization based on German
Wikipedia and the German children's lexicon "Klexikon", consisting of almost
2900 documents. We release a document-aligned version that particularly
highlights the summarization aspect, and provide statistical evidence that this
resource is well suited to simplification as well. Code and data are available
on Github: https://github.com/dennlinger/klexikon
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Cascades. (arXiv:2207.10342v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10342">
<div class="article-summary-box-inner">
<span><p>Prompted models have demonstrated impressive few-shot learning abilities.
Repeated interactions at test-time with a single model, or the composition of
multiple models together, further expands capabilities. These compositions are
probabilistic models, and may be expressed in the language of graphical models
with random variables whose values are complex data types such as strings.
Cases with control flow and dynamic structure require techniques from
probabilistic programming, which allow implementing disparate model structures
and inference strategies in a unified language. We formalize several existing
techniques from this perspective, including scratchpads / chain of thought,
verifiers, STaR, selection-inference, and tool use. We refer to the resulting
programs as language model cascades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\mu\text{KG}$: A Library for Multi-source Knowledge Graph Embeddings and Applications. (arXiv:2207.11442v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11442">
<div class="article-summary-box-inner">
<span><p>This paper presents $\mu\text{KG}$, an open-source Python library for
representation learning over knowledge graphs. $\mu\text{KG}$ supports joint
representation learning over multi-source knowledge graphs (and also a single
knowledge graph), multiple deep learning libraries (PyTorch and TensorFlow2),
multiple embedding tasks (link prediction, entity alignment, entity typing, and
multi-source link prediction), and multiple parallel computing modes
(multi-process and multi-GPU computing). It currently implements 26 popular
knowledge graph embedding models and supports 16 benchmark datasets.
$\mu\text{KG}$ provides advanced implementations of embedding techniques with
simplified pipelines of different tasks. It also comes with high-quality
documentation for ease of use. $\mu\text{KG}$ is more comprehensive than
existing knowledge graph embedding libraries. It is useful for a thorough
comparison and analysis of various embedding models and tasks. We show that the
jointly learned embeddings can greatly help knowledge-powered downstream tasks,
such as multi-hop knowledge graph question answering. We will stay abreast of
the latest developments in the related fields and incorporate them into
$\mu\text{KG}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13243">
<div class="article-summary-box-inner">
<span><p>The last decade of machine learning has seen drastic increases in scale and
capabilities, and deep neural networks (DNNs) are increasingly being deployed
across a wide range of domains. However, the inner workings of DNNs are
generally difficult to understand, raising concerns about the safety of using
these systems without a rigorous understanding of how they function. In this
survey, we review literature on techniques for interpreting the inner
components of DNNs, which we call "inner" interpretability methods.
Specifically, we review methods for interpreting weights, neurons, subnetworks,
and latent representations with a focus on how these techniques relate to the
goal of designing safer, more trustworthy AI systems. We also highlight
connections between interpretability and work in modularity, adversarial
robustness, continual learning, network compression, and studying the human
visual system. Finally, we discuss key challenges and argue for future work in
interpretability for AI safety that focuses on diagnostics, benchmarking, and
robustness.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Break and Make: Interactive Structural Understanding Using LEGO Bricks. (arXiv:2207.13738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13738">
<div class="article-summary-box-inner">
<span><p>Visual understanding of geometric structures with complex spatial
relationships is a fundamental component of human intelligence. As children, we
learn how to reason about structure not only from observation, but also by
interacting with the world around us -- by taking things apart and putting them
back together again. The ability to reason about structure and compositionality
allows us to not only build things, but also understand and reverse-engineer
complex systems. In order to advance research in interactive reasoning for
part-based geometric understanding, we propose a challenging new assembly
problem using LEGO bricks that we call Break and Make. In this problem an agent
is given a LEGO model and attempts to understand its structure by interactively
inspecting and disassembling it. After this inspection period, the agent must
then prove its understanding by rebuilding the model from scratch using
low-level action primitives. In order to facilitate research on this problem we
have built LTRON, a fully interactive 3D simulator that allows learning agents
to assemble, disassemble and manipulate LEGO models. We pair this simulator
with a new dataset of fan-made LEGO creations that have been uploaded to the
internet in order to provide complex scenes containing over a thousand unique
brick shapes. We take a first step towards solving this problem using
sequence-to-sequence models that provide guidance for how to make progress on
this challenging problem. Our simulator and data are available at
github.com/aaronwalsman/ltron. Additional training code and PyTorch examples
are available at github.com/aaronwalsman/ltron-torch-eccv22.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lighting (In)consistency of Paint by Text. (arXiv:2207.13744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13744">
<div class="article-summary-box-inner">
<span><p>Whereas generative adversarial networks are capable of synthesizing highly
realistic images of faces, cats, landscapes, or almost any other single
category, paint-by-text synthesis engines can -- from a single text prompt --
synthesize realistic images of seemingly endless categories with arbitrary
configurations and combinations. This powerful technology poses new challenges
to the photo-forensic community. Motivated by the fact that paint by text is
not based on explicit geometric or physical models, and the human visual
system's general insensitivity to lighting inconsistencies, we provide an
initial exploration of the lighting consistency of DALL-E-2 synthesized images
to determine if physics-based forensic analyses will prove fruitful in
detecting this new breed of synthetic media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAUDI: A Neural Architect for Immersive 3D Scene Generation. (arXiv:2207.13751v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13751">
<div class="article-summary-box-inner">
<span><p>We introduce GAUDI, a generative model capable of capturing the distribution
of complex and realistic 3D scenes that can be rendered immersively from a
moving camera. We tackle this challenging problem with a scalable yet powerful
approach, where we first optimize a latent representation that disentangles
radiance fields and camera poses. This latent representation is then used to
learn a generative model that enables both unconditional and conditional
generation of 3D scenes. Our model generalizes previous works that focus on
single objects by removing the assumption that the camera pose distribution can
be shared across samples. We show that GAUDI obtains state-of-the-art
performance in the unconditional generative setting across multiple datasets
and allows for conditional generation of 3D scenes given conditioning variables
like sparse image observations or text that describes the scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Classification of Thyroid Nodules on Ultrasound: Validation on an Independent Dataset. (arXiv:2207.13765v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13765">
<div class="article-summary-box-inner">
<span><p>Objectives: The purpose is to apply a previously validated deep learning
algorithm to a new thyroid nodule ultrasound image dataset and compare its
performances with radiologists. Methods: Prior study presented an algorithm
which is able to detect thyroid nodules and then make malignancy
classifications with two ultrasound images. A multi-task deep convolutional
neural network was trained from 1278 nodules and originally tested with 99
separate nodules. The results were comparable with that of radiologists. The
algorithm was further tested with 378 nodules imaged with ultrasound machines
from different manufacturers and product types than the training cases. Four
experienced radiologists were requested to evaluate the nodules for comparison
with deep learning. Results: The Area Under Curve (AUC) of the deep learning
algorithm and four radiologists were calculated with parametric, binormal
estimation. For the deep learning algorithm, the AUC was 0.70 (95% CI: 0.64 -
0.75). The AUC of radiologists were 0.66 (95% CI: 0.61 - 0.71), 0.67 (95%
CI:0.62 - 0.73), 0.68 (95% CI: 0.63 - 0.73), and 0.66 (95%CI: 0.61 - 0.71).
Conclusion: In the new testing dataset, the deep learning algorithm achieved
similar performances with all four radiologists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion Sensing. (arXiv:2207.13784v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13784">
<div class="article-summary-box-inner">
<span><p>Today's Mixed Reality head-mounted displays track the user's head pose in
world space as well as the user's hands for interaction in both Augmented
Reality and Virtual Reality scenarios. While this is adequate to support user
input, it unfortunately limits users' virtual representations to just their
upper bodies. Current systems thus resort to floating avatars, whose limitation
is particularly evident in collaborative settings. To estimate full-body poses
from the sparse input sources, prior work has incorporated additional trackers
and sensors at the pelvis or lower body, which increases setup complexity and
limits practical application in mobile settings. In this paper, we present
AvatarPoser, the first learning-based method that predicts full-body poses in
world coordinates using only motion input from the user's head and hands. Our
method builds on a Transformer encoder to extract deep features from the input
signals and decouples global motion from the learned local joint orientations
to guide pose estimation. To obtain accurate full-body motions that resemble
motion capture animations, we refine the arm joints' positions using an
optimization routine with inverse kinematics to match the original tracking
input. In our evaluation, AvatarPoser achieved new state-of-the-art results in
evaluations on large motion capture datasets (AMASS). At the same time, our
method's inference speed supports real-time operation, providing a practical
interface to support holistic avatar control and representation for Metaverse
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Assess Danger from Movies for Cooperative Escape Planning in Hazardous Environments. (arXiv:2207.13791v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13791">
<div class="article-summary-box-inner">
<span><p>There has been a plethora of work towards improving robot perception and
navigation, yet their application in hazardous environments, like during a fire
or an earthquake, is still at a nascent stage. We hypothesize two key
challenges here: first, it is difficult to replicate such scenarios in the real
world, which is necessary for training and testing purposes. Second, current
systems are not fully able to take advantage of the rich multi-modal data
available in such hazardous environments. To address the first challenge, we
propose to harness the enormous amount of visual content available in the form
of movies and TV shows, and develop a dataset that can represent hazardous
environments encountered in the real world. The data is annotated with
high-level danger ratings for realistic disaster images, and corresponding
keywords are provided that summarize the content of the scene. In response to
the second challenge, we propose a multi-modal danger estimation pipeline for
collaborative human-robot escape scenarios. Our Bayesian framework improves
danger estimation by fusing information from robot's camera sensor and language
inputs from the human. Furthermore, we augment the estimation module with a
risk-aware planner that helps in identifying safer paths out of the dangerous
environment. Through extensive simulations, we exhibit the advantages of our
multi-modal perception framework that gets translated into tangible benefits
such as higher success rate in a collaborative human-robot mission.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look at Adjacent Frames: Video Anomaly Detection without Offline Training. (arXiv:2207.13798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13798">
<div class="article-summary-box-inner">
<span><p>We propose a solution to detect anomalous events in videos without the need
to train a model offline. Specifically, our solution is based on a
randomly-initialized multilayer perceptron that is optimized online to
reconstruct video frames, pixel-by-pixel, from their frequency information.
Based on the information shifts between adjacent frames, an incremental learner
is used to update parameters of the multilayer perceptron after observing each
frame, thus allowing to detect anomalous events along the video stream.
Traditional solutions that require no offline training are limited to operating
on videos with only a few abnormal frames. Our solution breaks this limit and
achieves strong performance on benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields. (arXiv:2207.13807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13807">
<div class="article-summary-box-inner">
<span><p>We present Pose-NDF, a continuous model for plausible human poses based on
neural distance fields (NDFs). Pose or motion priors are important for
generating realistic new poses and for reconstructing accurate poses from noisy
or partial observations. Pose-NDF learns a manifold of plausible poses as the
zero level set of a neural implicit function, extending the idea of modeling
implicit surfaces in 3D to the high-dimensional domain SO(3)^K, where a human
pose is defined by a single data point, represented by K quaternions. The
resulting high-dimensional implicit function can be differentiated with respect
to the input poses and thus can be used to project arbitrary poses onto the
manifold by using gradient descent on the set of 3-dimensional hyperspheres. In
contrast to previous VAE-based human pose priors, which transform the pose
space into a Gaussian distribution, we model the actual pose manifold,
preserving the distances between poses. We demonstrate that PoseNDF outperforms
existing state-of-the-art methods as a prior in various downstream tasks,
ranging from denoising real-world human mocap data, pose recovery from occluded
data to 3D pose reconstruction from images. Furthermore, we show that it can be
used to generate more diverse poses by random sampling and projection than
VAE-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers. (arXiv:2207.13820v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13820">
<div class="article-summary-box-inner">
<span><p>Transformer encoder architectures have recently achieved state-of-the-art
results on monocular 3D human mesh reconstruction, but they require a
substantial number of parameters and expensive computations. Due to the large
memory overhead and slow inference speed, it is difficult to deploy such models
for practical use. In this paper, we propose a novel transformer
encoder-decoder architecture for 3D human mesh reconstruction from a single
image, called FastMETRO. We identify the performance bottleneck in the
encoder-based transformers is caused by the token design which introduces high
complexity interactions among input tokens. We disentangle the interactions via
an encoder-decoder architecture, which allows our model to demand much fewer
parameters and shorter inference time. In addition, we impose the prior
knowledge of human body's morphological relationship via attention masking and
mesh upsampling operations, which leads to faster convergence with higher
accuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency,
and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore,
we validate its generalizability on FreiHAND.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-Morphomics, Morphological Features on CT scans for lung nodule malignancy diagnosis. (arXiv:2207.13830v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13830">
<div class="article-summary-box-inner">
<span><p>Pathologies systematically induce morphological changes, thus providing a
major but yet insufficiently quantified source of observables for diagnosis.
The study develops a predictive model of the pathological states based on
morphological features (3D-morphomics) on Computed Tomography (CT) volumes. A
complete workflow for mesh extraction and simplification of an organ's surface
is developed, and coupled with an automatic extraction of morphological
features given by the distribution of mean curvature and mesh energy. An
XGBoost supervised classifier is then trained and tested on the 3D-morphomics
to predict the pathological states. This framework is applied to the prediction
of the malignancy of lung's nodules. On a subset of NLST database with
malignancy confirmed biopsy, using 3D-morphomics only, the classification model
of lung nodules into malignant vs. benign achieves 0.964 of AUC. Three other
sets of classical features are trained and tested, (1) clinical relevant
features gives an AUC of 0.58, (2) 111 radiomics gives an AUC of 0.976, (3)
radiologist ground truth (GT) containing the nodule size, attenuation and
spiculation qualitative annotations gives an AUC of 0.979. We also test the
Brock model and obtain an AUC of 0.826. Combining 3D-morphomics and radiomics
features achieves state-of-the-art results with an AUC of 0.978 where the
3D-morphomics have some of the highest predictive powers. As a validation on a
public independent cohort, models are applied to the LIDC dataset, the
3D-morphomics achieves an AUC of 0.906 and the 3D-morphomics+radiomics achieves
an AUC of 0.958, which ranks second in the challenge among deep models. It
establishes the curvature distributions as efficient features for predicting
lung nodule malignancy and a new method that can be applied directly to
arbitrary computer aided diagnosis task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extraction of Coronary Vessels in Fluoroscopic X-Ray Sequences Using Vessel Correspondence Optimization. (arXiv:2207.13837v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13837">
<div class="article-summary-box-inner">
<span><p>We present a method to extract coronary vessels from fluoroscopic x-ray
sequences. Given the vessel structure for the source frame, vessel
correspondence candidates in the subsequent frame are generated by a novel
hierarchical search scheme to overcome the aperture problem. Optimal
correspondences are determined within a Markov random field optimization
framework. Post-processing is performed to extract vessel branches newly
visible due to the inflow of contrast agent. Quantitative and qualitative
evaluation conducted on a dataset of 18 sequences demonstrates the
effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EEG2Mel: Reconstructing Sound from Brain Responses to Music. (arXiv:2207.13845v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13845">
<div class="article-summary-box-inner">
<span><p>Information retrieval from brain responses to auditory and visual stimuli has
shown success through classification of song names and image classes presented
to participants while recording EEG signals. Information retrieval in the form
of reconstructing auditory stimuli has also shown some success, but here we
improve on previous methods by reconstructing music stimuli well enough to be
perceived and identified independently. Furthermore, deep learning models were
trained on time-aligned music stimuli spectrum for each corresponding
one-second window of EEG recording, which greatly reduces feature extraction
steps needed when compared to prior studies. The NMED-Tempo and NMED-Hindi
datasets of participants passively listening to full length songs were used to
train and validate Convolutional Neural Network (CNN) regressors. The efficacy
of raw voltage versus power spectrum inputs and linear versus mel spectrogram
outputs were tested, and all inputs and outputs were converted into 2D images.
The quality of reconstructed spectrograms was assessed by training classifiers
which showed 81% accuracy for mel-spectrograms and 72% for linear spectrograms
(10% chance accuracy). Lastly, reconstructions of auditory music stimuli were
discriminated by listeners at an 85% success rate (50% chance) in a
two-alternative match-to-sample task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer. (arXiv:2207.13861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13861">
<div class="article-summary-box-inner">
<span><p>Real-world image denoising is a practical image restoration problem that aims
to obtain clean images from in-the-wild noisy input. Recently, Vision
Transformer (ViT) exhibits a strong ability to capture long-range dependencies
and many researchers attempt to apply ViT to image denoising tasks. However,
real-world image is an isolated frame that makes the ViT build the long-range
dependencies on the internal patches, which divides images into patches and
disarranges the noise pattern and gradient continuity. In this article, we
propose to resolve this issue by using a continuous Wavelet Sliding-Transformer
that builds frequency correspondence under real-world scenes, called DnSwin.
Specifically, we first extract the bottom features from noisy input images by
using a CNN encoder. The key to DnSwin is to separate high-frequency and
low-frequency information from the features and build frequency dependencies.
To this end, we propose Wavelet Sliding-Window Transformer that utilizes
discrete wavelet transform, self-attention and inverse discrete wavelet
transform to extract deep features. Finally, we reconstruct the deep features
into denoised images using a CNN decoder. Both quantitative and qualitative
evaluations on real-world denoising benchmarks demonstrate that the proposed
DnSwin performs favorably against the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MKANet: A Lightweight Network with Sobel Boundary Loss for Efficient Land-cover Classification of Satellite Remote Sensing Imagery. (arXiv:2207.13866v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13866">
<div class="article-summary-box-inner">
<span><p>Land cover classification is a multi-class segmentation task to classify each
pixel into a certain natural or man-made category of the earth surface, such as
water, soil, natural vegetation, crops, and human infrastructure. Limited by
hardware computational resources and memory capacity, most existing studies
preprocessed original remote sensing images by down sampling or cropping them
into small patches less than 512*512 pixels before sending them to a deep
neural network. However, down sampling images incurs spatial detail loss,
renders small segments hard to discriminate, and reverses the spatial
resolution progress obtained by decades of years of efforts. Cropping images
into small patches causes a loss of long-range context information, and
restoring the predicted results to their original size brings extra latency. In
response to the above weaknesses, we present an efficient lightweight semantic
segmentation network termed MKANet. Aimed at the characteristics of top view
high-resolution remote sensing imagery, MKANet utilizes sharing kernels to
simultaneously and equally handle ground segments of inconsistent scales, and
also employs parallel and shallow architecture to boost inference speed and
friendly support image patches more than 10X larger. To enhance boundary and
small segments discrimination, we also propose a method that captures category
impurity areas, exploits boundary information and exerts an extra penalty on
boundaries and small segment misjudgment. Both visual interpretations and
quantitative metrics of extensive experiments demonstrate that MKANet acquires
state-of-the-art accuracy on two land-cover classification datasets and infers
2X faster than other competitive lightweight networks. All these merits
highlight the potential of MKANet in practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Steganography Network. (arXiv:2207.13867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13867">
<div class="article-summary-box-inner">
<span><p>Steganography usually modifies cover media to embed secret data. A new
steganographic approach called generative steganography (GS) has emerged
recently, in which stego images (images containing secret data) are generated
from secret data directly without cover media. However, existing GS schemes are
often criticized for their poor performances. In this paper, we propose an
advanced generative steganography network (GSN) that can generate realistic
stego images without using cover images, in which mutual information is firstly
introduced in stego image generation. Our model contains four sub-networks,
i.e., an image generator ($G$), a discriminator ($D$), a steganalyzer ($S$),
and a data extractor ($E$). $D$ and $S$ act as two adversarial discriminators
to ensure the visual and statistical imperceptibility of generated stego
images. $E$ is to extract the hidden secret from generated stego images. The
generator $G$ is flexibly constructed to synthesize either cover or stego
images with different inputs. It facilitates covert communication by hiding the
function of generating stego images in a normal image generator. A module named
secret block is designed delicately to conceal secret data in the feature maps
during image generation, with which high hiding capacity and image fidelity are
achieved. In addition, a novel hierarchical gradient decay skill is developed
to resist steganalysis detection. Experiments demonstrate the superiority of
our work over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extraction of Vascular Wall in Carotid Ultrasound via a Novel Boundary-Delineation Network. (arXiv:2207.13868v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13868">
<div class="article-summary-box-inner">
<span><p>Ultrasound imaging plays an important role in the diagnosis of vascular
lesions. Accurate segmentation of the vascular wall is important for the
prevention, diagnosis and treatment of vascular diseases. However, existing
methods have inaccurate localization of the vascular wall boundary.
Segmentation errors occur in discontinuous vascular wall boundaries and dark
boundaries. To overcome these problems, we propose a new boundary-delineation
network (BDNet). We use the boundary refinement module to re-delineate the
boundary of the vascular wall to obtain the correct boundary location. We
designed the feature extraction module to extract and fuse multi-scale features
and different receptive field features to solve the problem of dark boundaries
and discontinuous boundaries. We use a new loss function to optimize the model.
The interference of class imbalance on model optimization is prevented to
obtain finer and smoother boundaries. Finally, to facilitate clinical
applications, we design the model to be lightweight. Experimental results show
that our model achieves the best segmentation results and significantly reduces
memory consumption compared to existing models for the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Repulsive Force Unit for Garment Collision Handling in Neural Networks. (arXiv:2207.13871v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13871">
<div class="article-summary-box-inner">
<span><p>Despite recent success, deep learning-based methods for predicting 3D garment
deformation under body motion suffer from interpenetration problems between the
garment and the body. To address this problem, we propose a novel collision
handling neural network layer called Repulsive Force Unit (ReFU). Based on the
signed distance function (SDF) of the underlying body and the current garment
vertex positions, ReFU predicts the per-vertex offsets that push any
interpenetrating vertex to a collision-free configuration while preserving the
fine geometric details. We show that ReFU is differentiable with trainable
parameters and can be integrated into different network backbones that predict
3D garment deformations. Our experiments show that ReFU significantly reduces
the number of collisions between the body and the garment and better preserves
geometric details compared to prior methods based on collision loss or
post-processing optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real Image Restoration via Structure-preserving Complementarity Attention. (arXiv:2207.13879v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13879">
<div class="article-summary-box-inner">
<span><p>Since convolutional neural networks perform well in learning generalizable
image priors from large-scale data, these models have been widely used in image
denoising tasks. However, the computational complexity increases dramatically
as well on complex model. In this paper, We propose a novel lightweight
Complementary Attention Module, which includes a density module and a sparse
module, which can cooperatively mine dense and sparse features for feature
complementary learning to build an efficient lightweight architecture.
Moreover, to reduce the loss of details caused by denoising, this paper
constructs a gradient-based structure-preserving branch. We utilize
gradient-based branches to obtain additional structural priors for denoising,
and make the model pay more attention to image geometric details through
gradient loss optimization.Based on the above, we propose an efficiently Unet
structured network with dual branch, the visual results show that can
effectively preserve the structural details of the original image, we evaluate
benchmarks including SIDD and DND, where SCANet achieves state-of-the-art
performance in PSNR and SSIM while significantly reducing computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperVessel: Segmenting High-resolution Vessel from Low-resolution Retinal Image. (arXiv:2207.13882v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13882">
<div class="article-summary-box-inner">
<span><p>Vascular segmentation extracts blood vessels from images and serves as the
basis for diagnosing various diseases, like ophthalmic diseases.
Ophthalmologists often require high-resolution segmentation results for
analysis, which leads to super-computational load by most existing methods. If
based on low-resolution input, they easily ignore tiny vessels or cause
discontinuity of segmented vessels. To solve these problems, the paper proposes
an algorithm named SuperVessel, which gives out high-resolution and accurate
vessel segmentation using low-resolution images as input. We first take
super-resolution as our auxiliary branch to provide potential high-resolution
detail features, which can be deleted in the test phase. Secondly, we propose
two modules to enhance the features of the interested segmentation region,
including an upsampling with feature decomposition (UFD) module and a feature
interaction module (FIM) with a constraining loss to focus on the interested
features. Extensive experiments on three publicly available datasets
demonstrate that our proposed SuperVessel can segment more tiny vessels with
higher segmentation accuracy IoU over 6%, compared with other state-of-the-art
algorithms. Besides, the stability of SuperVessel is also stronger than other
algorithms. We will release the code after the paper is published.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Accuracy Is Not Enough: The Need for Consistency in Object Detection. (arXiv:2207.13890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13890">
<div class="article-summary-box-inner">
<span><p>Object detectors are vital to many modern computer vision applications.
However, even state-of-the-art object detectors are not perfect. On two images
that look similar to human eyes, the same detector can make different
predictions because of small image distortions like camera sensor noise and
lighting changes. This problem is called inconsistency. Existing accuracy
metrics do not properly account for inconsistency, and similar work in this
area only targets improvements on artificial image distortions. Therefore, we
propose a method to use non-artificial video frames to measure object detection
consistency over time, across frames. Using this method, we show that the
consistency of modern object detectors ranges from 83.2% to 97.1% on different
video datasets from the Multiple Object Tracking Challenge. We conclude by
showing that applying image distortion corrections like .WEBP Image Compression
and Unsharp Masking can improve consistency by as much as 5.1%, with no loss in
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Data Augmentation Technique for Out-of-Distribution Sample Detection using Compounded Corruptions. (arXiv:2207.13916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13916">
<div class="article-summary-box-inner">
<span><p>Modern deep neural network models are known to erroneously classify
out-of-distribution (OOD) test data into one of the in-distribution (ID)
training classes with high confidence. This can have disastrous consequences
for safety-critical applications. A popular mitigation strategy is to train a
separate classifier that can detect such OOD samples at the test time. In most
practical settings OOD examples are not known at the train time, and hence a
key question is: how to augment the ID data with synthetic OOD samples for
training such an OOD detector? In this paper, we propose a novel Compounded
Corruption technique for the OOD data augmentation termed CnC. One of the major
advantages of CnC is that it does not require any hold-out data apart from the
training set. Further, unlike current state-of-the-art (SOTA) techniques, CnC
does not require backpropagation or ensembling at the test time, making our
method much faster at inference. Our extensive comparison with 20 methods from
the major conferences in last 4 years show that a model trained using CnC based
data augmentation, significantly outperforms SOTA, both in terms of OOD
detection accuracy as well as inference time. We include a detailed post-hoc
analysis to investigate the reasons for the success of our method and identify
higher relative entropy and diversity of CnC samples as probable causes. We
also provide theoretical insights via a piece-wise decomposition analysis on a
two-dimensional dataset to reveal (visually and quantitatively) that our
approach leads to a tighter boundary around ID classes, leading to better
detection of OOD samples. Source code link: https://github.com/cnc-ood
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Learning based Degradation Representation for Blind Super-Resolution. (arXiv:2207.13963v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13963">
<div class="article-summary-box-inner">
<span><p>The most of CNN based super-resolution (SR) methods assume that the
degradation is known (\eg, bicubic). These methods will suffer a severe
performance drop when the degradation is different from their assumption.
Therefore, some approaches attempt to train SR networks with the complex
combination of multiple degradations to cover the real degradation space. To
adapt to multiple unknown degradations, introducing an explicit degradation
estimator can actually facilitate SR performance. However, previous explicit
degradation estimation methods usually predict Gaussian blur with the
supervision of groundtruth blur kernels, and estimation errors may lead to SR
failure. Thus, it is necessary to design a method that can extract implicit
discriminative degradation representation. To this end, we propose a
Meta-Learning based Region Degradation Aware SR Network (MRDA), including
Meta-Learning Network (MLN), Degradation Extraction Network (DEN), and Region
Degradation Aware SR Network (RDAN). To handle the lack of groundtruth
degradation, we use the MLN to rapidly adapt to the specific complex
degradation after several iterations and extract implicit degradation
information. Subsequently, a teacher network MRDA$_{T}$ is designed to further
utilize the degradation information extracted by MLN for SR. However, MLN
requires iterating on paired low-resolution (LR) and corresponding
high-resolution (HR) images, which is unavailable in the inference phase.
Therefore, we adopt knowledge distillation (KD) to make the student network
learn to directly extract the same implicit degradation representation (IDR) as
the teacher from LR images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effects of Different Types of Label Noise in Multi-Label Remote Sensing Image Classification. (arXiv:2207.13975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13975">
<div class="article-summary-box-inner">
<span><p>The development of accurate methods for multi-label classification (MLC) of
remote sensing (RS) images is one of the most important research topics in RS.
To address MLC problems, the use of deep neural networks that require a high
number of reliable training images annotated by multiple land-cover class
labels (multi-labels) have been found popular in RS. However, collecting such
annotations is time-consuming and costly. A common procedure to obtain
annotations at zero labeling cost is to rely on thematic products or
crowdsourced labels. As a drawback, these procedures come with the risk of
label noise that can distort the learning process of the MLC algorithms. In the
literature, most label noise robust methods are designed for single label
classification (SLC) problems in computer vision (CV), where each image is
annotated by a single label. Unlike SLC, label noise in MLC can be associated
with: 1) subtractive label-noise (a land cover class label is not assigned to
an image while that class is present in the image); 2) additive label-noise (a
land cover class label is assigned to an image although that class is not
present in the given image); and 3) mixed label-noise (a combination of both).
In this paper, we investigate three different noise robust CV SLC methods and
adapt them to be robust for multi-label noise scenarios in RS. During
experiments we study the effects of different types of multi-label noise and
evaluate the adapted methods rigorously. To this end, we also introduce a
synthetic multi-label noise injection strategy that is more adequate to
simulate operational scenarios compared to the uniform label noise injection
strategy, in which the labels of absent and present classes are flipped at
uniform probability. Further, we study the relevance of different evaluation
metrics in MLC problems under noisy multi-labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Mask Transfiner for High-Quality Video Instance Segmentation. (arXiv:2207.14012v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14012">
<div class="article-summary-box-inner">
<span><p>While Video Instance Segmentation (VIS) has seen rapid progress, current
approaches struggle to predict high-quality masks with accurate boundary
details. Moreover, the predicted segmentations often fluctuate over time,
suggesting that temporal consistency cues are neglected or not fully utilized.
In this paper, we set out to tackle these issues, with the aim of achieving
highly detailed and more temporally stable mask predictions for VIS. We first
propose the Video Mask Transfiner (VMT) method, capable of leveraging
fine-grained high-resolution features thanks to a highly efficient video
transformer structure. Our VMT detects and groups sparse error-prone
spatio-temporal regions of each tracklet in the video segment, which are then
refined using both local and instance-level cues. Second, we identify that the
coarse boundary annotations of the popular YouTube-VIS dataset constitute a
major limiting factor. Based on our VMT architecture, we therefore design an
automated annotation refinement approach by iterative training and
self-correction. To benchmark high-quality mask predictions for VIS, we
introduce the HQ-YTVIS dataset, consisting of a manually re-annotated test set
and our automatically refined training data. We compare VMT with the most
recent state-of-the-art methods on the HQ-YTVIS, as well as the Youtube-VIS,
OVIS and BDD100K MOTS benchmarks. Experimental results clearly demonstrate the
efficacy and effectiveness of our method on segmenting complex and dynamic
objects, by capturing precise details.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer. (arXiv:2207.14024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14024">
<div class="article-summary-box-inner">
<span><p>Large-scale deployment of autonomous vehicles has been continually delayed
due to safety concerns. On the one hand, comprehensive scene understanding is
indispensable, a lack of which would result in vulnerability to rare but
complex traffic situations, such as the sudden emergence of unknown objects.
However, reasoning from a global context requires access to sensors of multiple
types and adequate fusion of multi-modal sensor signals, which is difficult to
achieve. On the other hand, the lack of interpretability in learning models
also hampers the safety with unverifiable failure causes. In this paper, we
propose a safety-enhanced autonomous driving framework, named Interpretable
Sensor Fusion Transformer(InterFuser), to fully process and fuse information
from multi-modal multi-view sensors for achieving comprehensive scene
understanding and adversarial event detection. Besides, intermediate
interpretable features are generated from our framework, which provide more
semantics and are exploited to better constrain actions to be within the safe
sets. We conducted extensive experiments on CARLA benchmarks, where our model
outperforms prior methods, ranking the first on the public CARLA Leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separable Quaternion Matrix Factorization for Polarization Images. (arXiv:2207.14039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14039">
<div class="article-summary-box-inner">
<span><p>Polarization is a unique characteristic of transverse wave and is represented
by Stokes parameters. Analysis of polarization states can reveal valuable
information about the sources. In this paper, we propose a separable low-rank
quaternion linear mixing model to polarized signals: we assume each column of
the source factor matrix equals a column of polarized data matrix and refer to
the corresponding problem as separable quaternion matrix factorization (SQMF).
We discuss some properties of the matrix that can be decomposed by SQMF. To
determine the source factor matrix in quaternion space, we propose a heuristic
algorithm called quaternion successive projection algorithm (QSPA) inspired by
the successive projection algorithm. To guarantee the effectiveness of QSPA, a
new normalization operator is proposed for the quaternion matrix. We use a
block coordinate descent algorithm to compute nonnegative factor activation
matrix in real number space. We test our method on the applications of
polarization image representation and spectro-polarimetric imaging unmixing to
verify its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Self-Tuning Data Association for Geo-Referencing Using Lane Markings. (arXiv:2207.14042v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14042">
<div class="article-summary-box-inner">
<span><p>Localization in aerial imagery-based maps offers many advantages, such as
global consistency, geo-referenced maps, and the availability of publicly
accessible data. However, the landmarks that can be observed from both aerial
imagery and on-board sensors is limited. This leads to ambiguities or aliasing
during the data association.
</p>
<p>Building upon a highly informative representation (that allows efficient data
association), this paper presents a complete pipeline for resolving these
ambiguities. Its core is a robust self-tuning data association that adapts the
search area depending on the entropy of the measurements. Additionally, to
smooth the final result, we adjust the information matrix for the associated
data as a function of the relative transform produced by the data association
process.
</p>
<p>We evaluate our method on real data from urban and rural scenarios around the
city of Karlsruhe in Germany. We compare state-of-the-art outlier mitigation
methods with our self-tuning approach, demonstrating a considerable
improvement, especially for outer-urban scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Strands: Learning Hair Geometry and Appearance from Multi-View Images. (arXiv:2207.14067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14067">
<div class="article-summary-box-inner">
<span><p>We present Neural Strands, a novel learning framework for modeling accurate
hair geometry and appearance from multi-view image inputs. The learned hair
model can be rendered in real-time from any viewpoint with high-fidelity
view-dependent effects. Our model achieves intuitive shape and style control
unlike volumetric counterparts. To enable these properties, we propose a novel
hair representation based on a neural scalp texture that encodes the geometry
and appearance of individual strands at each texel location. Furthermore, we
introduce a novel neural rendering framework based on rasterization of the
learned hair strands. Our neural rendering is strand-accurate and anti-aliased,
making the rendering view-consistent and photorealistic. Combining appearance
with a multi-view geometric prior, we enable, for the first time, the joint
learning of appearance and explicit hair geometry from a multi-view setup. We
demonstrate the efficacy of our approach in terms of fidelity and efficiency
for various hairstyles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PEA: Improving the Performance of ReLU Networks for Free by Using Progressive Ensemble Activations. (arXiv:2207.14074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14074">
<div class="article-summary-box-inner">
<span><p>In recent years novel activation functions have been proposed to improve the
performance of neural networks, and they show superior performance compared to
the ReLU counterpart. However, there are environments, where the availability
of complex activations is limited, and usually only the ReLU is supported. In
this paper we propose methods that can be used to improve the performance of
ReLU networks by using these efficient novel activations during model training.
More specifically, we propose ensemble activations that are composed of the
ReLU and one of these novel activations. Furthermore, the coefficients of the
ensemble are neither fixed nor learned, but are progressively updated during
the training process in a way that by the end of the training only the ReLU
activations remain active in the network and the other activations can be
removed. This means that in inference time the network contains ReLU
activations only. We perform extensive evaluations on the ImageNet
classification task using various compact network architectures and various
novel activation functions. Results show 0.2-0.8% top-1 accuracy gain, which
confirms the applicability of the proposed methods. Furthermore, we demonstrate
the proposed methods on semantic segmentation and we boost the performance of a
compact segmentation network by 0.34% mIOU on the Cityscapes dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topological Analysis of Ensembles of Hydrodynamic Turbulent Flows -- An Experimental Study. (arXiv:2207.14080v1 [physics.flu-dyn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14080">
<div class="article-summary-box-inner">
<span><p>This application paper presents a comprehensive experimental evaluation of
the suitability of Topological Data Analysis (TDA) for the quantitative
comparison of turbulent flows. Specifically, our study documents the usage of
the persistence diagram of the maxima of flow enstrophy (an established
vorticity indicator), for the topological representation of 180 ensemble
members, generated by a coarse sampling of the parameter space of five
numerical solvers. We document five main hypotheses reported by domain experts,
describing their expectations regarding the variability of the flows generated
by the distinct solver configurations. We contribute three evaluation protocols
to assess the validation of the above hypotheses by two comparison measures:
(i) a standard distance used in scientific imaging (the L2 norm) and (ii) an
established topological distance between persistence diagrams (the
L2-Wasserstein metric). Extensive experiments on the input ensemble demonstrate
the superiority of the topological distance (ii) to report as close to each
other flows which are expected to be similar by domain experts, due to the
configuration of their vortices. Overall, the insights reported by our study
bring an experimental evidence of the suitability of TDA for representing and
comparing turbulent flows, thereby providing to the fluid dynamics community
confidence for its usage in future work. Also, our flow data and evaluation
protocols provide to the TDA community an application-approved benchmark for
the evaluation and design of further topological distances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Camouflaged Object Detection with Scribble Annotations. (arXiv:2207.14083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14083">
<div class="article-summary-box-inner">
<span><p>Existing camouflaged object detection (COD) methods rely heavily on
large-scale datasets with pixel-wise annotations. However, due to the ambiguous
boundary, it is very time-consuming and labor-intensive to annotate camouflage
objects pixel-wisely (which takes ~ 60 minutes per image). In this paper, we
propose the first weakly-supervised camouflaged object detection (COD) method,
using scribble annotations as supervision. To achieve this, we first construct
a scribble-based camouflaged object dataset with 4,040 images and corresponding
scribble annotations. It is worth noting that annotating the scribbles used in
our dataset takes only ~ 10 seconds per image, which is 360 times faster than
per-pixel annotations. However, the network directly using scribble annotations
for supervision will fail to localize the boundary of camouflaged objects and
tend to have inconsistent predictions since scribble annotations only describe
the primary structure of objects without details. To tackle this problem, we
propose a novel consistency loss composed of two parts: a reliable cross-view
loss to attain reliable consistency over different images, and a soft
inside-view loss to maintain consistency inside a single prediction map.
Besides, we observe that humans use semantic information to segment regions
near boundaries of camouflaged objects. Therefore, we design a feature-guided
loss, which includes visual features directly extracted from images and
semantically significant features captured by models. Moreover, we propose a
novel network that detects camouflaged objects by scribble learning on
structural information and semantic relations. Experimental results show that
our model outperforms relevant state-of-the-art methods on three COD benchmarks
with an average improvement of 11.0% on MAE, 3.2% on S-measure, 2.5% on
E-measure and 4.4% on weighted F-measure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CubeMLP: A MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation. (arXiv:2207.14087v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14087">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis and depression estimation are two important
research topics that aim to predict human mental states using multimodal data.
Previous research has focused on developing effective fusion strategies for
exchanging and integrating mind-related information from different modalities.
Some MLP-based techniques have recently achieved considerable success in a
variety of computer vision tasks. Inspired by this, we explore multimodal
approaches with a feature-mixing perspective in this study. To this end, we
introduce CubeMLP, a multimodal feature processing framework based entirely on
MLP. CubeMLP consists of three independent MLP units, each of which has two
affine transformations. CubeMLP accepts all relevant modality features as input
and mixes them across three axes. After extracting the characteristics using
CubeMLP, the mixed multimodal features are flattened for task predictions. Our
experiments are conducted on sentiment analysis datasets: CMU-MOSI and
CMU-MOSEI, and depression estimation dataset: AVEC2019. The results show that
CubeMLP can achieve state-of-the-art performance with a much lower computing
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Large-Scale Small Object Detection: Survey and Benchmarks. (arXiv:2207.14096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14096">
<div class="article-summary-box-inner">
<span><p>With the rise of deep convolutional neural networks, object detection has
achieved prominent advances in past years. However, such prosperity could not
camouflage the unsatisfactory situation of Small Object Detection (SOD), one of
the notoriously challenging tasks in computer vision, owing to the poor visual
appearance and noisy representation caused by the intrinsic structure of small
targets. In addition, large-scale dataset for benchmarking small object
detection methods remains a bottleneck. In this paper, we first conduct a
thorough review of small object detection. Then, to catalyze the development of
SOD, we construct two large-scale Small Object Detection dAtasets (SODA),
SODA-D and SODA-A, which focus on the Driving and Aerial scenarios
respectively. SODA-D includes 24704 high-quality traffic images and 277596
instances of 9 categories. For SODA-A, we harvest 2510 high-resolution aerial
images and annotate 800203 instances over 9 classes. The proposed datasets, as
we know, are the first-ever attempt to large-scale benchmarks with a vast
collection of exhaustively annotated instances tailored for multi-category SOD.
Finally, we evaluate the performance of mainstream methods on SODA. We expect
the released benchmarks could facilitate the development of SOD and spawn more
breakthroughs in this field. Datasets and codes will be available soon at:
\url{https://shaunyuan22.github.io/SODA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RHA-Net: An Encoder-Decoder Network with Residual Blocks and Hybrid Attention Mechanisms for Pavement Crack Segmentation. (arXiv:2207.14166v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14166">
<div class="article-summary-box-inner">
<span><p>The acquisition and evaluation of pavement surface data play an essential
role in pavement condition evaluation. In this paper, an efficient and
effective end-to-end network for automatic pavement crack segmentation, called
RHA-Net, is proposed to improve the pavement crack segmentation accuracy. The
RHA-Net is built by integrating residual blocks (ResBlocks) and hybrid
attention blocks into the encoder-decoder architecture. The ResBlocks are used
to improve the ability of RHA-Net to extract high-level abstract features. The
hybrid attention blocks are designed to fuse both low-level features and
high-level features to help the model focus on correct channels and areas of
cracks, thereby improving the feature presentation ability of RHA-Net. An image
data set containing 789 pavement crack images collected by a self-designed
mobile robot is constructed and used for training and evaluating the proposed
model. Compared with other state-of-the-art networks, the proposed model
achieves better performance and the functionalities of adding residual blocks
and hybrid attention mechanisms are validated in a comprehensive ablation
study. Additionally, a light-weighted version of the model generated by
introducing depthwise separable convolution achieves better a performance and a
much faster processing speed with 1/30 of the number of U-Net parameters. The
developed system can segment pavement crack in real-time on an embedded device
Jetson TX2 (25 FPS). The video taken in real-time experiments is released at
https://youtu.be/3XIogk0fiG4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-oriented learned image compression. (arXiv:2207.14168v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14168">
<div class="article-summary-box-inner">
<span><p>In recent years, with the development of deep neural networks, end-to-end
optimized image compression has made significant progress and exceeded the
classic methods in terms of rate-distortion performance. However, most
learning-based image compression methods are unlabeled and do not consider
image semantics or content when optimizing the model. In fact, human eyes have
different sensitivities to different content, so the image content also needs
to be considered. In this paper, we propose a content-oriented image
compression method, which handles different kinds of image contents with
different strategies. Extensive experiments show that the proposed method
achieves competitive subjective results compared with state-of-the-art
end-to-end learned image compression methods or classic methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aligned Matching for Enhanced DETR Convergence and Multi-Scale Feature Fusion. (arXiv:2207.14172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14172">
<div class="article-summary-box-inner">
<span><p>The recently proposed DEtection TRansformer (DETR) has established a fully
end-to-end paradigm for object detection. However, DETR suffers from slow
training convergence, which hinders its applicability to various detection
tasks. We observe that DETR's slow convergence is largely attributed to the
difficulty in matching object queries to relevant regions due to the unaligned
semantics between object queries and encoded image features. With this
observation, we design Semantic-Aligned-Matching DETR++ (SAM-DETR++) to
accelerate DETR's convergence and improve detection performance. The core of
SAM-DETR++ is a plug-and-play module that projects object queries and encoded
image features into the same feature embedding space, where each object query
can be easily matched to relevant regions with similar semantics. Besides,
SAM-DETR++ searches for multiple representative keypoints and exploits their
features for semantic-aligned matching with enhanced representation capacity.
Furthermore, SAM-DETR++ can effectively fuse multi-scale features in a
coarse-to-fine manner on the basis of the designed semantic-aligned matching.
Extensive experiments show that the proposed SAM-DETR++ achieves superior
convergence speed and competitive detection accuracy. Additionally, as a
plug-and-play method, SAM-DETR++ can complement existing DETR convergence
solutions with even better performance, achieving 44.8% AP with merely 12
training epochs and 49.1% AP with 50 training epochs on COCO val2017 with
ResNet-50. Codes are available at https://github.com/ZhangGongjie/SAM-DETR .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Limited Annotations: A Survey on Deep Semi-Supervised Learning for Medical Image Segmentation. (arXiv:2207.14191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14191">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation is a fundamental and critical step in many
image-guided clinical approaches. Recent success of deep learning-based
segmentation methods usually relies on a large amount of labeled data, which is
particularly difficult and costly to obtain especially in the medical imaging
domain where only experts can provide reliable and accurate annotations.
Semi-supervised learning has emerged as an appealing strategy and been widely
applied to medical image segmentation tasks to train deep models with limited
annotations. In this paper, we present a comprehensive review of recently
proposed semi-supervised learning methods for medical image segmentation and
summarized both the technical novelties and empirical results. Furthermore, we
analyze and discuss the limitations and several unsolved problems of existing
approaches. We hope this review could inspire the research community to explore
solutions for this challenge and further promote the developments in medical
image segmentation field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Cross-Person Cues for Body-Part Interactiveness Learning in HOI Detection. (arXiv:2207.14192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14192">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) detection plays a crucial role in activity
understanding. Though significant progress has been made, interactiveness
learning remains a challenging problem in HOI detection: existing methods
usually generate redundant negative H-O pair proposals and fail to effectively
extract interactive pairs. Though interactiveness has been studied in both
whole body- and part- level and facilitates the H-O pairing, previous works
only focus on the target person once (i.e., in a local perspective) and
overlook the information of the other persons. In this paper, we argue that
comparing body-parts of multi-person simultaneously can afford us more useful
and supplementary interactiveness cues. That said, to learn body-part
interactiveness from a global perspective: when classifying a target person's
body-part interactiveness, visual cues are explored not only from
herself/himself but also from other persons in the image. We construct
body-part saliency maps based on self-attention to mine cross-person
informative cues and learn the holistic relationships between all the
body-parts. We evaluate the proposed method on widely-used benchmarks HICO-DET
and V-COCO. With our new perspective, the holistic global-local body-part
interactiveness learning achieves significant improvements over
state-of-the-art. Our code is available at
https://github.com/enlighten0707/Body-Part-Map-for-Interactiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Voronoi Diagram Subdivision: Towards A Holistic Geometric Framework for Exemplar-free Class-Incremental Learning. (arXiv:2207.14202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14202">
<div class="article-summary-box-inner">
<span><p>Exemplar-free Class-incremental Learning (CIL) is a challenging problem
because rehearsing data from previous phases is strictly prohibited, causing
catastrophic forgetting of Deep Neural Networks (DNNs). In this paper, we
present iVoro, a holistic framework for CIL, derived from computational
geometry. We found Voronoi Diagram (VD), a classical model for space
subdivision, is especially powerful for solving the CIL problem, because VD
itself can be constructed favorably in an incremental manner -- the newly added
sites (classes) will only affect the proximate classes, making the
non-contiguous classes hardly forgettable. Further, in order to find a better
set of centers for VD construction, we colligate DNN with VD using Power
Diagram and show that the VD structure can be optimized by integrating local
DNN models using a divide-and-conquer algorithm. Moreover, our VD construction
is not restricted to the deep feature space, but is also applicable to multiple
intermediate feature spaces, promoting VD to be multi-centered VD (CIVD) that
efficiently captures multi-grained features from DNN. Importantly, iVoro is
also capable of handling uncertainty-aware test-time Voronoi cell assignment
and has exhibited high correlations between geometric uncertainty and
predictive accuracy (up to ~0.9). Putting everything together, iVoro achieves
up to 25.26%, 37.09%, and 33.21% improvements on CIFAR-100, TinyImageNet, and
ImageNet-Subset, respectively, compared to the state-of-the-art non-exemplar
CIL approaches. In conclusion, iVoro enables highly accurate,
privacy-preserving, and geometrically interpretable CIL that is particularly
useful when cross-phase data sharing is forbidden, e.g. in medical
applications. Our code is available at https://machunwei.github.io/ivoro.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Humans disagree with the IoU for measuring object detector localization error. (arXiv:2207.14221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14221">
<div class="article-summary-box-inner">
<span><p>The localization quality of automatic object detectors is typically evaluated
by the Intersection over Union (IoU) score. In this work, we show that humans
have a different view on localization quality. To evaluate this, we conduct a
survey with more than 70 participants. Results show that for localization
errors with the exact same IoU score, humans might not consider that these
errors are equal, and express a preference. Our work is the first to evaluate
IoU with humans and makes it clear that relying on IoU scores alone to evaluate
localization errors might not be sufficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Electricity Price Forecasting Model based on Gated Recurrent Units. (arXiv:2207.14225v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14225">
<div class="article-summary-box-inner">
<span><p>The participation of consumers and producers in demand response programs has
increased in smart grids, which reduces investment and operation costs of power
systems. Also, with the advent of renewable energy sources, the electricity
market is becoming more complex and unpredictable. To effectively implement
demand response programs, forecasting the future price of electricity is very
crucial for producers in the electricity market. Electricity prices are very
volatile and change under the influence of various factors such as temperature,
wind speed, rainfall, intensity of commercial and daily activities, etc.
Therefore, considering the influencing factors as dependent variables can
increase the accuracy of the forecast. In this paper, a model for electricity
price forecasting is presented based on Gated Recurrent Units. The electrical
load consumption is considered as an input variable in this model. Noise in
electricity price seriously reduces the efficiency and effectiveness of
analysis. Therefore, an adaptive noise reducer is integrated into the model for
noise reduction. The SAEs are then used to extract features from the de-noised
electricity price. Finally, the de-noised features are fed into the GRU to
train predictor. Results on real dataset shows that the proposed methodology
can perform effectively in prediction of electricity price.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Recognition by Request. (arXiv:2207.14227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14227">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel protocol of annotation and evaluation for
visual recognition. Different from traditional settings, the protocol does not
require the labeler/algorithm to annotate/recognize all targets (objects,
parts, etc.) at once, but instead raises a number of recognition instructions
and the algorithm recognizes targets by request. This mechanism brings two
beneficial properties to reduce the burden of annotation, namely, (i) variable
granularity: different scenarios can have different levels of annotation, in
particular, object parts can be labeled only in large and clear instances, (ii)
being open-domain: new concepts can be added to the database in minimal costs.
To deal with the proposed setting, we maintain a knowledge base and design a
query-based visual recognition framework that constructs queries on-the-fly
based on the requests. We evaluate the recognition system on two
mixed-annotated datasets, CPP and ADE20K, and demonstrate its promising ability
of learning from partially labeled data as well as adapting to new concepts
with only text labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-thinking and Re-labeling LIDC-IDRI for Robust Pulmonary Cancer Prediction. (arXiv:2207.14238v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14238">
<div class="article-summary-box-inner">
<span><p>The LIDC-IDRI database is the most popular benchmark for lung cancer
prediction. However, with subjective assessment from radiologists, nodules in
LIDC may have entirely different malignancy annotations from the pathological
ground truth, introducing label assignment errors and subsequent supervision
bias during training. The LIDC database thus requires more objective labels for
learning-based cancer prediction. Based on an extra small dataset containing
180 nodules diagnosed by pathological examination, we propose to re-label LIDC
data to mitigate the effect of original annotation bias verified on this robust
benchmark. We demonstrate in this paper that providing new labels by similar
nodule retrieval based on metric learning would be an effective re-labeling
strategy. Training on these re-labeled LIDC nodules leads to improved model
performance, which is enhanced when new labels of uncertain nodules are added.
We further infer that re-labeling LIDC is current an expedient way for robust
lung cancer prediction while building a large pathological-proven nodule
database provides the long-term solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining human parsing with analytical feature extraction and ranking schemes for high-generalization person reidentification. (arXiv:2207.14243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14243">
<div class="article-summary-box-inner">
<span><p>Person reidentification (re-ID) has been receiving increasing attention in
recent years due to its importance for both science and society. Machine
learning and particularly Deep Learning (DL) has become the main re-id tool
that allowed researches to achieve unprecedented accuracy levels on benchmark
datasets. However, there is a known problem of poor generalization of DL
models. That is, models trained to achieve high accuracy on one dataset perform
poorly on other ones and require re-training. To address this issue, we present
a model without trainable parameters which shows great potential for high
generalization. It combines a fully analytical feature extraction and
similarity ranking scheme with DL-based human parsing used to obtain the
initial subregion classification. We show that such combination to a high
extent eliminates the drawbacks of existing analytical methods. We use
interpretable color and texture features which have human-readable similarity
measures associated with them. To verify the proposed method we conduct
experiments on Market1501 and CUHK03 datasets achieving competitive rank-1
accuracy comparable with that of DL-models. Most importantly we show that our
method achieves 63.9% and 93.5% rank-1 cross-domain accuracy when applied to
transfer learning tasks. It is significantly higher than previously reported
30-50% transfer accuracy. We discuss the potential ways of adding new features
to further improve the model. We also show the advantage of interpretable
features for constructing human-generated queries from verbal description to
conduct search without a query image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonteBoxFinder: Detecting and Filtering Primitives to Fit a Noisy Point Cloud. (arXiv:2207.14268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14268">
<div class="article-summary-box-inner">
<span><p>We present MonteBoxFinder, a method that, given a noisy input point cloud,
fits cuboids to the input scene. Our primary contribution is a discrete
optimization algorithm that, from a dense set of initially detected cuboids, is
able to efficiently filter good boxes from the noisy ones. Inspired by recent
applications of MCTS to scene understanding problems, we develop a stochastic
algorithm that is, by design, more efficient for our task. Indeed, the quality
of a fit for a cuboid arrangement is invariant to the order in which the
cuboids are added into the scene. We develop several search baselines for our
problem and demonstrate, on the ScanNet dataset, that our approach is more
efficient and precise. Finally, we strongly believe that our core algorithm is
very general and that it could be extended to many other problems in 3D scene
understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CuDi: Curve Distillation for Efficient and Controllable Exposure Adjustment. (arXiv:2207.14273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14273">
<div class="article-summary-box-inner">
<span><p>We present Curve Distillation, CuDi, for efficient and controllable exposure
adjustment without the requirement of paired or unpaired data during training.
Our method inherits the zero-reference learning and curve-based framework from
an effective low-light image enhancement method, Zero-DCE, with further speed
up in its inference speed, reduction in its model size, and extension to
controllable exposure adjustment. The improved inference speed and lightweight
model are achieved through novel curve distillation that approximates the
time-consuming iterative operation in the conventional curve-based framework by
high-order curve's tangent line. The controllable exposure adjustment is made
possible with a new self-supervised spatial exposure control loss that
constrains the exposure levels of different spatial regions of the output to be
close to the brightness distribution of an exposure map serving as an input
condition. Different from most existing methods that can only correct either
underexposed or overexposed photos, our approach corrects both underexposed and
overexposed photos with a single model. Notably, our approach can additionally
adjust the exposure levels of a photo globally or locally with the guidance of
an input condition exposure map, which can be pre-defined or manually set in
the inference stage. Through extensive experiments, we show that our method is
appealing for its fast, robust, and flexible performance, outperforming
state-of-the-art methods in real scenes. Project page:
https://li-chongyi.github.io/CuDi_files/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The One Where They Reconstructed 3D Humans and Environments in TV Shows. (arXiv:2207.14279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14279">
<div class="article-summary-box-inner">
<span><p>TV shows depict a wide variety of human behaviors and have been studied
extensively for their potential to be a rich source of data for many
applications. However, the majority of the existing work focuses on 2D
recognition tasks. In this paper, we make the observation that there is a
certain persistence in TV shows, i.e., repetition of the environments and the
humans, which makes possible the 3D reconstruction of this content. Building on
this insight, we propose an automatic approach that operates on an entire
season of a TV show and aggregates information in 3D; we build a 3D model of
the environment, compute camera information, static 3D scene structure and body
scale information. Then, we demonstrate how this information acts as rich 3D
context that can guide and improve the recovery of 3D human pose and position
in these environments. Moreover, we show that reasoning about humans and their
environment in 3D enables a broad range of downstream applications:
re-identification, gaze estimation, cinematography and image editing. We apply
our approach on environments from seven iconic TV shows and perform an
extensive evaluation of the proposed system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions. (arXiv:2207.14284v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14284">
<div class="article-summary-box-inner">
<span><p>Recent progress in vision Transformers exhibits great success in various
tasks driven by the new spatial modeling mechanism based on dot-product
self-attention. In this paper, we show that the key ingredients behind the
vision Transformers, namely input-adaptive, long-range and high-order spatial
interactions, can also be efficiently implemented with a convolution-based
framework. We present the Recursive Gated Convolution
($\textit{g}^\textit{n}$Conv) that performs high-order spatial interactions
with gated convolutions and recursive designs. The new operation is highly
flexible and customizable, which is compatible with various variants of
convolution and extends the two-order interactions in self-attention to
arbitrary orders without introducing significant extra computation.
$\textit{g}^\textit{n}$Conv can serve as a plug-and-play module to improve
various vision Transformers and convolution-based models. Based on the
operation, we construct a new family of generic vision backbones named HorNet.
Extensive experiments on ImageNet classification, COCO object detection and
ADE20K semantic segmentation show HorNet outperform Swin Transformers and
ConvNeXt by a significant margin with similar overall architecture and training
configurations. HorNet also shows favorable scalability to more training data
and a larger model size. Apart from the effectiveness in visual encoders, we
also show $\textit{g}^\textit{n}$Conv can be applied to task-specific decoders
and consistently improve dense prediction performance with less computation.
Our results demonstrate that $\textit{g}^\textit{n}$Conv can be a new basic
module for visual modeling that effectively combines the merits of both vision
Transformers and CNNs. Code is available at
https://github.com/raoyongming/HorNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Field Networks for Generalizable Multi-view Scene Representation. (arXiv:2207.14287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14287">
<div class="article-summary-box-inner">
<span><p>Modern 3D computer vision leverages learning to boost geometric reasoning,
mapping image data to classical structures such as cost volumes or epipolar
constraints to improve matching. These architectures are specialized according
to the particular problem, and thus require significant task-specific tuning,
often leading to poor domain generalization performance. Recently, generalist
Transformer architectures have achieved impressive results in tasks such as
optical flow and depth estimation by encoding geometric priors as inputs rather
than as enforced constraints. In this paper, we extend this idea and propose to
learn an implicit, multi-view consistent scene representation, introducing a
series of 3D data augmentation techniques as a geometric inductive prior to
increase view diversity. We also show that introducing view synthesis as an
auxiliary task further improves depth estimation. Our Depth Field Networks
(DeFiNe) achieve state-of-the-art results in stereo and video depth estimation
without explicit geometric constraints, and improve on zero-shot domain
generalization by a wide margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rewriting Geometric Rules of a GAN. (arXiv:2207.14288v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14288">
<div class="article-summary-box-inner">
<span><p>Deep generative models make visual content creation more accessible to novice
users by automating the synthesis of diverse, realistic content based on a
collected dataset. However, the current machine learning approaches miss a key
element of the creative process -- the ability to synthesize things that go far
beyond the data distribution and everyday experience. To begin to address this
issue, we enable a user to "warp" a given model by editing just a handful of
original model outputs with desired geometric changes. Our method applies a
low-rank update to a single model layer to reconstruct edited examples.
Furthermore, to combat overfitting, we propose a latent space augmentation
method based on style-mixing. Our method allows a user to create a model that
synthesizes endless objects with defined geometric changes, enabling the
creation of a new generative model without the burden of curating a large-scale
dataset. We also demonstrate that edited models can be composed to achieve
aggregated effects, and we present an interactive interface to enable users to
create new models through composition. Empirical measurements on multiple test
cases suggest the advantage of our method against recent GAN fine-tuning
methods. Finally, we showcase several applications using the edited models,
including latent space interpolation and image editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Initialization and Alignment for Adversarial Texture Optimization. (arXiv:2207.14289v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.14289">
<div class="article-summary-box-inner">
<span><p>While recovery of geometry from image and video data has received a lot of
attention in computer vision, methods to capture the texture for a given
geometry are less mature. Specifically, classical methods for texture
generation often assume clean geometry and reasonably well-aligned image data.
While very recent methods, e.g., adversarial texture optimization, better
handle lower-quality data obtained from hand-held devices, we find them to
still struggle frequently. To improve robustness, particularly of recent
adversarial texture optimization, we develop an explicit initialization and an
alignment procedure. It deals with complex geometry due to a robust mapping of
the geometry to the texture map and a hard-assignment-based initialization. It
deals with misalignment of geometry and images by integrating fast
image-alignment into the texture refinement optimization. We demonstrate
efficacy of our texture generation on a dataset of 11 scenes with a total of
2807 frames, observing 7.8% and 11.1% relative improvements regarding
perceptual and sharpness measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2009.08753v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08753">
<div class="article-summary-box-inner">
<span><p>Learning to generate new images for a novel category based on only a few
images, named as few-shot image generation, has attracted increasing research
interest. Several state-of-the-art works have yielded impressive results, but
the diversity is still limited. In this work, we propose a novel Delta
Generative Adversarial Network (DeltaGAN), which consists of a reconstruction
subnetwork and a generation subnetwork. The reconstruction subnetwork captures
intra-category transformation, i.e., "delta", between same-category pairs. The
generation subnetwork generates sample-specific "delta" for an input image,
which is combined with this input image to generate a new image within the same
category. Besides, an adversarial delta matching loss is designed to link the
above two subnetworks together. Extensive experiments on five few-shot image
datasets demonstrate the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Deep Morphological Networks with Neural Architecture Search. (arXiv:2106.07714v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07714">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are generated by sequentially performing linear
and non-linear processes. Using a combination of linear and non-linear
procedures is critical for generating a sufficiently deep feature space. The
majority of non-linear operators are derivations of activation functions or
pooling functions. Mathematical morphology is a branch of mathematics that
provides non-linear operators for a variety of image processing problems. We
investigate the utility of integrating these operations in an end-to-end deep
learning framework in this paper. DNNs are designed to acquire a realistic
representation for a particular job. Morphological operators give topological
descriptors that convey salient information about the shapes of objects
depicted in images. We propose a method based on meta-learning to incorporate
morphological operators into DNNs. The learned architecture demonstrates how
our novel morphological operations significantly increase DNN performance on
various tasks, including picture classification and edge detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells. (arXiv:2108.08195v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08195">
<div class="article-summary-box-inner">
<span><p>Due to morphological similarity at the microscopic level, making an accurate
and time-sensitive distinction between blood cells affected by Acute
Lymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage
of machine learning architectures. However, three of the most common models,
VGG, ResNet, and Inception, each come with their own set of flaws with room for
improvement which demands the need for a superior model. ALLNet, the proposed
hybrid convolutional neural network architecture, consists of a combination of
the VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019
(available here) contains 10,691 images of white blood cells which were used to
train and test the models. 7,272 of the images in the dataset are of cells with
ALL and 3,419 of them are of healthy cells. Of the images, 60% were used to
train the model, 20% were used for the cross-validation set, and 20% were used
for the test set. ALLNet outperformed the VGG, ResNet, and the Inception models
across the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%,
a specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803
in the cross-validation set. In the test set, ALLNet achieved an accuracy of
92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of
0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the
clinical workspace can better treat the thousands of people suffering from ALL
across the world, many of whom are children.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised learning for joint SAR and multispectral land cover classification. (arXiv:2108.09075v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09075">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning techniques are gaining popularity due to their
capability of building models that are effective, even when scarce amounts of
labeled data are available. In this paper, we present a framework and specific
tasks for self-supervised pretraining of \textit{multichannel} models, such as
the fusion of multispectral and synthetic aperture radar images. We show that
the proposed self-supervised approach is highly effective at learning features
that correlate with the labels for land cover classification. This is enabled
by an explicit design of pretraining tasks which promotes bridging the gaps
between sensing modalities and exploiting the spectral characteristics of the
input. In a semi-supervised setting, when limited labels are available, using
the proposed self-supervised pretraining, followed by supervised finetuning for
land cover classification with SAR and multispectral data, outperforms
conventional approaches such as purely supervised learning, initialization from
training on ImageNet and other recent self-supervised approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TACS: Taxonomy Adaptive Cross-Domain Semantic Segmentation. (arXiv:2109.04813v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04813">
<div class="article-summary-box-inner">
<span><p>Traditional domain adaptive semantic segmentation addresses the task of
adapting a model to a novel target domain under limited or no additional
supervision. While tackling the input domain gap, the standard domain
adaptation settings assume no domain change in the output space. In semantic
prediction tasks, different datasets are often labeled according to different
semantic taxonomies. In many real-world settings, the target domain task
requires a different taxonomy than the one imposed by the source domain. We
therefore introduce the more general taxonomy adaptive cross-domain semantic
segmentation (TACS) problem, allowing for inconsistent taxonomies between the
two domains. We further propose an approach that jointly addresses the
image-level and label-level domain adaptation. On the label-level, we employ a
bilateral mixed sampling strategy to augment the target domain, and a
relabelling method to unify and align the label spaces. We address the
image-level domain gap by proposing an uncertainty-rectified contrastive
learning method, leading to more domain-invariant and class-discriminative
features. We extensively evaluate the effectiveness of our framework under
different TACS settings: open taxonomy, coarse-to-fine taxonomy, and
implicitly-overlapping taxonomy. Our approach outperforms the previous
state-of-the-art by a large margin, while being capable of adapting to target
taxonomies. Our implementation is publicly available at
https://github.com/ETHRuiGong/TADA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00232">
<div class="article-summary-box-inner">
<span><p>In visual recognition tasks, few-shot learning requires the ability to learn
object categories with few support examples. Its re-popularity in light of the
deep learning development is mainly in image classification. This work focuses
on few-shot semantic segmentation, which is still a largely unexplored field. A
few recent advances are often restricted to single-class few-shot segmentation.
In this paper, we first present a novel multi-way (class) encoding and decoding
architecture which effectively fuses multi-scale query information and
multi-class support information into one query-support embedding. Multi-class
segmentation is directly decoded upon this embedding. For better feature
fusion, a multi-level attention mechanism is proposed within the architecture,
which includes the attention for support feature modulation and attention for
multi-scale combination. Last, to enhance the embedding space learning, an
additional pixel-wise metric learning module is introduced with triplet loss
formulated on the pixel-level embedding of the input image. Extensive
experiments on standard benchmarks PASCAL-5i and COCO-20i show clear benefits
of our method over the state of the art in few-shot segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00246">
<div class="article-summary-box-inner">
<span><p>Perceiving and interacting with 3D articulated objects, such as cabinets,
doors, and faucets, pose particular challenges for future home-assistant robots
performing daily tasks in human environments. Besides parsing the articulated
parts and joint parameters, researchers recently advocate learning manipulation
affordance over the input shape geometry which is more task-aware and
geometrically fine-grained. However, taking only passive observations as
inputs, these methods ignore many hidden but important kinematic constraints
(e.g., joint location and limits) and dynamic factors (e.g., joint friction and
restitution), therefore losing significant accuracy for test cases with such
uncertainties. In this paper, we propose a novel framework, named AdaAfford,
that learns to perform very few test-time interactions for quickly adapting the
affordance priors to more accurate instance-specific posteriors. We conduct
large-scale experiments using the PartNet-Mobility dataset and prove that our
system performs better than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Entity Tagging with Multimodal Knowledge Base. (arXiv:2201.00693v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00693">
<div class="article-summary-box-inner">
<span><p>To enhance research on multimodal knowledge base and multimodal information
processing, we propose a new task called multimodal entity tagging (MET) with a
multimodal knowledge base (MKB). We also develop a dataset for the problem
using an existing MKB. In an MKB, there are entities and their associated texts
and images. In MET, given a text-image pair, one uses the information in the
MKB to automatically identify the related entity in the text-image pair. We
solve the task by using the information retrieval paradigm and implement
several baselines using state-of-the-art methods in NLP and CV. We conduct
extensive experiments and make analyses on the experimental results. The
results show that the task is challenging, but current technologies can achieve
relatively high performance. We will release the dataset, code, and models for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Snapshot Spectral Compressive Imaging Reconstruction Using Convolution and Contextual Transformer. (arXiv:2201.05768v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05768">
<div class="article-summary-box-inner">
<span><p>Spectral compressive imaging (SCI) is able to encode the high-dimensional
hyperspectral image to a 2D measurement, and then uses algorithms to
reconstruct the spatio-spectral data-cube. At present, the main bottleneck of
SCI is the reconstruction algorithm, and the state-of-the-art (SOTA)
reconstruction methods generally face the problem of long reconstruction time
and/or poor detail recovery. In this paper, we propose a novel hybrid network
module, namely CCoT (Convolution and Contextual Transformer) block, which can
acquire the inductive bias ability of convolution and the powerful modeling
ability of transformer simultaneously,and is conducive to improving the quality
of reconstruction to restore fine details. We integrate the proposed CCoT block
into deep unfolding framework based on the generalized alternating projection
algorithm, and further propose the GAP-CCoT network. Through the experiments of
extensive synthetic and real data, our proposed model achieves higher
reconstruction quality ($&gt;$2dB in PSNR on simulated benchmark datasets) and
shorter running time than existing SOTA algorithms by a large margin. The code
and models are publicly available at https://github.com/ucaswangls/GAP-CCoT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration. (arXiv:2203.00927v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00927">
<div class="article-summary-box-inner">
<span><p>Traditional video-based human activity recognition has experienced remarkable
progress linked to the rise of deep learning, but this effect was slower as it
comes to the downstream task of driver behavior understanding. Understanding
the situation inside the vehicle cabin is essential for Advanced Driving
Assistant System (ADAS) as it enables identifying distraction, predicting
driver's intent and leads to more convenient human-vehicle interaction. At the
same time, driver observation systems face substantial obstacles as they need
to capture different granularities of driver states, while the complexity of
such secondary activities grows with the rising automation and increased driver
freedom. Furthermore, a model is rarely deployed under conditions identical to
the ones in the training set, as sensor placements and types vary from vehicle
to vehicle, constituting a substantial obstacle for real-life deployment of
data-driven models. In this work, we present a novel vision-based framework for
recognizing secondary driver behaviours based on visual transformers and an
additional augmented feature distribution calibration module. This module
operates in the latent feature-space enriching and diversifying the training
set at feature-level in order to improve generalization to novel data
appearances, (e.g., sensor changes) and general feature quality. Our framework
consistently leads to better recognition rates, surpassing previous
state-of-the-art results of the public Drive&amp;Act benchmark on all granularity
levels. Our code is publicly available at
https://github.com/KPeng9510/TransDARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02486">
<div class="article-summary-box-inner">
<span><p>In many object recognition applications, the set of possible categories is an
open set, and the deployed recognition system will encounter novel objects
belonging to categories unseen during training. Detecting such "novel category"
objects is usually formulated as an anomaly detection problem. Anomaly
detection algorithms for feature-vector data identify anomalies as outliers,
but outlier detection has not worked well in deep learning. Instead, methods
based on the computed logits of visual object classifiers give state-of-the-art
performance. This paper proposes the Familiarity Hypothesis that these methods
succeed because they are detecting the absence of familiar learned features
rather than the presence of novelty. This distinction is important, because
familiarity-based detection will fail in many situations where novelty is
present. For example when an image contains both a novel object and a familiar
one, the familiarity score will be high, so the novel object will not be
noticed. The paper reviews evidence from the literature and presents additional
evidence from our own experiments that provide strong support for this
hypothesis. The paper concludes with a discussion of whether familiarity-based
detection is an inevitable consequence of representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Stream Multi-Level Alignment for Vision-Language Pretraining. (arXiv:2203.14395v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14395">
<div class="article-summary-box-inner">
<span><p>Self-supervised vision-language pretraining from pure images and text with a
contrastive loss is effective, but ignores fine-grained alignment due to a
dual-stream architecture that aligns image and text representations only on a
global level. Earlier, supervised, non-contrastive methods were capable of
finer-grained alignment, but required dense annotations that were not scalable.
We propose a single stream architecture that aligns images and language at
multiple levels: global, fine-grained patch-token, and conceptual/semantic,
using two novel tasks: symmetric cross-modality reconstruction (XMM) and a
pseudo-labeled key word prediction (PSL). In XMM, we mask input tokens from one
modality and use cross-modal information to reconstruct the masked token, thus
improving fine-grained alignment between the two modalities. In PSL, we use
attention to select keywords in a caption, use a momentum encoder to recommend
other important keywords that are missing from the caption but represented in
the image, and then train the visual encoder to predict the presence of those
keywords, helping it learn semantic concepts that are essential for grounding a
textual token to an image region. We demonstrate competitive performance and
improved data efficiency on image-text retrieval, grounding, visual question
answering/reasoning against larger models and models trained on more data. Code
and models available at zaidkhan.me/SIMLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15371">
<div class="article-summary-box-inner">
<span><p>Image BERT pre-training with masked image modeling (MIM) becomes a popular
practice to cope with self-supervised representation learning. A seminal work,
BEiT, casts MIM as a classification task with a visual vocabulary, tokenizing
the continuous visual signals into discrete vision tokens using a pre-learned
dVAE. Despite a feasible solution, the improper discretization hinders further
improvements of image pre-training. Since image discretization has no
ground-truth answers, we believe that the masked patch should not be assigned
with a unique token id even if a better tokenizer can be obtained. In this
work, we introduce an improved BERT-style image pre-training method, namely
mc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice
training objectives. Specifically, the multi-choice supervision for the masked
image patches is formed by the soft probability vectors of the discrete token
ids, which are predicted by the off-the-shelf image tokenizer and further
refined by high-level inter-patch perceptions resorting to the observation that
similar patches should share their choices. Extensive experiments on
classification, segmentation, and detection tasks demonstrate the superiority
of our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning
accuracy on ImageNet-1K classification, 49.2% AP^b and 44.0% AP^m of object
detection and instance segmentation on COCO, 50.8% mIOU on ADE20K semantic
segmentation, outperforming the competitive counterparts. The code will be
available at https://github.com/lixiaotong97/mc-BEiT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts. (arXiv:2204.02028v2 [physics.ao-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02028">
<div class="article-summary-box-inner">
<span><p>Despite continuous improvements, precipitation forecasts are still not as
accurate and reliable as those of other meteorological variables. A major
contributing factor to this is that several key processes affecting
precipitation distribution and intensity occur below the resolved scale of
global weather models. Generative adversarial networks (GANs) have been
demonstrated by the computer vision community to be successful at
super-resolution problems, i.e., learning to add fine-scale structure to coarse
images. Leinonen et al. (2020) previously applied a GAN to produce ensembles of
reconstructed high-resolution atmospheric fields, given coarsened input data.
In this paper, we demonstrate this approach can be extended to the more
challenging problem of increasing the accuracy and resolution of comparatively
low-resolution input from a weather forecasting model, using high-resolution
radar measurements as a "ground truth". The neural network must learn to add
resolution and structure whilst accounting for non-negligible forecast error.
We show that GANs and VAE-GANs can match the statistical properties of
state-of-the-art pointwise post-processing methods whilst creating
high-resolution, spatially coherent precipitation maps. Our model compares
favourably to the best existing downscaling methods in both pixel-wise and
pooled CRPS scores, power spectrum information and rank histograms (used to
assess calibration). We test our models and show that they perform in a range
of scenarios, including heavy rainfall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning. (arXiv:2204.02863v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02863">
<div class="article-summary-box-inner">
<span><p>We present DOME, a novel method for one-shot imitation learning, where a task
can be learned from just a single demonstration and then be deployed
immediately, without any further data collection or training. DOME does not
require prior task or object knowledge, and can perform the task in novel
object configurations and with distractors. At its core, DOME uses an
image-conditioned object segmentation network followed by a learned visual
servoing network, to move the robot's end-effector to the same relative pose to
the object as during the demonstration, after which the task can be completed
by replaying the demonstration's end-effector velocities. We show that DOME
achieves near 100% success rate on 7 real-world everyday tasks, and we perform
several studies to thoroughly understand each individual component of DOME.
Videos and supplementary material are available at:
https://www.robot-learning.uk/dome .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03359">
<div class="article-summary-box-inner">
<span><p>Image-Text matching (ITM) is a common task for evaluating the quality of
Vision and Language (VL) models. However, existing ITM benchmarks have a
significant limitation. They have many missing correspondences, originating
from the data construction process itself. For example, a caption is only
matched with one image although the caption can be matched with other similar
images and vice versa. To correct the massive false negatives, we construct the
Extended COCO Validation (ECCV) Caption dataset by supplying the missing
associations with machine and human annotators. We employ five state-of-the-art
ITM models with diverse properties for our annotation process. Our dataset
provides x3.6 positive image-to-caption associations and x8.5 caption-to-image
associations compared to the original MS-COCO. We also propose to use an
informative ranking-based metric mAP@R, rather than the popular Recall@K (R@K).
We re-evaluate the existing 25 VL models on existing and proposed benchmarks.
Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K
R@K, CxC R@1 are highly correlated with each other, while the rankings change
when we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias
introduced by the choice of machine annotator. Source code and dataset are
available at https://github.com/naver-ai/eccv-caption
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Few-Shot Part Segmentation using Coarse Supervision. (arXiv:2204.05393v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05393">
<div class="article-summary-box-inner">
<span><p>A significant bottleneck in training deep networks for part segmentation is
the cost of obtaining detailed annotations. We propose a framework to exploit
coarse labels such as figure-ground masks and keypoint locations that are
readily available for some categories to improve part segmentation models. A
key challenge is that these annotations were collected for different tasks and
with different labeling styles and cannot be readily mapped to the part labels.
To this end, we propose to jointly learn the dependencies between labeling
styles and the part segmentation model, allowing us to utilize supervision from
diverse labels. To evaluate our approach we develop a benchmark on the
Caltech-UCSD birds and OID Aircraft dataset. Our approach outperforms baselines
based on multi-task learning, semi-supervised learning, and competitive methods
relying on loss functions manually designed to exploit sparse-supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly. (arXiv:2204.13631v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13631">
<div class="article-summary-box-inner">
<span><p>Machine learning has advanced dramatically, narrowing the accuracy gap to
humans in multimodal tasks like visual question answering (VQA). However, while
humans can say "I don't know" when they are uncertain (i.e., abstain from
answering a question), such ability has been largely neglected in multimodal
research, despite the importance of this problem to the usage of VQA in real
settings. In this work, we promote a problem formulation for reliable VQA,
where we prefer abstention over providing an incorrect answer. We first enable
abstention capabilities for several VQA models, and analyze both their
coverage, the portion of questions answered, and risk, the error on that
portion. For that, we explore several abstention approaches. We find that
although the best performing models achieve over 71% accuracy on the VQA v2
dataset, introducing the option to abstain by directly using a model's softmax
scores limits them to answering less than 8% of the questions to achieve a low
risk of error (i.e., 1%). This motivates us to utilize a multimodal selection
function to directly estimate the correctness of the predicted answers, which
we show can increase the coverage by, for example, 2.4x from 6.8% to 16.3% at
1% risk. While it is important to analyze both coverage and risk, these metrics
have a trade-off which makes comparing VQA models challenging. To address this,
we also propose an Effective Reliability metric for VQA that places a larger
cost on incorrect answers compared to abstentions. This new problem
formulation, metric, and analysis for VQA provide the groundwork for building
effective and reliable VQA models that have the self-awareness to abstain if
and only if they don't know the answer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinction Maximization Loss: Efficiently Improving Uncertainty Estimation and Out-of-Distribution Detection by Simply Replacing the Loss and Calibrating. (arXiv:2205.05874v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05874">
<div class="article-summary-box-inner">
<span><p>Building robust deterministic neural networks remains a challenge. On the one
hand, some approaches improve out-of-distribution detection at the cost of
reducing classification accuracy in some situations. On the other hand, some
methods simultaneously increase classification accuracy, uncertainty
estimation, and out-of-distribution detection at the expense of reducing the
inference efficiency. In this paper, we propose training deterministic neural
networks using our DisMax loss, which works as a drop-in replacement for the
usual SoftMax loss (i.e., the combination of the linear output layer, the
SoftMax activation, and the cross-entropy loss). Starting from the IsoMax+
loss, we create each logit based on the distances to all prototypes, rather
than just the one associated with the correct class. We also introduce a
mechanism to combine images to construct what we call fractional probability
regularization. Moreover, we present a fast way to calibrate the network after
training. Finally, we propose a composite score to perform out-of-distribution
detection. Our experiments show that DisMax usually outperforms current
approaches simultaneously in classification accuracy, uncertainty estimation,
and out-of-distribution detection while maintaining deterministic neural
network inference efficiency. The code to reproduce the results is available at
https://github.com/dlmacedo/distinction-maximization-loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localized Vision-Language Matching for Open-vocabulary Object Detection. (arXiv:2205.06160v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06160">
<div class="article-summary-box-inner">
<span><p>In this work, we propose an open-vocabulary object detection method that,
based on image-caption pairs, learns to detect novel object classes along with
a given set of known classes. It is a two-stage training approach that first
uses a location-guided image-caption matching technique to learn class labels
for both novel and known classes in a weakly-supervised manner and second
specializes the model for the object detection task using known class
annotations. We show that a simple language model fits better than a large
contextualized language model for detecting novel objects. Moreover, we
introduce a consistency-regularization technique to better exploit
image-caption pair information. Our method compares favorably to existing
open-vocabulary detection approaches while being data-efficient. Source code is
available at https://github.com/lmb-freiburg/locov .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TBraTS: Trusted Brain Tumor Segmentation. (arXiv:2206.09309v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09309">
<div class="article-summary-box-inner">
<span><p>Despite recent improvements in the accuracy of brain tumor segmentation, the
results still exhibit low levels of confidence and robustness. Uncertainty
estimation is one effective way to change this situation, as it provides a
measure of confidence in the segmentation results. In this paper, we propose a
trusted brain tumor segmentation network which can generate robust segmentation
results and reliable uncertainty estimations without excessive computational
burden and modification of the backbone network. In our method, uncertainty is
modeled explicitly using subjective logic theory, which treats the predictions
of backbone neural network as subjective opinions by parameterizing the class
probabilities of the segmentation as a Dirichlet distribution. Meanwhile, the
trusted segmentation framework learns the function that gathers reliable
evidence from the feature leading to the final segmentation results. Overall,
our unified trusted segmentation framework endows the model with reliability
and robustness to out-of-distribution samples. To evaluate the effectiveness of
our model in robustness and reliability, qualitative and quantitative
experiments are conducted on the BraTS 2019 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation with Representative Teacher Keys Based on Attention Mechanism for Image Classification Model Compression. (arXiv:2206.12788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12788">
<div class="article-summary-box-inner">
<span><p>With the improvement of AI chips (e.g., GPU, TPU, and NPU) and the fast
development of the Internet of Things (IoT), some robust deep neural networks
(DNNs) are usually composed of millions or even hundreds of millions of
parameters. Such a large model may not be suitable for directly deploying on
low computation and low capacity units (e.g., edge devices). Knowledge
distillation (KD) has recently been recognized as a powerful model compression
method to decrease the model parameters effectively. The central concept of KD
is to extract useful information from the feature maps of a large model (i.e.,
teacher model) as a reference to successfully train a small model (i.e.,
student model) in which the model size is much smaller than the teacher one.
Although many KD methods have been proposed to utilize the information from the
feature maps of intermediate layers in the teacher model, most did not consider
the similarity of feature maps between the teacher model and the student model.
As a result, it may make the student model learn useless information. Inspired
by the attention mechanism, we propose a novel KD method called representative
teacher key (RTK) that not only considers the similarity of feature maps but
also filters out the useless information to improve the performance of the
target student model. In the experiments, we validate our proposed method with
several backbone networks (e.g., ResNet and WideResNet) and datasets (e.g.,
CIFAR10, CIFAR100, SVHN, and CINIC10). The results show that our proposed RTK
can effectively improve the classification accuracy of the state-of-the-art
attention-based KD method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Modelling With Inverse Heat Dissipation. (arXiv:2206.13397v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13397">
<div class="article-summary-box-inner">
<span><p>While diffusion models have shown great success in image generation, their
noise-inverting generative process does not explicitly consider the structure
of images, such as their inherent multi-scale nature. Inspired by diffusion
models and the desirability of coarse-to-fine modelling, we propose a new model
that generates images through iteratively inverting the heat equation, a PDE
that locally erases fine-scale information when run over the 2D plane of the
image. In our novel methodology, the solution of the forward heat equation is
interpreted as a variational approximation in a directed graphical model. We
demonstrate promising image quality and point out emergent qualitative
properties not seen in diffusion models, such as disentanglement of overall
colour and shape in images and aspects of neural network interpretability.
Spectral analysis on natural images positions our model as a type of dual to
diffusion models and reveals implicit inductive biases in them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenLDN: Learning to Discover Novel Classes for Open-World Semi-Supervised Learning. (arXiv:2207.02261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02261">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) is one of the dominant approaches to address
the annotation bottleneck of supervised learning. Recent SSL methods can
effectively leverage a large repository of unlabeled data to improve
performance while relying on a small set of labeled data. One common assumption
in most SSL methods is that the labeled and unlabeled data are from the same
data distribution. However, this is hardly the case in many real-world
scenarios, which limits their applicability. In this work, instead, we attempt
to solve the challenging open-world SSL problem that does not make such an
assumption. In the open-world SSL problem, the objective is to recognize
samples of known classes, and simultaneously detect and cluster samples
belonging to novel classes present in unlabeled data. This work introduces
OpenLDN that utilizes a pairwise similarity loss to discover novel classes.
Using a bi-level optimization rule this pairwise similarity loss exploits the
information available in the labeled set to implicitly cluster novel class
samples, while simultaneously recognizing samples from known classes. After
discovering novel classes, OpenLDN transforms the open-world SSL problem into a
standard SSL problem to achieve additional performance gains using existing SSL
methods. Our extensive experiments demonstrate that OpenLDN outperforms the
current state-of-the-art methods on multiple popular classification benchmarks
while providing a better accuracy/training time trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Realistic Semi-Supervised Learning. (arXiv:2207.02269v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02269">
<div class="article-summary-box-inner">
<span><p>Deep learning is pushing the state-of-the-art in many computer vision
applications. However, it relies on large annotated data repositories, and
capturing the unconstrained nature of the real-world data is yet to be solved.
Semi-supervised learning (SSL) complements the annotated training data with a
large corpus of unlabeled data to reduce annotation cost. The standard SSL
approach assumes unlabeled data are from the same distribution as annotated
data. Recently, a more realistic SSL problem, called open-world SSL, is
introduced, where the unannotated data might contain samples from unknown
classes. In this paper, we propose a novel pseudo-label based approach to
tackle SSL in open-world setting. At the core of our method, we utilize sample
uncertainty and incorporate prior knowledge about class distribution to
generate reliable class-distribution-aware pseudo-labels for unlabeled data
belonging to both known and unknown classes. Our extensive experimentation
showcases the effectiveness of our approach on several benchmark datasets,
where it substantially outperforms the existing state-of-the-art on seven
diverse datasets including CIFAR-100 (~17%), ImageNet-100 (~5%), and Tiny
ImageNet (~9%). We also highlight the flexibility of our approach in solving
novel class discovery task, demonstrate its stability in dealing with
imbalanced data, and complement our approach with a technique to estimate the
number of novel classes
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence. (arXiv:2207.04630v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04630">
<div class="article-summary-box-inner">
<span><p>Ten years into the revival of deep networks and artificial intelligence, we
propose a theoretical framework that sheds light on understanding deep networks
within a bigger picture of Intelligence in general. We introduce two
fundamental principles, Parsimony and Self-consistency, that address two
fundamental questions regarding Intelligence: what to learn and how to learn,
respectively. We believe the two principles are the cornerstones for the
emergence of Intelligence, artificial or natural. While these two principles
have rich classical roots, we argue that they can be stated anew in entirely
measurable and computable ways. More specifically, the two principles lead to
an effective and efficient computational framework, compressive closed-loop
transcription, that unifies and explains the evolution of modern deep networks
and many artificial intelligence practices. While we mainly use modeling of
visual data as an example, we believe the two principles will unify
understanding of broad families of autonomous intelligent systems and provide a
framework for understanding the brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Grand Unification of Object Tracking. (arXiv:2207.07078v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07078">
<div class="article-summary-box-inner">
<span><p>We present a unified method, termed Unicorn, that can simultaneously solve
four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the
same model parameters. Due to the fragmented definitions of the object tracking
problem itself, most existing trackers are developed to address a single or
part of tasks and overspecialize on the characteristics of specific tasks. By
contrast, Unicorn provides a unified solution, adopting the same input,
backbone, embedding, and head across all tracking tasks. For the first time, we
accomplish the great unification of the tracking network architecture and
learning paradigm. Unicorn performs on-par or better than its task-specific
counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,
BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will
serve as a solid step towards the general vision model. Code is available at
https://github.com/MasterBin-IIAU/Unicorn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07929">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have exhibited remarkable performance in image
super-resolution (SR) tasks by learning a mapping from low-resolution (LR)
images to high-resolution (HR) images. However, the SR problem is typically an
ill-posed problem and existing methods would come with several limitations.
First, the possible mapping space of SR can be extremely large since there may
exist many different HR images that can be downsampled to the same LR image. As
a result, it is hard to directly learn a promising SR mapping from such a large
space. Second, it is often inevitable to develop very large models with
extremely high computational cost to yield promising SR performance. In
practice, one can use model compression techniques to obtain compact models by
reducing model redundancy. Nevertheless, it is hard for existing model
compression methods to accurately identify the redundant components due to the
extremely large SR mapping space. To alleviate the first challenge, we propose
a dual regression learning scheme to reduce the space of possible SR mappings.
Specifically, in addition to the mapping from LR to HR images, we learn an
additional dual regression mapping to estimate the downsampling kernel and
reconstruct LR images. In this way, the dual mapping acts as a constraint to
reduce the space of possible mappings. To address the second challenge, we
propose a lightweight dual regression compression method to reduce model
redundancy in both layer-level and channel-level based on channel pruning.
Specifically, we first develop a channel number search method that minimizes
the dual regression loss to determine the redundancy of each layer. Given the
searched channel numbers, we further exploit the dual regression manner to
evaluate the importance of channels and prune the redundant ones. Extensive
experiments show the effectiveness of our method in obtaining accurate and
efficient SR models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos. (arXiv:2207.09725v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09725">
<div class="article-summary-box-inner">
<span><p>Although many approaches for multi-human pose estimation in videos have shown
profound results, they require densely annotated data which entails excessive
man labor. Furthermore, there exists occlusion and motion blur that inevitably
lead to poor estimation performance. To address these problems, we propose a
method that leverages an attention mask for occluded joints and encodes
temporal dependency between frames using transformers. First, our framework
composes different combinations of sparsely annotated frames that denote the
track of the overall joint movement. We propose an occlusion attention mask
from these combinations that enable encoding occlusion-aware heatmaps as a
semi-supervised task. Second, the proposed temporal encoder employs transformer
architecture to effectively aggregate the temporal relationship and
keypoint-wise attention from each time step and accurately refines the target
frame's final pose estimation. We achieve state-of-the-art pose estimation
results for PoseTrack2017 and PoseTrack2018 datasets and demonstrate the
robustness of our approach to occlusion and motion blur in sparsely annotated
video data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2207.10271v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10271">
<div class="article-summary-box-inner">
<span><p>Learning to generate new images for a novel category based on only a few
images, named as few-shot image generation, has attracted increasing research
interest. Several state-of-the-art works have yielded impressive results, but
the diversity is still limited. In this work, we propose a novel Delta
Generative Adversarial Network (DeltaGAN), which consists of a reconstruction
subnetwork and a generation subnetwork. The reconstruction subnetwork captures
intra-category transformation, i.e., delta, between same-category pairs. The
generation subnetwork generates sample-specific delta for an input image, which
is combined with this input image to generate a new image within the same
category. Besides, an adversarial delta matching loss is designed to link the
above two subnetworks together. Extensive experiments on six benchmark datasets
demonstrate the effectiveness of our proposed method. Our code is available at
https://github.com/bcmi/DeltaGAN-Few-Shot-Image-Generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields. (arXiv:2207.10312v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10312">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis has recently been revolutionized by learning neural
radiance fields directly from sparse observations. However, rendering images
with this new paradigm is slow due to the fact that an accurate quadrature of
the volume rendering equation requires a large number of samples for each ray.
Previous work has mainly focused on speeding up the network evaluations that
are associated with each sample point, e.g., via caching of radiance values
into explicit spatial data structures, but this comes at the expense of model
compactness. In this paper, we propose a novel dual-network architecture that
takes an orthogonal direction by learning how to best reduce the number of
required sample points. To this end, we split our network into a sampling and
shading network that are jointly trained. Our training scheme employs fixed
sample positions along each ray, and incrementally introduces sparsity
throughout training to achieve high quality even at low sample counts. After
fine-tuning with the target number of samples, the resulting compact neural
representation can be rendered in real-time. Our experiments demonstrate that
our approach outperforms concurrent compact neural representations in terms of
quality and frame rate and performs on par with highly efficient hybrid
representations. Code and supplementary material is available at
https://thomasneff.github.io/adanerf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Patch-Based Neural Rendering. (arXiv:2207.10662v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10662">
<div class="article-summary-box-inner">
<span><p>Neural rendering has received tremendous attention since the advent of Neural
Radiance Fields (NeRF), and has pushed the state-of-the-art on novel-view
synthesis considerably. The recent focus has been on models that overfit to a
single scene, and the few attempts to learn models that can synthesize novel
views of unseen scenes mostly consist of combining deep convolutional features
with a NeRF-like model. We propose a different paradigm, where no deep features
and no NeRF-like volume rendering are needed. Our method is capable of
predicting the color of a target ray in a novel scene directly, just from a
collection of patches sampled from the scene. We first leverage epipolar
geometry to extract patches along the epipolar lines of each reference view.
Each patch is linearly projected into a 1D feature vector and a sequence of
transformers process the collection. For positional encoding, we parameterize
rays as in a light field representation, with the crucial difference that the
coordinates are canonicalized with respect to the target ray, which makes our
method independent of the reference frame and improves generalization. We show
that our approach outperforms the state-of-the-art on novel view synthesis of
unseen scenes even when being trained with considerably less data than prior
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Object Counting and Detection. (arXiv:2207.10988v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10988">
<div class="article-summary-box-inner">
<span><p>We tackle a new task of few-shot object counting and detection. Given a few
exemplar bounding boxes of a target object class, we seek to count and detect
all objects of the target class. This task shares the same supervision as the
few-shot object counting but additionally outputs the object bounding boxes
along with the total object count. To address this challenging problem, we
introduce a novel two-stage training strategy and a novel uncertainty-aware
few-shot object detector: Counting-DETR. The former is aimed at generating
pseudo ground-truth bounding boxes to train the latter. The latter leverages
the pseudo ground-truth provided by the former but takes the necessary steps to
account for the imperfection of pseudo ground-truth. To validate the
performance of our method on the new task, we introduce two new datasets named
FSCD-147 and FSCD-LVIS. Both datasets contain images with complex scenes,
multiple object classes per image, and a huge variation in object shapes,
sizes, and appearance. Our proposed approach outperforms very strong baselines
adapted from few-shot object counting and few-shot object detection with a
large margin in both counting and detection metrics. The code and models are
available at https://github.com/VinAIResearch/Counting-DETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Video Captioning with Evolving Pseudo-Tokens. (arXiv:2207.11100v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11100">
<div class="article-summary-box-inner">
<span><p>We introduce a zero-shot video captioning method that employs two frozen
networks: the GPT-2 language model and the CLIP image-text matching model. The
matching score is used to steer the language model toward generating a sentence
that has a high average matching score to a subset of the video frames. Unlike
zero-shot image captioning methods, our work considers the entire sentence at
once. This is achieved by optimizing, during the generation process, part of
the prompt from scratch, by modifying the representation of all other tokens in
the prompt, and by repeating the process iteratively, gradually improving the
specificity and comprehensiveness of the generated sentence. Our experiments
show that the generated captions are coherent and display a broad range of
real-world knowledge. Our code is available at:
https://github.com/YoadTew/zero-shot-video-to-text
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Riemannian Geometry Approach for Minimizing Distortion and its Applications. (arXiv:2207.12038v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12038">
<div class="article-summary-box-inner">
<span><p>Given an affine transformation $T$, we define its Fisher distortion
$Dist_F(T)$. We show that the Fisher distortion has Riemannian metric structure
and provide an algorithm for finding mean distorting transformation -- namely
-- for a given set $\{T_{i}\}_{i=1}^N$ of affine transformations, find an
affine transformation $T$ that minimize the overall distortion
$\sum_{i=1}^NDist_F^{2}(T^{-1}T_{i}).$ The mean distorting transformation can
be useful in some fields -- in particular, we apply it for rendering affine
panoramas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLO and Mask R-CNN for Vehicle Number Plate Identification. (arXiv:2207.13165v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13165">
<div class="article-summary-box-inner">
<span><p>License plate scanners have grown in popularity in parking lots during the
past few years. In order to quickly identify license plates, traditional plate
recognition devices used in parking lots employ a fixed source of light and
shooting angles. For skewed angles, such as license plate images taken with
ultra-wide angle or fisheye lenses, deformation of the license plate
recognition plate can also be quite severe, impairing the ability of standard
license plate recognition systems to identify the plate. Mask RCNN gadget that
may be utilised for oblique pictures and various shooting angles. The results
of the experiments show that the suggested design will be capable of
classifying license plates with bevel angles larger than 0/60. Character
recognition using the suggested Mask R-CNN approach has advanced significantly
as well. The proposed Mask R-CNN method has also achieved significant progress
in character recognition, which is tilted more than 45 degrees as compared to
the strategy of employing the YOLOv2 model. Experiment results also suggest
that the methodology presented in the open data plate collecting is better than
other techniques (known as the AOLP dataset).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13243">
<div class="article-summary-box-inner">
<span><p>The last decade of machine learning has seen drastic increases in scale and
capabilities, and deep neural networks (DNNs) are increasingly being deployed
across a wide range of domains. However, the inner workings of DNNs are
generally difficult to understand, raising concerns about the safety of using
these systems without a rigorous understanding of how they function. In this
survey, we review literature on techniques for interpreting the inner
components of DNNs, which we call "inner" interpretability methods.
Specifically, we review methods for interpreting weights, neurons, subnetworks,
and latent representations with a focus on how these techniques relate to the
goal of designing safer, more trustworthy AI systems. We also highlight
connections between interpretability and work in modularity, adversarial
robustness, continual learning, network compression, and studying the human
visual system. Finally, we discuss key challenges and argue for future work in
interpretability for AI safety that focuses on diagnostics, benchmarking, and
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Closer to Your Enemy: Learning to Attack via Teacher-student Mimicking. (arXiv:2207.13381v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13381">
<div class="article-summary-box-inner">
<span><p>This paper aims to generate realistic attack samples of person
re-identification, ReID, by reading the enemy's mind (VM). In this paper, we
propose a novel inconspicuous and controllable ReID attack baseline, LCYE, to
generate adversarial query images. Concretely, LCYE first distills VM's
knowledge via teacher-student memory mimicking in the proxy task. Then this
knowledge prior acts as an explicit cipher conveying what is essential and
realistic, believed by VM, for accurate adversarial misleading. Besides,
benefiting from the multiple opposing task framework of LCYE, we further
investigate the interpretability and generalization of ReID models from the
view of the adversarial attack, including cross-domain adaption, cross-model
consensus, and online learning process. Extensive experiments on four ReID
benchmarks show that our method outperforms other state-of-the-art attackers
with a large margin in white-box, black-box, and target attacks. Our code is
now available at https://gitfront.io/r/user-3704489/mKXusqDT4ffr/LCYE/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistical Keystroke Synthesis for Improved Bot Detection. (arXiv:2207.13394v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13394">
<div class="article-summary-box-inner">
<span><p>This work proposes two statistical approaches for the synthesis of keystroke
biometric data based on Universal and User-dependent Models. Both approaches
are validated on the bot detection task, using the keystroke synthetic data to
better train the systems. Our experiments include a dataset with 136 million
keystroke events from 168,000 subjects. We have analyzed the performance of the
two synthesis approaches through qualitative and quantitative experiments.
Different bot detectors are considered based on two supervised classifiers
(Support Vector Machine and Long Short-Term Memory network) and a learning
framework including human and generated samples. Our results prove that the
proposed statistical approaches are able to generate realistic human-like
synthetic keystroke samples. Also, the classification results suggest that in
scenarios with large labeled data, these synthetic samples can be detected with
high accuracy. However, in few-shot learning scenarios it represents an
important challenge.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-31 23:08:50.878430606 UTC">2022-07-31 23:08:50 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>