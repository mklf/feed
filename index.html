<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-19T01:30:00Z">01-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dark Side of the Language: Pre-trained Transformers in the DarkNet. (arXiv:2201.05613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05613">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformers are challenging human performances in many natural
language processing tasks. The gigantic datasets used for pre-training seem to
be the key for their success on existing tasks. In this paper, we explore how a
range of pre-trained natural language understanding models perform on truly
novel and unexplored data, provided by classification tasks over a DarkNet
corpus. Surprisingly, results show that syntactic and lexical neural networks
largely outperform pre-trained Transformers. This seems to suggest that
pre-trained Transformers have serious difficulties in adapting to radically
novel texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Reducing Manual Workload in Technology-Assisted Reviews: Estimating Ranking Performance. (arXiv:2201.05648v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05648">
<div class="article-summary-box-inner">
<span><p>Conducting a systematic review (SR) is comprised of multiple tasks: (i)
collect documents (studies) that are likely to be relevant from digital
libraries (eg., PubMed), (ii) manually read and label the documents as relevant
or irrelevant, (iii) extract information from the relevant studies, and (iv)
analyze and synthesize the information and derive a conclusion of SR. When
researchers label studies, they can screen ranked documents where relevant
documents are higher than irrelevant ones. This practice, known as screening
prioritization (ie., document ranking approach), speeds up the process of
conducting a SR as the documents labelled as relevant can move to the next
tasks earlier. However, the approach is limited in reducing the manual workload
because the total number of documents to screen remains the same. Towards
reducing the manual workload in the screening process, we investigate the
quality of document ranking of SR. This can signal researchers whereabouts in
the ranking relevant studies are located and let them decide where to stop the
screening. After extensive analysis on SR document rankings from different
ranking models, we hypothesize 'topic broadness' as a factor that affects the
ranking quality of SR. Finally, we propose a measure that estimates the topic
broadness and demonstrate that the proposed measure is a simple yet effective
method to predict the qualities of document rankings for SRs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Models for Extracting Information from Registration and Legal Documents. (arXiv:2201.05658v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05658">
<div class="article-summary-box-inner">
<span><p>A typical information extraction pipeline consists of token- or span-level
classification models coupled with a series of pre- and post-processing
scripts. In a production pipeline, requirements often change, with classes
being added and removed, which leads to nontrivial modifications to the source
code and the possible introduction of bugs. In this work, we evaluate
sequence-to-sequence models as an alternative to token-level classification
methods for information extraction of legal and registration documents. We
finetune models that jointly extract the information and generate the output
already in a structured format. Post-processing steps are learned during
training, thus eliminating the need for rule-based methods and simplifying the
pipeline. Furthermore, we propose a novel method to align the output with the
input text, thus facilitating system inspection and auditing. Our experiments
on four real-world datasets show that the proposed method is an alternative to
classical pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Stability with Continuous Data Updates. (arXiv:2201.05692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05692">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the "stability" of machine learning (ML) models
within the context of larger, complex NLP systems with continuous training data
updates. For this study, we propose a methodology for the assessment of model
stability (which we refer to as jitter under various experimental conditions.
We find that model design choices, including network architecture and input
representation, have a critical impact on stability through experiments on four
text classification tasks and two sequence labeling tasks. In classification
tasks, non-RNN-based models are observed to be more stable than RNN-based ones,
while the encoder-decoder model is less stable in sequence labeling tasks.
Moreover, input representations based on pre-trained fastText embeddings
contribute to more stability than other choices. We also show that two learning
strategies -- ensemble models and incremental training -- have a significant
influence on stability. We recommend ML model designers account for trade-offs
in accuracy and jitter when making modeling choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cost-Effective Training in Low-Resource Neural Machine Translation. (arXiv:2201.05700v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05700">
<div class="article-summary-box-inner">
<span><p>While Active Learning (AL) techniques are explored in Neural Machine
Translation (NMT), only a few works focus on tackling low annotation budgets
where a limited number of sentences can get translated. Such situations are
especially challenging and can occur for endangered languages with few human
annotators or having cost constraints to label large amounts of data. Although
AL is shown to be helpful with large budgets, it is not enough to build
high-quality translation systems in these low-resource conditions. In this
work, we propose a cost-effective training procedure to increase the
performance of NMT models utilizing a small number of annotated sentences and
dictionary entries. Our method leverages monolingual data with self-supervised
objectives and a small-scale, inexpensive dictionary for additional supervision
to initialize the NMT model before applying AL. We show that improving the
model using a combination of these knowledge sources is essential to exploit AL
strategies and increase gains in low-resource conditions. We also present a
novel AL strategy inspired by domain adaptation for NMT and show that it is
effective for low budgets. We propose a new hybrid data-driven approach, which
samples sentences that are diverse from the labelled data and also most similar
to unlabelled data. Finally, we show that initializing the NMT model and
further using our AL strategy can achieve gains of up to $13$ BLEU compared to
conventional AL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Space Situational Awareness Events from News Text. (arXiv:2201.05721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05721">
<div class="article-summary-box-inner">
<span><p>Space situational awareness typically makes use of physical measurements from
radar, telescopes, and other assets to monitor satellites and other spacecraft
for operational, navigational, and defense purposes. In this work we explore
using textual input for the space situational awareness task. We construct a
corpus of 48.5k news articles spanning all known active satellites between 2009
and 2020. Using a dependency-rule-based extraction system designed to target
three high-impact events -- spacecraft launches, failures, and
decommissionings, we identify 1,787 space-event sentences that are then
annotated by humans with 15.9k labels for event slots. We empirically
demonstrate a state-of-the-art neural extraction system achieves an overall F1
between 53 and 91 per slot for event extraction in this low-resource,
high-impact domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05729">
<div class="article-summary-box-inner">
<span><p>Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kformer: Knowledge Injection in Transformer Feed-Forward Layers. (arXiv:2201.05742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05742">
<div class="article-summary-box-inner">
<span><p>Knowledge-Enhanced Model have developed a diverse set of techniques for
knowledge integration on different knowledge sources. However, most previous
work neglect the language model's own ability and simply concatenate external
knowledge at the input. Recent work proposed that Feed Forward Network (FFN) in
pre-trained language model can be seen as an memory that stored factual
knowledge. In this work, we explore the FFN in Transformer and propose a novel
knowledge fusion model, namely Kformer, which incorporates external knowledge
through the feed-forward layer in Transformer. We empirically find that simply
injecting knowledge into FFN can enhance the pre-trained language model's
ability and facilitate current knowledge fusion methods. Our results on two
benchmarks in the commonsense reasoning (i.e., SocialIQA) and medical question
answering (i.e., MedQA-USMLE) domains demonstrate that Kformer can utilize
external knowledge deeply and achieves absolute improvements in these tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Transformer for Efficient and Accurate Ranking Tasks: an Application to Question Answering Systems. (arXiv:2201.05767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05767">
<div class="article-summary-box-inner">
<span><p>Large transformer models can highly improve Answer Sentence Selection (AS2)
task, but their high computational costs prevent their use in many real world
applications. In this paper, we explore the following research question: How
can we make the AS2models more accurate without significantly increasing their
model complexity? To address the question, we propose a Multiple Heads Student
architecture (MHS), an efficient neural network designed to distill an ensemble
of large transformers into a single smaller model. An MHS model consists of two
components: a stack of transformer layers that is used to encode inputs, and a
set of ranking heads; each of them is trained by distilling a different large
transformer architecture. Unlike traditional distillation techniques, our
approach leverages individual models in ensemble as teachers in a way that
preserves the diversity of the ensemble members. The resulting model captures
the knowledge of different types of transformer models by using just a few
extra parameters. We show the effectiveness of MHS on three English datasets
for AS2; our proposed approach outperforms all single-model distillations we
consider, rivaling the state-of-the-art large AS2 models that have 2.7x more
parameters and run 2.5x slower.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics. (arXiv:2201.05771v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05771">
<div class="article-summary-box-inner">
<span><p>We present an expanded version of our previously released Kazakh
text-to-speech (KazakhTTS) synthesis corpus. In the new KazakhTTS2 corpus, the
overall size is increased from 93 hours to 271 hours, the number of speakers
has risen from two to five (three females and two males), and the topic
coverage is diversified with the help of new sources, including a book and
Wikipedia articles. This corpus is necessary for building high-quality TTS
systems for Kazakh, a Central Asian agglutinative language from the Turkic
family, which presents several linguistic challenges. We describe the corpus
construction process and provide the details of the training and evaluation
procedures for the TTS system. Our experimental results indicate that the
constructed corpus is sufficient to build robust TTS models for real-world
applications, with a subjective mean opinion score of above 4.0 for all the
five speakers. We believe that our corpus will facilitate speech and language
research for Kazakh and other Turkic languages, which are widely considered to
be low-resource due to the limited availability of free linguistic data. The
constructed corpus, code, and pretrained models are publicly available in our
GitHub repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Learning for Few-Shot Dialogue State Tracking. (arXiv:2201.05780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05780">
<div class="article-summary-box-inner">
<span><p>Collecting dialogue state labels, slots and values, for learning dialogue
state tracking (DST) models can be costly, especially with the wide application
of dialogue systems in new-rising domains. In this paper, we focus on how to
learn a DST model efficiently with limited labeled data. We design a prompt
learning framework for few-shot DST, which consists of two main components:
value-based prompt and inverse prompt mechanism. This framework aims to utilize
the language understanding and generation ability of pre-trained language
models (PLM). First, we design value-based prompt functions to probe the
DST-related knowledge from PLM, which do not rely on the known ontology of
slots. Further, an inverse prompt mechanism is utilized to self-check the
"prompted" knowledge and help the PLM understand the essence of DST task
further. Experiments show that our model can generate unseen slots and
outperforms existing state-of-the-art few-shot methods. It indicates that
DST-related knowledge can be probed from PLM and utilized to address
low-resource DST efficiently with the help of prompt learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Benchmark for Generalizable and Interpretable Temporal Question Answering over Knowledge Bases. (arXiv:2201.05793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05793">
<div class="article-summary-box-inner">
<span><p>Knowledge Base Question Answering (KBQA) tasks that involve complex reasoning
are emerging as an important research direction. However, most existing KBQA
datasets focus primarily on generic multi-hop reasoning over explicit facts,
largely ignoring other reasoning types such as temporal, spatial, and taxonomic
reasoning. In this paper, we present a benchmark dataset for temporal
reasoning, TempQA-WD, to encourage research in extending the present approaches
to target a more challenging set of complex reasoning tasks. Specifically, our
benchmark is a temporal question answering dataset with the following
advantages: (a) it is based on Wikidata, which is the most frequently curated,
openly available knowledge base, (b) it includes intermediate sparql queries to
facilitate the evaluation of semantic parsing based approaches for KBQA, and
(c) it generalizes to multiple knowledge bases: Freebase and Wikidata. The
TempQA-WD dataset is available at https://github.com/IBM/tempqa-wd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Lexical Simplification for Turkish. (arXiv:2201.05878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05878">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the first automatic lexical simplification system
for the Turkish language. Recent text simplification efforts rely on manually
crafted simplified corpora and comprehensive NLP tools that can analyse the
target text both in word and sentence levels. Turkish is a morphologically rich
agglutinative language that requires unique considerations such as the proper
handling of inflectional cases. Being a low-resource language in terms of
available resources and industrial-strength tools, it makes the text
simplification task harder to approach. We present a new text simplification
pipeline based on pretrained representation model BERT together with
morphological features to generate grammatically correct and semantically
appropriate word-level simplifications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning over Hybrid Chain for Table-and-Text Open Domain QA. (arXiv:2201.05880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05880">
<div class="article-summary-box-inner">
<span><p>Tabular and textual question answering requires systems to perform reasoning
over heterogeneous information, considering table structure, and the
connections among table and text. In this paper, we propose a ChAin-centric
Reasoning and Pre-training framework (CARP). CARP utilizes hybrid chain to
model the explicit intermediate reasoning process across table and text for
question answering. We also propose a novel chain-centric pre-training method,
to enhance the pre-trained model in identifying the cross-modality reasoning
process and alleviating the data sparsity problem. This method constructs the
large-scale reasoning corpus by synthesizing pseudo heterogeneous reasoning
paths from Wikipedia and generating corresponding questions. We evaluate our
system on OTT-QA, a large-scale table-and-text open-domain question answering
benchmark, and our system achieves the state-of-the-art performance. Further
analyses illustrate that the explicit hybrid chain offers substantial
performance improvement and interpretablity of the intermediate reasoning
process, and the chain-centric pre-training boosts the performance on the chain
extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Correction of Syntactic Dependency Annotation Differences. (arXiv:2201.05891v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05891">
<div class="article-summary-box-inner">
<span><p>Annotation inconsistencies between data sets can cause problems for
low-resource NLP, where noisy or inconsistent data cannot be as easily replaced
compared with resource-rich languages. In this paper, we propose a method for
automatically detecting annotation mismatches between dependency parsing
corpora, as well as three related methods for automatically converting the
mismatches. All three methods rely on comparing an unseen example in a new
corpus with similar examples in an existing corpus. These three methods include
a simple lexical replacement using the most frequent tag of the example in the
existing corpus, a GloVe embedding-based replacement that considers a wider
pool of examples, and a BERT embedding-based replacement that uses
contextualized embeddings to provide examples fine-tuned to our specific data.
We then evaluate these conversions by retraining two dependency parsers --
Stanza (Qi et al. 2020) and Parsing as Tagging (PaT) (Vacareanu et al. 2020) --
on the converted and unconverted data. We find that applying our conversions
yields significantly better performance in many cases. Some differences
observed between the two parsers are observed. Stanza has a more complex
architecture with a quadratic algorithm, so it takes longer to train, but it
can generalize better with less data. The PaT parser has a simpler architecture
with a linear algorithm, speeding up training time but requiring more training
data to reach comparable or better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unobserved Local Structures Make Compositional Generalization Hard. (arXiv:2201.05899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05899">
<div class="article-summary-box-inner">
<span><p>While recent work has convincingly showed that sequence-to-sequence models
struggle to generalize to new compositions (termed compositional
generalization), little is known on what makes compositional generalization
hard on a particular test instance. In this work, we investigate what are the
factors that make generalization to certain test instances challenging. We
first substantiate that indeed some examples are more difficult than others by
showing that different models consistently fail or succeed on the same test
instances. Then, we propose a criterion for the difficulty of an example: a
test instance is hard if it contains a local structure that was not observed at
training time. We formulate a simple decision rule based on this criterion and
empirically show it predicts instance-level generalization well across 5
different semantic parsing datasets, substantially better than alternative
decision rules. Last, we show local structures can be leveraged for creating
difficult adversarial compositional splits and also to improve compositional
generalization under limited training budgets by strategically selecting
examples for the training set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Addressing the Challenges of Cross-Lingual Hate Speech Detection. (arXiv:2201.05922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05922">
<div class="article-summary-box-inner">
<span><p>The goal of hate speech detection is to filter negative online content aiming
at certain groups of people. Due to the easy accessibility of social media
platforms it is crucial to protect everyone which requires building hate speech
detection systems for a wide range of languages. However, the available labeled
hate speech datasets are limited making it problematic to build systems for
many languages. In this paper we focus on cross-lingual transfer learning to
support hate speech detection in low-resource languages. We leverage
cross-lingual word embeddings to train our neural network systems on the source
language and apply it to the target language, which lacks labeled examples, and
show that good performance can be achieved. We then incorporate unlabeled
target language data for further model improvements by bootstrapping labels
using an ensemble of different model architectures. Furthermore, we investigate
the issue of label imbalance of hate speech datasets, since the high ratio of
non-hate examples compared to hate examples often leads to low model
performance. We test simple data undersampling and oversampling techniques and
show their effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation. (arXiv:2201.05955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05955">
<div class="article-summary-box-inner">
<span><p>A recurring challenge of crowdsourcing NLP datasets at scale is that human
writers often rely on repetitive patterns when crafting examples, leading to a
lack of linguistic diversity. We introduce a novel paradigm for dataset
creation based on human and machine collaboration, which brings together the
generative strength of language models and the evaluative strength of humans.
Starting with an existing dataset, MultiNLI, our approach uses dataset
cartography to automatically identify examples that demonstrate challenging
reasoning patterns, and instructs GPT-3 to compose new examples with similar
patterns. Machine generated examples are then automatically filtered, and
finally revised and labeled by human crowdworkers to ensure quality. The
resulting dataset, WANLI, consists of 108,357 natural language inference (NLI)
examples that present unique empirical strengths over existing NLI datasets.
Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger)
improves performance on seven out-of-domain test sets we consider, including by
11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is
more effective than combining with other augmentation sets that have been
introduced. Our results demonstrate the potential of natural language
generation techniques to curate NLP datasets of enhanced quality and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. (arXiv:2201.05966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05966">
<div class="article-summary-box-inner">
<span><p>Structured knowledge grounding (SKG) leverages structured knowledge to
complete user requests, such as semantic parsing over databases and question
answering over knowledge bases. Since the inputs and outputs of SKG tasks are
heterogeneous, they have been studied separately by different communities,
which limits systematic and compatible research on SKG. In this paper, we
overcome this limitation by proposing the SKG framework, which unifies 21 SKG
tasks into a text-to-text format, aiming to promote systematic SKG research,
instead of being exclusive to a single task, domain, or dataset. We use
UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple
modifications when necessary, achieves state-of-the-art performance on almost
all of the 21 tasks. We further demonstrate that multi-task prefix-tuning
improves the performance on most tasks, largely improving the overall
performance. UnifiedSKG also facilitates the investigation of zero-shot and
few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot
and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of
controlled experiments on structured knowledge encoding variants across SKG
tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at
https://github.com/hkunlp/unifiedskg; latest collections at
https://unifiedskg.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples. (arXiv:2201.05979v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05979">
<div class="article-summary-box-inner">
<span><p>Unsupervised sentence embedding aims to obtain the most appropriate embedding
for a sentence to reflect its semantic. Contrastive learning has been
attracting developing attention. For a sentence, current models utilize diverse
data augmentation methods to generate positive samples, while consider other
independent sentences as negative samples. Then they adopt InfoNCE loss to pull
the embeddings of positive pairs gathered, and push those of negative pairs
scattered. Although these models have made great progress on sentence
embedding, we argue that they may suffer from feature suppression. The models
fail to distinguish and decouple textual similarity and semantic similarity.
And they may overestimate the semantic similarity of any pairs with similar
textual regardless of the actual semantic difference between them. This is
because positive pairs in unsupervised contrastive learning come with similar
and even the same textual through data augmentation. To alleviate feature
suppression, we propose contrastive learning for unsupervised sentence
embedding with soft negative samples (SNCSE). Soft negative samples share
highly similar textual but have surely and apparently different semantic with
the original samples. Specifically, we take the negation of original sentences
as soft negative samples, and propose Bidirectional Margin Loss (BML) to
introduce them into traditional contrastive learning framework, which merely
involves positive and negative samples. Our experimental results show that
SNCSE can obtain state-of-the-art performance on semantic textual similarity
(STS) task with average Spearman's correlation coefficient of 78.97% on
BERTbase and 79.23% on RoBERTabase. Besides, we adopt rank-based error analysis
method to detect the weakness of SNCSE for future study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Double Retrieval and Ranking for Accurate Question Answering. (arXiv:2201.05981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05981">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that an answer verification step introduced in
Transformer-based answer selection models can significantly improve the state
of the art in Question Answering. This step is performed by aggregating the
embeddings of top $k$ answer candidates to support the verification of a target
answer. Although the approach is intuitive and sound still shows two
limitations: (i) the supporting candidates are ranked only according to the
relevancy with the question and not with the answer, and (ii) the support
provided by the other answer candidates is suboptimal as these are retrieved
independently of the target answer. In this paper, we address both drawbacks by
proposing (i) a double reranking model, which, for each target answer, selects
the best support; and (ii) a second neural retrieval stage designed to encode
question and answer pair as the query, which finds more specific verification
information. The results on three well-known datasets for AS2 show consistent
and significant improvement of the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Situ Answer Sentence Selection at Web-scale. (arXiv:2201.05984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05984">
<div class="article-summary-box-inner">
<span><p>Current answer sentence selection (AS2) applied in open-domain question
answering (ODQA) selects answers by ranking a large set of possible candidates,
i.e., sentences, extracted from the retrieved text. In this paper, we present
Passage-based Extracting Answer Sentence In-place (PEASI), a novel design for
AS2 optimized for Web-scale setting, that, instead, computes such answer
without processing each candidate individually. Specifically, we design a
Transformer-based framework that jointly (i) reranks passages retrieved for a
question and (ii) identifies a probable answer from the top passages in place.
We train PEASI in a multi-task learning framework that encourages feature
sharing between the components: passage reranker and passage-based answer
sentence extractor. To facilitate our development, we construct a new
Web-sourced large-scale QA dataset consisting of 800,000+ labeled
passages/sentences for 60,000+ questions. The experiments show that our
proposed design effectively outperforms the current state-of-the-art setting
for AS2, i.e., a point-wise model for ranking sentences independently, by 6.51%
in accuracy, from 48.86% to 55.37%. In addition, PEASI is exceptionally
efficient in computing answer sentences, requiring only ~20% inferences
compared to the standard setting, i.e., reranking all possible candidates. We
believe the release of PEASI, both the dataset and our proposed design, can
contribute to advancing the research and development in deploying question
answering services at Web scale.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06009">
<div class="article-summary-box-inner">
<span><p>Large LMs such as GPT-3, while powerful, are not immune to mistakes, but are
prohibitively costly to retrain. One failure mode is misinterpreting a user's
instruction (e.g., GPT-3 interpreting "What word is similar to good?" to mean a
homonym, while the user intended a synonym). Our goal is to allow users to
correct such errors directly through interaction -- without retraining. Our
approach pairs GPT-3 with a growing memory of cases where the model
misunderstood the user's intent and was provided with feedback, clarifying the
instruction. Given a new query, our memory-enhanced GPT-3 uses feedback from
similar, prior queries to enrich the prompt. Through simple proof-of-concept
experiments, we show how a (simulated) user can interactively teach a deployed
GPT-3, doubling its accuracy on basic lexical tasks (e.g., generate a synonym)
where users query in different, novel (often misunderstood) ways. In such
scenarios, memory helps avoid repeating similar past mistakes. Our simple idea
is a first step towards strengthening deployed models, potentially broadening
their utility. All the code and data is available at
https://github.com/madaan/memprompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COLD: A Benchmark for Chinese Offensive Language Detection. (arXiv:2201.06025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06025">
<div class="article-summary-box-inner">
<span><p>Offensive language detection and prevention becomes increasing critical for
maintaining a healthy social platform and the safe deployment of language
models. Despite plentiful researches on toxic and offensive language problem in
NLP, existing studies mainly focus on English, while few researches involve
Chinese due to the limitation of resources. To facilitate Chinese offensive
language detection and model evaluation, we collect COLDataset, a Chinese
offensive language dataset containing 37k annotated sentences. With this
high-quality dataset, we provide a strong baseline classifier, COLDetector,
with 81% accuracy for offensive language detection. Furthermore, we also
utilize the proposed \textsc{COLDetector} to study output offensiveness of
popular Chinese language models (CDialGPT and CPM). We find that (1) CPM tends
to generate more offensive output than CDialGPT, and (2) certain type of
prompts, like anti-bias sentences, can trigger offensive outputs more
easily.Altogether, our resources and analyses are intended to help detoxify the
Chinese online communities and evaluate the safety performance of generative
language models. Disclaimer: The paper contains example data that may be
considered profane, vulgar, or offensive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Deduction through Search over Statement Compositions. (arXiv:2201.06028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06028">
<div class="article-summary-box-inner">
<span><p>In settings from fact-checking to question answering, we frequently want to
know whether a collection of evidence entails a hypothesis. Existing methods
primarily focus on end-to-end discriminative versions of this task, but less
work has treated the generative version in which a model searches over the
space of entailed statements to derive the hypothesis. We propose a system for
natural language deduction that decomposes the task into separate steps
coordinated by best-first search, producing a tree of intermediate conclusions
that faithfully reflects the system's reasoning process. Our experiments
demonstrate that the proposed system can better distinguish verifiable
hypotheses from unverifiable ones and produce natural language explanations
that are more internally consistent than those produced by an end-to-end T5
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inspecting state of the art performance and NLP metrics in image-based medical report generation. (arXiv:2011.09257v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09257">
<div class="article-summary-box-inner">
<span><p>Several deep learning architectures have been proposed over the last years to
deal with the problem of generating a written report given an imaging exam as
input. Most works evaluate the generated reports using standard Natural
Language Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant
progress. In this article, we contrast this progress by comparing state of the
art (SOTA) models against weak baselines. We show that simple and even naive
approaches yield near SOTA performance on most traditional NLP metrics. We
conclude that evaluation methods in this task should be further studied towards
correctly measuring clinical accuracy, ideally involving physicians to
contribute to this end.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASER: Towards Large-scale Commonsense Knowledge Acquisition via Higher-order Selectional Preference over Eventualities. (arXiv:2104.02137v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02137">
<div class="article-summary-box-inner">
<span><p>Commonsense knowledge acquisition and reasoning have long been a core
artificial intelligence problem. However, in the past, there has been a lack of
scalable methods to collect commonsense knowledge. In this paper, we propose to
develop principles for collecting commonsense knowledge based on selectional
preference. We generalize the definition of selectional preference from one-hop
linguistic syntactic relations to higher-order relations over linguistic
graphs. Unlike previous commonsense knowledge definition (e.g., ConceptNet),
selectional preference (SP) knowledge only relies on statistical distribution
over linguistic graphs, which can be efficiently and accurately acquired from
the unlabeled corpus with modern tools. Following this principle, we develop a
large-scale eventuality (a linguistic term covering activity, state, and
event)-based knowledge graph ASER, where each eventuality is represented as a
dependency graph, and the relation between them is a discourse relation defined
in shallow discourse parsing. The higher-order selectional preference over
collected linguistic graphs reflects various kinds of commonsense knowledge.
Moreover, motivated by the observation that humans understand events by
abstracting the observed events to a higher level and can thus transfer their
knowledge to new events, we propose a conceptualization module to significantly
boost the coverage of ASER. In total, ASER contains 648 million edges between
438 million eventualities. After conceptualization with Probase, a selectional
preference based concept-instance relational knowledge base, our concept graph
contains 15 million conceptualized eventualities and 224 million edges between
them. Detailed analysis is provided to demonstrate its quality. All the
collected data, APIs, and tools are available at
https://github.com/HKUST-KnowComp/ASER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Network Pruning -- under the Pre-train and Fine-tune Paradigm. (arXiv:2104.08682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08682">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained language models have significantly improved the
performance of various natural language processing (NLP) tasks in the recent
years. While effective and prevalent, these models are usually prohibitively
large for resource-limited deployment scenarios. A thread of research has thus
been working on applying network pruning techniques under the
pretrain-then-finetune paradigm widely adopted in NLP. However, the existing
pruning results on benchmark transformers, such as BERT, are not as remarkable
as the pruning results in the literature of convolutional neural networks
(CNNs). In particular, common wisdom in pruning CNN states that sparse pruning
technique compresses a model more than that obtained by reducing number of
channels and layers (Elsen et al., 2020; Zhu and Gupta, 2017), while existing
works on sparse pruning of BERT yields inferior results than its small-dense
counterparts such as TinyBERT (Jiao et al., 2020). In this work, we aim to fill
this gap by studying how knowledge are transferred and lost during the
pre-train, fine-tune, and pruning process, and proposing a knowledge-aware
sparse pruning process that achieves significantly superior results than
existing literature. We show for the first time that sparse pruning compresses
a BERT model significantly more than reducing its number of channels and
layers. Experiments on multiple data sets of GLUE benchmark show that our
method outperforms the leading competitors with a 20-times weight/FLOPs
compression and neglectable loss in prediction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08793">
<div class="article-summary-box-inner">
<span><p>Augmenting pre-trained language models with knowledge graphs (KGs) has
achieved success on various commonsense reasoning tasks. However, for a given
task instance, the KG, or certain parts of the KG, may not be useful. Although
KG-augmented models often use attention to focus on specific KG components, the
KG is still always used, and the attention mechanism is never explicitly taught
which KG components should be used. Meanwhile, saliency methods can measure how
much a KG feature (e.g., graph, node, path) influences the model to make the
correct prediction, thus explaining which KG features are useful. This paper
explores how saliency explanations can be used to improve KG-augmented models'
performance. First, we propose to create coarse (Is the KG useful?) and fine
(Which nodes/paths in the KG are useful?) saliency explanations. Second, to
motivate saliency-based supervision, we analyze oracle KG-augmented models
which directly use saliency explanations as extra inputs for guiding their
attention. Third, we propose SalKG, a framework for KG-augmented models to
learn from coarse and/or fine saliency explanations. Given saliency
explanations created from a task's training set, SalKG jointly trains the model
to predict the explanations, then solve the task by attending to KG features
highlighted by the predicted explanations. On three commonsense QA benchmarks
(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can
yield considerable performance gains -- up to 2.76% absolute improvement on
CSQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition. (arXiv:2105.12708v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12708">
<div class="article-summary-box-inner">
<span><p>Anglicisms are a challenge in German speech recognition. Due to their
irregular pronunciation compared to native German words, automatically
generated pronunciation dictionaries often include faulty phoneme sequences for
Anglicisms. In this work, we propose a multitask sequence-to-sequence approach
for grapheme-to-phoneme conversion to improve the phonetization of Anglicisms.
We extended a grapheme-to-phoneme model with a classifier to distinguish
Anglicisms from native German words. With this approach, the model learns to
generate pronunciations differently depending on the classification result. We
used our model to create supplementary Anglicism pronunciation dictionaries
that are added to an existing German speech recognition model. Tested on a
dedicated Anglicism evaluation set, we improved the recognition of Anglicisms
compared to a baseline model, reducing the word error rate by 1 % and the
Anglicism error rate by 3 %. We show that multitask learning can help solving
the challenge of Anglicisms in German speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CatVRNN: Generating Category Texts via Multi-task Learning. (arXiv:2107.05219v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05219">
<div class="article-summary-box-inner">
<span><p>Controlling the model to generate texts of different categories is a
challenging task that is receiving increasing attention. Recently, generative
adversarial networks (GANs) have shown promising results for category text
generation. However, the texts generated by GANs usually suffer from problems
of mode collapse and training instability. To avoid the above problems, in this
study, inspired by multi-task learning, a novel model called category-aware
variational recurrent neural network (CatVRNN) is proposed. In this model,
generation and classification tasks are trained simultaneously to generate
texts of different categories. The use of multi-task learning can improve the
quality of the generated texts, when the classification task is appropriate. In
addition, a function is proposed to initialize the hidden state of the CatVRNN
to force the model to generate texts of a specific category. Experimental
results on three datasets demonstrate that the model can outperform
state-of-the-art text generation methods based on GAN in terms of diversity of
generated texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation. (arXiv:2108.12960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12960">
<div class="article-summary-box-inner">
<span><p>Standard multi-task benchmarks are essential for developing pretraining
models that can generalize to various downstream tasks. Existing benchmarks for
natural language processing (NLP) usually focus only on understanding or
generating short texts. However, long text modeling requires many distinct
abilities in contrast to short texts, such as the modeling of long-range
discourse and commonsense relations, and the coherence and controllability of
generation. The lack of standardized benchmarks makes it difficult to assess
these abilities of a model and fairly compare different models, especially
Chinese models. Therefore, we propose a story-centric benchmark named LOT for
evaluating Chinese long text modeling, which aggregates two understanding tasks
and two generation tasks. We construct new datasets for these tasks based on
human-written Chinese stories with hundreds of words. Furthermore, we release
an encoder-decoder-based Chinese long text pretraining model named LongLM with
up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two
generative tasks including text infilling and conditional continuation.
Extensive experiments show that LongLM outperforms similar-sized pretraining
models substantially on both the understanding and generation tasks in LOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SADGA: Structure-Aware Dual Graph Aggregation Network for Text-to-SQL. (arXiv:2111.00653v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00653">
<div class="article-summary-box-inner">
<span><p>The Text-to-SQL task, aiming to translate the natural language of the
questions into SQL queries, has drawn much attention recently. One of the most
challenging problems of Text-to-SQL is how to generalize the trained model to
the unseen database schemas, also known as the cross-domain Text-to-SQL task.
The key lies in the generalizability of (i) the encoding method to model the
question and the database schema and (ii) the question-schema linking method to
learn the mapping between words in the question and tables/columns in the
database schema. Focusing on the above two key issues, we propose a
Structure-Aware Dual Graph Aggregation Network (SADGA) for cross-domain
Text-to-SQL. In SADGA, we adopt the graph structure to provide a unified
encoding model for both the natural language question and database schema.
Based on the proposed unified modeling, we further devise a structure-aware
aggregation method to learn the mapping between the question-graph and
schema-graph. The structure-aware aggregation method is featured with Global
Graph Linking, Local Graph Linking, and Dual-Graph Aggregation Mechanism. We
not only study the performance of our proposal empirically but also achieved
3rd place on the challenging Text-to-SQL benchmark Spider at the time of
writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding. (arXiv:2111.09098v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09098">
<div class="article-summary-box-inner">
<span><p>EHR systems lack a unified code system forrepresenting medical concepts,
which acts asa barrier for the deployment of deep learningmodels in large scale
to multiple clinics and hos-pitals. To overcome this problem, we
introduceDescription-based Embedding,DescEmb, a code-agnostic representation
learning framework forEHR. DescEmb takes advantage of the flexibil-ity of
neural language understanding models toembed clinical events using their
textual descrip-tions rather than directly mapping each event toa dedicated
embedding. DescEmb outperformedtraditional code-based embedding in
extensiveexperiments, especially in a zero-shot transfertask (one hospital to
another), and was able totrain a single unified model for heterogeneousEHR
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoBERTuito: a pre-trained language model for social media text in Spanish. (arXiv:2111.09453v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09453">
<div class="article-summary-box-inner">
<span><p>Since BERT appeared, Transformer language models and transfer learning have
become state-of-the-art for Natural Language Understanding tasks. Recently,
some works geared towards pre-training specially-crafted models for particular
domains, such as scientific papers, medical documents, user-generated texts,
among others. These domain-specific models have been shown to improve
performance significantly in most tasks. However, for languages other than
English such models are not widely available.
</p>
<p>In this work, we present RoBERTuito, a pre-trained language model for
user-generated text in Spanish, trained on over 500 million tweets. Experiments
on a benchmark of tasks involving user-generated text showed that RoBERTuito
outperformed other pre-trained language models in Spanish. In addition to this,
our model achieves top results for some English-Spanish tasks of the Linguistic
Code-Switching Evaluation benchmark (LinCE) and has also competitive
performance against monolingual models in English tasks. To facilitate further
research, we make RoBERTuito publicly available at the HuggingFace model hub
together with the dataset used to pre-train it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00656">
<div class="article-summary-box-inner">
<span><p>Recently, by introducing large-scale dataset and strong transformer network,
video-language pre-training has shown great success especially for retrieval.
Yet, existing video-language transformer models do not explicitly fine-grained
semantic align. In this work, we present Object-aware Transformers, an
object-centric approach that extends video-language transformer to incorporate
object representations. The key idea is to leverage the bounding boxes and
object tags to guide the training process. We evaluate our model on three
standard sub-tasks of video-text matching on four widely used benchmarks. We
also provide deep analysis and detailed ablation about the proposed method. We
show clear improvement in performance across all tasks and datasets considered,
demonstrating the value of a model that incorporates object representations
into a video-language architecture. The code will be released at
\url{https://github.com/FingerRec/OA-Transformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTMap: A BERT-based Ontology Alignment System. (arXiv:2112.02682v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02682">
<div class="article-summary-box-inner">
<span><p>Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in
knowledge integration. Owing to the success of machine learning in many
domains, it has been applied in OM. However, the existing methods, which often
adopt ad-hoc feature engineering or non-contextual word embeddings, have not
yet outperformed rule-based systems especially in an unsupervised setting. In
this paper, we propose a novel OM system named BERTMap which can support both
unsupervised and semi-supervised settings. It first predicts mappings using a
classifier based on fine-tuning the contextual embedding model BERT on text
semantics corpora extracted from ontologies, and then refines the mappings
through extension and repair by utilizing the ontology structure and logic. Our
evaluation with three alignment tasks on biomedical ontologies demonstrates
that BERTMap can often perform better than the leading OM systems LogMap and
AML.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06223">
<div class="article-summary-box-inner">
<span><p>Code-switching is a speech phenomenon occurring when a speaker switches
language during a conversation. Despite the spontaneous nature of
code-switching in conversational spoken language, most existing works collect
code-switching data from read speech instead of spontaneous speech. ASCEND (A
Spontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English
code-switching corpus built on spontaneous multi-turn conversational dialogue
sources collected in Hong Kong. We report ASCEND's design and procedure for
collecting the speech data, including annotations. ASCEND consists of 10.62
hours of clean speech, collected from 23 bilingual speakers of Chinese and
English. Furthermore, we conduct baseline experiments using pre-trained wav2vec
2.0 models, achieving a best performance of 22.69\% character error rate and
27.05% mixed error rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDD-Eval: Self-Training on Augmented Data for Multi-Domain Dialogue Evaluation. (arXiv:2112.07194v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07194">
<div class="article-summary-box-inner">
<span><p>Chatbots are designed to carry out human-like conversations across different
domains, such as general chit-chat, knowledge exchange, and persona-grounded
conversations. To measure the quality of such conversational agents, a dialogue
evaluator is expected to conduct assessment across domains as well. However,
most of the state-of-the-art automatic dialogue evaluation metrics (ADMs) are
not designed for multi-domain evaluation. We are motivated to design a general
and robust framework, MDD-Eval, to address the problem. Specifically, we first
train a teacher evaluator with human-annotated data to acquire a rating skill
to tell good dialogue responses from bad ones in a particular domain and then,
adopt a self-training strategy to train a new evaluator with teacher-annotated
multi-domain data, that helps the new evaluator to generalize across multiple
domains. MDD-Eval is extensively assessed on six dialogue evaluation
benchmarks. Empirical results show that the MDD-Eval framework achieves a
strong performance with an absolute improvement of 7% over the state-of-the-art
ADMs in terms of mean Spearman correlation scores across all the evaluation
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Defeat of the Winograd Schema Challenge. (arXiv:2201.02387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02387">
<div class="article-summary-box-inner">
<span><p>The Winograd Schema Challenge -- a set of twin sentences involving pronoun
reference disambiguation that seem to require the use of commonsense knowledge
-- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems,
based on large pre-trained transformer-based language models and fine-tuned on
these kinds of problems, achieved better than 90% accuracy. In this paper, we
review the history of the Winograd Schema Challenge and assess its
significance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition Datasets in Cantonese: A Survey and New Dataset. (arXiv:2201.02419v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02419">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) on low resource languages improves the
access of linguistic minorities to technological advantages provided by
artificial intelligence (AI). In this paper, we address the problem of data
scarcity for the Hong Kong Cantonese language by creating a new Cantonese
dataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6
hours of clean read speech paired with transcripts, collected from Cantonese
audiobooks from Hong Kong. It comprises philosophy, politics, education,
culture, lifestyle and family domains, covering a wide range of topics. We also
review all existing Cantonese datasets and analyze them according to their
speech type, data source, total size and availability. We further conduct
experiments with Fairseq S2T Transformer, a state-of-the-art ASR model, on the
biggest existing dataset, Common Voice zh-HK, and our proposed MDCC, and the
results show the effectiveness of our dataset. In addition, we create a
powerful and robust Cantonese ASR model by applying multi-dataset learning on
MDCC and Common Voice zh-HK.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. (arXiv:2201.03713v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03713">
<div class="article-summary-box-inner">
<span><p>We introduce CVSS, a massively multilingual-to-English speech-to-speech
translation (S2ST) corpus, covering sentence-level parallel S2ST pairs from 21
languages into English. CVSS is derived from the Common Voice speech corpus and
the CoVoST 2 speech-to-text translation (ST) corpus, by synthesizing the
translation text from CoVoST 2 into speech using state-of-the-art TTS systems.
Two versions of translation speeches are provided: 1) CVSS-C: All the
translation speeches are in a single high-quality canonical voice; 2) CVSS-T:
The translation speeches are in voices transferred from the corresponding
source speeches. In addition, CVSS provides normalized translation text which
matches the pronunciation in the translation speech. On each version of CVSS,
we built baseline multilingual direct S2ST models and cascade S2ST models,
verifying the effectiveness of the corpus. To build strong cascade S2ST
baselines, we trained an ST model on CoVoST 2, which outperforms the previous
state-of-the-art trained on the corpus without extra data by 5.8 BLEU.
Nevertheless, the performance of the direct S2ST models approaches the strong
cascade baselines when trained from scratch, and with only 0.1 or 0.7 BLEU
difference on ASR transcribed translation when initialized from matching ST
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Warm Start and a Clean Crawled Corpus -- A Recipe for Good Language Models. (arXiv:2201.05601v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05601">
<div class="article-summary-box-inner">
<span><p>We train several language models for Icelandic, including IceBERT, that
achieve state-of-the-art performance in a variety of downstream tasks,
including part-of-speech tagging, named entity recognition, grammatical error
detection and constituency parsing. To train the models we introduce a new
corpus of Icelandic text, the Icelandic Common Crawl Corpus (IC3), a collection
of high quality texts found online by targeting the Icelandic top-level-domain
(TLD). Several other public data sources are also collected for a total of 16GB
of Icelandic text. To enhance the evaluation of model performance and to raise
the bar in baselines for Icelandic, we translate and adapt the WinoGrande
dataset for co-reference resolution. Through these efforts we demonstrate that
a properly cleaned crawled corpus is sufficient to achieve state-of-the-art
results in NLP applications for low to medium resource languages, by comparison
with models trained on a curated corpus. We further show that initializing
models using existing multilingual models can lead to state-of-the-art results
for some downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Staged Cross-Lingual Acoustic Model Adaption for Robust Speech Recognition in Real-World Applications -- A Case Study on German Oral History Interviews. (arXiv:2005.12562v1 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.12562">
<div class="article-summary-box-inner">
<span><p>While recent automatic speech recognition systems achieve remarkable
performance when large amounts of adequate, high quality annotated speech data
is used for training, the same systems often only achieve an unsatisfactory
result for tasks in domains that greatly deviate from the conditions
represented by the training data. For many real-world applications, there is a
lack of sufficient data that can be directly used for training robust speech
recognition systems. To address this issue, we propose and investigate an
approach that performs a robust acoustic model adaption to a target domain in a
cross-lingual, multi-staged manner. Our approach enables the exploitation of
large-scale training data from other domains in both the same and other
languages. We evaluate our approach using the challenging task of German oral
history interviews, where we achieve a relative reduction of the word error
rate by more than 30% compared to a model trained from scratch only on the
target domain, and 6-7% relative compared to a model trained robustly on 1000
hours of same-language out-of-domain training data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentanglement enables cross-domain Hippocampus Segmentation. (arXiv:2201.05650v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05650">
<div class="article-summary-box-inner">
<span><p>Limited amount of labelled training data are a common problem in medical
imaging. This makes it difficult to train a well-generalised model and
therefore often leads to failure in unknown domains. Hippocampus segmentation
from magnetic resonance imaging (MRI) scans is critical for the diagnosis and
treatment of neuropsychatric disorders. Domain differences in contrast or shape
can significantly affect segmentation. We address this issue by disentangling a
T1-weighted MRI image into its content and domain. This separation enables us
to perform a domain transfer and thus convert data from new sources into the
training domain. This step thus simplifies the segmentation problem, resulting
in higher quality segmentations. We achieve the disentanglement with the
proposed novel methodology 'Content Domain Disentanglement GAN', and we propose
to retrain the UNet on the transformed outputs to deal with GAN-specific
artefacts. With these changes, we are able to improve performance on unseen
domains by 6-13% and outperform state-of-the-art domain transfer methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in Action:Weakly Supervised Action Segmentation. (arXiv:2201.05675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05675">
<div class="article-summary-box-inner">
<span><p>The video action segmentation task is regularly explored under weaker forms
of supervision, such as transcript supervision, where a list of actions is
easier to obtain than dense frame-wise labels. In this formulation, the task
presents various challenges for sequence modeling approaches due to the
emphasis on action transition points, long sequence lengths, and frame
contextualization, making the task well-posed for transformers. Given
developments enabling transformers to scale linearly, we demonstrate through
our architecture how they can be applied to improve action alignment accuracy
over the equivalent RNN-based models with the attention mechanism focusing
around salient action transition regions. Additionally, given the recent focus
on inference-time transcript selection, we propose a supplemental transcript
embedding approach to select transcripts more quickly at inference-time.
Furthermore, we subsequently demonstrate how this approach can also improve the
overall segmentation performance. Finally, we evaluate our proposed methods
across the benchmark datasets to better understand the applicability of
transformers and the importance of transcript selection on this video-driven
weakly-supervised task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective Transformation Layer. (arXiv:2201.05706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05706">
<div class="article-summary-box-inner">
<span><p>Incorporating geometric transformations that reflect the relative position
changes between an observer and an object into computer vision and deep
learning models has attracted much attention in recent years. However, the
existing proposals mainly focus on affine transformations that cannot fully
show viewpoint changes. Furthermore, current solutions often apply a neural
network module to learn a single transformation matrix, which ignores the
possibility for various viewpoints and creates extra to-be-trained module
parameters. In this paper, a layer (PT layer) is proposed to learn the
perspective transformations that not only model the geometries in affine
transformation but also reflect the viewpoint changes. In addition, being able
to be directly trained with gradient descent like traditional layers such as
convolutional layers, a single proposed PT layer can learn an adjustable number
of multiple viewpoints without training extra module parameters. The
experiments and evaluations confirm the superiority of the proposed PT layer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-free Online Test-time Adaptation. (arXiv:2201.05718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05718">
<div class="article-summary-box-inner">
<span><p>Training state-of-the-art vision models has become prohibitively expensive
for researchers and practitioners. For the sake of accessibility and resource
reuse, it is important to focus on adapting these models to a variety of
downstream scenarios. An interesting and practical paradigm is online test-time
adaptation, according to which training data is inaccessible, no labelled data
from the test distribution is available, and adaptation can only happen at test
time and on a handful of samples. In this paper, we investigate how test-time
adaptation methods fare for a number of pre-trained models on a variety of
real-world scenarios, significantly extending the way they have been originally
evaluated. We show that they perform well only in narrowly-defined experimental
setups and sometimes fail catastrophically when their hyperparameters are not
selected for the same scenario in which they are being tested. Motivated by the
inherent uncertainty around the conditions that will ultimately be encountered
at test time, we propose a particularly "conservative" approach, which
addresses the problem with a Laplacian Adjusted Maximum-likelihood Estimation
(LAME) objective. By adapting the model's output (not its parameters), and
solving our objective with an efficient concave-convex procedure, our approach
exhibits a much higher average accuracy across scenarios than existing methods,
while being notably faster and have a much lower memory footprint. Code
available at https://github.com/fiveai/LAME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Temporally and Semantically Consistent Unpaired Video-to-video Translation Through Pseudo-Supervision From Synthetic Optical Flow. (arXiv:2201.05723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05723">
<div class="article-summary-box-inner">
<span><p>Unpaired video-to-video translation aims to translate videos between a source
and a target domain without the need of paired training data, making it more
feasible for real applications. Unfortunately, the translated videos generally
suffer from temporal and semantic inconsistency. To address this, many existing
works adopt spatiotemporal consistency constraints incorporating temporal
information based on motion estimation. However, the inaccuracies in the
estimation of motion deteriorate the quality of the guidance towards
spatiotemporal consistency, which leads to unstable translation. In this work,
we propose a novel paradigm that regularizes the spatiotemporal consistency by
synthesizing motions in input videos with the generated optical flow instead of
estimating them. Therefore, the synthetic motion can be applied in the
regularization paradigm to keep motions consistent across domains without the
risk of errors in motion estimation. Thereafter, we utilize our unsupervised
recycle and unsupervised spatial loss, guided by the pseudo-supervision
provided by the synthetic optical flow, to accurately enforce spatiotemporal
consistency in both domains. Experiments show that our method is versatile in
various scenarios and achieves state-of-the-art performance in generating
temporally and semantically consistent videos. Code is available at:
https://github.com/wangkaihong/Unsup_Recycle_GAN/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks. (arXiv:2201.05729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05729">
<div class="article-summary-box-inner">
<span><p>Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hierarchical Graph Representation for Image Manipulation Detection. (arXiv:2201.05730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05730">
<div class="article-summary-box-inner">
<span><p>The objective of image manipulation detection is to identify and locate the
manipulated regions in the images. Recent approaches mostly adopt the
sophisticated Convolutional Neural Networks (CNNs) to capture the tampering
artifacts left in the images to locate the manipulated regions. However, these
approaches ignore the feature correlations, i.e., feature inconsistencies,
between manipulated regions and non-manipulated regions, leading to inferior
detection performance. To address this issue, we propose a hierarchical Graph
Convolutional Network (HGCN-Net), which consists of two parallel branches: the
backbone network branch and the hierarchical graph representation learning
(HGRL) branch for image manipulation detection. Specifically, the feature maps
of a given image are extracted by the backbone network branch, and then the
feature correlations within the feature maps are modeled as a set of
fully-connected graphs for learning the hierarchical graph representation by
the HGRL branch. The learned hierarchical graph representation can sufficiently
capture the feature correlations across different scales, and thus it provides
high discriminability for distinguishing manipulated and non-manipulated
regions. Extensive experiments on four public datasets demonstrate that the
proposed HGCN-Net not only provides promising detection accuracy, but also
achieves strong robustness under a variety of common image attacks in the task
of image manipulation detection, compared to the state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-World Graph Convolution Networks (RW-GCNs) for Action Recognition in Smart Video Surveillance. (arXiv:2201.05739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05739">
<div class="article-summary-box-inner">
<span><p>Action recognition is a key algorithmic part of emerging on-the-edge smart
video surveillance and security systems. Skeleton-based action recognition is
an attractive approach which, instead of using RGB pixel data, relies on human
pose information to classify appropriate actions. However, existing algorithms
often assume ideal conditions that are not representative of real-world
limitations, such as noisy input, latency requirements, and edge resource
constraints.
</p>
<p>To address the limitations of existing approaches, this paper presents
Real-World Graph Convolution Networks (RW-GCNs), an architecture-level solution
for meeting the domain constraints of Real World Skeleton-based Action
Recognition. Inspired by the presence of feedback connections in the human
visual cortex, RW-GCNs leverage attentive feedback augmentation on existing
near state-of-the-art (SotA) Spatial-Temporal Graph Convolution Networks
(ST-GCNs). The ST-GCNs' design choices are derived from information
theory-centric principles to address both the spatial and temporal noise
typically encountered in end-to-end real-time and on-the-edge smart video
systems. Our results demonstrate RW-GCNs' ability to serve these applications
by achieving a new SotA accuracy on the NTU-RGB-D-120 dataset at 94.1%, and
achieving 32X less latency than baseline ST-GCN applications while still
achieving 90.4% accuracy on the Northwestern UCLA dataset in the presence of
spatial keypoint noise. RW-GCNs further show system scalability by running on
the 10X cost effective NVIDIA Jetson Nano (as opposed to NVIDIA Xavier NX),
while still maintaining a respectful range of throughput (15.6 to 5.5 Actions
per Second) on the resource constrained device. The code is available here:
https://github.com/TeCSAR-UNCC/RW-GCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on RGB-D Datasets. (arXiv:2201.05761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05761">
<div class="article-summary-box-inner">
<span><p>RGB-D data is essential for solving many problems in computer vision.
Hundreds of public RGB-D datasets containing various scenes, such as indoor,
outdoor, aerial, driving, and medical, have been proposed. These datasets are
useful for different applications and are fundamental for addressing classic
computer vision tasks, such as monocular depth estimation. This paper reviewed
and categorized image datasets that include depth information. We gathered 203
datasets that contain accessible data and grouped them into three categories:
scene/objects, body, and medical. We also provided an overview of the different
types of sensors, depth applications, and we examined trends and future
directions of the usage and creation of datasets containing depth data, and how
they can be applied to investigate the development of generalizable machine
learning models in the monocular depth estimation field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Compressive Imaging Reconstruction Using Convolution and Spectral Contextual Transformer. (arXiv:2201.05768v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05768">
<div class="article-summary-box-inner">
<span><p>Spectral compressive imaging (SCI) is able to encode the high-dimensional
hyperspectral image to a 2D measurement, and then uses algorithms to
reconstruct the spatio-spectral data-cube. At present, the main bottleneck of
SCI is the reconstruction algorithm, and the state-of-the-art (SOTA)
reconstruction methods generally face the problem of long reconstruction time
and/or poor detail recovery. In this paper, we propose a novel hybrid network
module, namely CSCoT (Convolution and Spectral Contextual Transformer) block,
which can acquire the local perception of convolution and the global perception
of transformer simultaneously, and is conducive to improving the quality of
reconstruction to restore fine details. We integrate the proposed CSCoT block
into deep unfolding framework based on the generalized alternating projection
algorithm, and further propose the GAP-CSCoT network. Finally, we apply the
GAP-CSCoT algorithm to SCI reconstruction. Through the experiments of extensive
synthetic and real data, our proposed model achieves higher reconstruction
quality ($&gt;$2dB in PSNR on simulated benchmark datasets) and shorter running
time than existing SOTA algorithms by a large margin. The code and models will
be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asymmetric Hash Code Learning for Remote Sensing Image Retrieval. (arXiv:2201.05772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05772">
<div class="article-summary-box-inner">
<span><p>Remote sensing image retrieval (RSIR), aiming at searching for a set of
similar items to a given query image, is a very important task in remote
sensing applications. Deep hashing learning as the current mainstream method
has achieved satisfactory retrieval performance. On one hand, various deep
neural networks are used to extract semantic features of remote sensing images.
On the other hand, the hashing techniques are subsequently adopted to map the
high-dimensional deep features to the low-dimensional binary codes. This kind
of methods attempts to learn one hash function for both the query and database
samples in a symmetric way. However, with the number of database samples
increasing, it is typically time-consuming to generate the hash codes of
large-scale database images. In this paper, we propose a novel deep hashing
method, named asymmetric hash code learning (AHCL), for RSIR. The proposed AHCL
generates the hash codes of query and database images in an asymmetric way. In
more detail, the hash codes of query images are obtained by binarizing the
output of the network, while the hash codes of database images are directly
learned by solving the designed objective function. In addition, we combine the
semantic information of each image and the similarity information of pairs of
images as supervised information to train a deep hashing network, which
improves the representation ability of deep features and hash codes. The
experimental results on three public datasets demonstrate that the proposed
method outperforms symmetric methods in terms of retrieval accuracy and
efficiency. The source code is available at
https://github.com/weiweisong415/Demo AHCL for TGRS2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainability Tools Enabling Deep Learning in Future In-Situ Real-Time Planetary Explorations. (arXiv:2201.05775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05775">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has proven to be an effective machine learning and
computer vision technique. DL-based image segmentation, object recognition and
classification will aid many in-situ Mars rover tasks such as path planning and
artifact recognition/extraction. However, most of the Deep Neural Network (DNN)
architectures are so complex that they are considered a 'black box'. In this
paper, we used integrated gradients to describe the attributions of each neuron
to the output classes. It provides a set of explainability tools (ET) that
opens the black box of a DNN so that the individual contribution of neurons to
category classification can be ranked and visualized. The neurons in each dense
layer are mapped and ranked by measuring expected contribution of a neuron to a
class vote given a true image label. The importance of neurons is prioritized
according to their correct or incorrect contribution to the output classes and
suppression or bolstering of incorrect classes, weighted by the size of each
class. ET provides an interface to prune the network to enhance high-rank
neurons and remove low-performing neurons. ET technology will make DNNs smaller
and more efficient for implementation in small embedded systems. It also leads
to more explainable and testable DNNs that can make systems easier for
Validation \&amp; Verification. The goal of ET technology is to enable the adoption
of DL in future in-situ planetary exploration missions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Multi-View Representation Learning. (arXiv:2201.05776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05776">
<div class="article-summary-box-inner">
<span><p>Learning from different data views by exploring the underlying complementary
information among them can endow the representation with stronger expressive
ability. However, high-dimensional features tend to contain noise, and
furthermore, the quality of data usually varies for different samples (even for
different views), i.e., one view may be informative for one sample but not the
case for another. Therefore, it is quite challenging to integrate multi-view
noisy data under unsupervised setting. Traditional multi-view methods either
simply treat each view with equal importance or tune the weights of different
views to fixed values, which are insufficient to capture the dynamic noise in
multi-view data. In this work, we devise a novel unsupervised multi-view
learning approach, termed as Dynamic Uncertainty-Aware Networks (DUA-Nets).
Guided by the uncertainty of data estimated from the generation perspective,
intrinsic information from multiple views is integrated to obtain noise-free
representations. Under the help of uncertainty, DUA-Nets weigh each view of
individual sample according to data quality so that the high-quality samples
(or views) can be fully exploited while the effects from the noisy samples (or
views) will be alleviated. Our model achieves superior performance in extensive
experiments and shows the robustness to noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic decoupled representation learning for remote sensing image change detection. (arXiv:2201.05778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05778">
<div class="article-summary-box-inner">
<span><p>Contemporary transfer learning-based methods to alleviate the data
insufficiency in change detection (CD) are mainly based on ImageNet
pre-training. Self-supervised learning (SSL) has recently been introduced to
remote sensing (RS) for learning in-domain representations. Here, we propose a
semantic decoupled representation learning for RS image CD. Typically, the
object of interest (e.g., building) is relatively small compared to the vast
background. Different from existing methods expressing an image into one
representation vector that may be dominated by irrelevant land-covers, we
disentangle representations of different semantic regions by leveraging the
semantic mask. We additionally force the model to distinguish different
semantic representations, which benefits the recognition of objects of interest
in the downstream CD task. We construct a dataset of bitemporal images with
semantic masks in an effortless manner for pre-training. Experiments on two CD
datasets show our model outperforms ImageNet pre-training, in-domain supervised
pre-training, and several recent SSL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OneDConv: Generalized Convolution For Transform-Invariant Representation. (arXiv:2201.05781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05781">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have exhibited their great power in a
variety of vision tasks. However, the lack of transform-invariant property
limits their further applications in complicated real-world scenarios. In this
work, we proposed a novel generalized one dimension convolutional operator
(OneDConv), which dynamically transforms the convolution kernels based on the
input features in a computationally and parametrically efficient manner. The
proposed operator can extract the transform-invariant features naturally. It
improves the robustness and generalization of convolution without sacrificing
the performance on common images. The proposed OneDConv operator can substitute
the vanilla convolution, thus it can be incorporated into current popular
convolutional architectures and trained end-to-end readily. On several popular
benchmarks, OneDConv outperforms the original convolution operation and other
proposed models both in canonical and distorted images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network for Tabular Data Classification. (arXiv:2201.05809v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05809">
<div class="article-summary-box-inner">
<span><p>In this paper, we first introduce batch normalization to the edRVFL network.
This re-normalization method can help the network avoid divergence of the
hidden features. Then we propose novel variants of Ensemble Deep Random Vector
Functional Link (edRVFL). Weighted edRVFL (WedRVFL) uses weighting methods to
give training samples different weights in different layers according to how
the samples were classified confidently in the previous layer thereby
increasing the ensemble's diversity and accuracy. Furthermore, a pruning-based
edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based
on their importance for classification before generating the next hidden layer.
Through this method, we ensure that the randomly generated inferior features
will not propagate to deeper layers. Subsequently, the combination of weighting
and pruning, called Weighting and Pruning based Ensemble Deep Random Vector
Functional Link Network (WPedRVFL), is proposed. We compare their performances
with other state-of-the-art deep feedforward neural networks (FNNs) on 24
tabular UCI classification datasets. The experimental results illustrate the
superior performance of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage is Enough: A Concise Deep Unfolding Reconstruction Network for Flexible Video Compressive Sensing. (arXiv:2201.05810v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05810">
<div class="article-summary-box-inner">
<span><p>We consider the reconstruction problem of video compressive sensing (VCS)
under the deep unfolding/rolling structure. Yet, we aim to build a flexible and
concise model using minimum stages. Different from existing deep unfolding
networks used for inverse problems, where more stages are used for higher
performance but without flexibility to different masks and scales, hereby we
show that a 2-stage deep unfolding network can lead to the state-of-the-art
(SOTA) results (with a 1.7dB gain in PSNR over the single stage model, RevSCI)
in VCS. The proposed method possesses the properties of adaptation to new masks
and ready to scale to large data without any additional training thanks to the
advantages of deep unfolding. Furthermore, we extend the proposed model for
color VCS to perform joint reconstruction and demosaicing. Experimental results
demonstrate that our 2-stage model has also achieved SOTA on color VCS
reconstruction, leading to a &gt;2.3dB gain in PSNR over the previous SOTA
algorithm based on plug-and-play framework, meanwhile speeds up the
reconstruction by &gt;17 times. In addition, we have found that our network is
also flexible to the mask modulation and scale size for color VCS
reconstruction so that a single trained network can be applied to different
hardware systems. The code and models will be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Critical Analysis of Image-based Camera Pose Estimation Techniques. (arXiv:2201.05816v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05816">
<div class="article-summary-box-inner">
<span><p>Camera, and associated with its objects within the field of view,
localization could benefit many computer vision fields, such as autonomous
driving, robot navigation, and augmented reality (AR). In this survey, we first
introduce specific application areas and the evaluation metrics for camera
localization pose according to different sub-tasks (learning-based 2D-2D task,
feature-based 2D-3D task, and 3D-3D task). Then, we review common methods for
structure-based camera pose estimation approaches, absolute pose regression and
relative pose regression approaches by critically modelling the methods to
inspire further improvements in their algorithms such as loss functions, neural
network structures. Furthermore, we summarise what are the popular datasets
used for camera localization and compare the quantitative and qualitative
results of these methods with detailed performance metrics. Finally, we discuss
future research possibilities and applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offline-Online Associated Camera-Aware Proxies for Unsupervised Person Re-identification. (arXiv:2201.05820v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05820">
<div class="article-summary-box-inner">
<span><p>Recently, unsupervised person re-identification (Re-ID) has received
increasing research attention due to its potential for label-free applications.
A promising way to address unsupervised Re-ID is clustering-based, which
generates pseudo labels by clustering and uses the pseudo labels to train a
Re-ID model iteratively. However, most clustering-based methods take each
cluster as a pseudo identity class, neglecting the intra-cluster variance
mainly caused by the change of cameras. To address this issue, we propose to
split each single cluster into multiple proxies according to camera views. The
camera-aware proxies explicitly capture local structures within clusters, by
which the intra-ID variance and inter-ID similarity can be better tackled.
Assisted with the camera-aware proxies, we design two proxy-level contrastive
learning losses that are, respectively, based on offline and online association
results. The offline association directly associates proxies according to the
clustering and splitting results, while the online strategy dynamically
associates proxies in terms of up-to-date features to reduce the noise caused
by the delayed update of pseudo labels. The combination of two losses enable us
to train a desirable Re-ID model. Extensive experiments on three person Re-ID
datasets and one vehicle Re-ID dataset show that our proposed approach
demonstrates competitive performance with state-of-the-art methods. Code will
be available at: https://github.com/Terminator8758/O2CAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View representation learning in Multi-Task Scene. (arXiv:2201.05829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05829">
<div class="article-summary-box-inner">
<span><p>Over recent decades have witnessed considerable progress in whether
multi-task learning or multi-view learning, but the situation that consider
both learning scenes simultaneously has received not too much attention. How to
utilize multiple views latent representation of each single task to improve
each learning task performance is a challenge problem. Based on this, we
proposed a novel semi-supervised algorithm, termed as Multi-Task Multi-View
learning based on Common and Special Features (MTMVCSF). In general,
multi-views are the different aspects of an object and every view includes the
underlying common or special information of this object. As a consequence, we
will mine multiple views jointly latent factor of each learning task which
consists of each view special feature and the common feature of all views. By
this way, the original multi-task multi-view data has degenerated into
multi-task data, and exploring the correlations among multiple tasks enables to
make an improvement on the performance of learning algorithm. Another obvious
advantage of this approach is that we get latent representation of the set of
unlabeled instances by the constraint of regression task with labeled
instances. The performance of classification and semi-supervised clustering
task in these latent representations perform obviously better than it in raw
data. Furthermore, an anti-noise multi-task multi-view algorithm called
AN-MTMVCSF is proposed, which has a strong adaptability to noise labels. The
effectiveness of these algorithms is proved by a series of well-designed
experiments on both real world and synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tailor Versatile Multi-modal Learning for Multi-label Emotion Recognition. (arXiv:2201.05834v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05834">
<div class="article-summary-box-inner">
<span><p>Multi-modal Multi-label Emotion Recognition (MMER) aims to identify various
human emotions from heterogeneous visual, audio and text modalities. Previous
methods mainly focus on projecting multiple modalities into a common latent
space and learning an identical representation for all labels, which neglects
the diversity of each modality and fails to capture richer semantic information
for each label from different perspectives. Besides, associated relationships
of modalities and labels have not been fully exploited. In this paper, we
propose versaTile multi-modAl learning for multI-labeL emOtion Recognition
(TAILOR), aiming to refine multi-modal representations and enhance
discriminative capacity of each label. Specifically, we design an adversarial
multi-modal refinement module to sufficiently explore the commonality among
different modalities and strengthen the diversity of each modality. To further
exploit label-modal dependence, we devise a BERT-like cross-modal encoder to
gradually fuse private and common modality representations in a granularity
descent way, as well as a label-guided decoder to adaptively generate a
tailored representation for each label with the guidance of label semantics. In
addition, we conduct experiments on the benchmark MMER dataset CMU-MOSEI in
both aligned and unaligned settings, which demonstrate the superiority of
TAILOR over the state-of-the-arts. Code is available at
https://github.com/kniter1/TAILOR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Parking Space Detection under Hazy conditions using Convolutional Neural Networks: A Novel Approach. (arXiv:2201.05858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05858">
<div class="article-summary-box-inner">
<span><p>Limited urban parking space combined with urbanization has necessitated the
development of smart parking systems that can communicate the availability of
parking slots to the end users. Towards this, various deep learning based
solutions using convolutional neural networks have been proposed for parking
space occupation detection. Though these approaches are robust to partial
obstructions and lighting conditions, their performance is found to degrade in
the presence of haze conditions. Looking in this direction, this paper
investigates the use of dehazing networks that improves the performance of
parking space occupancy classifier under hazy conditions. Additionally,
training procedures are proposed for dehazing networks to maximize the
performance of the system on both hazy and non-hazy conditions. The proposed
system is deployable as part of existing smart parking systems where limited
number of cameras are used to monitor hundreds of parking spaces. To validate
our approach, we have developed a custom hazy parking system dataset from
real-world task-driven test set of RESIDE-\b{eta} dataset. The proposed
approach is tested against existing state-of-the-art parking space detectors on
CNRPark-EXT and hazy parking system datasets. Experimental results indicate
that there is a significant accuracy improvement of the proposed approach on
the hazy parking system dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDT-DCSCN for Simultaneous Super-Resolution and Deblurring of Text Images. (arXiv:2201.05865v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05865">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (Deep CNN) have achieved hopeful
performance for single image super-resolution. In particular, the Deep CNN skip
Connection and Network in Network (DCSCN) architecture has been successfully
applied to natural images super-resolution. In this work we propose an approach
called SDT-DCSCN that jointly performs super-resolution and deblurring of
low-resolution blurry text images based on DCSCN. Our approach uses subsampled
blurry images in the input and original sharp images as ground truth. The used
architecture is consists of a higher number of filters in the input CNN layer
to a better analysis of the text details. The quantitative and qualitative
evaluation on different datasets prove the high performance of our model to
reconstruct high-resolution and sharp text images. In addition, in terms of
computational time, our proposed method gives competitive performance compared
to state of the art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype Guided Network for Anomaly Segmentation. (arXiv:2201.05869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05869">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation methods can not directly identify abnormal objects in
images. Anomaly Segmentation algorithm from this realistic setting can
distinguish between in-distribution objects and Out-Of-Distribution (OOD)
objects and output the anomaly probability for pixels. In this paper, a
Prototype Guided Anomaly segmentation Network (PGAN) is proposed to extract
semantic prototypes for in-distribution training data from limited annotated
images. In the model, prototypes are used to model the hierarchical category
semantic information and distinguish OOD pixels. The proposed PGAN model
includes a semantic segmentation network and a prototype extraction network.
Similarity measures are adopted to optimize the prototypes. The learned
semantic prototypes are used as category semantics to compare the similarity
with features extracted from test images and then to generate semantic
segmentation prediction. The proposed prototype extraction network can also be
integrated into most semantic segmentation networks and recognize OOD pixels.
On the StreetHazards dataset, the proposed PGAN model produced mIoU of 53.4%
for anomaly segmentation. The experimental results demonstrate PGAN may achieve
the SOTA performance in the anomaly segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation via Bidirectional Cross-Attention Transformer. (arXiv:2201.05887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05887">
<div class="article-summary-box-inner">
<span><p>Domain Adaptation (DA) aims to leverage the knowledge learned from a source
domain with ample labeled data to a target domain with unlabeled data only.
Most existing studies on DA contribute to learning domain-invariant feature
representations for both domains by minimizing the domain gap based on
convolution-based neural networks. Recently, vision transformers significantly
improved performance in multiple vision tasks. Built on vision transformers, in
this paper we propose a Bidirectional Cross-Attention Transformer (BCAT) for DA
with the aim to improve the performance. In the proposed BCAT, the attention
mechanism can extract implicit source and target mix-up feature representations
to narrow the domain discrepancy. Specifically, in BCAT, we design a
weight-sharing quadruple-branch transformer with a bidirectional
cross-attention mechanism to learn domain-invariant feature representations.
Extensive experiments demonstrate that the proposed BCAT model achieves
superior performance on four benchmark datasets over existing state-of-the-art
DA methods that are based on convolutions or transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data. (arXiv:2201.05905v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05905">
<div class="article-summary-box-inner">
<span><p>Capsule network is a recent new deep network architecture that has been
applied successfully for medical image segmentation tasks. This work extends
capsule networks for volumetric medical image segmentation with self-supervised
learning. To improve on the problem of weight initialization compared to
previous capsule networks, we leverage self-supervised learning for capsule
networks pre-training, where our pretext-task is optimized by
self-reconstruction. Our capsule network, SS-3DCapsNet, has a UNet-based
architecture with a 3D Capsule encoder and 3D CNNs decoder. Our experiments on
multiple datasets including iSeg-2017, Hippocampus, and Cardiac demonstrate
that our 3D capsule network with self-supervised pre-training considerably
outperforms previous capsule networks and 3D-UNets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-shot Sign Language Recognition. (arXiv:2201.05914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05914">
<div class="article-summary-box-inner">
<span><p>This paper tackles the problem of zero-shot sign language recognition
(ZSSLR), where the goal is to leverage models learned over the seen sign
classes to recognize the instances of unseen sign classes. In this context,
readily available textual sign descriptions and attributes collected from sign
language dictionaries are utilized as semantic class representations for
knowledge transfer. For this novel problem setup, we introduce three benchmark
datasets with their accompanying textual and attribute descriptions to analyze
the problem in detail. Our proposed approach builds spatiotemporal models of
body and hand regions. By leveraging the descriptive text and attribute
embeddings along with these visual representations within a zero-shot learning
framework, we show that textual and attribute based class definitions can
provide effective knowledge for the recognition of previously unseen sign
classes. We additionally introduce techniques to analyze the influence of
binary attributes in correct and incorrect zero-shot predictions. We anticipate
that the introduced approaches and the accompanying datasets will provide a
basis for further exploration of zero-shot learning in sign language
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Second-order Few-shot Learning. (arXiv:2201.05916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05916">
<div class="article-summary-box-inner">
<span><p>We propose a Multi-level Second-order (MlSo) few-shot learning network for
supervised or unsupervised few-shot image classification and few-shot action
recognition. We leverage so-called power-normalized second-order base learner
streams combined with features that express multiple levels of visual
abstraction, and we use self-supervised discriminating mechanisms. As
Second-order Pooling (SoP) is popular in image recognition, we employ its basic
element-wise variant in our pipeline. The goal of multi-level feature design is
to extract feature representations at different layer-wise levels of CNN,
realizing several levels of visual abstraction to achieve robust few-shot
learning. As SoP can handle convolutional feature maps of varying spatial
sizes, we also introduce image inputs at multiple spatial scales into MlSo. To
exploit the discriminative information from multi-level and multi-scale
features, we develop a Feature Matching (FM) module that reweights their
respective branches. We also introduce a self-supervised step, which is a
discriminator of the spatial level and the scale of abstraction. Our pipeline
is trained in an end-to-end manner. With a simple architecture, we demonstrate
respectable results on standard datasets such as Omniglot, mini-ImageNet,
tiered-ImageNet, Open MIC, fine-grained datasets such as CUB Birds, Stanford
Dogs and Cars, and action recognition datasets such as HMDB51, UCF101, and
mini-MIT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViTBIS: Vision Transformer for Biomedical Image Segmentation. (arXiv:2201.05920v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05920">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel network named Vision Transformer for
Biomedical Image Segmentation (ViTBIS). Our network splits the input feature
maps into three parts with $1\times 1$, $3\times 3$ and $5\times 5$
convolutions in both encoder and decoder. Concat operator is used to merge the
features before being fed to three consecutive transformer blocks with
attention mechanism embedded inside it. Skip connections are used to connect
encoder and decoder transformer blocks. Similarly, transformer blocks and multi
scale architecture is used in decoder before being linearly projected to
produce the output segmentation map. We test the performance of our network
using Synapse multi-organ segmentation dataset, Automated cardiac diagnosis
challenge dataset, Brain tumour MRI segmentation dataset and Spleen CT
segmentation dataset. Without bells and whistles, our network outperforms most
of the previous state of the art CNN and transformer based models using Dice
score and the Hausdorff distance as the evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradTail: Learning Long-Tailed Data Using Gradient-based Sample Weighting. (arXiv:2201.05938v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05938">
<div class="article-summary-box-inner">
<span><p>We propose GradTail, an algorithm that uses gradients to improve model
performance on the fly in the face of long-tailed training data distributions.
Unlike conventional long-tail classifiers which operate on converged - and
possibly overfit - models, we demonstrate that an approach based on gradient
dot product agreement can isolate long-tailed data early on during model
training and improve performance by dynamically picking higher sample weights
for that data. We show that such upweighting leads to model improvements for
both classification and regression models, the latter of which are relatively
unexplored in the long-tail literature, and that the long-tail examples found
by gradient alignment are consistent with our semantic expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Regular Network for Writer Identification. (arXiv:2201.05951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05951">
<div class="article-summary-box-inner">
<span><p>Writer identification has practical applications for forgery detection and
forensic science. Most models based on deep neural networks extract features
from character image or sub-regions in character image, which ignoring features
contained in page-region image. Our proposed global regular network (GRN) pays
attention to these features. GRN network consists of two branches: one branch
takes page handwriting as input to extract global features, and the other takes
word handwriting as input to extract local features. Global features and local
features merge in a global residual way to form overall features of the
handwriting. The proposed GRN has two attributions: one is adding a branch to
extract features contained in page; the other is using residual attention
network to extract local feature. Experiments demonstrate the effectiveness of
both strategies. On CVL dataset, our models achieve impressive 99.98% top-1
accuracy and 100% top-5 accuracy with shorter training time and fewer network
parameters, which exceeded the state-of-the-art structure. The experiment shows
the powerful ability of the network in the field of writer identification. The
source code is available at https://github.com/wangshiyu001/GRN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Centroid Ripple Pattern for Facial Expression Recognition. (arXiv:2201.05958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05958">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new feature descriptor Cross-Centroid Ripple
Pattern (CRIP) for facial expression recognition. CRIP encodes the transitional
pattern of a facial expression by incorporating cross-centroid relationship
between two ripples located at radius r1 and r2 respectively. These ripples are
generated by dividing the local neighborhood region into subregions. Thus, CRIP
has ability to preserve macro and micro structural variations in an extensive
region, which enables it to deal with side views and spontaneous expressions.
Furthermore, gradient information between cross centroid ripples provides
strenght to captures prominent edge features in active patches: eyes, nose and
mouth, that define the disparities between different facial expressions. Cross
centroid information also provides robustness to irregular illumination.
Moreover, CRIP utilizes the averaging behavior of pixels at subregions that
yields robustness to deal with noisy conditions. The performance of proposed
descriptor is evaluated on seven comprehensive expression datasets consisting
of challenging conditions such as age, pose, ethnicity and illumination
variations. The experimental results show that our descriptor consistently
achieved better accuracy rate as compared to existing state-of-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Residual Encoder-Decoder Network for Segmentation of Retinal Image-Based Exudates in Diabetic Retinopathy Screening. (arXiv:2201.05963v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05963">
<div class="article-summary-box-inner">
<span><p>Diabetic retinopathy refers to the pathology of the retina induced by
diabetes and is one of the leading causes of preventable blindness in the
world. Early detection of diabetic retinopathy is critical to avoid vision
problem through continuous screening and treatment. In traditional clinical
practice, the involved lesions are manually detected using photographs of the
fundus. However, this task is cumbersome and time-consuming and requires
intense effort due to the small size of lesion and low contrast of the images.
Thus, computer-assisted diagnosis of diabetic retinopathy based on the
detection of red lesions is actively being explored recently. In this paper, we
present a convolutional neural network with residual skip connection for the
segmentation of exudates in retinal images. To improve the performance of
network architecture, a suitable image augmentation technique is used. The
proposed network can robustly segment exudates with high accuracy, which makes
it suitable for diabetic retinopathy screening. Comparative performance
analysis of three benchmark databases: HEI-MED, E-ophtha, and DiaretDB1 is
presented. It is shown that the proposed method achieves accuracy (0.98, 0.99,
0.98) and sensitivity (0.97, 0.92, and 0.95) on E-ophtha, HEI-MED, and
DiaReTDB1, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Cross-scale Attention Network for Efficient LiDAR Panoptic Segmentation. (arXiv:2201.05972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05972">
<div class="article-summary-box-inner">
<span><p>Two major challenges of 3D LiDAR Panoptic Segmentation (PS) are that point
clouds of an object are surface-aggregated and thus hard to model the
long-range dependency especially for large instances, and that objects are too
close to separate each other. Recent literature addresses these problems by
time-consuming grouping processes such as dual-clustering, mean-shift offsets,
etc., or by bird-eye-view (BEV) dense centroid representation that downplays
geometry. However, the long-range geometry relationship has not been
sufficiently modeled by local feature learning from the above methods. To this
end, we present SCAN, a novel sparse cross-scale attention network to first
align multi-scale sparse features with global voxel-encoded attention to
capture the long-range relationship of instance context, which can boost the
regression accuracy of the over-segmented large objects. For the
surface-aggregated points, SCAN adopts a novel sparse class-agnostic
representation of instance centroids, which can not only maintain the sparsity
of aligned features to solve the under-segmentation on small objects, but also
reduce the computation amount of the network through sparse convolution. Our
method outperforms previous methods by a large margin in the SemanticKITTI
dataset for the challenging 3D PS task, achieving 1st place with a real-time
inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Object-level Topological Semantic Mapping and Long-term Global Localization based on Graph Matching. (arXiv:2201.05977v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05977">
<div class="article-summary-box-inner">
<span><p>Mapping and localization are two essential tasks for mobile robots in
real-world applications. However, largescale and dynamic scenes challenge the
accuracy and robustness of most current mature solutions. This situation
becomes even worse when computational resources are limited. In this paper, we
present a novel lightweight object-level mapping and localization method with
high accuracy and robustness. Different from previous methods, our method does
not need a prior constructed precise geometric map, which greatly releases the
storage burden, especially for large-scale navigation. We use object-level
features with both semantic and geometric information to model landmarks in the
environment. Particularly, a learning topological primitive is first proposed
to efficiently obtain and organize the object-level landmarks. On the basis of
this, we use a robot-centric mapping framework to represent the environment as
a semantic topology graph and relax the burden of maintaining global
consistency at the same time. Besides, a hierarchical memory management
mechanism is introduced to improve the efficiency of online mapping with
limited computational resources. Based on the proposed map, the robust
localization is achieved by constructing a novel local semantic scene graph
descriptor, and performing multi-constraint graph matching to compare scene
similarity. Finally, we test our method on a low-cost embedded platform to
demonstrate its advantages. Experimental results on a large scale and
multi-session real-world environment show that the proposed method outperforms
the state of arts in terms of lightweight and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels. (arXiv:2201.05986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05986">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a dynamic convolution kernel (DCK) strategy for
convolutional neural networks. Using a fully convolutional network with the
proposed DCKs, high-quality talking-face video can be generated from
multi-modal sources (i.e., unmatched audio and video) in real time, and our
trained model is robust to different identities, head postures, and input
audios. Our proposed DCKs are specially designed for audio-driven talking face
video generation, leading to a simple yet effective end-to-end system. We also
provide a theoretical analysis to interpret why DCKs work. Experimental results
show that our method can generate high-quality talking-face video with
background at 60 fps. Comparison and evaluation between our method and the
state-of-the-art methods demonstrate the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. (arXiv:2201.05989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05989">
<div class="article-summary-box-inner">
<span><p>Neural graphics primitives, parameterized by fully connected neural networks,
can be costly to train and evaluate. We reduce this cost with a versatile new
input encoding that permits the use of a smaller network without sacrificing
quality, thus significantly reducing the number of floating point and memory
access operations: a small neural network is augmented by a multiresolution
hash table of trainable feature vectors whose values are optimized through
stochastic gradient descent. The multiresolution structure allows the network
to disambiguate hash collisions, making for a simple architecture that is
trivial to parallelize on modern GPUs. We leverage this parallelism by
implementing the whole system using fully-fused CUDA kernels with a focus on
minimizing wasted bandwidth and compute operations. We achieve a combined
speedup of several orders of magnitude, enabling training of high-quality
neural graphics primitives in a matter of seconds, and rendering in tens of
milliseconds at a resolution of ${1920\!\times\!1080}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Transformers: A Survey. (arXiv:2201.05991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05991">
<div class="article-summary-box-inner">
<span><p>Transformer models have shown great success modeling long-range interactions.
Nevertheless, they scale quadratically with input length and lack inductive
biases. These limitations can be further exacerbated when dealing with the high
dimensionality of video. Proper modeling of video, which can span from seconds
to hours, requires handling long-range interactions. This makes Transformers a
promising tool for solving video related tasks, but some adaptations are
required. While there are previous works that study the advances of
Transformers for vision tasks, there is none that focus on in-depth analysis of
video-specific designs. In this survey we analyse and summarize the main
contributions and trends for adapting Transformers to model video data.
Specifically, we delve into how videos are embedded and tokenized, finding a
very widspread use of large CNN backbones to reduce dimensionality and a
predominance of patches and frames as tokens. Furthermore, we study how the
Transformer layer has been tweaked to handle longer sequences, generally by
reducing the number of tokens in single attention operation. Also, we analyse
the self-supervised losses used to train Video Transformers, which to date are
mostly constrained to contrastive approaches. Finally, we explore how other
modalities are integrated with video and conduct a performance comparison on
the most common benchmark for Video Transformers (i.e., action classification),
finding them to outperform 3D CNN counterparts with equivalent FLOPs and no
significant parameter increase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hardware Implementation of Multimodal Biometric using Fingerprint and Iris. (arXiv:2201.05996v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05996">
<div class="article-summary-box-inner">
<span><p>In this paper, a hardware architecture of a multimodal biometric system is
presented that massively exploits the inherent parallelism. The proposed system
is based on multiple biometric fusion that uses two biometric traits,
fingerprint and iris. Each biometric trait is first optimised at the software
level, by addressing some of the issues that directly affect the FAR and FRR.
Then the hardware architectures for both biometric traits are presented,
followed by a final multimodal hardware architecture. To the best of the
author's knowledge, no other FPGA-based design exits that used these two
traits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines. (arXiv:1804.06579v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.06579">
<div class="article-summary-box-inner">
<span><p>We present a semi-supervised co-analysis method for learning 3D shape styles
from projected feature lines, achieving style patch localization with only weak
supervision. Given a collection of 3D shapes spanning multiple object
categories and styles, we perform style co-analysis over projected feature
lines of each 3D shape and then backproject the learned style features onto the
3D shapes. Our core analysis pipeline starts with mid-level patch sampling and
pre-selection of candidate style patches. Projective features are then encoded
via patch convolution. Multi-view feature integration and style clustering are
carried out under the framework of partially shared latent factor (PSLF)
learning, a multi-view feature learning scheme. PSLF achieves effective
multi-view feature fusion by distilling and exploiting consistent and
complementary feature information from multiple views, while also selecting
style patches from the candidates. Our style analysis approach supports both
unsupervised and semi-supervised analysis. For the latter, our method accepts
both user-specified shape labels and style-ranked triplets as clustering
constraints.We demonstrate results from 3D shape style analysis and patch
localization as well as improvements over state-of-the-art methods. We also
present several applications enabled by our style analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Generalization Ability of Convolutional Neural Networks and Capsule Networks for Image Classification via Top-2 Classification. (arXiv:1901.10112v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1901.10112">
<div class="article-summary-box-inner">
<span><p>Image classification is a challenging problem which aims to identify the
category of object in the image. In recent years, deep Convolutional Neural
Networks (CNNs) have been applied to handle this task, and impressive
improvement has been achieved. However, some research showed the output of CNNs
can be easily altered by adding relatively small perturbations to the input
image, such as modifying few pixels. Recently, Capsule Networks (CapsNets) are
proposed, which can help eliminating this limitation. Experiments on MNIST
dataset revealed that capsules can better characterize the features of object
than CNNs. But it's hard to find a suitable quantitative method to compare the
generalization ability of CNNs and CapsNets. In this paper, we propose a new
image classification task called Top-2 classification to evaluate the
generalization ability of CNNs and CapsNets. The models are trained on single
label image samples same as the traditional image classification task. But in
the test stage, we randomly concatenate two test image samples which contain
different labels, and then use the trained models to predict the top-2 labels
on the unseen newly-created two label image samples. This task can provide us
precise quantitative results to compare the generalization ability of CNNs and
CapsNets. Back to the CapsNet, because it uses Full Connectivity (FC) mechanism
among all capsules, it requires many parameters. To reduce the number of
parameters, we introduce the Parameter-Sharing (PS) mechanism between capsules.
Experiments on five widely used benchmark image datasets demonstrate the method
significantly reduces the number of parameters, without losing the
effectiveness of extracting features. Further, on the Top-2 classification
task, the proposed PS CapsNets obtain impressive higher accuracy compared to
the traditional CNNs and FC CapsNets by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PartNet: A Recursive Part Decomposition Network for Fine-grained and Hierarchical Shape Segmentation. (arXiv:1903.00709v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.00709">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches to 3D shape segmentation are typically formulated as
a multi-class labeling problem. Existing models are trained for a fixed set of
labels, which greatly limits their flexibility and adaptivity. We opt for
top-down recursive decomposition and develop the first deep learning model for
hierarchical segmentation of 3D shapes, based on recursive neural networks.
Starting from a full shape represented as a point cloud, our model performs
recursive binary decomposition, where the decomposition network at all nodes in
the hierarchy share weights. At each node, a node classifier is trained to
determine the type (adjacency or symmetry) and stopping criteria of its
decomposition. The features extracted in higher level nodes are recursively
propagated to lower level ones. Thus, the meaningful decompositions in higher
levels provide strong contextual cues constraining the segmentations in lower
levels. Meanwhile, to increase the segmentation accuracy at each node, we
enhance the recursive contextual feature with the shape feature extracted for
the corresponding part. Our method segments a 3D shape in point cloud into an
unfixed number of parts, depending on the shape complexity, showing strong
generality and flexibility. It achieves the state-of-the-art performance, both
for fine-grained and semantic segmentation, on the public benchmark and a new
benchmark of fine-grained segmentation proposed in this work. We also
demonstrate its application for fine-grained part refinements in image-to-shape
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Discourse Parsing. (arXiv:1903.02252v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.02252">
<div class="article-summary-box-inner">
<span><p>Text-level discourse parsing aims to unmask how two segments (or sentences)
in the text are related to each other. We propose the task of Visual Discourse
Parsing, which requires understanding discourse relations among scenes in a
video. Here we use the term scene to refer to a subset of video frames that can
better summarize the video. In order to collect a dataset for learning
discourse cues from videos, one needs to manually identify the scenes from a
large pool of video frames and then annotate the discourse relations between
them. This is clearly a time consuming, expensive and tedious task. In this
work, we propose an approach to identify discourse cues from the videos without
the need to explicitly identify and annotate the scenes. We also present a
novel dataset containing 310 videos and the corresponding discourse cues to
evaluate our approach. We believe that many of the multi-discipline Artificial
Intelligence problems such as Visual Dialog and Visual Storytelling would
greatly benefit from the use of visual discourse cues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAMMA: A General Agent Motion Model for Autonomous Driving. (arXiv:1906.01566v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.01566">
<div class="article-summary-box-inner">
<span><p>This paper presents GAMMA, a general motion prediction model that enables
large-scale real-time simulation and planning for autonomous driving. GAMMA
models heterogeneous, interactive traffic agents. They operate under diverse
road conditions, with various geometric and kinematic constraints. GAMMA treats
the prediction task as constrained optimization in traffic agents' velocity
space. The objective is to optimize an agent's driving performance, while
obeying all the constraints resulting from the agent's kinematics, collision
avoidance with other agents, and the environmental context. Further, GAMMA
explicitly conditions the prediction on human behavioral states as parameters
of the optimization model, in order to account for versatile human behaviors.
We evaluated GAMMA on a set of real-world benchmark datasets. The results show
that GAMMA achieves high prediction accuracy on both homogeneous and
heterogeneous traffic datasets, with sub-millisecond execution time. Further,
the computational efficiency and the flexibility of GAMMA enable (i) simulation
of mixed urban traffic at many locations worldwide and (ii) planning for
autonomous driving in dense traffic with uncertain driver behaviors, both in
real-time. The open-source code of GAMMA is available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Guided Single Image Reflection Removal. (arXiv:1907.11912v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.11912">
<div class="article-summary-box-inner">
<span><p>Reflection is common in images capturing scenes behind a glass window, which
is not only a disturbance visually but also influence the performance of other
computer vision algorithms. Single image reflection removal is an ill-posed
problem because the color at each pixel needs to be separated into two values,
i.e., the desired clear background and the reflection. To solve it, existing
methods propose priors such as smoothness, color consistency. However, the
low-level priors are not reliable in complex scenes, for instance, when
capturing a real outdoor scene through a window, both the foreground and
background contain both smooth and sharp area and a variety of color. In this
paper, inspired by the fact that human can separate the two layers easily by
recognizing the objects, we use the object semantic as guidance to force the
same semantic object belong to the same layer. Extensive experiments on
different datasets show that adding the semantic information offers a
significant improvement to reflection separation. We also demonstrate the
applications of the proposed method to other computer vision tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Scale Open-Set Deep Logo Detection. (arXiv:1911.07440v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.07440">
<div class="article-summary-box-inner">
<span><p>We present an open-set logo detection (OSLD) system, which can detect
(localize and recognize) any number of unseen logo classes without re-training;
it only requires a small set of canonical logo images for each logo class. We
achieve this using a two-stage approach: (1) Generic logo detection to detect
candidate logo regions in an image. (2) Logo matching for matching the detected
logo regions to a set of canonical logo images to recognize them.
</p>
<p>We constructed an open-set logo detection dataset with 12.1k logo classes and
released it for research purposes.We demonstrate the effectiveness of OSLD on
our dataset and on the standard Flickr-32 logo dataset, outperforming the
state-of-the-art open-set and closed-set logo detection methods by a large
margin. OSLD is scalable to millions of logo classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Posterior Adaptation With New Priors. (arXiv:2007.01386v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01386">
<div class="article-summary-box-inner">
<span><p>Classification approaches based on the direct estimation and analysis of
posterior probabilities will degrade if the original class priors begin to
change. We prove that a unique (up to scale) solution is possible to recover
the data likelihoods for a test example from its original class posteriors and
dataset priors. Given the recovered likelihoods and a set of new priors, the
posteriors can be re-computed using Bayes' Rule to reflect the influence of the
new priors. The method is simple to compute and allows a dynamic update of the
original posteriors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weight-dependent Gates for Network Pruning. (arXiv:2007.02066v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02066">
<div class="article-summary-box-inner">
<span><p>In this paper, a simple yet effective network pruning framework is proposed
to simultaneously address the problems of pruning indicator, pruning ratio, and
efficiency constraint. This paper argues that the pruning decision should
depend on the convolutional weights, and thus proposes novel weight-dependent
gates (W-Gates) to learn the information from filter weights and obtain binary
gates to prune or keep the filters automatically. To prune the network under
efficiency constraints, a switchable Efficiency Module is constructed to
predict the hardware latency or FLOPs of candidate pruned networks. Combined
with the proposed Efficiency Module, W-Gates can perform filter pruning in an
efficiency-aware manner and achieve a compact network with a better
accuracy-efficiency trade-off. We have demonstrated the effectiveness of the
proposed method on ResNet34, ResNet50, and MobileNet V2, respectively achieving
up to 1.33/1.28/1.1 higher Top-1 accuracy with lower hardware latency on
ImageNet. Compared with state-of-the-art methods, W-Gates also achieves
superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Contrastive Motion Learning for Video Action Recognition. (arXiv:2007.10321v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.10321">
<div class="article-summary-box-inner">
<span><p>One central question for video action recognition is how to model motion. In
this paper, we present hierarchical contrastive motion learning, a new
self-supervised learning framework to extract effective motion representations
from raw video frames. Our approach progressively learns a hierarchy of motion
features that correspond to different abstraction levels in a network. This
hierarchical design bridges the semantic gap between low-level motion cues and
high-level recognition tasks, and promotes the fusion of appearance and motion
information at multiple levels. At each level, an explicit motion
self-supervision is provided via contrastive learning to enforce the motion
features at the current level to predict the future ones at the previous level.
Thus, the motion features at higher levels are trained to gradually capture
semantic dynamics and evolve more discriminative for action recognition. Our
motion learning module is lightweight and flexible to be embedded into various
backbone networks. Extensive experiments on four benchmarks show that the
proposed approach consistently achieves superior results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v7 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08170">
<div class="article-summary-box-inner">
<span><p>In the paper, we propose a class of accelerated zeroth-order and first-order
momentum methods for both nonconvex mini-optimization and minimax-optimization.
Specifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)
method for black-box mini-optimization where only function values can be
obtained. Moreover, we prove that our Acc-ZOM method achieves a lower query
complexity of $\tilde{O}(d^{3/4}\epsilon^{-3})$ for finding an
$\epsilon$-stationary point, which improves the best known result by a factor
of $O(d^{1/4})$ where $d$ denotes the variable dimension. In particular, our
Acc-ZOM does not need large batches required in the existing zeroth-order
stochastic algorithms. Meanwhile, we propose an accelerated zeroth-order
momentum descent ascent (Acc-ZOMDA) method for black-box minimax optimization,
where only function values can be obtained. Our Acc-ZOMDA obtains a low query
complexity of $\tilde{O}((d_1+d_2)^{3/4}\kappa_y^{4.5}\epsilon^{-3})$ without
requiring large batches for finding an $\epsilon$-stationary point, where $d_1$
and $d_2$ denote variable dimensions and $\kappa_y$ is condition number.
Moreover, we propose an accelerated first-order momentum descent ascent
(Acc-MDA) method for minimax optimization, whose explicit gradients are
accessible. Our Acc-MDA achieves a low gradient complexity of
$\tilde{O}(\kappa_y^{4.5}\epsilon^{-3})$ without requiring large batches for
finding an $\epsilon$-stationary point. In particular, our Acc-MDA can obtain a
lower gradient complexity of $\tilde{O}(\kappa_y^{2.5}\epsilon^{-3})$ with a
batch size $O(\kappa_y^4)$, which improves the best known result by a factor of
$O(\kappa_y^{1/2})$. Extensive experimental results on black-box adversarial
attack to deep neural networks and poisoning attack to logistic regression
demonstrate efficiency of our algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inspecting state of the art performance and NLP metrics in image-based medical report generation. (arXiv:2011.09257v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09257">
<div class="article-summary-box-inner">
<span><p>Several deep learning architectures have been proposed over the last years to
deal with the problem of generating a written report given an imaging exam as
input. Most works evaluate the generated reports using standard Natural
Language Processing (NLP) metrics (e.g. BLEU, ROUGE), reporting significant
progress. In this article, we contrast this progress by comparing state of the
art (SOTA) models against weak baselines. We show that simple and even naive
approaches yield near SOTA performance on most traditional NLP metrics. We
conclude that evaluation methods in this task should be further studied towards
correctly measuring clinical accuracy, ideally involving physicians to
contribute to this end.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical VAEs Know What They Don't Know. (arXiv:2102.08248v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08248">
<div class="article-summary-box-inner">
<span><p>Deep generative models have been demonstrated as state-of-the-art density
estimators. Yet, recent work has found that they often assign a higher
likelihood to data from outside the training distribution. This seemingly
paradoxical behavior has caused concerns over the quality of the attained
density estimates. In the context of hierarchical variational autoencoders, we
provide evidence to explain this behavior by out-of-distribution data having
in-distribution low-level features. We argue that this is both expected and
desirable behavior. With this insight in hand, we develop a fast, scalable and
fully unsupervised likelihood-ratio score for OOD detection that requires data
to be in-distribution across all feature-levels. We benchmark the method on a
vast set of data and model combinations and achieve state-of-the-art results on
out-of-distribution detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Margin Preserving Self-paced Contrastive Learning Towards Domain Adaptation for Medical Image Segmentation. (arXiv:2103.08454v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08454">
<div class="article-summary-box-inner">
<span><p>To bridge the gap between the source and target domains in unsupervised
domain adaptation (UDA), the most common strategy puts focus on matching the
marginal distributions in the feature space through adversarial learning.
However, such category-agnostic global alignment lacks of exploiting the
class-level joint distributions, causing the aligned distribution less
discriminative. To address this issue, we propose in this paper a novel margin
preserving self-paced contrastive Learning (MPSCL) model for cross-modal
medical image segmentation. Unlike the conventional construction of contrastive
pairs in contrastive learning, the domain-adaptive category prototypes are
utilized to constitute the positive and negative sample pairs. With the
guidance of progressively refined semantic prototypes, a novel margin
preserving contrastive loss is proposed to boost the discriminability of
embedded representation space. To enhance the supervision for contrastive
learning, more informative pseudo-labels are generated in target domain in a
self-paced way, thus benefiting the category-aware distribution alignment for
UDA. Furthermore, the domain-invariant representations are learned through
joint contrastive learning between the two domains. Extensive experiments on
cross-modal cardiac segmentation tasks demonstrate that MPSCL significantly
improves semantic segmentation performance, and outperforms a wide variety of
state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stepwise Goal-Driven Networks for Trajectory Prediction. (arXiv:2103.14107v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14107">
<div class="article-summary-box-inner">
<span><p>We propose to predict the future trajectories of observed agents (e.g.,
pedestrians or vehicles) by estimating and using their goals at multiple time
scales. We argue that the goal of a moving agent may change over time, and
modeling goals continuously provides more accurate and detailed information for
future trajectory estimation. In this paper, we present a novel recurrent
network for trajectory prediction, called Stepwise Goal-Driven Network (SGNet).
Unlike prior work that models only a single, long-term goal, SGNet estimates
and uses goals at multiple temporal scales. In particular, the framework
incorporates an encoder module that captures historical information, a stepwise
goal estimator that predicts successive goals into the future, and a decoder
module that predicts future trajectory. We evaluate our model on three
first-person traffic datasets (HEV-I, JAAD, and PIE) as well as on two bird's
eye view datasets (ETH and UCY), and show that our model outperforms the
state-of-the-art methods in terms of both average and final displacement errors
on all datasets. Code has been made available at:
https://github.com/ChuhuaW/SGNet.pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Generalization in Deep Neural Networks via Sparsity. (arXiv:2104.00851v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00851">
<div class="article-summary-box-inner">
<span><p>Generalization is the key capability for deep neural networks (DNNs).
However, it is challenging to give a reliable measure of the generalization
ability of a DNN via only its nature. In this paper, we propose a novel method
for estimating the generalization gap based on network sparsity. In our method,
two key quantities are proposed first. They have close relationship with the
generalization ability and can be calculated directly from the training results
alone. Then a simple linear model involving two key quantities are constructed
to give accurate estimation of the generalization gap. By training DNNs with a
wide range of generalization gap on popular datasets, we show that our key
quantities and linear model could be efficient tools for estimating the
generalization gap of DNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Meta-Network for Blind Super-Resolution with Multiple Degradations. (arXiv:2104.03926v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03926">
<div class="article-summary-box-inner">
<span><p>Although single-image super-resolution (SISR) methods have achieved great
success on single degradation, they still suffer performance drop with multiple
degrading effects in real scenarios. Recently, some blind and non-blind models
for multiple degradations have been explored. However, those methods usually
degrade significantly for distribution shifts between the training and test
data. Towards this end, we propose a conditional meta-network framework (named
CMDSR) for the first time, which helps SR framework learn how to adapt to
changes in input distribution. We extract degradation prior at task-level with
the proposed ConditionNet, which will be used to adapt the parameters of the
basic SR network (BaseNet). Specifically, the ConditionNet of our framework
first learns the degradation prior from a support set, which is composed of a
series of degraded image patches from the same task. Then the adaptive BaseNet
rapidly shifts its parameters according to the conditional features. Moreover,
in order to better extract degradation prior, we propose a task contrastive
loss to decrease the inner-task distance and increase the cross-task distance
between task-level features. Without predefining degradation maps, our blind
framework can conduct one single parameter update to yield considerable SR
results. Extensive experiments demonstrate the effectiveness of CMDSR over
various blind, even non-blind methods. The flexible BaseNet structure also
reveals that CMDSR can be a general framework for large series of SISR models.
Our code is available at \url{https://github.com/guanghaoyin/CMDSR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Contrastive Learning for Visual Representation Learning. (arXiv:2104.12565v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12565">
<div class="article-summary-box-inner">
<span><p>We present a collaborative learning method called Mutual Contrastive Learning
(MCL) for general visual representation learning. The core idea of MCL is to
perform mutual interaction and transfer of contrastive distributions among a
cohort of networks. A crucial component of MCL is Interactive Contrastive
Learning (ICL). Compared with vanilla contrastive learning, ICL can aggregate
cross-network embedding information and maximize the lower bound to the mutual
information between two networks. This enables each network to learn extra
contrastive knowledge from others, leading to better feature representations
for visual recognition tasks. We emphasize that the resulting MCL is
conceptually simple yet empirically powerful. It is a generic framework that
can be applied to both supervised and self-supervised representation learning.
Experimental results on image classification and transfer learning to object
detection show that MCL can lead to consistent performance gains, demonstrating
that MCL can guide the network to generate better feature representations. Code
is available at https://github.com/winycg/MCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers. (arXiv:2105.08059v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08059">
<div class="article-summary-box-inner">
<span><p>Supervised reconstruction models are characteristically trained on matched
pairs of undersampled and fully-sampled data to capture an MRI prior, along
with supervision regarding the imaging operator to enforce data consistency. To
reduce supervision requirements, the recent deep image prior framework instead
conjoins untrained MRI priors with the imaging operator during inference. Yet,
canonical convolutional architectures are suboptimal in capturing long-range
relationships, and priors based on randomly initialized networks may yield
suboptimal performance. To address these limitations, here we introduce a novel
unsupervised MRI reconstruction method based on zero-Shot Learned Adversarial
TransformERs (SLATER). SLATER embodies a deep adversarial network with
cross-attention transformers to map noise and latent variables onto
coil-combined MR images. During pre-training, this unconditional network learns
a high-quality MRI prior in an unsupervised generative modeling task. During
inference, a zero-shot reconstruction is then performed by incorporating the
imaging operator and optimizing the prior to maximize consistency to
undersampled data. Comprehensive experiments on brain MRI datasets clearly
demonstrate the superior performance of SLATER against state-of-the-art
unsupervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on Self-Supervised Learning. (arXiv:2105.10123v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10123">
<div class="article-summary-box-inner">
<span><p>Large-scale unlabeled data has spurred recent progress in self-supervised
learning methods that learn rich visual representations. State-of-the-art
self-supervised methods for learning representations from images (e.g., MoCo,
BYOL, MSF) use an inductive bias that random augmentations (e.g., random crops)
of an image should produce similar embeddings. We show that such methods are
vulnerable to backdoor attacks - where an attacker poisons a small part of the
unlabeled data by adding a trigger (image patch chosen by the attacker) to the
images. The model performance is good on clean test images, but the attacker
can manipulate the decision of the model by showing the trigger at test time.
Backdoor attacks have been studied extensively in supervised learning and to
the best of our knowledge, we are the first to study them for self-supervised
learning. Backdoor attacks are more practical in self-supervised learning,
since the use of large unlabeled data makes data inspection to remove poisons
prohibitive. We show that in our targeted attack, the attacker can produce many
false positives for the target category by using the trigger at test time. We
also propose a knowledge distillation based defense algorithm that succeeds in
neutralizing the attack. Our code is available here:
https://github.com/UMBCvision/SSL-Backdoor .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training. (arXiv:2105.11333v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11333">
<div class="article-summary-box-inner">
<span><p>Recently a number of studies demonstrated impressive performance on diverse
vision-language multimodal tasks such as image captioning and visual question
answering by extending the self-attention based Transformer architecture with
multimodal pre-training objectives. Despite its huge potential, vision-language
multimodal pre-training in the medical domain has only recently received
attention, and only demonstrated improved diagnosis accuracy of vision-language
pre-trained models. In this work we explore a broad set of multimodal
representation learning tasks in the medical domain, specifically using
radiology images and the unstructured report. We propose a new model which
adopts a Transformer based architecture combined with a novel multimodal
attention masking scheme to maximize generalization performance for both
vision-language understanding task (e.g., diagnosis classification) and
vision-language generation task (e.g., radiology report generation). By
rigorously evaluating the proposed model on four downstream tasks with three
radiographic image-text datasets (MIMIC-CXR, Open-I, and VQA-RAD), we
empirically demonstrate the superior downstream task performance and generality
of our model against various baselines including task specific architectures.
In addition, we qualitatively analyze our model by showing the results of
retrieved image-report pairs, the attention map visualization, and generated
reports. Our proposed multimodal pre-training model could flexibly adapt to
multiple downstream tasks of vision-language understanding and generation with
a novel self-attention scheme. We believe that our approach can provide the
basis for a wide range of interpretations of vision-language multimodal in the
medical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stylizing 3D Scene via Implicit Representation and HyperNetwork. (arXiv:2105.13016v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13016">
<div class="article-summary-box-inner">
<span><p>In this work, we aim to address the 3D scene stylization problem - generating
stylized images of the scene at arbitrary novel view angles. A straightforward
solution is to combine existing novel view synthesis and image/video style
transfer approaches, which often leads to blurry results or inconsistent
appearance. Inspired by the high-quality results of the neural radiance fields
(NeRF) method, we propose a joint framework to directly render novel views with
the desired style. Our framework consists of two components: an implicit
representation of the 3D scene with the neural radiance fields model, and a
hypernetwork to transfer the style information into the scene representation.
In particular, our implicit representation model disentangles the scene into
the geometry and appearance branches, and the hypernetwork learns to predict
the parameters of the appearance branch from the reference style image. To
alleviate the training difficulties and memory burden, we propose a two-stage
training procedure and a patch sub-sampling approach to optimize the style and
content losses with the neural radiance fields model. After optimization, our
model is able to render consistent novel views at arbitrary view angles with
arbitrary style. Both quantitative evaluation and human subject study have
demonstrated that the proposed method generates faithful stylization results
with consistent appearance across different views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02634">
<div class="article-summary-box-inner">
<span><p>Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a single network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02701">
<div class="article-summary-box-inner">
<span><p>Recent advances in brain clearing and imaging have made it possible to image
entire mammalian brains at sub-micron resolution. These images offer the
potential to assemble brain-wide atlases of neuron morphology, but manual
neuron reconstruction remains a bottleneck. Several automatic reconstruction
algorithms exist, but most focus on single neuron images. In this paper, we
present a probabilistic reconstruction method, ViterBrain, which combines a
hidden Markov state process that encodes neuron geometry with a random field
appearance model of neuron fluorescence. Our method utilizes dynamic
programming to compute the global maximizers of what we call the "most
probable" neuron path. Our most probable estimation method models the task of
reconstructing neuronal processes in the presence of other neurons, and thus is
applicable in images with several neurons. Our method operates on image
segmentations in order to leverage cutting edge computer vision technology. We
applied our algorithm to imperfect image segmentations where false negatives
severed neuronal processes, and showed that it can follow axons in the presence
of noise or nearby neurons. Additionally, it creates a framework where users
can intervene to, for example, fit start and endpoints. The code used in this
work is available in our open-source Python package brainlit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence. (arXiv:2106.03743v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03743">
<div class="article-summary-box-inner">
<span><p>We investigate the reasons for the performance degradation incurred with
batch-independent normalization. We find that the prototypical techniques of
layer normalization and instance normalization both induce the appearance of
failure modes in the neural network's pre-activations: (i) layer normalization
induces a collapse towards channel-wise constant functions; (ii) instance
normalization induces a lack of variability in instance statistics, symptomatic
of an alteration of the expressivity. To alleviate failure mode (i) without
aggravating failure mode (ii), we introduce the technique "Proxy Normalization"
that normalizes post-activations using a proxy distribution. When combined with
layer normalization or group normalization, this batch-independent
normalization emulates batch normalization's behavior and consistently matches
or exceeds its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06927">
<div class="article-summary-box-inner">
<span><p>Despite unconditional feature inverters being the foundation of many
synthesis tasks, training them requires a large computational overhead,
decoding capacity or additional autoregressive priors. We propose to train an
adversarially robust encoder to learn disentangled and perceptually-aligned
bottleneck features, making them easily invertible. Then, by training a simple
generator with the mirror architecture of the encoder, we achieve superior
reconstructions and generalization over standard approaches. We exploit such
properties using an encoding-decoding network based on AR features and
demonstrate its oustanding performance on three applications: anomaly
detection, style transfer and image denoising. Comparisons against alternative
learn-based methods show that our model attains improved performance with
significantly less training parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Modular Should Neural Module Networks Be for Systematic Generalization?. (arXiv:2106.08170v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08170">
<div class="article-summary-box-inner">
<span><p>Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via
composition of modules that tackle a sub-task. NMNs are a promising strategy to
achieve systematic generalization, i.e., overcoming biasing factors in the
training distribution. However, the aspects of NMNs that facilitate systematic
generalization are not fully understood. In this paper, we demonstrate that the
degree of modularity of the NMN have large influence on systematic
generalization. In a series of experiments on three VQA datasets (VQA-MNIST,
SQOOP, and CLEVR-CoGenT), our results reveal that tuning the degree of
modularity, especially at the image encoder stage, reaches substantially higher
systematic generalization. These findings lead to new NMN architectures that
outperform previous ones in terms of systematic generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised GANs with Label Augmentation. (arXiv:2106.08601v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08601">
<div class="article-summary-box-inner">
<span><p>Recently, transformation-based self-supervised learning has been applied to
generative adversarial networks (GANs) to mitigate catastrophic forgetting in
the discriminator by introducing a stationary learning environment. However,
the separate self-supervised tasks in existing self-supervised GANs cause a
goal inconsistent with generative modeling due to the fact that their
self-supervised classifiers are agnostic to the generator distribution. To
address this problem, we propose a novel self-supervised GAN that unifies the
GAN task with the self-supervised task by augmenting the GAN labels (real or
fake) via self-supervision of data transformation. Specifically, the original
discriminator and self-supervised classifier are unified into a label-augmented
discriminator that predicts the augmented labels to be aware of both the
generator distribution and the data distribution under every transformation,
and then provide the discrepancy between them to optimize the generator.
Theoretically, we prove that the optimal generator could converge to replicate
the real data distribution. Empirically, we show that the proposed method
significantly outperforms previous self-supervised and data augmentation GANs
on both generative modeling and representation learning across benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00710">
<div class="article-summary-box-inner">
<span><p>Manic episodes of bipolar disorder can lead to uncritical behaviour and
delusional psychosis, often with destructive consequences for those affected
and their surroundings. Early detection and intervention of a manic episode are
crucial to prevent escalation, hospital admission and premature death. However,
people with bipolar disorder may not recognize that they are experiencing a
manic episode and symptoms such as euphoria and increased productivity can also
deter affected individuals from seeking help. This work proposes to perform
user-independent, automatic mood-state detection based on actigraphy and
electrodermal activity acquired from a wrist-worn device during mania and after
recovery (euthymia). This paper proposes a new deep learning-based ensemble
method leveraging long (20h) and short (5 minutes) time-intervals to
discriminate between the mood-states. When tested on 47 bipolar patients, the
proposed classification scheme achieves an average accuracy of 91.59% in
euthymic/manic mood-state recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05445">
<div class="article-summary-box-inner">
<span><p>Humans are incredibly good at transferring knowledge from one domain to
another, enabling rapid learning of new tasks. Likewise, transfer learning has
enabled enormous success in many computer vision problems using pretraining.
However, the benefits of transfer in multi-domain learning, where a network
learns multiple tasks defined by different datasets, has not been adequately
studied. Learning multiple domains could be beneficial, or these domains could
interfere with each other given limited network capacity. Understanding how
deep neural networks of varied capacity facilitate transfer across inputs from
different distributions is a critical step towards open world learning. In this
work, we decipher the conditions where interference and knowledge transfer
occur in multi-domain learning. We propose new metrics disentangling
interference and transfer, set up experimental protocols, and examine the roles
of network capacity, task grouping, and dynamic loss weighting in reducing
interference and facilitating transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CentripetalText: An Efficient Text Instance Representation for Scene Text Detection. (arXiv:2107.05945v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05945">
<div class="article-summary-box-inner">
<span><p>Scene text detection remains a grand challenge due to the variation in text
curvatures, orientations, and aspect ratios. One of the hardest problems in
this task is how to represent text instances of arbitrary shapes. Although many
methods have been proposed to model irregular texts in a flexible manner, most
of them lose simplicity and robustness. Their complicated post-processings and
the regression under Dirac delta distribution undermine the detection
performance and the generalization ability. In this paper, we propose an
efficient text instance representation named CentripetalText (CT), which
decomposes text instances into the combination of text kernels and centripetal
shifts. Specifically, we utilize the centripetal shifts to implement pixel
aggregation, guiding the external text pixels to the internal text kernels. The
relaxation operation is integrated into the dense regression for centripetal
shifts, allowing the correct prediction in a range instead of a specific value.
The convenient reconstruction of text contours and the tolerance of prediction
errors in our method guarantee the high detection accuracy and the fast
inference speed, respectively. Besides, we shrink our text detector into a
proposal generation module, namely CentripetalText Proposal Network, replacing
Segmentation Proposal Network in Mask TextSpotter v3 and producing more
accurate proposals. To validate the effectiveness of our method, we conduct
experiments on several commonly used scene text benchmarks, including both
curved and multi-oriented text datasets. For the task of scene text detection,
our approach achieves superior or competitive performance compared to other
existing methods, e.g., F-measure of 86.3% at 40.0 FPS on Total-Text, F-measure
of 86.1% at 34.8 FPS on MSRA-TD500, etc. For the task of end-to-end scene text
recognition, our method outperforms Mask TextSpotter v3 by 1.1% on Total-Text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Designing Good Representation Learning Models. (arXiv:2107.05948v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05948">
<div class="article-summary-box-inner">
<span><p>The goal of representation learning is different from the ultimate objective
of machine learning such as decision making, it is therefore very difficult to
establish clear and direct objectives for training representation learning
models. It has been argued that a good representation should disentangle the
underlying variation factors, yet how to translate this into training
objectives remains unknown. This paper presents an attempt to establish direct
training criterions and design principles for developing good representation
learning models. We propose that a good representation learning model should be
maximally expressive, i.e., capable of distinguishing the maximum number of
input configurations. We formally define expressiveness and introduce the
maximum expressiveness (MEXS) theorem of a general learning model. We propose
to train a model by maximizing its expressiveness while at the same time
incorporating general priors such as model smoothness. We present a conscience
competitive learning algorithm which encourages the model to reach its MEXS
whilst at the same time adheres to model smoothness prior. We also introduce a
label consistent training (LCT) technique to boost model smoothness by
encouraging it to assign consistent labels to similar samples. We present
extensive experimental results to show that our method can indeed design
representation learning models capable of developing representations that are
as good as or better than state of the art. We also show that our technique is
computationally efficient, robust against different parameter settings and can
work effectively on a variety of datasets. Code available at
https://github.com/qlilx/odgrlm.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-Guided NIR Image Colorization via Adaptive Fusion of Semantic and Texture Clues. (arXiv:2107.09237v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09237">
<div class="article-summary-box-inner">
<span><p>Near infrared (NIR) imaging has been widely applied in low-light imaging
scenarios; however, it is difficult for human and algorithms to perceive the
real scene in the colorless NIR domain. While Generative Adversarial Network
(GAN) has been widely employed in various image colorization tasks, it is
challenging for a direct mapping mechanism, such as a conventional GAN, to
transform an image from the NIR to the RGB domain with correct semantic
reasoning, well-preserved textures, and vivid color combinations concurrently.
In this work, we propose a novel Attention-based NIR image colorization
framework via Adaptive Fusion of Semantic and Texture clues, aiming at
achieving these goals within the same framework. The tasks of texture transfer
and semantic reasoning are carried out in two separate network blocks.
Specifically, the Texture Transfer Block (TTB) aims at extracting texture
features from the NIR image's Laplacian component and transferring them for
subsequent color fusion. The Semantic Reasoning Block (SRB) extracts semantic
clues and maps the NIR pixel values to the RGB domain. Finally, a Fusion
Attention Block (FAB) is proposed to adaptively fuse the features from the two
branches and generate an optimized colorization result. In order to enhance the
network's learning capacity in semantic reasoning as well as mapping precision
in texture transfer, we have proposed the Residual Coordinate Attention Block
(RCAB), which incorporates coordinate attention into a residual learning
framework, enabling the network to capture long-range dependencies along the
channel direction and meanwhile precise positional information can be preserved
along spatial directions. RCAB is also incorporated into FAB to facilitate
accurate texture alignment during fusion. Both quantitative and qualitative
evaluations show that the proposed method outperforms state-of-the-art NIR
image colorization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularising Inverse Problems with Generative Machine Learning Models. (arXiv:2107.11191v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11191">
<div class="article-summary-box-inner">
<span><p>Deep neural network approaches to inverse imaging problems have produced
impressive results in the last few years. In this paper, we consider the use of
generative models in a variational regularisation approach to inverse problems.
The considered regularisers penalise images that are far from the range of a
generative model that has learned to produce images similar to a training
dataset. We name this family \textit{generative regularisers}. The success of
generative regularisers depends on the quality of the generative model and so
we propose a set of desired criteria to assess models and guide future
research. In our numerical experiments, we evaluate three common generative
models, autoencoders, variational autoencoders and generative adversarial
networks, against our desired criteria. We also test three different generative
regularisers on the inverse problems of deblurring, deconvolution, and
tomography. We show that the success of solutions restricted to lie exactly in
the range of the generator is highly dependent on the ability of the generative
model but that allowing small deviations from the range of the generator
produces more consistent results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00166">
<div class="article-summary-box-inner">
<span><p>Micro-expressions are spontaneous, unconscious facial movements that show
people's true inner emotions and have great potential in related fields of
psychological testing. Since the face is a 3D deformation object, the
occurrence of an expression can arouse spatial deformation of the face, but
limited by the available databases are 2D videos, lacking the description of 3D
spatial information of micro-expressions. Therefore, we proposed a new
micro-expression database containing 2D video sequences and 3D point clouds
sequences. The database includes 373 micro-expressions sequences, and these
samples were classified using the objective method based on facial action
coding system, as well as the non-objective method that combines video contents
and participants' self-reports. We extracted 2D and 3D features using the local
binary patterns on three orthogonal planes (LBP-TOP) and curvature algorithms,
respectively, and evaluated the classification accuracies of these two features
and their fusion results with leave-one-subject-out (LOSO) and 10-fold
cross-validation. Further, we performed various neural network algorithms for
database classification, the results show that classification accuracies are
improved by fusing 3D features than using only 2D features. The database offers
original and cropped micro-expression samples, which will facilitate the
exploration and research on 3D Spatio-temporal features of micro-expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inference via Sparse Coding in a Hierarchical Vision Model. (arXiv:2108.01548v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01548">
<div class="article-summary-box-inner">
<span><p>Sparse coding has been incorporated in models of the visual cortex for its
computational advantages and connection to biology. But how the level of
sparsity contributes to performance on visual tasks is not well understood. In
this work, sparse coding has been integrated into an existing hierarchical V2
model (Hosoya and Hyv\"arinen, 2015), but replacing its independent component
analysis (ICA) with an explicit sparse coding in which the degree of sparsity
can be controlled. After training, the sparse coding basis functions with a
higher degree of sparsity resembled qualitatively different structures, such as
curves and corners. The contributions of the models were assessed with image
classification tasks, specifically tasks associated with mid-level vision
including figure-ground classification, texture classification, and angle
prediction between two line stimuli. In addition, the models were assessed in
comparison to a texture sensitivity measure that has been reported in V2
(Freeman et al., 2013), and a deleted-region inference task. The results from
the experiments show that while sparse coding performed worse than ICA at
classifying images, only sparse coding was able to better match the texture
sensitivity level of V2 and infer deleted image regions, both by increasing the
degree of sparsity in sparse coding. Higher degrees of sparsity allowed for
inference over larger deleted image regions. The mechanism that allows for this
inference capability in sparse coding is described here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uniform Sampling over Episode Difficulty. (arXiv:2108.01662v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01662">
<div class="article-summary-box-inner">
<span><p>Episodic training is a core ingredient of few-shot learning to train models
on tasks with limited labelled data. Despite its success, episodic training
remains largely understudied, prompting us to ask the question: what is the
best way to sample episodes? In this paper, we first propose a method to
approximate episode sampling distributions based on their difficulty. Building
on this method, we perform an extensive analysis and find that sampling
uniformly over episode difficulty outperforms other sampling schemes, including
curriculum and easy-/hard-mining. As the proposed sampling method is algorithm
agnostic, we can leverage these insights to improve few-shot learning
accuracies across many episodic training algorithms. We demonstrate the
efficacy of our method across popular few-shot learning datasets, algorithms,
network architectures, and protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-repository of screening mammography classifiers. (arXiv:2108.04800v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04800">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) is showing promise in improving clinical
diagnosis. In breast cancer screening, recent studies show that AI has the
potential to improve early cancer diagnosis and reduce unnecessary workup. As
the number of proposed models and their complexity grows, it is becoming
increasingly difficult to re-implement them. To enable reproducibility of
research and to enable comparison between different methods, we release a
meta-repository containing models for classification of screening mammograms.
This meta-repository creates a framework that enables the evaluation of AI
models on any screening mammography data set. At its inception, our
meta-repository contains five state-of-the-art models with open-source
implementations and cross-platform compatibility. We compare their performance
on seven international data sets. Our framework has a flexible design that can
be generalized to other medical image analysis tasks. The meta-repository is
available at https://www.github.com/nyukat/mammography_metarepository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning vs XNOR-Net: A Comprehensive Study of Deep Learning for Audio Classification on Edge-devices. (arXiv:2108.06128v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06128">
<div class="article-summary-box-inner">
<span><p>Deep learning has celebrated resounding successes in many application areas
of relevance to the Internet of Things (IoT), such as computer vision and
machine listening. These technologies must ultimately be brought directly to
the edge to fully harness the power of deep learning for the IoT. The obvious
challenge is that deep learning techniques can only be implemented on strictly
resource-constrained edge devices if the models are radically downsized. This
task relies on different model compression techniques, such as network pruning,
quantization, and the recent advancement of XNOR-Net. This study examines the
suitability of these techniques for audio classification on microcontrollers.
We present an application of XNOR-Net for end-to-end raw audio classification
and a comprehensive empirical study comparing this approach with
pruning-and-quantization methods. We show that raw audio classification with
XNOR yields comparable performance to regular full precision networks for small
numbers of classes while reducing memory requirements 32-fold and computation
requirements 58-fold. However, as the number of classes increases
significantly, performance degrades, and pruning-and-quantization based
compression techniques take over as the preferred technique being able to
satisfy the same space constraints but requiring approximately 8x more
computation. We show that these insights are consistent between raw audio
classification and image classification using standard benchmark sets. To the
best of our knowledge, this is the first study to apply XNOR to end-to-end
audio classification and evaluate it in the context of alternative techniques.
All codes are publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06227">
<div class="article-summary-box-inner">
<span><p>Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, most existing
learning-based approaches usually suffer from limited manually annotated
medical data, which poses a major practical problem for accurate and robust
medical image segmentation. In addition, most existing semi-supervised
approaches are usually not robust compared with the supervised counterparts,
and also lack explicit modeling of geometric structure and semantic
information, both of which limit the segmentation accuracy. In this work, we
present SimCVD, a simple contrastive distillation framework that significantly
advances state-of-the-art voxel-wise representation learning. We first describe
an unsupervised training strategy, which takes two views of an input volume and
predicts their signed distance maps of object boundaries in a contrastive
objective, with only two independent dropout as mask. This simple approach
works surprisingly well, performing on the same level as previous fully
supervised methods with much less labeled data. We hypothesize that dropout can
be viewed as a minimal form of data augmentation and makes the network robust
to representation collapse. Then, we propose to perform structural distillation
by distilling pair-wise similarities. We evaluate SimCVD on two popular
datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT
dataset. The results on the LA dataset demonstrate that, in two types of
labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of
90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to
previous best results. Our method can be trained in an end-to-end fashion,
showing the promise of utilizing SimCVD as a general framework for downstream
tasks, such as medical image synthesis, enhancement, and registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-level Active Detector Learning. (arXiv:2108.09186v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09186">
<div class="article-summary-box-inner">
<span><p>Active learning for object detection is conventionally achieved by applying
techniques developed for classification in a way that aggregates individual
detections into image-level selection criteria. This is typically coupled with
the costly assumption that every image selected for labelling must be
exhaustively annotated. This yields incremental improvements on well-curated
vision datasets and struggles in the presence of data imbalance and visual
clutter that occurs in real-world imagery. Alternatives to the image-level
approach are surprisingly under-explored in the literature. In this work, we
introduce a new strategy that subsumes previous Image-level and Object-level
approaches into a generalized, Region-level approach that promotes
spatial-diversity by avoiding nearby redundant queries from the same image and
minimizes context-switching for the labeler. We show that this approach
significantly decreases labeling effort and improves rare object search on
realistic data with inherent class-imbalance and cluttered scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Self-Attention Network for Video Saliency Prediction. (arXiv:2108.10696v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10696">
<div class="article-summary-box-inner">
<span><p>3D convolutional neural networks have achieved promising results for video
tasks in computer vision, including video saliency prediction that is explored
in this paper. However, 3D convolution encodes visual representation merely on
fixed local spacetime according to its kernel size, while human attention is
always attracted by relational visual features at different time. To overcome
this limitation, we propose a novel Spatio-Temporal Self-Attention 3D Network
(STSANet) for video saliency prediction, in which multiple Spatio-Temporal
Self-Attention (STSA) modules are employed at different levels of 3D
convolutional backbone to directly capture long-range relations between
spatio-temporal features of different time steps. Besides, we propose an
Attentional Multi-Scale Fusion (AMSF) module to integrate multi-level features
with the perception of context in semantic and spatio-temporal subspaces.
Extensive experiments demonstrate the contributions of key components of our
method, and the results on DHF1K, Hollywood-2, UCF, and DIEM benchmark datasets
clearly prove the superiority of the proposed model compared with all
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BVMatch: Lidar-based Place Recognition Using Bird's-eye View Images. (arXiv:2109.00317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00317">
<div class="article-summary-box-inner">
<span><p>Recognizing places using Lidar in large-scale environments is challenging due
to the sparse nature of point cloud data. In this paper we present BVMatch, a
Lidar-based frame-to-frame place recognition framework, that is capable of
estimating 2D relative poses. Based on the assumption that the ground area can
be approximated as a plane, we uniformly discretize the ground area into grids
and project 3D Lidar scans to bird's-eye view (BV) images. We further use a
bank of Log-Gabor filters to build a maximum index map (MIM) that encodes the
orientation information of the structures in the images. We analyze the
orientation characteristics of MIM theoretically and introduce a novel
descriptor called bird's-eye view feature transform (BVFT). The proposed BVFT
is insensitive to rotation and intensity variations of BV images. Leveraging
the BVFT descriptors, we unify the Lidar place recognition and pose estimation
tasks into the BVMatch framework. The experiments conducted on three
large-scale datasets show that BVMatch outperforms the state-of-the-art methods
in terms of both recall rate of place recognition and pose estimation accuracy.
The source code of our method is publicly available at
https://github.com/zjuluolun/BVMatch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing domain adaptation techniques for mitosis detection in multi-scanner breast cancer histopathology images. (arXiv:2109.00869v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00869">
<div class="article-summary-box-inner">
<span><p>Breast cancer is the most commonly diagnosed cancer worldwide, with over two
million new cases each year. During diagnostic tumour grading, pathologists
manually count the number of dividing cells (mitotic figures) in biopsy or
tumour resection specimens. Since the process is subjective and time-consuming,
data-driven artificial intelligence (AI) methods have been developed to
automatically detect mitotic figures. However, these methods often generalise
poorly, with performance reduced by variations in tissue types, staining
protocols, or the scanners used to digitise whole-slide images. Domain
adaptation approaches have been adopted in various applications to mitigate
this issue of domain shift. We evaluate two unsupervised domain adaptation
methods, CycleGAN and Neural Style Transfer, using the MIDOG 2021 Challenge
dataset. This challenge focuses on detecting mitotic figures in whole-slide
images digitised using different scanners. Two baseline mitosis detection
models based on U-Net and RetinaNet were investigated in combination with the
aforementioned domain adaptation methods. Both baseline models achieved human
expert level performance, but had reduced performance when evaluated on images
which had been digitised using a different scanner. The domain adaptation
techniques were each found to be beneficial for detection with data from some
scanners but not for others, with the only average increase across all scanners
being achieved by CycleGAN on the RetinaNet detector. These techniques require
further refinement to ensure consistency in mitosis detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual 3D Scene Flow Learning with Context-Aware Feature Extraction. (arXiv:2109.04685v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04685">
<div class="article-summary-box-inner">
<span><p>Scene flow estimation is the task to predict the point-wise or pixel-wise 3D
displacement vector between two consecutive frames of point clouds or images,
which has important application in fields such as service robots and autonomous
driving. Although many previous works have explored greatly on scene flow
estimation based on point clouds, there are two problems that have not been
noticed or well solved before: 1) Points of adjacent frames in repetitive
patterns may be wrongly associated due to similar spatial structure in their
neighbourhoods; 2) Scene flow between adjacent frames of point clouds with
long-distance movement may be inaccurately estimated. To solve the first
problem, a novel context-aware set convolution layer is proposed in this paper
to exploit contextual structure information of Euclidean space and learn soft
aggregation weights for local point features. This design is inspired by human
perception of contextual structure information during scene understanding with
repetitive patterns. The context-aware set convolution layer is incorporated in
a context-aware point feature pyramid module of 3D point clouds for scene flow
estimation. For the second problem, an explicit residual flow learning
structure is proposed in the residual flow refinement layer to cope with
long-distance movement. The experiments and ablation study on FlyingThings3D
and KITTI scene flow datasets demonstrate the effectiveness of each proposed
component. The qualitative results show that the problems of ambiguous
inter-frame association and long-distance movement estimation are well handled.
Quantitative results on both FlyingThings3D and KITTI scene flow datasets show
that the proposed method achieves state-of-the-art performance, surpassing all
other previous works to the best of our knowledge by at least 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rapid detection and recognition of whole brain activity in a freely behaving Caenorhabditis elegans. (arXiv:2109.10474v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10474">
<div class="article-summary-box-inner">
<span><p>Advanced volumetric imaging methods and genetically encoded activity
indicators have permitted a comprehensive characterization of whole brain
activity at single neuron resolution in \textit{Caenorhabditis elegans}. The
constant motion and deformation of the mollusc nervous system, however, impose
a great challenge for consistent identification of densely packed neurons in a
behaving animal. Here, we propose a cascade solution for long-term and rapid
recognition of head ganglion neurons in a freely moving \textit{C. elegans}.
First, potential neuronal regions from a stack of fluorescence images are
detected by a deep learning algorithm. Second, 2-dimensional neuronal regions
are fused into 3-dimensional neuron entities. Third, by exploiting the neuronal
density distribution surrounding a neuron and relative positional information
between neurons, a multi-class artificial neural network transforms engineered
neuronal feature vectors into digital neuronal identities. With a small number
of training samples, our bottom-up approach is able to process each volume -
$1024 \times 1024 \times 18$ in voxels - in less than 1 second and achieves an
accuracy of $91\%$ in neuronal detection and $80\%$ in neuronal recognition.
Our work represents a step towards rapid and fully automated algorithms for
decoding whole brain activity underlying naturalistic behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-Aware Path Planning via Introspective Perception. (arXiv:2109.13974v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13974">
<div class="article-summary-box-inner">
<span><p>Robots deployed in the real world over extended periods of time need to
reason about unexpected failures, learn to predict them, and to proactively
take actions to avoid future failures. Existing approaches for competence-aware
planning are either model-based, requiring explicit enumeration of known
failure modes, or purely statistical, using state- and location-specific
failure statistics to infer competence. We instead propose a structured
model-free approach to competence-aware planning by reasoning about plan
execution failures due to errors in perception, without requiring a priori
enumeration of failure sources or requiring location-specific failure
statistics. We introduce competence-aware path planning via introspective
perception (CPIP), a Bayesian framework to iteratively learn and exploit
task-level competence in novel deployment environments. CPIP factorizes the
competence-aware planning problem into two components. First, perception errors
are learned in a model-free and location-agnostic setting via introspective
perception prior to deployment in novel environments. Second, during actual
deployments, the prediction of task-level failures is learned in a
context-aware setting. Experiments in a simulation show that the proposed CPIP
approach outperforms the frequentist baseline in multiple mobile robot tasks,
and is further validated via real robot experiments in an environment with
perceptually challenging obstacles and terrain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Communication-Efficient and Privacy-Preserving Federated Representation Learning. (arXiv:2109.14611v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14611">
<div class="article-summary-box-inner">
<span><p>This paper investigates the feasibility of federated representation learning
under the constraints of communication cost and privacy protection. Existing
works either conduct annotation-guided local training which requires frequent
communication or aggregates the client models via weight averaging which has
potential risks of privacy exposure. To tackle the above problems, we first
identify that self-supervised contrastive local training is robust against the
non-identically distributed data, which provides the feasibility of longer
local training and thus reduces the communication cost. Then based on the
aforementioned robustness, we propose a novel Federated representation Learning
framework with Ensemble Similarity Distillation~(FLESD) that utilizes this
robustness. At each round of communication, the server first gathers a fraction
of the clients' inferred similarity matrices on a public dataset. Then it
ensembles the similarity matrices and train the global model via similarity
distillation. We verify the effectiveness of FLESD by a series of empirical
experiments and show that, despite stricter constraints, it achieves comparable
results under multiple settings on multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TranSalNet: Towards perceptually relevant visual saliency prediction. (arXiv:2110.03593v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03593">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have significantly advanced
computational modelling for saliency prediction. However, accurately simulating
the mechanisms of visual attention in the human cortex remains an academic
challenge. It is critical to integrate properties of human vision into the
design of CNN architectures, leading to perceptually more relevant saliency
prediction. Due to the inherent inductive biases of CNN architectures, there is
a lack of sufficient long-range contextual encoding capacity. This hinders
CNN-based saliency models from capturing properties that emulate viewing
behaviour of humans. Transformers have shown great potential in encoding
long-range information by leveraging the self-attention mechanism. In this
paper, we propose a novel saliency model that integrates transformer components
to CNNs to capture the long-range contextual visual information. Experimental
results show that the transformers provide added value to saliency prediction,
enhancing its perceptual relevance in the performance. Our proposed saliency
model using transformers has achieved superior results on public benchmarks and
competitions for saliency prediction models.
</p>
<p>The source code of our proposed saliency model TranSalNet is available at:
https://github.com/LJOVO/TranSalNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Semantic Aggregation and Calibration for Separated Domain Generalization. (arXiv:2110.06736v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06736">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims to learn from multiple known source domains a
model that can generalize well to unknown target domains. The existing DG
methods usually exploit the fusion of shared multi-source data for capturing
domain invariance and training a generalizable model. However, tremendous data
is distributed across lots of places nowadays that can not be shared due to
strict privacy policies. A dilemma is thus raised between the generalization
learning with shared multi-source data and the privacy protection of real-world
sensitive data. In this paper, we introduce a separated domain generalization
task with separated source datasets that can only be accessed locally for data
privacy protection. We propose a novel solution called Collaborative Semantic
Aggregation and Calibration (CSAC) to enable this challenging task. To fully
absorb multi-source semantic information while avoiding unsafe data fusion, we
conduct data-free semantic aggregation by fusing the models trained on the
separated domains layer-by-layer. To address the semantic dislocation problem
caused by domain shift, we further design cross-layer semantic calibration with
an elaborate attention mechanism to align each semantic level and enhance
domain invariance. We unify multi-source semantic learning and alignment in a
collaborative way by repeating the semantic aggregation and calibration
alternately, keeping each dataset localized, and the data privacy is thus
carefully protected. Extensive experiments show the significant performance of
our method in addressing this challenging task, which is even comparable to the
previous DG methods with shared source data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. (arXiv:2110.08263v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08263">
<div class="article-summary-box-inner">
<span><p>The recently proposed FixMatch achieved state-of-the-art results on most
semi-supervised learning (SSL) benchmarks. However, like other modern SSL
algorithms, FixMatch uses a pre-defined constant threshold for all classes to
select unlabeled data that contribute to the training, thus failing to consider
different learning status and learning difficulties of different classes. To
address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum
learning approach to leverage unlabeled data according to the model's learning
status. The core of CPL is to flexibly adjust thresholds for different classes
at each time step to let pass informative unlabeled data and their pseudo
labels. CPL does not introduce additional parameters or computations (forward
or backward propagation). We apply CPL to FixMatch and call our improved
algorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a
variety of SSL benchmarks, with especially strong performances when the labeled
data are extremely limited or when the task is challenging. For example,
FlexMatch achieves 13.96% and 18.96% error rate reduction over FixMatch on
CIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per
class. CPL also significantly boosts the convergence speed, e.g., FlexMatch can
use only 1/5 training time of FixMatch to achieve even better performance.
Furthermore, we show that CPL can be easily adapted to other SSL algorithms and
remarkably improve their performances. We open-source our code at
https://github.com/TorchSSL/TorchSSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithmic encoding of protected characteristics in image-based models for disease detection. (arXiv:2110.14755v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14755">
<div class="article-summary-box-inner">
<span><p>It has been rightfully emphasized that the use of AI for clinical decision
making could amplify health disparities. A machine learning model may pick up
undesirable correlations, for example, between a patient's racial identity and
clinical outcome. Such correlations are often present in (historical) data used
for model development. There has been an increase in studies reporting biases
in image-based disease detection models. Besides the scarcity of data from
underserved populations, very little is known about how these biases are
encoded and how one may reduce or even remove disparate performance. There are
concerns that an algorithm may recognize patient characteristics such as
biological sex or racial identity, and then directly or indirectly use this
information when making predictions. But it remains unclear how we can
establish whether such information is actually used. This article aims to shed
some light on these issues by exploring different methodology for assessing the
inner working of disease detection models. We explore multitask learning and
model inspection to assess the relationship between protected characteristics
and prediction of disease. We believe our analysis framework could provide
valuable insights in future studies in medical imaging AI. Our findings also
call for further research to better understand the underlying causes of
performance disparities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Distilled Collaboration Graph for Multi-Agent Perception. (arXiv:2111.00643v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00643">
<div class="article-summary-box-inner">
<span><p>To promote better performance-bandwidth trade-off for multi-agent perception,
we propose a novel distilled collaboration graph (DiscoGraph) to model
trainable, pose-aware, and adaptive collaboration among agents. Our key
novelties lie in two aspects. First, we propose a teacher-student framework to
train DiscoGraph via knowledge distillation. The teacher model employs an early
collaboration with holistic-view inputs; the student model is based on
intermediate collaboration with single-view inputs. Our framework trains
DiscoGraph by constraining post-collaboration feature maps in the student model
to match the correspondences in the teacher model. Second, we propose a
matrix-valued edge weight in DiscoGraph. In such a matrix, each element
reflects the inter-agent attention at a specific spatial region, allowing an
agent to adaptively highlight the informative regions. During inference, we
only need to use the student model named as the distilled collaboration network
(DiscoNet). Attributed to the teacher-student framework, multiple agents with
the shared DiscoNet could collaboratively approach the performance of a
hypothetical teacher model with a holistic view. Our approach is validated on
V2X-Sim 1.0, a large-scale multi-agent perception dataset that we synthesized
using CARLA and SUMO co-simulation. Our quantitative and qualitative
experiments in multi-agent 3D object detection show that DiscoNet could not
only achieve a better performance-bandwidth trade-off than the state-of-the-art
collaborative perception methods, but also bring more straightforward design
rationale. Our code is available on https://github.com/ai4ce/DiscoNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">First steps on Gamification of Lung Fluid Cells Annotations in the Flower Domain. (arXiv:2111.03663v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03663">
<div class="article-summary-box-inner">
<span><p>Annotating data, especially in the medical domain, requires expert knowledge
and a lot of effort. This limits the amount and/or usefulness of available
medical data sets for experimentation. Therefore, developing strategies to
increase the number of annotations while lowering the needed domain knowledge
is of interest. A possible strategy is the use of gamification, i.e.
transforming the annotation task into a game. We propose an approach to gamify
the task of annotating lung fluid cells from pathological whole slide images
(WSIs). As the domain is unknown to non-expert annotators, we transform images
of cells to the domain of flower images using a CycleGAN architecture. In this
more assessable domain, non-expert annotators can be (t)asked to annotate
different kinds of flowers in a playful setting. In order to provide a proof of
concept, this work shows that the domain transfer is possible by evaluating an
image classification network trained on real cell images and tested on the cell
images generated by the CycleGAN network (reconstructed cell images) as well as
real cell images. The classification network reaches an average accuracy of
94.73 % on the original lung fluid cells and 95.25 % on the transformed lung
fluid cells, respectively. Our study lays the foundation for future research on
gamification using CycleGANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks. (arXiv:2111.07492v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07492">
<div class="article-summary-box-inner">
<span><p>One major problem in black-box adversarial attacks is the high query
complexity in the hard-label attack setting, where only the top-1 predicted
label is available. In this paper, we propose a novel geometric-based approach
called Tangent Attack (TA), which identifies an optimal tangent point of a
virtual hemisphere located on the decision boundary to reduce the distortion of
the attack. Assuming the decision boundary is locally flat, we theoretically
prove that the minimum $\ell_2$ distortion can be obtained by reaching the
decision boundary along the tangent line passing through such tangent point in
each iteration. To improve the robustness of our method, we further propose a
generalized method which replaces the hemisphere with a semi-ellipsoid to adapt
to curved decision boundaries. Our approach is free of pre-training. Extensive
experiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that
our approach can consume only a small number of queries to achieve the
low-magnitude distortion. The implementation source code is released online at
https://github.com/machanic/TangentAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00656">
<div class="article-summary-box-inner">
<span><p>Recently, by introducing large-scale dataset and strong transformer network,
video-language pre-training has shown great success especially for retrieval.
Yet, existing video-language transformer models do not explicitly fine-grained
semantic align. In this work, we present Object-aware Transformers, an
object-centric approach that extends video-language transformer to incorporate
object representations. The key idea is to leverage the bounding boxes and
object tags to guide the training process. We evaluate our model on three
standard sub-tasks of video-text matching on four widely used benchmarks. We
also provide deep analysis and detailed ablation about the proposed method. We
show clear improvement in performance across all tasks and datasets considered,
demonstrating the value of a model that incorporates object representations
into a video-language architecture. The code will be released at
\url{https://github.com/FingerRec/OA-Transformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to automate cryo-electron microscopy data collection with Ptolemy. (arXiv:2112.01534v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01534">
<div class="article-summary-box-inner">
<span><p>Over the past decade, cryogenic electron microscopy (cryo-EM) has emerged as
a primary method for determining near-native, near-atomic resolution 3D
structures of biological macromolecules. In order to meet increasing demand for
cryo-EM, automated methods to improve throughput and efficiency while lowering
costs are needed. Currently, all high-magnification cryo-EM data collection
softwares require human input and manual tuning of parameters. Expert operators
must navigate low- and medium-magnification images to find good
high-magnification collection locations. Automating this is non-trivial: the
images suffer from low signal-to-noise ratio and are affected by a range of
experimental parameters that can differ for each collection session. Here, we
use various computer vision algorithms, including mixture models, convolutional
neural networks, and U-Nets to develop the first pipeline to automate low- and
medium-magnification targeting. Learned models in this pipeline are trained on
a large internal dataset of images from real world cryo-EM data collection
sessions, labeled with locations that were selected by operators. Using these
models, we show that we can effectively detect and classify regions of interest
in low- and medium-magnification images, and can generalize to unseen sessions,
as well as to images captured using different microscopes from external
facilities. We expect our open-source pipeline, Ptolemy, will be both
immediately useful as a tool for automation of cryo-EM data collection, and
serve as a foundation for future advanced methods for efficient and automated
cryo-EM microscopy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action Units That Constitute Trainable Micro-expressions (and A Large-scale Synthetic Dataset). (arXiv:2112.01730v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01730">
<div class="article-summary-box-inner">
<span><p>Because of the expensive data collection process, micro-expression (MiE)
datasets are generally much smaller in scale than those in other computer
vision fields, rendering large-scale training less feasible. This paper
develops a protocol to automatically synthesize MiE training data that 1) are
of a large scale and 2) allow us to train accurate recognition models for
real-world test data. Specifically, we discover three types of Action Units
(AUs) that can constitute trainable MiEs. These AUs come from real-world MiEs,
early frames of macro-expression videos, and the relationship between AUs and
expression categories defined by human expert knowledge. With these AUs, our
protocol then employs large numbers of face images of various identities and an
off-the-shelf face generator for MiE synthesis, yielding the MiE-X dataset. MiE
recognition models are trained or pre-trained on MiE-X and evaluated on
real-world test sets, where competitive accuracy is obtained. Experimental
results not only validate the effectiveness of these AUs and our MiE-X dataset
but also reveal some critical properties of MiEs: they generalize across faces,
are close to early-stage macro-expressions, and can be manually defined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DualFormer: Local-Global Stratified Transformer for Efficient Video Recognition. (arXiv:2112.04674v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04674">
<div class="article-summary-box-inner">
<span><p>While transformers have shown great potential on video recognition tasks with
their strong capability of capturing long-range dependencies, they often suffer
high computational costs induced by self-attention operation on the huge number
of 3D tokens in a video. In this paper, we propose a new transformer
architecture, termed DualFormer, which can effectively and efficiently perform
space-time attention for video recognition. Specifically, our DualFormer
stratifies the full space-time attention into dual cascaded levels, i.e., to
first learn fine-grained local space-time interactions among nearby 3D tokens,
followed by the capture of coarse-grained global dependencies between the query
token and the coarse-grained global pyramid contexts. Different from existing
methods that apply space-time factorization or restrict attention computations
within local windows for improving efficiency, our local-global stratified
strategy can well capture both short- and long-range spatiotemporal
dependencies, and meanwhile greatly reduces the number of keys and values in
attention computation to boost efficiency. Experimental results show the
superiority of DualFormer on five video benchmarks against existing methods. In
particular, DualFormer sets new state-of-the-art 82.9%/85.2% top-1 accuracy on
Kinetics-400/600 with around 1000G inference FLOPs which is at least 3.2 times
fewer than existing methods with similar performances. We have released our
code at https://github.com/sail-sg/dualformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimIPU: Simple 2D Image and 3D Point Cloud Unsupervised Pre-Training for Spatial-Aware Visual Representations. (arXiv:2112.04680v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04680">
<div class="article-summary-box-inner">
<span><p>Pre-training has become a standard paradigm in many computer vision tasks.
However, most of the methods are generally designed on the RGB image domain.
Due to the discrepancy between the two-dimensional image plane and the
three-dimensional space, such pre-trained models fail to perceive spatial
information and serve as sub-optimal solutions for 3D-related tasks. To bridge
this gap, we aim to learn a spatial-aware visual representation that can
describe the three-dimensional space and is more suitable and effective for
these tasks. To leverage point clouds, which are much more superior in
providing spatial information compared to images, we propose a simple yet
effective 2D Image and 3D Point cloud Unsupervised pre-training strategy,
called SimIPU. Specifically, we develop a multi-modal contrastive learning
framework that consists of an intra-modal spatial perception module to learn a
spatial-aware representation from point clouds and an inter-modal feature
interaction module to transfer the capability of perceiving spatial information
from the point cloud encoder to the image encoder, respectively. Positive pairs
for contrastive losses are established by the matching algorithm and the
projection matrix. The whole framework is trained in an unsupervised end-to-end
fashion. To the best of our knowledge, this is the first study to explore
contrastive learning pre-training strategies for outdoor multi-modal datasets,
containing paired camera images and LIDAR point clouds. Codes and models are
available at https://github.com/zhyever/SimIPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Translation Prior: Test-time Training for Photorealistic Style Transfer. (arXiv:2112.06150v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06150">
<div class="article-summary-box-inner">
<span><p>Recent techniques to solve photorealistic style transfer within deep
convolutional neural networks (CNNs) generally require intensive training from
large-scale datasets, thus having limited applicability and poor generalization
ability to unseen images or styles. To overcome this, we propose a novel
framework, dubbed Deep Translation Prior (DTP), to accomplish photorealistic
style transfer through test-time training on given input image pair with
untrained networks, which learns an image pair-specific translation prior and
thus yields better performance and generalization. Tailored for such test-time
training for style transfer, we present novel network architectures, with two
sub-modules of correspondence and generation modules, and loss functions
consisting of contrastive content, style, and cycle consistency losses. Our
framework does not require offline training phase for style transfer, which has
been one of the main challenges in existing methods, but the networks are to be
solely learned during test-time. Experimental results prove that our framework
has a better generalization ability to unseen image pairs and even outperforms
the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Human-Object Interaction Detection via Phrase Learning and Label Composition. (arXiv:2112.07383v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07383">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) detection is a fundamental task in high-level
human-centric scene understanding. We propose PhraseHOI, containing a HOI
branch and a novel phrase branch, to leverage language prior and improve
relation expression. Specifically, the phrase branch is supervised by semantic
embeddings, whose ground truths are automatically converted from the original
HOI annotations without extra human efforts. Meanwhile, a novel label
composition method is proposed to deal with the long-tailed problem in HOI,
which composites novel phrase labels by semantic neighbors. Further, to
optimize the phrase branch, a loss composed of a distilling loss and a balanced
triplet loss is proposed. Extensive experiments are conducted to prove the
effectiveness of the proposed PhraseHOI, which achieves significant improvement
over the baseline and surpasses previous state-of-the-art methods on Full and
NonRare on the challenging HICO-DET benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensembling Off-the-shelf Models for GAN Training. (arXiv:2112.09130v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09130">
<div class="article-summary-box-inner">
<span><p>The advent of large-scale training has produced a cornucopia of powerful
visual recognition models. However, generative models, such as GANs, have
traditionally been trained from scratch in an unsupervised manner. Can the
collective "knowledge" from a large bank of pretrained vision models be
leveraged to improve GAN training? If so, with so many models to choose from,
which one(s) should be selected, and in what manner are they most effective? We
find that pretrained computer vision models can significantly improve
performance when used in an ensemble of discriminators. Notably, the particular
subset of selected models greatly affects performance. We propose an effective
selection mechanism, by probing the linear separability between real and fake
samples in pretrained model embeddings, choosing the most accurate model, and
progressively adding it to the discriminator ensemble. Interestingly, our
method can improve GAN training in both limited data and large-scale settings.
Given only 10k training samples, our FID on LSUN Cat matches the StyleGAN2
trained on 1.6M images. On the full dataset, our method improves FID by 1.5x to
2x on cat, church, and horse categories of LSUN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroVL: A Strong Baseline for Aligning Vision-Language Representations with Limited Resources. (arXiv:2112.09331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09331">
<div class="article-summary-box-inner">
<span><p>Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have
revealed the potential of aligning multi-modal representations with contrastive
learning. However, these works require a tremendous amount of data and
computational resources (e.g., billion-level web data and hundreds of GPUs),
which prevent researchers with limited resources from reproduction and further
exploration. To this end, we explore a stack of simple but effective
heuristics, and provide a comprehensive training guidance, which allows us to
conduct dual-encoder multi-modal representation alignment with limited
resources. We provide a reproducible strong baseline of competitive results,
namely ZeroVL, with only 14M publicly accessible academic datasets and 8 V100
GPUs. Additionally, we collect 100M web data for pre-training, and achieve
comparable or superior results than state-of-the-art methods, further proving
the effectiveness of our method on large-scale data. We hope that this work
will provide useful data points and experience for future research in
multi-modal pre-training. Our code is available at
https://github.com/zerovl/ZeroVL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Subsampling for ROI-based Visual Tracking: Algorithms and FPGA Implementation. (arXiv:2112.09775v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09775">
<div class="article-summary-box-inner">
<span><p>There is tremendous scope for improving the energy efficiency of embedded
vision systems by incorporating programmable region-of-interest (ROI) readout
in the image sensor design. In this work, we study how ROI programmability can
be leveraged for tracking applications by anticipating where the ROI will be
located in future frames and switching pixels off outside of this region. We
refer to this process of ROI prediction and corresponding sensor configuration
as adaptive subsampling. Our adaptive subsampling algorithms comprise an object
detector and an ROI predictor (Kalman filter) which operate in conjunction to
optimize the energy efficiency of the vision pipeline with the end task being
object tracking. To further facilitate the implementation of our adaptive
algorithms in real life, we select a candidate algorithm and map it onto an
FPGA. Leveraging Xilinx Vitis AI tools, we designed and accelerated a YOLO
object detector-based adaptive subsampling algorithm. In order to further
improve the algorithm post-deployment, we evaluated several competing baselines
on the OTB100 and LaSOT datasets. We found that coupling the ECO tracker with
the Kalman filter has a competitive AUC score of 0.4568 and 0.3471 on the
OTB100 and LaSOT datasets respectively. Further, the power efficiency of this
algorithm is on par with, and in a couple of instances superior to, the other
baselines. The ECO-based algorithm incurs a power consumption of approximately
4 W averaged across both datasets while the YOLO-based approach requires power
consumption of approximately 6 W (as per our power consumption model). In terms
of accuracy-latency tradeoff, the ECO-based algorithm provides near-real-time
performance (19.23 FPS) while managing to attain competitive tracking
precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Implicit Body Representations from Double Diffusion Based Neural Radiance Fields. (arXiv:2112.12390v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12390">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel double diffusion based neural radiance
field, dubbed DD-NeRF, to reconstruct human body geometry and render the human
body appearance in novel views from a sparse set of images. We first propose a
double diffusion mechanism to achieve expressive representations of input
images by fully exploiting human body priors and image appearance details at
two levels. At the coarse level, we first model the coarse human body poses and
shapes via an unclothed 3D deformable vertex model as guidance. At the fine
level, we present a multi-view sampling network to capture subtle geometric
deformations and image detailed appearances, such as clothing and hair, from
multiple input views. Considering the sparsity of the two level features, we
diffuse them into feature volumes in the canonical space to construct neural
radiance fields. Then, we present a signed distance function (SDF) regression
network to construct body surfaces from the diffused features. Thanks to our
double diffused representations, our method can even synthesize novel views of
unseen subjects. Experiments on various datasets demonstrate that our approach
outperforms the state-of-the-art in both geometric reconstruction and novel
view synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01293">
<div class="article-summary-box-inner">
<span><p>This paper presents a transformer-based Siamese network architecture
(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of
co-registered remote sensing images. Different from recent CD frameworks, which
are based on fully convolutional networks (ConvNets), the proposed method
unifies hierarchically structured transformer encoder with Multi-Layer
Perception (MLP) decoder in a Siamese network architecture to efficiently
render multi-scale long-range details required for accurate CD. Experiments on
two CD datasets show that the proposed end-to-end trainable ChangeFormer
architecture achieves better CD performance than previous counterparts. Our
code is available at https://github.com/wgcban/ChangeFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Uniform Point Distribution in Feature-preserving Point Cloud Filtering. (arXiv:2201.01503v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01503">
<div class="article-summary-box-inner">
<span><p>As a popular representation of 3D data, point cloud may contain noise and
need to be filtered before use. Existing point cloud filtering methods either
cannot preserve sharp features or result in uneven point distribution in the
filtered output. To address this problem, this paper introduces a point cloud
filtering method that considers both point distribution and feature
preservation during filtering. The key idea is to incorporate a repulsion term
with a data term in energy minimization. The repulsion term is responsible for
the point distribution, while the data term is to approximate the noisy
surfaces while preserving the geometric features. This method is capable of
handling models with fine-scale features and sharp features. Extensive
experiments show that our method yields better results with a more uniform
point distribution ($5.8\times10^{-5}$ Chamfer Distance on average) in seconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Intracranial Aneurysm Classification and Segmentation via Unsupervised Dual-branch Learning. (arXiv:2201.02198v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02198">
<div class="article-summary-box-inner">
<span><p>Intracranial aneurysms are common nowadays and how to detect them
intelligently is of great significance in digital health. While most existing
deep learning research focused on medical images in a supervised way, we
introduce an unsupervised method for the detection of intracranial aneurysms
based on 3D point cloud data. In particular, our method consists of two stages:
unsupervised pre-training and downstream tasks. As for the former, the main
idea is to pair each point cloud with its jittered counterpart and maximise
their correspondence. Then we design a dual-branch contrastive network with an
encoder for each branch and a subsequent common projection head. As for the
latter, we design simple networks for supervised classification and
segmentation training. Experiments on the public dataset (IntrA) show that our
unsupervised method achieves comparable or even better performance than some
state-of-the-art supervised techniques, and it is most prominent in the
detection of aneurysmal vessels. Experiments on the ModelNet40 also show that
our method achieves the accuracy of 90.79\% which outperforms existing
state-of-the-art unsupervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Scale-Invariance and Uncertainty for Self-Supervised Domain Adaptation of Foggy Scenes Segmentation. (arXiv:2201.02588v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02588">
<div class="article-summary-box-inner">
<span><p>This paper presents FogAdapt, a novel approach for domain adaptation of
semantic segmentation for dense foggy scenes. Although significant research has
been directed to reduce the domain shift in semantic segmentation, adaptation
to scenes with adverse weather conditions remains an open question. Large
variations in the visibility of the scene due to weather conditions, such as
fog, smog, and haze, exacerbate the domain shift, thus making unsupervised
adaptation in such scenarios challenging. We propose a self-entropy and
multi-scale information augmented self-supervised domain adaptation method
(FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported
by the empirical evidence that an increase in fog density results in high
self-entropy for segmentation probabilities, we introduce a self-entropy based
loss function to guide the adaptation method. Furthermore, inferences obtained
at different image scales are combined and weighted by the uncertainty to
generate scale-invariant pseudo-labels for the target domain. These
scale-invariant pseudo-labels are robust to visibility and scale variations. We
evaluate the proposed model on real clear-weather scenes to real foggy scenes
adaptation and synthetic non-foggy images to real foggy scenes adaptation
scenarios. Our experiments demonstrate that FogAdapt significantly outperforms
the current state-of-the-art in semantic segmentation of foggy images.
Specifically, by considering the standard settings compared to state-of-the-art
(SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy
Driving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes
to Foggy Zurich.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counteracting Dark Web Text-Based CAPTCHA with Generative Adversarial Learning for Proactive Cyber Threat Intelligence. (arXiv:2201.02799v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02799">
<div class="article-summary-box-inner">
<span><p>Automated monitoring of dark web (DW) platforms on a large scale is the first
step toward developing proactive Cyber Threat Intelligence (CTI). While there
are efficient methods for collecting data from the surface web, large-scale
dark web data collection is often hindered by anti-crawling measures. In
particular, text-based CAPTCHA serves as the most prevalent and prohibiting
type of these measures in the dark web. Text-based CAPTCHA identifies and
blocks automated crawlers by forcing the user to enter a combination of
hard-to-recognize alphanumeric characters. In the dark web, CAPTCHA images are
meticulously designed with additional background noise and variable character
length to prevent automated CAPTCHA breaking. Existing automated CAPTCHA
breaking methods have difficulties in overcoming these dark web challenges. As
such, solving dark web text-based CAPTCHA has been relying heavily on human
involvement, which is labor-intensive and time-consuming. In this study, we
propose a novel framework for automated breaking of dark web CAPTCHA to
facilitate dark web data collection. This framework encompasses a novel
generative method to recognize dark web text-based CAPTCHA with noisy
background and variable character length. To eliminate the need for human
involvement, the proposed framework utilizes Generative Adversarial Network
(GAN) to counteract dark web background noise and leverages an enhanced
character segmentation algorithm to handle CAPTCHA images with variable
character length. Our proposed framework, DW-GAN, was systematically evaluated
on multiple dark web CAPTCHA testbeds. DW-GAN significantly outperformed the
state-of-the-art benchmark methods on all datasets, achieving over 94.4%
success rate on a carefully collected real-world dark web dataset...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resolving Camera Position for a Practical Application of Gaze Estimation on Edge Devices. (arXiv:2201.02946v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02946">
<div class="article-summary-box-inner">
<span><p>Most Gaze estimation research only works on a setup condition that a camera
perfectly captures eyes gaze. They have not literarily specified how to set up
a camera correctly for a given position of a person. In this paper, we carry
out a study on gaze estimation with a logical camera setup position. We further
bring our research in a practical application by using inexpensive edge devices
with a realistic scenario. That is, we first set up a shopping environment
where we want to grasp customers gazing behaviors. This setup needs an optimal
camera position in order to maintain estimation accuracy from existing gaze
estimation research. We then apply the state-of-the-art of few-shot learning
gaze estimation to reduce training sampling in the inference phase. In the
experiment, we perform our implemented research on NVIDIA Jetson TX2 and
achieve a reasonable speed, 12 FPS which is faster compared with our reference
work, without much degradation of gaze estimation accuracy. The source code is
released at https://github.com/linh-gist/GazeEstimationTX2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducing BowNet: Learning Representations by Predicting Bags of Visual Words. (arXiv:2201.03556v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03556">
<div class="article-summary-box-inner">
<span><p>This work aims to reproduce results from the CVPR 2020 paper by Gidaris et
al. Self-supervised learning (SSL) is used to learn feature representations of
an image using an unlabeled dataset. This work proposes to use bag-of-words
(BoW) deep feature descriptors as a self-supervised learning target to learn
robust, deep representations. BowNet is trained to reconstruct the histogram of
visual words (ie. the deep BoW descriptor) of a reference image when presented
a perturbed version of the image as input. Thus, this method aims to learn
perturbation-invariant and context-aware image features that can be useful for
few-shot tasks or supervised downstream tasks. In the paper, the author
describes BowNet as a network consisting of a convolutional feature extractor
$\Phi(\cdot)$ and a Dense-softmax layer $\Omega(\cdot)$ trained to predict BoW
features from images. After BoW training, the features of $\Phi$ are used in
downstream tasks. For this challenge we were trying to build and train a
network that could reproduce the CIFAR-100 accuracy improvements reported in
the original paper. However, we were unsuccessful in reproducing an accuracy
improvement comparable to what the authors mentioned. This could be for a
variety of factors and we believe that time constraints were the primary
bottleneck.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PrintsGAN: Synthetic Fingerprint Generator. (arXiv:2201.03674v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03674">
<div class="article-summary-box-inner">
<span><p>A major impediment to researchers working in the area of fingerprint
recognition is the lack of publicly available, large-scale, fingerprint
datasets. The publicly available datasets that do exist contain very few
identities and impressions per finger. This limits research on a number of
topics, including e.g., using deep networks to learn fixed length fingerprint
embeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator
capable of generating unique fingerprints along with multiple impressions for a
given fingerprint. Using PrintsGAN, we synthesize a database of 525k
fingerprints (35K distinct fingers, each with 15 impressions). Next, we show
the utility of the PrintsGAN generated dataset by training a deep network to
extract a fixed-length embedding from a fingerprint. In particular, an
embedding model trained on our synthetic fingerprints and fine-tuned on a small
number of publicly available real fingerprints (25K prints from NIST SD302)
obtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from
TAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint
generation methods do not enable such performance gains due to i) lack of
realism or ii) inability to generate multiple impressions per finger. We plan
to release our database of synthetic fingerprints to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning. (arXiv:2201.04182v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04182">
<div class="article-summary-box-inner">
<span><p>In this work we propose a HyperTransformer, a transformer-based model for
few-shot learning that generates weights of a convolutional neural network
(CNN) directly from support samples. Since the dependence of a small generated
CNN model on a specific task is encoded by a high-capacity transformer model,
we effectively decouple the complexity of the large task space from the
complexity of individual tasks. Our method is particularly effective for small
target CNN architectures where learning a fixed universal task-independent
embedding is not optimal and better performance is attained when the
information about the task can modulate all model parameters. For larger models
we discover that generating the last layer alone allows us to produce
competitive or better results than those obtained with state-of-the-art methods
while being end-to-end differentiable. Finally, we extend our approach to a
semi-supervised regime utilizing unlabeled samples in the support set and
further improving few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Capacitance: A New Perspective of Neural Network Selection via Edge Dynamics. (arXiv:2201.04194v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04194">
<div class="article-summary-box-inner">
<span><p>Efficient model selection for identifying a suitable pre-trained neural
network to a downstream task is a fundamental yet challenging task in deep
learning. Current practice requires expensive computational costs in model
training for performance prediction. In this paper, we propose a novel
framework for neural network selection by analyzing the governing dynamics over
synaptic connections (edges) during training. Our framework is built on the
fact that back-propagation during neural network training is equivalent to the
dynamical evolution of synaptic connections. Therefore, a converged neural
network is associated with an equilibrium state of a networked system composed
of those edges. To this end, we construct a network mapping $\phi$, converting
a neural network $G_A$ to a directed line graph $G_B$ that is defined on those
edges in $G_A$. Next, we derive a neural capacitance metric $\beta_{\rm eff}$
as a predictive measure universally capturing the generalization capability of
$G_A$ on the downstream task using only a handful of early training results. We
carried out extensive experiments using 17 popular pre-trained ImageNet models
and five benchmark datasets, including CIFAR10, CIFAR100, SVHN, Fashion MNIST
and Birds, to evaluate the fine-tuning performance of our framework. Our neural
capacitance metric is shown to be a powerful indicator for model selection
based only on early training results and is more efficient than
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04756">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the solution of roadside LiDAR object detection
using a combination of two unsupervised learning algorithms. The 3D point
clouds data are firstly converted into spherical coordinates and filled into
the azimuth grid matrix using a hash function. After that, the raw LiDAR data
were rearranged into spatial-temporal data structures to store the information
of range, azimuth, and intensity. Dynamic Mode Decomposition method is applied
for decomposing the point cloud data into low-rank backgrounds and sparse
foregrounds based on intensity channel pattern recognition. The Triangle
Algorithm automatically finds the dividing value to separate the moving targets
from static background according to range information. After intensity and
range background subtraction, the foreground moving objects will be detected
using a density-based detector and encoded into the state-space model for
tracking. The output of the proposed model includes vehicle trajectories that
can enable many mobility and safety applications. The method was validated
against a commercial traffic data collection platform and demonstrated to be an
efficient and reliable solution for infrastructure LiDAR object detection. In
contrast to the previous methods that process directly on the scattered and
discrete point clouds, the proposed method can establish the less sophisticated
linear relationship of the 3D measurement data, which captures the
spatial-temporal structure that we often desire.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Cross-Modality Retinal Vessel Segmentation via Disentangling Representation Style Transfer and Collaborative Consistency Learning. (arXiv:2201.04812v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04812">
<div class="article-summary-box-inner">
<span><p>Various deep learning models have been developed to segment anatomical
structures from medical images, but they typically have poor performance when
tested on another target domain with different data distribution. Recently,
unsupervised domain adaptation methods have been proposed to alleviate this
so-called domain shift issue, but most of them are designed for scenarios with
relatively small domain shifts and are likely to fail when encountering a large
domain gap. In this paper, we propose DCDA, a novel cross-modality unsupervised
domain adaptation framework for tasks with large domain shifts, e.g.,
segmenting retinal vessels from OCTA and OCT images. DCDA mainly consists of a
disentangling representation style transfer (DRST) module and a collaborative
consistency learning (CCL) module. DRST decomposes images into content
components and style codes and performs style transfer and image
reconstruction. CCL contains two segmentation models, one for source domain and
the other for target domain. The two models use labeled data (together with the
corresponding transferred images) for supervised learning and perform
collaborative consistency learning on unlabeled data. Each model focuses on the
corresponding single domain and aims to yield an expertized domain-specific
segmentation model. Through extensive experiments on retinal vessel
segmentation, our framework achieves Dice scores close to target-trained oracle
both from OCTA to OCT and from OCT to OCTA, significantly outperforming other
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Style Image Super-Resolution using Conditional Objective. (arXiv:2201.04898v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04898">
<div class="article-summary-box-inner">
<span><p>Recent studies have significantly enhanced the performance of single-image
super-resolution (SR) using convolutional neural networks (CNNs). While there
can be many high-resolution (HR) solutions for a given input, most existing
CNN-based methods do not explore alternative solutions during the inference. A
typical approach to obtaining alternative SR results is to train multiple SR
models with different loss weightings and exploit the combination of these
models. Instead of using multiple models, we present a more efficient method to
train a single adjustable SR model on various combinations of losses by taking
advantage of multi-task learning. Specifically, we optimize an SR model with a
conditional objective during training, where the objective is a weighted sum of
multiple perceptual losses at different feature levels. The weights vary
according to given conditions, and the set of weights is defined as a style
controller. Also, we present an architecture appropriate for this training
scheme, which is the Residual-in-Residual Dense Block equipped with spatial
feature transformation layers. At the inference phase, our trained model can
generate locally different outputs conditioned on the style control map.
Extensive experiments show that the proposed SR model produces various
desirable reconstructions without artifacts and yields comparable quantitative
performance to state-of-the-art SR methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers. (arXiv:2201.05047v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05047">
<div class="article-summary-box-inner">
<span><p>Detection Transformer (DETR) and Deformable DETR have been proposed to
eliminate the need for many hand-designed components in object detection while
demonstrating good performance as previous complex hand-crafted detectors.
However, their performance on Video Object Detection (VOD) has not been well
explored. In this paper, we present TransVOD, the first end-to-end video object
detection system based on spatial-temporal Transformer architectures. The first
goal of this paper is to streamline the pipeline of VOD, effectively removing
the need for many hand-crafted components for feature aggregation, e.g.,
optical flow model, relation networks. Besides, benefited from the object query
design in DETR, our method does not need complicated post-processing methods
such as Seq-NMS. In particular, we present a temporal Transformer to aggregate
both the spatial object queries and the feature memories of each frame. Our
temporal transformer consists of two components: Temporal Query Encoder (TQE)
to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to
obtain current frame detection results. These designs boost the strong baseline
deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID
dataset. Then, we present two improved versions of TransVOD including
TransVOD++ and TransVOD Lite. The former fuses object-level information into
object query via dynamic convolution while the latter models the entire video
clips as the output to speed up the inference time. We give detailed analysis
of all three models in the experiment part. In particular, our proposed
TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet
VID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and
accuracy trade-off with 83.7% mAP while running at around 30 FPS on a single
V100 GPU device. Code and models will be available for further research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-19 23:06:33.379743244 UTC">2022-01-19 23:06:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>