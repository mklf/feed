<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-01T01:30:00Z">04-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Refinement for Coreference Resolution. (arXiv:2203.16574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16574">
<div class="article-summary-box-inner">
<span><p>The state-of-the-art models for coreference resolution are based on
independent mention pair-wise decisions. We propose a modelling approach that
learns coreference at the document-level and takes global decisions. For this
purpose, we model coreference links in a graph structure where the nodes are
tokens in the text, and the edges represent the relationship between them. Our
model predicts the graph in a non-autoregressive manner, then iteratively
refines it based on previous predictions, allowing global dependencies between
decisions. The experimental results show improvements over various baselines,
reinforcing the hypothesis that document-level information improves conference
resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Switched and Code Mixed Speech Recognition for Indic languages. (arXiv:2203.16578v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16578">
<div class="article-summary-box-inner">
<span><p>Training multilingual automatic speech recognition (ASR) systems is
challenging because acoustic and lexical information is typically language
specific. Training multilingual system for Indic languages is even more tougher
due to lack of open source datasets and results on different approaches. We
compare the performance of end to end multilingual speech recognition system to
the performance of monolingual models conditioned on language identification
(LID). The decoding information from a multilingual model is used for language
identification and then combined with monolingual models to get an improvement
of 50% WER across languages. We also propose a similar technique to solve the
Code Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English
and Bengali-English respectively. Our work talks on how transformer based ASR
especially wav2vec 2.0 can be applied in developing multilingual ASR and code
switched ASR for Indic languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Speech Recognition for Indic Languages using Language Model. (arXiv:2203.16595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16595">
<div class="article-summary-box-inner">
<span><p>We study the effect of applying a language model (LM) on the output of
Automatic Speech Recognition (ASR) systems for Indic languages. We fine-tune
wav2vec $2.0$ models for $18$ Indic languages and adjust the results with
language models trained on text derived from a variety of sources. Our findings
demonstrate that the average Character Error Rate (CER) decreases by over $28$
\% and the average Word Error Rate (WER) decreases by about $36$ \% after
decoding with LM. We show that a large LM may not provide a substantial
improvement as compared to a diverse one. We also demonstrate that high quality
transcriptions can be obtained on domain-specific data without retraining the
ASR model and show results on biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Word Error Rate a good evaluation metric for Speech Recognition in Indic Languages?. (arXiv:2203.16601v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16601">
<div class="article-summary-box-inner">
<span><p>We propose a new method for the calculation of error rates in Automatic
Speech Recognition (ASR). This new metric is for languages that contain half
characters and where the same character can be written in different forms. We
implement our methodology in Hindi which is one of the main languages from
Indic context and we think this approach is scalable to other similar languages
containing a large character set. We call our metrics Alternate Word Error Rate
(AWER) and Alternate Character Error Rate (ACER).
</p>
<p>We train our ASR models using wav2vec 2.0\cite{baevski2020wav2vec} for Indic
languages. Additionally we use language models to improve our model
performance. Our results show a significant improvement in analyzing the error
rates at word and character level and the interpretability of the ASR system is
improved upto $3$\% in AWER and $7$\% in ACER for Hindi. Our experiments
suggest that in languages which have complex pronunciation, there are multiple
ways of writing words without changing their meaning. In such cases AWER and
ACER will be more useful rather than WER and CER as metrics. Furthermore, we
open source a new benchmarking dataset of 21 hours for Hindi with the new
metric scripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Language Models without Positional Encodings Still Learn Positional Information. (arXiv:2203.16634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16634">
<div class="article-summary-box-inner">
<span><p>Transformers typically require some form of positional encoding, such as
positional embeddings, to process natural language sequences. Surprisingly, we
find that transformer language models without any explicit positional encoding
are still competitive with standard models, and that this phenomenon is robust
across different datasets, model sizes, and sequence lengths. Probing
experiments reveal that such models acquire an implicit notion of absolute
positions throughout the network, effectively compensating for the missing
information. We conjecture that causal attention enables the model to infer the
number of predecessors that each token can attend to, thereby approximating its
absolute position.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16639">
<div class="article-summary-box-inner">
<span><p>We present a meta-learning framework for learning new visual concepts
quickly, from just one or a few examples, guided by multiple naturally
occurring data streams: simultaneously looking at images, reading sentences
that describe the objects in the scene, and interpreting supplemental sentences
that relate the novel concept with other concepts. The learned concepts support
downstream applications, such as answering questions by reasoning about unseen
images. Our model, namely FALCON, represents individual visual concepts, such
as colors and shapes, as axis-aligned boxes in a high-dimensional space (the
"box embedding space"). Given an input image and its paired sentence, our model
first resolves the referential expression in the sentence and associates the
novel concept with particular objects in the scene. Next, our model interprets
supplemental sentences to relate the novel concept with other known concepts,
such as "X has property Y" or "X is a kind of Y". Finally, it infers an optimal
box embedding for the novel concept that jointly 1) maximizes the likelihood of
the observed instances in the image, and 2) satisfies the relationships between
the novel concepts and the known ones. We demonstrate the effectiveness of our
model on both synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation of Speaker Representations Using Heterogeneous Training Batch Assembly. (arXiv:2203.16646v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16646">
<div class="article-summary-box-inner">
<span><p>In traditional speaker diarization systems, a well-trained speaker model is a
key component to extract representations from consecutive and partially
overlapping segments in a long speech session. To be more consistent with the
back-end segmentation and clustering, we propose a new CNN-based speaker
modeling scheme, which takes into account the heterogeneity of the speakers in
each training segment and batch. We randomly and synthetically augment the
training data into a set of segments, each of which contains more than one
speaker and some overlapping parts. A soft label is imposed on each segment
based on its speaker occupation ratio, and the standard cross entropy loss is
implemented in model training. In this way, the speaker model should have the
ability to generate a geometrically meaningful embedding for each multi-speaker
segment. Experimental results show that our system is superior to the baseline
system using x-vectors in two speaker diarization tasks. In the CALLHOME task
trained on the NIST SRE and Switchboard datasets, our system achieves a
relative reduction of 12.93% in DER. In Track 2 of CHiME-6, our system provides
13.24%, 12.60%, and 5.65% relative reductions in DER, JER, and WER,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo. (arXiv:2203.16682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16682">
<div class="article-summary-box-inner">
<span><p>We present a debiased dataset for the Person-centric Visual Grounding (PCVG)
task first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an
image and a caption, PCVG requires pairing up a person's name mentioned in a
caption with a bounding box that points to the person in the image. We find
that the original Who's Waldo dataset compiled for this task contains a large
number of biased samples that are solvable simply by heuristic methods; for
instance, in many cases the first name in the sentence corresponds to the
largest bounding box, or the sequence of names in the sentence corresponds to
an exact left-to-right order in the image. Naturally, models trained on these
biased data lead to over-estimation of performance on the benchmark. To enforce
models being correct for the correct reasons, we design automated tools to
filter and debias the original dataset by ruling out all examples of
insufficient context, such as those with no verb or with a long chain of
conjunct names in their captions. Our experiments show that our new sub-sampled
dataset contains less bias with much lowered heuristic performances and widened
gaps between heuristic and supervised methods. We also demonstrate the same
benchmark model trained on our debiased training set outperforms that trained
on the original biased (and larger) training set on our debiased test set. We
argue our debiased dataset offers the PCVG task a more practical baseline for
reliable benchmarking and future improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings. (arXiv:2203.16685v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16685">
<div class="article-summary-box-inner">
<span><p>This paper presents a streaming speaker-attributed automatic speech
recognition (SA-ASR) model that can recognize "who spoke what" with low latency
even when multiple people are speaking simultaneously. Our model is based on
token-level serialized output training (t-SOT) which was recently proposed to
transcribe multi-talker speech in a streaming fashion. To further recognize
speaker identities, we propose an encoder-decoder based speaker embedding
extractor that can estimate a speaker representation for each recognized token
not only from non-overlapping speech but also from overlapping speech. The
proposed speaker embedding, named t-vector, is extracted synchronously with the
t-SOT ASR model, enabling joint execution of speaker identification (SID) or
speaker diarization (SD) with the multi-talker transcription with low latency.
We evaluate the proposed model for a joint task of ASR and SID/SD by using
LibriSpeechMix and LibriCSS corpora. The proposed model achieves substantially
better accuracy than a prior streaming model and shows comparable or sometimes
even superior results to the state-of-the-art offline SA-ASR model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAE-AST: Masked Autoencoding Audio Spectrogram Transformer. (arXiv:2203.16691v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16691">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a simple yet powerful improvement over the recent
Self-Supervised Audio Spectrogram Transformer (SSAST) model for speech and
audio classification. Specifically, we leverage the insight that the SSAST uses
a very high masking ratio (75%) during pretraining, meaning that the vast
majority of self-attention compute is performed on mask tokens. We address this
by integrating the encoder-decoder architecture from Masked Autoencoders are
Scalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on
only unmasked input, and a shallow decoder operates on encoder outputs and mask
tokens. We find that MAE-like pretraining can provide a 3x speedup and 2x
memory usage reduction over the vanilla SSAST using current audio pretraining
strategies with ordinary model and input sizes. When fine-tuning on downstream
tasks, which only uses the encoder, we find that our approach outperforms the
SSAST on a variety of downstream tasks. We further conduct comprehensive
evaluations into different strategies of pretraining and explore differences in
MAE-style pretraining between the visual and audio domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Disentangled Variational Speech Representation Learning for Zero-shot Voice Conversion. (arXiv:2203.16705v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16705">
<div class="article-summary-box-inner">
<span><p>Traditional studies on voice conversion (VC) have made progress with parallel
training data and known speakers. Good voice conversion quality is obtained by
exploring better alignment modules or expressive mapping functions. In this
study, we investigate zero-shot VC from a novel perspective of self-supervised
disentangled speech representation learning. Specifically, we achieve the
disentanglement by balancing the information flow between global speaker
representation and time-varying content representation in a sequential
variational autoencoder (VAE). A zero-shot voice conversion is performed by
feeding an arbitrary speaker embedding and content embeddings to the VAE
decoder. Besides that, an on-the-fly data augmentation training strategy is
applied to make the learned representation noise invariant. On TIMIT and VCTK
datasets, we achieve state-of-the-art performance on both objective evaluation,
i.e., speaker verification (SV) on speaker embedding and content embedding, and
subjective evaluation, i.e., voice naturalness and similarity, and remains to
be robust even with noisy source/target utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Table Question Answering via Retrieval-Augmented Generation. (arXiv:2203.16714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16714">
<div class="article-summary-box-inner">
<span><p>Most existing end-to-end Table Question Answering (Table QA) models consist
of a two-stage framework with a retriever to select relevant table candidates
from a corpus and a reader to locate the correct answers from table candidates.
Even though the accuracy of the reader models is significantly improved with
the recent transformer-based approaches, the overall performance of such
frameworks still suffers from the poor accuracy of using traditional
information retrieval techniques as retrievers. To alleviate this problem, we
introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense
vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence
model to generate answer tokens. Given any natural language question, T-RAG
utilizes a unified pipeline to automatically search through a table corpus to
directly locate the correct answer from the table cells. We apply T-RAG to
recent open-domain Table QA benchmarks and demonstrate that the fine-tuned
T-RAG model is able to achieve state-of-the-art performance in both the
end-to-end Table QA and the table retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving speaker de-identification with functional data analysis of f0 trajectories. (arXiv:2203.16738v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16738">
<div class="article-summary-box-inner">
<span><p>Due to a constantly increasing amount of speech data that is stored in
different types of databases, voice privacy has become a major concern. To
respond to such concern, speech researchers have developed various methods for
speaker de-identification. The state-of-the-art solutions utilize deep learning
solutions which can be effective but might be unavailable or impractical to
apply for, for example, under-resourced languages. Formant modification is a
simpler, yet effective method for speaker de-identification which requires no
training data. Still, remaining intonational patterns in formant-anonymized
speech may contain speaker-dependent cues. This study introduces a novel
speaker de-identification method, which, in addition to simple formant shifts,
manipulates f0 trajectories based on functional data analysis. The proposed
speaker de-identification method will conceal plausibly identifying pitch
characteristics in a phonetically controllable manner and improve formant-based
speaker de-identification up to 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis. (arXiv:2203.16747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16747">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a trend to investigate the factual knowledge
captured by Pre-trained Language Models (PLMs). Many works show the PLMs'
ability to fill in the missing factual words in cloze-style prompts such as
"Dante was born in [MASK]." However, it is still a mystery how PLMs generate
the results correctly: relying on effective clues or shortcut patterns? We try
to answer this question by a causal-inspired analysis that quantitatively
measures and evaluates the word-level patterns that PLMs depend on to generate
the missing words. We check the words that have three typical associations with
the missing words: knowledge-dependent, positionally close, and highly
co-occurred. Our analysis shows: (1) PLMs generate the missing factual words
more by the positionally close and highly co-occurred words than the
knowledge-dependent words; (2) the dependence on the knowledge-dependent words
is more effective than the positionally close and highly co-occurred words.
Accordingly, we conclude that the PLMs capture the factual knowledge
ineffectively because of depending on the inadequate associations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Single-Channel Speech for Multi-Channel End-to-End Speech Recognition: A Comparative Study. (arXiv:2203.16757v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16757">
<div class="article-summary-box-inner">
<span><p>Recently, the end-to-end training approach for multi-channel ASR has shown
its effectiveness, which usually consists of a beamforming front-end and a
recognition back-end. However, the end-to-end training becomes more difficult
due to the integration of multiple modules, particularly considering that
multi-channel speech data recorded in real environments are limited in size.
This raises the demand to exploit the single-channel data for multi-channel
end-to-end ASR. In this paper, we systematically compare the performance of
three schemes to exploit external single-channel data for multi-channel
end-to-end ASR, namely back-end pre-training, data scheduling, and data
simulation, under different settings such as the sizes of the single-channel
data and the choices of the front-end. Extensive experiments on CHiME-4 and
AISHELL-4 datasets demonstrate that while all three methods improve the
multi-channel end-to-end speech recognition performance, data simulation
outperforms the other two, at the cost of longer training time. Data scheduling
outperforms back-end pre-training marginally but nearly consistently,
presumably because that in the pre-training stage, the back-end tends to
overfit on the single-channel data, especially when the single-channel data
size is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR. (arXiv:2203.16758v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16758">
<div class="article-summary-box-inner">
<span><p>History and future contextual information are known to be important for
accurate acoustic modeling. However, acquiring future context brings latency
for streaming ASR. In this paper, we propose a new framework - Chunking,
Simulating Future Context and Decoding (CUSIDE) for streaming speech
recognition. A new simulation module is introduced to recursively simulate the
future contextual frames, without waiting for future context. The simulation
module is jointly trained with the ASR model using a self-supervised loss; the
ASR model is optimized with the usual ASR loss, e.g., CTC-CRF as used in our
experiments. Experiments show that, compared to using real future frames as
right context, using simulated future context can drastically reduce latency
while maintaining recognition accuracy. With CUSIDE, we obtain new
state-of-the-art streaming ASR results on the AISHELL-1 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16773">
<div class="article-summary-box-inner">
<span><p>Speech representations learned from Self-supervised learning (SSL) models
have been found beneficial for various speech processing tasks. However,
utilizing SSL representations usually requires fine-tuning the pre-trained
models or designing task-specific downstream models and loss functions, causing
much memory usage and human labor. On the other hand, prompting in Natural
Language Processing (NLP) is an efficient and widely used technique to leverage
pre-trained language models (LMs). Nevertheless, such a paradigm is little
studied in the speech community. We report in this paper the first exploration
of the prompt tuning paradigm for speech processing tasks based on Generative
Spoken Language Model (GSLM). Experiment results show that the prompt tuning
technique achieves competitive performance in speech classification tasks with
fewer trainable parameters than fine-tuning specialized downstream models. We
further study the technique in challenging sequence generation tasks. Prompt
tuning also demonstrates its potential, while the limitation and possible
research directions are discussed in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bangla hate speech detection on social media using attention-based recurrent neural network. (arXiv:2203.16775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16775">
<div class="article-summary-box-inner">
<span><p>Hate speech has spread more rapidly through the daily use of technology and,
most notably, by sharing your opinions or feelings on social media in a
negative aspect. Although numerous works have been carried out in detecting
hate speeches in English, German, and other languages, very few works have been
carried out in the context of the Bengali language. In contrast, millions of
people communicate on social media in Bengali. The few existing works that have
been carried out need improvements in both accuracy and interpretability. This
article proposed encoder decoder based machine learning model, a popular tool
in NLP, to classify user's Bengali comments on Facebook pages. A dataset of
7,425 Bengali comments, consisting of seven distinct categories of hate
speeches, was used to train and evaluate our model. For extracting and encoding
local features from the comments, 1D convolutional layers were used. Finally,
the attention mechanism, LSTM, and GRU based decoders have been used for
predicting hate speech categories. Among the three encoder decoder algorithms,
the attention-based decoder obtained the best accuracy (77%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Language Model Integration for Transducer based Speech Recognition. (arXiv:2203.16776v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16776">
<div class="article-summary-box-inner">
<span><p>Utilizing text-only data with an external language model (LM) in end-to-end
RNN-Transducer (RNN-T) for speech recognition is challenging. Recently, a class
of methods such as density ratio (DR) and ILM estimation (ILME) have been
developed, outperforming the classic shallow fusion (SF) method. The basic idea
behind these methods is that RNN-T posterior should first subtract the
implicitly learned ILM prior, in order to integrate the external LM. While
recent studies suggest that RNN-T only learns some low-order language model
information, the DR method uses a well-trained ILM. We hypothesize that this
setting is appropriate and may deteriorate the performance of the DR method,
and propose a low-order density ratio method (LODR) by training a low-order
weak ILM for DR. Extensive empirical experiments are conducted on both
in-domain and cross-domain scenarios on English LibriSpeech &amp; Tedlium-2 and
Chinese WenetSpeech &amp; AISHELL-1 datasets. It is shown that LODR consistently
outperforms SF in all tasks, while performing generally close to ILME and
better than DR in most tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misogynistic Meme Detection using Early Fusion Model with Graph Network. (arXiv:2203.16781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16781">
<div class="article-summary-box-inner">
<span><p>In recent years , there has been an upsurge in a new form of entertainment
medium called memes. These memes although seemingly innocuous have transcended
onto the boundary of online harassment against women and created an unwanted
bias against them . To help alleviate this problem , we propose an early fusion
model for prediction and identification of misogynistic memes and its type in
this paper for which we participated in SemEval-2022 Task 5 . The model
receives as input meme image with its text transcription with a target vector.
Given that a key challenge with this task is the combination of different
modalities to predict misogyny, our model relies on pretrained contextual
representations from different state-of-the-art transformer-based language
models and pretrained image pretrained models to get an effective image
representation. Our model achieved competitive results on both SubTask-A and
SubTask-B with the other competition teams and significantly outperforms the
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices. (arXiv:2203.16788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16788">
<div class="article-summary-box-inner">
<span><p>Environmental, Social, and Governance (ESG) are non-financial factors that
are garnering attention from investors as they increasingly look to apply these
as part of their analysis to identify material risks and growth opportunities.
Some of this attention is also driven by clients who, now more aware than ever,
are demanding for their money to be managed and invested responsibly. As the
interest in ESG grows, so does the need for investors to have access to
consumable ESG information. Since most of it is in text form in reports,
disclosures, press releases, and 10-Q filings, we see a need for sophisticated
NLP techniques for classification tasks for ESG text. We hypothesize that an
ESG domain-specific pre-trained model will help with such and study building of
the same in this paper. We explored doing this by fine-tuning BERTs pre-trained
weights using ESG specific text and then further fine-tuning the model for a
classification task. We were able to achieve accuracy better than the original
BERT and baseline models in environment-specific classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMER: Multimodal Multi-task learning for Emotion Recognition in Spoken Utterances. (arXiv:2203.16794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16794">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition (ER) aims to classify human utterances into different
emotion categories. Based on early-fusion and self-attention-based multimodal
interaction between text and acoustic modalities, in this paper, we propose a
multimodal multitask learning approach for ER from individual utterances in
isolation. Experiments on the IEMOCAP benchmark show that our proposed model
performs better than our re-implementation of state-of-the-art and achieves
better performance than all other unimodal and multimodal approaches in
literature. In addition, strong baselines and ablation studies prove the
effectiveness of our proposed approach. We make all our codes publicly
available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Discourse Aware Sequence Learning Approach for Emotion Recognition in Conversations. (arXiv:2203.16799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16799">
<div class="article-summary-box-inner">
<span><p>The expression of emotions is a crucial part of daily human communication.
Modeling the conversational and sequential context has seen much success and
plays a vital role in Emotion Recognition in Conversations (ERC). However,
existing approaches either model only one of the two or employ naive
late-fusion methodologies to obtain final utterance representations. This paper
proposes a novel idea to incorporate both these contexts and better model the
intrinsic structure within a conversation. More precisely, we propose a novel
architecture boosted by a modified LSTM cell, which we call DiscLSTM, that
better captures the interaction between conversational and sequential context.
DiscLSTM brings together the best of both worlds and provides a more intuitive
and efficient way to model the information flow between individual utterances
by better capturing long-distance conversational background through discourse
relations and sequential context through recurrence. We conduct experiments on
four benchmark datasets for ERC and show that our model achieves performance
competitive to state-of-the-art and at times performs better than other
graph-based approaches in literature, with a conversational graph that is both
sparse and avoids complicated edge relations like much of previous work. We
make all our codes publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BRIO: Bringing Order to Abstractive Summarization. (arXiv:2203.16804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16804">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization models are commonly trained using maximum
likelihood estimation, which assumes a deterministic (one-point) target
distribution in which an ideal model will assign all the probability mass to
the reference summary. This assumption may lead to performance degradation
during inference, where the model needs to compare several system-generated
(candidate) summaries that have deviated from the reference summary. To address
this problem, we propose a novel training paradigm which assumes a
non-deterministic distribution so that different candidate summaries are
assigned probability mass according to their quality. Our method achieves a new
state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07
ROUGE-1) datasets. Further analysis also shows that our model can estimate
probabilities of candidate summaries that are more correlated with their level
of quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Pre-trained Wav2Vec2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. (arXiv:2203.16822v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16822">
<div class="article-summary-box-inner">
<span><p>Recent work on self-supervised pre-training focus on leveraging large-scale
unlabeled speech data to build robust end-to-end (E2E) acoustic models (AM)
that can be later fine-tuned on downstream tasks e.g., automatic speech
recognition (ASR). Yet, few works investigated the impact on performance when
the data substantially differs between the pre-training and downstream
fine-tuning phases (i.e., domain shift). We target this scenario by analyzing
the robustness of Wav2Vec2.0 and XLS-R models on downstream ASR for a
completely unseen domain, i.e., air traffic control (ATC) communications. We
benchmark the proposed models on four challenging ATC test sets
(signal-to-noise ratio varies between 5 to 20 dB). Relative word error rate
(WER) reduction between 20% to 40% are obtained in comparison to hybrid-based
state-of-the-art ASR baselines by fine-tuning E2E acoustic models with a small
fraction of labeled data. We also study the impact of fine-tuning data size on
WERs, going from 5 minutes (few-shot) to 15 hours.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of text to speech pseudo labels for forced alignment and cross lingual pretrained models for low resource speech recognition. (arXiv:2203.16823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16823">
<div class="article-summary-box-inner">
<span><p>In the recent years end to end (E2E) automatic speech recognition (ASR)
systems have achieved promising results given sufficient resources. Even for
languages where not a lot of labelled data is available, state of the art E2E
ASR systems can be developed by pretraining on huge amounts of high resource
languages and finetune on low resource languages. For a lot of low resource
languages the current approaches are still challenging, since in many cases
labelled data is not available in open domain. In this paper we present an
approach to create labelled data for Maithili, Bhojpuri and Dogri by utilising
pseudo labels from text to speech for forced alignment. The created data was
inspected for quality and then further used to train a transformer based
wav2vec 2.0 ASR model. All data and models are available in open domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">indic-punct: An automatic punctuation restoration and inverse text normalization framework for Indic languages. (arXiv:2203.16825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16825">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) generates text which is most of the times
devoid of any punctuation. Absence of punctuation is text can affect
readability. Also, down stream NLP tasks such as sentiment analysis, machine
translation, greatly benefit by having punctuation and sentence boundary
information. We present an approach for automatic punctuation of text using a
pretrained IndicBERT model. Inverse text normalization is done by hand writing
weighted finite state transducer (WFST) grammars. We have developed this tool
for 11 Indic languages namely Hindi, Tamil, Telugu, Kannada, Gujarati, Marathi,
Odia, Bengali, Assamese, Malayalam and Punjabi. All code and data is publicly.
available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings. (arXiv:2203.16834v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16834">
<div class="article-summary-box-inner">
<span><p>In this paper, we conduct a comparative study on speaker-attributed automatic
speech recognition (SA-ASR) in the multi-party meeting scenario, a topic with
increasing attention in meeting rich transcription. Specifically, three
approaches are evaluated in this study. The first approach, FD-SOT, consists of
a frame-level diarization model to identify speakers and a multi-talker ASR to
recognize utterances. The speaker-attributed transcriptions are obtained by
aligning the diarization results and recognized hypotheses. However, such an
alignment strategy may suffer from erroneous timestamps due to the modular
independence, severely hindering the model performance. Therefore, we propose
the second approach, WD-SOT, to address alignment errors by introducing a
word-level diarization model, which can get rid of such timestamp alignment
dependency. To further mitigate the alignment issues, we propose the third
approach, TS-ASR, which trains a target-speaker separation module and an ASR
module jointly. By comparing various strategies for each SA-ASR approach,
experimental results on a real meeting scenario corpus, AliMeeting, reveal that
the WD-SOT approach achieves 10.7% relative reduction on averaged
speaker-dependent character error rate (SD-CER), compared with the FD-SOT
approach. In addition, the TS-ASR approach also outperforms the FD-SOT approach
and brings 16.5% relative average SD-CER reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset. (arXiv:2203.16844v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16844">
<div class="article-summary-box-inner">
<span><p>This paper introduces a high-quality rich annotated Mandarin conversational
(RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains
180 hours of conversational speech data recorded from native speakers of
Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs
in MagicData-RAMC are classified into 15 diversified domains and tagged with
topic labels, ranging from science and technology to ordinary life. Accurate
transcription and precise speaker voice activity timestamps are manually
labeled for each sample. Speakers' detailed information is also provided. As a
Mandarin speech dataset designed for dialog scenarios with high quality and
rich annotations, MagicData-RAMC enriches the data diversity in the Mandarin
speech community and allows extensive research on a series of speech-related
tasks, including automatic speech recognition, speaker diarization, topic
detection, keyword search, text-to-speech, etc. We also conduct several
relevant tasks and provide experimental results to help evaluate the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Efficient Training of RNN-Transducer with Sampled Softmax. (arXiv:2203.16868v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16868">
<div class="article-summary-box-inner">
<span><p>RNN-Transducer has been one of promising architectures for end-to-end
automatic speech recognition. Although RNN-Transducer has many advantages
including its strong accuracy and streaming-friendly property, its high memory
consumption during training has been a critical problem for development. In
this work, we propose to apply sampled softmax to RNN-Transducer, which
requires only a small subset of vocabulary during training thus saves its
memory consumption. We further extend sampled softmax to optimize memory
consumption for a minibatch, and employ distributions of auxiliary CTC losses
for sampling vocabulary to improve model accuracy. We present experimental
results on LibriSpeech, AISHELL-1, and CSJ-APS, where sampled softmax greatly
reduces memory consumption and still maintains the accuracy of the baseline
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A bilingual approach to specialised adjectives through word embeddings in the karstology domain. (arXiv:2203.16885v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16885">
<div class="article-summary-box-inner">
<span><p>We present an experiment in extracting adjectives which express a specific
semantic relation using word embeddings. The results of the experiment are then
thoroughly analysed and categorised into groups of adjectives exhibiting formal
or semantic similarity. The experiment and analysis are performed for English
and Croatian in the domain of karstology using data sets and methods developed
in the TermFrame project. The main original contributions of the article are
twofold: firstly, proposing a new and promising method of extracting
semantically related words relevant for terminology, and secondly, providing a
detailed evaluation of the output so that we gain a better understanding of the
domain-specific semantic structures on the one hand and the types of
similarities extracted by word embeddings on the other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A survey of neural models for the automatic analysis of conversation: Towards a better integration of the social sciences. (arXiv:2203.16891v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16891">
<div class="article-summary-box-inner">
<span><p>Some exciting new approaches to neural architectures for the analysis of
conversation have been introduced over the past couple of years. These include
neural architectures for detecting emotion, dialogue acts, and sentiment
polarity. They take advantage of some of the key attributes of contemporary
machine learning, such as recurrent neural networks with attention mechanisms
and transformer-based approaches. However, while the architectures themselves
are extremely promising, the phenomena they have been applied to to date are
but a small part of what makes conversation engaging. In this paper we survey
these neural architectures and what they have been applied to. On the basis of
the social science literature, we then describe what we believe to be the most
fundamental and definitional feature of conversation, which is its
co-construction over time by two or more interlocutors. We discuss how neural
architectures of the sort surveyed could profitably be applied to these more
fundamental aspects of conversation, and what this buys us in terms of a better
analysis of conversation and even, in the longer term, a better way of
generating conversation for a conversational system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Character-level Span-based Model for Mandarin Prosodic Structure Prediction. (arXiv:2203.16922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16922">
<div class="article-summary-box-inner">
<span><p>The accuracy of prosodic structure prediction is crucial to the naturalness
of synthesized speech in Mandarin text-to-speech system, but now is limited by
widely-used sequence-to-sequence framework and error accumulation from previous
word segmentation results. In this paper, we propose a span-based Mandarin
prosodic structure prediction model to obtain an optimal prosodic structure
tree, which can be converted to corresponding prosodic label sequence. Instead
of the prerequisite for word segmentation, rich linguistic features are
provided by Chinese character-level BERT and sent to encoder with
self-attention architecture. On top of this, span representation and label
scoring are used to describe all possible prosodic structure trees, of which
each tree has its corresponding score. To find the optimal tree with the
highest score for a given sentence, a bottom-up CKY-style algorithm is further
used. The proposed method can predict prosodic labels of different levels at
the same time and accomplish the process directly from Chinese characters in an
end-to-end manner. Experiment results on two real-world datasets demonstrate
the excellent performance of our span-based method over all
sequence-to-sequence baseline approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aplica\c{c}\~ao de ros como ferramenta de ensino a rob\'otica / using ros as a robotics teaching tool. (arXiv:2203.16923v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16923">
<div class="article-summary-box-inner">
<span><p>The study of robotic manipulators is the main goal of Industrial Robotics
Class, part of Control Engineers training course. There is a difficulty in
preparing academic practices and projects in the area of robotics due to the
high cost of specific educational equipment. The practical classes and the
development of projects are very important for engineers training, it is
proposed to use simulation software in order to provide practical experience
for the students of the discipline. In this context, the present article aims
to expose the use of the Robot Operation System (ROS) as a tool to develop a
robotic arm and implement the functionality of forward and inverse kinematics.
Such development could be used as an educational tool to increase the interest
and learning of students in the robotics discipline and to expand research
areas for the discipline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation for Sparse-Data Settings: What Do We Gain by Not Using Bert?. (arXiv:2203.16926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16926">
<div class="article-summary-box-inner">
<span><p>The practical success of much of NLP depends on the availability of training
data. However, in real-world scenarios, training data is often scarce, not
least because many application domains are restricted and specific. In this
work, we compare different methods to handle this problem and provide
guidelines for building NLP applications when there is only a small amount of
labeled training data available for a specific domain. While transfer learning
with pre-trained language models outperforms other methods across tasks,
alternatives do not perform much worse while requiring much less computational
effort, thus significantly reducing monetary and environmental cost. We examine
the performance tradeoffs of several such alternatives, including models that
can be trained up to 175K times faster and do not require a single GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying PBL in the Development and Modeling of kinematics for Robotic Manipulators with Interdisciplinarity between Computer-Assisted Project, Robotics, and Microcontrollers. (arXiv:2203.16927v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16927">
<div class="article-summary-box-inner">
<span><p>Considering the difficulty of students in calculating the direct and inverse
kinematics of a robotic manipulator using only conventional tools of a
classroom, this article proposes the application of Project Based Learning
(ABP) through the design, development, mathematical modeling of a robotic
manipulator as an integrative project of the disciplines of Industrial
Robotics, Microcontrollers and Computer Assisted Design with students of the
Control and Automation Engineering of the University of Fortaleza. Once
designed and machined, the manipulator arm was assembled using servo motors
connected to a microcontroled prototyping board, to then have its kinematics
calculated. At the end are presented the results that the project has brought
to the learning of the disciplines on the optics of the tutor and students.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Search for Speech Emotion Recognition. (arXiv:2203.16928v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16928">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have brought significant advancements to speech emotion
recognition (SER). However, the architecture design in SER is mainly based on
expert knowledge and empirical (trial-and-error) evaluations, which is
time-consuming and resource intensive. In this paper, we propose to apply
neural architecture search (NAS) techniques to automatically configure the SER
models. To accelerate the candidate architecture optimization, we propose a
uniform path dropout strategy to encourage all candidate architecture
operations to be equally optimized. Experimental results of two different
neural structures on IEMOCAP show that NAS can improve SER performance (54.89\%
to 56.28\%) while maintaining model parameter sizes. The proposed dropout
strategy also shows superiority over the previous approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WavThruVec: Latent speech representation as intermediate features for neural speech synthesis. (arXiv:2203.16930v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16930">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural text-to-speech research have been dominated by
two-stage pipelines utilizing low-level intermediate speech representation such
as mel-spectrograms. However, such predetermined features are fundamentally
limited, because they do not allow to exploit the full potential of a
data-driven approach through learning hidden representations. For this reason,
several end-to-end methods have been proposed. However, such models are harder
to train and require a large number of high-quality recordings with
transcriptions. Here, we propose WavThruVec - a two-stage architecture that
resolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as
intermediate speech representation. Since these hidden activations provide
high-level linguistic features, they are more robust to noise. That allows us
to utilize annotated speech datasets of a lower quality to train the
first-stage module. At the same time, the second-stage component can be trained
on large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are
time-aligned and speaker-independent. This results in an increased
generalization capability to out-of-vocabulary words, as well as to a better
generalization to unseen speakers. We show that the proposed model not only
matches the quality of state-of-the-art neural models, but also presents useful
properties enabling tasks like voice conversion or zero-shot synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-end Chinese Text Normalization Model based on Rule-guided Flat-Lattice Transformer. (arXiv:2203.16954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16954">
<div class="article-summary-box-inner">
<span><p>Text normalization, defined as a procedure transforming non standard words to
spoken-form words, is crucial to the intelligibility of synthesized speech in
text-to-speech system. Rule-based methods without considering context can not
eliminate ambiguation, whereas sequence-to-sequence neural network based
methods suffer from the unexpected and uninterpretable errors problem. Recently
proposed hybrid system treats rule-based model and neural model as two cascaded
sub-modules, where limited interaction capability makes neural network model
cannot fully utilize expert knowledge contained in the rules. Inspired by
Flat-LAttice Transformer (FLAT), we propose an end-to-end Chinese text
normalization model, which accepts Chinese characters as direct input and
integrates expert knowledge contained in rules into the neural network, both
contribute to the superior performance of proposed model for the text
normalization task. We also release a first publicly accessible largescale
dataset for Chinese text normalization. Our proposed model has achieved
excellent results on this dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16965">
<div class="article-summary-box-inner">
<span><p>While self-supervised speech representation learning (SSL) models serve a
variety of downstream tasks, these models have been observed to overfit to the
domain from which the unlabelled data originates. To alleviate this issue, we
propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant
weights from models pre-trained on large amounts of out-of-domain (OOD) data.
Intuitively, this helps to make space for the target-domain ASR finetuning. The
redundant weights can be identified through various pruning strategies which
have been discussed in detail as a part of this work. Specifically, we
investigate the effect of the recently discovered Task-Agnostic and Task-Aware
pruning on PADA and propose a new pruning paradigm based on the latter, which
we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial
pruning mask from a well fine-tuned OOD model, which makes it starkly different
from the rest of the pruning strategies discussed in the paper. Our proposed
CD-TAW methodology achieves up to 20.6% relative WER improvement over our
baseline when fine-tuned on a 2-hour subset of Switchboard data without
language model (LM) decoding. Furthermore, we conduct a detailed analysis to
highlight the key design choices of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16973">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) to learn high-level speech representations has
been a popular approach to building Automatic Speech Recognition (ASR) systems
in low-resource settings. However, the common assumption made in literature is
that a considerable amount of unlabeled data is available for the same domain
or language that can be leveraged for SSL pre-training, which we acknowledge is
not feasible in a real-world setting. In this paper, as part of the Interspeech
Gram Vaani ASR challenge, we try to study the effect of domain, language,
dataset size, and other aspects of our upstream pre-training SSL data on the
final performance low-resource downstream ASR task. We also build on the
continued pre-training paradigm to study the effect of prior knowledge
possessed by models trained using SSL. Extensive experiments and studies reveal
that the performance of ASR systems is susceptible to the data used for SSL
pre-training. Their performance improves with an increase in similarity and
volume of pre-training data. We believe our work will be helpful to the speech
community in building better ASR systems in low-resource settings and steer
research towards improving generalization in SSL-based pre-training for speech
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial Coupling of Optimal Transport for Spoken Language Identification. (arXiv:2203.17036v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17036">
<div class="article-summary-box-inner">
<span><p>In order to reduce domain discrepancy to improve the performance of
cross-domain spoken language identification (SLID) system, as an unsupervised
domain adaptation (UDA) method, we have proposed a joint distribution alignment
(JDA) model based on optimal transport (OT). A discrepancy measurement based on
OT was adopted for JDA between training and test data sets. In our previous
study, it was supposed that the training and test sets share the same label
space. However, in real applications, the label space of the test set is only a
subset of that of the training set. Fully matching training and test domains
for distribution alignment may introduce negative domain transfer. In this
paper, we propose an JDA model based on partial optimal transport (POT), i.e.,
only partial couplings of OT are allowed during JDA. Moreover, since the label
of test data is unknown, in the POT, a soft weighting on the coupling based on
transport cost is adaptively set during domain alignment. Experiments were
carried out on a cross-domain SLID task to evaluate the proposed UDA. Results
showed that our proposed UDA significantly improved the performance due to the
consideration of the partial couplings in OT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manipulation of oral cancer speech using neural articulatory synthesis. (arXiv:2203.17072v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17072">
<div class="article-summary-box-inner">
<span><p>We present an articulatory synthesis framework for the synthesis and
manipulation of oral cancer speech for clinical decision making and alleviation
of patient stress. Objective and subjective evaluations demonstrate that the
framework has acceptable naturalness and is worth further investigation. A
subsequent subjective vowel and consonant identification experiment showed that
the articulatory synthesis system can manipulate the articulatory trajectories
so that the synthesised speech reproduces problems present in the ground truth
oral cancer speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientific and Technological Text Knowledge Extraction Method of based on Word Mixing and GRU. (arXiv:2203.17079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17079">
<div class="article-summary-box-inner">
<span><p>The knowledge extraction task is to extract triple relations (head
entity-relation-tail entity) from unstructured text data. The existing
knowledge extraction methods are divided into "pipeline" method and joint
extraction method. The "pipeline" method is to separate named entity
recognition and entity relationship extraction and use their own modules to
extract them. Although this method has better flexibility, the training speed
is slow. The learning model of joint extraction is an end-to-end model
implemented by neural network to realize entity recognition and relationship
extraction at the same time, which can well preserve the association between
entities and relationships, and convert the joint extraction of entities and
relationships into a sequence annotation problem. In this paper, we propose a
knowledge extraction method for scientific and technological resources based on
word mixture and GRU, combined with word mixture vector mapping method and
self-attention mechanism, to effectively improve the effect of text
relationship extraction for Chinese scientific and technological resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretation of Black Box NLP Models: A Survey. (arXiv:2203.17081v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17081">
<div class="article-summary-box-inner">
<span><p>An increasing number of machine learning models have been deployed in domains
with high stakes such as finance and healthcare. Despite their superior
performances, many models are black boxes in nature which are hard to explain.
There are growing efforts for researchers to develop methods to interpret these
black-box models. Post hoc explanations based on perturbations, such as LIME,
are widely used approaches to interpret a machine learning model after it has
been built. This class of methods has been shown to exhibit large instability,
posing serious challenges to the effectiveness of the method itself and harming
user trust. In this paper, we propose S-LIME, which utilizes a hypothesis
testing framework based on central limit theorem for determining the number of
perturbation points needed to guarantee stability of the resulting explanation.
Experiments on both simulated and real world data sets are provided to
demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PANGUBOT: Efficient Generative Dialogue Pre-training from Pre-trained Language Model. (arXiv:2203.17090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17090">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce PANGUBOT, a Chinese pre-trained open-domain
dialogue generation model based on a large pre-trained language model (PLM)
PANGU-alpha (Zeng et al.,2021). Different from other pre-trained dialogue
models trained over a massive amount of dialogue data from scratch, we aim to
build a powerful dialogue model with relatively fewer data and computation
costs by inheriting valuable language capabilities and knowledge from PLMs. To
this end, we train PANGUBOT from the large PLM PANGU-alpha, which has been
proven well-performed on a variety of Chinese natural language tasks. We
investigate different aspects of responses generated by PANGUBOT, including
response quality, knowledge, and safety. We show that PANGUBOT outperforms
state-of-the-art Chinese dialogue systems (CDIALGPT (Wang et al., 2020), EVA
(Zhou et al., 2021)) w.r.t. the above three aspects. We also demonstrate that
PANGUBOT can be easily deployed to generate emotional responses without further
training. Throughout our empirical analysis, we also point out that the
PANGUBOT response quality, knowledge correctness, and safety are still far from
perfect, and further explorations are indispensable to building reliable and
smart dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$k$NN-NER: Named Entity Recognition with Nearest Neighbor Search. (arXiv:2203.17103v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17103">
<div class="article-summary-box-inner">
<span><p>Inspired by recent advances in retrieval augmented methods in
NLP~\citep{khandelwal2019generalization,khandelwal2020nearest,meng2021gnn}, in
this paper, we introduce a $k$ nearest neighbor NER ($k$NN-NER) framework,
which augments the distribution of entity labels by assigning $k$ nearest
neighbors retrieved from the training set. This strategy makes the model more
capable of handling long-tail cases, along with better few-shot learning
abilities. $k$NN-NER requires no additional operation during the training
phase, and by interpolating $k$ nearest neighbors search into the vanilla NER
model, $k$NN-NER consistently outperforms its vanilla counterparts: we achieve
a new state-of-the-art F1-score of 72.03 (+1.25) on the Chinese Weibo dataset
and improved results on a variety of widely used NER benchmarks. Additionally,
we show that $k$NN-NER can achieve comparable results to the vanilla NER model
with 40\% less amount of training data. Code available at
\url{https://github.com/ShannonAI/KNN-NER}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of Acoustic Noise on Alzheimer's Disease Detection from Speech: Should You Let Baby Cry?. (arXiv:2203.17110v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17110">
<div class="article-summary-box-inner">
<span><p>Research related to automatically detecting Alzheimer's disease (AD) is
important, given the high prevalence of AD and the high cost of traditional
methods. Since AD significantly affects the acoustics of spontaneous speech,
speech processing and machine learning (ML) provide promising techniques for
reliably detecting AD. However, speech audio may be affected by different types
of background noise and it is important to understand how the noise influences
the accuracy of ML models detecting AD from speech. In this paper, we study the
effect of fifteen types of noise from five different categories on the
performance of four ML models trained with three types of acoustic
representations. We perform a thorough analysis showing how ML models and
acoustic features are affected by different types of acoustic noise. We show
that acoustic noise is not necessarily harmful - certain types of noise are
beneficial for AD detection models and help increasing accuracy by up to 4.8\%.
We provide recommendations on how to utilize acoustic noise in order to achieve
the best performance results with the ML models deployed in real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17152">
<div class="article-summary-box-inner">
<span><p>Speech enhancement (SE) performance has improved considerably since the use
of deep learning (DL) models as a base function. In this study, we propose a
perceptual contrast stretching (PCS) approach to further improve SE
performance. PCS is derived based on the critical band importance function and
applied to modify the targets of the SE model. Specifically, PCS stretches the
contract of target features according to perceptual importance, thereby
improving the overall SE performance. Compared to post-processing based
implementations, incorporating PCS into the training phase preserves
performance and reduces online computation. It is also worth noting that PCS
can be suitably combined with different SE model architectures and training
criteria. Meanwhile, PCS does not affect the causality or convergence of the SE
model training. Experimental results on the VoiceBank-DEMAND dataset showed
that the proposed method can achieve state-of-the-art performance on both
causal (PESQ=3.07) and non-causal (PESQ=3.35) SE tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evaluation of NLP-based Models for Software Engineering. (arXiv:2203.17166v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17166">
<div class="article-summary-box-inner">
<span><p>NLP-based models have been increasingly incorporated to address SE problems.
These models are either employed in the SE domain with little to no change, or
they are greatly tailored to source code and its unique characteristics. Many
of these approaches are considered to be outperforming or complementing
existing solutions. However, an important question arises here: "Are these
models evaluated fairly and consistently in the SE community?". To answer this
question, we reviewed how NLP-based models for SE problems are being evaluated
by researchers. The findings indicate that currently there is no consistent and
widely-accepted protocol for the evaluation of these models. While different
aspects of the same task are being assessed in different studies, metrics are
defined based on custom choices, rather than a system, and finally, answers are
collected and interpreted case by case. Consequently, there is a dire need to
provide a methodological way of evaluating NLP-based models to have a
consistent assessment and preserve the possibility of fair and efficient
comparison.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$. (arXiv:2203.17189v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17189">
<div class="article-summary-box-inner">
<span><p>Recent neural network-based language models have benefited greatly from
scaling up the size of training datasets and the number of parameters in the
models themselves. Scaling can be complicated due to various factors including
the need to distribute computation on supercomputer clusters (e.g., TPUs),
prevent bottlenecks when infeeding data, and ensure reproducible results. In
this work, we present two software libraries that ease these issues:
$\texttt{t5x}$ simplifies the process of building and training large language
models at scale while maintaining ease of use, and $\texttt{seqio}$ provides a
task-based API for simple creation of fast and reproducible training data and
evaluation pipelines. These open-source libraries have been used to train
models with hundreds of billions of parameters on datasets with multiple
terabytes of training data.
</p>
<p>Along with the libraries, we release configurations and instructions for
T5-like encoder-decoder models as well as GPT-like decoder-only architectures.
</p>
<p>$\texttt{t5x}$ and $\texttt{seqio}$ are open source and available at
https://github.com/google-research/t5x and https://github.com/google/seqio,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech. (arXiv:2203.17190v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17190">
<div class="article-summary-box-inner">
<span><p>Recently, leveraging BERT pre-training to improve the phoneme encoder in text
to speech (TTS) has drawn increasing attention. However, the works apply
pre-training with character-based units to enhance the TTS phoneme encoder,
which is inconsistent with the TTS fine-tuning that takes phonemes as input.
Pre-training only with phonemes as input can alleviate the input mismatch but
lack the ability to model rich representations and semantic information due to
limited phoneme vocabulary. In this paper, we propose MixedPhoneme BERT, a
novel variant of the BERT model that uses mixed phoneme and sup-phoneme
representations to enhance the learning capability. Specifically, we merge the
adjacent phonemes into sup-phonemes and combine the phoneme sequence and the
merged sup-phoneme sequence as the model input, which can enhance the model
capacity to learn rich contextual representations. Experiment results
demonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS
performance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The
Mixed-Phoneme BERT achieves 3x inference speedup and similar voice quality to
the previous TTS pre-trained model PnG BERT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CatIss: An Intelligent Tool for Categorizing Issues Reports using Transformers. (arXiv:2203.17196v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17196">
<div class="article-summary-box-inner">
<span><p>Users use Issue Tracking Systems to keep track and manage issue reports in
their repositories. An issue is a rich source of software information that
contains different reports including a problem, a request for new features, or
merely a question about the software product. As the number of these issues
increases, it becomes harder to manage them manually. Thus, automatic
approaches are proposed to help facilitate the management of issue reports.
</p>
<p>This paper describes CatIss, an automatic CATegorizer of ISSue reports which
is built upon the Transformer-based pre-trained RoBERTa model. CatIss
classifies issue reports into three main categories of Bug reports,
Enhancement/feature requests, and Questions. First, the datasets provided for
the NLBSE tool competition are cleaned and preprocessed. Then, the pre-trained
RoBERTa model is fine-tuned on the preprocessed dataset. Evaluating CatIss on
about 80 thousand issue reports from GitHub, indicates that it performs very
well surpassing the competition baseline, TicketTagger, and achieving 87.2%
F1-score (micro average). Additionally, as CatIss is trained on a wide set of
repositories, it is a generic prediction model, hence applicable for any unseen
software project or projects with little historical data. Scripts for cleaning
the datasets, training CatIss, and evaluating the model are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Wrap-Up Effects through an Information-Theoretic Lens. (arXiv:2203.17213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17213">
<div class="article-summary-box-inner">
<span><p>Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called "wrap-up effects," which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the probability-quality paradox in language generation. (arXiv:2203.17217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17217">
<div class="article-summary-box-inner">
<span><p>When generating natural language from neural probabilistic models, high
probability does not always coincide with high quality: It has often been
observed that mode-seeking decoding methods, i.e., those that produce
high-probability text under the model, lead to unnatural language. On the other
hand, the lower-probability text generated by stochastic methods is perceived
as more human-like. In this note, we offer an explanation for this phenomenon
by analyzing language generation through an information-theoretic lens.
Specifically, we posit that human-like language should contain an amount of
information (quantified as negative log-probability) that is close to the
entropy of the distribution over natural strings. Further, we posit that
language with substantially more (or less) information is undesirable. We
provide preliminary empirical evidence in favor of this hypothesis; quality
ratings of both human and machine-generated text -- covering multiple tasks and
common decoding strategies -- suggest high-quality text has an information
content significantly closer to the entropy than we would expect by chance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Baseline Readability Model for Cebuano. (arXiv:2203.17225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17225">
<div class="article-summary-box-inner">
<span><p>In this study, we developed the first baseline readability model for the
Cebuano language. Cebuano is the second most-used native language in the
Philippines with about 27.5 million speakers. As the baseline, we extracted
traditional or surface-based features, syllable patterns based from Cebuano's
documented orthography, and neural embeddings from the multilingual BERT model.
Results show that the use of the first two handcrafted linguistic features
obtained the best performance trained on an optimized Random Forest model with
approximately 84\% across all metrics. The feature sets and algorithm used also
is similar to previous results in readability assessment for the Filipino
language showing potential of crosslingual application. To encourage more work
for readability assessment in Philippine languages such as Cebuano, we
open-sourced both code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17247">
<div class="article-summary-box-inner">
<span><p>Breakthroughs in transformer-based models have revolutionized not only the
NLP field, but also vision and multimodal systems. However, although
visualization and interpretability tools have become available for NLP models,
internal mechanisms of vision and multimodal transformers remain largely
opaque. With the success of these transformers, it is increasingly critical to
understand their inner workings, as unraveling these black-boxes will lead to
more capable and trustworthy models. To contribute to this quest, we propose
VL-InterpreT, which provides novel interactive visualizations for interpreting
the attentions and hidden representations in multimodal transformers.
VL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety
of statistics in attention heads throughout all layers for both vision and
language components, (2) visualizes cross-modal and intra-modal attentions
through easily readable heatmaps, and (3) plots the hidden representations of
vision and language tokens as they pass through the transformer layers. In this
paper, we demonstrate the functionalities of VL-InterpreT through the analysis
of KD-VLP, an end-to-end pretraining vision-language multimodal
transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and
WebQA, two visual question answering benchmarks. Furthermore, we also present a
few interesting findings about multimodal transformer behaviors that were
learned through our tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17255">
<div class="article-summary-box-inner">
<span><p>This theoretical article examines how to construct human-like working memory
and thought processes within a computer. There should be two working memory
stores, one analogous to sustained firing in association cortex, and one
analogous to synaptic potentiation in the cerebral cortex. These stores must be
constantly updated with new representations that arise from either
environmental stimulation or internal processing. They should be updated
continuously, and in an iterative fashion, meaning that, in the next state,
some items in the set of coactive items should always be retained. Thus, the
set of concepts coactive in working memory will evolve gradually and
incrementally over time. This makes each state is a revised iteration of the
preceding state and causes successive states to overlap and blend with respect
to the set of representations they contain. As new representations are added
and old ones are subtracted, some remain active for several seconds over the
course of these changes. This persistent activity, similar to that used in
artificial recurrent neural networks, is used to spread activation energy
throughout the global workspace to search for the next associative update. The
result is a chain of associatively linked intermediate states that are capable
of advancing toward a solution or goal. Iterative updating is conceptualized
here as an information processing strategy, a computational and
neurophysiological determinant of the stream of thought, and an algorithm for
designing and programming artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v5 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.02704">
<div class="article-summary-box-inner">
<span><p>A character network is a graph extracted from a narrative, in which vertices
represent characters and edges correspond to interactions between them. A
number of narrative-related problems can be addressed automatically through the
analysis of character networks, such as summarization, classification, or role
detection. Character networks are particularly relevant when considering works
of fictions (e.g. novels, plays, movies, TV series), as their exploitation
allows developing information retrieval and recommendation systems. However,
works of fiction possess specific properties making these tasks harder. This
survey aims at presenting and organizing the scientific literature related to
the extraction of character networks from works of fiction, as well as their
analysis. We first describe the extraction process in a generic way, and
explain how its constituting steps are implemented in practice, depending on
the medium of the narrative, the goal of the network analysis, and other
factors. We then review the descriptive tools used to characterize character
networks, with a focus on the way they are interpreted in this context. We
illustrate the relevance of character networks by also providing a review of
applications derived from their analysis. Finally, we identify the limitations
of the existing approaches, and the most promising perspectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09966">
<div class="article-summary-box-inner">
<span><p>Sender-receiver interactions, and specifically persuasion games, are widely
researched in economic modeling and artificial intelligence. However, in the
classic persuasion games setting, the messages sent from the expert to the
decision-maker (DM) are abstract or well-structured signals rather than natural
language messages. This paper addresses the use of natural language in
persuasion games. For this purpose, we conduct an online repeated interaction
experiment. At each trial of the interaction, an informed expert aims to sell
an uninformed decision-maker a vacation in a hotel, by sending her a review
that describes the hotel. While the expert is exposed to several scored
reviews, the decision-maker observes only the single review sent by the expert,
and her payoff in case she chooses to take the hotel is a random draw from the
review score distribution available to the expert only. We also compare the
behavioral patterns in this experiment to the equivalent patterns in similar
experiments where the communication is based on the numerical values of the
reviews rather than the reviews' text, and observe substantial differences
which can be explained through an equilibrium analysis of the game. We consider
a number of modeling approaches for our verbal communication setup, differing
from each other in the model type (deep neural network vs. linear classifier),
the type of features used by the model (textual, behavioral or both) and the
source of the textual features (DNN-based vs. hand-crafted). Our results
demonstrate that given a prefix of the interaction sequence, our models can
predict the future decisions of the decision-maker, particularly when a
sequential modeling approach and hand-crafted textual features are applied.
Further analysis of the hand-crafted textual features allows us to make initial
observations about the aspects of text that drive decision making in our setup
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Speaker Adaptation for Text-to-Speech Synthesis. (arXiv:2103.14512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14512">
<div class="article-summary-box-inner">
<span><p>Training a multi-speaker Text-to-Speech (TTS) model from scratch is
computationally expensive and adding new speakers to the dataset requires the
model to be re-trained. The naive solution of sequential fine-tuning of a model
for new speakers can lead to poor performance of older speakers. This
phenomenon is known as catastrophic forgetting. In this paper, we look at TTS
modeling from a continual learning perspective, where the goal is to add new
speakers without forgetting previous speakers. Therefore, we first propose an
experimental setup and show that serial fine-tuning for new speakers can cause
the forgetting of the earlier speakers. Then we exploit two well-known
techniques for continual learning, namely experience replay and weight
regularization. We reveal how one can mitigate the effect of degradation in
speech synthesis diversity in sequential training of new speakers using these
methods. Finally, we present a simple extension to experience replay to improve
the results in extreme setups where we have access to very small buffers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Pretrained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection. (arXiv:2106.04564v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04564">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformer-based models were reported to be robust in intent
classification. In this work, we first point out the importance of in-domain
out-of-scope detection in few-shot intent recognition tasks and then illustrate
the vulnerability of pre-trained Transformer-based models against samples that
are in-domain but out-of-scope (ID-OOS). We construct two new datasets, and
empirically show that pre-trained models do not perform well on both ID-OOS
examples and general out-of-scope examples, especially on fine-grained few-shot
intent detection tasks. To figure out how the models mistakenly classify ID-OOS
intents as in-scope intents, we further conduct analysis on confidence scores
and the overlapping keywords, as well as point out several prospective
directions for future work. Resources are available on
https://github.com/jianguoz/Few-Shot-Intent-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Speaking Styles in Conversational Text-to-Speech Synthesis with Graph-based Multi-modal Context Modeling. (arXiv:2106.06233v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06233">
<div class="article-summary-box-inner">
<span><p>Comparing with traditional text-to-speech (TTS) systems, conversational TTS
systems are required to synthesize speeches with proper speaking style
confirming to the conversational context. However, state-of-the-art context
modeling methods in conversational TTS only model the textual information in
context with a recurrent neural network (RNN). Such methods have limited
ability in modeling the inter-speaker influence in conversations, and also
neglect the speaking styles and the intra-speaker inertia inside each speaker.
Inspired by DialogueGCN and its superiority in modeling such conversational
influences than RNN based approaches, we propose a graph-based multi-modal
context modeling method and adopt it to conversational TTS to enhance the
speaking styles of synthesized speeches. Both the textual and speaking style
information in the context are extracted and processed by DialogueGCN to model
the inter- and intra-speaker influence in conversations. The outputs of
DialogueGCN are then summarized by attention mechanism, and converted to the
enhanced speaking style for current utterance. An English conversation corpus
is collected and annotated for our research and released to public. Experiment
results on this corpus demonstrate the effectiveness of our proposed approach,
which outperforms the state-of-the-art context modeling method in
conversational TTS in both MOS and ABX preference rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preliminary Steps Towards Federated Sentiment Classification. (arXiv:2107.11956v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11956">
<div class="article-summary-box-inner">
<span><p>Automatically mining sentiment tendency contained in natural language is a
fundamental research to some artificial intelligent applications, where
solutions alternate with challenges. Transfer learning and multi-task learning
techniques have been leveraged to mitigate the supervision sparsity and
collaborate multiple heterogeneous domains correspondingly. Recent years, the
sensitive nature of users' private data raises another challenge for sentiment
classification, i.e., data privacy protection. In this paper, we resort to
federated learning for multiple domain sentiment classification under the
constraint that the corpora must be stored on decentralized devices. In view of
the heterogeneous semantics across multiple parties and the peculiarities of
word embedding, we pertinently provide corresponding solutions. First, we
propose a Knowledge Transfer Enhanced Private-Shared (KTEPS) framework for
better model aggregation and personalization in federated sentiment
classification. Second, we propose KTEPS$^\star$ with the consideration of the
rich semantic and huge embedding size properties of word vectors, utilizing
Projection-based Dimension Reduction (PDR) methods for privacy protection and
efficient transmission simultaneously. We propose two federated sentiment
classification scenes based on public benchmarks, and verify the superiorities
of our proposed methods with abundant experimental investigations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Structure Matters Most: Perturbation Study in NLU. (arXiv:2107.13955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13955">
<div class="article-summary-box-inner">
<span><p>Recent research analyzing the sensitivity of natural language understanding
models to word-order perturbations has shown that neural models are
surprisingly insensitive to the order of words. In this paper, we investigate
this phenomenon by developing order-altering perturbations on the order of
words, subwords, and characters to analyze their effect on neural models'
performance on language understanding tasks. We experiment with measuring the
impact of perturbations to the local neighborhood of characters and global
position of characters in the perturbed texts and observe that perturbation
functions found in prior literature only affect the global ordering while the
local ordering remains relatively unperturbed. We empirically show that neural
models, invariant of their inductive biases, pretraining scheme, or the choice
of tokenization, mostly rely on the local structure of text to build
understanding and make limited use of the global structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05885">
<div class="article-summary-box-inner">
<span><p>Obtaining human-like performance in NLP is often argued to require
compositional generalisation. Whether neural networks exhibit this ability is
usually studied by training models on highly compositional synthetic data.
However, compositionality in natural language is much more complex than the
rigid, arithmetic-like version such data adheres to, and artificial
compositionality tests thus do not allow us to determine how neural models deal
with more realistic forms of compositionality. In this work, we re-instantiate
three compositionality tests from the literature and reformulate them for
neural machine translation (NMT). Our results highlight that: i) unfavourably,
models trained on more data are more compositional; ii) models are sometimes
less compositional than expected, but sometimes more, exemplifying that
different levels of compositionality are required, and models are not always
able to modulate between them correctly; iii) some of the non-compositional
behaviours are mistakes, whereas others reflect the natural variation in data.
Apart from an empirical study, our work is a call to action: we should rethink
the evaluation of compositionality in neural networks and develop benchmarks
using real data to evaluate compositionality on natural language, where
composing meaning is not as straightforward as doing the math.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03910">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage large language models (LMs) to perform zero-shot
text style transfer. We present a prompting method that we call augmented
zero-shot learning, which frames style transfer as a sentence rewriting task
and requires only a natural language instruction, without model fine-tuning or
exemplars in the target style. Augmented zero-shot learning is simple and
demonstrates promising results not just on standard style transfer tasks such
as sentiment, but also on arbitrary transformations such as "make this
melodramatic" or "insert a metaphor."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01661">
<div class="article-summary-box-inner">
<span><p>Iterating with new and improved OCR solutions enforces decision making when
it comes to targeting the right candidates for reprocessing. This especially
applies when the underlying data collection is of considerable size and rather
diverse in terms of fonts, languages, periods of publication and consequently
OCR quality. This article captures the efforts of the National Library of
Luxembourg to support those targeting decisions. They are crucial in order to
guarantee low computational overhead and reduced quality degradation risks,
combined with a more quantifiable OCR improvement. In particular, this work
explains the methodology of the library with respect to text block level
quality assessment. Through extension of this technique, a regression model,
that is able to take into account the enhancement potential of a new OCR
engine, is also presented. They both mark promising approaches, especially for
cultural institutions dealing with historical data of lower quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTraffic: BERT-based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications. (arXiv:2110.05781v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05781">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) allows transcribing the communications
between air traffic controllers (ATCOs) and aircraft pilots. The transcriptions
are used later to extract ATC named entities e.g., aircraft callsigns, command
types, or values. One common challenge is Speech Activity Detection (SAD) and
diarization system. If one of them fails then two or more single speaker
segments remain in the same recording, jeopardizing the overall system's
performance. We propose a system that combines the segmentation of a SAD module
with a BERT model that performs speaker change detection (SCD) and speaker role
detection (SRD) by chunking ASR transcripts i.e., diarization with a defined
number of speakers together with SRD. The proposed model is evaluated on
real-life ATC test sets. It reaches up to 0.90/0.95 F1-score on ATCO/pilot SRD,
which means a 27% relative improvement on diarization error rate (DER) compared
to standard acoustic-based diarization. Results are measured on ASR transcripts
of challenging ATC test sets with $\sim$13\% word error rate, and the
robustness of the system is even validated on noisy ASR transcripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06537">
<div class="article-summary-box-inner">
<span><p>The conventional wisdom behind learning deep classification models is to
focus on bad-classified examples and ignore well-classified examples that are
far from the decision boundary. For instance, when training with cross-entropy
loss, examples with higher likelihoods (i.e., well-classified examples)
contribute smaller gradients in back-propagation. However, we theoretically
show that this common practice hinders representation learning, energy
optimization, and margin growth. To counteract this deficiency, we propose to
reward well-classified examples with additive bonuses to revive their
contribution to the learning process. This counterexample theoretically
addresses these three issues. We empirically support this claim by directly
verifying the theoretical results or significant performance improvement with
our counterexample on diverse tasks, including image classification, graph
classification, and machine translation. Furthermore, this paper shows that we
can deal with complex scenarios, such as imbalanced classification, OOD
detection, and applications under adversarial attacks because our idea can
solve these three issues. Code is available at:
https://github.com/lancopku/well-classified-examples-are-underestimated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings. (arXiv:2110.07274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07274">
<div class="article-summary-box-inner">
<span><p>Many mispronunciation detection and diagnosis (MD&amp;D) research approaches try
to exploit both the acoustic and linguistic features as input. Yet the
improvement of the performance is limited, partially due to the shortage of
large amount annotated training data at the phoneme level. Phonetic embeddings,
extracted from ASR models trained with huge amount of word level annotations,
can serve as a good representation of the content of input speech, in a
noise-robust and speaker-independent manner. These embeddings, when used as
implicit phonetic supplementary information, can alleviate the data shortage of
explicit phoneme annotations. We propose to utilize Acoustic, Phonetic and
Linguistic (APL) embedding features jointly for building a more powerful MD&amp;D
system. Experimental results obtained on the L2-ARCTIC database show the
proposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the
detection accuracy, diagnosis error rate and the F-measure, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07298">
<div class="article-summary-box-inner">
<span><p>Existing approaches to lifelong language learning rely on plenty of labeled
data for learning a new task, which is hard to obtain in most real scenarios.
Considering that humans can continually learn new tasks from a handful of
examples, we expect the models also to be able to generalize well on new
few-shot tasks without forgetting the previous ones. In this work, we define
this more challenging yet practical problem as Lifelong Few-shot Language
Learning (LFLL) and propose a unified framework for it based on prompt tuning
of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot
learning ability, and simultaneously trains the model as a task solver and a
data generator. Before learning a new domain of the same task type, LFPT5
generates pseudo (labeled) samples of previously learned domains, and later
gets trained on those samples to alleviate forgetting of previous knowledge as
it learns the new domain. In addition, a KL divergence loss is minimized to
achieve label consistency between the previous and the current model. While
adapting to a new task type, LFPT5 includes and tunes additional prompt
embeddings for the new task. With extensive experiments, we demonstrate that
LFPT5 can be applied to various different types of tasks and significantly
outperform previous methods in different LFLL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BI-RADS BERT & Using Section Segmentation to Understand Radiology Reports. (arXiv:2110.07552v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07552">
<div class="article-summary-box-inner">
<span><p>Radiology reports are one of the main forms of communication between
radiologists and other clinicians and contain important information for patient
care. In order to use this information for research and automated patient care
programs, it is necessary to convert the raw text into structured data suitable
for analysis. State-of-the-art natural language processing (NLP)
domain-specific contextual word embeddings have been shown to achieve
impressive accuracy for these tasks in medicine, but have yet to be utilized
for section structure segmentation. In this work, we pre-trained a contextual
embedding BERT model using breast radiology reports and developed a classifier
that incorporated the embedding with auxiliary global textual features in order
to perform section segmentation. This model achieved a 98% accuracy at
segregating free text reports sentence by sentence into sections of information
outlined in the Breast Imaging Reporting and Data System (BI-RADS) lexicon, a
significant improvement over the Classic BERT model without auxiliary
information. We then evaluated whether using section segmentation improved the
downstream extraction of clinically relevant information such as
modality/procedure, previous cancer, menopausal status, the purpose of the
exam, breast density, and breast MRI background parenchymal enhancement. Using
the BERT model pre-trained on breast radiology reports combined with section
segmentation resulted in an overall accuracy of 95.9% in the field extraction
tasks. This is a 17% improvement compared to an overall accuracy of 78.9% for
field extraction with models using Classic BERT embeddings and not using
section segmentation. Our work shows the strength of using BERT in radiology
report analysis and the advantages of section segmentation in identifying key
features of patient factors recorded in breast radiology reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER. (arXiv:2110.08454v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08454">
<div class="article-summary-box-inner">
<span><p>Recent advances in prompt-based learning have shown strong results on
few-shot text classification by using cloze-style templates. Similar attempts
have been made on named entity recognition (NER) which manually design
templates to predict entity types for every text span in a sentence. However,
such methods may suffer from error propagation induced by entity span
detection, high cost due to enumeration of all possible text spans, and
omission of inter-dependencies among token labels in a sentence. Here we
present a simple demonstration-based learning method for NER, which lets the
input be prefaced by task demonstrations for in-context learning. We perform a
systematic study on demonstration strategy regarding what to include (entity
examples, with or without surrounding context), how to select the examples, and
what templates to use. Results on in-domain learning and domain adaptation show
that the model's performance in low-resource settings can be largely improved
with a suitable demonstration strategy (e.g., a 4-17% improvement on 25 train
instances). We also find that good demonstration can save many labeled examples
and consistency in demonstration contributes to better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum Description Length Recurrent Neural Networks. (arXiv:2111.00600v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00600">
<div class="article-summary-box-inner">
<span><p>We train neural networks to optimize a Minimum Description Length score,
i.e., to balance between the complexity of the network and its accuracy at a
task. We show that networks optimizing this objective function master tasks
involving memory challenges and go beyond context-free languages. These
learners master languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^{2n}$,
$a^nb^mc^{n+m}$, and they perform addition. Moreover, they often do so with
100% accuracy. The networks are small, and their inner workings are
transparent. We thus provide formal proofs that their perfect accuracy holds
not only on a given test set, but for any input sequence. To our knowledge, no
other connectionist model has been shown to capture the underlying grammars for
these languages in full generality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Transduction with Graph-based Supervision. (arXiv:2111.01272v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01272">
<div class="article-summary-box-inner">
<span><p>The recurrent neural network transducer (RNN-T) objective plays a major role
in building today's best automatic speech recognition (ASR) systems for
production. Similarly to the connectionist temporal classification (CTC)
objective, the RNN-T loss uses specific rules that define how a set of
alignments is generated to form a lattice for the full-sum training. However,
it is yet largely unknown if these rules are optimal and do lead to the best
possible ASR results. In this work, we present a new transducer objective
function that generalizes the RNN-T loss to accept a graph representation of
the labels, thus providing a flexible and efficient framework to manipulate
training lattices, e.g., for studying different transition rules, implementing
different transducer losses, or restricting alignments. We demonstrate that
transducer-based ASR with CTC-like lattice achieves better results compared to
standard RNN-T, while also ensuring a strictly monotonic alignment, which will
allow better optimization of the decoding procedure. For example, the proposed
CTC-like transducer achieves an improvement of 4.8% on the test-other condition
of LibriSpeech relative to an equivalent RNN-T based system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic. (arXiv:2111.14447v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14447">
<div class="article-summary-box-inner">
<span><p>Recent text-to-image matching models apply contrastive learning to large
corpora of uncurated pairs of images and sentences. While such models can
provide a powerful score for matching and subsequent zero-shot tasks, they are
not capable of generating caption given an image. In this work, we repurpose
such models to generate a descriptive text given an image at inference time,
without any further training or tuning steps. This is done by combining the
visual-semantic model with a large language model, benefiting from the
knowledge in both web-scale models. The resulting captions are much less
restrictive than those obtained by supervised captioning methods. Moreover, as
a zero-shot learning method, it is extremely flexible and we demonstrate its
ability to perform image arithmetic in which the inputs can be either images or
text, and the output is a sentence. This enables novel high-level vision
capabilities such as comparing two images or solving visual analogy tests. Our
code is available at: https://github.com/YoadTew/zero-shot-image-to-text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make It Move: Controllable Image-to-Video Generation with Text Descriptions. (arXiv:2112.02815v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02815">
<div class="article-summary-box-inner">
<span><p>Generating controllable videos conforming to user intentions is an appealing
yet challenging topic in computer vision. To enable maneuverable control in
line with user intentions, a novel video generation task, named
Text-Image-to-Video generation (TI2V), is proposed. With both controllable
appearance and motion, TI2V aims at generating videos from a static image and a
text description. The key challenges of TI2V task lie both in aligning
appearance and motion from different modalities, and in handling uncertainty in
text descriptions. To address these challenges, we propose a Motion
Anchor-based video GEnerator (MAGE) with an innovative motion anchor (MA)
structure to store appearance-motion aligned representation. To model the
uncertainty and increase the diversity, it further allows the injection of
explicit condition and implicit randomness. Through three-dimensional axial
transformers, MA is interacted with given image to generate next frames
recursively with satisfying controllability and diversity. Accompanying the new
task, we build two new video-text paired datasets based on MNIST and CATER for
evaluation. Experiments conducted on these datasets verify the effectiveness of
MAGE and show appealing potentials of TI2V task. Source code for model and
datasets will be available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Semantic Concepts into End-to-End Image Captioning. (arXiv:2112.05230v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05230">
<div class="article-summary-box-inner">
<span><p>Tremendous progress has been made in recent years in developing better image
captioning models, yet most of them rely on a separate object detector to
extract regional features. Recent vision-language studies are shifting towards
the detector-free trend by leveraging grid representations for more flexible
model training and faster inference speed. However, such development is
primarily focused on image understanding tasks, and remains less investigated
for the caption generation task. In this paper, we are concerned with a
better-performing detector-free image captioning model, and propose a pure
vision transformer-based image captioning model, dubbed as ViTCAP, in which
grid representations are used without extracting the regional features. For
improved performance, we introduce a novel Concept Token Network (CTN) to
predict the semantic concepts and then incorporate them into the end-to-end
captioning. In particular, the CTN is built on the basis of a vision
transformer and is designed to predict the concept tokens through a
classification task, from which the rich semantic information contained greatly
benefits the captioning task. Compared with the previous detector-based models,
ViTCAP drastically simplifies the architectures and at the same time achieves
competitive performance on various challenging image captioning datasets. In
particular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split,
93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Abstractive Summarization with Adaptive Length Controlling. (arXiv:2112.07534v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07534">
<div class="article-summary-box-inner">
<span><p>Document summarization, as a fundamental task in natural language generation,
aims to generate a short and coherent summary for a given document.
Controllable summarization, especially of the length, is an important issue for
some practical applications, especially how to trade-off the length constraint
and information integrity. In this paper, we propose an \textbf{A}daptive
\textbf{L}ength \textbf{C}ontrolling \textbf{O}ptimization (\textbf{ALCO})
method to leverage two-stage abstractive summarization model via reinforcement
learning. ALCO incorporates length constraint into the stage of sentence
extraction to penalize the overlength extracted sentences. Meanwhile, a
saliency estimation mechanism is designed to preserve the salient information
in the generated sentences. A series of experiments have been conducted on a
wildly-used benchmark dataset \textit{CNN/Daily Mail}. The results have shown
that ALCO performs better than the popular baselines in terms of length
controllability and content preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforcing Semantic-Symmetry for Document Summarization. (arXiv:2112.07583v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07583">
<div class="article-summary-box-inner">
<span><p>Document summarization condenses a long document into a short version with
salient information and accurate semantic descriptions. The main issue is how
to make the output summary semantically consistent with the input document. To
reach this goal, recently, researchers have focused on supervised end-to-end
hybrid approaches, which contain an extractor module and abstractor module.
Among them, the extractor identifies the salient sentences from the input
document, and the abstractor generates a summary from the salient sentences.
This model successfully keeps the consistency between the generated summary and
the reference summary via various strategies (e.g., reinforcement learning).
There are two semantic gaps when training the hybrid model (one is between
document and extracted sentences, and the other is between extracted sentences
and summary). However, they are not explicitly considered in the existing
methods, which usually results in a semantic bias of summary. To mitigate the
above issue, in this paper, a new \textbf{r}einforcing
s\textbf{e}mantic-\textbf{sy}mmetry learning \textbf{m}odel is proposed for
document summarization (\textbf{ReSyM}). ReSyM introduces a
semantic-consistency reward in the extractor to bridge the first gap. A
semantic dual-reward is designed to bridge the second gap in the abstractor.
The whole document summarization process is implemented via reinforcement
learning with a hybrid reward mechanism (combining the above two rewards).
Moreover, a comprehensive sentence representation learning method is presented
to sufficiently capture the information from the original document. A series of
experiments have been conducted on two wildly used benchmark datasets CNN/Daily
Mail and BigPatent. The results have shown the superiority of ReSyM by
comparing it with the state-of-the-art baselines in terms of various evaluation
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Encoding for Extractive Summarization. (arXiv:2112.09572v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09572">
<div class="article-summary-box-inner">
<span><p>Document summarization provides an instrument for faster understanding the
collection of text documents and has several real-life applications. With the
growth of online text data, numerous summarization models have been proposed
recently. The Sequence-to-Sequence (Seq2Seq) based neural summarization model
is the most widely used in the summarization field due to its high performance.
This is because semantic information and structure information in the text is
adequately considered when encoding. However, the existing extractive
summarization models pay little attention to and use the central topic
information to assist the generation of summaries, which leads to models not
ensuring the generated summary under the primary topic. A lengthy document can
span several topics, and a single summary cannot do justice to all the topics.
Therefore, the key to generating a high-quality summary is determining the
central topic and building a summary based on it, especially for a long
document. We propose a topic-aware encoding for document summarization to deal
with this issue. This model effectively combines syntactic-level and
topic-level information to build a comprehensive sentence representation.
Specifically, a neural topic model is added in the neural-based sentence-level
representation learning to adequately consider the central topic information
for capturing the critical content in the original document. The experimental
results on three public datasets show that our model outperforms the
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C2-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System. (arXiv:2201.02732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02732">
<div class="article-summary-box-inner">
<span><p>Conversational recommender systems (CRS) aim to recommend suitable items to
users through natural language conversations. For developing effective CRSs, a
major technical issue is how to accurately infer user preference from very
limited conversation context. To address issue, a promising solution is to
incorporate external data for enriching the context information. However, prior
studies mainly focus on designing fusion models tailored for some specific type
of external data, which is not general to model and utilize multi-type external
data.
</p>
<p>To effectively leverage multi-type external data, we propose a novel
coarse-to-fine contrastive learning framework to improve data semantic fusion
for CRS. In our approach, we first extract and represent multi-grained semantic
units from different data signals, and then align the associated multi-type
semantic units in a coarse-to-fine way. To implement this framework, we design
both coarse-grained and fine-grained procedures for modeling user preference,
where the former focuses on more general, coarse-grained semantic fusion and
the latter focuses on more specific, fine-grained semantic fusion. Such an
approach can be extended to incorporate more kinds of external data. Extensive
experiments on two public CRS datasets have demonstrated the effectiveness of
our approach in both recommendation and conversation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Learning for Short Text Classification. (arXiv:2202.11345v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11345">
<div class="article-summary-box-inner">
<span><p>In the short text, the extremely short length, feature sparsity, and high
ambiguity pose huge challenges to classification tasks. Recently, as an
effective method for tuning Pre-trained Language Models for specific downstream
tasks, prompt-learning has attracted a vast amount of attention and research.
The main intuition behind the prompt-learning is to insert the template into
the input and convert the text classification tasks into equivalent cloze-style
tasks. However, most prompt-learning methods expand label words manually or
only consider the class name for knowledge incorporating in cloze-style
prediction, which will inevitably incur omissions and bias in short text
classification tasks. In this paper, we propose a simple short text
classification approach that makes use of prompt-learning based on
knowledgeable expansion. Taking the special characteristics of short text into
consideration, the method can consider both the short text itself and class
name during expanding label words space. Specifically, the top $N$ concepts
related to the entity in the short text are retrieved from the open Knowledge
Graph like Probase, and we further refine the expanded label words by the
distance calculation between selected concepts and class labels. Experimental
results show that our approach obtains obvious improvement compared with other
fine-tuning, prompt-learning, and knowledgeable prompt-tuning methods,
outperforming the state-of-the-art by up to 6 Accuracy points on three
well-known datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaNames: A Massively Multilingual Entity Name Corpus. (arXiv:2202.14035v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.14035">
<div class="article-summary-box-inner">
<span><p>This preprint describes work in progress on ParaNames, a multilingual
parallel name resource consisting of names for approximately 14 million
entities. The included names span over 400 languages, and almost all entities
are mapped to standardized entity types (PER/LOC/ORG). Using Wikidata as a
source, we create the largest resource of this type to-date. We describe our
approach to filtering and standardizing the data to provide the best quality
possible. ParaNames is useful for multilingual language processing, both in
defining tasks for name translation/transliteration and as supplementary data
for tasks such as named entity recognition and linking. We demonstrate an
application of ParaNames by training a multilingual model for canonical name
translation to and from English. Our resource is released at
\url{https://github.com/bltlab/paranames} under a Creative Commons license (CC
BY 4.0).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding. (arXiv:2203.01515v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01515">
<div class="article-summary-box-inner">
<span><p>Automatic ICD coding is defined as assigning disease codes to electronic
medical records (EMRs). Existing methods usually apply label attention with
code representations to match related text snippets. Unlike these works that
model the label with the code hierarchy or description, we argue that the code
synonyms can provide more comprehensive knowledge based on the observation that
the code expressions in EMRs vary from their descriptions in ICD. By aligning
codes to concepts in UMLS, we collect synonyms of every code. Then, we propose
a multiple synonyms matching network to leverage synonyms for better code
representation learning, and finally help the code classification. Experiments
on the MIMIC-III dataset show that our proposed method outperforms previous
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Simultaneous to Streaming Machine Translation by Leveraging Streaming History. (arXiv:2203.02459v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02459">
<div class="article-summary-box-inner">
<span><p>Simultaneous Machine Translation is the task of incrementally translating an
input sentence before it is fully available. Currently, simultaneous
translation is carried out by translating each sentence independently of the
previously translated text. More generally, Streaming MT can be understood as
an extension of Simultaneous MT to the incremental translation of a continuous
input text stream. In this work, a state-of-the-art simultaneous sentence-level
MT system is extended to the streaming setup by leveraging the streaming
history. Extensive empirical results are reported on IWSLT Translation Tasks,
showing that leveraging the streaming history leads to significant quality
gains. In particular, the proposed system proves to compare favorably to the
best performing systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking. (arXiv:2203.03123v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03123">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking (DST) aims to extract essential information from
multi-turn dialogue situations and take appropriate actions. A belief state,
one of the core pieces of information, refers to the subject and its specific
content, and appears in the form of domain-slot-value. The trained model
predicts "accumulated" belief states in every turn, and joint goal accuracy and
slot accuracy are mainly used to evaluate the prediction; however, we specify
that the current evaluation metrics have a critical limitation when evaluating
belief states accumulated as the dialogue proceeds, especially in the most used
MultiWOZ dataset. Additionally, we propose relative slot accuracy to complement
existing metrics. Relative slot accuracy does not depend on the number of
predefined slots, and allows intuitive evaluation by assigning relative scores
according to the turn of each dialogue. This study also encourages not solely
the reporting of joint goal accuracy, but also various complementary metrics in
DST tasks for the sake of a realistic evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05598">
<div class="article-summary-box-inner">
<span><p>The study of the applicability of the BERTScore metric was conducted to
translation quality assessment at the sentence level for English -&gt; Russian
direction. Experiments were performed with a pre-trained Multilingual BERT as
well as with a pair of Monolingual BERT models. To align monolingual
embeddings, an orthogonal transformation based on anchor tokens was used. It
was demonstrated that such transformation helps to prevent mismatching issue
and shown that this approach gives better results than using embeddings of the
Multilingual model. To improve the token matching process it is proposed to
combine all incomplete WorkPiece tokens into meaningful words and use simple
averaging of corresponding vectors and to calculate BERTScore based on anchor
tokens only. Such modifications allowed us to achieve a better correlation of
the model predictions with human judgments. In addition to evaluating machine
translation, several versions of human translation were evaluated as well, the
problems of this approach were listed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07836">
<div class="article-summary-box-inner">
<span><p>Abstract meaning representation (AMR) highlights the core semantic
information of text in a graph structure. Recently, pre-trained language models
(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,
respectively. However, PLMs are typically pre-trained on textual data, thus are
sub-optimal for modeling structural knowledge. To this end, we investigate
graph self-supervised training to improve the structure awareness of PLMs over
AMR graphs. In particular, we introduce two graph auto-encoding strategies for
graph-to-graph pre-training and four tasks to integrate text and graph
information during pre-training. We further design a unified framework to
bridge the gap between pre-training and fine-tuning tasks. Experiments on both
AMR parsing and AMR-to-text generation show the superiority of our model. To
our knowledge, we are the first to consider pre-training on semantic graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Converse: A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12187">
<div class="article-summary-box-inner">
<span><p>Creating a system that can have meaningful conversations with humans to help
accomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).
It has defined the meaning of AI since the beginning. A lot has been
accomplished in this area recently, with voice assistant products entering our
daily lives and chat bot systems becoming commonplace in customer service. At
first glance there seems to be no shortage of options for dialogue systems.
However, the frequently deployed dialogue systems today seem to all struggle
with a critical weakness - they are hard to build and harder to maintain. At
the core of the struggle is the need to script every single turn of
interactions between the bot and the human user. This makes the dialogue
systems more difficult to maintain as the tasks become more complex and more
tasks are added to the system. In this paper, we propose Converse, a flexible
tree-based modular task-oriented dialogue system. Converse uses an and-or tree
structure to represent tasks and offers powerful multi-task dialogue
management. Converse supports task dependency and task switching, which are
unique features compared to other open-source dialogue frameworks. At the same
time, Converse aims to make the bot building process easy and simple, for both
professional and non-professional software developers. The code is available at
https://github.com/salesforce/Converse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). (arXiv:2203.13366v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13366">
<div class="article-summary-box-inner">
<span><p>For a long period, different recommendation tasks typically require designing
task-specific architectures and training objectives. As a result, it is hard to
transfer the learned knowledge and representations from one task to another,
thus restricting the generalization ability of existing recommendation
approaches, e.g., a sequential recommendation model can hardly be applied or
transferred to a review generation method. To deal with such issues,
considering that language grounding is a powerful medium to describe and
represent various problems or tasks, we present a flexible and unified
text-to-text paradigm called "Pretrain, Personalized Prompt, and Predict
Paradigm" (P5) for recommendation, which unifies various recommendation tasks
in a shared framework. In P5, all data such as user-item interactions, item
metadata, and user reviews are converted to a common format -- natural language
sequences. The rich information from natural language assist P5 to capture
deeper semantics for recommendation. P5 learns different tasks with the same
language modeling objective during pretraining. Thus, it possesses the
potential to serve as the foundation model for downstream recommendation tasks,
allows easy integration with other modalities, and enables instruction-based
recommendation, which will revolutionize the technical form of recommender
system towards universal recommendation engine. With adaptive personalized
prompt for different users, P5 is able to make predictions in a zero-shot or
few-shot manner and largely reduces the necessity for extensive fine-tuning. On
several recommendation benchmarks, we conduct experiments to show the
effectiveness of our generative approach. We will release our prompts and
pretrained P5 language model to help advance future research on Recommendation
as Language Processing (RLP) and Personalized Foundation Models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISC: A MIxed Strategy-Aware Model Integrating COMET for Emotional Support Conversation. (arXiv:2203.13560v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13560">
<div class="article-summary-box-inner">
<span><p>Applying existing methods to emotional support conversation -- which provides
valuable assistance to people who are in need -- has two major limitations: (a)
they generally employ a conversation-level emotion label, which is too
coarse-grained to capture user's instant mental state; (b) most of them focus
on expressing empathy in the response(s) rather than gradually reducing user's
distress. To address the problems, we propose a novel model \textbf{MISC},
which firstly infers the user's fine-grained emotional status, and then
responds skillfully using a mixture of strategy. Experimental results on the
benchmark dataset demonstrate the effectiveness of our method and reveal the
benefits of fine-grained emotion understanding as well as mixed-up strategy
modeling. Our code and data could be found in
\url{https://github.com/morecry/MISC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding. (arXiv:2203.16487v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16487">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Generalized Aggressive Decoding (GAD) -- a novel
approach to accelerating autoregressive translation with no quality loss,
through the collaboration of autoregressive and non-autoregressive translation
(NAT) of the Transformer. At each decoding iteration, GAD aggressively decodes
a number of tokens in parallel as a draft through NAT and then verifies them in
the autoregressive manner, where only the tokens that pass the verification are
kept as decoded tokens. GAD can achieve the same performance as autoregressive
translation but much more efficiently because both NAT drafting and
autoregressive verification are fast due to parallel computing. We conduct
experiments in the WMT14 English-German translation task and confirm that the
vanilla GAD yields exactly the same results as greedy decoding with an around
3x speedup, and that its variant (GAD++) with an advanced verification strategy
not only outperforms the greedy translation and even achieves the comparable
translation quality with the beam search result, but also further improves the
decoding speed, resulting in an around 5x speedup over autoregressive
translation. Our models and codes are available at
https://github.com/hemingkx/Generalized-Aggressive-Decoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15173">
<div class="article-summary-box-inner">
<span><p>Word embedding is a modern distributed word representations approach widely
used in many natural language processing tasks. Converting the vocabulary in a
legal document into a word embedding model facilitates subjecting legal
documents to machine learning, deep learning, and other algorithms and
subsequently performing the downstream tasks of natural language processing
vis-\`a-vis, for instance, document classification, contract review, and
machine translation. The most common and practical approach of accuracy
evaluation with the word embedding model uses a benchmark set with linguistic
rules or the relationship between words to perform analogy reasoning via
algebraic calculation. This paper proposes establishing a 1,134 Legal
Analogical Reasoning Questions Set (LARQS) from the 2,388 Chinese Codex corpus
using five kinds of legal relations, which are then used to evaluate the
accuracy of the Chinese word embedding model. Moreover, we discovered that
legal relations might be ubiquitous in the word embedding model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment. (arXiv:2203.15937v1 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15937">
<div class="article-summary-box-inner">
<span><p>Current leading mispronunciation detection and diagnosis (MDD) systems
achieve promising performance via end-to-end phoneme recognition. One challenge
of such end-to-end solutions is the scarcity of human-annotated phonemes on
natural L2 speech. In this work, we leverage unlabeled L2 speech via a
pseudo-labeling (PL) procedure and extend the fine-tuning approach based on
pre-trained self-supervised learning (SSL) models. Specifically, we use Wav2vec
2.0 as our SSL model, and fine-tune it using original labeled L2 speech samples
plus the created pseudo-labeled L2 speech samples. Our pseudo labels are
dynamic and are produced by an ensemble of the online model on-the-fly, which
ensures that our model is robust to pseudo label noise. We show that
fine-tuning with pseudo labels gains a 5.35% phoneme error rate reduction and
2.48% MDD F1 score improvement over a labeled-samples-only fine-tuning
baseline. The proposed PL method is also shown to outperform conventional
offline PL methods. Compared to the state-of-the-art MDD systems, our MDD
solution achieves a more accurate and consistent phonetic error diagnosis. In
addition, we conduct an open test on a separate UTD-4Accents dataset, where our
system recognition outputs show a strong correlation with human perception,
based on accentedness and intelligibility.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Multimodal Depth Estimation from Light Fields. (arXiv:2203.16542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16542">
<div class="article-summary-box-inner">
<span><p>Light field applications, especially light field rendering and depth
estimation, developed rapidly in recent years. While state-of-the-art light
field rendering methods handle semi-transparent and reflective objects well,
depth estimation methods either ignore these cases altogether or only deliver a
weak performance. We argue that this is due current methods only considering a
single "true" depth, even when multiple objects at different depths contributed
to the color of a single pixel. Based on the simple idea of outputting a
posterior depth distribution instead of only a single estimate, we develop and
explore several different deep-learning-based approaches to the problem.
Additionally, we contribute the first "multimodal light field depth dataset"
that contains the depths of all objects which contribute to the color of a
pixel. This allows us to supervise the multimodal depth prediction and also
validate all methods by measuring the KL divergence of the predicted
posteriors. With our thorough analysis and novel dataset, we aim to start a new
line of depth estimation research that overcomes some of the long-standing
limitations of this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COSMOS: Cross-Modality Unsupervised Domain Adaptation for 3D Medical Image Segmentation based on Target-aware Domain Translation and Iterative Self-Training. (arXiv:2203.16557v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16557">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning-based medical image segmentation studies
achieve nearly human-level performance when in fully supervised condition.
However, acquiring pixel-level expert annotations is extremely expensive and
laborious in medical imaging fields. Unsupervised domain adaptation can
alleviate this problem, which makes it possible to use annotated data in one
imaging modality to train a network that can successfully perform segmentation
on target imaging modality with no labels. In this work, we propose a
self-training based unsupervised domain adaptation framework for 3D medical
image segmentation named COSMOS and validate it with automatic segmentation of
Vestibular Schwannoma (VS) and cochlea on high-resolution T2 Magnetic Resonance
Images (MRI). Our target-aware contrast conversion network translates source
domain annotated T1 MRI to pseudo T2 MRI to enable segmentation training on
target domain, while preserving important anatomical features of interest in
the converted images. Iterative self-training is followed to incorporate
unlabeled data to training and incrementally improve the quality of
pseudo-labels, thereby leading to improved performance of segmentation. COSMOS
won the 1\textsuperscript{st} place in the Cross-Modality Domain Adaptation
(crossMoDA) challenge held in conjunction with the 24th International
Conference on Medical Image Computing and Computer Assisted Intervention
(MICCAI 2021). It achieves mean Dice score and Average Symmetric Surface
Distance of 0.871(0.063) and 0.437(0.270) for VS, and 0.842(0.020) and
0.152(0.030) for cochlea.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Cycle-Consistent Learning for Instruction Following and Generation in Vision-Language Navigation. (arXiv:2203.16586v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16586">
<div class="article-summary-box-inner">
<span><p>Since the rise of vision-language navigation (VLN), great progress has been
made in instruction following -- building a follower to navigate environments
under the guidance of instructions. However, far less attention has been paid
to the inverse task: instruction generation -- learning a speaker~to generate
grounded descriptions for navigation routes. Existing VLN methods train a
speaker independently and often treat it as a data augmentation tool to
strengthen the follower while ignoring rich cross-task relations. Here we
describe an approach that learns the two tasks simultaneously and exploits
their intrinsic correlations to boost the training of each: the follower judges
whether the speaker-created instruction explains the original navigation route
correctly, and vice versa. Without the need of aligned instruction-path pairs,
such cycle-consistent learning scheme is complementary to task-specific
training targets defined on labeled data, and can also be applied over
unlabeled paths (sampled without paired instructions). Another agent,
called~creator is added to generate counterfactual environments. It greatly
changes current scenes yet leaves novel items -- which are vital for the
execution of original instructions -- unchanged. Thus more informative training
scenes are synthesized and the three agents compose a powerful VLN learning
system. Extensive experiments on a standard benchmark show that our approach
improves the performance of various follower models and produces accurate
navigation instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Few-shot Class-incremental Learning. (arXiv:2203.16588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16588">
<div class="article-summary-box-inner">
<span><p>Continually learning new classes from fresh data without forgetting previous
knowledge of old classes is a very challenging research problem. Moreover, it
is imperative that such learning must respect certain memory and computational
constraints such as (i) training samples are limited to only a few per class,
(ii) the computational cost of learning a novel class remains constant, and
(iii) the memory footprint of the model grows at most linearly with the number
of classes observed. To meet the above constraints, we propose C-FSCIL, which
is architecturally composed of a frozen meta-learned feature extractor, a
trainable fixed-size fully connected layer, and a rewritable dynamically
growing memory that stores as many vectors as the number of encountered
classes. C-FSCIL provides three update modes that offer a trade-off between
accuracy and compute-memory cost of learning novel classes. C-FSCIL exploits
hyperdimensional embedding that allows to continually express many more classes
than the fixed dimensions in the vector space, with minimal interference. The
quality of class vector representations is further improved by aligning them
quasi-orthogonally to each other by means of novel loss functions. Experiments
on the CIFAR100, miniImageNet, and Omniglot datasets show that C-FSCIL
outperforms the baselines with remarkable accuracy and compression. It also
scales up to the largest problem size ever tried in this few-shot setting by
learning 423 novel classes on top of 1200 base classes with less than 1.6%
accuracy drop. Our code is available at
https://github.com/IBM/constrained-FSCIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Local Displacements for Point Cloud Completion. (arXiv:2203.16600v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16600">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach aimed at object and semantic scene completion
from a partial scan represented as a 3D point cloud. Our architecture relies on
three novel layers that are used successively within an encoder-decoder
structure and specifically developed for the task at hand. The first one
carries out feature extraction by matching the point features to a set of
pre-trained local descriptors. Then, to avoid losing individual descriptors as
part of standard operations such as max-pooling, we propose an alternative
neighbor-pooling operation that relies on adopting the feature vectors with the
highest activations. Finally, up-sampling in the decoder modifies our feature
extraction in order to increase the output dimension. While this model is
already able to achieve competitive results with the state of the art, we
further propose a way to increase the versatility of our approach to process
point clouds. To this aim, we introduce a second model that assembles our
layers within a transformer architecture. We evaluate both architectures on
object and indoor scene completion tasks, achieving state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Cancer Prediction in Challenging Screen-Detected Incident Lung Nodules Using Time-Series Deep Learning. (arXiv:2203.16606v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16606">
<div class="article-summary-box-inner">
<span><p>Lung cancer is the leading cause of cancer-related mortality worldwide. Lung
cancer screening (LCS) using annual low-dose computed tomography (CT) scanning
has been proven to significantly reduce lung cancer mortality by detecting
cancerous lung nodules at an earlier stage. Improving risk stratification of
malignancy risk in lung nodules can be enhanced using machine/deep learning
algorithms. However most existing algorithms: a) have primarily assessed single
time-point CT data alone thereby failing to utilize the inherent advantages
contained within longitudinal imaging datasets; b) have not integrated into
computer models pertinent clinical data that might inform risk prediction; c)
have not assessed algorithm performance on the spectrum of nodules that are
most challenging for radiologists to interpret and where assistance from
analytic tools would be most beneficial.
</p>
<p>Here we show the performance of our time-series deep learning model
(DeepCAD-NLM-L) which integrates multi-model information across three
longitudinal data domains: nodule-specific, lung-specific, and clinical
demographic data. We compared our time-series deep learning model to a)
radiologist performance on CTs from the National Lung Screening Trial enriched
with the most challenging nodules for diagnosis; b) a nodule management
algorithm from a North London LCS study (SUMMIT). Our model demonstrated
comparable and complementary performance to radiologists when interpreting
challenging lung nodules and showed improved performance (AUC=88\%) against
models utilizing single time-point data only. The results emphasise the
importance of time-series, multi-modal analysis when interpreting malignancy
risk in LCS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16616">
<div class="article-summary-box-inner">
<span><p>Knowledge-based entity prediction (KEP) is a novel task that aims to improve
machine perception in autonomous systems. KEP leverages relational knowledge
from heterogeneous sources in predicting potentially unrecognized entities. In
this paper, we provide a formal definition of KEP as a knowledge completion
task. Three potential solutions are then introduced, which employ several
machine learning and data mining techniques. Finally, the applicability of KEP
is demonstrated on two autonomous systems from different domains; namely,
autonomous driving and smart manufacturing. We argue that in complex real-world
systems, the use of KEP would significantly improve machine perception while
pushing the current technology one step closer to achieving the full autonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Document Recognition and Understanding with Dessurt. (arXiv:2203.16618v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16618">
<div class="article-summary-box-inner">
<span><p>We introduce Dessurt, a relatively simple document understanding transformer
capable of being fine-tuned on a greater variety of document tasks than prior
methods. It receives a document image and task string as input and generates
arbitrary text autoregressively as output. Because Dessurt is an end-to-end
architecture that performs text recognition in addition to the document
understanding, it does not require an external recognition model as prior
methods do, making it easier to fine-tune to new visual domains. We show that
this model is effective at 9 different dataset-task combinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TR-MOT: Multi-Object Tracking by Reference. (arXiv:2203.16621v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16621">
<div class="article-summary-box-inner">
<span><p>Multi-object Tracking (MOT) generally can be split into two sub-tasks, i.e.,
detection and association. Many previous methods follow the tracking by
detection paradigm, which first obtain detections at each frame and then
associate them between adjacent frames. Though with an impressive performance
by utilizing a strong detector, it will degrade their detection and association
performance under scenes with many occlusions and large motion if not using
temporal information. In this paper, we propose a novel Reference Search (RS)
module to provide a more reliable association based on the deformable
transformer structure, which is natural to learn the feature alignment for each
object among frames. RS takes previous detected results as references to
aggregate the corresponding features from the combined features of the adjacent
frames and makes a one-to-one track state prediction for each reference in
parallel. Therefore, RS can attain a reliable association coping with
unexpected motions by leveraging visual temporal features while maintaining the
strong detection performance by decoupling from the detector. Our RS module can
also be compatible with the structure of the other tracking by detection
frameworks. Furthermore, we propose a joint training strategy and an effective
matching pipeline for our online MOT framework with the RS module. Our method
achieves competitive results on MOT17 and MOT20 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16622">
<div class="article-summary-box-inner">
<span><p>We evaluate the performance of federated learning (FL) in developing deep
learning models for analysis of digitized tissue sections. A classification
application was considered as the example use case, on quantifiying the
distribution of tumor infiltrating lymphocytes within whole slide images
(WSIs). A deep learning classification model was trained using 50*50 square
micron patches extracted from the WSIs. We simulated a FL environment in which
a dataset, generated from WSIs of cancer from numerous anatomical sites
available by The Cancer Genome Atlas repository, is partitioned in 8 different
nodes. Our results show that the model trained with the federated training
approach achieves similar performance, both quantitatively and qualitatively,
to that of a model trained with all the training data pooled at a centralized
location. Our study shows that FL has tremendous potential for enabling
development of more robust and accurate models for histopathology image
analysis without having to collect large and diverse training data at a single
location.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DDNeRF: Depth Distribution Neural Radiance Fields. (arXiv:2203.16626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16626">
<div class="article-summary-box-inner">
<span><p>In recent years, the field of implicit neural representation has progressed
significantly. Models such as neural radiance fields (NeRF), which uses
relatively small neural networks, can represent high-quality scenes and achieve
state-of-the-art results for novel view synthesis. Training these types of
networks, however, is still computationally very expensive. We present depth
distribution neural radiance field (DDNeRF), a new method that significantly
increases sampling efficiency along rays during training while achieving
superior results for a given sampling budget. DDNeRF achieves this by learning
a more accurate representation of the density distribution along rays. More
specifically, we train a coarse model to predict the internal distribution of
the transparency of an input volume in addition to the volume's total density.
This finer distribution then guides the sampling procedure of the fine model.
This method allows us to use fewer samples during training while reducing
computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Augmentations for Video Representation Learning. (arXiv:2203.16632v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16632">
<div class="article-summary-box-inner">
<span><p>This paper focuses on self-supervised video representation learning. Most
existing approaches follow the contrastive learning pipeline to construct
positive and negative pairs by sampling different clips. However, this
formulation tends to bias to static background and have difficulty establishing
global temporal structures. The major reason is that the positive pairs, i.e.,
different clips sampled from the same video, have limited temporal receptive
field, and usually share similar background but differ in motions. To address
these problems, we propose a framework to jointly utilize local clips and
global videos to learn from detailed region-level correspondence as well as
general long-term temporal relations. Based on a set of controllable
augmentations, we achieve accurate appearance and motion pattern alignment
through soft spatio-temporal region contrast. Our formulation is able to avoid
the low-level redundancy shortcut by mutual information minimization to improve
the generalization. We also introduce local-global temporal order dependency to
further bridge the gap between clip-level and video-level representations for
robust temporal modeling. Extensive experiments demonstrate that our framework
is superior on three video benchmarks in action recognition and video
retrieval, capturing more accurate temporal dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations. (arXiv:2203.16639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16639">
<div class="article-summary-box-inner">
<span><p>We present a meta-learning framework for learning new visual concepts
quickly, from just one or a few examples, guided by multiple naturally
occurring data streams: simultaneously looking at images, reading sentences
that describe the objects in the scene, and interpreting supplemental sentences
that relate the novel concept with other concepts. The learned concepts support
downstream applications, such as answering questions by reasoning about unseen
images. Our model, namely FALCON, represents individual visual concepts, such
as colors and shapes, as axis-aligned boxes in a high-dimensional space (the
"box embedding space"). Given an input image and its paired sentence, our model
first resolves the referential expression in the sentence and associates the
novel concept with particular objects in the scene. Next, our model interprets
supplemental sentences to relate the novel concept with other known concepts,
such as "X has property Y" or "X is a kind of Y". Finally, it infers an optimal
box embedding for the novel concept that jointly 1) maximizes the likelihood of
the observed instances in the image, and 2) satisfies the relationships between
the novel concepts and the known ones. We demonstrate the effectiveness of our
model on both synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination. (arXiv:2203.16669v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16669">
<div class="article-summary-box-inner">
<span><p>In Heterogeneous Face Recognition (HFR), the objective is to match faces
across two different domains such as visible and thermal. Large domain
discrepancy makes HFR a difficult problem. Recent methods attempting to fill
the gap via synthesis have achieved promising results, but their performance is
still limited by the scarcity of paired training data. In practice, large-scale
heterogeneous face data are often inaccessible due to the high cost of
acquisition and annotation process as well as privacy regulations. In this
paper, we propose a new face hallucination paradigm for HFR, which not only
enables data-efficient synthesis but also allows to scale up model training
without breaking any privacy policy. Unlike existing methods that learn face
synthesis entirely from scratch, our approach is particularly designed to take
advantage of rich and diverse facial priors from visible domain for more
faithful hallucination. On the other hand, large-scale training is enabled by
introducing a new federated learning scheme to allow institution-wise
collaborations while avoiding explicit data sharing. Extensive experiments
demonstrate the advantages of our approach in tackling HFR under current data
limitations. In a unified framework, our method yields the state-of-the-art
hallucination results on multiple HFR datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIE-Net: Photometric Invariant Edge Guided Network for Intrinsic Image Decomposition. (arXiv:2203.16670v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16670">
<div class="article-summary-box-inner">
<span><p>Intrinsic image decomposition is the process of recovering the image
formation components (reflectance and shading) from an image. Previous methods
employ either explicit priors to constrain the problem or implicit constraints
as formulated by their losses (deep learning). These methods can be negatively
influenced by strong illumination conditions causing shading-reflectance
leakages.
</p>
<p>Therefore, in this paper, an end-to-end edge-driven hybrid CNN approach is
proposed for intrinsic image decomposition. Edges correspond to illumination
invariant gradients. To handle hard negative illumination transitions, a
hierarchical approach is taken including global and local refinement layers. We
make use of attention layers to further strengthen the learning process.
</p>
<p>An extensive ablation study and large scale experiments are conducted showing
that it is beneficial for edge-driven hybrid IID networks to make use of
illumination invariant descriptors and that separating global and local cues
helps in improving the performance of the network. Finally, it is shown that
the proposed method obtains state of the art performance and is able to
generalise well to real world images. The project page with pretrained models,
finetuned models and network code can be found at
https://ivi.fnwi.uva.nl/cv/pienet/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Spreader: Learning Facial Action Unit Dynamics with Extremely Limited Labels. (arXiv:2203.16678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16678">
<div class="article-summary-box-inner">
<span><p>Recent studies on the automatic detection of facial action unit (AU) have
extensively relied on large-sized annotations. However, manually AU labeling is
difficult, time-consuming, and costly. Most existing semi-supervised works
ignore the informative cues from the temporal domain, and are highly dependent
on densely annotated videos, making the learning process less efficient. To
alleviate these problems, we propose a deep semi-supervised framework
Knowledge-Spreader (KS), which differs from conventional methods in two
aspects. First, rather than only encoding human knowledge as constraints, KS
also learns the Spatial-Temporal AU correlation knowledge in order to
strengthen its out-of-distribution generalization ability. Second, we approach
KS by applying consistency regularization and pseudo-labeling in multiple
student networks alternately and dynamically. It spreads the spatial knowledge
from labeled frames to unlabeled data, and completes the temporal information
of partially labeled video clips. Thus, the design allows KS to learn AU
dynamics from video clips with only one label allocated, which significantly
reduce the requirements of using annotations. Extensive experiments demonstrate
that the proposed KS achieves competitive performance as compared to the state
of the arts under the circumstances of using only 2% labels on BP4D and 5%
labels on DISFA. In addition, we test it on our newly developed large-scale
comprehensive emotion database, which contains considerable samples across
well-synchronized and aligned sensor modalities for easing the scarcity issue
of annotations and identities in human affective computing. The new database
will be released to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Effect of Registration Hyperparameters with HyperMorph. (arXiv:2203.16680v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16680">
<div class="article-summary-box-inner">
<span><p>We introduce HyperMorph, a framework that facilitates efficient
hyperparameter tuning in learning-based deformable image registration.
Classical registration algorithms perform an iterative pair-wise optimization
to compute a deformation field that aligns two images. Recent learning-based
approaches leverage large image datasets to learn a function that rapidly
estimates a deformation for a given image pair. In both strategies, the
accuracy of the resulting spatial correspondences is strongly influenced by the
choice of certain hyperparameter values. However, an effective hyperparameter
search consumes substantial time and human effort as it often involves training
multiple models for different fixed hyperparameter values and may lead to
suboptimal registration. We propose an amortized hyperparameter learning
strategy to alleviate this burden by learning the impact of hyperparameters on
deformation fields. We design a meta network, or hypernetwork, that predicts
the parameters of a registration network for input hyperparameters, thereby
comprising a single model that generates the optimal deformation field
corresponding to given hyperparameter values. This strategy enables fast,
high-resolution hyperparameter search at test-time, reducing the inefficiency
of traditional approaches while increasing flexibility. We also demonstrate
additional benefits of HyperMorph, including enhanced robustness to model
initialization and the ability to rapidly identify optimal hyperparameter
values specific to a dataset, image contrast, task, or even anatomical region,
all without the need to retrain models. We make our code publicly available at
<a href="http://hypermorph.voxelmorph.net.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Relighting with Geometrically Consistent Shadows. (arXiv:2203.16681v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16681">
<div class="article-summary-box-inner">
<span><p>Most face relighting methods are able to handle diffuse shadows, but struggle
to handle hard shadows, such as those cast by the nose. Methods that propose
techniques for handling hard shadows often do not produce geometrically
consistent shadows since they do not directly leverage the estimated face
geometry while synthesizing them. We propose a novel differentiable algorithm
for synthesizing hard shadows based on ray tracing, which we incorporate into
training our face relighting model. Our proposed algorithm directly utilizes
the estimated face geometry to synthesize geometrically consistent hard
shadows. We demonstrate through quantitative and qualitative experiments on
Multi-PIE and FFHQ that our method produces more geometrically consistent
shadows than previous face relighting methods while also achieving
state-of-the-art face relighting performance under directional lighting. In
addition, we demonstrate that our differentiable hard shadow modeling improves
the quality of the estimated face geometry over diffuse shading models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo. (arXiv:2203.16682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16682">
<div class="article-summary-box-inner">
<span><p>We present a debiased dataset for the Person-centric Visual Grounding (PCVG)
task first proposed by Cui et al. (2021) in the Who's Waldo dataset. Given an
image and a caption, PCVG requires pairing up a person's name mentioned in a
caption with a bounding box that points to the person in the image. We find
that the original Who's Waldo dataset compiled for this task contains a large
number of biased samples that are solvable simply by heuristic methods; for
instance, in many cases the first name in the sentence corresponds to the
largest bounding box, or the sequence of names in the sentence corresponds to
an exact left-to-right order in the image. Naturally, models trained on these
biased data lead to over-estimation of performance on the benchmark. To enforce
models being correct for the correct reasons, we design automated tools to
filter and debias the original dataset by ruling out all examples of
insufficient context, such as those with no verb or with a long chain of
conjunct names in their captions. Our experiments show that our new sub-sampled
dataset contains less bias with much lowered heuristic performances and widened
gaps between heuristic and supervised methods. We also demonstrate the same
benchmark model trained on our debiased training set outperforms that trained
on the original biased (and larger) training set on our debiased test set. We
argue our debiased dataset offers the PCVG task a more practical baseline for
reliable benchmarking and future improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Adaptive Parameter Sharing for Multi-Task Learning. (arXiv:2203.16708v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16708">
<div class="article-summary-box-inner">
<span><p>Adapting pre-trained models with broad capabilities has become standard
practice for learning a wide range of downstream tasks. The typical approach of
fine-tuning different models for each task is performant, but incurs a
substantial memory cost. To efficiently learn multiple downstream tasks we
introduce Task Adaptive Parameter Sharing (TAPS), a general method for tuning a
base model to a new task by adaptively modifying a small, task-specific subset
of layers. This enables multi-task learning while minimizing resources used and
competition between tasks. TAPS solves a joint optimization problem which
determines which layers to share with the base model and the value of the
task-specific weights. Further, a sparsity penalty on the number of active
layers encourages weight sharing with the base model. Compared to other
methods, TAPS retains high accuracy on downstream tasks while introducing few
task-specific parameters. Moreover, TAPS is agnostic to the model architecture
and requires only minor changes to the training scheme. We evaluate our method
on a suite of fine-tuning tasks and architectures (ResNet, DenseNet, ViT) and
show that it achieves state-of-the-art performance while being simple to
implement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Explainable Metrics for Augmented SGD. (arXiv:2203.16723v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16723">
<div class="article-summary-box-inner">
<span><p>Explaining the generalization characteristics of deep learning is an emerging
topic in advanced machine learning. There are several unanswered questions
about how learning under stochastic optimization really works and why certain
strategies are better than others. In this paper, we address the following
question: \textit{can we probe intermediate layers of a deep neural network to
identify and quantify the learning quality of each layer?} With this question
in mind, we propose new explainability metrics that measure the redundant
information in a network's layers using a low-rank factorization framework and
quantify a complexity measure that is highly correlated with the generalization
performance of a given optimizer, network, and dataset. We subsequently exploit
these metrics to augment the Stochastic Gradient Descent (SGD) optimizer by
adaptively adjusting the learning rate in each layer to improve in
generalization performance. Our augmented SGD -- dubbed RMSGD -- introduces
minimal computational overhead compared to SOTA methods and outperforms them by
exhibiting strong generalization characteristics across application,
architecture, and dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Image Aesthetics Assessment with Rich Attributes. (arXiv:2203.16754v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16754">
<div class="article-summary-box-inner">
<span><p>Personalized image aesthetics assessment (PIAA) is challenging due to its
highly subjective nature. People's aesthetic tastes depend on diversified
factors, including image characteristics and subject characters. The existing
PIAA databases are limited in terms of annotation diversity, especially the
subject aspect, which can no longer meet the increasing demands of PIAA
research. To solve the dilemma, we conduct so far, the most comprehensive
subjective study of personalized image aesthetics and introduce a new
Personalized image Aesthetics database with Rich Attributes (PARA), which
consists of 31,220 images with annotations by 438 subjects. PARA features
wealthy annotations, including 9 image-oriented objective attributes and 4
human-oriented subjective attributes. In addition, desensitized subject
information, such as personality traits, is also provided to support study of
PIAA and user portraits. A comprehensive analysis of the annotation data is
provided and statistic study indicates that the aesthetic preferences can be
mirrored by proposed subjective attributes. We also propose a conditional PIAA
model by utilizing subject information as conditional prior. Experimental
results indicate that the conditional PIAA model can outperform the control
group, which is also the first attempt to demonstrate how image aesthetics and
subject characters interact to produce the intricate personalized tastes on
image aesthetics. We believe the database and the associated analysis would be
useful for conducting next-generation PIAA study. The project page of PARA can
be found at: https://cv-datasets.institutecv.com/#/data-sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Backpropagation: A Memory Efficient Strategy for Training Video Models. (arXiv:2203.16755v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16755">
<div class="article-summary-box-inner">
<span><p>We propose a memory efficient method, named Stochastic Backpropagation (SBP),
for training deep neural networks on videos. It is based on the finding that
gradients from incomplete execution for backpropagation can still effectively
train the models with minimal accuracy loss, which attributes to the high
redundancy of video. SBP keeps all forward paths but randomly and independently
removes the backward paths for each network layer in each training step. It
reduces the GPU memory cost by eliminating the need to cache activation values
corresponding to the dropped backward paths, whose amount can be controlled by
an adjustable keep-ratio. Experiments show that SBP can be applied to a wide
range of models for video tasks, leading to up to 80.0% GPU memory saving and
10% training speedup with less than 1% accuracy drop on action recognition and
temporal action detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Casual 6-DoF: free-viewpoint panorama using a handheld 360 camera. (arXiv:2203.16756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16756">
<div class="article-summary-box-inner">
<span><p>Six degrees-of-freedom (6-DoF) video provides telepresence by enabling users
to move around in the captured scene with a wide field of regard. Compared to
methods requiring sophisticated camera setups, the image-based rendering method
based on photogrammetry can work with images captured with any poses, which is
more suitable for casual users. However, existing image-based rendering methods
are based on perspective images. When used to reconstruct 6-DoF views, it often
requires capturing hundreds of images, making data capture a tedious and
time-consuming process. In contrast to traditional perspective images,
360{\deg} images capture the entire surrounding view in a single shot, thus,
providing a faster capturing process for 6-DoF view reconstruction. This paper
presents a novel method to provide 6-DoF experiences over a wide area using an
unstructured collection of 360{\deg} panoramas captured by a conventional
360{\deg} camera. Our method consists of 360{\deg} data capturing, novel depth
estimation to produce a high-quality spherical depth panorama, and
high-fidelity free-viewpoint generation. We compared our method against
state-of-the-art methods, using data captured in various environments. Our
method shows better visual quality and robustness in the tested scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeMOT: Multi-Object Tracking with Memory. (arXiv:2203.16761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16761">
<div class="article-summary-box-inner">
<span><p>We propose an online tracking algorithm that performs the object detection
and data association under a common framework, capable of linking objects after
a long time span. This is realized by preserving a large spatio-temporal memory
to store the identity embeddings of the tracked objects, and by adaptively
referencing and aggregating useful information from the memory as needed. Our
model, called MeMOT, consists of three main modules that are all
Transformer-based: 1) Hypothesis Generation that produce object proposals in
the current video frame; 2) Memory Encoding that extracts the core information
from the memory for each tracked object; and 3) Memory Decoding that solves the
object detection and data association tasks simultaneously for multi-object
tracking. When evaluated on widely adopted MOT benchmark datasets, MeMOT
observes very competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREATE: A Benchmark for Chinese Short Video Retrieval and Title Generation. (arXiv:2203.16763v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16763">
<div class="article-summary-box-inner">
<span><p>Previous works of video captioning aim to objectively describe the video's
actual content, which lacks subjective and attractive expression, limiting its
practical application scenarios. Video titling is intended to achieve this
goal, but there is a lack of a proper benchmark. In this paper, we propose to
CREATE, the first large-scale Chinese shoRt vidEo retrievAl and Title
gEneration benchmark, to facilitate research and application in video titling
and video retrieval in Chinese. CREATE consists of a high-quality labeled 210K
dataset and two large-scale 3M/10M pre-training datasets, covering 51
categories, 50K+ tags, 537K manually annotated titles and captions, and 10M+
short videos. Based on CREATE, we propose a novel model ALWIG which combines
video retrieval and video titling tasks to achieve the purpose of multi-modal
ALignment WIth Generation with the help of video tags and a GPT pre-trained
model. CREATE opens new directions for facilitating future research and
applications on video titling and video retrieval in the field of Chinese short
videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpatioTemporal Focus for Skeleton-based Action Recognition. (arXiv:2203.16767v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16767">
<div class="article-summary-box-inner">
<span><p>Graph convolutional networks (GCNs) are widely adopted in skeleton-based
action recognition due to their powerful ability to model data topology. We
argue that the performance of recent proposed skeleton-based action recognition
methods is limited by the following factors. First, the predefined graph
structures are shared throughout the network, lacking the flexibility and
capacity to model the multi-grain semantic information. Second, the relations
among the global joints are not fully exploited by the graph local convolution,
which may lose the implicit joint relevance. For instance, actions such as
running and waving are performed by the co-movement of body parts and joints,
e.g., legs and arms, however, they are located far away in physical connection.
Inspired by the recent attention mechanism, we propose a multi-grain contextual
focus module, termed MCF, to capture the action associated relation information
from the body joints and parts. As a result, more explainable representations
for different skeleton action sequences can be obtained by MCF. In this study,
we follow the common practice that the dense sample strategy of the input
skeleton sequences is adopted and this brings much redundancy since number of
instances has nothing to do with actions. To reduce the redundancy, a temporal
discrimination focus module, termed TDF, is developed to capture the local
sensitive points of the temporal dynamics. MCF and TDF are integrated into the
standard GCN network to form a unified architecture, named STF-Net. It is noted
that STF-Net provides the capability to capture robust movement patterns from
these skeleton topology structures, based on multi-grain context aggregation
and temporal dependency. Extensive experimental results show that our STF-Net
significantly achieves state-of-the-art results on three challenging benchmarks
NTU RGB+D 60, NTU RGB+D 120, and Kinetics-skeleton.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReSTR: Convolution-free Referring Image Segmentation Using Transformers. (arXiv:2203.16768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16768">
<div class="article-summary-box-inner">
<span><p>Referring image segmentation is an advanced semantic segmentation task where
target is not a predefined class but is described in natural language. Most of
existing methods for this task rely heavily on convolutional neural networks,
which however have trouble capturing long-range dependencies between entities
in the language expression and are not flexible enough for modeling
interactions between the two different modalities. To address these issues, we
present the first convolution-free model for referring image segmentation using
transformers, dubbed ReSTR. Since it extracts features of both modalities
through transformer encoders, it can capture long-range dependencies between
entities within each modality. Also, ReSTR fuses features of the two modalities
by a self-attention encoder, which enables flexible and adaptive interactions
between the two modalities in the fusion process. The fused features are fed to
a segmentation module, which works adaptively according to the image and
language expression in hand. ReSTR is evaluated and compared with previous work
on all public benchmarks, where it outperforms all existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAKe-Net: Topology-Aware Point Cloud Completion by Localizing Aligned Keypoints. (arXiv:2203.16771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16771">
<div class="article-summary-box-inner">
<span><p>Point cloud completion aims at completing geometric and topological shapes
from a partial observation. However, some topology of the original shape is
missing, existing methods directly predict the location of complete points,
without predicting structured and topological information of the complete
shape, which leads to inferior performance. To better tackle the missing
topology part, we propose LAKe-Net, a novel topology-aware point cloud
completion model by localizing aligned keypoints, with a novel
Keypoints-Skeleton-Shape prediction manner. Specifically, our method completes
missing topology using three steps: 1) Aligned Keypoint Localization. An
asymmetric keypoint locator, including an unsupervised multi-scale keypoint
detector and a complete keypoint generator, is proposed for localizing aligned
keypoints from complete and partial point clouds. We theoretically prove that
the detector can capture aligned keypoints for objects within a sub-category.
2) Surface-skeleton Generation. A new type of skeleton, named Surface-skeleton,
is generated from keypoints based on geometric priors to fully represent the
topological information captured from keypoints and better recover the local
details. 3) Shape Refinement. We design a refinement subnet where multi-scale
surface-skeletons are fed into each recursive skeleton-assisted refinement
module to assist the completion process. Experimental results show that our
method achieves the state-of-the-art performance on point cloud completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask Atari for Deep Reinforcement Learning as POMDP Benchmarks. (arXiv:2203.16777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16777">
<div class="article-summary-box-inner">
<span><p>We present Mask Atari, a new benchmark to help solve partially observable
Markov decision process (POMDP) problems with Deep Reinforcement Learning
(DRL)-based approaches. To achieve a simulation environment for the POMDP
problems, Mask Atari is constructed based on Atari 2600 games with
controllable, moveable, and learnable masks as the observation area for the
target agent, especially with the active information gathering (AIG) setting in
POMDPs. Given that one does not yet exist, Mask Atari provides a challenging,
efficient benchmark for evaluating the methods that focus on the above problem.
Moreover, the mask operation is a trial for introducing the receptive field in
the human vision system into a simulation environment for an agent, which means
the evaluations are not biased from the sensing ability and purely focus on the
cognitive performance of the methods when compared with the human baseline. We
describe the challenges and features of our benchmark and evaluate several
baselines with Mask Atari.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval. (arXiv:2203.16778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16778">
<div class="article-summary-box-inner">
<span><p>Visual appearance is considered to be the most important cue to understand
images for cross-modal retrieval, while sometimes the scene text appearing in
images can provide valuable information to understand the visual semantics.
Most of existing cross-modal retrieval approaches ignore the usage of scene
text information and directly adding this information may lead to performance
degradation in scene text free scenarios. To address this issue, we propose a
full transformer architecture to unify these cross-modal retrieval scenarios in
a single $\textbf{Vi}$sion and $\textbf{S}$cene $\textbf{T}$ext
$\textbf{A}$ggregation framework (ViSTA). Specifically, ViSTA utilizes
transformer blocks to directly encode image patches and fuse scene text
embedding to learn an aggregated visual representation for cross-modal
retrieval. To tackle the modality missing problem of scene text, we propose a
novel fusion token based transformer aggregation approach to exchange the
necessary scene text information only through the fusion token and concentrate
on the most important features in each modality. To further strengthen the
visual modality, we develop dual contrastive learning losses to embed both
image-text pairs and fusion-text pairs into a common cross-modal space.
Compared to existing methods, ViSTA enables to aggregate relevant scene text
semantics with visual appearance, and hence improve results under both scene
text free and scene text aware scenarios. Experimental results show that ViSTA
outperforms other methods by at least $\bf{8.4}\%$ at Recall@1 for scene text
aware retrieval task. Compared with state-of-the-art scene text free retrieval
methods, ViSTA can achieve better accuracy on Flicker30K and MSCOCO while
running at least three times faster during the inference stage, which validates
the effectiveness of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Patch Label Inference Networks for Efficient Pavement Distress Detection and Recognition in the Wild. (arXiv:2203.16782v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16782">
<div class="article-summary-box-inner">
<span><p>Automatic image-based pavement distress detection and recognition are vital
for pavement maintenance and management. However, existing deep learning-based
methods largely omit the specific characteristics of pavement images, such as
high image resolution and low distress area ratio, and are not end-to-end
trainable. In this paper, we present a series of simple yet effective
end-to-end deep learning approaches named Weakly Supervised Patch Label
Inference Networks (WSPLIN) for efficiently addressing these tasks under
various application settings. To fully exploit the resolution and scale
information, WSPLIN first divides the pavement image under different scales
into patches with different collection strategies and then employs a Patch
Label Inference Network (PLIN) to infer the labels of these patches. Notably,
we design a patch label sparsity constraint based on the prior knowledge of
distress distribution, and leverage the Comprehensive Decision Network (CDN) to
guide the training of PLIN in a weakly supervised way. Therefore, the patch
labels produced by PLIN provide interpretable intermediate information, such as
the rough location and the type of distress. We evaluate our method on a
large-scale bituminous pavement distress dataset named CQU-BPDD. Extensive
results demonstrate the superiority of our method over baselines in both
performance and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-Text Representation Learning via Differentiable Weak Temporal Alignment. (arXiv:2203.16784v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16784">
<div class="article-summary-box-inner">
<span><p>Learning generic joint representations for video and text by a supervised
method requires a prohibitively substantial amount of manually annotated video
datasets. As a practical alternative, a large-scale but uncurated and narrated
video dataset, HowTo100M, has recently been introduced. But it is still
challenging to learn joint embeddings of video and text in a self-supervised
manner, due to its ambiguity and non-sequential alignment. In this paper, we
propose a novel multi-modal self-supervised framework Video-Text Temporally
Weak Alignment-based Contrastive Learning (VT-TWINS) to capture significant
information from noisy and weakly correlated data using a variant of Dynamic
Time Warping (DTW). We observe that the standard DTW inherently cannot handle
weakly correlated data and only considers the globally optimal alignment path.
To address these problems, we develop a differentiable DTW which also reflects
local information with weak temporal alignment. Moreover, our proposed model
applies a contrastive learning scheme to learn feature representations on
weakly correlated data. Our extensive experiments demonstrate that VT-TWINS
attains significant improvements in multi-modal representation learning and
outperforms various challenging downstream tasks. Code is available at
https://github.com/mlvlab/VT-TWINS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflection and Rotation Symmetry Detection via Equivariant Learning. (arXiv:2203.16787v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16787">
<div class="article-summary-box-inner">
<span><p>The inherent challenge of detecting symmetries stems from arbitrary
orientations of symmetry patterns; a reflection symmetry mirrors itself against
an axis with a specific orientation while a rotation symmetry matches its
rotated copy with a specific orientation. Discovering such symmetry patterns
from an image thus benefits from an equivariant feature representation, which
varies consistently with reflection and rotation of the image. In this work, we
introduce a group-equivariant convolutional network for symmetry detection,
dubbed EquiSym, which leverages equivariant feature maps with respect to a
dihedral group of reflection and rotation. The proposed network is built
end-to-end with dihedrally-equivariant layers and trained to output a spatial
map for reflection axes or rotation centers. We also present a new dataset,
DENse and DIverse symmetry (DENDI), which mitigates limitations of existing
benchmarks for reflection and rotation symmetry detection. Experiments show
that our method achieves the state of the arts in symmetry detection on LDRS
and DENDI datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable Video Transformer. (arXiv:2203.16795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16795">
<div class="article-summary-box-inner">
<span><p>Video transformers have recently emerged as an effective alternative to
convolutional networks for action classification. However, most prior video
transformers adopt either global space-time attention or hand-defined
strategies to compare patches within and across frames. These fixed attention
schemes not only have high computational cost but, by comparing patches at
predetermined locations, they neglect the motion dynamics in the video. In this
paper, we introduce the Deformable Video Transformer (DVT), which dynamically
predicts a small subset of video patches to attend for each query location
based on motion information, thus allowing the model to decide where to look in
the video based on correspondences across frames. Crucially, these motion-based
correspondences are obtained at zero-cost from information stored in the
compressed format of the video. Our deformable attention mechanism is optimised
directly with respect to classification performance, thus eliminating the need
for suboptimal hand-design of attention strategies. Experiments on four
large-scale video benchmarks (Kinetics-400, Something-Something-V2,
EPIC-KITCHENS and Diving-48) demonstrate that, compared to existing video
transformers, our model achieves higher accuracy at the same or lower
computational cost, and it attains state-of-the-art results on these four
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ternary and Binary Quantization for Improved Classification. (arXiv:2203.16798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16798">
<div class="article-summary-box-inner">
<span><p>Dimension reduction and data quantization are two important methods for
reducing data complexity. In the paper, we study the methodology of first
reducing data dimension by random projection and then quantizing the
projections to ternary or binary codes, which has been widely applied in
classification. Usually, the quantization will seriously degrade the accuracy
of classification due to high quantization errors. Interestingly, however, we
observe that the quantization could provide comparable and often superior
accuracy, as the data to be quantized are sparse features generated with common
filters. Furthermore, this quantization property could be maintained in the
random projections of sparse features, if both the features and random
projection matrices are sufficiently sparse. By conducting extensive
experiments, we validate and analyze this intriguing property.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Temporal Contrastive Learning for Weakly-supervised Temporal Action Localization. (arXiv:2203.16800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16800">
<div class="article-summary-box-inner">
<span><p>We target at the task of weakly-supervised action localization (WSAL), where
only video-level action labels are available during model training. Despite the
recent progress, existing methods mainly embrace a
localization-by-classification paradigm and overlook the fruitful fine-grained
temporal distinctions between video sequences, thus suffering from severe
ambiguity in classification learning and classification-to-localization
adaption. This paper argues that learning by contextually comparing
sequence-to-sequence distinctions offers an essential inductive bias in WSAL
and helps identify coherent action instances. Specifically, under a
differentiable dynamic programming formulation, two complementary contrastive
objectives are designed, including Fine-grained Sequence Distance (FSD)
contrasting and Longest Common Subsequence (LCS) contrasting, where the first
one considers the relations of various action/background proposals by using
match, insert, and delete operators and the second one mines the longest common
subsequences between two videos. Both contrasting modules can enhance each
other and jointly enjoy the merits of discriminative action-background
separation and alleviated task gap between classification and localization.
Extensive experiments show that our method achieves state-of-the-art
performance on two popular benchmarks. Our code is available at
https://github.com/MengyuanChen21/CVPR2022-FTCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Portrait Matting with Privacy Preserving. (arXiv:2203.16828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16828">
<div class="article-summary-box-inner">
<span><p>Recently, there has been an increasing concern about the privacy issue raised
by using personally identifiable information in machine learning. However,
previous portrait matting methods were all based on identifiable portrait
images. To fill the gap, we present P3M-10k in this paper, which is the first
large-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M).
P3M-10k consists of 10,000 high-resolution face-blurred portrait images along
with high-quality alpha mattes. We systematically evaluate both trimap-free and
trimap-based matting methods on P3M-10k and find that existing matting methods
show different generalization abilities under the privacy preserving training
setting, i.e., training only on face-blurred images while testing on arbitrary
images. Based on the gained insights, we propose a unified matting model named
P3M-Net consisting of three carefully designed integration modules that can
perform privacy-insensitive semantic perception and detail-reserved matting
simultaneously. We further design multiple variants of P3M-Net with different
CNN and transformer backbones and identify the difference in their
generalization abilities. To further mitigate this issue, we devise a simple
yet effective Copy and Paste strategy (P3M-CP) that can borrow facial
information from public celebrity images without privacy concerns and direct
the network to reacquire the face context at both data and feature levels.
P3M-CP only brings a few additional computations during training, while
enabling the matting model to process both face-blurred and normal images
without extra effort during inference. Extensive experiments on P3M-10k
demonstrate the superiority of P3M-Net over state-of-the-art methods and the
effectiveness of P3M-CP in improving the generalization ability of P3M-Net,
implying a great significance of P3M for future research and real-world
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Scene Understanding via Disentangled Instance Mesh Reconstruction. (arXiv:2203.16832v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16832">
<div class="article-summary-box-inner">
<span><p>Semantic scene reconstruction from point cloud is an essential and
challenging task for 3D scene understanding. This task requires not only to
recognize each instance in the scene, but also to recover their geometries
based on the partial observed point cloud. Existing methods usually attempt to
directly predict occupancy values of the complete object based on incomplete
point cloud proposals from a detection-based backbone. However, this framework
always fails to reconstruct high fidelity mesh due to the obstruction of
various detected false positive object proposals and the ambiguity of
incomplete point observations for learning occupancy values of complete
objects. To circumvent the hurdle, we propose a Disentangled Instance Mesh
Reconstruction (DIMR) framework for effective point scene understanding. A
segmentation-based backbone is applied to reduce false positive object
proposals, which further benefits our exploration on the relationship between
recognition and reconstruction. Based on the accurate proposals, we leverage a
mesh-aware latent code space to disentangle the processes of shape completion
and mesh generation, relieving the ambiguity caused by the incomplete point
observations. Furthermore, with access to the CAD model pool at test time, our
model can also be used to improve the reconstruction quality by performing mesh
retrieval without extra training. We thoroughly evaluate the reconstructed mesh
quality with multiple metrics, and demonstrate the superiority of our method on
the challenging ScanNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Extraction with Co-Speech Gestures Cue. (arXiv:2203.16840v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16840">
<div class="article-summary-box-inner">
<span><p>Speaker extraction seeks to extract the clean speech of a target speaker from
a multi-talker mixture speech. There have been studies to use a pre-recorded
speech sample or face image of the target speaker as the speaker cue. In human
communication, co-speech gestures that are naturally timed with speech also
contribute to speech perception. In this work, we explore the use of co-speech
gestures sequence, e.g. hand and body movements, as the speaker cue for speaker
extraction, which could be easily obtained from low-resolution video
recordings, thus more available than face recordings. We propose two networks
using the co-speech gestures cue to perform attentive listening on the target
speaker, one that implicitly fuses the co-speech gestures cue in the speaker
extraction process, the other performs speech separation first, followed by
explicitly using the co-speech gestures cue to associate a separated speech to
the target speaker. The experimental results show that the co-speech gestures
cue is informative in associating the target speaker, and the quality of the
extracted speech shows significant improvements over the unprocessed mixture
speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Driving-Oriented Metric for Lane Detection Models. (arXiv:2203.16851v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16851">
<div class="article-summary-box-inner">
<span><p>After the 2017 TuSimple Lane Detection Challenge, its dataset and evaluation
based on accuracy and F1 score have become the de facto standard to measure the
performance of lane detection methods. While they have played a major role in
improving the performance of lane detection methods, the validity of this
evaluation method in downstream tasks has not been adequately researched. In
this study, we design 2 new driving-oriented metrics for lane detection:
End-to-End Lateral Deviation metric (E2E-LD) is directly formulated based on
the requirements of autonomous driving, a core downstream task of lane
detection; Per-frame Simulated Lateral Deviation metric (PSLD) is a lightweight
surrogate metric of E2E-LD. To evaluate the validity of the metrics, we conduct
a large-scale empirical study with 4 major types of lane detection approaches
on the TuSimple dataset and our newly constructed dataset Comma2k19-LD. Our
results show that the conventional metrics have strongly negative correlations
($\leq$-0.55) with E2E-LD, meaning that some recent improvements purely
targeting the conventional metrics may not have led to meaningful improvements
in autonomous driving, but rather may actually have made it worse by
overfitting to the conventional metrics. As autonomous driving is a
security/safety-critical system, the underestimation of robustness hinders the
sound development of practical lane detection models. We hope that our study
will help the community achieve more downstream task-aware evaluations for lane
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Modality Bias in Audio Visual Video Parsing. (arXiv:2203.16860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16860">
<div class="article-summary-box-inner">
<span><p>We focus on the audio-visual video parsing (AVVP) problem that involves
detecting audio and visual event labels with temporal boundaries. The task is
especially challenging since it is weakly supervised with only event labels
available as a bag of labels for each video. An existing state-of-the-art model
for AVVP uses a hybrid attention network (HAN) to generate cross-modal features
for both audio and visual modalities, and an attentive pooling module that
aggregates predicted audio and visual segment-level event probabilities to
yield video-level event probabilities. We provide a detailed analysis of
modality bias in the existing HAN architecture, where a modality is completely
ignored during prediction. We also propose a variant of feature aggregation in
HAN that leads to an absolute gain in F-scores of about 2% and 1.6% for visual
and audio-visual events at both segment-level and event-level, in comparison to
the existing HAN model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16875">
<div class="article-summary-box-inner">
<span><p>There has been rapid progress recently on 3D human rendering, including novel
view synthesis and pose animation, based on the advances of neural radiance
fields (NeRF). However, most existing methods focus on person-specific training
and their training typically requires multi-view videos. This paper deals with
a new challenging task -- rendering novel views and novel poses for a person
unseen in training, using only multiview images as input. For this task, we
propose a simple yet effective method to train a generalizable NeRF with
multiview images as conditional input. The key ingredient is a dedicated
representation combining a canonical NeRF and a volume deformation scheme.
Using a canonical space enables our method to learn shared properties of human
and easily generalize to different people. Volume deformation is used to
connect the canonical space with input and target images and query image
features for radiance and density prediction. We leverage the parametric 3D
human model fitted on the input images to derive the deformation, which works
quite well in practice when combined with our canonical NeRF. The experiments
on both real and synthetic data with the novel view synthesis and pose
animation tasks collectively demonstrate the efficacy of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformation and Correspondence Aware Unsupervised Synthetic-to-Real Scene Flow Estimation for Point Clouds. (arXiv:2203.16895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16895">
<div class="article-summary-box-inner">
<span><p>Point cloud scene flow estimation is of practical importance for dynamic
scene navigation in autonomous driving. Since scene flow labels are hard to
obtain, current methods train their models on synthetic data and transfer them
to real scenes. However, large disparities between existing synthetic datasets
and real scenes lead to poor model transfer. We make two major contributions to
address that. First, we develop a point cloud collector and scene flow
annotator for GTA-V engine to automatically obtain diverse realistic training
samples without human intervention. With that, we develop a large-scale
synthetic scene flow dataset GTA-SF. Second, we propose a mean-teacher-based
domain adaptation framework that leverages self-generated pseudo-labels of the
target domain. It also explicitly incorporates shape deformation regularization
and surface correspondence refinement to address distortions and misalignments
in domain transfer. Through extensive experiments, we show that our GTA-SF
dataset leads to a consistent boost in model generalization to three real
datasets (i.e., Waymo, Lyft and KITTI) as compared to the most widely used FT3D
dataset. Moreover, our framework achieves superior adaptation performance on
six source-target dataset pairs, remarkably closing the average domain gap by
60%. Data and codes are available at https://github.com/leolyj/DCA-SRSFE
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow. (arXiv:2203.16896v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16896">
<div class="article-summary-box-inner">
<span><p>Optical flow estimation aims to find the 2D motion field by identifying
corresponding pixels between two images. Despite the tremendous progress of
deep learning-based optical flow methods, it remains a challenge to accurately
estimate large displacements with motion blur. This is mainly because the
correlation volume, the basis of pixel matching, is computed as the dot product
of the convolutional features of the two images. The locality of convolutional
features makes the computed correlations susceptible to various noises. On
large displacements with motion blur, noisy correlations could cause severe
errors in the estimated flow. To overcome this challenge, we propose a new
architecture "CRoss-Attentional Flow Transformer" (CRAFT), aiming to revitalize
the correlation volume computation. In CRAFT, a Semantic Smoothing Transformer
layer transforms the features of one frame, making them more global and
semantically stable. In addition, the dot-product correlations are replaced
with transformer Cross-Frame Attention. This layer filters out feature noises
through the Query and Key projections, and computes more accurate correlations.
On Sintel (Final) and KITTI (foreground) benchmarks, CRAFT has achieved new
state-of-the-art performance. Moreover, to test the robustness of different
models on large motions, we designed an image shifting attack that shifts input
images to generate large artificial motions. Under this attack, CRAFT performs
much more robustly than two representative methods, RAFT and GMA. The code of
CRAFT is is available at https://github.com/askerlee/craft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Granularity Alignment Domain Adaptation for Object Detection. (arXiv:2203.16897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16897">
<div class="article-summary-box-inner">
<span><p>Domain adaptive object detection is challenging due to distinctive data
distribution between source domain and target domain. In this paper, we propose
a unified multi-granularity alignment based object detection framework towards
domain-invariant feature learning. To this end, we encode the dependencies
across different granularity perspectives including pixel-, instance-, and
category-levels simultaneously to align two domains. Based on pixel-level
feature maps from the backbone network, we first develop the omni-scale gated
fusion module to aggregate discriminative representations of instances by
scale-aware convolutions, leading to robust multi-scale object detection.
Meanwhile, the multi-granularity discriminators are proposed to identify which
domain different granularities of samples(i.e., pixels, instances, and
categories) come from. Notably, we leverage not only the instance
discriminability in different categories but also the category consistency
between two domains. Extensive experiments are carried out on multiple domain
adaptation scenarios, demonstrating the effectiveness of our framework over
state-of-the-art algorithms on top of anchor-free FCOS and anchor-based Faster
RCNN detectors with different backbones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis. (arXiv:2203.16898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16898">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed substantial progress in semantic image synthesis,
it is still challenging in synthesizing photo-realistic images with rich
details. Most previous methods focus on exploiting the given semantic map,
which just captures an object-level layout for an image. Obviously, a
fine-grained part-level semantic layout will benefit object details generation,
and it can be roughly inferred from an object's shape. In order to exploit the
part-level layouts, we propose a Shape-aware Position Descriptor (SPD) to
describe each pixel's positional feature, where object shape is explicitly
encoded into the SPD feature. Furthermore, a Semantic-shape Adaptive Feature
Modulation (SAFM) block is proposed to combine the given semantic map and our
positional features to produce adaptively modulated features. Extensive
experiments demonstrate that the proposed SPD and SAFM significantly improve
the generation of objects with rich details. Moreover, our method performs
favorably against the SOTA methods in terms of quantitative and qualitative
evaluation. The source code and model are available at
https://github.com/cszy98/SAFM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Trajectory Distribution Prediction Based on Occupancy Grid Maps. (arXiv:2203.16910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16910">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to forecast a future trajectory distribution of a
moving agent in the real world, given the social scene images and historical
trajectories. Yet, it is a challenging task because the ground-truth
distribution is unknown and unobservable, while only one of its samples can be
applied for supervising model learning, which is prone to bias. Most recent
works focus on predicting diverse trajectories in order to cover all modes of
the real distribution, but they may despise the precision and thus give too
much credit to unrealistic predictions. To address the issue, we learn the
distribution with symmetric cross-entropy using occupancy grid maps as an
explicit and scene-compliant approximation to the ground-truth distribution,
which can effectively penalize unlikely predictions. In specific, we present an
inverse reinforcement learning based multi-modal trajectory distribution
forecasting framework that learns to plan by an approximate value iteration
network in an end-to-end manner. Besides, based on the predicted distribution,
we generate a small set of representative trajectories through a differentiable
Transformer-based network, whose attention mechanism helps to model the
relations of trajectories. In experiments, our method achieves state-of-the-art
performance on the Stanford Drone Dataset and Intersection Drone Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques. (arXiv:2203.16915v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16915">
<div class="article-summary-box-inner">
<span><p>A dataset of street light images is presented. Our dataset consists of
$\sim350\textrm{k}$ images, taken from 140 UMBRELLA nodes installed in the
South Gloucestershire region in the UK. Each UMBRELLA node is installed on the
pole of a lamppost and is equipped with a Raspberry Pi Camera Module v1 facing
upwards towards the sky and lamppost light bulb. Each node collects an image at
hourly intervals for 24h every day. The data collection spans for a period of
six months.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond. (arXiv:2203.16931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16931">
<div class="article-summary-box-inner">
<span><p>Rain removal aims to remove rain streaks from images/videos and reduce the
disruptive effects caused by rain. It not only enhances image/video visibility
but also allows many computer vision algorithms to function properly. This
paper makes the first attempt to conduct a comprehensive study on the
robustness of deep learning-based rain removal methods against adversarial
attacks. Our study shows that, when the image/video is highly degraded, rain
removal methods are more vulnerable to the adversarial attacks as small
distortions/perturbations become less noticeable or detectable. In this paper,
we first present a comprehensive empirical evaluation of various methods at
different levels of attacks and with various losses/targets to generate the
perturbations from the perspective of human perception and machine analysis
tasks. A systematic evaluation of key modules in existing methods is performed
in terms of their robustness against adversarial attacks. From the insights of
our analysis, we construct a more robust deraining method by integrating these
effective modules. Finally, we examine various types of adversarial attacks
that are specific to deraining problems and their effects on both human and
machine vision tasks, including 1) rain region attacks, adding perturbations
only in the rain regions to make the perturbations in the attacked rain images
less visible; 2) object-sensitive attacks, adding perturbations only in regions
near the given objects. Code is available at
https://github.com/yuyi-sd/Robust_Rain_Removal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contributions to interframe coding. (arXiv:2203.16934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16934">
<div class="article-summary-box-inner">
<span><p>Advanced motion models (4 or 6 parameters) are needed for a good
representation of the motion experimented by the different objects contained in
a sequence of images. If the image is split in very small blocks, then an
accurate description of complex movements can be achieved with only 2
parameters. This alternative implies a large set of vectors per image. We
propose a new approach to reduce the number of vectors, using different block
sizes as a function of the local characteristics of the image, without
increasing the error accepted with the smallest blocks. A second algorithm is
proposed for an inter/intraframe coder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Pose Verification for Outdoor Visual Localization with Self-supervised Contrastive Learning. (arXiv:2203.16945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16945">
<div class="article-summary-box-inner">
<span><p>Any city-scale visual localization system has to overcome long-term
appearance changes, such as varying illumination conditions or seasonal changes
between query and database images. Since semantic content is more robust to
such changes, we exploit semantic information to improve visual localization.
In our scenario, the database consists of gnomonic views generated from
panoramic images (e.g. Google Street View) and query images are collected with
a standard field-of-view camera at a different time. To improve localization,
we check the semantic similarity between query and database images, which is
not trivial since the position and viewpoint of the cameras do not exactly
match. To learn similarity, we propose training a CNN in a self-supervised
fashion with contrastive learning on a dataset of semantically segmented
images. With experiments we showed that this semantic similarity estimation
approach works better than measuring the similarity at pixel-level. Finally, we
used the semantic similarity scores to verify the retrievals obtained by a
state-of-the-art visual localization method and observed that contrastive
learning-based pose verification increases top-1 recall value to 0.90 which
corresponds to a 2% improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Fusion Transformer for Remote Sensing Image Classification. (arXiv:2203.16952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16952">
<div class="article-summary-box-inner">
<span><p>Vision transformer (ViT) has been trending in image classification tasks due
to its promising performance when compared to convolutional neural networks
(CNNs). As a result, many researchers have tried to incorporate ViT models in
hyperspectral image (HSI) classification tasks, but without achieving
satisfactory performance. To this paper, we introduce a new multimodal fusion
transformer (MFT) network for HSI land-cover classification, which utilizes
other sources of multimodal data in addition to HSI. Instead of using
conventional feature fusion techniques, other multimodal data are used as an
external classification (CLS) token in the transformer encoder, which helps
achieving better generalization. ViT and other similar transformer models use a
randomly initialized external classification token {and fail to generalize
well}. However, the use of a feature embedding derived from other sources of
multimodal data, such as light detection and ranging (LiDAR), offers the
potential to improve those models by means of a CLS. The concept of
tokenization is used in our work to generate CLS and HSI patch tokens, helping
to learn key features in a reduced feature space. We also introduce a new
attention mechanism for improving the exchange of information between HSI
tokens and the CLS (e.g., LiDAR) token. Extensive experiments are carried out
on widely used and benchmark datasets i.e., the University of Houston, Trento,
University of Southern Mississippi Gulfpark (MUUFL), and Augsburg. In the
results section, we compare the proposed MFT model with other state-of-the-art
transformer models, classical CNN models, as well as conventional classifiers.
The superior performance achieved by the proposed model is due to the use of
multimodal information as external classification tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Instance Segmentation and Tracking via Data Association and Single-stage Detector. (arXiv:2203.16966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16966">
<div class="article-summary-box-inner">
<span><p>Human video instance segmentation plays an important role in computer
understanding of human activities and is widely used in video processing, video
surveillance, and human modeling in virtual reality. Most current VIS methods
are based on Mask-RCNN framework, where the target appearance and motion
information for data matching will increase computational cost and have an
impact on segmentation real-time performance; on the other hand, the existing
datasets for VIS focus less on all the people appearing in the video. In this
paper, to solve the problems, we develop a new method for human video instance
segmentation based on single-stage detector. To tracking the instance across
the video, we have adopted data association strategy for matching the same
instance in the video sequence, where we jointly learn target instance
appearances and their affinities in a pair of video frames in an end-to-end
fashion. We have also adopted the centroid sampling strategy for enhancing the
embedding extraction ability of instance, which is to bias the instance
position to the inside of each instance mask with heavy overlap condition. As a
result, even there exists a sudden change in the character activity, the
instance position will not move out of the mask, so that the problem that the
same instance is represented by two different instances can be alleviated.
Finally, we collect PVIS dataset by assembling several video instance
segmentation datasets to fill the gap of the current lack of datasets dedicated
to human video segmentation. Extensive simulations based on such dataset has
been conduct. Simulation results verify the effectiveness and efficiency of the
proposed work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification. (arXiv:2203.16983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16983">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has drawn increasing attention in pathological
image analysis in recent years. However, the prevalent contrastive SSL is
suboptimal in feature representation under this scenario due to the homogeneous
visual appearance. Alternatively, masked autoencoders (MAE) build SSL from a
generative paradigm. They are more friendly to pathological image modeling. In
this paper, we firstly introduce MAE to pathological image analysis. A novel
SD-MAE model is proposed to enable a self-distillation augmented SSL on top of
the raw MAE. Besides the reconstruction loss on masked image patches, SD-MAE
further imposes the self-distillation loss on visible patches. It guides the
encoder to perceive high-level semantics that benefit downstream tasks. We
apply SD-MAE to the image classification task on two pathological and one
natural image datasets. Experiments demonstrate that SD-MAE performs highly
competitive when compared with leading contrastive SSL methods. The results,
which are pre-trained using a moderate size of pathological images, are also
comparable to the method pre-trained with two orders of magnitude more images.
Our code will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring hand use in the home after cervical spinal cord injury using egocentric video. (arXiv:2203.16996v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16996">
<div class="article-summary-box-inner">
<span><p>Background: Egocentric video has recently emerged as a potential solution for
monitoring hand function in individuals living with tetraplegia in the
community, especially for its ability to detect functional use in the home
environment. Objective: To develop and validate a wearable vision-based system
for measuring hand use in the home among individuals living with tetraplegia.
Methods: Several deep learning algorithms for detecting functional hand-object
interactions were developed and compared. The most accurate algorithm was used
to extract measures of hand function from 65 hours of unscripted video recorded
at home by 20 participants with tetraplegia. These measures were: the
percentage of interaction time over total recording time (Perc); the average
duration of individual interactions (Dur); the number of interactions per hour
(Num). To demonstrate the clinical validity of the technology, egocentric
measures were correlated with validated clinical assessments of hand function
and independence (Graded Redefined Assessment of Strength, Sensibility and
Prehension - GRASSP, Upper Extremity Motor Score - UEMS, and Spinal Cord
Independent Measure - SCIM). Results: Hand-object interactions were
automatically detected with a median F1-score of 0.80 (0.67-0.87). Our results
demonstrated that higher UEMS and better prehension were related to greater
time spent interacting, whereas higher SCIM and better hand sensation resulted
in a higher number of interactions performed during the egocentric video
recordings. Conclusions: For the first time, measures of hand function
automatically estimated in an unconstrained environment in individuals with
tetraplegia have been validated against internationally accepted measures of
hand function. Future work will necessitate a formal evaluation of the
reliability and responsiveness of the egocentric-based performance measures for
hand use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher. (arXiv:2203.17008v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17008">
<div class="article-summary-box-inner">
<span><p>Model quantization is considered as a promising method to greatly reduce the
resource requirements of deep neural networks. To deal with the performance
drop induced by quantization errors, a popular method is to use training data
to fine-tune quantized networks. In real-world environments, however, such a
method is frequently infeasible because training data is unavailable due to
security, privacy, or confidentiality concerns. Zero-shot quantization
addresses such problems, usually by taking information from the weights of a
full-precision teacher network to compensate the performance drop of the
quantized networks. In this paper, we first analyze the loss surface of
state-of-the-art zero-shot quantization techniques and provide several
findings. In contrast to usual knowledge distillation problems, zero-shot
quantization often suffers from 1) the difficulty of optimizing multiple loss
terms together, and 2) the poor generalization capability due to the use of
synthetic samples. Furthermore, we observe that many weights fail to cross the
rounding threshold during training the quantized networks even when it is
necessary to do so for better performance. Based on the observations, we
propose AIT, a simple yet powerful technique for zero-shot quantization, which
addresses the aforementioned two problems in the following way: AIT i) uses a
KL distance loss only without a cross-entropy loss, and ii) manipulates
gradients to guarantee that a certain portion of weights are properly updated
after crossing the rounding thresholds. Experiments show that AIT outperforms
the performance of many existing methods by a great margin, taking over the
overall state-of-the-art position in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Temporal Learning Approach to Inpainting Endoscopic Specularities and Its effect on Image Correspondence. (arXiv:2203.17013v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17013">
<div class="article-summary-box-inner">
<span><p>Video streams are utilised to guide minimally-invasive surgery and diagnostic
procedures in a wide range of procedures, and many computer assisted techniques
have been developed to automatically analyse them. These approaches can provide
additional information to the surgeon such as lesion detection, instrument
navigation, or anatomy 3D shape modeling. However, the necessary image features
to recognise these patterns are not always reliably detected due to the
presence of irregular light patterns such as specular highlight reflections. In
this paper, we aim at removing specular highlights from endoscopic videos using
machine learning. We propose using a temporal generative adversarial network
(GAN) to inpaint the hidden anatomy under specularities, inferring its
appearance spatially and from neighbouring frames where they are not present in
the same location. This is achieved using in-vivo data of gastric endoscopy
(Hyper-Kvasir) in a fully unsupervised manner that relies on automatic
detection of specular highlights. System evaluations show significant
improvements to traditional methods through direct comparison as well as other
machine learning techniques through an ablation study that depicts the
importance of the network's temporal and transfer learning components. The
generalizability of our system to different surgical setups and procedures was
also evaluated qualitatively on in-vivo data of gastric endoscopy and ex-vivo
porcine data (SERV-CT, SCARED). We also assess the effect of our method in
computer vision tasks that underpin 3D reconstruction and camera motion
estimation, namely stereo disparity, optical flow, and sparse point feature
matching. These are evaluated quantitatively and qualitatively and results show
a positive effect of specular highlight inpainting on these tasks in a novel
comprehensive analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logit Normalization for Long-tail Object Detection. (arXiv:2203.17020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17020">
<div class="article-summary-box-inner">
<span><p>Real-world data exhibiting skewed distributions pose a serious challenge to
existing object detectors. Moreover, the samplers in detectors lead to shifted
training label distributions, while the tremendous proportion of background to
foreground samples severely harms foreground classification. To mitigate these
issues, in this paper, we propose Logit Normalization (LogN), a simple
technique to self-calibrate the classified logits of detectors in a similar way
to batch normalization. In general, our LogN is training- and tuning-free (i.e.
require no extra training and tuning process), model- and label
distribution-agnostic (i.e. generalization to different kinds of detectors and
datasets), and also plug-and-play (i.e. direct application without any bells
and whistles). Extensive experiments on the LVIS dataset demonstrate superior
performance of LogN to state-of-the-art methods with various detectors and
backbones. We also provide in-depth studies on different aspects of our LogN.
Further experiments on ImageNet-LT reveal its competitiveness and
generalizability. Our LogN can serve as a strong baseline for long-tail object
detection and is expected to inspire future research in this field. Code and
trained models will be publicly available at https://github.com/MCG-NJU/LogN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Class-Incremental Learning by Sampling Multi-Phase Tasks. (arXiv:2203.17030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17030">
<div class="article-summary-box-inner">
<span><p>New classes arise frequently in our ever-changing world, e.g., emerging
topics in social media and new types of products in e-commerce. A model should
recognize new classes and meanwhile maintain discriminability over old classes.
Under severe circumstances, only limited novel instances are available to
incrementally update the model. The task of recognizing few-shot new classes
without forgetting old classes is called few-shot class-incremental learning
(FSCIL). In this work, we propose a new paradigm for FSCIL based on
meta-learning by LearnIng Multi-phase Incremental Tasks (LIMIT), which
synthesizes fake FSCIL tasks from the base dataset. The data format of fake
tasks is consistent with the `real' incremental tasks, and we can build a
generalizable feature space for the unseen tasks through meta-learning.
Besides, LIMIT also constructs a calibration module based on transformer, which
calibrates the old class classifiers and new class prototypes into the same
scale and fills in the semantic gap. The calibration module also adaptively
contextualizes the instance-specific embedding with a set-to-set function.
LIMIT efficiently adapts to new classes and meanwhile resists forgetting over
old classes. Experiments on three benchmark datasets (CIFAR100, miniImageNet,
and CUB200) and large-scale dataset, i.e., ImageNet ILSVRC2012 validate that
LIMIT achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection. (arXiv:2203.17054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17054">
<div class="article-summary-box-inner">
<span><p>Single frame data contains finite information which limits the performance of
the existing vision-based multi-camera 3D object detection paradigms. For
fundamentally pushing the performance boundary in this area, BEVDet4D is
proposed to lift the scalable BEVDet paradigm from the spatial-only 3D space to
the spatial-temporal 4D space. We upgrade the framework with a few
modifications just for fusing the feature from the previous frame with the
corresponding one in the current frame. In this way, with negligible extra
computing budget, we enable the algorithm to access the temporal cues by
querying and comparing the two candidate features. Beyond this, we also
simplify the velocity learning task by removing the factors of ego-motion and
time, which equips BEVDet4D with robust generalization performance and reduces
the velocity error by 52.8%. This makes vision-based methods, for the first
time, become comparable with those relied on LiDAR or radar in this aspect. On
challenge benchmark nuScenes, we report a new record of 51.5% NDS with the
high-performance configuration dubbed BEVDet4D-Base, which surpasses the
previous leading method BEVDet by +4.3% NDS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Learning of Graph Representations using Radar Point Cloud for Long-Range Gesture Recognition. (arXiv:2203.17066v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17066">
<div class="article-summary-box-inner">
<span><p>Gesture recognition is one of the most intuitive ways of interaction and has
gathered particular attention for human computer interaction. Radar sensors
possess multiple intrinsic properties, such as their ability to work in low
illumination, harsh weather conditions, and being low-cost and compact, making
them highly preferable for a gesture recognition solution. However, most
literature work focuses on solutions with a limited range that is lower than a
meter. We propose a novel architecture for a long-range (1m - 2m) gesture
recognition solution that leverages a point cloud-based cross-learning approach
from camera point cloud to 60-GHz FMCW radar point cloud, which allows learning
better representations while suppressing noise. We use a variant of Dynamic
Graph CNN (DGCNN) for the cross-learning, enabling us to model relationships
between the points at a local and global level and to model the temporal
dynamics a Bi-LSTM network is employed. In the experimental results section, we
demonstrate our model's overall accuracy of 98.4% for five gestures and its
generalization capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CADG: A Model Based on Cross Attention for Domain Generalization. (arXiv:2203.17067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17067">
<div class="article-summary-box-inner">
<span><p>In Domain Generalization (DG) tasks, models are trained by using only
training data from the source domains to achieve generalization on an unseen
target domain, this will suffer from the distribution shift problem. So it's
important to learn a classifier to focus on the common representation which can
be used to classify on multi-domains, so that this classifier can achieve a
high performance on an unseen target domain as well. With the success of cross
attention in various cross-modal tasks, we find that cross attention is a
powerful mechanism to align the features come from different distributions. So
we design a model named CADG (cross attention for domain generalization),
wherein cross attention plays a important role, to address distribution shift
problem. Such design makes the classifier can be adopted on multi-domains, so
the classifier will generalize well on an unseen domain. Experiments show that
our proposed method achieves state-of-the-art performance on a variety of
domain generalization benchmarks compared with other single model and can even
achieve a better performance than some ensemble-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Hyperspectral Unmixing using Transformer Network. (arXiv:2203.17076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17076">
<div class="article-summary-box-inner">
<span><p>Currently, this paper is under review in IEEE. Transformers have intrigued
the vision research community with their state-of-the-art performance in
natural language processing. With their superior performance, transformers have
found their way in the field of hyperspectral image classification and achieved
promising results. In this article, we harness the power of transformers to
conquer the task of hyperspectral unmixing and propose a novel deep unmixing
model with transformers. We aim to utilize the ability of transformers to
better capture the global feature dependencies in order to enhance the quality
of the endmember spectra and the abundance maps. The proposed model is a
combination of a convolutional autoencoder and a transformer. The hyperspectral
data is encoded by the convolutional encoder. The transformer captures
long-range dependencies between the representations derived from the encoder.
The data are reconstructed using a convolutional decoder. We applied the
proposed unmixing model to three widely used unmixing datasets, i.e., Samson,
Apex, and Washington DC mall and compared it with the state-of-the-art in terms
of root mean squared error and spectral angle distance. The source code for the
proposed model will be made publicly available at
\url{https://github.com/preetam22n/DeepTrans-HSU}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AEGNN: Asynchronous Event-based Graph Neural Networks. (arXiv:2203.17149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17149">
<div class="article-summary-box-inner">
<span><p>The best performing learning algorithms devised for event cameras work by
first converting events into dense representations that are then processed
using standard CNNs. However, these steps discard both the sparsity and high
temporal resolution of events, leading to high computational burden and
latency. For this reason, recent works have adopted Graph Neural Networks
(GNNs), which process events as "static" spatio-temporal graphs, which are
inherently "sparse". We take this trend one step further by introducing
Asynchronous, Event-based Graph Neural Networks (AEGNNs), a novel
event-processing paradigm that generalizes standard GNNs to process events as
"evolving" spatio-temporal graphs. AEGNNs follow efficient update rules that
restrict recomputation of network activations only to the nodes affected by
each new event, thereby significantly reducing both computation and latency for
event-by-event processing. AEGNNs are easily trained on synchronous inputs and
can be converted to efficient, "asynchronous" networks at test time. We
thoroughly validate our method on object classification and detection tasks,
where we show an up to a 200-fold reduction in computational complexity
(FLOPs), with similar or even better performance than state-of-the-art
asynchronous methods. This reduction in computation directly translates to an
8-fold reduction in computational latency when compared to standard GNNs, which
opens the door to low-latency event-based processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Mean-Residue Loss for Robust Facial Age Estimation. (arXiv:2203.17156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17156">
<div class="article-summary-box-inner">
<span><p>Automated facial age estimation has diverse real-world applications in
multimedia analysis, e.g., video surveillance, and human-computer interaction.
However, due to the randomness and ambiguity of the aging process, age
assessment is challenging. Most research work over the topic regards the task
as one of age regression, classification, and ranking problems, and cannot well
leverage age distribution in representing labels with age ambiguity. In this
work, we propose a simple yet effective loss function for robust facial age
estimation via distribution learning, i.e., adaptive mean-residue loss, in
which, the mean loss penalizes the difference between the estimated age
distribution's mean and the ground-truth age, whereas the residue loss
penalizes the entropy of age probability out of dynamic top-K in the
distribution. Experimental results in the datasets FG-NET and CLAP2016 have
validated the effectiveness of the proposed loss. Our code is available at
https://github.com/jacobzhaoziyuan/AMR-Loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Equivariant Graph Implicit Functions. (arXiv:2203.17178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17178">
<div class="article-summary-box-inner">
<span><p>In recent years, neural implicit representations have made remarkable
progress in modeling of 3D shapes with arbitrary topology. In this work, we
address two key limitations of such representations, in failing to capture
local 3D geometric fine details, and to learn from and generalize to shapes
with unseen 3D transformations. To this end, we introduce a novel family of
graph implicit functions with equivariant layers that facilitates modeling fine
local details and guaranteed robustness to various groups of geometric
transformations, through local $k$-NN graph embeddings with sparse point set
observations at multiple resolutions. Our method improves over the existing
rotation-equivariant implicit function from 0.69 to 0.89 (IoU) on the ShapeNet
reconstruction task. We also show that our equivariant implicit function can be
extended to other types of similarity transformations and generalizes to unseen
translations and scaling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion. (arXiv:2203.17191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17191">
<div class="article-summary-box-inner">
<span><p>Recently, video frame interpolation using a combination of frame- and
event-based cameras has surpassed traditional image-based methods both in terms
of performance and memory efficiency. However, current methods still suffer
from (i) brittle image-level fusion of complementary interpolation results,
that fails in the presence of artifacts in the fused image, (ii) potentially
temporally inconsistent and inefficient motion estimation procedures, that run
for every inserted frame and (iii) low contrast regions that do not trigger
events, and thus cause events-only motion estimation to generate artifacts.
Moreover, previous methods were only tested on datasets consisting of planar
and faraway scenes, which do not capture the full complexity of the real world.
In this work, we address the above problems by introducing multi-scale
feature-level fusion and computing one-shot non-linear inter-frame motion from
events and images, which can be efficiently sampled for image warping. We also
collect the first large-scale events and frames dataset consisting of more than
100 challenging scenes with depth variations, captured with a new experimental
setup based on a beamsplitter. We show that our method improves the
reconstruction quality by up to 0.2 dB in terms of PSNR and up to 15% in LPIPS
score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leverage Your Local and Global Representations: A New Self-Supervised Learning Strategy. (arXiv:2203.17205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17205">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) methods aim to learn view-invariant
representations by maximizing the similarity between the features extracted
from different crops of the same image regardless of cropping size and content.
In essence, this strategy ignores the fact that two crops may truly contain
different image information, e.g., background and small objects, and thus tends
to restrain the diversity of the learned representations. %To this end, the
existing strategies typically employ loss functions that enforces the networks
to discard part of valuable information, e.g. background and small objects, and
sacrifices the diversity of representation. In this work, we address this issue
by introducing a new self-supervised learning strategy, LoGo, that explicitly
reasons about {\bf Lo}cal and {\bf G}l{\bf o}bal crops. To achieve view
invariance, LoGo encourages similarity between global crops from the same
image, as well as between a global and a local crop. However, to correctly
encode the fact that the content of smaller crops may differ entirely, LoGo
promotes two local crops to have dissimilar representations, while being close
to global crops. Our LoGo strategy can easily be applied to existing SSL
methods. Our extensive experiments on a variety of datasets and using different
self-supervised learning frameworks validate its superiority over existing
approaches. Noticeably, we achieve better results than supervised models on
transfer learning when using only $1/10$ of the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimVQA: Exploring Simulated Environments for Visual Question Answering. (arXiv:2203.17219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17219">
<div class="article-summary-box-inner">
<span><p>Existing work on VQA explores data augmentation to achieve better
generalization by perturbing the images in the dataset or modifying the
existing questions and answers. While these methods exhibit good performance,
the diversity of the questions and answers are constrained by the available
image set. In this work we explore using synthetic computer-generated data to
fully control the visual and language space, allowing us to provide more
diverse scenarios. We quantify the effect of synthetic data in real-world VQA
benchmarks and to which extent it produces results that generalize to real
data. By exploiting 3D and physics simulation platforms, we provide a pipeline
to generate synthetic data to expand and replace type-specific questions and
answers without risking the exposure of sensitive or personal data that might
be present in real images. We offer a comprehensive analysis while expanding
existing hyper-realistic datasets to be used for VQA. We also propose Feature
Swapping (F-SWAP) -- where we randomly switch object-level features during
training to make a VQA model more domain invariant. We show that F-SWAP is
effective for enhancing a currently existing VQA dataset of real images without
compromising on the accuracy to answer existing questions in the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions. (arXiv:2203.17234v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17234">
<div class="article-summary-box-inner">
<span><p>We present a method that can recognize new objects and estimate their 3D pose
in RGB images even under partial occlusions. Our method requires neither a
training phase on these objects nor real images depicting them, only their CAD
models. It relies on a small set of training objects to learn local object
representations, which allow us to locally match the input image to a set of
"templates", rendered images of the CAD models for the new objects. In contrast
with the state-of-the-art methods, the new objects on which our method is
applied can be very different from the training objects. As a result, we are
the first to show generalization without retraining on the LINEMOD and
Occlusion-LINEMOD datasets. Our analysis of the failure modes of previous
template-based approaches further confirms the benefits of local features for
template matching. We outperform the state-of-the-art template matching methods
on the LINEMOD, Occlusion-LINEMOD and T-LESS datasets. Our source code and data
are publicly available at https://github.com/nv-nguyen/template-pose
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImpDet: Exploring Implicit Fields for 3D Object Detection. (arXiv:2203.17240v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17240">
<div class="article-summary-box-inner">
<span><p>Conventional 3D object detection approaches concentrate on bounding boxes
representation learning with several parameters, i.e., localization, dimension,
and orientation. Despite its popularity and universality, such a
straightforward paradigm is sensitive to slight numerical deviations,
especially in localization. By exploiting the property that point clouds are
naturally captured on the surface of objects along with accurate location and
intensity information, we introduce a new perspective that views bounding box
regression as an implicit function. This leads to our proposed framework,
termed Implicit Detection or ImpDet, which leverages implicit field learning
for 3D object detection. Our ImpDet assigns specific values to points in
different local 3D spaces, thereby high-quality boundaries can be generated by
classifying points inside or outside the boundary. To solve the problem of
sparsity on the object surface, we further present a simple yet efficient
virtual sampling strategy to not only fill the empty region, but also learn
rich semantic features to help refine the boundaries. Extensive experimental
results on KITTI and Waymo benchmarks demonstrate the effectiveness and
robustness of unifying implicit fields into object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. (arXiv:2203.17247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17247">
<div class="article-summary-box-inner">
<span><p>Breakthroughs in transformer-based models have revolutionized not only the
NLP field, but also vision and multimodal systems. However, although
visualization and interpretability tools have become available for NLP models,
internal mechanisms of vision and multimodal transformers remain largely
opaque. With the success of these transformers, it is increasingly critical to
understand their inner workings, as unraveling these black-boxes will lead to
more capable and trustworthy models. To contribute to this quest, we propose
VL-InterpreT, which provides novel interactive visualizations for interpreting
the attentions and hidden representations in multimodal transformers.
VL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety
of statistics in attention heads throughout all layers for both vision and
language components, (2) visualizes cross-modal and intra-modal attentions
through easily readable heatmaps, and (3) plots the hidden representations of
vision and language tokens as they pass through the transformer layers. In this
paper, we demonstrate the functionalities of VL-InterpreT through the analysis
of KD-VLP, an end-to-end pretraining vision-language multimodal
transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and
WebQA, two visual question answering benchmarks. Furthermore, we also present a
few interesting findings about multimodal transformer behaviors that were
learned through our tool.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Scene Representations for Embodied AI. (arXiv:2203.17251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17251">
<div class="article-summary-box-inner">
<span><p>We propose Continuous Scene Representations (CSR), a scene representation
constructed by an embodied agent navigating within a space, where objects and
their relationships are modeled by continuous valued embeddings. Our method
captures feature relationships between objects, composes them into a graph
structure on-the-fly, and situates an embodied agent within the representation.
Our key insight is to embed pair-wise relationships between objects in a latent
space. This allows for a richer representation compared to discrete relations
(e.g., [support], [next-to]) commonly used for building scene representations.
CSR can track objects as the agent moves in a scene, update the representation
accordingly, and detect changes in room configurations. Using CSR, we
outperform state-of-the-art approaches for the challenging downstream task of
visual room rearrangement, without any task specific training. Moreover, we
show the learned embeddings capture salient spatial details of the scene and
show applicability to real world data. A summery video and code is available at
https://prior.allenai.org/projects/csr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17255">
<div class="article-summary-box-inner">
<span><p>This theoretical article examines how to construct human-like working memory
and thought processes within a computer. There should be two working memory
stores, one analogous to sustained firing in association cortex, and one
analogous to synaptic potentiation in the cerebral cortex. These stores must be
constantly updated with new representations that arise from either
environmental stimulation or internal processing. They should be updated
continuously, and in an iterative fashion, meaning that, in the next state,
some items in the set of coactive items should always be retained. Thus, the
set of concepts coactive in working memory will evolve gradually and
incrementally over time. This makes each state is a revised iteration of the
preceding state and causes successive states to overlap and blend with respect
to the set of representations they contain. As new representations are added
and old ones are subtracted, some remain active for several seconds over the
course of these changes. This persistent activity, similar to that used in
artificial recurrent neural networks, is used to spread activation energy
throughout the global workspace to search for the next associative update. The
result is a chain of associatively linked intermediate states that are capable
of advancing toward a solution or goal. Iterative updating is conceptualized
here as an information processing strategy, a computational and
neurophysiological determinant of the stream of thought, and an algorithm for
designing and programming artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Video Salient Object Ranking. (arXiv:2203.17257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17257">
<div class="article-summary-box-inner">
<span><p>Salient Object Ranking (SOR) involves ranking the degree of saliency of
multiple salient objects in an input image. Most recently, a method is proposed
for ranking salient objects in an input video based on a predicted fixation
map. It relies solely on the density of the fixations within the salient
objects to infer their saliency ranks, which is incompatible with human
perception of saliency ranking. In this work, we propose to explicitly learn
the spatial and temporal relations between different salient objects to produce
the saliency ranks. To this end, we propose an end-to-end method for video
salient object ranking (VSOR), with two novel modules: an intra-frame adaptive
relation (IAR) module to learn the spatial relation among the salient objects
in the same frame locally and globally, and an inter-frame dynamic relation
(IDR) module to model the temporal relation of saliency across different
frames. In addition, to address the limited video types (just sports and
movies) and scene diversity in the existing VSOR dataset, we propose a new
dataset that covers different video types and diverse scenes on a large scale.
Experimental results demonstrate that our method outperforms state-of-the-art
methods in relevant fields. We will make the source code and our proposed
dataset available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating High Fidelity Data from Low-density Regions using Diffusion Models. (arXiv:2203.17260v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17260">
<div class="article-summary-box-inner">
<span><p>Our work focuses on addressing sample deficiency from low-density regions of
data manifold in common image datasets. We leverage diffusion process based
generative models to synthesize novel images from low-density regions. We
observe that uniform sampling from diffusion models predominantly samples from
high-density regions of the data manifold. Therefore, we modify the sampling
process to guide it towards low-density regions while simultaneously
maintaining the fidelity of synthetic data. We rigorously demonstrate that our
process successfully generates novel high fidelity samples from low-density
regions. We further examine generated samples and show that the model does not
memorize low-density data and indeed learns to generate novel samples from
low-density regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17261">
<div class="article-summary-box-inner">
<span><p>Recent research explosion on Neural Radiance Field (NeRF) shows the
encouraging potential to represent complex scenes with neural networks. One
major drawback of NeRF is its prohibitive inference time: Rendering a single
pixel requires querying the NeRF network hundreds of times. To resolve it,
existing efforts mainly attempt to reduce the number of required sampled
points. However, the problem of iterative sampling still exists. On the other
hand, Neural Light Field (NeLF) presents a more straightforward representation
over NeRF in novel view synthesis -- the rendering of a pixel amounts to one
single forward pass without ray-marching. In this work, we present a deep
residual MLP network (88 layers) to effectively learn the light field. We show
the key to successfully learning such a deep NeLF network is to have sufficient
data, for which we transfer the knowledge from a pre-trained NeRF model via
data distillation. Extensive experiments on both synthetic and real-world
scenes show the merits of our method over other counterpart algorithms. On the
synthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x
runtime speedup, meanwhile delivering significantly better (1.4-2.8 dB average
PSNR improvement) rendering quality than NeRF without any customized
implementation tricks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Speech Codecs: Rethinking Audio-Visual Speech Enhancement by Re-Synthesis. (arXiv:2203.17263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17263">
<div class="article-summary-box-inner">
<span><p>Since facial actions such as lip movements contain significant information
about speech content, it is not surprising that audio-visual speech enhancement
methods are more accurate than their audio-only counterparts. Yet,
state-of-the-art approaches still struggle to generate clean, realistic speech
without noise artifacts and unnatural distortions in challenging acoustic
environments. In this paper, we propose a novel audio-visual speech enhancement
framework for high-fidelity telecommunications in AR/VR. Our approach leverages
audio-visual speech cues to generate the codes of a neural speech codec,
enabling efficient synthesis of clean, realistic speech from noisy signals.
Given the importance of speaker-specific cues in speech, we focus on developing
personalized models that work well for individual speakers. We demonstrate the
efficacy of our approach on a new audio-visual speech dataset collected in an
unconstrained, large vocabulary setting, as well as existing audio-visual
datasets, outperforming speech enhancement baselines on both quantitative
metrics and human evaluation studies. Please see the supplemental video for
qualitative results at
https://github.com/facebookresearch/facestar/releases/download/paper_materials/video.mp4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing. (arXiv:2203.17266v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17266">
<div class="article-summary-box-inner">
<span><p>Recent advances like StyleGAN have promoted the growth of controllable facial
editing. To address its core challenge of attribute decoupling in a single
latent space, attempts have been made to adopt dual-space GAN for better
disentanglement of style and content representations. Nonetheless, these
methods are still incompetent to obtain plausible editing results with high
controllability, especially for complicated attributes. In this study, we
highlight the importance of interaction in a dual-space GAN for more
controllable editing. We propose TransEditor, a novel Transformer-based
framework to enhance such interaction. Besides, we develop a new dual-space
editing and inversion strategy to provide additional editing flexibility.
Extensive experiments demonstrate the superiority of the proposed framework in
image quality and editing capability, suggesting the effectiveness of
TransEditor for highly controllable facial editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17269">
<div class="article-summary-box-inner">
<span><p>Continual learning describes a setting where machine learning models learn
novel concepts from continuously shifting training data, while simultaneously
avoiding degradation of knowledge on previously seen classes (a phenomenon
known as the catastrophic forgetting problem) which may disappear from the
training data for extended periods of time. Current approaches for continual
learning of a single expanding task (aka class-incremental continual learning)
require extensive rehearsal of previously seen data to avoid this degradation
of knowledge. Unfortunately, rehearsal comes at a sharp cost to memory and
computation, and it may also violate data-privacy. Instead, we explore
combining knowledge distillation and parameter regularization in new ways to
achieve strong continual learning performance without rehearsal. Specifically,
we take a deep dive into common continual learning techniques: prediction
distillation, feature distillation, L2 parameter regularization, and EWC
parameter regularization. We first disprove the common assumption that
parameter regularization techniques fail for rehearsal-free continual learning
of a single, expanding task. Next, we explore how to leverage knowledge from a
pre-trained model in rehearsal-free continual learning and find that vanilla L2
parameter regularization outperforms EWC parameter regularization and feature
distillation. We then highlight the impact of the rehearsal-free continual
learning settings with a classifier expansion benchmark, showing that a
strategy based on our findings combined with a positive/negative label
balancing heuristic can close the performance gap between the upper bound and
the existing strategies by up to roughly 50%. Finally, we show that a simple
method consisting of pre-training, L2 regularization, and prediction
distillation can even outperform rehearsal-based methods on the common
CIFAR-100 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers. (arXiv:2203.17270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17270">
<div class="article-summary-box-inner">
<span><p>3D visual perception tasks, including 3D detection and map segmentation based
on multi-camera images, are essential for autonomous driving systems. In this
work, we present a new framework termed BEVFormer, which learns unified BEV
representations with spatiotemporal transformers to support multiple autonomous
driving perception tasks. In a nutshell, BEVFormer exploits both spatial and
temporal information by interacting with spatial and temporal space through
predefined grid-shaped BEV queries. To aggregate spatial information, we design
a spatial cross-attention that each BEV query extracts the spatial features
from the regions of interest across camera views. For temporal information, we
propose a temporal self-attention to recurrently fuse the history BEV
information. Our approach achieves the new state-of-the-art 56.9\% in terms of
NDS metric on the nuScenes test set, which is 9.0 points higher than previous
best arts and on par with the performance of LiDAR-based baselines. We further
show that BEVFormer remarkably improves the accuracy of velocity estimation and
recall of objects under low visibility conditions. The code will be released at
https://github.com/zhiqi-li/BEVFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Vision-Language Pretrained Models Learn Primitive Concepts?. (arXiv:2203.17271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17271">
<div class="article-summary-box-inner">
<span><p>Vision-language pretrained models have achieved impressive performance on
multimodal reasoning and zero-shot recognition tasks. Many of these VL models
are pretrained on unlabeled image and caption pairs from the internet. In this
paper, we study whether the notion of primitive concepts, such as color and
shape attributes, emerges automatically from these pretrained VL models. We
propose to learn compositional derivations that map primitive concept
activations into composite concepts, a task which we demonstrate to be
straightforward given true primitive concept annotations. This compositional
derivation learning (CompDL) framework allows us to quantitively measure the
usefulness and interpretability of the learned derivations, by jointly
considering the entire set of candidate primitive concepts. Our study reveals
that state-of-the-art VL pretrained models learn primitive concepts that are
highly useful as visual descriptors, as demonstrated by their strong
performance on fine-grained visual recognition tasks, but those concepts
struggle to provide interpretable compositional derivations, which highlights
limitations of existing VL models. Code and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MyStyle: A Personalized Generative Prior. (arXiv:2203.17272v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17272">
<div class="article-summary-box-inner">
<span><p>We introduce MyStyle, a personalized deep generative prior trained with a few
shots of an individual. MyStyle allows to reconstruct, enhance and edit images
of a specific person, such that the output is faithful to the person's key
facial characteristics. Given a small reference set of portrait images of a
person (~100), we tune the weights of a pretrained StyleGAN face generator to
form a local, low-dimensional, personalized manifold in the latent space. We
show that this manifold constitutes a personalized region that spans latent
codes associated with diverse portrait images of the individual. Moreover, we
demonstrate that we obtain a personalized generative prior, and propose a
unified approach to apply it to various ill-posed image enhancement problems,
such as inpainting and super-resolution, as well as semantic editing. Using the
personalized generative prior we obtain outputs that exhibit high-fidelity to
the input images and are also faithful to the key facial characteristics of the
individual in the reference set. We demonstrate our method with fair-use images
of numerous widely recognizable individuals for whom we have the prior
knowledge for a qualitative evaluation of the expected outcome. We evaluate our
approach against few-shots baselines and show that our personalized prior,
quantitatively and qualitatively, outperforms state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FindIt: Generalized Localization with Natural Language Queries. (arXiv:2203.17273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17273">
<div class="article-summary-box-inner">
<span><p>We propose FindIt, a simple and versatile framework that unifies a variety of
visual grounding and localization tasks including referring expression
comprehension, text-based localization, and object detection. Key to our
architecture is an efficient multi-scale fusion module that unifies the
disparate localization requirements across the tasks. In addition, we discover
that a standard object detector is surprisingly effective in unifying these
tasks without a need for task-specific design, losses, or pre-computed
detections. Our end-to-end trainable framework responds flexibly and accurately
to a wide range of referring expression, localization or detection queries for
zero, one, or multiple objects. Jointly trained on these tasks, FindIt
outperforms the state of the art on both referring expression and text-based
localization, and shows competitive performance on object detection. Finally,
FindIt generalizes better to out-of-distribution data and novel categories
compared to strong single-task baselines. All of these are accomplished by a
single, unified and efficient model. The code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models. (arXiv:2203.17274v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17274">
<div class="article-summary-box-inner">
<span><p>Prompting has recently become a popular paradigm for adapting language models
to downstream tasks. Rather than fine-tuning model parameters or adding
task-specific heads, this approach steers a model to perform a new task simply
by adding a text prompt to the model's inputs. In this paper, we explore the
question: can we create prompts with pixels instead? In other words, can
pre-trained vision models be adapted to a new task solely by adding pixels to
their inputs? We introduce visual prompting, which learns a task-specific image
perturbation such that a frozen pre-trained model prompted with this
perturbation performs a new task. We discover that changing only a few pixels
is enough to adapt models to new tasks and datasets, and performs on par with
linear probing, the current de facto approach to lightweight adaptation. The
surprising effectiveness of visual prompting provides a new perspective on how
to adapt pre-trained models in vision, and opens up the possibility of adapting
models solely through their inputs, which, unlike model parameters or outputs,
are typically under an end-user's control. Code is available at
<a href="http://hjbahng.github.io/visual_prompting">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools. (arXiv:2203.17275v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17275">
<div class="article-summary-box-inner">
<span><p>We consider the problem of sequential robotic manipulation of deformable
objects using tools. Previous works have shown that differentiable physics
simulators provide gradients to the environment state and help trajectory
optimization to converge orders of magnitude faster than model-free
reinforcement learning algorithms for deformable object manipulation. However,
such gradient-based trajectory optimization typically requires access to the
full simulator states and can only solve short-horizon, single-skill tasks due
to local optima. In this work, we propose a novel framework, named DiffSkill,
that uses a differentiable physics simulator for skill abstraction to solve
long-horizon deformable object manipulation tasks from sensory observations. In
particular, we first obtain short-horizon skills using individual tools from a
gradient-based optimizer, using the full state information in a differentiable
simulator; we then learn a neural skill abstractor from the demonstration
trajectories which takes RGBD images as input. Finally, we plan over the skills
by finding the intermediate goals and then solve long-horizon tasks. We show
the advantages of our method in a new set of sequential deformable object
manipulation tasks compared to previous reinforcement learning algorithms and
compared to the trajectory optimizer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bringing Old Films Back to Life. (arXiv:2203.17276v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17276">
<div class="article-summary-box-inner">
<span><p>We present a learning-based framework, recurrent transformer network (RTN),
to restore heavily degraded old films. Instead of performing frame-wise
restoration, our method is based on the hidden knowledge learned from adjacent
frames that contain abundant information about the occlusion, which is
beneficial to restore challenging artifacts of each frame while ensuring
temporal coherency. Moreover, contrasting the representation of the current
frame and the hidden knowledge makes it possible to infer the scratch position
in an unsupervised manner, and such defect localization generalizes well to
real-world degradations. To better resolve mixed degradation and compensate for
the flow estimation error during frame alignment, we propose to leverage more
expressive transformer blocks for spatial restoration. Experiments on both
synthetic dataset and real-world old films demonstrate the significant
superiority of the proposed RTN over existing solutions. In addition, the same
framework can effectively propagate the color from keyframes to the whole
video, ultimately yielding compelling restored films. The implementation and
model will be released at
https://github.com/raywzy/Bringing-Old-Films-Back-to-Life.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Part Mining for Fine-grained Image Classification. (arXiv:1902.09941v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.09941">
<div class="article-summary-box-inner">
<span><p>Fine-grained image classification remains challenging due to the large
intra-class variance and small inter-class variance. Since the subtle visual
differences are only in local regions of discriminative parts among
subcategories, part localization is a key issue for fine-grained image
classification. Most existing approaches localize object or parts in an image
with object or part annotations, which are expensive and labor-consuming. To
tackle this issue, we propose a fully unsupervised part mining (UPM) approach
to localize the discriminative parts without even image-level annotations,
which largely improves the fine-grained classification performance. We first
utilize pattern mining techniques to discover frequent patterns, i.e.,
co-occurrence highlighted regions, in the feature maps extracted from a
pre-trained convolutional neural network (CNN) model. Inspired by the fact that
these relevant meaningful patterns typically hold appearance and spatial
consistency, we then cluster the mined regions to obtain the cluster centers
and the discriminative parts surrounding the cluster centers are generated.
Importantly, any annotations and sophisticated training procedures are not used
in our proposed part localization approach. Finally, a multi-stream
classification network is built for aggregating the original, object-level and
part-level features simultaneously. Compared with other state-of-the-art
approaches, our UPM approach achieves the competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extraction and Analysis of Fictional Character Networks: A Survey. (arXiv:1907.02704v5 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.02704">
<div class="article-summary-box-inner">
<span><p>A character network is a graph extracted from a narrative, in which vertices
represent characters and edges correspond to interactions between them. A
number of narrative-related problems can be addressed automatically through the
analysis of character networks, such as summarization, classification, or role
detection. Character networks are particularly relevant when considering works
of fictions (e.g. novels, plays, movies, TV series), as their exploitation
allows developing information retrieval and recommendation systems. However,
works of fiction possess specific properties making these tasks harder. This
survey aims at presenting and organizing the scientific literature related to
the extraction of character networks from works of fiction, as well as their
analysis. We first describe the extraction process in a generic way, and
explain how its constituting steps are implemented in practice, depending on
the medium of the narrative, the goal of the network analysis, and other
factors. We then review the descriptive tools used to characterize character
networks, with a focus on the way they are interpreted in this context. We
illustrate the relevance of character networks by also providing a review of
applications derived from their analysis. Finally, we identify the limitations
of the existing approaches, and the most promising perspectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDN: Salient Object Detection via Extremely-Downsampled Network. (arXiv:2012.13093v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13093">
<div class="article-summary-box-inner">
<span><p>Recent progress on salient object detection (SOD) mainly benefits from
multi-scale learning, where the high-level and low-level features collaborate
in locating salient objects and discovering fine details, respectively.
However, most efforts are devoted to low-level feature learning by fusing
multi-scale features or enhancing boundary representations. High-level
features, which although have long proven effective for many other tasks, yet
have been barely studied for SOD. In this paper, we tap into this gap and show
that enhancing high- level features is essential for SOD as well. To this end,
we introduce an Extremely-Downsampled Network (EDN), which employs an extreme
downsampling technique to effectively learn a global view of the whole image,
leading to accurate salient object localization. To accomplish better
multi-level feature fusion, we construct the Scale-Correlated Pyramid
Convolution (SCPC) to build an elegant decoder for recovering object details
from the above extreme downsampling. Extensive experiments demonstrate that EDN
achieves state-of-the-art performance with real-time speed. Our efficient
EDN-Lite also achieves competitive performance with a speed of 316fps. Hence,
this work is expected to spark some new thinking in SOD. Code is available at
https://github.com/yuhuan-wu/EDN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localization Distillation for Dense Object Detection. (arXiv:2102.12252v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12252">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has witnessed its powerful capability in learning
compact models in object detection. Previous KD methods for object detection
mostly focus on imitating deep features within the imitation regions instead of
mimicking classification logit due to its inefficiency in distilling
localization information and trivial improvement. In this paper, by
reformulating the knowledge distillation process on localization, we present a
novel localization distillation (LD) method which can efficiently transfer the
localization knowledge from the teacher to the student. Moreover, we also
heuristically introduce the concept of valuable localization region that can
aid to selectively distill the semantic and localization knowledge for a
certain region. Combining these two new components, for the first time, we show
that logit mimicking can outperform feature imitation and localization
knowledge distillation is more important and efficient than semantic knowledge
for distilling object detectors. Our distillation scheme is simple as well as
effective and can be easily applied to different dense object detectors.
Experiments show that our LD can boost the AP score of GFocal-ResNet-50 with a
single-scale 1x training schedule from 40.1 to 42.1 on the COCO benchmark
without any sacrifice on the inference speed. Our source code and trained
models are publicly available at https://github.com/HikariTJU/LD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cloth-Changing Person Re-identification from A Single Image with Gait Prediction and Regularization. (arXiv:2103.15537v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15537">
<div class="article-summary-box-inner">
<span><p>Cloth-Changing person re-identification (CC-ReID) aims at matching the same
person across different locations over a long-duration, e.g., over days, and
therefore inevitably meets challenge of changing clothing. In this paper, we
focus on handling well the CC-ReID problem under a more challenging setting,
i.e., just from a single image, which enables high-efficiency and latency-free
pedestrian identify for real-time surveillance applications. Specifically, we
introduce Gait recognition as an auxiliary task to drive the Image ReID model
to learn cloth-agnostic representations by leveraging personal unique and
cloth-independent gait information, we name this framework as GI-ReID. GI-ReID
adopts a two-stream architecture that consists of a image ReID-Stream and an
auxiliary gait recognition stream (Gait-Stream). The Gait-Stream, that is
discarded in the inference for high computational efficiency, acts as a
regulator to encourage the ReID-Stream to capture cloth-invariant biometric
motion features during the training. To get temporal continuous motion cues
from a single image, we design a Gait Sequence Prediction (GSP) module for
Gait-Stream to enrich gait information. Finally, a high-level semantics
consistency over two streams is enforced for effective knowledge
regularization. Experiments on multiple image-based Cloth-Changing ReID
benchmarks, e.g., LTCC, PRCC, Real28, and VC-Clothes, demonstrate that GI-ReID
performs favorably against the state-of-the-arts. Codes are available at
https://github.com/jinx-USTC/GI-ReID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DINE: Domain Adaptation from Single and Multiple Black-box Predictors. (arXiv:2104.01539v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01539">
<div class="article-summary-box-inner">
<span><p>To ease the burden of labeling, unsupervised domain adaptation (UDA) aims to
transfer knowledge in previous and related labeled datasets (sources) to a new
unlabeled dataset (target). Despite impressive progress, prior methods always
need to access the raw source data and develop data-dependent alignment
approaches to recognize the target samples in a transductive learning manner,
which may raise privacy concerns from source individuals. Several recent
studies resort to an alternative solution by exploiting the well-trained
white-box model from the source domain, yet, it may still leak the raw data
through generative adversarial learning. This paper studies a practical and
interesting setting for UDA, where only black-box source models (i.e., only
network predictions are available) are provided during adaptation in the target
domain. To solve this problem, we propose a new two-step knowledge adaptation
framework called DIstill and fine-tuNE (DINE). Taking into consideration the
target data structure, DINE first distills the knowledge from the source
predictor to a customized target model, then fine-tunes the distilled model to
further fit the target domain. Besides, neural networks are not required to be
identical across domains in DINE, even allowing effective adaptation on a
low-resource device. Empirical results on three UDA scenarios (i.e.,
single-source, multi-source, and partial-set) confirm that DINE achieves highly
competitive performance compared to state-of-the-art data-dependent approaches.
Code is available at \url{https://github.com/tim-learn/DINE/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart. (arXiv:2105.14785v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14785">
<div class="article-summary-box-inner">
<span><p>Correctly classifying adversarial examples is an essential but challenging
requirement for safely deploying machine learning models. As reported in
RobustBench, even the state-of-the-art adversarially trained models struggle to
exceed 67% robust test accuracy on CIFAR-10, which is far from practical. A
complementary way towards robustness is to introduce a rejection option,
allowing the model to not return predictions on uncertain inputs, where
confidence is a commonly used certainty proxy. Along with this routine, we find
that confidence and a rectified confidence (R-Con) can form two coupled
rejection metrics, which could provably distinguish wrongly classified inputs
from correctly classified ones. This intriguing property sheds light on using
coupling strategies to better detect and reject adversarial examples. We
evaluate our rectified rejection (RR) module on CIFAR-10, CIFAR-10-C, and
CIFAR-100 under several attacks including adaptive ones, and demonstrate that
the RR module is compatible with different adversarial training frameworks on
improving robustness, with little extra computation. The code is available at
https://github.com/P2333/Rectified-Rejection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02689">
<div class="article-summary-box-inner">
<span><p>Vision transformer (ViT) has recently shown its strong capability in
achieving comparable results to convolutional neural networks (CNNs) on image
classification. However, vanilla ViT simply inherits the same architecture from
the natural language processing directly, which is often not optimized for
vision applications. Motivated by this, in this paper, we propose a new
architecture that adopts the pyramid structure and employ a novel
regional-to-local attention rather than global self-attention in vision
transformers. More specifically, our model first generates regional tokens and
local tokens from an image with different patch sizes, where each regional
token is associated with a set of local tokens based on the spatial location.
The regional-to-local attention includes two steps: first, the regional
self-attention extract global information among all regional tokens and then
the local self-attention exchanges the information among one regional token and
the associated local tokens via self-attention. Therefore, even though local
self-attention confines the scope in a local region but it can still receive
global information. Extensive experiments on four vision tasks, including image
classification, object and keypoint detection, semantics segmentation and
action recognition, show that our approach outperforms or is on par with
state-of-the-art ViT variants including many concurrent works. Our source codes
and models are available at https://github.com/ibm/regionvit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Flows with Invertible Attentions. (arXiv:2106.03959v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03959">
<div class="article-summary-box-inner">
<span><p>Flow-based generative models have shown an excellent ability to explicitly
learn the probability density function of data via a sequence of invertible
transformations. Yet, learning attentions in generative flows remains
understudied, while it has made breakthroughs in other domains. To fill the
gap, this paper introduces two types of invertible attention mechanisms, i.e.,
map-based and transformer-based attentions, for both unconditional and
conditional generative flows. The key idea is to exploit a masked scheme of
these two attentions to learn long-range data dependencies in the context of
generative flows. The masked scheme allows for invertible attention modules
with tractable Jacobian determinants, enabling its seamless integration at any
positions of the flow-based models. The proposed attention mechanisms lead to
more efficient generative flows, due to their capability of modeling the
long-term data dependencies. Evaluation on multiple image synthesis tasks shows
that the proposed attention flows result in efficient models and compare
favorably against the state-of-the-art unconditional and conditional generative
flows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Short Temporal Contrastive Learning of Video Transformers. (arXiv:2106.09212v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09212">
<div class="article-summary-box-inner">
<span><p>Video transformers have recently emerged as a competitive alternative to 3D
CNNs for video understanding. However, due to their large number of parameters
and reduced inductive biases, these models require supervised pretraining on
large-scale image datasets to achieve top performance. In this paper, we
empirically demonstrate that self-supervised pretraining of video transformers
on video-only datasets can lead to action recognition results that are on par
or better than those obtained with supervised pretraining on large-scale image
datasets, even massive ones such as ImageNet-21K. Since transformer-based
models are effective at capturing dependencies over extended temporal spans, we
propose a simple learning procedure that forces the model to match a long-term
view to a short-term view of the same video. Our approach, named Long-Short
Temporal Contrastive Learning (LSTCL), enables video transformers to learn an
effective clip-level representation by predicting temporal context captured
from a longer temporal extent. To demonstrate the generality of our findings,
we implement and validate our approach under three different self-supervised
contrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct
video-transformer architectures, including an improved variant of the Swin
Transformer augmented with space-time attention. We conduct a thorough ablation
study and show that LSTCL achieves competitive performance on multiple video
benchmarks and represents a convincing alternative to supervised image-based
pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10811">
<div class="article-summary-box-inner">
<span><p>Shape implicit neural representations (INRs) have recently shown to be
effective in shape analysis and reconstruction tasks. Existing INRs require
point coordinates to learn the implicit level sets of the shape. When a normal
vector is available for each point, a higher fidelity representation can be
learned, however normal vectors are often not provided as raw data.
Furthermore, the method's initialization has been shown to play a crucial role
for surface reconstruction. In this paper, we propose a divergence guided shape
representation learning approach that does not require normal vectors as input.
We show that incorporating a soft constraint on the divergence of the distance
function favours smooth solutions that reliably orients gradients to match the
unknown normal at each point, in some cases even better than approaches that
use ground truth normal vectors directly. Additionally, we introduce a novel
geometric initialization method for sinusoidal INRs that further improves
convergence to the desired solution. We evaluate the effectiveness of our
approach on the task of surface reconstruction and shape space learning and
show SOTA performance compared to other unoriented methods. Code and model
parameters available at our project page https://chumbyte.github.io/DiGS-Site/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Opcode Sequence Based Malware Detection. (arXiv:2106.11821v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11821">
<div class="article-summary-box-inner">
<span><p>In this paper we study data augmentation for opcode sequence based Android
malware detection. Data augmentation has been successfully used in many areas
of deep-learning to significantly improve model performance. Typically, data
augmentation simulates realistic variations in data to increase the apparent
diversity of the training-set. However, for opcode-based malware analysis it is
not immediately clear how to apply data augmentation. Hence we first study the
use of fixed transformations, then progress to adaptive methods. We propose a
novel data augmentation method -- Self-Embedding Language Model Augmentation --
that uses a malware detection network's own opcode embedding layer to measure
opcode similarity for adaptive augmentation. To the best of our knowledge this
is the first paper to carry out a systematic study of different augmentation
methods for opcode sequence based Android malware classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demystifying the Transferability of Adversarial Attacks in Computer Networks. (arXiv:2110.04488v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04488">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) models are one of the most frequently
used deep learning networks, and extensively used in both academia and
industry. Recent studies demonstrated that adversarial attacks against such
models can maintain their effectiveness even when used on models other than the
one targeted by the attacker. This major property is known as transferability,
and makes CNNs ill-suited for security applications. In this paper, we provide
the first comprehensive study which assesses the robustness of CNN-based models
for computer networks against adversarial transferability. Furthermore, we
investigate whether the transferability property issue holds in computer
networks applications. In our experiments, we first consider five different
attacks: the Iterative Fast Gradient Method (I-FGSM), the Jacobian-based
Saliency Map (JSMA), the Limited-memory Broyden Fletcher Goldfarb Shanno BFGS
(L- BFGS), the Projected Gradient Descent (PGD), and the DeepFool attack. Then,
we perform these attacks against three well- known datasets: the Network-based
Detection of IoT (N-BaIoT) dataset, the Domain Generating Algorithms (DGA)
dataset, and the RIPE Atlas dataset. Our experimental results show clearly that
the transferability happens in specific use cases for the I- FGSM, the JSMA,
and the LBFGS attack. In such scenarios, the attack success rate on the target
network range from 63.00% to 100%. Finally, we suggest two shielding strategies
to hinder the attack transferability, by considering the Most Powerful Attacks
(MPAs), and the mismatch LSTM architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06537">
<div class="article-summary-box-inner">
<span><p>The conventional wisdom behind learning deep classification models is to
focus on bad-classified examples and ignore well-classified examples that are
far from the decision boundary. For instance, when training with cross-entropy
loss, examples with higher likelihoods (i.e., well-classified examples)
contribute smaller gradients in back-propagation. However, we theoretically
show that this common practice hinders representation learning, energy
optimization, and margin growth. To counteract this deficiency, we propose to
reward well-classified examples with additive bonuses to revive their
contribution to the learning process. This counterexample theoretically
addresses these three issues. We empirically support this claim by directly
verifying the theoretical results or significant performance improvement with
our counterexample on diverse tasks, including image classification, graph
classification, and machine translation. Furthermore, this paper shows that we
can deal with complex scenarios, such as imbalanced classification, OOD
detection, and applications under adversarial attacks because our idea can
solve these three issues. Code is available at:
https://github.com/lancopku/well-classified-examples-are-underestimated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Rounding for Image Interpolation and Scan Conversion. (arXiv:2110.12983v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12983">
<div class="article-summary-box-inner">
<span><p>The stochastic rounding (SR) function is proposed to evaluate and demonstrate
the effects of stochastically rounding row and column subscripts in image
interpolation and scan conversion. The proposed SR function is based on a
pseudorandom number, enabling the pseudorandom rounding up or down any
non-integer row and column subscripts. Also, the SR function exceptionally
enables rounding up any possible cases of subscript inputs that are inferior to
a pseudorandom number. The algorithm of interest is the nearest-neighbor
interpolation (NNI) which is traditionally based on the deterministic rounding
(DR) function. Experimental simulation results are provided to demonstrate the
performance of NNI-SR and NNI-DR algorithms before and after applying smoothing
and sharpening filters of interest. Additional results are also provided to
demonstrate the performance of NNI-SR and NNI-DR interpolated scan conversion
algorithms in cardiac ultrasound videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable image registration with deep network priors: a study on longitudinal PET images. (arXiv:2111.11873v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11873">
<div class="article-summary-box-inner">
<span><p>Longitudinal image registration is challenging and has not yet benefited from
major performance improvements thanks to deep-learning. Inspired by Deep Image
Prior, this paper introduces a different use of deep architectures as
regularizers to tackle the image registration question. We propose a
subject-specific deformable registration method called MIRRBA, relying on a
deep pyramidal architecture to be the prior parametric model constraining the
deformation field. Diverging from the supervised learning paradigm, MIRRBA does
not require a learning database, but only the pair of images to be registered
to optimize the network's parameters and provide a deformation field. We
demonstrate the regularizing power of deep architectures and present new
elements to understand the role of the architecture in deep learning methods
for registration. Hence, to study the impact of the network parameters, we ran
our method with different architectural configurations on a private dataset of
110 metastatic breast cancer full-body PET images with manual segmentations of
the brain, bladder and metastatic lesions. We compared it against conventional
iterative registration approaches and supervised deep learning-based models.
Global and local registration accuracies were evaluated using the detection
rate and the Dice score respectively, while registration realism was evaluated
using the Jacobian's determinant. Moreover, we computed the ability of the
different methods to shrink vanishing lesions with the disappearing rate.
MIRRBA significantly improves the organ and lesion Dice scores of supervised
models. Regarding the disappearing rate, MIRRBA more than doubles the best
performing conventional approach SyNCC score. Our work therefore proposes an
alternative way to bridge the performance gap between conventional and deep
learning-based methods and demonstrates the regularizing power of deep
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferability Metrics for Selecting Source Model Ensembles. (arXiv:2111.13011v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13011">
<div class="article-summary-box-inner">
<span><p>We address the problem of ensemble selection in transfer learning: Given a
large pool of source models we want to select an ensemble of models which,
after fine-tuning on the target training set, yields the best performance on
the target test set. Since fine-tuning all possible ensembles is
computationally prohibitive, we aim at predicting performance on the target
dataset using a computationally efficient transferability metric. We propose
several new transferability metrics designed for this task and evaluate them in
a challenging and realistic transfer learning setup for semantic segmentation:
we create a large and diverse pool of source models by considering 17 source
datasets covering a wide variety of image domain, two different architectures,
and two pre-training schemes. Given this pool, we then automatically select a
subset to form an ensemble performing well on a given target dataset. We
compare the ensemble selected by our method to two baselines which select a
single source model, either (1) from the same pool as our method; or (2) from a
pool containing large source models, each with similar capacity as an ensemble.
Averaged over 17 target datasets, we outperform these baselines by 6.0% and
2.5% relative mean IoU, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Implicit Values of A Good Hand Shake: Handheld Multi-Frame Neural Depth Refinement. (arXiv:2111.13738v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13738">
<div class="article-summary-box-inner">
<span><p>Modern smartphones can continuously stream multi-megapixel RGB images at
60Hz, synchronized with high-quality 3D pose information and low-resolution
LiDAR-driven depth estimates. During a snapshot photograph, the natural
unsteadiness of the photographer's hands offers millimeter-scale variation in
camera pose, which we can capture along with RGB and depth in a circular
buffer. In this work we explore how, from a bundle of these measurements
acquired during viewfinding, we can combine dense micro-baseline parallax cues
with kilopixel LiDAR depth to distill a high-fidelity depth map. We take a
test-time optimization approach and train a coordinate MLP to output
photometrically and geometrically consistent depth estimates at the continuous
coordinates along the path traced by the photographer's natural hand shake.
With no additional hardware, artificial hand motion, or user interaction beyond
the press of a button, our proposed method brings high-resolution depth
estimates to point-and-shoot "tabletop" photography -- textured objects at
close range.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning. (arXiv:2111.14213v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14213">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) is a promising strategy for performing
privacy-preserving, distributed learning with a network of clients (i.e., edge
devices). However, the data distribution among clients is often non-IID in
nature, making efficient optimization difficult. To alleviate this issue, many
FL algorithms focus on mitigating the effects of data heterogeneity across
clients by introducing a variety of proximal terms, some incurring considerable
compute and/or memory overheads, to restrain local updates with respect to the
global model. Instead, we consider rethinking solutions to data heterogeneity
in FL with a focus on local learning generality rather than proximal
restriction. To this end, we first present a systematic study informed by
second-order indicators to better understand algorithm effectiveness in FL.
Interestingly, we find that standard regularization methods are surprisingly
strong performers in mitigating data heterogeneity effects. Based on our
findings, we further propose a simple and effective method, FedAlign, to
overcome data heterogeneity and the pitfalls of previous methods. FedAlign
achieves competitive accuracy with state-of-the-art FL methods across a variety
of settings while minimizing computation and memory overhead. Code is available
at https://github.com/mmendiet/FedAlign
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic. (arXiv:2111.14447v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14447">
<div class="article-summary-box-inner">
<span><p>Recent text-to-image matching models apply contrastive learning to large
corpora of uncurated pairs of images and sentences. While such models can
provide a powerful score for matching and subsequent zero-shot tasks, they are
not capable of generating caption given an image. In this work, we repurpose
such models to generate a descriptive text given an image at inference time,
without any further training or tuning steps. This is done by combining the
visual-semantic model with a large language model, benefiting from the
knowledge in both web-scale models. The resulting captions are much less
restrictive than those obtained by supervised captioning methods. Moreover, as
a zero-shot learning method, it is extremely flexible and we demonstrate its
ability to perform image arithmetic in which the inputs can be either images or
text, and the output is a sentence. This enables novel high-level vision
capabilities such as comparing two images or solving visual analogy tests. Our
code is available at: https://github.com/YoadTew/zero-shot-image-to-text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDR-NeRF: High Dynamic Range Neural Radiance Fields. (arXiv:2111.14451v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14451">
<div class="article-summary-box-inner">
<span><p>We present High Dynamic Range Neural Radiance Fields (HDR-NeRF) to recover an
HDR radiance field from a set of low dynamic range (LDR) views with different
exposures. Using the HDR-NeRF, we are able to generate both novel HDR views and
novel LDR views under different exposures. The key to our method is to model
the physical imaging process, which dictates that the radiance of a scene point
transforms to a pixel value in the LDR image with two implicit functions: a
radiance field and a tone mapper. The radiance field encodes the scene radiance
(values vary from 0 to +infty), which outputs the density and radiance of a ray
by giving corresponding ray origin and ray direction. The tone mapper models
the mapping process that a ray hitting on the camera sensor becomes a pixel
value. The color of the ray is predicted by feeding the radiance and the
corresponding exposure time into the tone mapper. We use the classic volume
rendering technique to project the output radiance, colors, and densities into
HDR and LDR images, while only the input LDR images are used as the
supervision. We collect a new forward-facing HDR dataset to evaluate the
proposed method. Experimental results on synthetic and real-world scenes
validate that our method can not only accurately control the exposures of
synthesized views but also render views with a high dynamic range.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image. (arXiv:2112.02753v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02753">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a framework for single-view hand mesh
reconstruction, which can simultaneously achieve high reconstruction accuracy,
fast inference speed, and temporal coherence. Specifically, for 2D encoding, we
propose lightweight yet effective stacked structures. Regarding 3D decoding, we
provide an efficient graph operator, namely depth-separable spiral convolution.
Moreover, we present a novel feature lifting module for bridging the gap
between 2D and 3D representations. This module begins with a map-based position
regression (MapReg) block to integrate the merits of both heatmap encoding and
position regression paradigms for improved 2D accuracy and temporal coherence.
Furthermore, MapReg is followed by pose pooling and pose-to-vertex lifting
approaches, which transform 2D pose encodings to semantic features of 3D
vertices. Overall, our hand reconstruction framework, called MobRecon,
comprises affordable computational costs and miniature model size, which
reaches a high inference speed of 83FPS on Apple A14 CPU. Extensive experiments
on popular datasets such as FreiHAND, RHD, and HO3Dv2 demonstrate that our
MobRecon achieves superior performance on reconstruction accuracy and temporal
coherence. Our code is publicly available at
https://github.com/SeanChenxy/HandMesh.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make It Move: Controllable Image-to-Video Generation with Text Descriptions. (arXiv:2112.02815v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02815">
<div class="article-summary-box-inner">
<span><p>Generating controllable videos conforming to user intentions is an appealing
yet challenging topic in computer vision. To enable maneuverable control in
line with user intentions, a novel video generation task, named
Text-Image-to-Video generation (TI2V), is proposed. With both controllable
appearance and motion, TI2V aims at generating videos from a static image and a
text description. The key challenges of TI2V task lie both in aligning
appearance and motion from different modalities, and in handling uncertainty in
text descriptions. To address these challenges, we propose a Motion
Anchor-based video GEnerator (MAGE) with an innovative motion anchor (MA)
structure to store appearance-motion aligned representation. To model the
uncertainty and increase the diversity, it further allows the injection of
explicit condition and implicit randomness. Through three-dimensional axial
transformers, MA is interacted with given image to generate next frames
recursively with satisfying controllability and diversity. Accompanying the new
task, we build two new video-text paired datasets based on MNIST and CATER for
evaluation. Experiments conducted on these datasets verify the effectiveness of
MAGE and show appealing potentials of TI2V task. Source code for model and
datasets will be available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Points: Point Cloud Representation with Neural Fields for Arbitrary Upsampling. (arXiv:2112.04148v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04148">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Neural Points, a novel point cloud representation
and apply it to the arbitrary-factored upsampling task. Different from
traditional point cloud representation where each point only represents a
position or a local plane in the 3D space, each point in Neural Points
represents a local continuous geometric shape via neural fields. Therefore,
Neural Points contain more shape information and thus have a stronger
representation ability. Neural Points is trained with surface containing rich
geometric details, such that the trained model has enough expression ability
for various shapes. Specifically, we extract deep local features on the points
and construct neural fields through the local isomorphism between the 2D
parametric domain and the 3D local patch. In the final, local neural fields are
integrated together to form the global surface. Experimental results show that
Neural Points has powerful representation ability and demonstrate excellent
robustness and generalization ability. With Neural Points, we can resample
point cloud with arbitrary resolutions, and it outperforms the state-of-the-art
point cloud upsampling methods. Code is available at
https://github.com/WanquanF/NeuralPoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Semantic Concepts into End-to-End Image Captioning. (arXiv:2112.05230v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05230">
<div class="article-summary-box-inner">
<span><p>Tremendous progress has been made in recent years in developing better image
captioning models, yet most of them rely on a separate object detector to
extract regional features. Recent vision-language studies are shifting towards
the detector-free trend by leveraging grid representations for more flexible
model training and faster inference speed. However, such development is
primarily focused on image understanding tasks, and remains less investigated
for the caption generation task. In this paper, we are concerned with a
better-performing detector-free image captioning model, and propose a pure
vision transformer-based image captioning model, dubbed as ViTCAP, in which
grid representations are used without extracting the regional features. For
improved performance, we introduce a novel Concept Token Network (CTN) to
predict the semantic concepts and then incorporate them into the end-to-end
captioning. In particular, the CTN is built on the basis of a vision
transformer and is designed to predict the concept tokens through a
classification task, from which the rich semantic information contained greatly
benefits the captioning task. Compared with the previous detector-based models,
ViTCAP drastically simplifies the architectures and at the same time achieves
competitive performance on various challenging image captioning datasets. In
particular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split,
93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Erasing and Diffusion Network for Occluded Person Re-Identification. (arXiv:2112.08740v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08740">
<div class="article-summary-box-inner">
<span><p>Occluded person re-identification (ReID) aims at matching occluded person
images to holistic ones across different camera views. Target Pedestrians (TP)
are usually disturbed by Non-Pedestrian Occlusions (NPO) and NonTarget
Pedestrians (NTP). Previous methods mainly focus on increasing model's
robustness against NPO while ignoring feature contamination from NTP. In this
paper, we propose a novel Feature Erasing and Diffusion Network (FED) to
simultaneously handle NPO and NTP. Specifically, NPO features are eliminated by
our proposed Occlusion Erasing Module (OEM), aided by the NPO augmentation
strategy which simulates NPO on holistic pedestrian images and generates
precise occlusion masks. Subsequently, we Subsequently, we diffuse the
pedestrian representations with other memorized features to synthesize NTP
characteristics in the feature space which is achieved by a novel Feature
Diffusion Module (FDM) through a learnable cross attention mechanism. With the
guidance of the occlusion scores from OEM, the feature diffusion process is
mainly conducted on visible body parts, which guarantees the quality of the
synthesized NTP characteristics. By jointly optimizing OEM and FDM in our
proposed FED network, we can greatly improve the model's perception ability
towards TP and alleviate the influence of NPO and NTP. Furthermore, the
proposed FDM only works as an auxiliary module for training and will be
discarded in the inference phase, thus introducing little inference
computational overhead. Experiments on occluded and holistic person ReID
benchmarks demonstrate the superiority of FED over state-of-the-arts, where FED
achieves 86.3% Rank-1 accuracy on Occluded-REID, surpassing others by at least
4.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScanQA: 3D Question Answering for Spatial Scene Understanding. (arXiv:2112.10482v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10482">
<div class="article-summary-box-inner">
<span><p>We propose a new 3D spatial understanding task of 3D Question Answering
(3D-QA). In the 3D-QA task, models receive visual information from the entire
3D scene of the rich RGB-D indoor scan and answer the given textual questions
about the 3D scene. Unlike the 2D-question answering of VQA, the conventional
2D-QA models suffer from problems with spatial understanding of object
alignment and directions and fail the object identification from the textual
questions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model,
where the model learns a fused descriptor from 3D object proposals and encoded
sentence embeddings. This learned descriptor correlates the language
expressions with the underlying geometric features of the 3D scan and
facilitates the regression of 3D bounding boxes to determine described objects
in textual questions and outputs correct answers. We collected human-edited
question-answer pairs with free-form answers that are grounded to 3D objects in
each 3D scene. Our new ScanQA dataset contains over 40K question-answer pairs
from the 800 indoor scenes drawn from the ScanNet dataset. To the best of our
knowledge, the proposed 3D-QA task is the first large-scale effort to perform
object-grounded question-answering in 3D environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View. (arXiv:2112.11790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11790">
<div class="article-summary-box-inner">
<span><p>Autonomous driving perceives its surroundings for decision making, which is
one of the most complex scenarios in visual perception. The success of paradigm
innovation in solving the 2D object detection task inspires us to seek an
elegant, feasible, and scalable paradigm for fundamentally pushing the
performance boundary in this area. To this end, we contribute the BEVDet
paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View
(BEV), where most target values are defined and route planning can be handily
performed. We merely reuse existing modules to build its framework but
substantially develop its performance by constructing an exclusive data
augmentation strategy and upgrading the Non-Maximum Suppression strategy. In
the experiment, BEVDet offers an excellent trade-off between accuracy and
time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS
on the nuScenes val set. It is comparable with FCOS3D, but requires just 11%
computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS.
Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2%
NDS, significantly exceeding all published results. With a comparable inference
speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The
code will be released to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12970">
<div class="article-summary-box-inner">
<span><p>Scene Graph Generation (SGG) remains a challenging visual understanding task
due to its compositional property. Most previous works adopt a bottom-up
two-stage or a point-based one-stage approach, which often suffers from high
time complexity or sub-optimal designs. In this work, we propose a novel SGG
method to address the aforementioned issues, formulating the task as a
bipartite graph construction problem. To solve the problem, we develop a
transformer-based end-to-end framework that first generates the entity and
predicate proposal set, followed by inferring directed edges to form the
relation triplets. In particular, we develop a new entity-aware predicate
representation based on a structural predicate generator that leverages the
compositional property of relationships. Moreover, we design a graph assembling
module to infer the connectivity of the bipartite scene graph based on our
entity-aware structure, enabling us to generate the scene graph in an
end-to-end manner. Extensive experimental results show that our design is able
to achieve the state-of-the-art or comparable performance on two challenging
benchmarks, surpassing most of the existing approaches and enjoying higher
efficiency in inference. We hope our model can serve as a strong baseline for
the Transformer-based scene graph generation. Code is available:
https://github.com/Scarecrow0/SGTR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Cross-dataset Generalization in License Plate Recognition. (arXiv:2201.00267v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00267">
<div class="article-summary-box-inner">
<span><p>Automatic License Plate Recognition (ALPR) systems have shown remarkable
performance on license plates (LPs) from multiple regions due to advances in
deep learning and the increasing availability of datasets. The evaluation of
deep ALPR systems is usually done within each dataset; therefore, it is
questionable if such results are a reliable indicator of generalization
ability. In this paper, we propose a traditional-split versus
leave-one-dataset-out experimental setup to empirically assess the
cross-dataset generalization of 12 Optical Character Recognition (OCR) models
applied to LP recognition on nine publicly available datasets with a great
variety in several aspects (e.g., acquisition settings, image resolution, and
LP layouts). We also introduce a public dataset for end-to-end ALPR that is the
first to contain images of vehicles with Mercosur LPs and the one with the
highest number of motorcycle images. The experimental results shed light on the
limitations of the traditional-split protocol for evaluating approaches in the
ALPR context, as there are significant drops in performance for most datasets
when training and testing the models in a leave-one-dataset-out fashion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arbitrary Handwriting Image Style Transfer. (arXiv:2201.05346v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05346">
<div class="article-summary-box-inner">
<span><p>This paper proposed a method to imitate handwriting style by style transfer.
We proposed an neural network model based on conditional generative adversarial
networks (cGAN) for handwriting style transfer. This paper improved the loss
function on the basis of the GAN. Compared with other handwriting imitation
methods, the handwriting style transfer's effect and efficiency have been
significantly improved. The experiments showed that the shape of the generated
Chinese characters is clear and the analysis of experimental data showed the
Generative adversarial networks showed excellent performance in handwriting
style transfer. The generated text image is closer to the real handwriting and
achieved a better performance in term of handwriting imitation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08377">
<div class="article-summary-box-inner">
<span><p>Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint. (arXiv:2202.00886v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00886">
<div class="article-summary-box-inner">
<span><p>Multi-perspective cameras are quickly gaining importance in many applications
such as smart vehicles and virtual or augmented reality. However, a large
system size or absence of overlap in neighbouring fields-of-view often
complicate their calibration. We present a novel solution which relies on the
availability of an external motion capture system. Our core contribution
consists of an extension to the hand-eye calibration problem which jointly
solves multi-eye-to-base problems in closed form. We furthermore demonstrate
its equivalence to the multi-eye-in-hand problem. The practical validity of our
approach is supported by our experiments, indicating that the method is highly
efficient and accurate, and outperforms existing closed-form alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06484">
<div class="article-summary-box-inner">
<span><p>In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present D2ADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate labeling efficiency, we design a
dynamic scheduling policy to adjust the labeling budgets between domain
exploration and model uncertainty over time. Extensive experiments show that
our method outperforms existing active learning and domain adaptation baselines
on two benchmarks, GTA5 -&gt; Cityscapes and SYNTHIA -&gt; Cityscapes. With less than
5% target domain annotations, our method reaches comparable results with that
of full supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Satellite Imagery using Deep Learning for the Sensor To Shooter Timeline. (arXiv:2203.00116v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00116">
<div class="article-summary-box-inner">
<span><p>The sensor to shooter timeline is affected by two main variables: satellite
positioning and asset positioning. Speeding up satellite positioning by adding
more sensors or by decreasing processing time is important only if there is a
prepared shooter, otherwise the main source of time is getting the shooter into
position. However, the intelligence community should work towards the
exploitation of sensors to the highest speed and effectiveness possible.
Achieving a high effectiveness while keeping speed high is a tradeoff that must
be considered in the sensor to shooter timeline. In this paper we investigate
two main ideas, increasing the effectiveness of satellite imagery through image
manipulation and how on-board image manipulation would affect the sensor to
shooter timeline. We cover these ideas in four scenarios: Discrete Event
Simulation of onboard processing versus ground station processing, quality of
information with cloud cover removal, information improvement with super
resolution, and data reduction with image to caption. This paper will show how
image manipulation techniques such as Super Resolution, Cloud Removal, and
Image to Caption will improve the quality of delivered information in addition
to showing how those processes effect the sensor to shooter timeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Attention Network: Transformer Meets U-Net. (arXiv:2203.01932v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01932">
<div class="article-summary-box-inner">
<span><p>Currently, convolutional neural networks (CNN) (e.g., U-Net) have become the
de facto standard and attained immense success in medical image segmentation.
However, as a downside, CNN based methods are a double-edged sword as they fail
to build long-range dependencies and global context connections due to the
limited receptive field that stems from the intrinsic characteristics of the
convolution operation. Hence, recent articles have exploited Transformer
variants for medical image segmentation tasks which open up great opportunities
due to their innate capability of capturing long-range correlations through the
attention mechanism. Although being feasibly designed, most of the cohort
studies incur prohibitive performance in capturing local information, thereby
resulting in less lucidness of boundary areas. In this paper, we propose a
contextual attention network to tackle the aforementioned limitations. The
proposed method uses the strength of the Transformer module to model the
long-range contextual dependency. Simultaneously, it utilizes the CNN encoder
to capture local semantic information. In addition, an object-level
representation is included to model the regional interaction map. The extracted
hierarchical features are then fed to the contextual attention module to
adaptively recalibrate the representation space using the local information.
Then, they emphasize the informative regions while taking into account the
long-range contextual dependency derived by the Transformer module. We validate
our method on several large-scale public medical image segmentation datasets
and achieve state-of-the-art performance. We have provided the implementation
code in https://github.com/rezazad68/TMUnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Context Matters: Enhancing Single Image Prediction with Disease Progression Representations. (arXiv:2203.01933v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01933">
<div class="article-summary-box-inner">
<span><p>Clinical outcome or severity prediction from medical images has largely
focused on learning representations from single-timepoint or snapshot scans. It
has been shown that disease progression can be better characterized by temporal
imaging. We therefore hypothesized that outcome predictions can be improved by
utilizing the disease progression information from sequential images. We
present a deep learning approach that leverages temporal progression
information to improve clinical outcome predictions from single-timepoint
images. In our method, a self-attention based Temporal Convolutional Network
(TCN) is used to learn a representation that is most reflective of the disease
trajectory. Meanwhile, a Vision Transformer is pretrained in a self-supervised
fashion to extract features from single-timepoint images. The key contribution
is to design a recalibration module that employs maximum mean discrepancy loss
(MMD) to align distributions of the above two contextual representations. We
train our system to predict clinical outcomes and severity grades from
single-timepoint images. Experiments on chest and osteoarthritis radiography
datasets demonstrate that our approach outperforms other state-of-the-art
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation. (arXiv:2203.02231v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02231">
<div class="article-summary-box-inner">
<span><p>Light field disparity estimation is an essential task in computer vision with
various applications. Although supervised learning-based methods have achieved
both higher accuracy and efficiency than traditional optimization-based
methods, the dependency on ground-truth disparity for training limits the
overall generalization performance not to say for real-world scenarios where
the ground-truth disparity is hard to capture. In this paper, we argue that
unsupervised methods can achieve comparable accuracy, but, more importantly,
much higher generalization capacity and efficiency than supervised methods.
Specifically, we present the Occlusion Pattern Aware Loss, named OPAL, which
successfully extracts and encodes the general occlusion patterns inherent in
the light field for loss calculation. OPAL enables: i) accurate and robust
estimation by effectively handling occlusions without using any ground-truth
information for training and ii) much efficient performance by significantly
reducing the network parameters required for accurate inference. Besides, a
transformer-based network and a refinement module are proposed for achieving
even more accurate results. Extensive experiments demonstrate our method not
only significantly improves the accuracy compared with the SOTA unsupervised
methods, but also possesses strong generalization capacity, even for real-world
data, compared with supervised methods. Our code will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Self-Supervised Category-Level Object Pose and Size Estimation. (arXiv:2203.02884v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02884">
<div class="article-summary-box-inner">
<span><p>In this work, we tackle the challenging problem of category-level object pose
and size estimation from a single depth image. Although previous
fully-supervised works have demonstrated promising performance, collecting
ground-truth pose labels is generally time-consuming and labor-intensive.
Instead, we propose a label-free method that learns to enforce the geometric
consistency between category template mesh and observed object point cloud
under a self-supervision manner. Specifically, our method consists of three key
components: differentiable shape deformation, registration, and rendering. In
particular, shape deformation and registration are applied to the template mesh
to eliminate the differences in shape, pose and scale. A differentiable
renderer is then deployed to enforce geometric consistency between point clouds
lifted from the rendered depth and the observed scene for self-supervision. We
evaluate our approach on real-world datasets and find that our approach
outperforms the simple traditional baseline by large margins while being
competitive with some fully-supervised approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A study on joint modeling and data augmentation of multi-modalities for audio-visual scene classification. (arXiv:2203.04114v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04114">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose two techniques, namely joint modeling and data
augmentation, to improve system performances for audio-visual scene
classification (AVSC). We employ pre-trained networks trained only on image
data sets to extract video embedding; whereas for audio embedding models, we
decide to train them from scratch. We explore different neural network
architectures for joint modeling to effectively combine the video and audio
modalities. Moreover, data augmentation strategies are investigated to increase
audio-visual training set size. For the video modality the effectiveness of
several operations in RandAugment is verified. An audio-video joint mixup
scheme is proposed to further improve AVSC performances. Evaluated on the
development set of TAU Urban Audio Visual Scenes 2021, our final system can
achieve the best accuracy of 94.2% among all single AVSC systems submitted to
DCASE 2021 Task 1b.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment. (arXiv:2203.04121v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04121">
<div class="article-summary-box-inner">
<span><p>Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection. (arXiv:2203.06398v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06398">
<div class="article-summary-box-inner">
<span><p>Domain Adaptive Object Detection (DAOD) leverages a labeled domain to learn
an object detector generalizing to a novel domain free of annotations. Recent
advances align class-conditional distributions by narrowing down cross-domain
prototypes (class centers). Though great success,they ignore the significant
within-class variance and the domain-mismatched semantics within the training
batch, leading to a sub-optimal adaptation. To overcome these challenges, we
propose a novel SemantIc-complete Graph MAtching (SIGMA) framework for DAOD,
which completes mismatched semantics and reformulates the adaptation with graph
matching. Specifically, we design a Graph-embedded Semantic Completion module
(GSC) that completes mismatched semantics through generating hallucination
graph nodes in missing categories. Then, we establish cross-image graphs to
model class-conditional distributions and learn a graph-guided memory bank for
better semantic completion in turn. After representing the source and target
data as graphs, we reformulate the adaptation as a graph matching problem,
i.e., finding well-matched node pairs across graphs to reduce the domain gap,
which is solved with a novel Bipartite Graph Matching adaptor (BGM). In a
nutshell, we utilize graph nodes to establish semantic-aware node affinity and
leverage graph edges as quadratic constraints in a structure-aware matching
loss, achieving fine-grained adaptation with a node-to-node graph matching.
Extensive experiments verify that SIGMA outperforms existing works
significantly. Our code is available at
https://github.com/CityU-AIM-Group/SIGMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs. (arXiv:2203.08516v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08516">
<div class="article-summary-box-inner">
<span><p>The discovery of interpretable directions in the latent spaces of pre-trained
GAN models has recently become a popular topic. In particular, StyleGAN2 has
enabled various image generation and manipulation tasks due to its rich and
disentangled latent spaces. The discovery of such directions is typically done
either in a supervised manner, which requires annotated data for each desired
manipulation or in an unsupervised manner, which requires a manual effort to
identify the directions. As a result, existing work typically finds only a
handful of directions in which controllable edits can be made. In this study,
we design a novel submodular framework that finds the most representative and
diverse subset of directions in the latent space of StyleGAN2. Our approach
takes advantage of the latent space of channel-wise style parameters, so-called
style space, in which we cluster channels that perform similar manipulations
into groups. Our framework promotes diversity by using the notion of clusters
and can be efficiently solved with a greedy optimization scheme. We evaluate
our framework with qualitative and quantitative experiments and show that our
method finds more diverse and disentangled directions. Our project page can be
found at <a href="http://catlab-team.github.io/fantasticstyles.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scribble-Supervised LiDAR Semantic Segmentation. (arXiv:2203.08537v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08537">
<div class="article-summary-box-inner">
<span><p>Densely annotating LiDAR point clouds remains too expensive and
time-consuming to keep up with the ever growing volume of data. While current
literature focuses on fully-supervised performance, developing efficient
methods that take advantage of realistic weak supervision have yet to be
explored. In this paper, we propose using scribbles to annotate LiDAR point
clouds and release ScribbleKITTI, the first scribble-annotated dataset for
LiDAR semantic segmentation. Furthermore, we present a pipeline to reduce the
performance gap that arises when using such weak annotations. Our pipeline
comprises of three stand-alone contributions that can be combined with any
LiDAR semantic segmentation model to achieve up to 95.7% of the
fully-supervised performance while using only 8% labeled points. Our scribble
annotations and code are available at github.com/ouenal/scribblekitti.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Feedback Synthesis Towards Multimodal Text and Image Data. (arXiv:2203.12692v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12692">
<div class="article-summary-box-inner">
<span><p>In this paper, we have defined a novel task of affective feedback synthesis
that deals with generating feedback for input text &amp; corresponding image in a
similar way as humans respond towards the multimodal data. A feedback synthesis
system has been proposed and trained using ground-truth human comments along
with image-text input. We have also constructed a large-scale dataset
consisting of image, text, Twitter user comments, and the number of likes for
the comments by crawling the news articles through Twitter feeds. The proposed
system extracts textual features using a transformer-based textual encoder
while the visual features have been extracted using a Faster region-based
convolutional neural networks model. The textual and visual features have been
concatenated to construct the multimodal features using which the decoder
synthesizes the feedback. We have compared the results of the proposed system
with the baseline models using quantitative and qualitative measures. The
generated feedbacks have been analyzed using automatic and human evaluation.
They have been found to be semantically similar to the ground-truth comments
and relevant to the given text-image input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13032">
<div class="article-summary-box-inner">
<span><p>In this paper, we briefly introduce our submission to the Valence-Arousal
Estimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)
competition. Our method utilizes the multi-modal information, i.e., the visual
and audio information, and employs a temporal encoder to model the temporal
context in the videos. Besides, a smooth processor is applied to get more
reasonable predictions, and a model ensemble strategy is used to improve the
performance of our proposed method. The experiment results show that our method
achieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation
set of the Aff-Wild2 dataset, which prove the effectiveness of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Compression and Actionable Intelligence With Deep Neural Networks. (arXiv:2203.13686v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13686">
<div class="article-summary-box-inner">
<span><p>If a unit cannot receive intelligence from a source due to external factors,
we consider them disadvantaged users. We categorize this as a preoccupied unit
working on a low connectivity device on the edge. This case requires that we
use a different approach to deliver intelligence, particularly satellite
imagery information, than normally employed. To address this, we propose a
survey of information reduction techniques to deliver the information from a
satellite image in a smaller package. We investigate four techniques to aid in
the reduction of delivered information: traditional image compression, neural
network image compression, object detection image cutout, and image to caption.
Each of these mechanisms have their benefits and tradeoffs when considered for
a disadvantaged user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14542">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning methods require a large repository of annotated
data; hence, label noise is inevitable. Training with such noisy data
negatively impacts the generalization performance of deep neural networks. To
combat label noise, recent state-of-the-art methods employ some sort of sample
selection mechanism to select a possibly clean subset of data. Next, an
off-the-shelf semi-supervised learning method is used for training where
rejected samples are treated as unlabeled data. Our comprehensive analysis
shows that current selection methods disproportionately select samples from
easy (fast learnable) classes while rejecting those from relatively harder
ones. This creates class imbalance in the selected clean set and in turn,
deteriorates performance under high label noise. In this work, we propose
UNICON, a simple yet effective sample selection method which is robust to high
label noise. To address the disproportionate selection of easy and hard
samples, we introduce a Jensen-Shannon divergence based uniform selection
mechanism which does not require any probabilistic modeling and hyperparameter
tuning. We complement our selection method with contrastive learning to further
combat the memorization of noisy labels. Extensive experimentation on multiple
benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%
improvement over the current state-of-the-art on CIFAR100 dataset with a 90%
noise rate. Our code is publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15536">
<div class="article-summary-box-inner">
<span><p>Our goal is to recover the 3D shape and pose of dogs from a single image.
This is a challenging task because dogs exhibit a wide range of shapes and
appearances, and are highly articulated. Recent work has proposed to directly
regress the SMAL animal model, with additional limb scale parameters, from
images. Our method, called BARC (Breed-Augmented Regression using
Classification), goes beyond prior work in several important ways. First, we
modify the SMAL shape space to be more appropriate for representing dog shape.
But, even with a better shape model, the problem of regressing dog shape from
an image is still challenging because we lack paired images with 3D ground
truth. To compensate for the lack of paired data, we formulate novel losses
that exploit information about dog breeds. In particular, we exploit the fact
that dogs of the same breed have similar body shapes. We formulate a novel
breed similarity loss consisting of two parts: One term encourages the shape of
dogs from the same breed to be more similar than dogs of different breeds. The
second one, a breed classification loss, helps to produce recognizable
breed-specific shapes. Through ablation studies, we find that our breed losses
significantly improve shape accuracy over a baseline without them. We also
compare BARC qualitatively to WLDO with a perceptual study and find that our
approach produces dogs that are significantly more realistic. This work shows
that a-priori information about genetic similarity can help to compensate for
the lack of 3D training data. This concept may be applicable to other animal
species or groups of species. Our code is publicly available for research
purposes at https://barc.is.tue.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15547">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks need the construction of informative features,
which are determined by channel-wise and spatial-wise information at the
network's layers. In this research, we focus on bringing in a novel solution
that uses sophisticated optimization for enhancing both the spatial and channel
components inside each layer's receptive field. Capsule Networks were used to
understand the spatial association between features in the feature map.
Standalone capsule networks have shown good results on comparatively simple
datasets than on complex datasets as a result of the inordinate amount of
feature information. Thus, to tackle this issue, we have proposed ME-CapsNet by
introducing deeper convolutional layers to extract important features before
passing through modules of capsule layers strategically to improve the
performance of the network significantly. The deeper convolutional layer
includes blocks of Squeeze-Excitation networks which use a stochastic sampling
approach for progressively reducing the spatial size thereby dynamically
recalibrating the channels by reconstructing their interdependencies without
much loss of important feature information. Extensive experimentation was done
using commonly used datasets demonstrating the efficiency of the proposed
ME-CapsNet, which clearly outperforms various research works by achieving
higher accuracy with minimal model complexity in complex datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15588">
<div class="article-summary-box-inner">
<span><p>The rapid development of diagnostic technologies in healthcare is leading to
higher requirements for physicians to handle and integrate the heterogeneous,
yet complementary data that are produced during routine practice. For instance,
the personalized diagnosis and treatment planning for a single cancer patient
relies on the various images (e.g., radiological, pathological, and camera
images) and non-image data (e.g., clinical data and genomic data). However,
such decision-making procedures can be subjective, qualitative, and have large
inter-subject variabilities. With the recent advances in multi-modal deep
learning technologies, an increasingly large number of efforts have been
devoted to a key question: how do we extract and aggregate multi-modal
information to ultimately provide more objective, quantitative computer-aided
clinical decision making? This paper reviews the recent studies on dealing with
such a question. Briefly, this review will include the (1) overview of current
multi-modal learning workflows, (2) summarization of multi-modal fusion
methods, (3) discussion of the performance, (4) applications in disease
diagnosis and prognosis, and (5) challenges and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proactive Image Manipulation Detection. (arXiv:2203.15880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15880">
<div class="article-summary-box-inner">
<span><p>Image manipulation detection algorithms are often trained to discriminate
between images manipulated with particular Generative Models (GMs) and
genuine/real images, yet generalize poorly to images manipulated with GMs
unseen in the training. Conventional detection algorithms receive an input
image passively. By contrast, we propose a proactive scheme to image
manipulation detection. Our key enabling technique is to estimate a set of
templates which when added onto the real image would lead to more accurate
manipulation detection. That is, a template protected real image, and its
manipulated version, is better discriminated compared to the original real
image vs. its manipulated one. These templates are estimated using certain
constraints based on the desired properties of templates. For image
manipulation detection, our proposed approach outperforms the prior work by an
average precision of 16% for CycleGAN and 32% for GauGAN. Our approach is
generalizable to a variety of GMs showing an improvement over prior work by an
average precision of 10% averaged across 12 GMs. Our code is available at
https://www.github.com/vishal3477/proactive_IMD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forecasting from LiDAR via Future Object Detection. (arXiv:2203.16297v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16297">
<div class="article-summary-box-inner">
<span><p>Object detection and forecasting are fundamental components of embodied
perception. These two problems, however, are largely studied in isolation by
the community. In this paper, we propose an end-to-end approach for detection
and motion forecasting based on raw sensor measurement as opposed to ground
truth tracks. Instead of predicting the current frame locations and forecasting
forward in time, we directly predict future object locations and backcast to
determine where each trajectory began. Our approach not only improves overall
accuracy compared to other modular or end-to-end baselines, it also prompts us
to rethink the role of explicit tracking for embodied perception. Additionally,
by linking future and current locations in a many-to-one manner, our approach
is able to reason about multiple futures, a capability that was previously
considered difficult for end-to-end approaches. We conduct extensive
experiments on the popular nuScenes dataset and demonstrate the empirical
effectiveness of our approach. In addition, we investigate the appropriateness
of reusing standard forecasting metrics for an end-to-end setup, and find a
number of limitations which allow us to build simple baselines to game these
metrics. We address this issue with a novel set of joint forecasting and
detection metrics that extend the commonly used AP metrics from the detection
community to measuring forecasting accuracy. Our code is available at
https://github.com/neeharperi/FutureDet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The impact of using voxel-level segmentation metrics on evaluating multifocal prostate cancer localisation. (arXiv:2203.16415v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16415">
<div class="article-summary-box-inner">
<span><p>Dice similarity coefficient (DSC) and Hausdorff distance (HD) are widely used
for evaluating medical image segmentation. They have also been criticised, when
reported alone, for their unclear or even misleading clinical interpretation.
DSCs may also differ substantially from HDs, due to boundary smoothness or
multiple regions of interest (ROIs) within a subject. More importantly, either
metric can also have a nonlinear, non-monotonic relationship with outcomes
based on Type 1 and 2 errors, designed for specific clinical decisions that use
the resulting segmentation. Whilst cases causing disagreement between these
metrics are not difficult to postulate. This work first proposes a new
asymmetric detection metric, adapting those used in object detection, for
planning prostate cancer procedures. The lesion-level metrics is then compared
with the voxel-level DSC and HD, whereas a 3D UNet is used for segmenting
lesions from multiparametric MR (mpMR) images. Based on experimental results we
report pairwise agreement and correlation 1) between DSC and HD, and 2) between
voxel-level DSC and recall-controlled precision at lesion-level, with Cohen's
[0.49, 0.61] and Pearson's [0.66, 0.76] (p-values}&lt;0.001) at varying cut-offs.
However, the differences in false-positives and false-negatives, between the
actual errors and the perceived counterparts if DSC is used, can be as high as
152 and 154, respectively, out of the 357 test set lesions. We therefore
carefully conclude that, despite of the significant correlations, voxel-level
metrics such as DSC can misrepresent lesion-level detection accuracy for
evaluating localisation of multifocal prostate cancer and should be interpreted
with caution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast, Accurate and Memory-Efficient Partial Permutation Synchronization. (arXiv:2203.16505v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16505">
<div class="article-summary-box-inner">
<span><p>Previous partial permutation synchronization (PPS) algorithms, which are
commonly used for multi-object matching, often involve computation-intensive
and memory-demanding matrix operations. These operations become intractable for
large scale structure-from-motion datasets. For pure permutation
synchronization, the recent Cycle-Edge Message Passing (CEMP) framework
suggests a memory-efficient and fast solution. Here we overcome the restriction
of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for
estimating the corruption levels of the observed partial permutations. It
allows us to subsequently implement a nonconvex weighted projected power method
without the need of spectral initialization. The resulting new PPS algorithm,
MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse
matrix operations, and thus enjoys lower time and space complexities in
comparison to previous PPS algorithms. We prove that under adversarial
corruption, though without additive noise and with certain assumptions,
CEMP-Partial is able to exactly classify corrupted and clean partial
permutations. We demonstrate the state-of-the-art accuracy, speed and memory
efficiency of our method on both synthetic and real datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaMixer: A Fast-Converging Query-Based Object Detector. (arXiv:2203.16507v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16507">
<div class="article-summary-box-inner">
<span><p>Traditional object detectors employ the dense paradigm of scanning over
locations and scales in an image. The recent query-based object detectors break
this convention by decoding image features with a set of learnable queries.
However, this paradigm still suffers from slow convergence, limited
performance, and design complexity of extra networks between backbone and
decoder. In this paper, we find that the key to these issues is the
adaptability of decoders for casting queries to varying objects. Accordingly,
we propose a fast-converging query-based detector, named AdaMixer, by improving
the adaptability of query-based decoding processes in two aspects. First, each
query adaptively samples features over space and scales based on estimated
offsets, which allows AdaMixer to efficiently attend to the coherent regions of
objects. Then, we dynamically decode these sampled features with an adaptive
MLP-Mixer under the guidance of each query. Thanks to these two critical
designs, AdaMixer enjoys architectural simplicity without requiring dense
attentional encoders or explicit pyramid networks. On the challenging MS COCO
benchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs,
reaches up to 45.0 AP on the validation set along with 27.9 APs in detecting
small objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN
and Swin-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple,
accurate, and fast converging architecture for query-based object detectors.
The code is made available at https://github.com/MCG-NJU/AdaMixer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-Grasp: Physically Plausible Dynamic Grasp Synthesis for Hand-Object Interactions. (arXiv:2112.03028v2 [cs.RO] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03028">
<div class="article-summary-box-inner">
<span><p>We introduce the dynamic grasp synthesis task: given an object with a known
6D pose and a grasp reference, our goal is to generate motions that move the
object to a target 6D pose. This is challenging, because it requires reasoning
about the complex articulation of the human hand and the intricate physical
interaction with the object. We propose a novel method that frames this problem
in the reinforcement learning framework and leverages a physics simulation,
both to learn and to evaluate such dynamic interactions. A hierarchical
approach decomposes the task into low-level grasping and high-level motion
synthesis. It can be used to generate novel hand sequences that approach,
grasp, and move an object to a desired location, while retaining
human-likeness. We show that our approach leads to stable grasps and generates
a wide range of motions. Furthermore, even imperfect labels can be corrected by
our method to generate dynamic interaction sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01522">
<div class="article-summary-box-inner">
<span><p>Despite the success of deep neural networks, there are still many challenges
in deep representation learning due to the data scarcity issues such as data
imbalance, unseen distribution, and domain shift. To address the
above-mentioned issues, a variety of methods have been devised to explore the
sample relationships in a vanilla way (i.e., from the perspectives of either
the input or the loss function), failing to explore the internal structure of
deep neural networks for learning with sample relationships. Inspired by this,
we propose to enable deep neural networks themselves with the ability to learn
the sample relationships from each mini-batch. Specifically, we introduce a
batch transformer module or BatchFormer, which is then applied into the batch
dimension of each mini-batch to implicitly explore sample relationships during
training. By doing this, the proposed method enables the collaboration of
different samples, e.g., the head-class samples can also contribute to the
learning of the tail classes for long-tailed recognition. Furthermore, to
mitigate the gap between training and testing, we share the classifier between
with or without the BatchFormer during training, which can thus be removed
during testing. We perform extensive experiments on over ten datasets and the
proposed method achieves significant improvements on different data scarcity
applications without any bells and whistles, including the tasks of long-tailed
recognition, compositional zero-shot learning, domain generalization, and
contrastive learning. Code will be made publicly available at
https://github.com/zhihou7/BatchFormer.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-02 23:07:32.855091448 UTC">2022-04-02 23:07:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>