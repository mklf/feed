<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-03T01:30:00Z">01-03</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Bidirectional LSTM for Deceptive Opinion Spam Classification. (arXiv:2112.14789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14789">
<div class="article-summary-box-inner">
<span><p>Online Reviews play a vital role in e commerce for decision making. Much of
the population makes the decision of which places, restaurant to visit, what to
buy and from where to buy based on the reviews posted on the respective
platforms. A fraudulent review or opinion spam is categorized as an untruthful
or deceptive review. Positive reviews of a product or a restaurant helps
attract customers and thereby lead to an increase in sales whereas negative
reviews may hamper the progress of a restaurant or sales of a product and
thereby lead to defamed reputation and loss. Fraudulent reviews are
deliberately posted on various online review platforms to trick customers to
buy, visit or distract against a product or a restaurant. They are also written
to commend or discredit the product's repute. The work aims at detecting and
classifying the reviews as deceptive or truthful. It involves use of various
deep learning techniques for classifying the reviews and an overview of
proposed approach involving Attention based Bidirectional LSTM to tackle issues
related to semantic information in reviews and a comparative study over
baseline machine learning techniques for review classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Materialized Knowledge Bases from Commonsense Transformers. (arXiv:2112.14815v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14815">
<div class="article-summary-box-inner">
<span><p>Starting from the COMET methodology by Bosselut et al. (2019), generating
commonsense knowledge directly from pre-trained language models has recently
received significant attention. Surprisingly, up to now no materialized
resource of commonsense knowledge generated this way is publicly available.
This paper fills this gap, and uses the materialized resources to perform a
detailed analysis of the potential of this approach in terms of precision and
recall. Furthermore, we identify common problem cases, and outline use cases
enabled by materialized resources. We posit that the availability of these
resources is important for the advancement of the field, as it enables an
off-the-shelf-use of the resulting knowledge, as well as further analyses on
its strengths and weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Hierarchical Temporal Memory Theory for Document Categorization. (arXiv:2112.14820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14820">
<div class="article-summary-box-inner">
<span><p>The current work intends to study the performance of the Hierarchical
Temporal Memory(HTM) theory for automated classification of text as well as
documents. HTM is a biologically inspired theory based on the working
principles of the human neocortex. The current study intends to provide an
alternative framework for document categorization using the Spatial Pooler
learning algorithm in the HTM Theory. As HTM accepts only a stream of binary
data as input, Latent Semantic Indexing(LSI) technique is used for extracting
the top features from the input and converting them into binary format. The
Spatial Pooler algorithm converts the binary input into sparse patterns with
similar input text having overlapping spatial patterns making it easy for
classifying the patterns into categories. The results obtained prove that HTM
theory, although is in its nascent stages, performs at par with most of the
popular machine learning based classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QEMind: Alibaba's Submission to the WMT21 Quality Estimation Shared Task. (arXiv:2112.14890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14890">
<div class="article-summary-box-inner">
<span><p>Quality Estimation, as a crucial step of quality control for machine
translation, has been explored for years. The goal is to investigate automatic
methods for estimating the quality of machine translation results without
reference translations. In this year's WMT QE shared task, we utilize the
large-scale XLM-Roberta pre-trained model and additionally propose several
useful features to evaluate the uncertainty of the translations to build our QE
system, named \textit{QEMind}. The system has been applied to the
sentence-level scoring task of Direct Assessment and the binary score
prediction task of Critical Error Detection. In this paper, we present our
submissions to the WMT 2021 QE shared task and an extensive set of experimental
results have shown us that our multilingual systems outperform the best system
in the Direct Assessment QE task of WMT 2020.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RheFrameDetect: A Text Classification System for Automatic Detection of Rhetorical Frames in AI from Open Sources. (arXiv:2112.14933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14933">
<div class="article-summary-box-inner">
<span><p>Rhetorical Frames in AI can be thought of as expressions that describe AI
development as a competition between two or more actors, such as governments or
companies. Examples of such Frames include robotic arms race, AI rivalry,
technological supremacy, cyberwarfare dominance and 5G race. Detection of
Rhetorical Frames from open sources can help us track the attitudes of
governments or companies towards AI, specifically whether attitudes are
becoming more cooperative or competitive over time. Given the rapidly
increasing volumes of open sources (online news media, twitter, blogs), it is
difficult for subject matter experts to identify Rhetorical Frames in (near)
real-time. Moreover, these sources are in general unstructured (noisy) and
therefore, detecting Frames from these sources will require state-of-the-art
text classification techniques. In this paper, we develop RheFrameDetect, a
text classification system for (near) real-time capture of Rhetorical Frames
from open sources. Given an input document, RheFrameDetect employs text
classification techniques at multiple levels (document level and paragraph
level) to identify all occurrences of Frames used in the discussion of AI. We
performed extensive evaluation of the text classification techniques used in
RheFrameDetect against human annotated Frames from multiple news sources. To
further demonstrate the effectiveness of RheFrameDetect, we show multiple case
studies depicting the Frames identified by RheFrameDetect compared against
human annotated Frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Mixed-Precision Quantization Search of BERT. (arXiv:2112.14938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14938">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models such as BERT have shown remarkable effectiveness
in various natural language processing tasks. However, these models usually
contain millions of parameters, which prevents them from practical deployment
on resource-constrained devices. Knowledge distillation, Weight pruning, and
Quantization are known to be the main directions in model compression. However,
compact models obtained through knowledge distillation may suffer from
significant accuracy drop even for a relatively small compression ratio. On the
other hand, there are only a few quantization attempts that are specifically
designed for natural language processing tasks. They suffer from a small
compression ratio or a large error rate since manual setting on
hyper-parameters is required and fine-grained subgroup-wise quantization is not
supported. In this paper, we proposed an automatic mixed-precision quantization
framework designed for BERT that can simultaneously conduct quantization and
pruning in a subgroup-wise level. Specifically, our proposed method leverages
Differentiable Neural Architecture Search to assign scale and precision for
parameters in each sub-group automatically, and at the same time pruning out
redundant groups of parameters. Extensive evaluations on BERT downstream tasks
reveal that our proposed method outperforms baselines by providing the same
performance with much smaller model size. We also show the feasibility of
obtaining the extremely light-weight model by combining our solution with
orthogonal methods such as DistilBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Matters: Radiology Report Generation with General and Specific Knowledge. (arXiv:2112.15009v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15009">
<div class="article-summary-box-inner">
<span><p>Automatic radiology report generation is critical in clinics which can
relieve experienced radiologists from the heavy workload and remind
inexperienced radiologists of misdiagnosis or missed diagnose. Existing
approaches mainly formulate radiology report generation as an image captioning
task and adopt the encoder-decoder framework. However, in the medical domain,
such pure data-driven approaches suffer from the following problems: 1) visual
and textual bias problem; 2) lack of expert knowledge. In this paper, we
propose a knowledge-enhanced radiology report generation approach introduces
two types of medical knowledge: 1) General knowledge, which is input
independent and provides the broad knowledge for report generation; 2) Specific
knowledge, which is input dependent and provides the fine-grained knowledge for
report generation. To fully utilize both the general and specific knowledge, we
also propose a knowledge-enhanced multi-head attention mechanism. By merging
the visual features of the radiology image with general knowledge and specific
knowledge, the proposed model can improve the quality of generated reports.
Experimental results on two publicly available datasets IU-Xray and MIMIC-CXR
show that the proposed knowledge enhanced approach outperforms state-of-the-art
image captioning based methods. Ablation studies also demonstrate that both
general and specific knowledge can help to improve the performance of radiology
report generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15011">
<div class="article-summary-box-inner">
<span><p>In clinics, a radiology report is crucial for guiding a patient's treatment.
Unfortunately, report writing imposes a heavy burden on radiologists. To
effectively reduce such a burden, we hereby present an automatic, multi-modal
approach for report generation from chest x-ray. Our approach, motivated by the
observation that the descriptions in radiology reports are highly correlated
with the x-ray images, features two distinct modules: (i) Learned knowledge
base. To absorb the knowledge embedded in the above-mentioned correlation, we
automatically build a knowledge base based on textual embedding. (ii)
Multi-modal alignment. To promote the semantic alignment among reports, disease
labels and images, we explicitly utilize textual embedding to guide the
learning of the visual feature space. We evaluate the performance of the
proposed model using metrics from both natural language generation and clinic
efficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that
each module contributes to improving the quality of generated reports.
Furthermore, with the aid of both modules, our approach clearly outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YACLC: A Chinese Learner Corpus with Multidimensional Annotation. (arXiv:2112.15043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15043">
<div class="article-summary-box-inner">
<span><p>Learner corpus collects language data produced by L2 learners, that is second
or foreign-language learners. This resource is of great relevance for second
language acquisition research, foreign-language teaching, and automatic
grammatical error correction. However, there is little focus on learner corpus
for Chinese as Foreign Language (CFL) learners. Therefore, we propose to
construct a large-scale, multidimensional annotated Chinese learner corpus. To
construct the corpus, we first obtain a large number of topic-rich texts
generated by CFL learners. Then we design an annotation scheme including a
sentence acceptability score as well as grammatical error and fluency-based
corrections. We build a crowdsourcing platform to perform the annotation
effectively (https://yaclc.wenmind.net). We name the corpus YACLC (Yet Another
Chinese Learner Corpus) and release it as part of the CUGE benchmark
(<a href="http://cuge.baai.ac.cn">this http URL</a>). By analyzing the original sentences and annotations
in the corpus, we found that YACLC has a considerable size and very high
annotation quality. We hope this corpus can further enhance the studies on
Chinese International Education and Chinese automatic grammatical error
correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does QA-based intermediate training help fine-tuning language models for text classification?. (arXiv:2112.15051v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15051">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained language models for downstream tasks has become a
norm for NLP. Recently it is found that intermediate training based on
high-level inference tasks such as Question Answering (QA) can improve the
performance of some language models for target tasks. However it is not clear
if intermediate training generally benefits various language models. In this
paper, using the SQuAD-2.0 QA task for intermediate training for target text
classification tasks, we experimented on eight tasks for single-sequence
classification and eight tasks for sequence-pair classification using two base
and two compact language models. Our experiments show that QA-based
intermediate training generates varying transfer performance across different
language models, except for similar QA tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextRGNN: Residual Graph Neural Networks for Text Classification. (arXiv:2112.15060v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15060">
<div class="article-summary-box-inner">
<span><p>Recently, text classification model based on graph neural network (GNN) has
attracted more and more attention. Most of these models adopt a similar network
paradigm, that is, using pre-training node embedding initialization and
two-layer graph convolution. In this work, we propose TextRGNN, an improved GNN
structure that introduces residual connection to deepen the convolution network
depth. Our structure can obtain a wider node receptive field and effectively
suppress the over-smoothing of node features. In addition, we integrate the
probabilistic language model into the initialization of graph node embedding,
so that the non-graph semantic information of can be better extracted. The
experimental results show that our model is general and efficient. It can
significantly improve the classification accuracy whether in corpus level or
text level, and achieve SOTA performance on a wide range of text classification
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KIND: an Italian Multi-Domain Dataset for Named Entity Recognition. (arXiv:2112.15099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15099">
<div class="article-summary-box-inner">
<span><p>In this paper we present KIND, an Italian dataset for Named-Entity
Recognition. It contains more than one million tokens with the annotation
covering three classes: persons, locations, and organizations. Most of the
dataset (around 600K tokens) contains manual gold annotations in three
different domains: news, literature, and political discourses. Texts and
annotations are downloadable for free from the Github repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing Wordnets for Cognate Detection among Indian Languages. (arXiv:2112.15124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15124">
<div class="article-summary-box-inner">
<span><p>Automatic Cognate Detection (ACD) is a challenging task which has been
utilized to help NLP applications like Machine Translation, Information
Retrieval and Computational Phylogenetics. Unidentified cognate pairs can pose
a challenge to these applications and result in a degradation of performance.
In this paper, we detect cognate word pairs among ten Indian languages with
Hindi and use deep learning methodologies to predict whether a word pair is
cognate or not. We identify IndoWordnet as a potential resource to detect
cognate word pairs based on orthographic similarity-based methods and train
neural network models using the data obtained from it. We identify parallel
corpora as another potential resource and perform the same experiments for
them. We also validate the contribution of Wordnets through further
experimentation and report improved performance of up to 26%. We discuss the
nuances of cognate detection among closely related Indian languages and release
the lists of detected cognates as a dataset. We also observe the behaviour of,
to an extent, unrelated Indian language pairs and release the lists of detected
cognates among them as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">First order linear logic and tensor type calculus for categorial grammars. (arXiv:2112.15253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15253">
<div class="article-summary-box-inner">
<span><p>We study relationship between first order multiplicative linear logic (MLL1),
which has been known to provide representations to different categorial
grammars, and the recently introduced extended tensor type calculus (ETTC). We
identify a fragment of MLL1, which seems sufficient for many grammar
representations, and establish a correspondence between ETTC and this fragment.
The system ETTC, thus, can be seen as an alternative syntax and intrinsic
deductive system together with a geometric representation for the latter. We
also give a natural deduction formulation of ETTC, which might be convenient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNMT: Neural Machine Translation Tookit. (arXiv:2112.15272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15272">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolkit for neural machine translation (NMT). The
new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along
with many other improvements detailed below, in order to create a
self-contained, simple to use, consistent and comprehensive framework for
Machine Translation tasks of various domains. It is tooled to support both
bilingual and multilingual translation tasks, starting from building the model
from respective corpora, to inferring new predictions or packaging the model to
serving-capable JIT format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What is Event Knowledge Graph: A Survey. (arXiv:2112.15280v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15280">
<div class="article-summary-box-inner">
<span><p>Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),
events are also an essential kind of knowledge in the world, which trigger the
spring up of event-centric knowledge representation form like Event KG (EKG).
It plays an increasingly important role in many machine learning and artificial
intelligence applications, such as intelligent search, question-answering,
recommendation, and text generation. This paper provides a comprehensive survey
of EKG from history, ontology, instance, and application views. Specifically,
to characterize EKG thoroughly, we focus on its history, definitions, schema
induction, acquisition, related representative graphs/systems, and
applications. The development processes and trends are studied therein. We
further summarize perspective directions to facilitate future research on EKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation. (arXiv:2112.15283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15283">
<div class="article-summary-box-inner">
<span><p>Conventional methods for the image-text generation tasks mainly tackle the
naturally bidirectional generation tasks separately, focusing on designing
task-specific frameworks to improve the quality and fidelity of the generated
samples. Recently, Vision-Language Pre-training models have greatly improved
the performance of the image-to-text generation tasks, but large-scale
pre-training models for text-to-image synthesis task are still under-developed.
In this paper, we propose ERNIE-ViLG, a unified generative pre-training
framework for bidirectional image-text generation with transformer model. Based
on the image quantization models, we formulate both image generation and text
generation as autoregressive generative tasks conditioned on the text/image
input. The bidirectional image-text generative modeling eases the semantic
alignments across vision and language. For the text-to-image generation
process, we further propose an end-to-end training method to jointly learn the
visual sequence generator and the image reconstructor. To explore the landscape
of large-scale pre-training for bidirectional text-image generation, we train a
10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million
(Chinese) image-text pairs which achieves state-of-the-art performance for both
text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for
text-to-image synthesis and best results on COCO-CN and AIC-ICC for image
captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation with Category Attention Network for Deep Sentiment Analysis. (arXiv:2112.15290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15290">
<div class="article-summary-box-inner">
<span><p>Domain adaptation tasks such as cross-domain sentiment classification aim to
utilize existing labeled data in the source domain and unlabeled or few labeled
data in the target domain to improve the performance in the target domain via
reducing the shift between the data distributions. Existing cross-domain
sentiment classification methods need to distinguish pivots, i.e., the
domain-shared sentiment words, and non-pivots, i.e., the domain-specific
sentiment words, for excellent adaptation performance. In this paper, we first
design a Category Attention Network (CAN), and then propose a model named
CAN-CNN to integrate CAN and a Convolutional Neural Network (CNN). On the one
hand, the model regards pivots and non-pivots as unified category attribute
words and can automatically capture them to improve the domain adaptation
performance; on the other hand, the model makes an attempt at interpretability
to learn the transferred category attribute words. Specifically, the
optimization objective of our model has three different components: 1) the
supervised classification loss; 2) the distributions loss of category feature
weights; 3) the domain invariance loss. Finally, the proposed model is
evaluated on three public sentiment analysis datasets and the results
demonstrate that CAN-CNN can outperform other various baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deconfounded Visual Grounding. (arXiv:2112.15324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15324">
<div class="article-summary-box-inner">
<span><p>We focus on the confounding bias between language and location in the visual
grounding pipeline, where we find that the bias is the major visual reasoning
bottleneck. For example, the grounding process is usually a trivial
language-location association without visual reasoning, e.g., grounding any
language query containing sheep to the nearly central regions, due to that most
queries about sheep have ground-truth locations at the image center. First, we
frame the visual grounding pipeline into a causal graph, which shows the
causalities among image, query, target location and underlying confounder.
Through the causal graph, we know how to break the grounding bottleneck:
deconfounded visual grounding. Second, to tackle the challenge that the
confounder is unobserved in general, we propose a confounder-agnostic approach
called: Referring Expression Deconfounder (RED), to remove the confounding
bias. Third, we implement RED as a simple language attention, which can be
applied in any grounding method. On popular benchmarks, RED improves various
state-of-the-art grounding methods by a significant margin. Code will soon be
available at: https://github.com/JianqiangH/Deconfounded_VG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Graph-Aware Reinforcement Learning to Identify Winning Strategies in Diplomacy Games (Student Abstract). (arXiv:2112.15331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15331">
<div class="article-summary-box-inner">
<span><p>This abstract proposes an approach towards goal-oriented modeling of the
detection and modeling complex social phenomena in multiparty discourse in an
online political strategy game. We developed a two-tier approach that first
encodes sociolinguistic behavior as linguistic features then use reinforcement
learning to estimate the advantage afforded to any player. In the first tier,
sociolinguistic behavior, such as Friendship and Reasoning, that speakers use
to influence others are encoded as linguistic features to identify the
persuasive strategies applied by each player in simultaneous two-party
dialogues. In the second tier, a reinforcement learning approach is used to
estimate a graph-aware reward function to quantify the advantage afforded to
each player based on their standing in this multiparty setup. We apply this
technique to the game Diplomacy, using a dataset comprising of over 15,000
messages exchanged between 78 users. Our graph-aware approach shows robust
performance compared to a context-agnostic setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering Vietnamese Conversations From Facebook Page To Build Training Dataset For Chatbot. (arXiv:2112.15338v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15338">
<div class="article-summary-box-inner">
<span><p>The biggest challenge of building chatbots is training data. The required
data must be realistic and large enough to train chatbots. We create a tool to
get actual training data from Facebook messenger of a Facebook page. After text
preprocessing steps, the newly obtained dataset generates FVnC and Sample
dataset. We use the Retraining of BERT for Vietnamese (PhoBERT) to extract
features of our text data. K-Means and DBSCAN clustering algorithms are used
for clustering tasks based on output embeddings from PhoBERT$_{base}$. We apply
V-measure score and Silhouette score to evaluate the performance of clustering
algorithms. We also demonstrate the efficiency of PhoBERT compared to other
models in feature extraction on Sample dataset. A GridSearch algorithm that
combines both clustering evaluations is also proposed to find optimal
parameters. Thanks to clustering such a number of conversations, we save a lot
of time and effort to build data and storylines for training chatbot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenQA: Hybrid QA System Relying on Structured Knowledge Base as well as Non-structured Data. (arXiv:2112.15356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15356">
<div class="article-summary-box-inner">
<span><p>Search engines based on keyword retrieval can no longer adapt to the way of
information acquisition in the era of intelligent Internet of Things due to the
return of keyword related Internet pages. How to quickly, accurately and
effectively obtain the information needed by users from massive Internet data
has become one of the key issues urgently needed to be solved. We propose an
intelligent question-answering system based on structured KB and unstructured
data, called OpenQA, in which users can give query questions and the model can
quickly give accurate answers back to users. We integrate KBQA structured
question answering based on semantic parsing and deep representation learning,
and two-stage unstructured question answering based on retrieval and neural
machine reading comprehension into OpenQA, and return the final answer with the
highest probability through the Transformer answer selection module in OpenQA.
We carry out preliminary experiments on our constructed dataset, and the
experimental results prove the effectiveness of the proposed intelligent
question answering system. At the same time, the core technology of each module
of OpenQA platform is still in the forefront of academic hot spots, and the
theoretical essence and enrichment of OpenQA will be further explored based on
these academic hot spots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypers at ComMA@ICON: Modelling Aggressiveness, Gender Bias and Communal Bias Identification. (arXiv:2112.15417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15417">
<div class="article-summary-box-inner">
<span><p>Due to the exponentially increasing reach of social media, it is essential to
focus on its negative aspects as it can potentially divide society and incite
people into violence. In this paper, we present our system description of work
on the shared task ComMA@ICON, where we have to classify how aggressive the
sentence is and if the sentence is gender-biased or communal biased. These
three could be the primary reasons to cause significant problems in society. As
team Hypers we have proposed an approach that utilizes different pretrained
models with Attention and mean pooling methods. We were able to get Rank 3 with
0.223 Instance F1 score on Bengali, Rank 2 with 0.322 Instance F1 score on
Multi-lingual set, Rank 4 with 0.129 Instance F1 score on Meitei and Rank 5
with 0.336 Instance F1 score on Hindi. The source code and the pretrained
models of this work can be found here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Using Gaze Behaviour for Natural Language Processing. (arXiv:2112.15471v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15471">
<div class="article-summary-box-inner">
<span><p>Gaze behaviour has been used as a way to gather cognitive information for a
number of years. In this paper, we discuss the use of gaze behaviour in solving
different tasks in natural language processing (NLP) without having to record
it at test time. This is because the collection of gaze behaviour is a costly
task, both in terms of time and money. Hence, in this paper, we focus on
research done to alleviate the need for recording gaze behaviour at run time.
We also mention different eye tracking corpora in multiple languages, which are
currently available and can be used in natural language processing. We conclude
our paper by discussing applications in a domain - education - and how learning
gaze behaviour can help in solving the tasks of complex word identification and
automatic essay grading.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training and Generating Neural Networks in Compressed Weight Space. (arXiv:2112.15545v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15545">
<div class="article-summary-box-inner">
<span><p>The inputs and/or outputs of some neural nets are weight matrices of other
neural nets. Indirect encodings or end-to-end compression of weight matrices
could help to scale such approaches. Our goal is to open a discussion on this
topic, starting with recurrent neural networks for character-level language
modelling whose weight matrices are encoded by the discrete cosine transform.
Our fast weight version thereof uses a recurrent neural network to parameterise
the compressed weights. We present experimental results on the enwik8 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTphone: Phonetically-Aware Encoder Representations for Utterance-Level Speaker and Language Recognition. (arXiv:1907.00457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.00457">
<div class="article-summary-box-inner">
<span><p>We introduce BERTphone, a Transformer encoder trained on large speech corpora
that outputs phonetically-aware contextual representation vectors that can be
used for both speaker and language recognition. This is accomplished by
training on two objectives: the first, inspired by adapting BERT to the
continuous domain, involves masking spans of input frames and reconstructing
the whole sequence for acoustic representation learning; the second, inspired
by the success of bottleneck features from ASR, is a sequence-level CTC loss
applied to phoneme labels for phonetic representation learning. We pretrain two
BERTphone models (one on Fisher and one on TED-LIUM) and use them as feature
extractors into x-vector-style DNNs for both tasks. We attain a
state-of-the-art $C_{\text{avg}}$ of 6.16 on the challenging LRE07 3sec
closed-set language recognition task. On Fisher and VoxCeleb speaker
recognition tasks, we see an 18% relative reduction in speaker EER when
training on BERTphone vectors instead of MFCCs. In general, BERTphone
outperforms previous phonetic pretraining approaches on the same data. We
release our code and models at
https://github.com/awslabs/speech-representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refining Language Models with Compositional Explanations. (arXiv:2103.10415v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10415">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been successful on text classification
tasks, but are prone to learning spurious correlations from biased datasets,
and are thus vulnerable when making inferences in a new domain. Prior work
reveals such spurious patterns via post-hoc explanation algorithms which
compute the importance of input features. Further, the model is regularized to
align the importance scores with human knowledge, so that the unintended model
behaviors are eliminated. However, such a regularization technique lacks
flexibility and coverage, since only importance scores towards a pre-defined
list of features are adjusted, while more complex human knowledge such as
feature interaction and pattern generalization can hardly be incorporated. In
this work, we propose to refine a learned language model for a target domain by
collecting human-provided compositional explanations regarding observed biases.
By parsing these explanations into executable logic rules, the human-specified
refinement advice from a small set of explanations can be generalized to more
training examples. We additionally introduce a regularization term allowing
adjustments for both importance and interaction of features to better rectify
model behavior. We demonstrate the effectiveness of the proposed approach on
two text classification tasks by showing improved performance in target domain
as well as improved model fairness after refinement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Emotion and Reasoning its Flip in Multi-Party Conversations using Masked Memory Network and Transformer. (arXiv:2103.12360v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12360">
<div class="article-summary-box-inner">
<span><p>Efficient discovery of a speaker's emotional states in a multi-party
conversation is significant to design human-like conversational agents. During
a conversation, the cognitive state of a speaker often alters due to certain
past utterances, which may lead to a flip in their emotional state. Therefore,
discovering the reasons (triggers) behind the speaker's emotion-flip during a
conversation is essential to explain the emotion labels of individual
utterances. In this paper, along with addressing the task of emotion
recognition in conversations (ERC), we introduce a novel task - Emotion-Flip
Reasoning (EFR), that aims to identify past utterances which have triggered
one's emotional state to flip at a certain time. We propose a masked memory
network to address the former and a Transformer-based network for the latter
task. To this end, we consider MELD, a benchmark emotion recognition dataset in
multi-party conversations for the task of ERC, and augment it with new
ground-truth labels for EFR. An extensive comparison with five state-of-the-art
models suggests improved performances of our models for both tasks. We further
present anecdotal evidence and both qualitative and quantitative error analyses
to support the superiority of our models compared to the baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full-Sentence Models Perform Better in Simultaneous Translation Using the Information Enhanced Decoding Strategy. (arXiv:2105.01893v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01893">
<div class="article-summary-box-inner">
<span><p>Simultaneous translation, which starts translating each sentence after
receiving only a few words in source sentence, has a vital role in many
scenarios. Although the previous prefix-to-prefix framework is considered
suitable for simultaneous translation and achieves good performance, it still
has two inevitable drawbacks: the high computational resource costs caused by
the need to train a separate model for each latency $k$ and the insufficient
ability to encode information because each target token can only attend to a
specific source prefix. We propose a novel framework that adopts a simple but
effective decoding strategy which is designed for full-sentence models. Within
this framework, training a single full-sentence model can achieve arbitrary
given latency and save computational resources. Besides, with the competence of
the full-sentence model to encode the whole sentence, our decoding strategy can
enhance the information maintained in the decoded states in real time.
Experimental results show that our method achieves better translation quality
than baselines on 4 directions: Zh$\rightarrow$En, En$\rightarrow$Ro and
En$\leftrightarrow$De.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing an Automatic Agent for Repeated Language based Persuasion Games. (arXiv:2105.04976v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04976">
<div class="article-summary-box-inner">
<span><p>Persuasion games are fundamental in economics and AI research and serve as
the basis for important applications. However, work on this setup assumes
communication with stylized messages that do not consist of rich human
language. In this paper we consider a repeated sender (expert) -- receiver
(decision maker) game, where the sender is fully informed about the state of
the world and aims to persuade the receiver to accept a deal by sending one of
several possible natural language reviews. We design an automatic expert that
plays this repeated game, aiming to achieve the maximal payoff. Our expert is
implemented within the Monte Carlo Tree Search (MCTS) algorithm, with deep
learning models that exploit behavioral and linguistic signals in order to
predict the next action of the decision maker, and the future payoff of the
expert given the state of the game and a candidate review. We demonstrate the
superiority of our expert over strong baselines, its adaptability to different
decision makers, and that its selected reviews are nicely adapted to the
proposed deal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition. (arXiv:2106.05642v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05642">
<div class="article-summary-box-inner">
<span><p>The unified streaming and non-streaming two-pass (U2) end-to-end model for
speech recognition has shown great performance in terms of streaming
capability, accuracy, real-time factor (RTF), and latency. In this paper, we
present U2++, an enhanced version of U2 to further improve the accuracy. The
core idea of U2++ is to use the forward and the backward information of the
labeling sequences at the same time at training to learn richer information,
and combine the forward and backward prediction at decoding to give more
accurate recognition results. We also proposed a new data augmentation method
called SpecSub to help the U2++ model to be more accurate and robust. Our
experiments show that, compared with U2, U2++ shows faster convergence at
training, better robustness to the decoding method, as well as consistent 5\% -
8\% word error rate reduction gain over U2. On the experiment of AISHELL-1, we
achieve a 4.63\% character error rate (CER) with a non-streaming setup and
5.05\% with a streaming setup with 320ms latency by U2++. To the best of our
knowledge, 5.05\% is the best-published streaming result on the AISHELL-1 test
set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Training and Decoding For End-to-end Speech Recognition Using Lattice-free MMI. (arXiv:2112.02498v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02498">
<div class="article-summary-box-inner">
<span><p>Recently, End-to-End (E2E) frameworks have achieved remarkable results on
various Automatic Speech Recognition (ASR) tasks. However, Lattice-Free Maximum
Mutual Information (LF-MMI), as one of the discriminative training criteria
that show superior performance in hybrid ASR systems, is rarely adopted in E2E
ASR frameworks. In this work, we propose a novel approach to integrate LF-MMI
criterion into E2E ASR frameworks in both training and decoding stages. The
proposed approach shows its effectiveness on two of the most widely used E2E
frameworks including Attention-Based Encoder-Decoders (AEDs) and Neural
Transducers (NTs). Experiments suggest that the introduction of the LF-MMI
criterion consistently leads to significant performance improvements on various
datasets and different E2E ASR frameworks. The best of our models achieves
competitive CER of 4.1\% / 4.4\% on Aishell-1 dev/test set; we also achieve
significant error reduction on Aishell-2 and Librispeech datasets over strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diformer: Directional Transformer for Neural Machine Translation. (arXiv:2112.11632v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11632">
<div class="article-summary-box-inner">
<span><p>Autoregressive (AR) and Non-autoregressive (NAR) models have their own
superiority on the performance and latency, combining them into one model may
take advantage of both. Current combination frameworks focus more on the
integration of multiple decoding paradigms with a unified generative model,
e.g. Masked Language Model. However, the generalization can be harmful to the
performance due to the gap between training objective and inference. In this
paper, we aim to close the gap by preserving the original objective of AR and
NAR under a unified framework. Specifically, we propose the Directional
Transformer (Diformer) by jointly modelling AR and NAR into three generation
directions (left-to-right, right-to-left and straight) with a newly introduced
direction variable, which works by controlling the prediction of each token to
have specific dependencies under that direction. The unification achieved by
direction successfully preserves the original dependency assumption used in AR
and NAR, retaining both generalization and performance. Experiments on 4 WMT
benchmarks demonstrate that Diformer outperforms current united-modelling works
with more than 1.5 BLEU points for both AR and NAR decoding, and is also
competitive to the state-of-the-art independent AR and NAR models.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Reconstruction from a Single Motion Blurred Image using Learned Dynamic Phase Coding. (arXiv:2112.14768v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14768">
<div class="article-summary-box-inner">
<span><p>Video reconstruction from a single motion-blurred image is a challenging
problem, which can enhance existing cameras' capabilities. Recently, several
works addressed this task using conventional imaging and deep learning. Yet,
such purely-digital methods are inherently limited, due to direction ambiguity
and noise sensitivity. Some works proposed to address these limitations using
non-conventional image sensors, however, such sensors are extremely rare and
expensive. To circumvent these limitations with simpler means, we propose a
hybrid optical-digital method for video reconstruction that requires only
simple modifications to existing optical systems. We use a learned dynamic
phase-coding in the lens aperture during the image acquisition to encode the
motion trajectories, which serve as prior information for the video
reconstruction process. The proposed computational camera generates a sharp
frame burst of the scene at various frame rates from a single coded
motion-blurred image, using an image-to-video convolutional neural network. We
present advantages and improved performance compared to existing methods, using
both simulations and a real-world camera prototype.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Graph Clustering via Dual Correlation Reduction. (arXiv:2112.14772v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14772">
<div class="article-summary-box-inner">
<span><p>Deep graph clustering, which aims to reveal the underlying graph structure
and divide the nodes into different groups, has attracted intensive attention
in recent years. However, we observe that, in the process of node encoding,
existing methods suffer from representation collapse which tends to map all
data into the same representation. Consequently, the discriminative capability
of the node representation is limited, leading to unsatisfied clustering
performance. To address this issue, we propose a novel self-supervised deep
graph clustering method termed Dual Correlation Reduction Network (DCRN) by
reducing information correlation in a dual manner. Specifically, in our method,
we first design a siamese network to encode samples. Then by forcing the
cross-view sample correlation matrix and cross-view feature correlation matrix
to approximate two identity matrices, respectively, we reduce the information
correlation in the dual-level, thus improving the discriminative capability of
the resulting features. Moreover, in order to alleviate representation collapse
caused by over-smoothing in GCN, we introduce a propagation regularization term
to enable the network to gain long-distance information with the shallow
network structure. Extensive experimental results on six benchmark datasets
demonstrate the effectiveness of the proposed DCRN against the existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning meets Liveness Detection: Recent Advancements and Challenges. (arXiv:2112.14796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14796">
<div class="article-summary-box-inner">
<span><p>Facial biometrics has been recently received tremendous attention as a
convenient replacement for traditional authentication systems. Consequently,
detecting malicious attempts has found great significance, leading to extensive
studies in face anti-spoofing~(FAS),i.e., face presentation attack detection.
Deep feature learning and techniques, as opposed to hand-crafted features, have
promised a dramatic increase in the FAS systems' accuracy, tackling the key
challenges of materializing the real-world application of such systems. Hence,
a new research area dealing with the development of more generalized as well as
accurate models is increasingly attracting the attention of the research
community and industry. In this paper, we present a comprehensive survey on the
literature related to deep-feature-based FAS methods since 2017. To shed light
on this topic, a semantic taxonomy based on various features and learning
methodologies is represented. Further, we cover predominant public datasets for
FAS in chronological order, their evolutional progress, and the evaluation
criteria (both intra-dataset and inter-dataset). Finally, we discuss the open
research challenges and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Inception Attention for Image Synthesis and Image Recognition. (arXiv:2112.14804v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14804">
<div class="article-summary-box-inner">
<span><p>Image synthesis and image recognition have witnessed remarkable progress, but
often at the expense of computationally expensive training and inference.
Learning lightweight yet expressive deep model has emerged as an important and
interesting direction. Inspired by the well-known split-transform-aggregate
design heuristic in the Inception building block, this paper proposes a
Skip-Layer Inception Module (SLIM) that facilitates efficient learning of image
synthesis models, and a same-layer variant (dubbed as SLIM too) as a stronger
alternative to the well-known ResNeXts for image recognition. In SLIM, the
input feature map is first split into a number of groups (e.g., 4).Each group
is then transformed to a latent style vector(via channel-wise attention) and a
latent spatial mask (via spatial attention). The learned latent masks and
latent style vectors are aggregated to modulate the target feature map. For
generative learning, SLIM is built on a recently proposed lightweight
Generative Adversarial Networks (i.e., FastGANs) which present a skip-layer
excitation(SLE) module. For few-shot image synthesis tasks, the proposed SLIM
achieves better performance than the SLE work and other related methods. For
one-shot image synthesis tasks, it shows stronger capability of preserving
images structures than prior arts such as the SinGANs. For image classification
tasks, the proposed SLIM is used as a drop-in replacement for convolution
layers in ResNets (resulting in ResNeXt-like models) and achieves better
accuracy in theImageNet-1000 dataset, with significantly smaller model
complexity
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Backdoor Defense Using Shapley Estimation. (arXiv:2112.14889v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14889">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have achieved impressive performance in a variety of
tasks over the last decade, such as autonomous driving, face recognition, and
medical diagnosis. However, prior works show that deep neural networks are
easily manipulated into specific, attacker-decided behaviors in the inference
stage by backdoor attacks which inject malicious small hidden triggers into
model training, raising serious security threats. To determine the triggered
neurons and protect against backdoor attacks, we exploit Shapley value and
develop a new approach called Shapley Pruning (ShapPruning) that successfully
mitigates backdoor attacks from models in a data-insufficient situation (1
image per class or even free of data). Considering the interaction between
neurons, ShapPruning identifies the few infected neurons (under 1% of all
neurons) and manages to protect the model's structure and accuracy after
pruning as many infected neurons as possible. To accelerate ShapPruning, we
further propose discarding threshold and $\epsilon$-greedy strategy to
accelerate Shapley estimation, making it possible to repair poisoned models
with only several minutes. Experiments demonstrate the effectiveness and
robustness of our method against various attacks and tasks compared to existing
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Generation and Hypothesis Verification for Reliable Face Anti-Spoofing. (arXiv:2112.14894v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14894">
<div class="article-summary-box-inner">
<span><p>Although existing face anti-spoofing (FAS) methods achieve high accuracy in
intra-domain experiments, their effects drop severely in cross-domain scenarios
because of poor generalization. Recently, multifarious techniques have been
explored, such as domain generalization and representation disentanglement.
However, the improvement is still limited by two issues: 1) It is difficult to
perfectly map all faces to a shared feature space. If faces from unknown
domains are not mapped to the known region in the shared feature space,
accidentally inaccurate predictions will be obtained. 2) It is hard to
completely consider various spoof traces for disentanglement. In this paper, we
propose a Feature Generation and Hypothesis Verification framework to alleviate
the two issues. Above all, feature generation networks which generate
hypotheses of real faces and known attacks are introduced for the first time in
the FAS task. Subsequently, two hypothesis verification modules are applied to
judge whether the input face comes from the real-face space and the real-face
distribution respectively. Furthermore, some analyses of the relationship
between our framework and Bayesian uncertainty estimation are given, which
provides theoretical support for reliable defense in unknown domains.
Experimental results show our framework achieves promising results and
outperforms the state-of-the-art approaches on extensive public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieving Black-box Optimal Images from External Databases. (arXiv:2112.14921v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14921">
<div class="article-summary-box-inner">
<span><p>Suppose we have a black-box function (e.g., deep neural network) that takes
an image as input and outputs a value that indicates preference. How can we
retrieve optimal images with respect to this function from an external database
on the Internet? Standard retrieval problems in the literature (e.g., item
recommendations) assume that an algorithm has full access to the set of items.
In other words, such algorithms are designed for service providers. In this
paper, we consider the retrieval problem under different assumptions.
Specifically, we consider how users with limited access to an image database
can retrieve images using their own black-box functions. This formulation
enables a flexible and finer-grained image search defined by each user. We
assume the user can access the database through a search query with tight API
limits. Therefore, a user needs to efficiently retrieve optimal images in terms
of the number of queries. We propose an efficient retrieval algorithm Tiara for
this problem. In the experiments, we confirm that our proposed method performs
better than several baselines under various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth. (arXiv:2112.14931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14931">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a dense depth estimation pipeline for multiview
360\degree\: images. The proposed pipeline leverages a spherical camera model
that compensates for radial distortion in 360\degree\: images. The key
contribution of this paper is the extension of a spherical camera model to
multiview by introducing a translation scaling scheme. Moreover, we propose an
effective dense depth estimation method by setting virtual depth and minimizing
photonic reprojection error. We validate the performance of the proposed
pipeline using the images of natural scenes as well as the synthesized dataset
for quantitive evaluation. The experimental results verify that the proposed
pipeline improves estimation accuracy compared to the current state-of-art
dense depth estimation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SFU-HW-Tracks-v1: Object Tracking Dataset on Raw Video Sequences. (arXiv:2112.14934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14934">
<div class="article-summary-box-inner">
<span><p>We present a dataset that contains object annotations with unique object
identities (IDs) for the High Efficiency Video Coding (HEVC) v1 Common Test
Conditions (CTC) sequences. Ground-truth annotations for 13 sequences were
prepared and released as the dataset called SFU-HW-Tracks-v1. For each video
frame, ground truth annotations include object class ID, object ID, and
bounding box location and its dimensions. The dataset can be used to evaluate
object tracking performance on uncompressed video sequences and study the
relationship between video compression and object tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Generator with Auxiliary Branch for Improving GAN Performance. (arXiv:2112.14968v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14968">
<div class="article-summary-box-inner">
<span><p>The generator in the generative adversarial network (GAN) learns image
generation in a coarse-to-fine manner in which earlier layers learn an overall
structure of the image and the latter ones refine the details. To propagate the
coarse information well, recent works usually build their generators by
stacking up multiple residual blocks. Although the residual block can produce
the high-quality image as well as be trained stably, it often impedes the
information flow in the network. To alleviate this problem, this brief
introduces a novel generator architecture that produces the image by combining
features obtained through two different branches: the main and auxiliary
branches. The goal of the main branch is to produce the image by passing
through the multiple residual blocks, whereas the auxiliary branch is to convey
the coarse information in the earlier layer to the later one. To combine the
features in the main and auxiliary branches successfully, we also propose a
gated feature fusion module that controls the information flow in those
branches. To prove the superiority of the proposed method, this brief provides
extensive experiments using various standard datasets including CIFAR-10,
CIFAR-100, LSUN, CelebA-HQ, AFHQ, and tiny- ImageNet. Furthermore, we conducted
various ablation studies to demonstrate the generalization ability of the
proposed method. Quantitative evaluations prove that the proposed method
exhibits impressive GAN performance in terms of Inception score (IS) and
Frechet inception distance (FID). For instance, the proposed method boosts the
FID and IS scores on the tiny-ImageNet dataset from 35.13 to 25.00 and 20.23 to
25.57, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Fine-grained Class Clustering via Generative Adversarial Networks. (arXiv:2112.14971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14971">
<div class="article-summary-box-inner">
<span><p>Unsupervised fine-grained class clustering is practical yet challenging task
due to the difficulty of feature representations learning of subtle object
details. We introduce C3-GAN, a method that leverages the categorical inference
power of InfoGAN by applying contrastive learning. We aim to learn feature
representations that encourage the data to form distinct cluster boundaries in
the embedding space, while also maximizing the mutual information between the
latent code and its observation. Our approach is to train the discriminator,
which is used for inferring clusters, to optimize the contrastive loss, where
the image-latent pairs that maximize the mutual information are considered as
positive pairs and the rest as negative pairs. Specifically, we map the input
of the generator, which has sampled from the categorical distribution, to the
embedding space of the discriminator and let them act as a cluster centroid. In
this way, C3-GAN achieved to learn a clustering-friendly embedding space where
each cluster is distinctively separable. Experimental results show that C3-GAN
achieved state-of-the-art clustering performance on four fine-grained benchmark
datasets, while also alleviating the mode collapse phenomenon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14976">
<div class="article-summary-box-inner">
<span><p>Semantic representation is of great benefit to the video text tracking(VTT)
task that requires simultaneously classifying, detecting, and tracking texts in
the video. Most existing approaches tackle this task by appearance similarity
in continuous frames, while ignoring the abundant semantic features. In this
paper, we explore to robustly track video text with contrastive learning of
semantic and visual representations. Correspondingly, we present an end-to-end
video text tracker with Semantic and Visual Representations(SVRep), which
detects and tracks texts by exploiting the visual and semantic relationships
between different texts in a video sequence. Besides, with a light-weight
architecture, SVRep achieves state-of-the-art performance while maintaining
competitive inference speed. Specifically, with a backbone of ResNet-18, SVRep
achieves an ${\rm ID_{F1}}$ of $\textbf{65.9\%}$, running at $\textbf{16.7}$
FPS, on the ICDAR2015(video) dataset with $\textbf{8.6\%}$ improvement than the
previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the pattern of Emotion in children with ASD as an early biomarker through Recurring-Convolution Neural Network (R-CNN). (arXiv:2112.14983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14983">
<div class="article-summary-box-inner">
<span><p>Autism Spectrum Disorder (ASD) is found to be a major concern among various
occupational therapists. The foremost challenge of this neurodevelopmental
disorder lies in the fact of analyzing and exploring various symptoms of the
children at their early stage of development. Such early identification could
prop up the therapists and clinicians to provide proper assistive support to
make the children lead an independent life. Facial expressions and emotions
perceived by the children could contribute to such early intervention of
autism. In this regard, the paper implements in identifying basic facial
expression and exploring their emotions upon a time variant factor. The
emotions are analyzed by incorporating the facial expression identified through
CNN using 68 landmark points plotted on the frontal face with a prediction
network formed by RNN known as RCNN-FER system. The paper adopts R-CNN to take
the advantage of increased accuracy and performance with decreased time
complexity in predicting emotion as a textual network analysis. The papers
proves better accuracy in identifying the emotion in autistic children when
compared over simple machine learning models built for such identifications
contributing to autistic society.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">THE Benchmark: Transferable Representation Learning for Monocular Height Estimation. (arXiv:2112.14985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14985">
<div class="article-summary-box-inner">
<span><p>Generating 3D city models rapidly is crucial for many applications. Monocular
height estimation is one of the most efficient and timely ways to obtain
large-scale geometric information. However, existing works focus primarily on
training and testing models using unbiased datasets, which don't align well
with real-world applications. Therefore, we propose a new benchmark dataset to
study the transferability of height estimation models in a cross-dataset
setting. To this end, we first design and construct a large-scale benchmark
dataset for cross-dataset transfer learning on the height estimation task. This
benchmark dataset includes a newly proposed large-scale synthetic dataset, a
newly collected real-world dataset, and four existing datasets from different
cities. Next, two new experimental protocols, zero-shot and few-shot
cross-dataset transfer, are designed. For few-shot cross-dataset transfer, we
enhance the window-based Transformer with the proposed scale-deformable
convolution module to handle the severe scale-variation problem. To improve the
generalizability of deep models in the zero-shot cross-dataset setting, a
max-normalization-based Transformer network is designed to decouple the
relative height map from the absolute heights. Experimental results have
demonstrated the effectiveness of the proposed methods in both the traditional
and cross-dataset transfer settings. The datasets and codes are publicly
available at https://thebenchmarkh.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Matters: Radiology Report Generation with General and Specific Knowledge. (arXiv:2112.15009v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15009">
<div class="article-summary-box-inner">
<span><p>Automatic radiology report generation is critical in clinics which can
relieve experienced radiologists from the heavy workload and remind
inexperienced radiologists of misdiagnosis or missed diagnose. Existing
approaches mainly formulate radiology report generation as an image captioning
task and adopt the encoder-decoder framework. However, in the medical domain,
such pure data-driven approaches suffer from the following problems: 1) visual
and textual bias problem; 2) lack of expert knowledge. In this paper, we
propose a knowledge-enhanced radiology report generation approach introduces
two types of medical knowledge: 1) General knowledge, which is input
independent and provides the broad knowledge for report generation; 2) Specific
knowledge, which is input dependent and provides the fine-grained knowledge for
report generation. To fully utilize both the general and specific knowledge, we
also propose a knowledge-enhanced multi-head attention mechanism. By merging
the visual features of the radiology image with general knowledge and specific
knowledge, the proposed model can improve the quality of generated reports.
Experimental results on two publicly available datasets IU-Xray and MIMIC-CXR
show that the proposed knowledge enhanced approach outperforms state-of-the-art
image captioning based methods. Ablation studies also demonstrate that both
general and specific knowledge can help to improve the performance of radiology
report generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiology Report Generation with a Learned Knowledge Base and Multi-modal Alignment. (arXiv:2112.15011v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15011">
<div class="article-summary-box-inner">
<span><p>In clinics, a radiology report is crucial for guiding a patient's treatment.
Unfortunately, report writing imposes a heavy burden on radiologists. To
effectively reduce such a burden, we hereby present an automatic, multi-modal
approach for report generation from chest x-ray. Our approach, motivated by the
observation that the descriptions in radiology reports are highly correlated
with the x-ray images, features two distinct modules: (i) Learned knowledge
base. To absorb the knowledge embedded in the above-mentioned correlation, we
automatically build a knowledge base based on textual embedding. (ii)
Multi-modal alignment. To promote the semantic alignment among reports, disease
labels and images, we explicitly utilize textual embedding to guide the
learning of the visual feature space. We evaluate the performance of the
proposed model using metrics from both natural language generation and clinic
efficacy on the public IU and MIMIC-CXR datasets. Our ablation study shows that
each module contributes to improving the quality of generated reports.
Furthermore, with the aid of both modules, our approach clearly outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Pose Representations and Motion Contexts Modeling for 3D Motion Prediction. (arXiv:2112.15012v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15012">
<div class="article-summary-box-inner">
<span><p>Predicting human motion from historical pose sequence is crucial for a
machine to succeed in intelligent interactions with humans. One aspect that has
been obviated so far, is the fact that how we represent the skeletal pose has a
critical impact on the prediction results. Yet there is no effort that
investigates across different pose representation schemes. We conduct an
indepth study on various pose representations with a focus on their effects on
the motion prediction task. Moreover, recent approaches build upon
off-the-shelf RNN units for motion prediction. These approaches process input
pose sequence sequentially and inherently have difficulties in capturing
long-term dependencies. In this paper, we propose a novel RNN architecture
termed AHMR (Attentive Hierarchical Motion Recurrent network) for motion
prediction which simultaneously models local motion contexts and a global
context. We further explore a geodesic loss and a forward kinematics loss for
the motion prediction task, which have more geometric significance than the
widely employed L2 loss. Interestingly, we applied our method to a range of
articulate objects including human, fish, and mouse. Empirical results show
that our approach outperforms the state-of-the-art methods in short-term
prediction and achieves much enhanced long-term prediction proficiency, such as
retaining natural human-like motions over 50 seconds predictions. Our codes are
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continually Learning Self-Supervised Representations with Projected Functional Regularization. (arXiv:2112.15022v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15022">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised learning methods are able to learn high-quality image
representations and are closing the gap with supervised methods. However, these
methods are unable to acquire new knowledge incrementally -- they are, in fact,
mostly used only as a pre-training phase with IID data. In this work we
investigate self-supervised methods in continual learning regimes without
additional memory or replay. To prevent forgetting of previous knowledge, we
propose the usage of functional regularization. We will show that naive
functional regularization, also known as feature distillation, leads to low
plasticity and therefore seriously limits continual learning performance. To
address this problem, we propose Projected Functional Regularization where a
separate projection network ensures that the newly learned feature space
preserves information of the previous feature space, while allowing for the
learning of new features. This allows us to prevent forgetting while
maintaining the plasticity of the learner. Evaluation against other incremental
learning approaches applied to self-supervision demonstrates that our method
obtains competitive performance in different scenarios and on multiple
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of a face mask detection pipeline for mask-wearing monitoring in the era of the COVID-19 pandemic: A modular approach. (arXiv:2112.15031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15031">
<div class="article-summary-box-inner">
<span><p>During the SARS-Cov-2 pandemic, mask-wearing became an effective tool to
prevent spreading and contracting the virus. The ability to monitor the
mask-wearing rate in the population would be useful for determining public
health strategies against the virus. However, artificial intelligence
technologies for detecting face masks have not been deployed at a large scale
in real-life to measure the mask-wearing rate in public. In this paper, we
present a two-step face mask detection approach consisting of two separate
modules: 1) face detection and alignment and 2) face mask classification. This
approach allowed us to experiment with different combinations of face detection
and face mask classification modules. More specifically, we experimented with
PyramidKey and RetinaFace as face detectors while maintaining a lightweight
backbone for the face mask classification module. Moreover, we also provide a
relabeled annotation of the test set of the AIZOO dataset, where we rectified
the incorrect labels for some face images. The evaluation results on the AIZOO
and Moxa 3K datasets showed that the proposed face mask detection pipeline
surpassed the state-of-the-art methods. The proposed pipeline also yielded a
higher mAP on the relabeled test set of the AIZOO dataset than the original
test set. Since we trained the proposed model using in-the-wild face images, we
can successfully deploy our model to monitor the mask-wearing rate using public
CCTV images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digital Rock Typing DRT Algorithm Formulation with Optimal Supervised Semantic Segmentation. (arXiv:2112.15068v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15068">
<div class="article-summary-box-inner">
<span><p>Each grid block in a 3D geological model requires a rock type that represents
all physical and chemical properties of that block. The properties that
classify rock types are lithology, permeability, and capillary pressure.
Scientists and engineers determined these properties using conventional
laboratory measurements, which embedded destructive methods to the sample or
altered some of its properties (i.e., wettability, permeability, and porosity)
because the measurements process includes sample crushing, fluid flow, or fluid
saturation. Lately, Digital Rock Physics (DRT) has emerged to quantify these
properties from micro-Computerized Tomography (uCT) and Magnetic Resonance
Imaging (MRI) images. However, the literature did not attempt rock typing in a
wholly digital context. We propose performing Digital Rock Typing (DRT) by: (1)
integrating the latest DRP advances in a novel process that honors digital rock
properties determination, while; (2) digitalizing the latest rock typing
approaches in carbonate, and (3) introducing a novel carbonate rock typing
process that utilizes computer vision capabilities to provide more insight
about the heterogeneous carbonate rock texture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose Estimation of Specific Rigid Objects. (arXiv:2112.15075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15075">
<div class="article-summary-box-inner">
<span><p>In this thesis, we address the problem of estimating the 6D pose of rigid
objects from a single RGB or RGB-D input image, assuming that 3D models of the
objects are available. This problem is of great importance to many application
fields such as robotic manipulation, augmented reality, and autonomous driving.
First, we propose EPOS, a method for 6D object pose estimation from an RGB
image. The key idea is to represent an object by compact surface fragments and
predict the probability distribution of corresponding fragments at each pixel
of the input image by a neural network. Each pixel is linked with a
data-dependent number of fragments, which allows systematic handling of
symmetries, and the 6D poses are estimated from the links by a RANSAC-based
fitting method. EPOS outperformed all RGB and most RGB-D and D methods on
several standard datasets. Second, we present HashMatch, an RGB-D method that
slides a window over the input image and searches for a match against
templates, which are pre-generated by rendering 3D object models in different
orientations. The method applies a cascade of evaluation stages to each window
location, which avoids exhaustive matching against all templates. Third, we
propose ObjectSynth, an approach to synthesize photorealistic images of 3D
object models for training methods based on neural networks. The images yield
substantial improvements compared to commonly used images of objects rendered
on top of random photographs. Fourth, we introduce T-LESS, the first dataset
for 6D object pose estimation that includes 3D models and RGB-D images of
industry-relevant objects. Fifth, we define BOP, a benchmark that captures the
status quo in the field. BOP comprises eleven datasets in a unified format, an
evaluation methodology, an online evaluation system, and public challenges held
at international workshops organized at the ICCV and ECCV conferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Extraction and Prediction for Hand Hygiene Gestures with KNN Algorithm. (arXiv:2112.15085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15085">
<div class="article-summary-box-inner">
<span><p>This work focuses upon the analysis of hand gestures involved in the process
of hand washing. There are six standard hand hygiene gestures for washing hands
as provided by World Health Organisation hand hygiene guidelines. In this
paper, hand features such as contours of hands, the centroid of the hands, and
extreme hand points along the largest contour are extracted with the use of the
computer vision library, OpenCV. These hand features are extracted for each
data frame in a hand hygiene video. A robust hand hygiene dataset of video
recordings was created in the project. A subset of this dataset is used in this
work. Extracted hand features are further grouped into classes based on the KNN
algorithm with a cross-fold validation technique for the classification and
prediction of the unlabelled data. A mean accuracy score of &gt;95% is achieved
and proves that the KNN algorithm with an appropriate input value of K=5 is
efficient for classification. A complete dataset with six distinct hand hygiene
classes will be used with the KNN classifier for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging in-domain supervision for unsupervised image-to-image translation tasks via multi-stream generators. (arXiv:2112.15091v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15091">
<div class="article-summary-box-inner">
<span><p>Supervision for image-to-image translation (I2I) tasks is hard to come by,
but bears significant effect on the resulting quality. In this paper, we
observe that for many Unsupervised I2I (UI2I) scenarios, one domain is more
familiar than the other, and offers in-domain prior knowledge, such as semantic
segmentation. We argue that for complex scenes, figuring out the semantic
structure of the domain is hard, especially with no supervision, but is an
important part of a successful I2I operation. We hence introduce two techniques
to incorporate this invaluable in-domain prior knowledge for the benefit of
translation quality: through a novel Multi-Stream generator architecture, and
through a semantic segmentation-based regularization loss term. In essence, we
propose splitting the input data according to semantic masks, explicitly
guiding the network to different behavior for the different regions of the
image. In addition, we propose training a semantic segmentation network along
with the translation task, and to leverage this output as a loss term that
improves robustness. We validate our approach on urban data, demonstrating
superior quality in the challenging UI2I tasks of converting day images to
night ones. In addition, we also demonstrate how reinforcing the target dataset
with our augmented images improves the training of downstream tasks such as the
classical detection one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study. (arXiv:2112.15093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15093">
<div class="article-summary-box-inner">
<span><p>The flourishing blossom of deep learning has witnessed the rapid development
of text recognition in recent years. However, the existing text recognition
methods are mainly for English texts, whereas ignoring the pivotal role of
Chinese texts. As another widely-spoken language, Chinese text recognition in
all ways has extensive application markets. Based on our observations, we
attribute the scarce attention on Chinese text recognition to the lack of
reasonable dataset construction standards, unified evaluation methods, and
results of the existing baselines. To fill this gap, we manually collect
Chinese text datasets from publicly available competitions, projects, and
papers, then divide them into four categories including scene, web, document,
and handwriting datasets. Furthermore, we evaluate a series of representative
text recognition methods on these datasets with unified evaluation methods to
provide experimental results. By analyzing the experimental results, we
surprisingly observe that state-of-the-art baselines for recognizing English
texts cannot perform well on Chinese scenarios. We consider that there still
remain numerous challenges under exploration due to the characteristics of
Chinese texts, which are quite different from English texts. The code and
datasets are made publicly available at
https://github.com/FudanVI/benchmarking-chinese-text-recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A general technique for the estimation of farm animal body part weights from CT scans and its applications in a rabbit breeding program. (arXiv:2112.15095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15095">
<div class="article-summary-box-inner">
<span><p>Various applications of farm animal imaging are based on the estimation of
weights of certain body parts and cuts from the CT images of animals. In many
cases, the complexity of the problem is increased by the enormous variability
of postures in CT images due to the scanning of non-sedated, living animals. In
this paper, we propose a general and robust approach for the estimation of the
weights of cuts and body parts from the CT images of (possibly) living animals.
We adapt multi-atlas based segmentation driven by elastic registration and
joint feature and model selection for the regression component to cape with the
large number of features and low number of samples. The proposed technique is
evaluated and illustrated through real applications in rabbit breeding
programs, showing r^2 scores 12% higher than previous techniques and methods
that used to drive the selection so far. The proposed technique is easily
adaptable to similar problems, consequently, it is shared in an open source
software package for the benefit of the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colour alignment for relative colour constancy via non-standard references. (arXiv:2112.15106v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15106">
<div class="article-summary-box-inner">
<span><p>Relative colour constancy is an essential requirement for many scientific
imaging applications. However, most digital cameras differ in their image
formations and native sensor output is usually inaccessible, e.g., in
smartphone camera applications. This makes it hard to achieve consistent colour
assessment across a range of devices, and that undermines the performance of
computer vision algorithms. To resolve this issue, we propose a colour
alignment model that considers the camera image formation as a black-box and
formulates colour alignment as a three-step process: camera response
calibration, response linearisation, and colour matching. The proposed model
works with non-standard colour references, i.e., colour patches without knowing
the true colour values, by utilising a novel balance-of-linear-distances
feature. It is equivalent to determining the camera parameters through an
unsupervised process. It also works with a minimum number of corresponding
colour patches across the images to be colour aligned to deliver the applicable
processing. Two challenging image datasets collected by multiple cameras under
various illumination and exposure conditions were used to evaluate the model.
Performance benchmarks demonstrated that our model achieved superior
performance compared to other popular and state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Layers in Vision Transformers. (arXiv:2112.15111v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15111">
<div class="article-summary-box-inner">
<span><p>We introduce fully stochastic layers in vision transformers, without causing
any severe drop in performance. The additional stochasticity boosts the
robustness of visual features and strengthens privacy. In this process, linear
layers with fully stochastic parameters are used, both during training and
inference, to transform the feature activations of each multilayer perceptron.
Such stochastic linear operations preserve the topological structure, formed by
the set of tokens passing through the shared multilayer perceptron. This
operation encourages the learning of the recognition task to rely on the
topological structures of the tokens, instead of their values, which in turn
offers the desired robustness and privacy of the visual features. In this
paper, we use our features for three different applications, namely,
adversarial robustness, network calibration, and feature privacy. Our features
offer exciting results on those tasks. Furthermore, we showcase an experimental
setup for federated and transfer learning, where the vision transformers with
stochastic layers are again shown to be well behaved. Our source code will be
made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15139">
<div class="article-summary-box-inner">
<span><p>Quantized neural networks typically require smaller memory footprints and
lower computation complexity, which is crucial for efficient deployment.
However, quantization inevitably leads to a distribution divergence from the
original network, which generally degrades the performance. To tackle this
issue, massive efforts have been made, but most existing approaches lack
statistical considerations and depend on several manual configurations. In this
paper, we present an adaptive-mapping quantization method to learn an optimal
latent sub-distribution that is inherent within models and smoothly
approximated with a concrete Gaussian Mixture (GM). In particular, the network
weights are projected in compliance with the GM-approximated sub-distribution.
This sub-distribution evolves along with the weight update in a co-tuning
schema guided by the direct task-objective optimization. Sufficient experiments
on image classification and object detection over various modern architectures
demonstrate the effectiveness, generalization property, and transferability of
the proposed method. Besides, an efficient deployment flow for the mobile CPU
is developed, achieving up to 7.46$\times$ inference acceleration on an
octa-core ARM CPU. Codes are publicly released at
https://github.com/RunpeiDong/DGMS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Resolution Enhancement Plug-in for Deformable Registration of Medical Images. (arXiv:2112.15180v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15180">
<div class="article-summary-box-inner">
<span><p>Image registration is a fundamental task for medical imaging. Resampling of
the intensity values is required during registration and better spatial
resolution with finer and sharper structures can improve the resampling
performance and hence the registration accuracy. Super-resolution (SR) is an
algorithmic technique targeting at spatial resolution enhancement which can
achieve an image resolution beyond the hardware limitation. In this work, we
consider SR as a preprocessing technique and present a CNN-based resolution
enhancement module (REM) which can be easily plugged into the registration
network in a cascaded manner. Different residual schemes and network
configurations of REM are investigated to obtain an effective architecture
design of REM. In fact, REM is not confined to image registration, it can also
be straightforwardly integrated into other vision tasks for enhanced
resolution. The proposed REM is thoroughly evaluated for deformable
registration on medical images quantitatively and qualitatively at different
upscaling factors. Experiments on LPBA40 brain MRI dataset demonstrate that REM
not only improves the registration accuracy, especially when the input images
suffer from degraded spatial resolution, but also generates resolution enhanced
images which can be exploited for successive diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robustness of Neural Networks. (arXiv:2112.15188v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15188">
<div class="article-summary-box-inner">
<span><p>We introduce several new datasets namely ImageNet-A/O and ImageNet-R as well
as a synthetic environment and testing suite we called CAOS. ImageNet-A/O allow
researchers to focus in on the blind spots remaining in ImageNet. ImageNet-R
was specifically created with the intention of tracking robust representation
as the representations are no longer simply natural but include artistic, and
other renditions. The CAOS suite is built off of CARLA simulator which allows
for the inclusion of anomalous objects and can create reproducible synthetic
environment and scenes for testing robustness. All of the datasets were created
for testing robustness and measuring progress in robustness. The datasets have
been used in various other works to measure their own progress in robustness
and allowing for tangential progress that does not focus exclusively on natural
accuracy.
</p>
<p>Given these datasets, we created several novel methods that aim to advance
robustness research. We build off of simple baselines in the form of Maximum
Logit, and Typicality Score as well as create a novel data augmentation method
in the form of DeepAugment that improves on the aforementioned benchmarks.
Maximum Logit considers the logit values instead of the values after the
softmax operation, while a small change produces noticeable improvements. The
Typicality Score compares the output distribution to a posterior distribution
over classes. We show that this improves performance over the baseline in all
but the segmentation task. Speculating that perhaps at the pixel level the
semantic information of a pixel is less meaningful than that of class level
information. Finally the new augmentation technique of DeepAugment utilizes
neural networks to create augmentations on images that are radically different
than the traditional geometric and camera based transformations used
previously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual and Object Geo-localization: A Comprehensive Survey. (arXiv:2112.15202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15202">
<div class="article-summary-box-inner">
<span><p>The concept of geo-localization refers to the process of determining where on
earth some `entity' is located, typically using Global Positioning System (GPS)
coordinates. The entity of interest may be an image, sequence of images, a
video, satellite image, or even objects visible within the image. As massive
datasets of GPS tagged media have rapidly become available due to smartphones
and the internet, and deep learning has risen to enhance the performance
capabilities of machine learning models, the fields of visual and object
geo-localization have emerged due to its significant impact on a wide range of
applications such as augmented reality, robotics, self-driving vehicles, road
maintenance, and 3D reconstruction. This paper provides a comprehensive survey
of geo-localization involving images, which involves either determining from
where an image has been captured (Image geo-localization) or geo-locating
objects within an image (Object geo-localization). We will provide an in-depth
study, including a summary of popular algorithms, a description of proposed
datasets, and an analysis of performance results to illustrate the current
state of each field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Free Knowledge Transfer: A Survey. (arXiv:2112.15278v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15278">
<div class="article-summary-box-inner">
<span><p>In the last decade, many deep learning models have been well trained and made
a great success in various fields of machine intelligence, especially for
computer vision and natural language processing. To better leverage the
potential of these well-trained models in intra-domain or cross-domain transfer
learning situations, knowledge distillation (KD) and domain adaptation (DA) are
proposed and become research highlights. They both aim to transfer useful
information from a well-trained model with original training data. However, the
original data is not always available in many cases due to privacy, copyright
or confidentiality. Recently, the data-free knowledge transfer paradigm has
attracted appealing attention as it deals with distilling valuable knowledge
from well-trained models without requiring to access to the training data. In
particular, it mainly consists of the data-free knowledge distillation (DFKD)
and source data-free domain adaptation (SFDA). On the one hand, DFKD aims to
transfer the intra-domain knowledge of original data from a cumbersome teacher
network to a compact student network for model compression and efficient
inference. On the other hand, the goal of SFDA is to reuse the cross-domain
knowledge stored in a well-trained source model and adapt it to a target
domain. In this paper, we provide a comprehensive survey on data-free knowledge
transfer from the perspectives of knowledge distillation and unsupervised
domain adaptation, to help readers have a better understanding of the current
research status and ideas. Applications and challenges of the two areas are
briefly reviewed, respectively. Furthermore, we provide some insights to the
subject of future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation. (arXiv:2112.15283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15283">
<div class="article-summary-box-inner">
<span><p>Conventional methods for the image-text generation tasks mainly tackle the
naturally bidirectional generation tasks separately, focusing on designing
task-specific frameworks to improve the quality and fidelity of the generated
samples. Recently, Vision-Language Pre-training models have greatly improved
the performance of the image-to-text generation tasks, but large-scale
pre-training models for text-to-image synthesis task are still under-developed.
In this paper, we propose ERNIE-ViLG, a unified generative pre-training
framework for bidirectional image-text generation with transformer model. Based
on the image quantization models, we formulate both image generation and text
generation as autoregressive generative tasks conditioned on the text/image
input. The bidirectional image-text generative modeling eases the semantic
alignments across vision and language. For the text-to-image generation
process, we further propose an end-to-end training method to jointly learn the
visual sequence generator and the image reconstructor. To explore the landscape
of large-scale pre-training for bidirectional text-image generation, we train a
10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million
(Chinese) image-text pairs which achieves state-of-the-art performance for both
text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for
text-to-image synthesis and best results on COCO-CN and AIC-ICC for image
captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSformer: Bridging Convolution and Transformer for Compressive Sensing. (arXiv:2112.15299v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15299">
<div class="article-summary-box-inner">
<span><p>Convolution neural networks (CNNs) have succeeded in compressive image
sensing. However, due to the inductive bias of locality and weight sharing, the
convolution operations demonstrate the intrinsic limitations in modeling the
long-range dependency. Transformer, designed initially as a
sequence-to-sequence model, excels at capturing global contexts due to the
self-attention-based architectures even though it may be equipped with limited
localization abilities. This paper proposes CSformer, a hybrid framework that
integrates the advantages of leveraging both detailed spatial information from
CNN and the global context provided by transformer for enhanced representation
learning. The proposed approach is an end-to-end compressive image sensing
method, composed of adaptive sampling and recovery. In the sampling module,
images are measured block-by-block by the learned sampling matrix. In the
reconstruction stage, the measurement is projected into dual stems. One is the
CNN stem for modeling the neighborhood relationships by convolution, and the
other is the transformer stem for adopting global self-attention mechanism. The
dual branches structure is concurrent, and the local features and global
representations are fused under different resolutions to maximize the
complementary of features. Furthermore, we explore a progressive strategy and
window-based transformer block to reduce the parameter and computational
complexity. The experimental results demonstrate the effectiveness of the
dedicated transformer-based architecture for compressive sensing, which
achieves superior performance compared to state-of-the-art methods on different
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SplitBrain: Hybrid Data and Model Parallel Deep Learning. (arXiv:2112.15317v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15317">
<div class="article-summary-box-inner">
<span><p>The recent success of deep learning applications has coincided with those
widely available powerful computational resources for training sophisticated
machine learning models with huge datasets. Nonetheless, training large models
such as convolutional neural networks using model parallelism (as opposed to
data parallelism) is challenging because the complex nature of communication
between model shards makes it difficult to partition the computation
efficiently across multiple machines with an acceptable trade-off. This paper
presents SplitBrain, a high performance distributed deep learning framework
supporting hybrid data and model parallelism. Specifically, SplitBrain provides
layer-specific partitioning that co-locates compute intensive convolutional
layers while sharding memory demanding layers. A novel scalable group
communication is proposed to further improve the training throughput with
reduced communication overhead. The results show that SplitBrain can achieve
nearly linear speedup while saving up to 67\% of memory consumption for data
and model parallel VGG over CIFAR-10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InverseMV: Composing Piano Scores with a Convolutional Video-Music Transformer. (arXiv:2112.15320v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15320">
<div class="article-summary-box-inner">
<span><p>Many social media users prefer consuming content in the form of videos rather
than text. However, in order for content creators to produce videos with a high
click-through rate, much editing is needed to match the footage to the music.
This posts additional challenges for more amateur video makers. Therefore, we
propose a novel attention-based model VMT (Video-Music Transformer) that
automatically generates piano scores from video frames. Using music generated
from models also prevent potential copyright infringements that often come with
using existing music. To the best of our knowledge, there is no work besides
the proposed VMT that aims to compose music for video. Additionally, there
lacks a dataset with aligned video and symbolic music. We release a new dataset
composed of over 7 hours of piano scores with fine alignment between pop music
videos and MIDI files. We conduct experiments with human evaluation on VMT,
SeqSeq model (our baseline), and the original piano version soundtrack. VMT
achieves consistent improvements over the baseline on music smoothness and
video relevance. In particular, with the relevance scores and our case study,
our model has shown the capability of multimodality on frame-level actors'
movement for music generation. Our VMT model, along with the new dataset,
presents a promising research direction toward composing the matching
soundtrack for videos. We have released our code at
https://github.com/linchintung/VMT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deconfounded Visual Grounding. (arXiv:2112.15324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15324">
<div class="article-summary-box-inner">
<span><p>We focus on the confounding bias between language and location in the visual
grounding pipeline, where we find that the bias is the major visual reasoning
bottleneck. For example, the grounding process is usually a trivial
language-location association without visual reasoning, e.g., grounding any
language query containing sheep to the nearly central regions, due to that most
queries about sheep have ground-truth locations at the image center. First, we
frame the visual grounding pipeline into a causal graph, which shows the
causalities among image, query, target location and underlying confounder.
Through the causal graph, we know how to break the grounding bottleneck:
deconfounded visual grounding. Second, to tackle the challenge that the
confounder is unobserved in general, we propose a confounder-agnostic approach
called: Referring Expression Deconfounder (RED), to remove the confounding
bias. Third, we implement RED as a simple language attention, which can be
applied in any grounding method. On popular benchmarks, RED improves various
state-of-the-art grounding methods by a significant margin. Code will soon be
available at: https://github.com/JianqiangH/Deconfounded_VG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Distinctive Properties of Universal Perturbations. (arXiv:2112.15329v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15329">
<div class="article-summary-box-inner">
<span><p>We identify properties of universal adversarial perturbations (UAPs) that
distinguish them from standard adversarial perturbations. Specifically, we show
that targeted UAPs generated by projected gradient descent exhibit two
human-aligned properties: semantic locality and spatial invariance, which
standard targeted adversarial perturbations lack. We also demonstrate that UAPs
contain significantly less signal for generalization than standard adversarial
perturbations -- that is, UAPs leverage non-robust features to a smaller extent
than standard adversarial perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P2P-Loc: Point to Point Tiny Person Localization. (arXiv:2112.15344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15344">
<div class="article-summary-box-inner">
<span><p>Bounding-box annotation form has been the most frequently used method for
visual object localization tasks. However, bounding-box annotation relies on
the large amounts of precisely annotating bounding boxes, which is expensive,
laborious, thus impossible in practical scenarios, and even redundant for some
applications caring not about size. Therefore, we propose a novel point-based
framework for the person localization task by annotating each person as a
coarse point (CoarsePoint) which can be any point within the object extent,
instead of an accurate bounding box. And then predict the person's location as
a 2D coordinate in the image. That greatly simplifies the data annotation
pipeline. However, the CoarsePoint annotation inevitably causes the label
reliability decrease (label uncertainty) and network confusion during training.
As a result, we propose a point self-refinement approach, which iteratively
updates point annotations in a self-paced way. The proposed refinement system
alleviates the label uncertainty and progressively improves localization
performance. Experiments show that our approach achieves comparable object
localization performance while saving annotation cost up to 80$\%$. Code is
enclosed in the supplementary materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Predict 3D Lane Shape and Camera Pose from a Single Image via Geometry Constraints. (arXiv:2112.15351v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15351">
<div class="article-summary-box-inner">
<span><p>Detecting 3D lanes from the camera is a rising problem for autonomous
vehicles. In this task, the correct camera pose is the key to generating
accurate lanes, which can transform an image from perspective-view to the
top-view. With this transformation, we can get rid of the perspective effects
so that 3D lanes would look similar and can accurately be fitted by low-order
polynomials. However, mainstream 3D lane detectors rely on perfect camera poses
provided by other sensors, which is expensive and encounters multi-sensor
calibration issues. To overcome this problem, we propose to predict 3D lanes by
estimating camera pose from a single image with a two-stage framework. The
first stage aims at the camera pose task from perspective-view images. To
improve pose estimation, we introduce an auxiliary 3D lane task and geometry
constraints to benefit from multi-task learning, which enhances consistencies
between 3D and 2D, as well as compatibility in the above two tasks. The second
stage targets the 3D lane task. It uses previously estimated pose to generate
top-view images containing distance-invariant lane appearances for predicting
accurate 3D lanes. Experiments demonstrate that, without ground truth camera
pose, our method outperforms the state-of-the-art perfect-camera-pose-based
methods and has the fewest parameters and computations. Codes are available at
https://github.com/liuruijin17/CLGo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse LiDAR Assisted Self-supervised Stereo Disparity Estimation. (arXiv:2112.15355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15355">
<div class="article-summary-box-inner">
<span><p>Deep stereo matching has made significant progress in recent years. However,
state-of-the-art methods are based on expensive 4D cost volume, which limits
their use in real-world applications. To address this issue, 3D correlation
maps and iterative disparity updates have been proposed. Regarding that in
real-world platforms, such as self-driving cars and robots, the Lidar is
usually installed. Thus we further introduce the sparse Lidar point into the
iterative updates, which alleviates the burden of network updating the
disparity from zero states. Furthermore, we propose training the network in a
self-supervised way so that it can be trained on any captured data for better
generalization ability. Experiments and comparisons show that the presented
method is effective and achieves comparable results with related methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generative Data-Free Knowledge Distillation based on Attention Transfer. (arXiv:2112.15358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15358">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation has made remarkable achievements in model compression.
However, most existing methods demand original training data, while real data
in practice are often unavailable due to privacy, security and transmission
limitation. To address this problem, we propose a conditional generative
data-free knowledge distillation (CGDD) framework to train efficient portable
network without any real data. In this framework, except using the knowledge
extracted from teacher model, we introduce preset labels as additional
auxiliary information to train the generator. Then, the trained generator can
produce meaningful training samples of specified category as required. In order
to promote distillation process, except using conventional distillation loss,
we treat preset label as ground truth label so that student network is directly
supervised by the category of synthetic training sample. Moreover, we force
student network to mimic the attention maps of teacher model and further
improve its performance. To verify the superiority of our method, we design a
new evaluation metric is called as relative accuracy to directly compare the
effectiveness of different distillation methods. Trained portable network
learned with proposed data-free distillation method obtains 99.63%, 99.07% and
99.84% relative accuracy on CIFAR10, CIFAR100 and Caltech101, respectively. The
experimental results demonstrate the superiority of proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrated Hyperspectral Image Reconstruction via Graph-based Self-Tuning Network. (arXiv:2112.15362v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15362">
<div class="article-summary-box-inner">
<span><p>Recently, hyperspectral imaging (HSI) has attracted increasing research
attention, especially for the ones based on a coded aperture snapshot spectral
imaging (CASSI) system. Existing deep HSI reconstruction models are generally
trained on paired data to retrieve original signals upon 2D compressed
measurements given by a particular optical hardware mask in CASSI, during which
the mask largely impacts the reconstruction performance and could work as a
"model hyperparameter" governing on data augmentations. This mask-specific
training style will lead to a hardware miscalibration issue, which sets up
barriers to deploying deep HSI models among different hardware and noisy
environments. To address this challenge, we introduce mask uncertainty for HSI
with a complete variational Bayesian learning treatment and explicitly model it
through a mask decomposition inspired by real hardware. Specifically, we
propose a novel Graph-based Self-Tuning (GST) network to reason uncertainties
adapting to varying spatial structures of masks among different hardware.
Moreover, we develop a bilevel optimization framework to balance HSI
reconstruction and uncertainty estimation, accounting for the hyperparameter
property of masks. Extensive experimental results and model discussions
validate the effectiveness (over 33/30 dB) of the proposed GST method under two
miscalibration scenarios and demonstrate a highly competitive performance
compared with the state-of-the-art well-calibrated methods. Our code and
pre-trained model are available at https://github.com/Jiamian
Wang/mask_uncertainty_spectral_SCI
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Change Detection Using Guided Anisotropic Difusion. (arXiv:2112.15367v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15367">
<div class="article-summary-box-inner">
<span><p>Large scale datasets created from crowdsourced labels or openly available
data have become crucial to provide training data for large scale learning
algorithms. While these datasets are easier to acquire, the data are frequently
noisy and unreliable, which is motivating research on weakly supervised
learning techniques. In this paper we propose original ideas that help us to
leverage such datasets in the context of change detection. First, we propose
the guided anisotropic diffusion (GAD) algorithm, which improves semantic
segmentation results using the input images as guides to perform edge
preserving filtering. We then show its potential in two weakly-supervised
learning strategies tailored for change detection. The first strategy is an
iterative learning method that combines model optimisation and data cleansing
using GAD to extract the useful information from a large scale change detection
dataset generated from open vector data. The second one incorporates GAD within
a novel spatial attention layer that increases the accuracy of weakly
supervised networks trained to perform pixel-level predictions from image-level
labels. Improvements with respect to state-of-the-art are demonstrated on 4
different public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Single Image Super-Resolution Using Dual Path Connections with Multiple Scale Learning. (arXiv:2112.15386v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15386">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks have been demonstrated to be effective for
SISR in recent years. On the one hand, residual connections and dense
connections have been used widely to ease forward information and backward
gradient flows to boost performance. However, current methods use residual
connections and dense connections separately in most network layers in a
sub-optimal way. On the other hand, although various networks and methods have
been designed to improve computation efficiency, save parameters, or utilize
training data of multiple scale factors for each other to boost performance, it
either do super-resolution in HR space to have a high computation cost or can
not share parameters between models of different scale factors to save
parameters and inference time. To tackle these challenges, we propose an
efficient single image super-resolution network using dual path connections
with multiple scale learning named as EMSRDPN. By introducing dual path
connections inspired by Dual Path Networks into EMSRDPN, it uses residual
connections and dense connections in an integrated way in most network layers.
Dual path connections have the benefits of both reusing common features of
residual connections and exploring new features of dense connections to learn a
good representation for SISR. To utilize the feature correlation of multiple
scale factors, EMSRDPN shares all network units in LR space between different
scale factors to learn shared features and only uses a separate reconstruction
unit for each scale factor, which can utilize training data of multiple scale
factors to help each other to boost performance, meanwhile which can save
parameters and support shared inference for multiple scale factors to improve
efficiency. Experiments show EMSRDPN achieves better performance and comparable
or even better parameter and inference efficiency over SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfoNeRF: Ray Entropy Minimization for Few-Shot Neural Volume Rendering. (arXiv:2112.15399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15399">
<div class="article-summary-box-inner">
<span><p>We present an information-theoretic regularization technique for few-shot
novel view synthesis based on neural implicit representation. The proposed
approach minimizes potential reconstruction inconsistency that happens due to
insufficient viewpoints by imposing the entropy constraint of the density in
each ray. In addition, to alleviate the potential degenerate issue when all
training images are acquired from almost redundant viewpoints, we further
incorporate the spatially smoothness constraint into the estimated images by
restricting information gains from a pair of rays with slightly different
viewpoints. The main idea of our algorithm is to make reconstructed scenes
compact along individual rays and consistent across rays in the neighborhood.
The proposed regularizers can be plugged into most of existing neural volume
rendering techniques based on NeRF in a straightforward way. Despite its
simplicity, we achieve consistently improved performance compared to existing
neural view synthesis methods by large margins on multiple standard benchmarks.
Our project website is available at
\url{<a href="http://cvlab.snu.ac.kr/research/InfoNeRF">this http URL</a>}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15402">
<div class="article-summary-box-inner">
<span><p>Continual learning requires models to learn new tasks while maintaining
previously learned knowledge. Various algorithms have been proposed to address
this real challenge. Till now, rehearsal-based methods, such as experience
replay, have achieved state-of-the-art performance. These approaches save a
small part of the data of the past tasks as a memory buffer to prevent models
from forgetting previously learned knowledge. However, most of them treat every
new task equally, i.e., fixed the hyperparameters of the framework while
learning different new tasks. Such a setting lacks the consideration of the
relationship/similarity between past and new tasks. For example, the previous
knowledge/features learned from dogs are more beneficial for the identification
of cats (new task), compared to those learned from buses. In this regard, we
propose a meta learning algorithm based on bi-level optimization to adaptively
tune the relationship between the knowledge extracted from the past and new
tasks. Therefore, the model can find an appropriate direction of gradient
during continual learning and avoid the serious overfitting problem on memory
buffer. Extensive experiments are conducted on three publicly available
datasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental
results demonstrate that the proposed method can consistently improve the
performance of all baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disjoint Contrastive Regression Learning for Multi-Sourced Annotations. (arXiv:2112.15411v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15411">
<div class="article-summary-box-inner">
<span><p>Large-scale datasets are important for the development of deep learning
models. Such datasets usually require a heavy workload of annotations, which
are extremely time-consuming and expensive. To accelerate the annotation
procedure, multiple annotators may be employed to label different subsets of
the data. However, the inconsistency and bias among different annotators are
harmful to the model training, especially for qualitative and subjective
tasks.To address this challenge, in this paper, we propose a novel contrastive
regression framework to address the disjoint annotations problem, where each
sample is labeled by only one annotator and multiple annotators work on
disjoint subsets of the data. To take account of both the intra-annotator
consistency and inter-annotator inconsistency, two strategies are
employed.Firstly, a contrastive-based loss is applied to learn the relative
ranking among different samples of the same annotator, with the assumption that
the ranking of samples from the same annotator is unanimous. Secondly, we apply
the gradient reversal layer to learn robust representations that are invariant
to different annotators. Experiments on the facial expression prediction task,
as well as the image quality assessment task, verify the effectiveness of our
proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Facial Synthesis: A New Challenge. (arXiv:2112.15439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15439">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to conduct a comprehensive study on the facial
sketch synthesis (FSS) problem. However, due to the high costs in obtaining
hand-drawn sketch datasets, there lacks a complete benchmark for assessing the
development of FSS algorithms over the last decade. As such, we first introduce
a high-quality dataset for FSS, named FS2K, which consists of 2,104
image-sketch pairs spanning three types of sketch styles, image backgrounds,
lighting conditions, skin colors, and facial attributes. FS2K differs from
previous FSS datasets in difficulty, diversity, and scalability, and should
thus facilitate the progress of FSS research. Second, we present the
largest-scale FSS study by investigating 139 classical methods, including 24
handcrafted feature based facial sketch synthesis approaches, 37 general
neural-style transfer methods, 43 deep image-to-image translation methods, and
35 image-to-sketch approaches. Besides, we elaborate comprehensive experiments
for existing 19 cutting-edge models. Third, we present a simple baseline for
FSS, named FSGAN. With only two straightforward components, i.e., facial-aware
masking and style-vector expansion, FSGAN surpasses the performance of all
previous state-of-the-art models on the proposed FS2K dataset, by a large
margin. Finally, we conclude with lessons learned over the past years, and
point out several unsolved challenges. Our open-source code is available at
https://github.com/DengPingFan/FSGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PiFeNet: Pillar-Feature Network for Real-Time 3D Pedestrian Detection from Point Cloud. (arXiv:2112.15458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15458">
<div class="article-summary-box-inner">
<span><p>We present PiFeNet, an efficient and accurate real-time 3D detector for
pedestrian detection from point clouds. We address two challenges that 3D
object detection frameworks encounter when detecting pedestrians: low
expressiveness of pillar features and small occupation areas of pedestrians in
point clouds. Firstly, we introduce a stackable Pillar Aware Attention (PAA)
module for enhanced pillar features extraction while suppressing noises in the
point clouds. By integrating multi-point-aware-pooling, point-wise,
channel-wise, and task-aware attention into a simple module, the representation
capabilities are boosted while requiring little additional computing resources.
We also present Mini-BiFPN, a small yet effective feature network that creates
bidirectional information flow and multi-level cross-scale feature fusion to
better integrate multi-resolution features. Our approach is ranked 1st in KITTI
pedestrian BEV and 3D leaderboards while running at 26 frames per second (FPS),
and achieves state-of-the-art performance on Nuscenes detection benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cloud Removal from Satellite Images. (arXiv:2112.15483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15483">
<div class="article-summary-box-inner">
<span><p>In this report, we have analyzed available cloud detection technique using
sentinel hub. We have also implemented spatial attention generative adversarial
network and improved quality of generated image compared to previous solution
[7].
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene-Adaptive Attention Network for Crowd Counting. (arXiv:2112.15509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15509">
<div class="article-summary-box-inner">
<span><p>In recent years, significant progress has been made on the research of crowd
counting. However, as the challenging scale variations and complex scenes
existed in crowds, neither traditional convolution networks nor recent
Transformer architectures with fixed-size attention could handle the task well.
To address this problem, this paper proposes a scene-adaptive attention
network, termed SAANet. First of all, we design a deformable attention in-built
Transformer backbone, which learns adaptive feature representations with
deformable sampling locations and dynamic attention weights. Then we propose
the multi-level feature fusion and count-attentive feature enhancement modules
further, to strengthen feature representation under the global image context.
The learned representations could attend to the foreground and are adaptive to
different scales of crowds. We conduct extensive experiments on four
challenging crowd counting benchmarks, demonstrating that our method achieves
state-of-the-art performance. Especially, our method currently ranks No.1 on
the public leaderboard of the NWPU-Crowd benchmark. We hope our method could be
a strong baseline to support future research in crowd counting. The source code
will be released to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer learning for cancer diagnosis in histopathological images. (arXiv:2112.15523v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15523">
<div class="article-summary-box-inner">
<span><p>Transfer learning allows us to exploit knowledge gained from one task to
assist in solving another but relevant task. In modern computer vision
research, the question is which architecture performs better for a given
dataset. In this paper, we compare the performance of 14 pre-trained ImageNet
models on the histopathologic cancer detection dataset, where each model has
been configured as a naive model, feature extractor model, or fine-tuned model.
Densenet161 has been shown to have high precision whilst Resnet101 has a high
recall. A high precision model is suitable to be used when follow-up
examination cost is high, whilst low precision but a high recall/sensitivity
model can be used when the cost of follow-up examination is low. Results also
show that transfer learning helps to converge a model faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">on the effectiveness of generative adversarial network on anomaly detection. (arXiv:2112.15541v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15541">
<div class="article-summary-box-inner">
<span><p>Identifying anomalies refers to detecting samples that do not resemble the
training data distribution. Many generative models have been used to find
anomalies, and among them, generative adversarial network (GAN)-based
approaches are currently very popular. GANs mainly rely on the rich contextual
information of these models to identify the actual training distribution.
Following this analogy, we suggested a new unsupervised model based on GANs --a
combination of an autoencoder and a GAN. Further, a new scoring function was
introduced to target anomalies where a linear combination of the internal
representation of the discriminator and the generator's visual representation,
plus the encoded representation of the autoencoder, come together to define the
proposed anomaly score. The model was further evaluated on benchmark datasets
such as SVHN, CIFAR10, and MNIST, as well as a public medical dataset of
leukemia images. In all the experiments, our model outperformed its existing
counterparts while slightly improving the inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Baselines in the Wild. (arXiv:2112.15550v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15550">
<div class="article-summary-box-inner">
<span><p>We share our experience with the recently released WILDS benchmark, a
collection of ten datasets dedicated to developing models and training
strategies which are robust to domain shifts. Several experiments yield a
couple of critical observations which we believe are of general interest for
any future work on WILDS. Our study focuses on two datasets: iWildCam and FMoW.
We show that (1) Conducting separate cross-validation for each evaluation
metric is crucial for both datasets, (2) A weak correlation between validation
and test performance might make model development difficult for iWildCam, (3)
Minor changes in the training of hyper-parameters improve the baseline by a
relatively large margin (mainly on FMoW), (4) There is a strong correlation
between certain domains and certain target labels (mainly on iWildCam). To the
best of our knowledge, no prior work on these datasets has reported these
observations despite their obvious importance. Our code is public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Domain Adaptation Model based on Dual-module Adversarial Training. (arXiv:2112.15555v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15555">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a dual-module network architecture that employs a
domain discriminative feature module to encourage the domain invariant feature
module to learn more domain invariant features. The proposed architecture can
be applied to any model that utilizes domain invariant features for
unsupervised domain adaptation to improve its ability to extract domain
invariant features. We conduct experiments with the Domain-Adversarial Training
of Neural Networks (DANN) model as a representative algorithm. In the training
process, we supply the same input to the two modules and then extract their
feature distribution and prediction results respectively. We propose a
discrepancy loss to find the discrepancy of the prediction results and the
feature distribution between the two modules. Through the adversarial training
by maximizing the loss of their feature distribution and minimizing the
discrepancy of their prediction results, the two modules are encouraged to
learn more domain discriminative and domain invariant features respectively.
Extensive comparative evaluations are conducted and the proposed approach
outperforms the state-of-the-art in most unsupervised domain adaptation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCACE: A Statistical Approach to Ranking Neurons for CNN Interpretability. (arXiv:2112.15571v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15571">
<div class="article-summary-box-inner">
<span><p>In this paper we introduce a new problem within the growing literature of
interpretability for convolution neural networks (CNNs). While previous work
has focused on the question of how to visually interpret CNNs, we ask what it
is that we care to interpret, that is, which layers and neurons are worth our
attention? Due to the vast size of modern deep learning network architectures,
automated, quantitative methods are needed to rank the relative importance of
neurons so as to provide an answer to this question. We present a new
statistical method for ranking the hidden neurons in any convolutional layer of
a network. We define importance as the maximal correlation between the
activation maps and the class score. We provide different ways in which this
method can be used for visualization purposes with MNIST and ImageNet, and show
a real-world application of our method to air pollution prediction with
street-level images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3-D Material Style Transfer for Reconstructing Unknown Appearance in Complex Natural Materials. (arXiv:2112.15589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15589">
<div class="article-summary-box-inner">
<span><p>We propose a 3-D material style transfer framework for reconstructing
invisible (or faded) appearance properties in complex natural materials. Our
algorithm addresses the technical challenge of transferring appearance
properties from one object to another of the same material when both objects
have intricate, noncorresponding color patterns. Eggshells, exoskeletons, and
minerals, for example, have patterns composed of highly randomized layers of
organic and inorganic compounds. These materials pose a challenge as the
distribution of compounds that determine surface color changes from object to
object and within local pattern regions. Our solution adapts appearance
observations from a material property distribution in an exemplar to the
material property distribution of a target object to reconstruct its unknown
appearance. We use measured reflectance in 3-D bispectral textures to record
changing material property distributions. Our novel implementation of spherical
harmonics uses principles from chemistry and biology to learn relationships
between color (hue and saturation) and material composition and concentration
in an exemplar. The encoded relationships are transformed to the property
distribution of a target for color recovery and material assignment.
Quantitative and qualitative evaluation methods show that we replicate color
patterns more accurately than methods that only rely on shape correspondences
and coarse-level perceptual differences. We demonstrate applications of our
work for reconstructing color in extinct fossils, restoring faded artifacts and
generating synthetic textures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A theory of independent mechanisms for extrapolation in generative models. (arXiv:2004.00184v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00184">
<div class="article-summary-box-inner">
<span><p>Generative models can be trained to emulate complex empirical data, but are
they useful to make predictions in the context of previously unobserved
environments? An intuitive idea to promote such extrapolation capabilities is
to have the architecture of such model reflect a causal graph of the true data
generating process, such that one can intervene on each node independently of
the others. However, the nodes of this graph are usually unobserved, leading to
overparameterization and lack of identifiability of the causal structure. We
develop a theoretical framework to address this challenging situation by
defining a weaker form of identifiability, based on the principle of
independence of mechanisms. We demonstrate on toy examples that classical
stochastic gradient descent can hinder the model's extrapolation capabilities,
suggesting independence of mechanisms should be enforced explicitly during
training. Experiments on deep generative models trained on real world data
support these insights and illustrate how the extrapolation capabilities of
such models can be leveraged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation. (arXiv:2007.01771v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01771">
<div class="article-summary-box-inner">
<span><p>Facial attributes (\eg, age and attractiveness) estimation performance has
been greatly improved by using convolutional neural networks. However, existing
methods have an inconsistency between the training objectives and the
evaluation metric, so they may be suboptimal. In addition, these methods always
adopt image classification or face recognition models with a large amount of
parameters, which carry expensive computation cost and storage overhead. In
this paper, we firstly analyze the essential relationship between two
state-of-the-art methods (Ranking-CNN and DLDL) and show that the Ranking
method is in fact learning label distribution implicitly. This result thus
firstly unifies two existing popular state-of-the-art methods into the DLDL
framework. Second, in order to alleviate the inconsistency and reduce resource
consumption, we design a lightweight network architecture and propose a unified
framework which can jointly learn facial attribute distribution and regress
attribute value. The effectiveness of our approach has been demonstrated on
both facial age and attractiveness estimation tasks. Our method achieves new
state-of-the-art results using the single model with 36$\times$ fewer
parameters and 3$\times$ faster inference speed on facial age/attractiveness
estimation. Moreover, our method can achieve comparable results as the
state-of-the-art even though the number of parameters is further reduced to
0.9M (3.8MB disk storage).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taking Modality-free Human Identification as Zero-shot Learning. (arXiv:2010.00975v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00975">
<div class="article-summary-box-inner">
<span><p>Human identification is an important topic in event detection, person
tracking, and public security. There have been numerous methods proposed for
human identification, such as face identification, person re-identification,
and gait identification. Typically, existing methods predominantly classify a
queried image to a specific identity in an image gallery set (I2I). This is
seriously limited for the scenario where only a textual description of the
query or an attribute gallery set is available in a wide range of video
surveillance applications (A2I or I2A). However, very few efforts have been
devoted towards modality-free identification, i.e., identifying a query in a
gallery set in a scalable way. In this work, we take an initial attempt, and
formulate such a novel Modality-Free Human Identification (named MFHI) task as
a generic zero-shot learning model in a scalable way. Meanwhile, it is capable
of bridging the visual and semantic modalities by learning a discriminative
prototype of each identity. In addition, the semantics-guided spatial attention
is enforced on visual modality to obtain representations with both high global
category-level and local attribute-level discrimination. Finally, we design and
conduct an extensive group of experiments on two common challenging
identification tasks, including face identification and person
re-identification, demonstrating that our method outperforms a wide variety of
state-of-the-art methods on modality-free human identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing Hand-Object Interactions in the Wild. (arXiv:2012.09856v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09856">
<div class="article-summary-box-inner">
<span><p>In this work we explore reconstructing hand-object interactions in the wild.
The core challenge of this problem is the lack of appropriate 3D labeled data.
To overcome this issue, we propose an optimization-based procedure which does
not require direct 3D supervision. The general strategy we adopt is to exploit
all available related data (2D bounding boxes, 2D hand keypoints, 2D instance
masks, 3D object models, 3D in-the-lab MoCap) to provide constraints for the 3D
reconstruction. Rather than optimizing the hand and object individually, we
optimize them jointly which allows us to impose additional constraints based on
hand-object contact, collision, and occlusion. Our method produces compelling
reconstructions on the challenging in-the-wild data from the EPIC Kitchens and
the 100 Days of Hands datasets, across a range of object categories.
Quantitatively, we demonstrate that our approach compares favorably to existing
approaches in the lab settings where ground truth 3D annotations are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transitional Learning: Exploring the Transition States of Degradation for Blind Super-resolution. (arXiv:2103.15290v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15290">
<div class="article-summary-box-inner">
<span><p>Being extremely dependent on iterative estimation of the degradation prior or
optimization of the model from scratch, the existing blind super-resolution
(SR) methods are generally time-consuming and less effective, as the estimation
of degradation proceeds from a blind initialization and lacks interpretable
degradation priors. To address it, this paper proposes a transitional learning
method for blind SR using an end-to-end network without any additional
iterations in inference, and explores an effective representation for unknown
degradation. To begin with, we analyze and demonstrate the transitionality of
degradations as interpretable prior information to indirectly infer the unknown
degradation model, including the widely used additive and convolutive
degradations. We then propose a novel Transitional Learning method for blind
Super-Resolution (TLSR), by adaptively inferring a transitional transformation
function to solve the unknown degradations without any iterative operations in
inference. Specifically, the end-to-end TLSR network consists of a degree of
transitionality (DoT) estimation network, a homogeneous feature extraction
network, and a transitional learning module. Quantitative and qualitative
evaluations on blind SR tasks demonstrate that the proposed TLSR achieves
superior performances and costs fewer complexities against the state-of-the-art
blind SR methods. The code is available at github.com/YuanfeiHuang/TLSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MutualNet: Adaptive ConvNet via Mutual Learning from Different Model Configurations. (arXiv:2105.07085v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07085">
<div class="article-summary-box-inner">
<span><p>Most existing deep neural networks are static, which means they can only do
inference at a fixed complexity. But the resource budget can vary substantially
across different devices. Even on a single device, the affordable budget can
change with different scenarios, and repeatedly training networks for each
required budget would be incredibly expensive. Therefore, in this work, we
propose a general method called MutualNet to train a single network that can
run at a diverse set of resource constraints. Our method trains a cohort of
model configurations with various network widths and input resolutions. This
mutual learning scheme not only allows the model to run at different
width-resolution configurations but also transfers the unique knowledge among
these configurations, helping the model to learn stronger representations
overall. MutualNet is a general training methodology that can be applied to
various network structures (e.g., 2D networks: MobileNets, ResNet, 3D networks:
SlowFast, X3D) and various tasks (e.g., image classification, object detection,
segmentation, and action recognition), and is demonstrated to achieve
consistent improvements on a variety of datasets. Since we only train the model
once, it also greatly reduces the training cost compared to independently
training several models. Surprisingly, MutualNet can also be used to
significantly boost the performance of a single network, if dynamic resource
constraint is not a concern. In summary, MutualNet is a unified method for both
static and adaptive, 2D and 3D networks. Codes and pre-trained models are
available at \url{https://github.com/taoyang1122/MutualNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding. (arXiv:2105.12723v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12723">
<div class="article-summary-box-inner">
<span><p>Hierarchical structures are popular in recent vision transformers, however,
they require sophisticated designs and massive datasets to work well. In this
paper, we explore the idea of nesting basic local transformers on
non-overlapping image blocks and aggregating them in a hierarchical way. We
find that the block aggregation function plays a critical role in enabling
cross-block non-local information communication. This observation leads us to
design a simplified architecture that requires minor code changes upon the
original vision transformer. The benefits of the proposed judiciously-selected
design are threefold: (1) NesT converges faster and requires much less training
data to achieve good generalization on both ImageNet and small datasets like
CIFAR; (2) when extending our key ideas to image generation, NesT leads to a
strong decoder that is 8$\times$ faster than previous transformer-based
generators; and (3) we show that decoupling the feature learning and
abstraction processes via this nested hierarchy in our design enables
constructing a novel method (named GradCAT) for visually interpreting the
learned model. Source code is available
https://github.com/google-research/nested-transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Pseudo Labels for Semi-Supervised Object Detection. (arXiv:2106.00168v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00168">
<div class="article-summary-box-inner">
<span><p>Recent advances in semi-supervised object detection (SSOD) are largely driven
by consistency-based pseudo-labeling methods for image classification tasks,
producing pseudo labels as supervisory signals. However, when using pseudo
labels, there is a lack of consideration in localization precision and
amplified class imbalance, both of which are critical for detection tasks. In
this paper, we introduce certainty-aware pseudo labels tailored for object
detection, which can effectively estimate the classification and localization
quality of derived pseudo labels. This is achieved by converting conventional
localization as a classification task followed by refinement. Conditioned on
classification and localization quality scores, we dynamically adjust the
thresholds used to generate pseudo labels and reweight loss functions for each
category to alleviate the class imbalance problem. Extensive experiments
demonstrate that our method improves state-of-the-art SSOD performance by 1-2%
AP on COCO and PASCAL VOC while being orthogonal and complementary to most
existing methods. In the limited-annotation regime, our approach improves
supervised baselines by up to 10% AP using only 1-10% labeled data from COCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCNet: A Structure Similarity Enhancement Method for Multispectral and Multimodal Image Registration. (arXiv:2106.05124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05124">
<div class="article-summary-box-inner">
<span><p>Multispectral and multimodal image processing is important in the community
of computer vision and computational photography. As the acquired multispectral
and multimodal data are generally misaligned due to the alternation or movement
of the image device, the image registration procedure is necessary. The
registration of multispectral or multimodal image is challenging due to the
non-linear intensity and gradient variation. To cope with this challenge, we
propose the phase congruency network (PCNet), which is able to enhance the
structure similarity and alleviate the non-linear intensity and gradient
variation. The images can then be aligned using the similarity enhanced
features produced by the network. PCNet is constructed under the guidance of
the phase congruency prior. The network contains three trainable layers
accompany with the modified learnable Gabor kernels according to the phase
congruency theory. Thanks to the prior knowledge, PCNet is extremely
light-weight. PCNet can be viewed to be fully convolutional and hence can take
input of arbitrary sizes. Once trained, PCNet is applicable on a variety of
multispectral and multimodal data such as RGB/NIR and flash/no-flash images
without additional further tuning. Experimental results validate that PCNet
outperforms current state-of-the-art registration algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NCIS: Neural Contextual Iterative Smoothing for Purifying Adversarial Perturbations. (arXiv:2106.11644v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11644">
<div class="article-summary-box-inner">
<span><p>We propose a novel and effective purification based adversarial defense
method against pre-processor blind white- and black-box attacks. Our method is
computationally efficient and trained only with self-supervised learning on
general images, without requiring any adversarial training or retraining of the
classification model. We first show an empirical analysis on the adversarial
noise, defined to be the residual between an original image and its adversarial
example, has almost zero mean, symmetric distribution. Based on this
observation, we propose a very simple iterative Gaussian Smoothing (GS) which
can effectively smooth out adversarial noise and achieve substantially high
robust accuracy. To further improve it, we propose Neural Contextual Iterative
Smoothing (NCIS), which trains a blind-spot network (BSN) in a self-supervised
manner to reconstruct the discriminative features of the original image that is
also smoothed out by GS. From our extensive experiments on the large-scale
ImageNet using four classification models, we show that our method achieves
both competitive standard accuracy and state-of-the-art robust accuracy against
most strong purifier-blind white- and black-box attacks. Also, we propose a new
benchmark for evaluating a purification method based on commercial image
classification APIs, such as AWS, Azure, Clarifai and Google. We generate
adversarial examples by ensemble transfer-based black-box attack, which can
induce complete misclassification of APIs, and demonstrate that our method can
be used to increase adversarial robustness of APIs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Measuring and Controlling the Spectral Bias of the Deep Image Prior. (arXiv:2107.01125v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01125">
<div class="article-summary-box-inner">
<span><p>The deep image prior showed that a randomly initialized network with a
suitable architecture can be trained to solve inverse imaging problems by
simply optimizing it's parameters to reconstruct a single degraded image.
However, it suffers from two practical limitations. First, it remains unclear
how to control the prior beyond the choice of the network architecture. Second,
training requires an oracle stopping criterion as during the optimization the
performance degrades after reaching an optimum value. To address these
challenges we introduce a frequency-band correspondence measure to characterize
the spectral bias of the deep image prior, where low-frequency image signals
are learned faster and better than high-frequency counterparts. Based on our
observations, we propose techniques to prevent the eventual performance
degradation and accelerate convergence. We introduce a Lipschitz-controlled
convolution layer and a Gaussian-controlled upsampling layer as plug-in
replacements for layers used in the deep architectures. The experiments show
that with these changes the performance does not degrade during optimization,
relieving us from the need for an oracle stopping criterion. We further outline
a stopping criterion to avoid superfluous computation. Finally, we show that
our approach obtains favorable results compared to current approaches across
various denoising, deblocking, inpainting, super-resolution and detail
enhancement tasks. Code is available at
\url{https://github.com/shizenglin/Measure-and-Control-Spectral-Bias}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Learning Technique for Video Segmentation. (arXiv:2107.01153v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01153">
<div class="article-summary-box-inner">
<span><p>Video segmentation, i.e., partitioning video frames into multiple segments or
objects, plays a critical role in a broad range of practical applications, from
enhancing visual effects in movie, to understanding scenes in autonomous
driving, to virtual background creation in video conferencing, just to name a
few. Recently, due to the renaissance of connectionism in computer vision,
there has been an influx of deep learning based approaches for video
segmentation that have delivered compelling performance. In this survey, we
comprehensively review two basic lines of research - generic object
segmentation (of unknown categories) in videos and video semantic segmentation
- by introducing their respective task settings, background concepts, perceived
need, development history, and main challenges. We also provide a detailed
overview of representative literature on both methods and datasets.
Additionally, we present quantitative performance comparisons of the reviewed
methods on benchmark datasets. Finally, we point out a set of unsolved open
issues in this field, and suggest possible opportunities for further research.
A public website is provided to continuously track recent developments in this
fast advancing field: https://github.com/tfzhou/VS-Survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Alignment Prediction for Few-Shot Video Classification. (arXiv:2107.11960v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11960">
<div class="article-summary-box-inner">
<span><p>The goal of few-shot video classification is to learn a classification model
with good generalization ability when trained with only a few labeled videos.
However, it is difficult to learn discriminative feature representations for
videos in such a setting. In this paper, we propose Temporal Alignment
Prediction (TAP) based on sequence similarity learning for few-shot video
classification. In order to obtain the similarity of a pair of videos, we
predict the alignment scores between all pairs of temporal positions in the two
videos with the temporal alignment prediction function. Besides, the inputs to
this function are also equipped with the context information in the temporal
domain. We evaluate TAP on two video classification benchmarks including
Kinetics and Something-Something V2. The experimental results verify the
effectiveness of TAP and show its superiority over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisBuddy -- A Smart Wearable Assistant for the Visually Challenged. (arXiv:2108.07761v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07761">
<div class="article-summary-box-inner">
<span><p>Vision plays a crucial role in comprehending the world around us. More than
85\% of the external information is obtained through the vision system. It
influences our mobility, cognition, information access, and interaction with
the environment and other people. Blindness prevents a person from gaining
knowledge of the surrounding environment and makes unassisted navigation,
object recognition, obstacle avoidance, and reading tasks significant
challenges. Many existing systems are often limited by cost and complexity. To
help the visually challenged overcome these difficulties faced in everyday
life, we propose VisBuddy, a smart assistant to help the visually challenged
with their day-to-day activities. VisBuddy is a voice-based assistant where the
user can give voice commands to perform specific tasks. It uses the techniques
of image captioning for describing the user's surroundings, optical character
recognition (OCR) for reading the text in the user's view, object detection to
search and find the objects in a room and web scraping to give the user the
latest news. VisBuddy has been built by combining the concepts from Deep
Learning and the Internet of Things. Thus, VisBuddy serves as a cost-efficient,
powerful, all-in-one assistant for the visually challenged by helping them with
their day-to-day activities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">roadscene2vec: A Tool for Extracting and Embedding Road Scene-Graphs. (arXiv:2109.01183v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01183">
<div class="article-summary-box-inner">
<span><p>Recently, road scene-graph representations used in conjunction with graph
learning techniques have been shown to outperform state-of-the-art deep
learning techniques in tasks including action classification, risk assessment,
and collision prediction. To enable the exploration of applications of road
scene-graph representations, we introduce roadscene2vec: an open-source tool
for extracting and embedding road scene-graphs. The goal of roadscene2vec is to
enable research into the applications and capabilities of road scene-graphs by
providing tools for generating scene-graphs, graph learning models to generate
spatio-temporal scene-graph embeddings, and tools for visualizing and analyzing
scene-graph-based methodologies. The capabilities of roadscene2vec include (i)
customized scene-graph generation from either video clips or data from the
CARLA simulator, (ii) multiple configurable spatio-temporal graph embedding
models and baseline CNN-based models, (iii) built-in functionality for using
graph and sequence embeddings for risk assessment and collision prediction
applications, (iv) tools for evaluating transfer learning, and (v) utilities
for visualizing scene-graphs and analyzing the explainability of graph learning
models. We demonstrate the utility of roadscene2vec for these use cases with
experimental results and qualitative evaluations for both graph learning models
and CNN-based models. roadscene2vec is available at
https://github.com/AICPS/roadscene2vec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Regularization in DCE-MR Image Reconstruction for Functional Imaging of Kidneys. (arXiv:2109.07548v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07548">
<div class="article-summary-box-inner">
<span><p>Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and
quantitative assessment of kidney function by estimating the tracer kinetic
(TK) model parameters. Accurate estimation of TK model parameters requires an
accurate measurement of the arterial input function (AIF) with high temporal
resolution. Accelerated imaging is used to achieve high temporal resolution,
which yields under-sampling artifacts in the reconstructed images. Compressed
sensing (CS) methods offer a variety of reconstruction options. Most commonly,
sparsity of temporal differences is encouraged for regularization to reduce
artifacts. Increasing regularization in CS methods removes the ambient
artifacts but also over-smooths the signal temporally which reduces the
parameter estimation accuracy. In this work, we propose a single image trained
deep neural network to reduce MRI under-sampling artifacts without reducing the
accuracy of functional imaging markers. Instead of regularizing with a penalty
term in optimization, we promote regularization by generating images from a
lower dimensional representation. In this manuscript we motivate and explain
the lower dimensional input design. We compare our approach to CS
reconstructions with multiple regularization weights. Proposed approach results
in kidney biomarkers that are highly correlated with the ground truth markers
estimated using the CS reconstruction which was optimized for functional
analysis. At the same time, the proposed approach reduces the artifacts in the
reconstructed images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning. (arXiv:2109.10563v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10563">
<div class="article-summary-box-inner">
<span><p>Due to difficulties in acquiring ground truth depth of equirectangular (360)
images, the quality and quantity of equirectangular depth data today is
insufficient to represent the various scenes in the world. Therefore, 360 depth
estimation studies, which relied solely on supervised learning, are destined to
produce unsatisfactory results. Although self-supervised learning methods
focusing on equirectangular images (EIs) are introduced, they often have
incorrect or non-unique solutions, causing unstable performance. In this paper,
we propose 360 monocular depth estimation methods which improve on the areas
that limited previous studies. First, we introduce a self-supervised 360 depth
learning method that only utilizes gravity-aligned videos, which has the
potential to eliminate the needs for depth data during the training procedure.
Second, we propose a joint learning scheme realized by combining supervised and
self-supervised learning. The weakness of each learning is compensated, thus
leading to more accurate depth estimation. Third, we propose a non-local fusion
block, which can further retain the global information encoded by vision
transformer when reconstructing the depths. With the proposed methods, we
successfully apply the transformer to 360 depth estimations, to the best of our
knowledge, which has not been tried before. On several benchmarks, our approach
achieves significant improvements over previous works and establishes a state
of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation. (arXiv:2109.12271v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12271">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have achieved remarkable success in
automatically segmenting organs or lesions on 3D medical images. Recently,
vision transformer networks have exhibited exceptional performance in 2D image
classification tasks. Compared with CNNs, transformer networks have an
appealing advantage of extracting long-range features due to their
self-attention algorithm. Therefore, we propose a CNN-Transformer combined
model, called BiTr-Unet, with specific modifications for brain tumor
segmentation on multi-modal MRI scans. Our BiTr-Unet achieves good performance
on the BraTS2021 validation dataset with median Dice score 0.9335, 0.9304 and
0.8899, and median Hausdorff distance 2.8284, 2.2361 and 1.4142 for the whole
tumor, tumor core, and enhancing tumor, respectively. On the BraTS2021 testing
dataset, the corresponding results are 0.9257, 0.9350 and 0.8874 for Dice
score, and 3, 2.2361 and 1.4142 for Hausdorff distance. The code is publicly
available at https://github.com/JustaTinyDot/BiTr-Unet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARKitScenes -- A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data. (arXiv:2111.08897v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08897">
<div class="article-summary-box-inner">
<span><p>Scene understanding is an active research area. Commercial depth sensors,
such as Kinect, have enabled the release of several RGB-D datasets over the
past few years which spawned novel methods in 3D scene understanding. More
recently with the launch of the LiDAR sensor in Apple's iPads and iPhones, high
quality RGB-D data is accessible to millions of people on a device they
commonly use. This opens a whole new era in scene understanding for the
Computer Vision community as well as app developers. The fundamental research
in scene understanding together with the advances in machine learning can now
impact people's everyday experiences. However, transforming these scene
understanding methods to real-world experiences requires additional innovation
and development. In this paper we introduce ARKitScenes. It is not only the
first RGB-D dataset that is captured with a now widely available depth sensor,
but to our best knowledge, it also is the largest indoor scene understanding
data released. In addition to the raw and processed data from the mobile
device, ARKitScenes includes high resolution depth maps captured using a
stationary laser scanner, as well as manually labeled 3D oriented bounding
boxes for a large taxonomy of furniture. We further analyze the usefulness of
the data for two downstream tasks: 3D object detection and color-guided depth
upsampling. We demonstrate that our dataset can help push the boundaries of
existing state-of-the-art methods and it introduces new challenges that better
represent real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ML-Decoder: Scalable and Versatile Classification Head. (arXiv:2111.12933v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12933">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce ML-Decoder, a new attention-based classification
head. ML-Decoder predicts the existence of class labels via queries, and
enables better utilization of spatial data compared to global average pooling.
By redesigning the decoder architecture, and using a novel group-decoding
scheme, ML-Decoder is highly efficient, and can scale well to thousands of
classes. Compared to using a larger backbone, ML-Decoder consistently provides
a better speed-accuracy trade-off. ML-Decoder is also versatile - it can be
used as a drop-in replacement for various classification heads, and generalize
to unseen classes when operated with word queries. Novel query augmentations
further improve its generalization ability. Using ML-Decoder, we achieve
state-of-the-art results on several classification tasks: on MS-COCO
multi-label, we reach 91.4% mAP; on NUS-WIDE zero-shot, we reach 31.1% ZSL mAP;
and on ImageNet single-label, we reach with vanilla ResNet50 backbone a new top
score of 80.7%, without extra data or distillation. Public code is available
at: https://github.com/Alibaba-MIIL/ML_Decoder
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MutualFormer: Multi-Modality Representation Learning via Mutual Transformer. (arXiv:2112.01177v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01177">
<div class="article-summary-box-inner">
<span><p>Aggregating multi-modality data to obtain accurate and reliable data
representation attracts more and more attention. The pristine researchers
generally adopt the CNN to extract features of independent modality and
aggregate them with a fusion module. However, the overall performance is
becoming saturated due to limited local convolutional features. Recent studies
demonstrate that Transformer models usually work comparable or even better than
CNN for multi-modality task, but they simply adopt concatenation or
cross-attention for feature fusion which may just obtain sub-optimal results.
In this work, we re-thinking the self-attention based Transformer and propose a
novel MutualFormer for multi-modality data fusion and representation. The core
of MutualFormer is the design of both token mixer and modality mixer to conduct
the communication among both tokens and modalities. Specifically, it contains
three main modules, i.e., i) Self-attention (SA) for intra-modality token
mixer, ii) Cross-diffusion attention (CDA) for inter-modality mixer and iii)
Aggregation module. The main advantage of the proposed CDA is that it is
defined based on individual domain similarities in the metric space which thus
can naturally avoid the issue of domain/modality gap in cross-modality
similarities computation. We successfully apply the MutualFormer to the
saliency detection problem and propose a novel approach to obtain the
reinforced features of RGB and Depth images. Extensive experiments on six
popular datasets demonstrate that our model achieves comparable results with 16
SOTA models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep learning based Document Image Enhancement. (arXiv:2112.02719v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02719">
<div class="article-summary-box-inner">
<span><p>Digitized documents such as scientific articles, tax forms, invoices,
contract papers, historic texts are widely used nowadays. These document images
could be degraded or damaged due to various reasons including poor lighting
conditions, shadow, distortions like noise and blur, aging, ink stain,
bleed-through, watermark, stamp, etc. Document image enhancement plays a
crucial role as a pre-processing step in many automated document analysis and
recognition tasks such as character recognition. With recent advances in deep
learning, many methods are proposed to enhance the quality of these document
images. In this paper, we review deep learning-based methods, datasets, and
metrics for six main document image enhancement tasks, including binarization,
debluring, denoising, defading, watermark removal, and shadow removal. We
summarize the recent works for each task and discuss their features,
challenges, and limitations. We introduce multiple document image enhancement
tasks that have received little to no attention, including over and under
exposure correction, super resolution, and bleed-through removal. We identify
several promising research directions and opportunities for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAC-GAN: Structure-Aware Image-to-Image Composition for Self-Driving. (arXiv:2112.06596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06596">
<div class="article-summary-box-inner">
<span><p>We present a compositional approach to image augmentation for self-driving
applications. It is an end-to-end neural network that is trained to seamlessly
compose an object (e.g., a vehicle or pedestrian) represented as a cropped
patch from an object image, into a background scene image. As our approach
emphasizes more on semantic and structural coherence of the composed images,
rather than their pixel-level RGB accuracies, we tailor the input and output of
our network with structure-aware features and design our network losses
accordingly. Specifically, our network takes the semantic layout features from
the input scene image, features encoded from the edges and silhouette in the
input object patch, as well as a latent code as inputs, and generates a 2D
spatial affine transform defining the translation and scaling of the object
patch. The learned parameters are further fed into a differentiable spatial
transformer network to transform the object patch into the target image, where
our model is trained adversarially using an affine transform discriminator and
a layout discriminator. We evaluate our network, coined SAC-GAN for
structure-aware composition, on prominent self-driving datasets in terms of
quality, composability, and generalizability of the composite images.
Comparisons are made to state-of-the-art alternatives, confirming superiority
of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural Architecture Search. (arXiv:2112.07918v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07918">
<div class="article-summary-box-inner">
<span><p>Image semantic segmentation technology is one of the key technologies for
intelligent systems to understand natural scenes. As one of the important
research directions in the field of visual intelligence, this technology has
broad application scenarios in the fields of mobile robots, drones, smart
driving, and smart security. However, in the actual application of mobile
robots, problems such as inaccurate segmentation semantic label prediction and
loss of edge information of segmented objects and background may occur. This
paper proposes an improved structure of a semantic segmentation network based
on a deep learning network that combines self-attention neural network and
neural network architecture search methods. First, a neural network search
method NAS (Neural Architecture Search) is used to find a semantic segmentation
network with multiple resolution branches. In the search process, combine the
self-attention network structure module to adjust the searched neural network
structure, and then combine the semantic segmentation network searched by
different branches to form a fast semantic segmentation network structure, and
input the picture into the network structure to get the final forecast result.
The experimental results on the Cityscapes dataset show that the accuracy of
the algorithm is 69.8%, and the segmentation speed is 48/s. It achieves a good
balance between real-time and accuracy, can optimize edge segmentation, and has
a better performance in complex scenes. Good robustness is suitable for
practical application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answering in 3D Real-World Scenes. (arXiv:2112.11691v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11691">
<div class="article-summary-box-inner">
<span><p>3D scene understanding is a relatively emerging research field. In this
paper, we introduce the Visual Question Answering task in 3D real-world scenes
(VQA-3D), which aims to answer all possible questions given a 3D scene. To
tackle this problem, the first VQA-3D dataset, namely CLEVR3D, is proposed,
which contains 60K questions in 1,129 real-world scenes. Specifically, we
develop a question engine leveraging 3D scene graph structures to generate
diverse reasoning questions, covering the questions of objects' attributes
(i.e., size, color, and material) and their spatial relationships. Built upon
this dataset, we further design the first VQA-3D baseline model, TransVQA3D.
The TransVQA3D model adopts well-designed Transformer architectures to achieve
superior VQA-3D performance, compared with the pure language baseline and
previous 3D reasoning methods directly applied to 3D scenarios. Experimental
results verify that taking VQA-3D as an auxiliary task can boost the
performance of 3D scene understanding, including scene graph analysis for the
node-wise classification and whole-graph recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense anomaly detection by robust learning on synthetic negative data. (arXiv:2112.12833v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12833">
<div class="article-summary-box-inner">
<span><p>Standard machine learning is unable to accommodate inputs which do not belong
to the training distribution. The resulting models often give rise to confident
incorrect predictions which may lead to devastating consequences. This problem
is especially demanding in the context of dense prediction since input images
may be partially anomalous. Previous work has addressed dense anomaly detection
by discriminative training on mixed-content images. We extend this approach
with synthetic negative patches which simultaneously achieve high inlier
likelihood and uniform discriminative prediction. We generate synthetic
negatives with normalizing flows due to their outstanding distribution coverage
and capability to generate samples at different resolutions. We also propose to
detect anomalies according to a principled information-theoretic criterion
which can be consistently applied through training and inference. The resulting
models set the new state of the art on standard benchmarks and datasets in
spite of minimal computational overhead and refraining from auxiliary negative
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12970">
<div class="article-summary-box-inner">
<span><p>Scene Graph Generation (SGG) remains a challenging visual understanding task
due to its complex compositional property. Most previous works adopt a
bottom-up two-stage or a point-based one-stage approach, which often suffers
from overhead time complexity or sub-optimal design assumption. In this work,
we propose a novel SGG method to address the aforementioned issues, which
formulates the task as a bipartite graph construction problem. To solve the
problem, we develop a transformer-based end-to-end framework that first
generates the entity and predicate proposal set, followed by inferring directed
edges to form the relation triplets. In particular, we develop a new
entity-aware predicate representation based on a structural predicate generator
to leverage the compositional property of relationships. Moreover, we design a
graph assembling module to infer the connectivity of the bipartite scene graph
based on our entity-aware structure, enabling us to generate the scene graph in
an end-to-end manner. Extensive experimental results show that our design is
able to achieve the state-of-the-art or comparable performance on two
challenging benchmarks, surpassing most of the existing approaches and enjoying
higher efficiency in inference. We hope our model can serve as a strong
baseline for the Transformer-based scene graph generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human View Synthesis using a Single Sparse RGB-D Input. (arXiv:2112.13889v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13889">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis for humans in motion is a challenging computer vision
problem that enables applications such as free-viewpoint video. Existing
methods typically use complex setups with multiple input views, 3D supervision,
or pre-trained models that do not generalize well to new identities. Aiming to
address these limitations, we present a novel view synthesis framework to
generate realistic renders from unseen views of any human captured from a
single-view sensor with sparse RGB-D, similar to a low-cost depth camera, and
without actor-specific models. We propose an architecture to learn dense
features in novel views obtained by sphere-based neural rendering, and create
complete renders using a global context inpainting model. Additionally, an
enhancer network leverages the overall fidelity, even in occluded areas from
the original view, producing crisp renders with fine details. We show our
method generates high-quality novel views of synthetic and real human actors
given a single sparse RGB-D input. It generalizes to unseen identities, new
poses and faithfully reconstructs facial expressions. Our approach outperforms
prior human view synthesis methods and is robust to different levels of input
sparsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaGraspNet: A Large-Scale Benchmark Dataset for Vision-driven Robotic Grasping via Physics-based Metaverse Synthesis. (arXiv:2112.14663v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14663">
<div class="article-summary-box-inner">
<span><p>There has been increasing interest in smart factories powered by robotics
systems to tackle repetitive, laborious tasks. One impactful yet challenging
task in robotics-powered smart factory applications is robotic grasping: using
robotic arms to grasp objects autonomously in different settings. Robotic
grasping requires a variety of computer vision tasks such as object detection,
segmentation, grasp prediction, pick planning, etc. While significant progress
has been made in leveraging of machine learning for robotic grasping,
particularly with deep learning, a big challenge remains in the need for
large-scale, high-quality RGBD datasets that cover a wide diversity of
scenarios and permutations. To tackle this big, diverse data problem, we are
inspired by the recent rise in the concept of metaverse, which has greatly
closed the gap between virtual worlds and the physical world. Metaverses allow
us to create digital twins of real-world manufacturing scenarios and to
virtually create different scenarios from which large volumes of data can be
generated for training models. In this paper, we present MetaGraspNet: a
large-scale benchmark dataset for vision-driven robotic grasping via
physics-based metaverse synthesis. The proposed dataset contains 100,000 images
and 25 different object types and is split into 5 difficulties to evaluate
object detection and segmentation model performance in different grasping
scenarios. We also propose a new layout-weighted performance metric alongside
the dataset for evaluating object detection and segmentation performance in a
manner that is more appropriate for robotic grasp applications compared to
existing general-purpose performance metrics. Our benchmark dataset is
available open-source on Kaggle, with the first phase consisting of detailed
object detection, segmentation, layout annotations, and a layout-weighted
performance metric script.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Music-to-Dance Generation with Optimal Transport. (arXiv:2112.01806v1 [cs.SD] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01806">
<div class="article-summary-box-inner">
<span><p>Dance choreography for a piece of music is a challenging task, having to be
creative in presenting distinctive stylistic dance elements while taking into
account the musical theme and rhythm. It has been tackled by different
approaches such as similarity retrieval, sequence-to-sequence modeling and
generative adversarial networks, but their generated dance sequences are often
short of motion realism, diversity and music consistency. In this paper, we
propose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning
to generate 3D dance choreographs from music. We introduce an optimal transport
distance for evaluating the authenticity of the generated dance distribution
and a Gromov-Wasserstein distance to measure the correspondence between the
dance distribution and the input music. This gives a well defined and
non-divergent training objective that mitigates the limitation of standard GAN
training which is frequently plagued with instability and divergent generator
loss issues. Extensive experiments demonstrate that our MDOT-Net can synthesize
realistic and diverse dances which achieve an organic unity with the input
music, reflecting the shared intentionality and matching the rhythmic
articulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPViT: Enabling Faster Vision Transformers via Soft Token Pruning. (arXiv:2112.13890v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13890">
<div class="article-summary-box-inner">
<span><p>Recently, Vision Transformer (ViT) has continuously established new
milestones in the computer vision field, while the high computation and memory
cost makes its propagation in industrial production difficult. Pruning, a
traditional model compression paradigm for hardware efficiency, has been widely
applied in various DNN structures. Nevertheless, it stays ambiguous on how to
perform exclusive pruning on the ViT structure. Considering three key points:
the structural characteristics, the internal data pattern of ViTs, and the
related edge device deployment, we leverage the input token sparsity and
propose a computation-aware soft pruning framework, which can be set up on
vanilla Transformers of both flatten and CNN-type structures, such as
Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based
multi-head token selector, which is a lightweight module for adaptive
instance-wise token selection. We further introduce a soft pruning technique,
which integrates the less informative tokens generated by the selector module
into a package token that will participate in subsequent calculations rather
than being completely discarded. Our framework is bound to the trade-off
between accuracy and computation constraints of specific edge devices through
our proposed computation-aware training strategy. Experimental results show
that our framework significantly reduces the computation cost of ViTs while
maintaining comparable performance on image classification. Moreover, our
framework can guarantee the identified model to meet resource specifications of
mobile devices and FPGA, and even achieve the real-time execution of DeiT-T on
mobile platforms. For example, our method reduces the latency of DeiT-T to 26
ms (26%$\sim $41% superior to existing works) on the mobile device with
0.25%$\sim $4% higher top-1 accuracy on ImageNet. Our code will be released
soon.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-03 23:07:08.605424434 UTC">2022-01-03 23:07:08 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>