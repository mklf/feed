<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-19T01:30:00Z">04-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07580">
<div class="article-summary-box-inner">
<span><p>Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In-BoXBART: Get Instructions into Biomedical Multi-Task Learning. (arXiv:2204.07600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07600">
<div class="article-summary-box-inner">
<span><p>Single-task models have proven pivotal in solving specific tasks; however,
they have limitations in real-world applications where multi-tasking is
necessary and domain shifts are exhibited. Recently, instructional prompts have
shown significant improvement towards multi-task generalization; however, the
effect of instructional prompts and Multi-Task Learning (MTL) has not been
systematically studied in the biomedical domain. Motivated by this, this paper
explores the impact of instructional prompts for biomedical MTL. We introduce
the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X)
various categories. Using this meta-dataset, we propose a unified model termed
In-BoXBART, that can jointly learn all tasks of the BoX without any
task-specific modules. To the best of our knowledge, this is the first attempt
to propose a unified model in the biomedical domain and use instructions to
achieve generalization across several biomedical tasks. Experimental results
indicate that the proposed model: 1) outperforms the single-task baseline by
~3% and multi-task (without instruction) baseline by ~18% on an average, and 2)
shows ~23% improvement compared to the single-task baseline in few-shot
learning (i.e., 32 instances per task) on an average. Our analysis indicates
that there is significant room for improvement across tasks in the BoX,
implying the scope for future research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Adapt Domain Shifts of Moral Values via Instance Weighting. (arXiv:2204.07603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07603">
<div class="article-summary-box-inner">
<span><p>Classifying moral values in user-generated text from social media is critical
in understanding community cultures and interpreting user behaviors of social
movements. Moral values and language usage can change across the social
movements; however, text classifiers are usually trained in source domains of
existing social movements and tested in target domains of new social issues
without considering the variations. In this study, we examine domain shifts of
moral values and language usage, quantify the effects of domain shifts on the
morality classification task, and propose a neural adaptation framework via
instance weighting to improve cross-domain classification tasks. The
quantification analysis suggests a strong correlation between morality shifts,
language usage, and classification performance. We evaluate the neural
adaptation framework on a public Twitter data across 7 social movements and
gain classification improvements up to 12.1\%. Finally, we release a new data
of the COVID-19 vaccine labeled with moral values and evaluate our approach on
the new target domain. For the case study of the COVID-19 vaccine, our
adaptation framework achieves up to 5.26\% improvements over neural baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07657">
<div class="article-summary-box-inner">
<span><p>Sepsis is a life-threatening condition with organ dysfunction and is a
leading cause of death and critical illness worldwide. Accurate detection of
sepsis during emergency department triage would allow early initiation of lab
analysis, antibiotic administration, and other sepsis treatment protocols. The
purpose of this study was to determine whether EHR data can be extracted and
synthesized with the latest machine learning algorithms (KATE Sepsis) and
clinical natural language processing to produce accurate sepsis models, and
compare KATE Sepsis performance with existing sepsis screening protocols, such
as SIRS and qSOFA. A machine learning model (KATE Sepsis) was developed using
patient encounters with triage data from 16 participating hospitals. KATE
Sepsis, SIRS, standard screening (SIRS with source of infection) and qSOFA were
tested in three settings. Cohort-A was a retrospective analysis on medical
records from a single Site 1. Cohort-B was a prospective analysis of Site 1.
Cohort-C was a retrospective analysis on Site 1 with 15 additional sites.
Across all cohorts, KATE Sepsis demonstrates an AUC of 0.94-0.963 with
73-74.87% TPR and 3.76-7.17% FPR. Standard screening demonstrates an AUC of
0.682-0.726 with 39.39-51.19% TPR and 2.9-6.02% FPR. The qSOFA protocol
demonstrates an AUC of 0.544-0.56, with 10.52-13.18% TPR and 1.22-1.68% FPR.
For severe sepsis, across all cohorts, KATE Sepsis demonstrates an AUC of
0.935-0.972 with 70-82.26% TPR and 4.64-8.62% FPR. For septic shock, across all
cohorts, KATE Sepsis demonstrates an AUC of 0.96-0.981 with 85.71-89.66% TPR
and 4.85-8.8% FPR. SIRS, standard screening, and qSOFA demonstrate low AUC and
TPR for severe sepsis and septic shock detection. KATE Sepsis provided
substantially better sepsis detection performance in triage than commonly used
screening protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection. (arXiv:2204.07660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07660">
<div class="article-summary-box-inner">
<span><p>Datasets that capture the connection between vision, language, and affection
are limited, causing a lack of understanding of the emotional aspect of human
intelligence. As a step in this direction, the ArtEmis dataset was recently
introduced as a large-scale dataset of emotional reactions to images along with
language explanations of these chosen emotions. We observed a significant
emotional bias towards instance-rich emotions, making trained neural speakers
less accurate in describing under-represented emotions. We show that collecting
new data, in the same way, is not effective in mitigating this emotional bias.
To remedy this problem, we propose a contrastive data collection approach to
balance ArtEmis with a new complementary dataset such that a pair of similar
images have contrasting emotions (one positive and one negative). We collected
260,533 instances using the proposed method, we combine them with ArtEmis,
creating a second iteration of the dataset. The new combined dataset, dubbed
ArtEmis v2.0, has a balanced distribution of emotions with explanations
revealing more fine details in the associated painting. Our experiments show
that neural speakers trained on the new dataset improve CIDEr and METEOR
evaluation metrics by 20% and 7%, respectively, compared to the biased dataset.
Finally, we also show that the performance per emotion of neural speakers is
improved across all the emotion categories, significantly on under-represented
emotions. The collected dataset and code are available at
https://artemisdataset-v2.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairly Accurate: Learning Optimal Accuracy vs. Fairness Tradeoffs for Hate Speech Detection. (arXiv:2204.07661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07661">
<div class="article-summary-box-inner">
<span><p>Recent work has emphasized the importance of balancing competing objectives
in model training (e.g., accuracy vs. fairness, or competing measures of
fairness). Such trade-offs reflect a broader class of multi-objective
optimization (MOO) problems in which optimization methods seek Pareto optimal
trade-offs between competing goals. In this work, we first introduce a
differentiable measure that enables direct optimization of group fairness
(specifically, balancing accuracy across groups) in model training. Next, we
demonstrate two model-agnostic MOO frameworks for learning Pareto optimal
parameterizations over different groups of neural classification models. We
evaluate our methods on the specific task of hate speech detection, in which
prior work has shown lack of group fairness across speakers of different
English dialects. Empirical results across convolutional, sequential, and
transformer-based neural architectures show superior empirical accuracy vs.
fairness trade-offs over prior work. More significantly, our measure enables
the Pareto machinery to ensure that each architecture achieves the best
possible trade-off between fairness and accuracy w.r.t. the dataset, given
user-prescribed error tolerance bounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spanish Abstract Meaning Representation: Annotation of a General Corpus. (arXiv:2204.07663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07663">
<div class="article-summary-box-inner">
<span><p>The Abstract Meaning Representation (AMR) formalism, designed originally for
English, has been adapted to a number of languages. We build on previous work
proposing the annotation of AMR in Spanish, which resulted in the release of 50
Spanish AMR annotations for the fictional text "The Little Prince." In this
work, we present the first sizable, general annotation project for Spanish
Abstract Meaning Representation. Our approach to annotation makes use of
Spanish rolesets from the AnCora-Net lexicon and extends English AMR with
semantic features specific to Spanish. In addition to our guidelines, we
release an annotated corpus (586 annotations total, for 486 unique sentences)
of multiple genres of documents from the "Abstract Meaning Representation 2.0 -
Four Translations" sembank. This corpus is commonly used for evaluation of AMR
parsing and generation, but does not include gold AMRs; we hope that providing
gold annotations for this dataset can result in a more complete approach to
cross-lingual AMR parsing. Finally, we perform a disagreement analysis and
discuss the implications of our work on the adaptability of AMR to languages
other than English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Fine-tune Twice: Selective Differential Privacy for Large Language Models. (arXiv:2204.07667v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07667">
<div class="article-summary-box-inner">
<span><p>With the increasing adoption of NLP models in real-world products, it becomes
more and more important to protect these models from privacy leakage. Because
private information in language data is sparse, previous research formalized a
Selective-Differential-Privacy (SDP) notion to provide protection for sensitive
tokens detected by policy functions, and prove its effectiveness on RNN-based
models. But the previous mechanism requires separating the private and public
model parameters and thus cannot be applied on large attention-based models. In
this paper, we propose a simple yet effective just-fine-tune-twice privacy
mechanism to first fine-tune on in-domain redacted data and then on in-domain
private data, to achieve SDP for large Transformer-based language models. We
also design explicit and contextual policy functions to provide protections at
different levels. Experiments show that our models achieve strong performance
while staying robust to the canary insertion attack. We further show that even
under low-resource settings with a small amount of in-domain data, SDP can
still improve the model utility. We will release the code, data and models to
facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CILDA: Contrastive Data Augmentation using Intermediate Layer Knowledge Distillation. (arXiv:2204.07674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07674">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is an efficient framework for compressing
large-scale pre-trained language models. Recent years have seen a surge of
research aiming to improve KD by leveraging Contrastive Learning, Intermediate
Layer Distillation, Data Augmentation, and Adversarial Training. In this work,
we propose a learning based data augmentation technique tailored for knowledge
distillation, called CILDA. To the best of our knowledge, this is the first
time that intermediate layer representations of the main task are used in
improving the quality of augmented samples. More precisely, we introduce an
augmentation technique for KD based on intermediate layer matching using
contrastive loss to improve masked adversarial data augmentation. CILDA
outperforms existing state-of-the-art KD approaches on the GLUE benchmark, as
well as in an out-of-domain evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation. (arXiv:2204.07675v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07675">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have demonstrated superior performance in various
natural language processing tasks. However, these models usually contain
hundreds of millions of parameters, which limits their practicality because of
latency requirements in real-world applications. Existing methods train small
compressed models via knowledge distillation. However, performance of these
small models drops significantly compared with the pre-trained models due to
their reduced model capacity. We propose MoEBERT, which uses a
Mixture-of-Experts structure to increase model capacity and inference speed. We
initialize MoEBERT by adapting the feed-forward neural networks in a
pre-trained model into multiple experts. As such, representation power of the
pre-trained model is largely retained. During inference, only one of the
experts is activated, such that speed can be improved. We also propose a
layer-wise distillation method to train MoEBERT. We validate the efficiency and
effectiveness of MoEBERT on natural language understanding and question
answering tasks. Results show that the proposed method outperforms existing
task-specific distillation algorithms. For example, our method outperforms
previous approaches by over 2% on the MNLI (mismatched) dataset. Our code is
publicly available at https://github.com/SimiaoZuo/MoEBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialAug: Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling. (arXiv:2204.07679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07679">
<div class="article-summary-box-inner">
<span><p>Retrieval-based conversational systems learn to rank response candidates for
a given dialogue context by computing the similarity between their vector
representations. However, training on a single textual form of the multi-turn
context limits the ability of a model to learn representations that generalize
to natural perturbations seen during inference. In this paper we propose a
framework that incorporates augmented versions of a dialogue context into the
learning objective. We utilize contrastive learning as an auxiliary objective
to learn robust dialogue context representations that are invariant to
perturbations injected through the augmentation method. We experiment with four
benchmark dialogue datasets and demonstrate that our framework combines well
with existing augmentation methods and can significantly improve over baseline
BERT-based ranking architectures. Furthermore, we propose a novel data
augmentation method, ConMix, that adds token level perturbations through
stochastic mixing of tokens from other contexts in the batch. We show that our
proposed augmentation method outperforms previous data augmentation approaches,
and provides dialogue representations that are more robust to common
perturbations seen during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners. (arXiv:2204.07689v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07689">
<div class="article-summary-box-inner">
<span><p>Traditional multi-task learning (MTL) methods use dense networks that use the
same set of shared weights across several different tasks. This often creates
interference where two or more tasks compete to pull model parameters in
different directions. In this work, we study whether sparsely activated
Mixture-of-Experts (MoE) improve multi-task learning by specializing some
weights for learning shared representations and using the others for learning
task-specific information. To this end, we devise task-aware gating functions
to route examples from different tasks to specialized experts which share
subsets of network weights conditioned on the task. This results in a sparsely
activated multi-task model with a large number of parameters, but with the same
computational cost as that of a dense model. We demonstrate such sparse
networks to improve multi-task learning along three key dimensions: (i)
transfer to low-resource tasks from related tasks in the training mixture; (ii)
sample-efficient generalization to tasks not seen during training by making use
of task-aware routing from seen related tasks; (iii) robustness to the addition
of unrelated tasks by avoiding catastrophic forgetting of existing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrating Trust of Multi-Hop Question Answering Systems with Decompositional Probes. (arXiv:2204.07693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07693">
<div class="article-summary-box-inner">
<span><p>Multi-hop Question Answering (QA) is a challenging task since it requires an
accurate aggregation of information from multiple context paragraphs and a
thorough understanding of the underlying reasoning chains. Recent work in
multi-hop QA has shown that performance can be boosted by first decomposing the
questions into simpler, single-hop questions. In this paper, we explore one
additional utility of the multi-hop decomposition from the perspective of
explainable NLP: to create explanation by probing a neural QA model with them.
We hypothesize that in doing so, users will be better able to construct a
mental model of when the underlying QA system will give the correct answer.
Through human participant studies, we verify that exposing the decomposition
probes and answers to the probes to users can increase their ability to predict
system performance on a question instance basis. We show that decomposition is
an effective form of probing QA systems as well as a promising approach to
explanation generation. In-depth analyses show the need for improvements in
decomposition systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Reinforcement Learning for Unsupervised Controlled Text Generation. (arXiv:2204.07696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07696">
<div class="article-summary-box-inner">
<span><p>Controlled text generation tasks such as unsupervised text style transfer
have increasingly adopted the use of Reinforcement Learning (RL). A major
challenge in applying RL to such tasks is the sparse reward, which is available
only after the full text is generated. Sparse rewards, combined with a large
action space make RL training sample-inefficient and difficult to converge.
Recently proposed reward-shaping strategies to address this issue have shown
only negligible gains. In contrast, this work proposes a novel approach that
provides dense rewards to each generated token. We evaluate our approach by its
usage in unsupervised text style transfer. Averaged across datasets, our style
transfer system improves upon current state-of-art systems by 21\% on human
evaluation and 12\% on automatic evaluation. Upon ablated comparison with the
current reward shaping approach (the `roll-out strategy'), using dense rewards
improves the overall style transfer quality by 22\% based on human evaluation.
Further the RL training is 2.5 times as sample efficient, and 7 times faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLCU-ICALL at SemEval-2022 Task 1: Cross-Attention Multitasking Framework for Definition Modeling. (arXiv:2204.07701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07701">
<div class="article-summary-box-inner">
<span><p>This paper describes the BLCU-ICALL system used in the SemEval-2022 Task 1
Comparing Dictionaries and Word Embeddings, the Definition Modeling subtrack,
achieving 1st on Italian, 2nd on Spanish and Russian, and 3rd on English and
French. We propose a transformer-based multitasking framework to explore the
task. The framework integrates multiple embedding architectures through the
cross-attention mechanism, and captures the structure of glosses through a
masking language model objective. Additionally, we also investigate a simple
but effective model ensembling strategy to further improve the robustness. The
evaluation results show the effectiveness of our solution. We release our code
at: https://github.com/blcuicall/SemEval2022-Task1-DM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks. (arXiv:2204.07705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07705">
<div class="article-summary-box-inner">
<span><p>How can we measure the generalization of models to a variety of unseen tasks
when provided with their language instructions? To facilitate progress in this
goal, we introduce Natural-Instructions v2, a collection of 1,600+ diverse
language tasks and their expert written instructions. More importantly, the
benchmark covers 70+ distinct task types, such as tagging, in-filling, and
rewriting. This benchmark is collected with contributions of NLP practitioners
in the community and through an iterative peer review process to ensure their
quality. This benchmark enables large-scale evaluation of cross-task
generalization of the models -- training on a subset of tasks and evaluating on
the remaining unseen ones. For instance, we are able to rigorously quantify
generalization as a function of various scaling parameters, such as the number
of observed tasks, the number of instances, and model sizes. As a by-product of
these experiments. we introduce Tk-Instruct, an encoder-decoder Transformer
that is trained to follow a variety of in-context instructions (plain language
task definitions or k-shot examples) which outperforms existing larger models
on our benchmark. We hope this benchmark facilitates future progress toward
more general-purpose language understanding models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVShowGuess: Character Comprehension in Stories as Speaker Guessing. (arXiv:2204.07721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07721">
<div class="article-summary-box-inner">
<span><p>We propose a new task for assessing machines' skills of understanding
fictional characters in narrative stories. The task, TVShowGuess, builds on the
scripts of TV series and takes the form of guessing the anonymous main
characters based on the backgrounds of the scenes and the dialogues. Our human
study supports that this form of task covers comprehension of multiple types of
character persona, including understanding characters' personalities, facts and
memories of personal experience, which are well aligned with the psychological
and literary theories about the theory of mind (ToM) of human beings on
understanding fictional characters during reading. We further propose new model
architectures to support the contextualized encoding of long scene texts.
Experiments show that our proposed approaches significantly outperform
baselines, yet still largely lag behind the (nearly perfect) human performance.
Our work serves as a first step toward the goal of narrative character
comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persua: A Visual Interactive System to Enhance the Persuasiveness of Arguments in Online Discussion. (arXiv:2204.07741v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07741">
<div class="article-summary-box-inner">
<span><p>Persuading people to change their opinions is a common practice in online
discussion forums on topics ranging from political campaigns to relationship
consultation. Enhancing people's ability to write persuasive arguments could
not only practice their critical thinking and reasoning but also contribute to
the effectiveness and civility in online communication. It is, however, not an
easy task in online discussion settings where written words are the primary
communication channel. In this paper, we derived four design goals for a tool
that helps users improve the persuasiveness of arguments in online discussions
through a survey with 123 online forum users and interviews with five debating
experts. To satisfy these design goals, we analyzed and built a labeled dataset
of fine-grained persuasive strategies (i.e., logos, pathos, ethos, and
evidence) in 164 arguments with high ratings on persuasiveness from
ChangeMyView, a popular online discussion forum. We then designed an
interactive visual system, Persua, which provides example-based guidance on
persuasive strategies to enhance the persuasiveness of arguments. In
particular, the system constructs portfolios of arguments based on different
persuasive strategies applied to a given discussion topic. It then presents
concrete examples based on the difference between the portfolios of user input
and high-quality arguments in the dataset. A between-subjects study shows
suggestive evidence that Persua encourages users to submit more times for
feedback and helps users improve more on the persuasiveness of their arguments
than a baseline system. Finally, a set of design considerations was summarized
to guide future intelligent systems that improve the persuasiveness in text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Attention-based Sentence-Level Meta-Embeddings from Contextualised Language Models. (arXiv:2204.07746v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07746">
<div class="article-summary-box-inner">
<span><p>A variety of contextualised language models have been proposed in the NLP
community, which are trained on diverse corpora to produce numerous Neural
Language Models (NLMs). However, different NLMs have reported different levels
of performances in downstream NLP applications when used as text
representations. We propose a sentence-level meta-embedding learning method
that takes independently trained contextualised word embedding models and
learns a sentence embedding that preserves the complementary strengths of the
input source NLMs. Our proposed method is unsupervised and is not tied to a
particular downstream task, which makes the learnt meta-embeddings in principle
applicable to different tasks that require sentence representations.
Specifically, we first project the token-level embeddings obtained by the
individual NLMs and learn attention weights that indicate the contributions of
source embeddings towards their token-level meta-embeddings. Next, we apply
mean and max pooling to produce sentence-level meta-embeddings from token-level
meta-embeddings. Experimental results on semantic textual similarity benchmarks
show that our proposed unsupervised sentence-level meta-embedding method
outperforms previously proposed sentence-level meta-embedding methods as well
as a supervised baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue. (arXiv:2204.07770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07770">
<div class="article-summary-box-inner">
<span><p>The goal-oriented document-grounded dialogue aims at responding to the user
query based on the dialogue context and supporting document. Existing studies
tackle this problem by decomposing it into two sub-tasks: knowledge
identification and response generation. However, such pipeline methods would
unavoidably suffer from the error propagation issue. This paper proposes to
unify these two sub-tasks via sequentially generating the grounding knowledge
and the response. We further develop a prompt-connected multi-task learning
strategy to model the characteristics and connections of different tasks and
introduce linear temperature scheduling to reduce the negative effect of
irrelevant document information. Experimental results demonstrate the
effectiveness of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TASTEset -- Recipe Dataset and Food Entities Recognition Benchmark. (arXiv:2204.07775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07775">
<div class="article-summary-box-inner">
<span><p>Food Computing is currently a fast-growing field of research. Natural
language processing (NLP) is also increasingly essential in this field,
especially for recognising food entities. However, there are still only a few
well-defined tasks that serve as benchmarks for solutions in this area. We
introduce a new dataset -- called \textit{TASTEset} -- to bridge this gap. In
this dataset, Named Entity Recognition (NER) models are expected to find or
infer various types of entities helpful in processing recipes, e.g.~food
products, quantities and their units, names of cooking processes, physical
quality of ingredients, their purpose, taste.
</p>
<p>The dataset consists of 700 recipes with more than 13,000 entities to
extract. We provide a few state-of-the-art baselines of named entity
recognition models, which show that our dataset poses a solid challenge to
existing models. The best model achieved, on average, 0.95 $F_1$ score,
depending on the entity type -- from 0.781 to 0.982. We share the dataset and
the task to encourage progress on more in-depth and complex information
extraction from recipes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimpleBERT: A Pre-trained Model That Learns to Generate Simple Words. (arXiv:2204.07779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07779">
<div class="article-summary-box-inner">
<span><p>Pre-trained models are widely used in the tasks of natural language
processing nowadays. However, in the specific field of text simplification, the
research on improving pre-trained models is still blank. In this work, we
propose a continued pre-training method for text simplification. Specifically,
we propose a new masked language modeling (MLM) mechanism, which does not
randomly mask words but only masks simple words. The new mechanism can make the
model learn to generate simple words. We use a small-scale simple text dataset
for continued pre-training and employ two methods to identify simple words from
the texts. We choose BERT, a representative pre-trained model, and continue
pre-training it using our proposed method. Finally, we obtain SimpleBERT, which
surpasses BERT in both lexical simplification and sentence simplification tasks
and has achieved state-of-the-art results on multiple datasets. What's more,
SimpleBERT can replace BERT in existing simplification models without
modification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unification of Discourse Annotation Frameworks. (arXiv:2204.07781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07781">
<div class="article-summary-box-inner">
<span><p>Discourse information is difficult to represent and annotate. Among the major
frameworks for annotating discourse information, RST, PDTB and SDRT are widely
discussed and used, each having its own theoretical foundation and focus.
Corpora annotated under different frameworks vary considerably. To make better
use of the existing discourse corpora and achieve the possible synergy of
different frameworks, it is worthwhile to investigate the systematic relations
between different frameworks and devise methods of unifying the frameworks.
Although the issue of framework unification has been a topic of discussion for
a long time, there is currently no comprehensive approach which considers
unifying both discourse structure and discourse relations and evaluates the
unified framework intrinsically and extrinsically. We plan to use automatic
means for the unification task and evaluate the result with structural
complexity and downstream tasks. We will also explore the application of the
unified framework in multi-task learning and graphical models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Hard Negative Entities for Entity Set Expansion. (arXiv:2204.07789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07789">
<div class="article-summary-box-inner">
<span><p>Entity Set Expansion (ESE) is a promising task which aims to expand entities
of the target semantic class described by a small seed entity set. Various NLP
and IR applications will benefit from ESE due to its ability to discover
knowledge. Although previous ESE methods have achieved great progress, most of
them still lack the ability to handle hard negative entities (i.e., entities
that are difficult to distinguish from the target entities), since two entities
may or may not belong to the same semantic class based on different granularity
levels we analyze on. To address this challenge, we devise an entity-level
masked language model with contrastive learning to refine the representation of
entities. In addition, we propose the ProbExpan, a novel probabilistic ESE
framework utilizing the entity representation obtained by the aforementioned
language model to expand entities. Extensive experiments and detailed analyses
on three datasets show that our method outperforms previous state-of-the-art
methods. The source codes of this paper are available at
https://github.com/geekjuruo/ProbExpan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logical Inference for Counting on Semi-structured Tables. (arXiv:2204.07803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07803">
<div class="article-summary-box-inner">
<span><p>Recently, the Natural Language Inference (NLI) task has been studied for
semi-structured tables that do not have a strict format. Although neural
approaches have achieved high performance in various types of NLI, including
NLI between semi-structured tables and texts, they still have difficulty in
performing a numerical type of inference, such as counting. To handle a
numerical type of inference, we propose a logical inference system for
reasoning between semi-structured tables and texts. We use logical
representations as meaning representations for tables and texts and use model
checking to handle a numerical type of inference between texts and tables. To
evaluate the extent to which our system can perform inference with numerical
comparatives, we make an evaluation protocol that focuses on numerical
understanding between semi-structured tables and texts in English. We show that
our system can more robustly perform inference between tables and texts that
requires numerical understanding compared with current neural approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Classify Open Intent via Soft Labeling and Manifold Mixup. (arXiv:2204.07804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07804">
<div class="article-summary-box-inner">
<span><p>Open intent classification is a practical yet challenging task in dialogue
systems. Its objective is to accurately classify samples of known intents while
at the same time detecting those of open (unknown) intents. Existing methods
usually use outlier detection algorithms combined with K-class classifier to
detect open intents, where K represents the class number of known intents.
Different from them, in this paper, we consider another way without using
outlier detection algorithms. Specifically, we directly train a (K+1)-class
classifier for open intent classification, where the (K+1)-th class represents
open intents. To address the challenge that training a (K+1)-class classifier
with training samples of only K classes, we propose a deep model based on Soft
Labeling and Manifold Mixup (SLMM). In our method, soft labeling is used to
reshape the label distribution of the known intent samples, aiming at reducing
model's overconfident on known intents. Manifold mixup is used to generate
pseudo samples for open intents, aiming at well optimizing the decision
boundary of open intents. Experiments on four benchmark datasets demonstrate
that our method outperforms previous methods and achieves state-of-the-art
performance. All the code and data of this work can be obtained at
https://github.com/zifengcheng/SLMM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Contrastive Cross-Channel Data Augmentation Framework for Aspect-based Sentiment Analysis. (arXiv:2204.07832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07832">
<div class="article-summary-box-inner">
<span><p>Aspect-Based Sentiment Analysis is a fine-grained sentiment analysis task,
which focuses on detecting the sentiment polarity towards the aspect in a
sentence. However, it is always sensitive to the multi-aspect challenge, where
features of multiple aspects in a sentence will affect each other. To mitigate
this issue, we design a novel training framework, called Contrastive
Cross-Channel Data Augmentation (C3DA). A source sentence will be fed a
domain-specific generator to obtain some synthetic sentences and is
concatenated with these generated sentences to conduct supervised training and
proposed contrastive training. To be specific, considering the limited ABSA
labeled data, we also introduce some parameter-efficient approaches to complete
sentences generation. This novel generation method consists of an Aspect
Augmentation Channel (AAC) to generate aspect-specific sentences and a Polarity
Augmentation (PAC) to generate polarity-inverted sentences. According to our
extensive experiments, our C3DA framework can outperform those baselines
without any augmentations by about 1\% on accuracy and Macro-F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Cross-Lingual Gaps During Leveraging the Multilingual Sequence-to-Sequence Pretraining for Text Generation. (arXiv:2204.07834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07834">
<div class="article-summary-box-inner">
<span><p>For multilingual sequence-to-sequence pretrained language models
(multilingual Seq2Seq PLMs), e.g. mBART, the self-supervised pretraining task
is trained on a wide range of monolingual languages, e.g. 25 languages from
commoncrawl, while the downstream cross-lingual tasks generally progress on a
bilingual language subset, e.g. English-German, making there exists the
cross-lingual data discrepancy, namely \textit{domain discrepancy}, and
cross-lingual learning objective discrepancy, namely \textit{task discrepancy},
between the pretrain and finetune stages. To bridge the above cross-lingual
domain and task gaps, we extend the vanilla pretrain-finetune pipeline with
extra code-switching restore task. Specifically, the first stage employs the
self-supervised code-switching restore task as a pretext task, allowing the
multilingual Seq2Seq PLM to acquire some in-domain alignment information. And
for the second stage, we continuously fine-tune the model on labeled data
normally. Experiments on a variety of cross-lingual NLG tasks, including 12
bilingual translation tasks, 36 zero-shot translation tasks, and cross-lingual
summarization tasks show our model outperforms the strong baseline mBART
consistently. Comprehensive analyses indicate our approach could narrow the
cross-lingual sentence representation distance and improve low-frequency word
translation with trivial computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What If: Generating Code to Answer Simulation Questions. (arXiv:2204.07835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07835">
<div class="article-summary-box-inner">
<span><p>Many texts, especially in chemistry and biology, describe complex processes.
We focus on texts that describe a chemical reaction process and questions that
ask about the process's outcome under different environmental conditions. To
answer questions about such processes, one needs to understand the interactions
between the different entities involved in the process and to simulate their
state transitions during the process execution under different conditions. A
state transition is defined as the memory modification the program does to the
variables during the execution. We hypothesize that generating code and
executing it to simulate the process will allow answering such questions. We,
therefore, define a domain-specific language (DSL) to represent processes. We
contribute to the community a unique dataset curated by chemists and annotated
by computer scientists. The dataset is composed of process texts, simulation
questions, and their corresponding computer codes represented by the DSL.We
propose a neural program synthesis approach based on reinforcement learning
with a novel state-transition semantic reward. The novel reward is based on the
run-time semantic similarity between the predicted code and the reference code.
This allows simulating complex process transitions and thus answering
simulation questions. Our approach yields a significant boost in accuracy for
simulation questions: 88\% accuracy as opposed to 83\% accuracy of the
state-of-the-art neural program synthesis approaches and 54\% accuracy of
state-of-the-art end-to-end text-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLISS: Robust Sequence-to-Sequence Learning via Self-Supervised Input Representation. (arXiv:2204.07837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07837">
<div class="article-summary-box-inner">
<span><p>Data augmentations (DA) are the cores to achieving robust
sequence-to-sequence learning on various natural language processing (NLP)
tasks. However, most of the DA approaches force the decoder to make predictions
conditioned on the perturbed input representation, underutilizing supervised
information provided by perturbed input. In this work, we propose a
framework-level robust sequence-to-sequence learning approach, named BLISS, via
self-supervised input representation, which has the great potential to
complement the data-level augmentation approaches. The key idea is to supervise
the sequence-to-sequence framework with both the \textit{supervised}
("input$\rightarrow$output") and \textit{self-supervised} ("perturbed
input$\rightarrow$input") information. We conduct comprehensive experiments to
validate the effectiveness of BLISS on various tasks, including machine
translation, grammatical error correction, and text summarization. The results
show that BLISS outperforms significantly the vanilla Transformer and
consistently works well across tasks than the other five contrastive baselines.
Extensive analyses reveal that BLISS learns robust representations and rich
linguistic knowledge, confirming our claim. Source code will be released upon
publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STRATA: Word Boundaries & Phoneme Recognition From Continuous Urdu Speech using Transfer Learning, Attention, & Data Augmentation. (arXiv:2204.07848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07848">
<div class="article-summary-box-inner">
<span><p>Phoneme recognition is a largely unsolved problem in NLP, especially for
low-resource languages like Urdu. The systems that try to extract the phonemes
from audio speech require hand-labeled phonetic transcriptions. This requires
expert linguists to annotate speech data with its relevant phonetic
representation which is both an expensive and a tedious task. In this paper, we
propose STRATA, a framework for supervised phoneme recognition that overcomes
the data scarcity issue for low resource languages using a seq2seq neural
architecture integrated with transfer learning, attention mechanism, and data
augmentation. STRATA employs transfer learning to reduce the network loss in
half. It uses attention mechanism for word boundaries and frame alignment
detection which further reduces the network loss by 4% and is able to identify
the word boundaries with 92.2% accuracy. STRATA uses various data augmentation
techniques to further reduce the loss by 1.5% and is more robust towards new
signals both in terms of generalization and accuracy. STRATA is able to achieve
a Phoneme Error Rate of 16.5% and improves upon the state of the art by 1.1%
for TIMIT dataset (English) and 11.5% for CSaLT dataset (Urdu).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVIBOT: A Smart Chatbot for Assistance and E-Awareness during COVID-19 Pandemic. (arXiv:2204.07851v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07851">
<div class="article-summary-box-inner">
<span><p>The coronavirus pandemic has spread over the past two years in our highly
connected and information-dense society. Nonetheless, disseminating accurate
and up-to-date information on the spread of this pandemic remains a challenge.
In this context, opting for a solution based on conversational artificial
intelligence, also known under the name of the chatbot, is proving to be an
unavoidable solution, especially since it has already shown its effectiveness
in fighting the coronavirus crisis in several countries. This work proposes to
design and implement a smart chatbot on the theme of COVID-19, called COVIBOT,
which will be useful in the context of Saudi Arabia. COVIBOT is a
generative-based contextual chatbot, which is built using machine learning APIs
that are offered by the cloud-based Azure Cognitive Services. Two versions of
COVIBOT are offered: English and Arabic versions. Use cases of COVIBOT are
tested and validated using a scenario-based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nigam@COLIEE-22: Legal Case Retrieval and Entailment using Cascading of Lexical and Semantic-based models. (arXiv:2204.07853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07853">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the Competition on Legal Information
Extraction/Entailment 2022 (COLIEE-2022) workshop on case law competition for
tasks 1 and 2. Task 1 is a legal case retrieval task, which involves reading a
new case and extracting supporting cases from the provided case law corpus to
support the decision. Task 2 is the legal case entailment task, which involves
the identification of a paragraph from existing cases that entails the decision
in a relevant case. We employed the neural models Sentence-BERT and Sent2Vec
for semantic understanding and the traditional retrieval model BM25 for exact
matching in both tasks. As a result, our team ("nigam") ranked 5th among all
the teams in Tasks 1 and 2. Experimental results indicate that the traditional
retrieval model BM25 still outperforms neural network-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?. (arXiv:2204.07931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07931">
<div class="article-summary-box-inner">
<span><p>Knowledge-grounded conversational models are known to suffer from producing
factually invalid statements, a phenomenon commonly called hallucination. In
this work, we investigate the underlying causes of this phenomenon: is
hallucination due to the training data, or to the models? We conduct a
comprehensive human study on both existing knowledge-grounded conversational
benchmarks and several state-of-the-art models. Our study reveals that the
standard benchmarks consist of &gt;60% hallucinated responses, leading to models
that not only hallucinate but even amplify hallucinations. Our findings raise
important questions on the quality of existing datasets and models trained
using them. We make our annotations publicly available for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Cross-Task Generalization via Retrieval Augmentation. (arXiv:2204.07937v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07937">
<div class="article-summary-box-inner">
<span><p>Humans can perform unseen tasks by recalling relevant skills that are
acquired previously and then generalizing them to the target tasks, even if
there is no supervision at all. In this paper, we aim to improve such
cross-task generalization ability of massive multi-task language models such as
T0 (Sanh et al., 2021) in an unsupervised setting. We propose a
retrieval-augmentation method named ReCross that takes a few unlabelled
examples as queries to retrieve a small subset of upstream data and uses them
to update the multi-task model for better generalization. Our empirical results
show that the proposed ReCross consistently outperforms non-retrieval baselines
by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07955">
<div class="article-summary-box-inner">
<span><p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment
Analysis (MABSA) has attracted increasing attention in recent years. However,
previous approaches either (i) use separately pre-trained visual and textual
models, which ignore the crossmodal alignment or (ii) use vision-language
models pre-trained with general pre-training tasks, which are inadequate to
identify finegrained aspects, opinions, and their alignments across modalities.
To tackle these limitations, we propose a task-specific Vision-Language
Pre-training framework for MABSA (VLPMABSA), which is a unified multimodal
encoder-decoder architecture for all the pretraining and downstream tasks. We
further design three types of task-specific pre-training tasks from the
language, vision, and multimodal modalities, respectively. Experimental results
show that our approach generally outperforms the state-of-the-art approaches on
three MABSA subtasks. Further analysis demonstrates the effectiveness of each
pretraining task. The source code is publicly released at
https://github.com/NUSTM/VLP-MABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED. (arXiv:2204.07980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07980">
<div class="article-summary-box-inner">
<span><p>DocRED is a widely used dataset for document-level relation extraction. In
the large-scale annotation, a \textit{recommend-revise} scheme is adopted to
reduce the workload. Within this scheme, annotators are provided with candidate
relation instances from distant supervision, and they then manually supplement
and remove relational facts based on the recommendations. However, when
comparing DocRED with a subset relabeled from scratch, we find that this scheme
results in a considerable amount of false negative samples and an obvious bias
towards popular entities and relations. Furthermore, we observe that the models
trained on DocRED have low recall on our relabeled dataset and inherit the same
bias in the training data. Through the analysis of annotators' behaviors, we
figure out the underlying reason for the problems above: the scheme actually
discourages annotators from supplementing adequate instances in the revision
phase. We appeal to future research to take into consideration the issues with
the recommend-revise scheme when designing new models and annotation schemes.
The relabeled dataset is released at
\url{https://github.com/AndrewZhe/Revisit-DocRED}, to serve as a more reliable
test set of document RE models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Effectively Learning of Knowledge in Continual Pre-training. (arXiv:2204.07994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07994">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) like BERT have made significant progress
in various downstream NLP tasks. However, by asking models to do cloze-style
tests, recent work finds that PLMs are short in acquiring knowledge from
unstructured text. To understand the internal behaviour of PLMs in retrieving
knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free
(K-F) tokens for unstructured text and ask professional annotators to label
some samples manually. Then, we find that PLMs are more likely to give wrong
predictions on K-B tokens and attend less attention to those tokens inside the
self-attention module. Based on these observations, we develop two solutions to
help the model learn more knowledge from unstructured text in a fully
self-supervised manner. Experiments on knowledge-intensive tasks show the
effectiveness of the proposed methods. To our best knowledge, we are the first
to explore fully self-supervised learning of knowledge in continual
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Named Entity Recognition as Holistic Structure Parsing. (arXiv:2204.08006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08006">
<div class="article-summary-box-inner">
<span><p>As a fundamental natural language processing task and one of core knowledge
extraction techniques, named entity recognition (NER) is widely used to extract
information from texts for downstream tasks. Nested NER is a branch of NER in
which the named entities (NEs) are nested with each other. However, most of the
previous studies on nested NER usually apply linear structure to model the
nested NEs which are actually accommodated in a hierarchical structure. Thus in
order to address this mismatch, this work models the full nested NEs in a
sentence as a holistic structure, then we propose a holistic structure parsing
algorithm to disclose the entire NEs once for all. Besides, there is no
research on applying corpus-level information to NER currently. To make up for
the loss of this information, we introduce Point-wise Mutual Information (PMI)
and other frequency features from corpus-aware statistics for even better
performance by holistic modeling from sentence-level to corpus-level.
Experiments show that our model yields promising results on widely-used
benchmarks which approach or even achieve state-of-the-art. Further empirical
studies show that our proposed corpus-aware features can substantially improve
NER domain adaptation, which demonstrates the surprising advantage of our
proposed corpus-level holistic structure modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiOmnia: generative QA corpus on the whole Russian Wikipedia. (arXiv:2204.08009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08009">
<div class="article-summary-box-inner">
<span><p>The General QA field has been developing the methodology referencing the
Stanford Question answering dataset (SQuAD) as the significant benchmark.
However, compiling factual questions is accompanied by time- and
labour-consuming annotation, limiting the training data's potential size. We
present the WikiOmnia dataset, a new publicly available set of QA-pairs and
corresponding Russian Wikipedia article summary sections, composed with a fully
automated generative pipeline. The dataset includes every available article
from Wikipedia for the Russian language. The WikiOmnia pipeline is available
open-source and is also tested for creating SQuAD-formatted QA on other
domains, like news texts, fiction, and social media. The resulting dataset
includes two parts: raw data on the whole Russian Wikipedia (7,930,873 QA pairs
with paragraphs for ruGPT-3 XL and 7,991,040 QA pairs with paragraphs for
ruT5-large) and cleaned data with strict automatic verification (over 160,000
QA pairs with paragraphs for ruGPT-3 XL and over 3,400,000 QA pairs with
paragraphs for ruT5-large).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pathologies of Pre-trained Language Models in Few-shot Fine-tuning. (arXiv:2204.08039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08039">
<div class="article-summary-box-inner">
<span><p>Although adapting pre-trained language models with few examples has shown
promising performance on text classification, there is a lack of understanding
of where the performance gain comes from. In this work, we propose to answer
this question by interpreting the adaptation behavior using post-hoc
explanations from model predictions. By modeling feature statistics of
explanations, we discover that (1) without fine-tuning, pre-trained models
(e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although
few-shot fine-tuning can mitigate the prediction bias and demonstrate promising
prediction performance, our analysis shows models gain performance improvement
by capturing non-task-related features (e.g. stop words) or shallow data
patterns (e.g. lexical overlaps). These observations alert that pursuing model
performance with fewer examples may incur pathological prediction behavior,
which requires further sanity check on model predictions and careful design in
model evaluations in few-shot fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Mixed-initiative Conversational Search Systems via User Simulation. (arXiv:2204.08046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08046">
<div class="article-summary-box-inner">
<span><p>Clarifying the underlying user information need by asking clarifying
questions is an important feature of modern conversational search system.
However, evaluation of such systems through answering prompted clarifying
questions requires significant human effort, which can be time-consuming and
expensive. In this paper, we propose a conversational User Simulator, called
USi, for automatic evaluation of such conversational search systems. Given a
description of an information need, USi is capable of automatically answering
clarifying questions about the topic throughout the search session. Through a
set of experiments, including automated natural language generation metrics and
crowdsourcing studies, we show that responses generated by USi are both inline
with the underlying information need and comparable to human-generated answers.
Moreover, we make the first steps towards multi-turn interactions, where
conversational search systems asks multiple questions to the (simulated) user
with a goal of clarifying the user need. To this end, we expand on currently
available datasets for studying clarifying questions, i.e., Qulac and ClariQ,
by performing a crowdsourcing-based multi-turn data acquisition. We show that
our generative, GPT2-based model, is capable of providing accurate and natural
answers to unseen clarifying questions in the single-turn setting and discuss
capabilities of our model in the multi-turn setting. We provide the code, data,
and the pre-trained model to be used for further research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Psycho-linguistic Analysis of BitChute. (arXiv:2204.08078v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08078">
<div class="article-summary-box-inner">
<span><p>In order to better support researchers, journalist, and practitioners in
their use of the MeLa-BitChute dataset for exploration and investigative
reporting, we provide new psycho-linguistic metadata for the videos, comments,
and channels in the dataset using LIWC22. This paper describes that metadata
and methods to filter the data using the metadata. In addition, we provide
basic analysis and comparison of the language on BitChute to other social media
platforms. The MeLa-BitChute dataset and LIWC metadata described in this paper
can be found at:
https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KRD1VS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">\`It\`ak\'ur\`oso: Exploiting Cross-Lingual Transferability for Natural Language Generation of Dialogues in Low-Resource, African Languages. (arXiv:2204.08083v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08083">
<div class="article-summary-box-inner">
<span><p>We investigate the possibility of cross-lingual transfer from a
state-of-the-art (SoTA) deep monolingual model (DialoGPT) to 6 African
languages and compare with 2 baselines (BlenderBot 90M, another SoTA, and a
simple Seq2Seq). The languages are Swahili, Wolof, Hausa, Nigerian Pidgin
English, Kinyarwanda &amp; Yor\`ub\'a. Generation of dialogues is known to be a
challenging task for many reasons. It becomes more challenging for African
languages which are low-resource in terms of data. Therefore, we translate a
small portion of the English multi-domain MultiWOZ dataset for each target
language. Besides intrinsic evaluation (i.e. perplexity), we conduct human
evaluation of single-turn conversations by using majority votes and measure
inter-annotator agreement (IAA). The results show that the hypothesis that deep
monolingual models learn some abstractions that generalise across languages
holds. We observe human-like conversations in 5 out of the 6 languages. It,
however, applies to different degrees in different languages, which is
expected. The language with the most transferable properties is the Nigerian
Pidgin English, with a human-likeness score of 78.1%, of which 34.4% are
unanimous. The main contributions of this paper include the representation
(through the provision of high-quality dialogue data) of under-represented
African languages and demonstrating the cross-lingual transferability
hypothesis for dialogue systems. We also provide the datasets and host the
model checkpoints/demos on the HuggingFace hub for public access.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">kpfriends at SemEval-2022 Task 2: NEAMER -- Named Entity Augmented Multi-word Expression Recognizer. (arXiv:2204.08102v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08102">
<div class="article-summary-box-inner">
<span><p>We present NEAMER -- Named Entity Augmented Multi-word Expression Recognizer.
This system is inspired by non-compositionality characteristics shared between
Named Entity and Idiomatic Expressions. We utilize transfer learning and
locality features to enhance idiom classification task. This system is our
submission for SemEval Task 2: Multilingual Idiomaticity Detection and Sentence
Embedding Subtask A OneShot shared task. We achieve SOTA with F1 0.9395 during
post-evaluation phase. We also observe improvement in training stability.
Lastly, we experiment with non-compositionality knowledge transfer,
cross-lingual fine-tuning and locality features, which we also introduce in
this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monte Carlo Tree Search for Interpreting Stress in Natural Language. (arXiv:2204.08105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08105">
<div class="article-summary-box-inner">
<span><p>Natural language processing can facilitate the analysis of a person's mental
state from text they have written. Previous studies have developed models that
can predict whether a person is experiencing a mental health condition from
social media posts with high accuracy. Yet, these models cannot explain why the
person is experiencing a particular mental state. In this work, we present a
new method for explaining a person's mental state from text using Monte Carlo
tree search (MCTS). Our MCTS algorithm employs trained classification models to
guide the search for key phrases that explain the writer's mental state in a
concise, interpretable manner. Furthermore, our algorithm can find both
explanations that depend on the particular context of the text (e.g., a recent
breakup) and those that are context-independent. Using a dataset of Reddit
posts that exhibit stress, we demonstrate the ability of our MCTS algorithm to
identify interpretable explanations for a person's feeling of stress in both a
context-dependent and context-independent manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering. (arXiv:2204.08109v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08109">
<div class="article-summary-box-inner">
<span><p>Question answering on knowledge bases (KBQA) poses a unique challenge for
semantic parsing research due to two intertwined factors: large search space
and ambiguities in schema linking. The predominant ranking-based KBQA models,
which rely on a candidate enumeration step to reduce the search space, struggle
with flexibility and have impractical online running time. In this paper, we
present ArcaneQA, a novel generation-based model that addresses both the large
search space and schema linking in a unified framework with two mutually
boosting ingredients: we use dynamic program induction to tackle the large
search space and dynamic contextualized encoding to enhance schema linking.
Experiment results on multiple popular KBQA datasets demonstrate the highly
competitive performance of ArcaneQA in both effectiveness and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Models. (arXiv:2204.08110v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08110">
<div class="article-summary-box-inner">
<span><p>English pretrained language models, which make up the backbone of many modern
NLP systems, require huge amounts of unlabeled training data. These models are
generally presented as being trained only on English text but have been found
to transfer surprisingly well to other languages. We investigate this
phenomenon and find that common English pretraining corpora actually contain
significant amounts of non-English text: even when less than 1% of data is not
English (well within the error rate of strong language classifiers), this leads
to hundreds of millions of foreign language tokens in large-scale datasets. We
then demonstrate that even these small percentages of non-English data
facilitate cross-lingual transfer for models trained on them, with target
language performance strongly correlated to the amount of in-language data seen
during pretraining. In light of these findings, we argue that no model is truly
monolingual when pretrained at scale, which should be considered when
evaluating cross-lingual transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HFT-ONLSTM: Hierarchical and Fine-Tuning Multi-label Text Classification. (arXiv:2204.08115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08115">
<div class="article-summary-box-inner">
<span><p>Many important classification problems in the real-world consist of a large
number of closely related categories in a hierarchical structure or taxonomy.
Hierarchical multi-label text classification (HMTC) with higher accuracy over
large sets of closely related categories organized in a hierarchy or taxonomy
has become a challenging problem. In this paper, we present a hierarchical and
fine-tuning approach based on the Ordered Neural LSTM neural network,
abbreviated as HFT-ONLSTM, for more accurate level-by-level HMTC. First, we
present a novel approach to learning the joint embeddings based on parent
category labels and textual data for accurately capturing the joint features of
both category labels and texts. Second, a fine tuning technique is adopted for
training parameters such that the text classification results in the upper
level should contribute to the classification in the lower one. At last, the
comprehensive analysis is made based on extensive experiments in comparison
with the state-of-the-art hierarchical and flat multi-label text classification
approaches over two benchmark datasets, and the experimental results show that
our HFT-ONLSTM approach outperforms these approaches, in particular reducing
computational costs while achieving superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Dense Video Captioning as Sequence Generation. (arXiv:2204.08121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08121">
<div class="article-summary-box-inner">
<span><p>Dense video captioning aims to identify the events of interest in an input
video, and generate descriptive captions for each event. Previous approaches
usually follow a two-stage generative process, which first proposes a segment
for each event, then renders a caption for each identified segment. Recent
advances in large-scale sequence generation pretraining have seen great success
in unifying task formulation for a great variety of tasks, but so far, more
complex tasks such as dense video captioning are not able to fully utilize this
powerful paradigm. In this work, we show how to model the two subtasks of dense
video captioning jointly as one sequence generation task, and simultaneously
predict the events and the corresponding descriptions. Experiments on YouCook2
and ViTT show encouraging results and indicate the feasibility of training
complex tasks such as end-to-end dense video captioning integrated into
large-scale pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parallel Text Style Transfer with Self-Parallel Supervision. (arXiv:2204.08123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08123">
<div class="article-summary-box-inner">
<span><p>The performance of existing text style transfer models is severely limited by
the non-parallel datasets on which the models are trained. In non-parallel
datasets, no direct mapping exists between sentences of the source and target
style; the style transfer models thus only receive weak supervision of the
target sentences during training, which often leads the model to discard too
much style-independent information, or utterly fail to transfer the style. In
this work, we propose LaMer, a novel text style transfer framework based on
large-scale language models. LaMer first mines the roughly parallel expressions
in the non-parallel datasets with scene graphs, and then employs MLE training,
followed by imitation learning refinement, to leverage the intrinsic
parallelism within the data. On two benchmark tasks (sentiment &amp; formality
transfer) and a newly proposed challenging task (political stance transfer),
our model achieves qualitative advances in transfer accuracy, content
preservation, and fluency. Further empirical and human evaluations demonstrate
that our model not only makes training more efficient, but also generates more
readable and diverse expressions than previous models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation. (arXiv:2204.08128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08128">
<div class="article-summary-box-inner">
<span><p>Personalized dialogue systems explore the problem of generating responses
that are consistent with the user's personality, which has raised much
attention in recent years. Existing personalized dialogue systems have tried to
extract user profiles from dialogue history to guide personalized response
generation. Since the dialogue history is usually long and noisy, most existing
methods truncate the dialogue history to model the user's personality. Such
methods can generate some personalized responses, but a large part of dialogue
history is wasted, leading to sub-optimal performance of personalized response
generation. In this work, we propose to refine the user dialogue history on a
large scale, based on which we can handle more dialogue history and obtain more
abundant and accurate persona information. Specifically, we design an MSP model
which consists of three personal information refiners and a personalized
response generator. With these multi-level refiners, we can sparsely extract
the most valuable information (tokens) from the dialogue history and leverage
other similar users' data to enhance personalization. Experimental results on
two real-world datasets demonstrate the superiority of our model in generating
more informative and personalized responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ingredient Extraction from Text in the Recipe Domain. (arXiv:2204.08137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08137">
<div class="article-summary-box-inner">
<span><p>In recent years, there has been an increase in the number of devices with
virtual assistants (e.g: Siri, Google Home, Alexa) in our living rooms and
kitchens. As a result of this, these devices receive several queries about
recipes. All these queries will contain terms relating to a "recipe-domain"
i.e: they will contain dish-names, ingredients, cooking times, dietary
preferences etc. Extracting these recipe-relevant aspects from the query thus
becomes important when it comes to addressing the user's information need. Our
project focuses on extracting ingredients from such plain-text user utterances.
Our best performing model was a fine-tuned BERT which achieved an F1-score of
$95.01$. We have released all our code in a GitHub repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Position Encoding for Transformers. (arXiv:2204.08142v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08142">
<div class="article-summary-box-inner">
<span><p>Recurrent models have been dominating the field of neural machine translation
(NMT) for the past few years. Transformers \citep{vaswani2017attention}, have
radically changed it by proposing a novel architecture that relies on a
feed-forward backbone and self-attention mechanism. Although Transformers are
powerful, they could fail to properly encode sequential/positional information
due to their non-recurrent nature. To solve this problem, position embeddings
are defined exclusively for each time step to enrich word information. However,
such embeddings are fixed after training regardless of the task and the word
ordering system of the source or target language.
</p>
<p>In this paper, we propose a novel architecture with new position embeddings
depending on the input text to address this shortcoming by taking the order of
target words into consideration. Instead of using predefined position
embeddings, our solution \textit{generates} new embeddings to refine each
word's position information. Since we do not dictate the position of source
tokens and learn them in an end-to-end fashion, we refer to our method as
\textit{dynamic} position encoding (DPE). We evaluated the impact of our model
on multiple datasets to translate from English into German, French, and Italian
and observed meaningful improvements in comparison to the original Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning. (arXiv:2204.08143v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08143">
<div class="article-summary-box-inner">
<span><p>Massive false rumors emerging along with breaking news or trending topics
severely hinder the truth. Existing rumor detection approaches achieve
promising performance on the yesterday`s news, since there is enough corpus
collected from the same domain for model training. However, they are poor at
detecting rumors about unforeseen events especially those propagated in
different languages due to the lack of training data and prior knowledge (i.e.,
low-resource regimes). In this paper, we propose an adversarial contrastive
learning framework to detect rumors by adapting the features learned from
well-resourced rumor data to that of the low-resourced. Our model explicitly
overcomes the restriction of domain and/or language usage via language
alignment and a novel supervised contrastive training paradigm. Moreover, we
develop an adversarial augmentation mechanism to further enhance the robustness
of low-resource rumor representation. Extensive experiments conducted on two
low-resource datasets collected from real-world microblog platforms demonstrate
that our framework achieves much better performance than state-of-the-art
methods and exhibits a superior capacity for detecting rumors at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling. (arXiv:2204.08152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08152">
<div class="article-summary-box-inner">
<span><p>Multi-turn dialogue modeling as a challenging branch of natural language
understanding (NLU), aims to build representations for machines to understand
human dialogues, which provides a solid foundation for multiple downstream
tasks. Recent studies of dialogue modeling commonly employ pre-trained language
models (PrLMs) to encode the dialogue history as successive tokens, which is
insufficient in capturing the temporal characteristics of dialogues. Therefore,
we propose Bidirectional Information Decoupling Network (BiDeN) as a universal
dialogue encoder, which explicitly incorporates both the past and future
contexts and can be generalized to a wide range of dialogue-related tasks.
Experimental results on datasets of different downstream tasks demonstrate the
universality and effectiveness of our BiDeN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on Prompt-based Few-Shot Learning Methods for Belief State Tracking in Task-oriented Dialog Systems. (arXiv:2204.08167v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08167">
<div class="article-summary-box-inner">
<span><p>We tackle the Dialogue Belief State Tracking(DST) problem of task-oriented
conversational systems. Recent approaches to this problem leveraging
Transformer-based models have yielded great results. However, training these
models is expensive, both in terms of computational resources and time.
Additionally, collecting high quality annotated dialogue datasets remains a
challenge for researchers because of the extensive annotation required for
training these models. Driven by the recent success of pre-trained language
models and prompt-based learning, we explore prompt-based few-shot learning for
Dialogue Belief State Tracking. We formulate the DST problem as a 2-stage
prompt-based language modelling task and train language models for both tasks
and present a comprehensive empirical analysis of their separate and joint
performance. We demonstrate the potential of prompt-based methods in few-shot
learning for DST and provide directions for future improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval. (arXiv:2204.08173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08173">
<div class="article-summary-box-inner">
<span><p>Entity retrieval--retrieving information about entity mentions in a query--is
a key step in open-domain tasks, such as question answering or fact checking.
However, state-of-the-art entity retrievers struggle to retrieve rare entities
for ambiguous mentions due to biases towards popular entities. Incorporating
knowledge graph types during training could help overcome popularity biases,
but there are several challenges: (1) existing type-based retrieval methods
require mention boundaries as input, but open-domain tasks run on unstructured
text, (2) type-based methods should not compromise overall performance, and (3)
type-based methods should be robust to noisy and missing types. In this work,
we introduce TABi, a method to jointly train bi-encoders on knowledge graph
types and unstructured text for entity retrieval for open-domain tasks. TABi
leverages a type-enforced contrastive loss to encourage entities and queries of
similar types to be close in the embedding space. TABi improves retrieval of
rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining
strong overall retrieval performance on open-domain tasks in the KILT benchmark
compared to state-of-the-art retrievers. TABi is also robust to incomplete type
systems, improving rare entity retrieval over baselines with only 5% type
coverage of the training dataset. We make our code publicly available at
https://github.com/HazyResearch/tabi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08198">
<div class="article-summary-box-inner">
<span><p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse
someone. It is commonly used on social media. The metaphorical and creative
nature of sarcasm presents a significant difficulty for sentiment analysis
systems based on affective computing. The methodology and results of our team,
UTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in
this paper. We put different models, and data augmentation approaches to the
test and report on which one works best. The tests begin with traditional
machine learning models and progress to transformer-based and attention-based
models. We employed data augmentation based on data mutation and data
generation. Using RoBERTa and mutation-based data augmentation, our best
approach achieved an F1-sarcastic of 0.38 in the competition's evaluation
phase. After the competition, we fixed our model's flaws and achieved an
F1-sarcastic of 0.414.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visio-Linguistic Brain Encoding. (arXiv:2204.08261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08261">
<div class="article-summary-box-inner">
<span><p>Enabling effective brain-computer interfaces requires understanding how the
human brain encodes stimuli across modalities such as visual, language (or
text), etc. Brain encoding aims at constructing fMRI brain activity given a
stimulus. There exists a plethora of neural encoding models which study brain
encoding for single mode stimuli: visual (pretrained CNNs) or text (pretrained
language models). Few recent papers have also obtained separate visual and text
representation models and performed late-fusion using simple heuristics.
However, previous work has failed to explore: (a) the effectiveness of image
Transformer models for encoding visual stimuli, and (b) co-attentive
multi-modal modeling for visual and text reasoning. In this paper, we
systematically explore the efficacy of image Transformers (ViT, DEiT, and BEiT)
and multi-modal Transformers (VisualBERT, LXMERT, and CLIP) for brain encoding.
Extensive experiments on two popular datasets, BOLD5000 and Pereira, provide
the following insights. (1) To the best of our knowledge, we are the first to
investigate the effectiveness of image and multi-modal Transformers for brain
encoding. (2) We find that VisualBERT, a multi-modal Transformer, significantly
outperforms previously proposed single-mode CNNs, image Transformers as well as
other previously proposed multi-modal models, thereby establishing new
state-of-the-art. The supremacy of visio-linguistic models raises the question
of whether the responses elicited in the visual regions are affected implicitly
by linguistic processing even when passively viewing images. Future fMRI tasks
can verify this computational insight in an appropriate experimental setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factual Error Correction for Abstractive Summaries Using Entity Retrieval. (arXiv:2204.08263v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08263">
<div class="article-summary-box-inner">
<span><p>Despite the recent advancements in abstractive summarization systems
leveraged from large-scale datasets and pre-trained language models, the
factual correctness of the summary is still insufficient. One line of trials to
mitigate this problem is to include a post-editing process that can detect and
correct factual errors in the summary. In building such a post-editing system,
it is strongly required that 1) the process has a high success rate and
interpretability and 2) has a fast running time. Previous approaches focus on
regeneration of the summary using the autoregressive models, which lack
interpretability and require high computing resources. In this paper, we
propose an efficient factual error correction system RFEC based on entities
retrieval post-editing process. RFEC first retrieves the evidence sentences
from the original document by comparing the sentences with the target summary.
This approach greatly reduces the length of text for a system to analyze. Next,
RFEC detects the entity-level errors in the summaries by considering the
evidence sentences and substitutes the wrong entities with the accurate
entities from the evidence sentences. Experimental results show that our
proposed error correction system shows more competitive performance than
baseline methods in correcting the factual errors with a much faster speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts. (arXiv:2204.08292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08292">
<div class="article-summary-box-inner">
<span><p>Inferring spatial relations in natural language is a crucial ability an
intelligent system should possess. The bAbI dataset tries to capture tasks
relevant to this domain (task 17 and 19). However, these tasks have several
limitations. Most importantly, they are limited to fixed expressions, they are
limited in the number of reasoning steps required to solve them, and they fail
to test the robustness of models to input that contains irrelevant or redundant
information. In this paper, we present a new Question-Answering dataset called
StepGame for robust multi-hop spatial reasoning in texts. Our experiments
demonstrate that state-of-the-art models on the bAbI dataset struggle on the
StepGame dataset. Moreover, we propose a Tensor-Product based Memory-Augmented
Neural Network (TP-MANN) specialized for spatial reasoning tasks. Experimental
results on both datasets show that our model outperforms all the baselines with
superior generalization and robustness performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UMass PCL at SemEval-2022 Task 4: Pre-trained Language Model Ensembles for Detecting Patronizing and Condescending Language. (arXiv:2204.08304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08304">
<div class="article-summary-box-inner">
<span><p>Patronizing and condescending language (PCL) is everywhere, but rarely is the
focus on its use by media towards vulnerable communities. Accurately detecting
PCL of this form is a difficult task due to limited labeled data and how subtle
it can be. In this paper, we describe our system for detecting such language
which was submitted to SemEval 2022 Task 4: Patronizing and Condescending
Language Detection. Our approach uses an ensemble of pre-trained language
models, data augmentation, and optimizing the threshold for detection.
Experimental results on the evaluation dataset released by the competition
hosts show that our work is reliably able to detect PCL, achieving an F1 score
of 55.47% on the binary classification task and a macro F1 score of 36.25% on
the fine-grained, multi-label detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding. (arXiv:2204.08325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08325">
<div class="article-summary-box-inner">
<span><p>Due to high data demands of current methods, attention to zero-shot
cross-lingual spoken language understanding (SLU) has grown, as such approaches
greatly reduce human annotation effort. However, existing models solely rely on
shared parameters, which can only perform implicit alignment across languages.
We present Global--Local Contrastive Learning Framework (GL-CLeF) to address
this shortcoming. Specifically, we employ contrastive learning, leveraging
bilingual dictionaries to construct multilingual views of the same utterance,
then encourage their representations to be more similar than negative example
pairs, which achieves to explicitly aligned representations of similar
sentences across languages. In addition, a key step in GL-CLeF is a proposed
Local and Global component, which achieves a fine-grained cross-lingual
transfer (i.e., sentence-level Local intent transfer, token-level Local slot
transfer, and semantic-level Global transfer across intent and slot).
Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and
successfully pulls representations of similar sentences across languages
closer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to execute or ask clarification questions. (arXiv:2204.08373v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08373">
<div class="article-summary-box-inner">
<span><p>Collaborative tasks are ubiquitous activities where a form of communication
is required in order to reach a joint goal. Collaborative building is one of
such tasks. We wish to develop an intelligent builder agent in a simulated
building environment (Minecraft) that can build whatever users wish to build by
just talking to the agent. In order to achieve this goal, such agents need to
be able to take the initiative by asking clarification questions when further
information is needed. Existing works on Minecraft Corpus Dataset only learn to
execute instructions neglecting the importance of asking for clarifications. In
this paper, we extend the Minecraft Corpus Dataset by annotating all builder
utterances into eight types, including clarification questions, and propose a
new builder agent model capable of determining when to ask or execute
instructions. Experimental results show that our model achieves
state-of-the-art performance on the collaborative building task with a
substantial improvement. We also define two new tasks, the learning to ask task
and the joint learning task. The latter consists of solving both collaborating
building and learning to ask tasks jointly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive Transformer-based End-to-end ASR using BERT. (arXiv:2104.04805v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04805">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have led to significant innovation in classical and
practical subjects as varied as speech processing, natural language processing,
and computer vision. On top of the Transformer, attention-based end-to-end
automatic speech recognition (ASR) models have recently become popular.
Specifically, non-autoregressive modeling, which boasts fast inference and
performance comparable to conventional autoregressive methods, is an emerging
research topic. In the context of natural language processing, the
bidirectional encoder representations from Transformers (BERT) model has
received widespread attention, partially due to its ability to infer
contextualized word representations and to enable superior performance for
downstream tasks while needing only simple fine-tuning. Motivated by the
success, we intend to view speech recognition as a downstream task of BERT,
thus an ASR system is expected to be deduced by performing fine-tuning.
Consequently, to not only inherit the advantages of non-autoregressive ASR
models but also enjoy the benefits of a pre-trained language model (e.g.,
BERT), we propose a non-autoregressive Transformer-based end-to-end ASR model
based on BERT. We conduct a series of experiments on the AISHELL-1 dataset that
demonstrate competitive or superior results for the model when compared to
state-of-the-art ASR systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Fully Trained to Fully Random Embeddings: Improving Neural Machine Translation with Compact Word Embedding Tables. (arXiv:2104.08677v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08677">
<div class="article-summary-box-inner">
<span><p>Embedding matrices are key components in neural natural language processing
(NLP) models that are responsible to provide numerical representations of input
tokens.\footnote{In this paper words and subwords are referred to as
\textit{tokens} and the term \textit{embedding} only refers to embeddings of
inputs.} In this paper, we analyze the impact and utility of such matrices in
the context of neural machine translation (NMT). We show that detracting
syntactic and semantic information from word embeddings and running NMT systems
with random embeddings is not as damaging as it initially sounds. We also show
how incorporating only a limited amount of task-specific knowledge from
fully-trained embeddings can boost the performance NMT systems. Our findings
demonstrate that in exchange for negligible deterioration in performance, any
NMT model can be run with partially random embeddings. Working with such
structures means a minimal memory requirement as there is no longer need to
store large embedding tables, which is a significant gain in industrial and
on-device settings. We evaluated our embeddings in translating {English} into
{German} and {French} and achieved a $5.3$x compression rate. Despite having a
considerably smaller architecture, our models in some cases are even able to
outperform state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transductive Learning for Abstractive News Summarization. (arXiv:2104.09500v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09500">
<div class="article-summary-box-inner">
<span><p>Pre-trained and fine-tuned news summarizers are expected to generalize to
news articles unseen in the fine-tuning (training) phase. However, these
articles often contain specifics, such as new events and people, a summarizer
could not learn about in training. This applies to scenarios such as a news
publisher training a summarizer on dated news and summarizing incoming recent
news. In this work, we explore the first application of transductive learning
to summarization where we further fine-tune models on test set inputs.
Specifically, we construct pseudo summaries from salient article sentences and
input randomly masked articles. Moreover, this approach is also beneficial in
the fine-tuning phase, where we jointly predict extractive pseudo references
and abstractive gold summaries in the training set. We show that our approach
yields state-of-the-art results on CNN/DM and NYT datasets, improving ROUGE-L
by 1.05 and 0.74, respectively. Importantly, our approach does not require any
changes of the original architecture. Moreover, we show the benefits of
transduction from dated to more recent CNN news. Finally, through human and
automatic evaluation, we demonstrate improvements in summary abstractiveness
and coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06232">
<div class="article-summary-box-inner">
<span><p>To diversify and enrich generated dialogue responses, knowledge-grounded
dialogue has been investigated in recent years. The existing methods tackle the
knowledge grounding challenge by retrieving the relevant sentences over a large
corpus and augmenting the dialogues with explicit extra information. Despite
their success, however, the existing works have drawbacks on the inference
efficiency. This paper proposes KnowExpert, an end-to-end framework to bypass
the explicit retrieval process and inject knowledge into the pre-trained
language models with lightweight adapters and adapt to the knowledge-grounded
dialogue task. To the best of our knowledge, this is the first attempt to
tackle this challenge without retrieval in this task under an open-domain
chit-chat scenario. The experimental results show that KknowExpert performs
comparably with some retrieval-based baselines while being time-efficient in
inference, demonstrating the potential of our proposed direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoramic-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01263">
<div class="article-summary-box-inner">
<span><p>Response selector is an essential component of generation-based dialogue
systems and it aims to pick out an optimal response in a candidate pool to
continue the dialogue. The current state-of-the-art methods are mainly based on
the encoding paradigm called Cross-Encoder, which separately encodes each
context-response pair and ranks the responses according to their fitness
scores. However, Cross-Encoder repeatedly encodes the same lengthy context for
each response, resulting in high computational costs. Moreover, without
considering the relationship among the candidates, it is difficult to figure
out which candidate is the best response purely based on the fitness score per
candidate. We aim to address these problems through a new paradigm called
Panoramic-Encoder. The proposed method encodes all candidates and the context
at once and realizes the mutual interaction using a tailored candidate
attention mechanism (CAM). It also enables the integration of some effective
training techniques, such as the in-batch negative training, which cannot be
used in Cross-Encoders. Extensive experiments across four benchmark datasets
show that our new method significantly outperforms the current state-of-the-art
with lower computational complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Language to Learn Program Abstractions and Search Heuristics. (arXiv:2106.11053v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11053">
<div class="article-summary-box-inner">
<span><p>Inductive program synthesis, or inferring programs from examples of desired
behavior, offers a general paradigm for building interpretable, robust, and
generalizable machine learning systems. Effective program synthesis depends on
two key ingredients: a strong library of functions from which to build
programs, and an efficient search strategy for finding programs that solve a
given task. We introduce LAPS (Language for Abstraction and Program Search), a
technique for using natural language annotations to guide joint learning of
libraries and neurally-guided search models for synthesis. When integrated into
a state-of-the-art library learning system (DreamCoder), LAPS produces
higher-quality libraries and improves search efficiency and generalization on
three domains -- string editing, image composition, and abstract reasoning
about scenes -- even when no natural language hints are available at test time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDQE: A More Accurate Direct Pretraining for Machine Translation Quality Estimation. (arXiv:2107.14600v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14600">
<div class="article-summary-box-inner">
<span><p>It is expensive to evaluate the results of Machine Translation(MT), which
usually requires manual translation as a reference. Machine Translation Quality
Estimation (QE) is a task of predicting the quality of machine translations
without relying on any reference. Recently, the emergence of
predictor-estimator framework which trains the predictor as a feature extractor
and estimator as a QE predictor, and pre-trained language models(PLM) have
achieved promising QE performance. However, we argue that there are still gaps
between the predictor and the estimator in both data quality and training
objectives, which preclude QE models from benefiting from a large number of
parallel corpora more directly. Based on previous related work that have
alleviated gaps to some extent, we propose a novel framework that provides a
more accurate direct pretraining for QE tasks. In this framework, a generator
is trained to produce pseudo data that is closer to the real QE data, and a
estimator is pretrained on these data with novel objectives that are the same
as the QE task. Experiments on widely used benchmarks show that our proposed
framework outperforms existing methods, without using any pretraining models
such as BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Knowledge Base Question Answering: A Survey. (arXiv:2108.06688v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06688">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) aims to answer a question over a
knowledge base (KB). Early studies mainly focused on answering simple questions
over KBs and achieved great success. However, their performance on complex
questions is still far from satisfactory. Therefore, in recent years,
researchers propose a large number of novel methods, which looked into the
challenges of answering complex questions. In this survey, we review recent
advances on KBQA with the focus on solving complex questions, which usually
contain multiple subjects, express compound relations, or involve numerical
operations. In detail, we begin with introducing the complex KBQA task and
relevant background. Then, we describe benchmark datasets for complex KBQA task
and introduce the construction process of these datasets. Next, we present two
mainstream categories of methods for complex KBQA, namely semantic
parsing-based (SP-based) methods and information retrieval-based (IR-based)
methods. Specifically, we illustrate their procedures with flow designs and
discuss their major differences and similarities. After that, we summarize the
challenges that these two categories of methods encounter when answering
complex questions, and explicate advanced solutions and techniques used in
existing work. Finally, we conclude and discuss several promising directions
related to complex KBQA for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IsoScore: Measuring the Uniformity of Embedding Space Utilization. (arXiv:2108.07344v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07344">
<div class="article-summary-box-inner">
<span><p>The recent success of distributed word representations has led to an
increased interest in analyzing the properties of their spatial distribution.
Several studies have suggested that contextualized word embedding models do not
isotropically project tokens into vector space. However, current methods
designed to measure isotropy, such as average random cosine similarity and the
partition score, have not been thoroughly analyzed and are not appropriate for
measuring isotropy. We propose IsoScore: a novel tool that quantifies the
degree to which a point cloud uniformly utilizes the ambient vector space.
Using rigorously designed tests, we demonstrate that IsoScore is the only tool
available in the literature that accurately measures how uniformly distributed
variance is across dimensions in vector space. Additionally, we use IsoScore to
challenge a number of recent conclusions in the NLP literature that have been
derived using brittle metrics of isotropy. We caution future studies from using
existing tools to measure isotropy in contextualized embedding space as
resulting conclusions will be misleading or altogether inaccurate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Molecular Representation Learning via Contrastive Pre-training. (arXiv:2109.08830v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08830">
<div class="article-summary-box-inner">
<span><p>Molecular representation learning plays an essential role in cheminformatics.
Recently, language model-based approaches have gained popularity as an
alternative to traditional expert-designed features to encode molecules.
However, these approaches only utilize a single molecular language for
representation learning. Motivated by the fact that a given molecule can be
described using different languages such as Simplified Molecular Line Entry
System (SMILES), The International Union of Pure and Applied Chemistry (IUPAC),
and The IUPAC International Chemical Identifier (InChI), we propose a
multilingual molecular embedding generation approach called MM-Deacon
(multilingual molecular domain embedding analysis via contrastive learning).
MM-Deacon is pre-trained using SMILES and IUPAC as two different languages on
large-scale molecules. We evaluated the robustness of our method on seven
molecular property prediction tasks from MoleculeNet benchmark, zero-shot
cross-lingual retrieval, and a drug-drug interaction prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Opinion Summarization via Collaborative Decoding. (arXiv:2110.07520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07520">
<div class="article-summary-box-inner">
<span><p>Opinion summarization focuses on generating summaries that reflect popular
subjective information expressed in multiple online reviews. While generated
summaries offer general and concise information about a particular hotel or
product, the information may be insufficient to help the user compare multiple
different choices. Thus, the user may still struggle with the question "Which
one should I pick?" In this paper, we propose the comparative opinion
summarization task, which aims at generating two contrastive summaries and one
common summary from two different candidate sets of reviews. We develop a
comparative summarization framework CoCoSum, which consists of two base
summarization models that jointly generate contrastive and common summaries.
Experimental results on a newly created benchmark CoCoTrip show that CoCoSum
can produce higher-quality contrastive and common summaries than
state-of-the-art opinion summarization models. The dataset and code are
available at https://github.com/megagonlabs/cocosum
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intent Classification Using Pre-trained Language Agnostic Embeddings For Low Resource Languages. (arXiv:2110.09264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09264">
<div class="article-summary-box-inner">
<span><p>Building Spoken Language Understanding (SLU) systems that do not rely on
language specific Automatic Speech Recognition (ASR) is an important yet less
explored problem in language processing. In this paper, we present a
comparative study aimed at employing a pre-trained acoustic model to perform
SLU in low resource scenarios. Specifically, we use three different embeddings
extracted using Allosaurus, a pre-trained universal phone decoder: (1) Phone
(2) Panphone, and (3) Allo embeddings. These embeddings are then used in
identifying the spoken intent. We perform experiments across three different
languages: English, Sinhala, and Tamil each with different data sizes to
simulate high, medium, and low resource scenarios. Our system improves on the
state-of-the-art (SOTA) intent classification accuracy by approximately 2.11%
for Sinhala and 7.00% for Tamil and achieves competitive results on English.
Furthermore, we present a quantitative analysis of how the performance scales
with the number of training examples used per intent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Transformers Are More Efficient Language Models. (arXiv:2110.13711v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13711">
<div class="article-summary-box-inner">
<span><p>Transformer models yield impressive results on many NLP and sequence modeling
tasks. Remarkably, Transformers can handle long sequences which allows them to
produce long coherent outputs: full paragraphs produced by GPT-3 or
well-structured images produced by DALL-E. These large language models are
impressive but also very inefficient and costly, which limits their
applications and accessibility. We postulate that having an explicit
hierarchical architecture is the key to Transformers that efficiently handle
long sequences. To verify this claim, we first study different ways to
downsample and upsample activations in Transformers so as to make them
hierarchical. We use the best performing upsampling and downsampling layers to
create Hourglass - a hierarchical Transformer language model. Hourglass
improves upon the Transformer baseline given the same amount of computation and
can yield the same results as Transformers more efficiently. In particular,
Hourglass sets new state-of-the-art for Transformer models on the ImageNet32
generation task and improves language modeling efficiency on the widely studied
enwik8 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing in-and-for Design Research. (arXiv:2111.13827v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13827">
<div class="article-summary-box-inner">
<span><p>We review the scholarly contributions that utilise Natural Language
Processing (NLP) techniques to support the design process. Using a heuristic
approach, we gathered 223 articles that are published in 32 journals within the
period 1991-present. We present state-of-the-art NLP in-and-for design research
by reviewing these articles according to the type of natural language text
sources: internal reports, design concepts, discourse transcripts, technical
publications, consumer opinions, and others. Upon summarizing and identifying
the gaps in these contributions, we utilise an existing design innovation
framework to identify the applications that are currently being supported by
NLP. We then propose a few methodological and theoretical directions for future
NLP in-and-for design research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval. (arXiv:2112.07577v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07577">
<div class="article-summary-box-inner">
<span><p>Dense retrieval approaches can overcome the lexical gap and lead to
significantly improved search results. However, they require large amounts of
training data which is not available for most domains. As shown in previous
work (Thakur et al., 2021b), the performance of dense retrievers severely
degrades under a domain shift. This limits the usage of dense retrieval
approaches to only a few domains with large training datasets.
</p>
<p>In this paper, we propose the novel unsupervised domain adaptation method
Generative Pseudo Labeling (GPL), which combines a query generator with pseudo
labeling from a cross-encoder. On six representative domain-specialized
datasets, we find the proposed GPL can outperform an out-of-the-box
state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL
requires less (unlabeled) data from the target domain and is more robust in its
training than previous methods.
</p>
<p>We further investigate the role of six recent pre-training methods in the
scenario of domain adaptation for retrieval tasks, where only three could yield
improved results. The best approach, TSDAE (Wang et al., 2021) can be combined
with GPL, yielding another average improvement of 1.4 points nDCG@10 across the
six tasks. Code and models are available at https://gpl.sbert.net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases. (arXiv:2112.07868v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07868">
<div class="article-summary-box-inner">
<span><p>Detecting social bias in text is challenging due to nuance, subjectivity, and
difficulty in obtaining good quality labeled datasets at scale, especially
given the evolving nature of social biases and society. To address these
challenges, we propose a few-shot instruction-based method for prompting
pre-trained language models (LMs). We select a few class-balanced exemplars
from a small support repository that are closest to the query to be labeled in
the embedding space. We then provide the LM with instruction that consists of
this subset of labeled exemplars, the query text to be classified, a definition
of bias, and prompt it to make a decision. We demonstrate that large LMs used
in a few-shot context can detect different types of fine-grained biases with
similar and sometimes superior accuracy to fine-tuned models. We observe that
the largest 530B parameter model is significantly more effective in detecting
social bias compared to smaller models (achieving at least 13% improvement in
AUC metric compared to other models). It also maintains a high AUC (dropping
less than 2%) when the labeled repository is reduced to as few as $100$
samples. Large pretrained language models thus make it easier and quicker to
build new bias detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving both domain robustness and domain adaptability in machine translation. (arXiv:2112.08288v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08288">
<div class="article-summary-box-inner">
<span><p>We consider two problems of NMT domain adaptation using meta-learning. First,
we want to reach domain robustness, i.e., we want to reach high quality on both
domains seen in the training data and unseen domains. Second, we want our
systems to be adaptive, i.e., making it possible to finetune systems with just
hundreds of in-domain parallel sentences. We study the domain adaptability of
meta-learning when improving the domain robustness of the model. In this paper,
we propose a novel approach, RMLNMT (Robust Meta-Learning Framework for Neural
Machine Translation Domain Adaptation), which improves the robustness of
existing meta-learning models. More specifically, we show how to use a domain
classifier in curriculum learning and we integrate the word-level domain mixing
model into the meta-learning framework with a balanced sampling strategy.
Experiments on English$\rightarrow$German and English$\rightarrow$Chinese
translation show that RMLNMT improves in terms of both domain robustness and
domain adaptability in seen and unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01647">
<div class="article-summary-box-inner">
<span><p>Biomedical research is growing at such an exponential pace that scientists,
researchers, and practitioners are no more able to cope with the amount of
published literature in the domain. The knowledge presented in the literature
needs to be systematized in such a way that claims and hypotheses can be easily
found, accessed, and validated. Knowledge graphs can provide such a framework
for semantic knowledge representation from literature. However, in order to
build a knowledge graph, it is necessary to extract knowledge as relationships
between biomedical entities and normalize both entities and relationship types.
In this paper, we present and compare a few rule-based and machine
learning-based (Naive Bayes, Random Forests as examples of traditional machine
learning methods and DistilBERT and T5-based models as examples of modern deep
learning transformers) methods for scalable relationship extraction from
biomedical literature, and for the integration into the knowledge graphs. We
examine how resilient are these various methods to unbalanced and fairly small
datasets. Our experiments show that transformer-based models handle well both
small (due to pre-training on a large dataset) and unbalanced datasets. The
best performing model was the DistilBERT-based model fine-tuned on balanced
data, with a reported F1-score of 0.89.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-based Data Augmentation for Math Word Problems. (arXiv:2201.02489v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02489">
<div class="article-summary-box-inner">
<span><p>It's hard for neural MWP solvers to deal with tiny local variances. In MWP
task, some local changes conserve the original semantic while the others may
totally change the underlying logic. Currently, existing datasets for MWP task
contain limited samples which are key for neural models to learn to
disambiguate different kinds of local variances in questions and solve the
questions correctly. In this paper, we propose a set of novel data augmentation
approaches to supplement existing datasets with such data that are augmented
with different kinds of local variances, and help to improve the generalization
ability of current neural models. New samples are generated by knowledge guided
entity replacement, and logic guided problem reorganization. The augmentation
approaches are ensured to keep the consistency between the new data and their
labels. Experimental results have shown the necessity and the effectiveness of
our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval. (arXiv:2201.02772v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02772">
<div class="article-summary-box-inner">
<span><p>Cross-Modal Retrieval (CMR) is an important research topic across multimodal
computing and information retrieval, which takes one type of data as the query
to retrieve relevant data of another type. It has been widely used in many
real-world applications. Recently, the vision-language pre-trained models
represented by CLIP demonstrate its superiority in learning the visual and
textual representations and gain impressive performance on various vision and
language related tasks. Although CLIP as well as the previous pre-trained
models have shown great performance improvement in the unsupervised CMR, the
performance and impact of these pre-trained models on the supervised CMR were
rarely explored due to the lack of common representation for the multimodal
class-level associations. In this paper, we take CLIP as the current
representative vision-language pre-trained model to conduct a comprehensive
empirical study. We evaluate its performance and impact on the supervised CMR,
and attempt to answer several key research questions. To this end, we first
propose a novel model CLIP4CMR (CLIP enhanced network for Cross-Modal
Retrieval) that employs the pre-trained CLIP as backbone network to perform the
supervised CMR. Then by means of the CLIP4CMR framework, we revisit the design
of different learning objectives in current CMR methods to provide new insights
on model design. Moreover, we investigate the most concerned aspects in
applying CMR, including the robustness to modality imbalance and sensitivity to
hyper-parameters, to provide new perspectives for practical applications.
Through extensive experiments, we show that CLIP4CMR achieves the SOTA results
with prominent improvements on the benchmark datasets, and can be used as a
fundamental framework to empirically study the key research issues of the
supervised CMR, with significant implications for model design and practical
considerations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Many Ways to Be Lonely: Fine-Grained Characterization of Loneliness and Its Potential Changes in COVID-19. (arXiv:2201.07423v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07423">
<div class="article-summary-box-inner">
<span><p>Loneliness has been associated with negative outcomes for physical and mental
health. Understanding how people express and cope with various forms of
loneliness is critical for early screening and targeted interventions to reduce
loneliness, particularly among vulnerable groups such as young adults. To
examine how different forms of loneliness and coping strategies manifest in
loneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained
Loneliness) by using Reddit posts in two young adult-focused forums and two
loneliness related forums consisting of a diverse age group. We provided
annotations by trained human annotators for binary and fine-grained loneliness
classifications of the posts. Trained on FIG-Loneliness, two BERT-based models
were used to understand loneliness forms and authors' coping strategies in
these forums. Our binary loneliness classification achieved an accuracy above
97%, and fine-grained loneliness category classification reached an average
accuracy of 77% across all labeled categories. With FIG-Loneliness and model
predictions, we found that loneliness expressions in the young adults related
forums were distinct from other forums. Those in young adult-focused forums
were more likely to express concerns pertaining to peer relationship, and were
potentially more sensitive to geographical isolation impacted by the COVID-19
pandemic lockdown. Also, we showed that different forms of loneliness have
differential use in coping strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11114">
<div class="article-summary-box-inner">
<span><p>Some neurons in deep networks specialize in recognizing highly specific
perceptual, structural, or semantic features of inputs. In computer vision,
techniques exist for identifying neurons that respond to individual concept
categories like colors, textures, and object classes. But these techniques are
limited in scope, labeling only a small subset of neurons and behaviors in any
network. Is a richer characterization of neuron-level computation possible? We
introduce a procedure (called MILAN, for mutual-information-guided linguistic
annotation of neurons) that automatically labels neurons with open-ended,
compositional, natural language descriptions. Given a neuron, MILAN generates a
description by searching for a natural language string that maximizes pointwise
mutual information with the image regions in which the neuron is active. MILAN
produces fine-grained descriptions that capture categorical, relational, and
logical structure in learned features. These descriptions obtain high agreement
with human-generated feature descriptions across a diverse set of model
architectures and tasks, and can aid in understanding and controlling learned
models. We highlight three applications of natural language neuron
descriptions. First, we use MILAN for analysis, characterizing the distribution
and importance of neurons selective for attribute, category, and relational
information in vision models. Second, we use MILAN for auditing, surfacing
neurons sensitive to human faces in datasets designed to obscure them. Finally,
we use MILAN for editing, improving robustness in an image classifier by
deleting neurons sensitive to text features spuriously correlated with class
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RerrFact: Reduced Evidence Retrieval Representations for Scientific Claim Verification. (arXiv:2202.02646v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02646">
<div class="article-summary-box-inner">
<span><p>Exponential growth in digital information outlets and the race to publish has
made scientific misinformation more prevalent than ever. However, the task to
fact-verify a given scientific claim is not straightforward even for
researchers. Scientific claim verification requires in-depth knowledge and
great labor from domain experts to substantiate supporting and refuting
evidence from credible scientific sources. The SciFact dataset and
corresponding task provide a benchmarking leaderboard to the community to
develop automatic scientific claim verification systems via extracting and
assimilating relevant evidence rationales from source abstracts. In this work,
we propose a modular approach that sequentially carries out binary
classification for every prediction subtask as in the SciFact leaderboard. Our
simple classifier-based approach uses reduced abstract representations to
retrieve relevant abstracts. These are further used to train the relevant
rationale-selection model. Finally, we carry out two-step stance predictions
that first differentiate non-relevant rationales and then identify supporting
or refuting rationales for a given claim. Experimentally, our system RerrFact
with no fine-tuning, simple design, and a fraction of model parameters fairs
competitively on the leaderboard against large-scale, modular, and joint
modeling approaches. We make our codebase available at
https://github.com/ashishrana160796/RerrFact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion. (arXiv:2202.13785v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13785">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs store a large number of factual triples while they are still
incomplete, inevitably. The previous knowledge graph completion (KGC) models
predict missing links between entities merely relying on fact-view data,
ignoring the valuable commonsense knowledge. The previous knowledge graph
embedding (KGE) techniques suffer from invalid negative sampling and the
uncertainty of fact-view link prediction, limiting KGC's performance. To
address the above challenges, we propose a novel and scalable Commonsense-Aware
Knowledge Embedding (CAKE) framework to automatically extract commonsense from
factual triples with entity concepts. The generated commonsense augments
effective self-supervision to facilitate both high-quality negative sampling
(NS) and joint commonsense and fact-view link prediction. Experimental results
on the KGC task demonstrate that assembling our framework could enhance the
performance of the original KGE models, and the proposed commonsense-aware NS
module is superior to other NS techniques. Besides, our proposed framework
could be easily adaptive to various KGE models and explain the predicted
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">USTC-NELSLIP at SemEval-2022 Task 11: Gazetteer-Adapted Integration Network for Multilingual Complex Named Entity Recognition. (arXiv:2203.03216v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03216">
<div class="article-summary-box-inner">
<span><p>This paper describes the system developed by the USTC-NELSLIP team for
SemEval-2022 Task 11 Multilingual Complex Named Entity Recognition
(MultiCoNER). We propose a gazetteer-adapted integration network (GAIN) to
improve the performance of language models for recognizing complex named
entities. The method first adapts the representations of gazetteer networks to
those of language models by minimizing the KL divergence between them. After
adaptation, these two networks are then integrated for backend supervised named
entity recognition (NER) training. The proposed method is applied to several
state-of-the-art Transformer-based NER models with a gazetteer built from
Wikidata, and shows great generalization ability across them. The final
predictions are derived from an ensemble of these trained models. Experimental
results and detailed analysis verify the effectiveness of the proposed method.
The official results show that our system ranked 1st on three tracks (Chinese,
Code-mixed and Bangla) and 2nd on the other ten tracks in this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkillNet: A Sparsely Activated Model for General-Purpose Natural Language Understanding. (arXiv:2203.03312v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03312">
<div class="article-summary-box-inner">
<span><p>Prevailing deep models are single-purpose and overspecialize at individual
tasks. However, when being extended to new tasks, they typically forget
previously learned skills and learn from scratch. We address this issue by
introducing SkillNet, a general-purpose model that stitches together existing
skills to learn new tasks more effectively. The key feature of our approach is
that it is sparsely activated guided by predefined skills. Different from
traditional dense models that always activate all the model parameters,
SkillNet only activates parts of the model parameters whose skills are relevant
to the target task. When learning for a new task, our approach precisely
activates required skills and also provides an option to add new skills. We
evaluate on natural language understandings tasks and have the following
findings. First, with only one model checkpoint, SkillNet performs better than
task-specific fine-tuning and two multi-task learning baselines (i.e., dense
model and Mixture-of-Experts model) on six tasks. Second, sparsely activated
pre-training further improves the overall performance. Third, SkillNet
significantly outperforms baseline systems when being extended to new tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons. (arXiv:2203.06063v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06063">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown the advantages of evaluating NLG systems using
pairwise comparisons as opposed to direct assessment. Given $k$ systems, a
naive approach for identifying the top-ranked system would be to uniformly
obtain pairwise comparisons from all ${k \choose 2}$ pairs of systems. However,
this can be very expensive as the number of human annotations required would
grow quadratically with $k$. In this work, we introduce Active Evaluation, a
framework to efficiently identify the top-ranked system by actively choosing
system pairs for comparison using dueling bandit algorithms. We perform
extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation
datasets spanning 5 tasks and show that the number of human annotations can be
reduced by 80%. To further reduce the number of human annotations, we propose
model-based dueling bandit algorithms which combine automatic evaluation
metrics with human evaluations. Specifically, we eliminate sub-optimal systems
even before the human annotation process and perform human evaluations only on
test examples where the automatic metric is highly uncertain. This reduces the
number of human annotations required further by 89%. In effect, we show that
identifying the top-ranked system requires only a few hundred human
annotations, which grow linearly with $k$. Lastly, we provide practical
recommendations and best practices to identify the top-ranked system
efficiently. Our code has been made publicly available at
https://github.com/akashkm99/duelnlg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALT: um software para an\'alise de legibilidade de textos em L\'ingua Portuguesa. (arXiv:2203.12135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12135">
<div class="article-summary-box-inner">
<span><p>In the initial stage of human life, communication, seen as a process of
social interaction, was always the best way to reach consensus between the
parties. Understanding and credibility in this process are essential for the
mutual agreement to be validated. But, how to do it so that this communication
reaches the great mass? This is the main challenge when what is sought is the
dissemination of information and its approval. In this context, this study
presents the ALT software, developed from original readability metrics adapted
to the Portuguese language, available on the web, to reduce communication
difficulties. The development of the software was motivated by the theory of
communicative action of Habermas, which uses a multidisciplinary style to
measure the credibility of the discourse in the communication channels used to
build and maintain a safe and healthy relationship with the public.
</p>
<p>--
</p>
<p>No est\'agio inicial da vida humana a comunica\c{c}\~ao, vista como um
processo de intera\c{c}\~ao social, foi sempre o melhor caminho para o consenso
entre as partes. O entendimento e a credibilidade nesse processo s\~ao
fundamentais para que o acordo m\'utuo seja validado. Mas, como faz\^e-lo de
forma que essa comunica\c{c}\~ao alcance a grande massa? Esse \'e o principal
desafio quando o que se busca \'e a difus\~ao da informa\c{c}\~ao e a sua
aprova\c{c}\~ao. Nesse contexto, este estudo apresenta o software ALT,
desenvolvido a partir de m\'etricas de legibilidade originais adaptadas para a
L\'ingua Portuguesa, dispon\'ivel na web, para reduzir as dificuldades na
comunica\c{c}\~ao. O desenvolvimento do software foi motivado pela teoria do
agir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para
medir a credibilidade do discurso nos canais de comunica\c{c}\~ao utilizados
para construir e manter uma rela\c{c}\~ao segura e saud\'avel com o p\'ublico.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Span Classification with Structured Information for Disfluency Detection in Spoken Utterances. (arXiv:2203.16028v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16028">
<div class="article-summary-box-inner">
<span><p>Existing approaches in disfluency detection focus on solving a token-level
classification task for identifying and removing disfluencies in text.
Moreover, most works focus on leveraging only contextual information captured
by the linear sequences in text, thus ignoring the structured information in
text which is efficiently captured by dependency trees. In this paper, building
on the span classification paradigm of entity recognition, we propose a novel
architecture for detecting disfluencies in transcripts from spoken utterances,
incorporating both contextual information through transformers and
long-distance structured information captured by dependency trees, through
graph convolutional networks (GCNs). Experimental results show that our
proposed model achieves state-of-the-art results on the widely used English
Switchboard for disfluency detection and outperforms prior-art by a significant
margin. We make all our codes publicly available on GitHub
(https://github.com/Sreyan88/Disfluency-Detection-with-Span-Classification)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Demonstration Tuning for Pre-trained Language Models. (arXiv:2204.04392v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04392">
<div class="article-summary-box-inner">
<span><p>Pretrained language models can be effectively stimulated by textual prompts
or demonstrations, especially in low-data scenarios. Recent works have focused
on automatically searching discrete or continuous prompts or optimized
verbalizers, yet studies for the demonstration are still limited. Concretely,
the demonstration examples are crucial for an excellent final performance of
prompt-tuning. In this paper, we propose a novel pluggable, extensible, and
efficient approach named contrastive demonstration tuning, which is free of
demonstration sampling. Furthermore, the proposed approach can be: (i) Plugged
to any previous prompt-tuning approaches; (ii) Extended to widespread
classification tasks with a large number of categories. Experimental results on
16 datasets illustrate that our method integrated with previous approaches
LM-BFF and P-tuning can yield better performance. Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/Demo-Tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias. (arXiv:2204.04902v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04902">
<div class="article-summary-box-inner">
<span><p>Media framing bias can lead to increased political polarization, and thus,
the need for automatic mitigation methods is growing. We propose a new task, a
neutral summary generation from multiple news headlines of the varying
political leanings to facilitate balanced and unbiased news reading. In this
paper, we first collect a new dataset, obtain insights about framing bias
through a case study, and propose a new effective metric and models for the
task. Lastly, we conduct experimental analyses to provide insights about
remaining challenges and future directions. One of the most interesting
observations is that generation models can hallucinate not only factually
inaccurate or unverifiable content, but also politically biased content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text Classification. (arXiv:2204.04952v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04952">
<div class="article-summary-box-inner">
<span><p>Text classification struggles to generalize to unseen classes with very few
labeled text instances per class. In such a few-shot learning (FSL) setting,
metric-based meta-learning approaches have shown promising results. Previous
studies mainly aim to derive a prototype representation for each class.
However, they neglect that it is challenging-yet-unnecessary to construct a
compact representation which expresses the entire meaning for each class. They
also ignore the importance to capture the inter-dependency between query and
the support set for few-shot text classification. To deal with these issues, we
propose a meta-learning based method MGIMN which performs instance-wise
comparison followed by aggregation to generate class-wise matching vectors
instead of prototype learning. The key of instance-wise comparison is the
interactive matching within the class-specific context and episode-specific
context. Extensive experiments demonstrate that the proposed method
significantly outperforms the existing state-of-the-art approaches, under both
the standard FSL and generalized FSL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving Deep into Regularity: A Simple but Effective Method for Chinese Named Entity Recognition. (arXiv:2204.05544v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05544">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the improving performance of Chinese Named Entity
Recognition (NER) from proposing new frameworks or incorporating word lexicons.
However, the inner composition of entity mentions in character-level Chinese
NER has been rarely studied. Actually, most mentions of regular types have
strong name regularity. For example, entities end with indicator words such as
"company" or "bank" usually belong to organization. In this paper, we propose a
simple but effective method for investigating the regularity of entity spans in
Chinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON).
Specifically, the proposed model consists of two branches: a regularity-aware
module and a regularityagnostic module. The regularity-aware module captures
the internal regularity of each span for better entity type prediction, while
the regularity-agnostic module is employed to locate the boundary of entities
and relieve the excessive attention to span regularity. An orthogonality space
is further constructed to encourage two modules to extract different aspects of
regularity features. To verify the effectiveness of our method, we conduct
extensive experiments on three benchmark datasets and a practical medical
dataset. The experimental results show that our RICON significantly outperforms
previous state-of-the-art methods, including various lexicon-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05999">
<div class="article-summary-box-inner">
<span><p>Code is seldom written in a single left-to-right pass and is instead
repeatedly edited and refined. We introduce InCoder, a unified generative model
that can perform program synthesis (via left-to-right generation) as well as
editing (via infilling). InCoder is trained to generate code files from a large
corpus of permissively licensed code, where regions of code have been randomly
masked and moved to the end of each file, allowing code infilling with
bidirectional context. Our model is the first generative model that is able to
directly perform zero-shot code infilling, which we evaluate on challenging
tasks such as type inference, comment generation, and variable re-naming. We
find that the ability to condition on bidirectional context substantially
improves performance on these tasks, while still performing comparably on
standard program synthesis benchmarks in comparison to left-to-right only
models pretrained at similar scale. The InCoder models and code are publicly
released. https://sites.google.com/view/incoder-code-models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing. (arXiv:2204.06625v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06625">
<div class="article-summary-box-inner">
<span><p>Model ensemble is a popular approach to produce a low-variance and
well-generalized model. However, it induces large memory and inference costs,
which are often not affordable for real-world deployment. Existing work has
resorted to sharing weights among models. However, when increasing the
proportion of the shared weights, the resulting models tend to be similar, and
the benefits of using model ensemble diminish. To retain ensemble benefits
while maintaining a low memory cost, we propose a consistency-regularized
ensemble learning approach based on perturbed models, named CAMERO.
Specifically, we share the weights of bottom layers across all models and apply
different perturbations to the hidden representations for different models,
which can effectively promote the model diversity. Meanwhile, we apply a
prediction consistency regularizer across the perturbed models to control the
variance due to the model diversity. Our experiments using large language
models demonstrate that CAMERO significantly improves the generalization
performance of the ensemble model. Specifically, CAMERO outperforms the
standard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a
significantly smaller model size (114.2M vs. 880.6M).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals. (arXiv:2204.06644v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06644">
<div class="article-summary-box-inner">
<span><p>We present an efficient method of pretraining large-scale autoencoding
language models using training signals generated by an auxiliary model.
Originated in ELECTRA, this training strategy has demonstrated
sample-efficiency to pretrain models at the scale of hundreds of millions of
parameters. In this work, we conduct a comprehensive empirical study, and
propose a recipe, namely "Model generated dEnoising TRaining Objective"
(METRO), which incorporates some of the best modeling techniques developed
recently to speed up, stabilize, and enhance pretrained language models without
compromising model effectiveness. The resultant models, METRO-LM, consisting of
up to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,
SuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in
that they often outperform previous large models with significantly smaller
model sizes and lower pretraining cost.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">$\Upsilon$-Net: A Spatiospectral Network for Retinal OCT Segmentation. (arXiv:2204.07613v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07613">
<div class="article-summary-box-inner">
<span><p>Automated segmentation of retinal optical coherence tomography (OCT) images
has become an important recent direction in machine learning for medical
applications. We hypothesize that the anatomic structure of layers and their
high-frequency variation in OCT images make retinal OCT a fitting choice for
extracting spectral-domain features and combining them with spatial domain
features. In this work, we present $\Upsilon$-Net, an architecture that
combines the frequency domain features with the image domain to improve the
segmentation performance of OCT images. The results of this work demonstrate
that the introduction of two branches, one for spectral and one for spatial
domain features, brings a very significant improvement in fluid segmentation
performance and allows outperformance as compared to the well-known U-Net
model. Our improvement was 13% on the fluid segmentation dice score and 1.9% on
the average dice score. Finally, removing selected frequency ranges in the
spectral domain demonstrates the impact of these features on the fluid
segmentation outperformance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Frame Self-Supervised Depth with Transformers. (arXiv:2204.07616v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07616">
<div class="article-summary-box-inner">
<span><p>Multi-frame depth estimation improves over single-frame approaches by also
leveraging geometric relationships between images via feature matching, in
addition to learning appearance-based features. In this paper we revisit
feature matching for self-supervised monocular depth estimation, and propose a
novel transformer architecture for cost volume generation. We use
depth-discretized epipolar sampling to select matching candidates, and refine
predictions through a series of self- and cross-attention layers. These layers
sharpen the matching probability between pixel features, improving over
standard similarity metrics prone to ambiguities and local minima. The refined
cost volume is decoded into depth estimates, and the whole pipeline is trained
end-to-end from videos using only a photometric objective. Experiments on the
KITTI and DDAD datasets show that our DepthFormer architecture establishes a
new state of the art in self-supervised monocular depth estimation, and is even
competitive with highly specialized supervised single-frame architectures. We
also show that our learned cross-attention network yields representations
transferable across datasets, increasing the effectiveness of pre-training
strategies. Project page: https://sites.google.com/tri.global/depthformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lagrangian Motion Magnification with Double Sparse Optical Flow Decomposition. (arXiv:2204.07636v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07636">
<div class="article-summary-box-inner">
<span><p>Motion magnification techniques aim at amplifying and hence revealing subtle
motion in videos. There are basically two main approaches to reach this goal,
namely via Eulerian or Lagrangian techniques. While the first one magnifies
motion implicitly by operating directly on image pixels, the Lagrangian
approach uses optical flow techniques to extract and amplify pixel
trajectories. Microexpressions are fast and spatially small facial expressions
that are difficult to detect. In this paper, we propose a novel approach for
local Lagrangian motion magnification of facial micromovements. Our
contribution is three-fold: first, we fine-tune the recurrent all-pairs field
transforms for optical flows (RAFT) deep learning approach for faces by adding
ground truth obtained from the variational dense inverse search (DIS) for
optical flow algorithm applied to the CASME II video set of faces. This enables
us to produce optical flows of facial videos in an efficient and sufficiently
accurate way. Second, since facial micromovements are both local in space and
time, we propose to approximate the optical flow field by sparse components
both in space and time leading to a double sparse decomposition. Third, we use
this decomposition to magnify micro-motions in specific areas of the face,
where we introduce a new forward warping strategy using a triangular splitting
of the image grid and barycentric interpolation of the RGB vectors at the
corners of the transformed triangles. We demonstrate the very good performance
of our approach by various examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-aided Direct Sparse Odometry. (arXiv:2204.07640v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07640">
<div class="article-summary-box-inner">
<span><p>We introduce EDS, a direct monocular visual odometry using events and frames.
Our algorithm leverages the event generation model to track the camera motion
in the blind time between frames. The method formulates a direct probabilistic
approach of observed brightness increments. Per-pixel brightness increments are
predicted using a sparse number of selected 3D points and are compared to the
events via the brightness increment error to estimate camera motion. The method
recovers a semi-dense 3D map using photometric bundle adjustment. EDS is the
first method to perform 6-DOF VO using events and frames with a direct
approach. By design, it overcomes the problem of changing appearance in
indirect methods. We also show that, for a target error performance, EDS can
work at lower frame rates than state-of-the-art frame-based VO solutions. This
opens the door to low-power motion-tracking applications where frames are
sparingly triggered "on demand" and our method tracks the motion in between. We
release code and datasets to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiEarth 2022 -- Multimodal Learning for Earth and Environment Workshop and Challenge. (arXiv:2204.07649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07649">
<div class="article-summary-box-inner">
<span><p>The Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022)
will be the first competition aimed at the monitoring and analysis of
deforestation in the Amazon rainforest at any time and in any weather
conditions. The goal of the Challenge is to provide a common benchmark for
multimodal information processing and to bring together the earth and
environmental science communities as well as multimodal representation learning
communities to compare the relative merits of the various multimodal learning
methods to deforestation estimation under well-defined and strictly comparable
conditions. MultiEarth 2022 will have three sub-challenges: 1) matrix
completion, 2) deforestation estimation, and 3) image-to-image translation.
This paper presents the challenge guidelines, datasets, and evaluation metrics
for the three sub-challenges. Our challenge website is available at
https://sites.google.com/view/rainforest-challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Unlearning via Randomized Conditionally Independent Hessians. (arXiv:2204.07655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07655">
<div class="article-summary-box-inner">
<span><p>Recent legislation has led to interest in machine unlearning, i.e., removing
specific training samples from a predictive model as if they never existed in
the training dataset. Unlearning may also be required due to
corrupted/adversarial data or simply a user's updated privacy requirement. For
models which require no training (k-NN), simply deleting the closest original
sample can be effective. But this idea is inapplicable to models which learn
richer representations. Recent ideas leveraging optimization-based updates
scale poorly with the model dimension d, due to inverting the Hessian of the
loss function. We use a variant of a new conditional independence coefficient,
L-CODEC, to identify a subset of the model parameters with the most semantic
overlap on an individual sample level. Our approach completely avoids the need
to invert a (possibly) huge matrix. By utilizing a Markov blanket selection, we
premise that L-CODEC is also suitable for deep unlearning, as well as other
applications in vision. Compared to alternatives, L-CODEC makes approximate
unlearning possible in settings that would otherwise be infeasible, including
vision models used for face recognition, person re-identification and NLP
models that may require unlearning samples identified for exclusion. Code can
be found at https://github.com/vsingh-group/LCODEC-deep-unlearning/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection. (arXiv:2204.07660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07660">
<div class="article-summary-box-inner">
<span><p>Datasets that capture the connection between vision, language, and affection
are limited, causing a lack of understanding of the emotional aspect of human
intelligence. As a step in this direction, the ArtEmis dataset was recently
introduced as a large-scale dataset of emotional reactions to images along with
language explanations of these chosen emotions. We observed a significant
emotional bias towards instance-rich emotions, making trained neural speakers
less accurate in describing under-represented emotions. We show that collecting
new data, in the same way, is not effective in mitigating this emotional bias.
To remedy this problem, we propose a contrastive data collection approach to
balance ArtEmis with a new complementary dataset such that a pair of similar
images have contrasting emotions (one positive and one negative). We collected
260,533 instances using the proposed method, we combine them with ArtEmis,
creating a second iteration of the dataset. The new combined dataset, dubbed
ArtEmis v2.0, has a balanced distribution of emotions with explanations
revealing more fine details in the associated painting. Our experiments show
that neural speakers trained on the new dataset improve CIDEr and METEOR
evaluation metrics by 20% and 7%, respectively, compared to the biased dataset.
Finally, we also show that the performance per emotion of neural speakers is
improved across all the emotion categories, significantly on under-represented
emotions. The collected dataset and code are available at
https://artemisdataset-v2.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Similarity Priors: Neural Collages as Differentiable Fractal Representations. (arXiv:2204.07673v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07673">
<div class="article-summary-box-inner">
<span><p>Many patterns in nature exhibit self-similarity: they can be compactly
described via self-referential transformations. Said patterns commonly appear
in natural and artificial objects, such as molecules, shorelines, galaxies and
even images. In this work, we investigate the role of learning in the automated
discovery of self-similarity and in its utilization for downstream tasks. To
this end, we design a novel class of implicit operators, Neural Collages, which
(1) represent data as the parameters of a self-referential, structured
transformation, and (2) employ hypernetworks to amortize the cost of finding
these parameters to a single forward pass. We investigate how to leverage the
representations produced by Neural Collages in various tasks, including data
compression and generation. Neural Collages image compressors are orders of
magnitude faster than other self-similarity-based algorithms during encoding
and offer compression rates competitive with implicit methods. Finally, we
showcase applications of Neural Collages for fractal art and as deep generative
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safe Self-Refinement for Transformer-based Domain Adaptation. (arXiv:2204.07683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07683">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) aims to leverage a label-rich source
domain to solve tasks on a related unlabeled target domain. It is a challenging
problem especially when a large domain gap lies between the source and target
domains. In this paper we propose a novel solution named SSRT (Safe
Self-Refinement for Transformer-based domain adaptation), which brings
improvement from two aspects. First, encouraged by the success of vision
transformers in various vision tasks, we arm SSRT with a transformer backbone.
We find that the combination of vision transformer with simple adversarial
adaptation surpasses best reported Convolutional Neural Network (CNN)-based
results on the challenging DomainNet benchmark, showing its strong transferable
feature representation. Second, to reduce the risk of model collapse and
improve the effectiveness of knowledge transfer between domains with large
gaps, we propose a Safe Self-Refinement strategy. Specifically, SSRT utilizes
predictions of perturbed target domain data to refine the model. Since the
model capacity of vision transformer is large and predictions in such
challenging tasks can be noisy, a safe training mechanism is designed to
adaptively adjust learning configuration. Extensive evaluations are conducted
on several widely tested UDA benchmarks and SSRT achieves consistently the best
performances, including 85.43% on Office-Home, 88.76% on VisDA-2017 and 45.2%
on DomainNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Image Classification Using Isotropic Network. (arXiv:2204.07707v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07707">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a privacy-preserving image classification method
that uses encrypted images and an isotropic network such as the vision
transformer. The proposed method allows us not only to apply images without
visual information to deep neural networks (DNNs) for both training and testing
but also to maintain a high classification accuracy. In addition, compressible
encrypted images, called encryption-then-compression (EtC) images, can be used
for both training and testing without any adaptation network. Previously, to
classify EtC images, an adaptation network was required before a classification
network, so methods with an adaptation network have been only tested on small
images. To the best of our knowledge, previous privacy-preserving image
classification methods have never considered image compressibility and patch
embedding-based isotropic networks. In an experiment, the proposed
privacy-preserving image classification was demonstrated to outperform
state-of-the-art methods even when EtC images were used in terms of
classification accuracy and robustness against various attacks under the use of
two isotropic networks: vision transformer and ConvMixer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAUSS: Guided Encoder-Decoder Architecture for Hyperspectral Unmixing with Spatial Smoothness. (arXiv:2204.07713v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07713">
<div class="article-summary-box-inner">
<span><p>In recent hyperspectral unmixing (HU) literature, the application of deep
learning (DL) has become more prominent, especially with the autoencoder (AE)
architecture. We propose a split architecture and use a pseudo-ground truth for
abundances to guide the `unmixing network' (UN) optimization. Preceding the UN,
an `approximation network' (AN) is proposed, which will improve the association
between the centre pixel and its neighbourhood. Hence, it will accentuate
spatial correlation in the abundances as its output is the input to the UN and
the reference for the `mixing network' (MN). In the Guided Encoder-Decoder
Architecture for Hyperspectral Unmixing with Spatial Smoothness (GAUSS), we
proposed using one-hot encoded abundances as the pseudo-ground truth to guide
the UN; computed using the k-means algorithm to exclude the use of prior HU
methods. Furthermore, we release the single-layer constraint on MN by
introducing the UN generated abundances in contrast to the standard AE for HU.
Secondly, we experimented with two modifications on the pre-trained network
using the GAUSS method. In GAUSS$_\textit{blind}$, we have concatenated the UN
and the MN to back-propagate the reconstruction error gradients to the encoder.
Then, in the GAUSS$_\textit{prime}$, abundance results of a signal processing
(SP) method with reliable abundance results were used as the pseudo-ground
truth with the GAUSS architecture. According to quantitative and graphical
results for four experimental datasets, the three architectures either
transcended or equated the performance of existing HU algorithms from both DL
and SP domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Performance Limit of Scene Text Recognizer without Human Annotation. (arXiv:2204.07714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07714">
<div class="article-summary-box-inner">
<span><p>Scene text recognition (STR) attracts much attention over the years because
of its wide application. Most methods train STR model in a fully supervised
manner which requires large amounts of labeled data. Although synthetic data
contributes a lot to STR, it suffers from the real-tosynthetic domain gap that
restricts model performance. In this work, we aim to boost STR models by
leveraging both synthetic data and the numerous real unlabeled images,
exempting human annotation cost thoroughly. A robust consistency regularization
based semi-supervised framework is proposed for STR, which can effectively
solve the instability issue due to domain inconsistency between synthetic and
real images. A character-level consistency regularization is designed to
mitigate the misalignment between characters in sequence recognition. Extensive
experiments on standard text recognition benchmarks demonstrate the
effectiveness of the proposed method. It can steadily improve existing STR
models, and boost an STR model to achieve new state-of-the-art results. To our
best knowledge, this is the first consistency regularization based framework
that applies successfully to STR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactiveness Field in Human-Object Interactions. (arXiv:2204.07718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07718">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) detection plays a core role in activity
understanding. Though recent two/one-stage methods have achieved impressive
results, as an essential step, discovering interactive human-object pairs
remains challenging. Both one/two-stage methods fail to effectively extract
interactive pairs instead of generating redundant negative pairs. In this work,
we introduce a previously overlooked interactiveness bimodal prior: given an
object in an image, after pairing it with the humans, the generated pairs are
either mostly non-interactive, or mostly interactive, with the former more
frequent than the latter. Based on this interactiveness bimodal prior we
propose the "interactiveness field". To make the learned field compatible with
real HOI image considerations, we propose new energy constraints based on the
cardinality and difference in the inherent "interactiveness field" underlying
interactive versus non-interactive pairs. Consequently, our method can detect
more precise pairs and thus significantly boost HOI detection performance,
which is validated on widely-used benchmarks where we achieve decent
improvements over state-of-the-arts. Our code is available at
https://github.com/Foruck/Interactiveness-Field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stress-Testing LiDAR Registration. (arXiv:2204.07719v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07719">
<div class="article-summary-box-inner">
<span><p>Point cloud registration (PCR) is an important task in many fields including
autonomous driving with LiDAR sensors. PCR algorithms have improved
significantly in recent years, by combining deep-learned features with robust
estimation methods. These algorithms succeed in scenarios such as indoor scenes
and object models registration. However, testing in the automotive LiDAR
setting, which presents its own challenges, has been limited. The standard
benchmark for this setting, KITTI-10m, has essentially been saturated by recent
algorithms: many of them achieve near-perfect recall.
</p>
<p>In this work, we stress-test recent PCR techniques with LiDAR data. We
propose a method for selecting balanced registration sets, which are
challenging sets of frame-pairs from LiDAR datasets. They contain a balanced
representation of the different relative motions that appear in a dataset, i.e.
small and large rotations, small and large offsets in space and time, and
various combinations of these.
</p>
<p>We perform a thorough comparison of accuracy and run-time on these
benchmarks. Perhaps unexpectedly, we find that the fastest and simultaneously
most accurate approach is a version of advanced RANSAC. We further improve
results with a novel pre-filtering method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching Intrinsic Dimensions of Vision Transformers. (arXiv:2204.07722v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07722">
<div class="article-summary-box-inner">
<span><p>It has been shown by many researchers that transformers perform as well as
convolutional neural networks in many computer vision tasks. Meanwhile, the
large computational costs of its attention module hinder further studies and
applications on edge devices. Some pruning methods have been developed to
construct efficient vision transformers, but most of them have considered image
classification tasks only. Inspired by these results, we propose SiDT, a method
for pruning vision transformer backbones on more complicated vision tasks like
object detection, based on the search of transformer dimensions. Experiments on
CIFAR-100 and COCO datasets show that the backbones with 20\% or 40\%
dimensions/parameters pruned can have similar or even better performance than
the unpruned models. Moreover, we have also provided the complexity analysis
and comparisons with the previous pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic interpretation for convolutional neural networks: What makes a cat a cat?. (arXiv:2204.07724v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07724">
<div class="article-summary-box-inner">
<span><p>The interpretability of deep neural networks has attracted increasing
attention in recent years, and several methods have been created to interpret
the "black box" model. Fundamental limitations remain, however, that impede the
pace of understanding the networks, especially the extraction of understandable
semantic space. In this work, we introduce the framework of semantic
explainable AI (S-XAI), which utilizes row-centered principal component
analysis to obtain the common traits from the best combination of superpixels
discovered by a genetic algorithm, and extracts understandable semantic spaces
on the basis of discovered semantically sensitive neurons and visualization
techniques. Statistical interpretation of the semantic space is also provided,
and the concept of semantic probability is proposed for the first time. Our
experimental results demonstrate that S-XAI is effective in providing a
semantic interpretation for the CNN, and offers broad usage, including
trustworthiness assessment and semantic sample searching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bidirectional Self-Training with Multiple Anisotropic Prototypes for Domain Adaptive Semantic Segmentation. (arXiv:2204.07730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07730">
<div class="article-summary-box-inner">
<span><p>A thriving trend for domain adaptive segmentation endeavors to generate the
high-quality pseudo labels for target domain and retrain the segmentor on them.
Under this self-training paradigm, some competitive methods have sought to the
latent-space information, which establishes the feature centroids (a.k.a
prototypes) of the semantic classes and determines the pseudo label candidates
by their distances from these centroids. In this paper, we argue that the
latent space contains more information to be exploited thus taking one step
further to capitalize on it. Firstly, instead of merely using the source-domain
prototypes to determine the target pseudo labels as most of the traditional
methods do, we bidirectionally produce the target-domain prototypes to degrade
those source features which might be too hard or disturbed for the adaptation.
Secondly, existing attempts simply model each category as a single and
isotropic prototype while ignoring the variance of the feature distribution,
which could lead to the confusion of similar categories. To cope with this
issue, we propose to represent each category with multiple and anisotropic
prototypes via Gaussian Mixture Model, in order to fit the de facto
distribution of source domain and estimate the likelihood of target samples
based on the probability density. We apply our method on GTA5-&gt;Cityscapes and
Synthia-&gt;Cityscapes tasks and achieve 61.2 and 62.8 respectively in terms of
mean IoU, substantially outperforming other competitive self-training methods.
Noticeably, in some categories which severely suffer from the categorical
confusion such as "truck" and "bus", our method achieves 56.4 and 68.8
respectively, which further demonstrates the effectiveness of our design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Linear Attention for Fast and Accurate Keypoint Matching. (arXiv:2204.07731v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07731">
<div class="article-summary-box-inner">
<span><p>Recently Transformers have provided state-of-the-art performance in sparse
matching, crucial to realize high-performance 3D vision applications. Yet,
these Transformers lack efficiency due to the quadratic computational
complexity of their attention mechanism. To solve this problem, we employ an
efficient linear attention for the linear computational complexity. Then, we
propose a new attentional aggregation that achieves high accuracy by
aggregating both the global and local information from sparse keypoints. To
further improve the efficiency, we propose the joint learning of feature
matching and description. Our learning enables simpler and faster matching than
Sinkhorn, often used in matching the learned descriptors from Transformers. Our
method achieves competitive performance with only 0.84M learnable parameters
against the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M
parameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GitNet: Geometric Prior-based Transformation for Birds-Eye-View Segmentation. (arXiv:2204.07733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07733">
<div class="article-summary-box-inner">
<span><p>Birds-eye-view (BEV) semantic segmentation is critical for autonomous driving
for its powerful spatial representation ability. It is challenging to estimate
the BEV semantic maps from monocular images due to the spatial gap, since it is
implicitly required to realize both the perspective-to-BEV transformation and
segmentation. We present a novel two-stage Geometry Prior-based Transformation
framework named GitNet, consisting of (i) the geometry-guided pre-alignment and
(ii) ray-based transformer. In the first stage, we decouple the BEV
segmentation into the perspective image segmentation and geometric prior-based
mapping, with explicit supervision by projecting the BEV semantic labels onto
the image plane to learn visibility-aware features and learnable geometry to
translate into BEV space. Second, the pre-aligned coarse BEV features are
further deformed by ray-based transformers to take visibility knowledge into
account. GitNet achieves the leading performance on the challenging nuScenes
and Argoverse Datasets. The code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Methods in Deep Learning: An In-Depth Survey. (arXiv:2204.07756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07756">
<div class="article-summary-box-inner">
<span><p>Inspired by the human cognitive system, attention is a mechanism that
imitates the human cognitive awareness about specific information, amplifying
critical details to focus more on the essential aspects of data. Deep learning
has employed attention to boost performance for many applications.
Interestingly, the same attention design can suit processing different data
modalities and can easily be incorporated into large networks. Furthermore,
multiple complementary attention mechanisms can be incorporated in one network.
Hence, attention techniques have become extremely attractive. However, the
literature lacks a comprehensive survey specific to attention techniques to
guide researchers in employing attention in their deep models. Note that,
besides being demanding in terms of training data and computational resources,
transformers only cover a single category in self-attention out of the many
categories available. We fill this gap and provide an in-depth survey of 50
attention techniques categorizing them by their most prominent features. We
initiate our discussion by introducing the fundamental concepts behind the
success of attention mechanism. Next, we furnish some essentials such as the
strengths and limitations of each attention category, describe their
fundamental building blocks, basic formulations with primary usage, and
applications specifically for computer vision. We also discuss the challenges
and open questions related to attention mechanism in general. Finally, we
recommend possible future research directions for deep attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Grounded Indoor 3D Semantic Segmentation in the Wild. (arXiv:2204.07761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07761">
<div class="article-summary-box-inner">
<span><p>Recent advances in 3D semantic segmentation with deep neural networks have
shown remarkable success, with rapid performance increase on available
datasets. However, current 3D semantic segmentation benchmarks contain only a
small number of categories -- less than 30 for ScanNet and SemanticKITTI, for
instance, which are not enough to reflect the diversity of real environments
(e.g., semantic image understanding covers hundreds to thousands of classes).
Thus, we propose to study a larger vocabulary for 3D semantic segmentation with
a new extended benchmark on ScanNet data with 200 class categories, an order of
magnitude more than previously studied. This large number of class categories
also induces a large natural class imbalance, both of which are challenging for
existing 3D semantic segmentation methods. To learn more robust 3D features in
this context, we propose a language-driven pre-training method to encourage
learned 3D features that might have limited training examples to lie close to
their pre-trained text embeddings. Extensive experiments show that our approach
consistently outperforms state-of-the-art 3D pre-training for 3D semantic
segmentation on our proposed benchmark (+9% relative mIoU), including
limited-data scenarios with +25% relative mIoU using only 5% annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometric verification of humans by means of hand geometry. (arXiv:2204.07764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07764">
<div class="article-summary-box-inner">
<span><p>This paper describes a hand geometry biometric identification system. We have
acquired a database of 22 people, 10 acquisitions per person, using a
conventional document scanner. We propose a feature extraction and classifier.
The experimental results reveal a maximum identification rate equal to 93.64%,
and a minimum value of the detection cost function equal to 2.92% using a multi
layer perceptron classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks. (arXiv:2204.07780v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07780">
<div class="article-summary-box-inner">
<span><p>Despite the exciting performance, Transformer is criticized for its excessive
parameters and computation cost. However, compressing Transformer remains as an
open problem due to its internal complexity of the layer designs, i.e.,
Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this
issue, we introduce Group-wise Transformation towards a universal yet
lightweight Transformer for vision-and-language tasks, termed as
LW-Transformer. LW-Transformer applies Group-wise Transformation to reduce both
the parameters and computations of Transformer, while also preserving its two
main properties, i.e., the efficient attention modeling on diverse subspaces of
MHA, and the expanding-scaling feature transformation of FFN. We apply
LW-Transformer to a set of Transformer-based networks, and quantitatively
measure them on three vision-and-language tasks and six benchmark datasets.
Experimental results show that while saving a large number of parameters and
computations, LW-Transformer achieves very competitive performance against the
original Transformer networks for vision-and-language tasks. To examine the
generalization ability, we also apply our optimization strategy to a recently
proposed image Transformer called Swin-Transformer for image classification,
where the effectiveness can be also confirmed
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UAMD-Net: A Unified Adaptive Multimodal Neural Network for Dense Depth Completion. (arXiv:2204.07791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07791">
<div class="article-summary-box-inner">
<span><p>Depth prediction is a critical problem in robotics applications especially
autonomous driving. Generally, depth prediction based on binocular stereo
matching and fusion of monocular image and laser point cloud are two mainstream
methods. However, the former usually suffers from overfitting while building
cost volume, and the latter has a limited generalization due to the lack of
geometric constraint. To solve these problems, we propose a novel multimodal
neural network, namely UAMD-Net, for dense depth completion based on fusion of
binocular stereo matching and the weak constrain from the sparse point clouds.
Specifically, the sparse point clouds are converted to sparse depth map and
sent to the multimodal feature encoder (MFE) with binocular image, constructing
a cross-modal cost volume. Then, it will be further processed by the multimodal
feature aggregator (MFA) and the depth regression layer. Furthermore, the
existing multimodal methods ignore the problem of modal dependence, that is,
the network will not work when a certain modal input has a problem. Therefore,
we propose a new training strategy called Modal-dropout which enables the
network to be adaptively trained with multiple modal inputs and inference with
specific modal inputs. Benefiting from the flexible network structure and
adaptive training method, our proposed network can realize unified training
under various modal input conditions. Comprehensive experiments conducted on
KITTI depth completion benchmark demonstrate that our method produces robust
results and outperforms other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring. (arXiv:2204.07820v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07820">
<div class="article-summary-box-inner">
<span><p>Blind image deblurring (BID) remains a challenging and significant task.
Benefiting from the strong fitting ability of deep learning, paired data-driven
supervised BID method has obtained great progress. However, paired data are
usually synthesized by hand, and the realistic blurs are more complex than
synthetic ones, which makes the supervised methods inept at modeling realistic
blurs and hinders their real-world applications. As such, unsupervised deep BID
method without paired data offers certain advantages, but current methods still
suffer from some drawbacks, e.g., bulky model size, long inference time, and
strict image resolution and domain requirements. In this paper, we propose a
lightweight and real-time unsupervised BID baseline, termed Frequency-domain
Contrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with
attractive properties, i.e., no image domain limitation, no image resolution
limitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the
lightweight property and performance superiority, two new collaboration units
called lightweight domain conversion unit(LDCU) and parameter-free
frequency-domain contrastive unit(PFCU) are designed. LDCU mainly implements
inter-domain conversion in lightweight manner. PFCU further explores the
similarity measure, external difference and internal connection between the
blurred domain and sharp domain images in frequency domain, without involving
extra parameters. Extensive experiments on several image datasets demonstrate
the effectiveness of our FCL-GAN in terms of performance, model size and
reference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Transfer Learning to improve Chest X-Ray pathology detection using limited triplets. (arXiv:2204.07824v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07824">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches applied to medical imaging have reached near-human
or better-than-human performance on many diagnostic tasks. For instance, the
CheXpert competition on detecting pathologies in chest x-rays has shown
excellent multi-class classification performance. However, training and
validating deep learning models require extensive collections of images and
still produce false inferences, as identified by a human-in-the-loop. In this
paper, we introduce a practical approach to improve the predictions of a
pre-trained model through Few-Shot Learning (FSL). After training and
validating a model, a small number of false inference images are collected to
retrain the model using \textbf{\textit{Image Triplets}} - a false positive or
false negative, a true positive, and a true negative. The retrained FSL model
produces considerable gains in performance with only a few epochs and few
images. In addition, FSL opens rapid retraining opportunities for
human-in-the-loop systems, where a radiologist can relabel false inferences,
and the model can be quickly retrained. We compare our retrained model
performance with existing FSL approaches in medical imaging that train and
evaluate models at once.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust and Scalable Attention Guided Deep Learning Framework for Movement Quality Assessment. (arXiv:2204.07840v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07840">
<div class="article-summary-box-inner">
<span><p>Physical rehabilitation programs frequently begin with a brief stay in the
hospital and continue with home-based rehabilitation. Lack of feedback on
exercise correctness is a significant issue in home-based rehabilitation.
Automated movement quality assessment (MQA) using skeletal movement data
(hereafter referred to as skeletal data) collected via depth imaging devices
can assist with home-based rehabilitation by providing the necessary
quantitative feedback. This paper aims to use recent advances in deep learning
to address the problem of MQA. Movement quality score generation is an
essential component of MQA. We propose three novel skeletal data augmentation
schemes. We show that using the proposed augmentations for generating movement
quality scores result in significant performance boosts over existing methods.
Finally, we propose a novel transformer based architecture for MQA. Four novel
feature extractors are proposed and studied that allow the transformer network
to operate on skeletal data. We show that adding the attention mechanism in the
design of the proposed feature extractor allows the transformer network to pay
attention to specific body parts that make a significant contribution towards
executing a movement. We report an improvement in movement quality score
prediction of 12% on UI-PRMD dataset and 21% on KIMORE dataset compared to the
existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Few-Shot Object Detection with Meta-Learning Based Cross-Modal Prompting. (arXiv:2204.07841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07841">
<div class="article-summary-box-inner">
<span><p>We study multimodal few-shot object detection (FSOD) in this paper, using
both few-shot visual examples and class semantic information for detection.
Most of previous works focus on either few-shot or zero-shot object detection,
ignoring the complementarity of visual and semantic information. We first show
that meta-learning and prompt-based learning, the most commonly-used methods
for few-shot learning and zero-shot transferring from pre-trained
vision-language models to downstream tasks, are conceptually similar. They both
reformulate the objective of downstream tasks the same as the pre-training
tasks, and mostly without tuning the parameters of pre-trained models. Based on
this observation, we propose to combine meta-learning with prompt-based
learning for multimodal FSOD without fine-tuning, by learning transferable
class-agnostic multimodal FSOD models over many-shot base classes.
Specifically, to better exploit the pre-trained vision-language models, the
meta-learning based cross-modal prompting is proposed to generate soft prompts
and further used to extract the semantic prototype, conditioned on the few-shot
visual examples. Then, the extracted semantic prototype and few-shot visual
prototype are fused to generate the multimodal prototype for detection. Our
models can efficiently fuse the visual and semantic information at both
token-level and feature-level. We comprehensively evaluate the proposed
multimodal FSOD models on multiple few-shot object detection benchmarks,
achieving promising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-guided Object Inpainting. (arXiv:2204.07845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07845">
<div class="article-summary-box-inner">
<span><p>Previous works on image inpainting mainly focus on inpainting background or
partially missing objects, while the problem of inpainting an entire missing
object remains unexplored. This work studies a new image inpainting task, i.e.
shape-guided object inpainting. Given an incomplete input image, the goal is to
fill in the hole by generating an object based on the context and implicit
guidance given by the hole shape. Since previous methods for image inpainting
are mainly designed for background inpainting, they are not suitable for this
task. Therefore, we propose a new data preparation method and a novel
Contextual Object Generator (CogNet) for the object inpainting task. On the
data side, we incorporate object priors into training data by using object
instances as holes. The CogNet has a two-stream architecture that combines the
standard bottom-up image completion process with a top-down object generation
process. A predictive class embedding module bridges the two streams by
predicting the class of the missing object from the bottom-up features, from
which a semantic object map is derived as the input of the top-down stream.
Experiments demonstrate that the proposed method can generate realistic objects
that fit the context in terms of both visual appearance and semantic meanings.
Code can be found at the project page
\url{https://zengxianyu.github.io/objpaint}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-organ Segmentation Network with Adversarial Performance Validator. (arXiv:2204.07850v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07850">
<div class="article-summary-box-inner">
<span><p>CT organ segmentation on computed tomography (CT) images becomes a
significant brick for modern medical image analysis, supporting clinic
workflows in multiple domains. Previous segmentation methods include 2D
convolution neural networks (CNN) based approaches, fed by CT image slices that
lack the structural knowledge in axial view, and 3D CNN-based methods with the
expensive computation cost in multi-organ segmentation applications. This paper
introduces an adversarial performance validation network into a 2D-to-3D
segmentation framework. The classifier and performance validator competition
contribute to accurate segmentation results via back-propagation. The proposed
network organically converts the 2D-coarse result to 3D high-quality
segmentation masks in a coarse-to-fine manner, allowing joint optimization to
improve segmentation accuracy. Besides, the structural information of one
specific organ is depicted by a statistics-meaningful prior bounding box, which
is transformed into a global feature leveraging the learning process in 3D fine
segmentation. The experiments on the NIH pancreas segmentation dataset
demonstrate the proposed network achieves state-of-the-art accuracy on small
organ segmentation and outperforms the previous best. High accuracy is also
reported on multi-organ segmentation in a dataset collected by ourselves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Deeper Understanding of Skeleton-based Gait Recognition. (arXiv:2204.07855v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07855">
<div class="article-summary-box-inner">
<span><p>Gait recognition is a promising biometric with unique properties for
identifying individuals from a long distance by their walking patterns. In
recent years, most gait recognition methods used the person's silhouette to
extract the gait features. However, silhouette images can lose fine-grained
spatial information, suffer from (self) occlusion, and be challenging to obtain
in real-world scenarios. Furthermore, these silhouettes also contain other
visual clues that are not actual gait features and can be used for
identification, but also to fool the system. Model-based methods do not suffer
from these problems and are able to represent the temporal motion of body
joints, which are actual gait features. The advances in human pose estimation
started a new era for model-based gait recognition with skeleton-based gait
recognition. In this work, we propose an approach based on Graph Convolutional
Networks (GCNs) that combines higher-order inputs, and residual networks to an
efficient architecture for gait recognition. Extensive experiments on the two
popular gait datasets, CASIA-B and OUMVLP-Pose, show a massive improvement (3x)
of the state-of-the-art (SotA) on the largest gait dataset OUMVLP-Pose and
strong temporal modeling capabilities. Finally, we visualize our method to
understand skeleton-based gait recognition better and to show that we model
real gait features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GHM Wavelet Transform for Deep Image Super Resolution. (arXiv:2204.07862v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07862">
<div class="article-summary-box-inner">
<span><p>The GHM multi-level discrete wavelet transform is proposed as preprocessing
for image super resolution with convolutional neural networks. Previous works
perform analysis with the Haar wavelet only. In this work, 37 single-level
wavelets are experimentally analyzed from Haar, Daubechies, Biorthogonal,
Reverse Biorthogonal, Coiflets, and Symlets wavelet families. All single-level
wavelets report similar results indicating that the convolutional neural
network is invariant to choice of wavelet in a single-level filter approach.
However, the GHM multi-level wavelet achieves higher quality reconstructions
than the single-level wavelets. Three large data sets are used for the
experiments: DIV2K, a dataset of textures, and a dataset of satellite images.
The approximate high resolution images are compared using seven objective error
measurements. A convolutional neural network based approach using wavelet
transformed images has good results in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Human Pose Estimation for Free-from and Moving Activities Using WiFi. (arXiv:2204.07878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07878">
<div class="article-summary-box-inner">
<span><p>This paper presents GoPose, a 3D skeleton-based human pose estimation system
that uses WiFi devices at home. Our system leverages the WiFi signals reflected
off the human body for 3D pose estimation. In contrast to prior systems that
need specialized hardware or dedicated sensors, our system does not require a
user to wear or carry any sensors and can reuse the WiFi devices that already
exist in a home environment for mass adoption. To realize such a system, we
leverage the 2D AoA spectrum of the signals reflected from the human body and
the deep learning techniques. In particular, the 2D AoA spectrum is proposed to
locate different parts of the human body as well as to enable
environment-independent pose estimation. Deep learning is incorporated to model
the complex relationship between the 2D AoA spectrums and the 3D skeletons of
the human body for pose tracking. Our evaluation results show GoPose achieves
around 4.7cm of accuracy under various scenarios including tracking unseen
activities and under NLoS scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping LiDAR and Camera Measurements in a Dual Top-View Grid Representation Tailored for Automated Vehicles. (arXiv:2204.07887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07887">
<div class="article-summary-box-inner">
<span><p>We present a generic evidential grid mapping pipeline designed for imaging
sensors such as LiDARs and cameras. Our grid-based evidential model contains
semantic estimates for cell occupancy and ground separately. We specify the
estimation steps for input data represented by point sets, but mainly focus on
input data represented by images such as disparity maps or LiDAR range images.
Instead of relying on an external ground segmentation only, we deduce occupancy
evidence by analyzing the surface orientation around measurements. We conduct
experiments and evaluate the presented method using LiDAR and stereo camera
data recorded in real traffic scenarios. Our method estimates cell occupancy
robustly and with a high level of detail while maximizing efficiency and
minimizing the dependency to external processing modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SymForce: Symbolic Computation and Code Generation for Robotics. (arXiv:2204.07889v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07889">
<div class="article-summary-box-inner">
<span><p>We present SymForce, a fast symbolic computation and code generation library
for robotics applications like computer vision, state estimation, motion
planning, and controls. SymForce combines the development speed and flexibility
of symbolic mathematics with the performance of autogenerated, highly optimized
code in C++ or any target runtime language. SymForce provides geometry and
camera types, Lie group operations, and branchless singularity handling for
creating and analyzing complex symbolic expressions in Python, built on top of
SymPy. Generated functions can be integrated as factors into our tangent space
nonlinear optimizer, which is highly optimized for real-time production use. We
introduce novel methods to automatically compute tangent space Jacobians,
eliminating the need for bug-prone handwritten derivatives. This workflow
enables faster runtime code, faster development time, and fewer lines of
handwritten code versus the state-of-the-art. Our experiments demonstrate that
our approach can yield order of magnitude speedups on computational tasks core
to robotics. Code is available at https://github.com/symforce-org/symforce .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Action Detection: Analysing Limitations and Challenges. (arXiv:2204.07892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07892">
<div class="article-summary-box-inner">
<span><p>Beyond possessing large enough size to feed data hungry machines (eg,
transformers), what attributes measure the quality of a dataset? Assuming that
the definitions of such attributes do exist, how do we quantify among their
relative existences? Our work attempts to explore these questions for video
action detection. The task aims to spatio-temporally localize an actor and
assign a relevant action class. We first analyze the existing datasets on video
action detection and discuss their limitations. Next, we propose a new dataset,
Multi Actor Multi Action (MAMA) which overcomes these limitations and is more
suitable for real world applications. In addition, we perform a biasness study
which analyzes a key property differentiating videos from static images: the
temporal aspect. This reveals if the actions in these datasets really need the
motion information of an actor, or whether they predict the occurrence of an
action even by looking at a single frame. Finally, we investigate the widely
held assumptions on the importance of temporal ordering: is temporal ordering
important for detecting these actions? Such extreme experiments show existence
of biases which have managed to creep into existing methods inspite of careful
modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction. (arXiv:2204.07908v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07908">
<div class="article-summary-box-inner">
<span><p>Existing leading methods for spectral reconstruction (SR) focus on designing
deeper or wider convolutional neural networks (CNNs) to learn the end-to-end
mapping from the RGB image to its hyperspectral image (HSI). These CNN-based
methods achieve impressive restoration performance while showing limitations in
capturing the long-range dependencies and self-similarity prior. To cope with
this problem, we propose a novel Transformer-based method, Multi-stage
Spectral-wise Transformer (MST++), for efficient spectral reconstruction. In
particular, we employ Spectral-wise Multi-head Self-attention (S-MSA) that is
based on the HSI spatially sparse while spectrally self-similar nature to
compose the basic unit, Spectral-wise Attention Block (SAB). Then SABs build up
Single-stage Spectral-wise Transformer (SST) that exploits a U-shaped structure
to extract multi-resolution contextual information. Finally, our MST++,
cascaded by several SSTs, progressively improves the reconstruction quality
from coarse to fine. Comprehensive experiments show that our MST++
significantly outperforms other state-of-the-art methods. In the NTIRE 2022
Spectral Reconstruction Challenge, our approach won the First place. Code and
pre-trained models are publicly available at
https://github.com/caiyuanhao1998/MST-plus-plus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Goes beyond Multi-modal Fusion in One-stage Referring Expression Comprehension: An Empirical Study. (arXiv:2204.07913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07913">
<div class="article-summary-box-inner">
<span><p>Most of the existing work in one-stage referring expression comprehension
(REC) mainly focuses on multi-modal fusion and reasoning, while the influence
of other factors in this task lacks in-depth exploration. To fill this gap, we
conduct an empirical study in this paper. Concretely, we first build a very
simple REC network called SimREC, and ablate 42 candidate designs/settings,
which covers the entire process of one-stage REC from network design to model
training. Afterwards, we conduct over 100 experimental trials on three
benchmark datasets of REC. The extensive experimental results not only show the
key factors that affect REC performance in addition to multi-modal fusion,
e.g., multi-scale features and data augmentation, but also yield some findings
that run counter to conventional understanding. For example, as a vision and
language (V&amp;L) task, REC does is less impacted by language prior. In addition,
with a proper combination of these findings, we can improve the performance of
SimREC by a large margin, e.g., +27.12% on RefCOCO+, which outperforms all
existing REC methods. But the most encouraging finding is that with much less
training overhead and parameters, SimREC can still achieve better performance
than a set of large-scale pre-trained models, e.g., UNITER and VILLA,
portraying the special role of REC in existing V&amp;L research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07921">
<div class="article-summary-box-inner">
<span><p>The geometric high-order regularization methods such as mean curvature and
Gaussian curvature, have been intensively studied during the last decades due
to their abilities in preserving geometric properties including image edges,
corners, and image contrast. However, the dilemma between restoration quality
and computational efficiency is an essential roadblock for high-order methods.
In this paper, we propose fast multi-grid algorithms for minimizing both mean
curvature and Gaussian curvature energy functionals without sacrificing the
accuracy for efficiency. Unlike the existing approaches based on operator
splitting and the Augmented Lagrangian method (ALM), no artificial parameters
are introduced in our formulation, which guarantees the robustness of the
proposed algorithm. Meanwhile, we adopt the domain decomposition method to
promote parallel computing and use the fine-to-coarse structure to accelerate
the convergence. Numerical experiments are presented on both image denoising
and CT reconstruction problem to demonstrate the ability to recover image
texture and the efficiency of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerated MRI With Deep Linear Convolutional Transform Learning. (arXiv:2204.07923v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07923">
<div class="article-summary-box-inner">
<span><p>Recent studies show that deep learning (DL) based MRI reconstruction
outperforms conventional methods, such as parallel imaging and compressed
sensing (CS), in multiple applications. Unlike CS that is typically implemented
with pre-determined linear representations for regularization, DL inherently
uses a non-linear representation learned from a large database. Another line of
work uses transform learning (TL) to bridge the gap between these two
approaches by learning linear representations from data. In this work, we
combine ideas from CS, TL and DL reconstructions to learn deep linear
convolutional transforms as part of an algorithm unrolling approach. Using
end-to-end training, our results show that the proposed technique can
reconstruct MR images to a level comparable to DL methods, while supporting
uniform undersampling patterns unlike conventional CS methods. Our proposed
method relies on convex sparse image reconstruction with linear representation
at inference time, which may be beneficial for characterizing robustness,
stability and generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleT2F: Generating Human Faces from Textual Description Using StyleGAN2. (arXiv:2204.07924v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07924">
<div class="article-summary-box-inner">
<span><p>AI-driven image generation has improved significantly in recent years.
Generative adversarial networks (GANs), like StyleGAN, are able to generate
high-quality realistic data and have artistic control over the output, as well.
In this work, we present StyleT2F, a method of controlling the output of
StyleGAN2 using text, in order to be able to generate a detailed human face
from textual description. We utilize StyleGAN's latent space to manipulate
different facial features and conditionally sample the required latent code,
which embeds the facial features mentioned in the input text. Our method proves
to capture the required features correctly and shows consistency between the
input text and the output images. Moreover, our method guarantees
disentanglement on manipulating a wide range of facial features that
sufficiently describes a human face.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Defense of Subspace Tracker: Orthogonal Embedding for Visual Tracking. (arXiv:2204.07927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07927">
<div class="article-summary-box-inner">
<span><p>The paper focuses on a classical tracking model, subspace learning, grounded
on the fact that the targets in successive frames are considered to reside in a
low-dimensional subspace or manifold due to the similarity in their
appearances. In recent years, a number of subspace trackers have been proposed
and obtained impressive results. Inspired by the most recent results that the
tracking performance is boosted by the subspace with discrimination capability
learned over the recently localized targets and their immediately surrounding
background, this work aims at solving such a problem: how to learn a robust
low-dimensional subspace to accurately and discriminatively represent these
target and background samples. To this end, a discriminative approach, which
reliably separates the target from its surrounding background, is injected into
the subspace learning by means of joint learning, achieving a
dimension-adaptive subspace with superior discrimination capability. The
proposed approach is extensively evaluated and compared with the
state-of-the-art trackers on four popular tracking benchmarks. The experimental
results demonstrate that the proposed tracker performs competitively against
its counterparts. In particular, it achieves more than 9% performance increase
compared with the state-of-the-art subspace trackers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Intervention for Subject-Deconfounded Facial Action Unit Recognition. (arXiv:2204.07935v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07935">
<div class="article-summary-box-inner">
<span><p>Subject-invariant facial action unit (AU) recognition remains challenging for
the reason that the data distribution varies among subjects. In this paper, we
propose a causal inference framework for subject-invariant facial action unit
recognition. To illustrate the causal effect existing in AU recognition task,
we formulate the causalities among facial images, subjects, latent AU semantic
relations, and estimated AU occurrence probabilities via a structural causal
model. By constructing such a causal diagram, we clarify the causal effect
among variables and propose a plug-in causal intervention module, CIS, to
deconfound the confounder \emph{Subject} in the causal diagram. Extensive
experiments conducted on two commonly used AU benchmark datasets, BP4D and
DISFA, show the effectiveness of our CIS, and the model with CIS inserted,
CISNet, has achieved state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wound Severity Classification using Deep Neural Network. (arXiv:2204.07942v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07942">
<div class="article-summary-box-inner">
<span><p>The classification of wound severity is a critical step in wound diagnosis.
An effective classifier can help wound professionals categorize wound
conditions more quickly and affordably, allowing them to choose the best
treatment option. This study used wound photos to construct a deep neural
network-based wound severity classifier that classified them into one of three
classes: green, yellow, or red. The green class denotes wounds still in the
early stages of healing and are most likely to recover with adequate care.
Wounds in the yellow category require more attention and treatment than those
in the green category. Finally, the red class denotes the most severe wounds
that require prompt attention and treatment. A dataset containing different
types of wound images is designed with the help of wound specialists. Nine deep
learning models are used with applying the concept of transfer learning.
Several stacked models are also developed by concatenating these transfer
learning models. The maximum accuracy achieved on multi-class classification is
68.49%. In addition, we achieved 78.79%, 81.40%, and 77.57% accuracies on green
vs. yellow, green vs. red, and yellow vs. red classifications for binary
classifications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-Supervised Contrastive Loss and View-Aware-Based Post-Processing for Vehicle Re-Identification. (arXiv:2204.07943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07943">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a Global-Supervised Contrastive loss and a
view-aware-based post-processing (VABPP) method for the field of vehicle
re-identification. The traditional supervised contrastive loss calculates the
distances of features within the batch, so it has the local attribute. While
the proposed Global-Supervised Contrastive loss has new properties and has good
global attributes, the positive and negative features of each anchor in the
training process come from the entire training set. The proposed VABPP method
is the first time that the view-aware-based method is used as a post-processing
method in the field of vehicle re-identification. The advantages of VABPP are
that, first, it is only used during testing and does not affect the training
process. Second, as a post-processing method, it can be easily integrated into
other trained re-id models. We directly apply the view-pair distance scaling
coefficient matrix calculated by the model trained in this paper to another
trained re-id model, and the VABPP method greatly improves its performance,
which verifies the feasibility of the VABPP method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DR-GAN: Distribution Regularization for Text-to-Image Generation. (arXiv:2204.07945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07945">
<div class="article-summary-box-inner">
<span><p>This paper presents a new Text-to-Image generation model, named Distribution
Regularization Generative Adversarial Network (DR-GAN), to generate images from
text descriptions from improved distribution learning. In DR-GAN, we introduce
two novel modules: a Semantic Disentangling Module (SDM) and a Distribution
Normalization Module (DNM). SDM combines the spatial self-attention mechanism
and a new Semantic Disentangling Loss (SDL) to help the generator distill key
semantic information for the image generation. DNM uses a Variational
Auto-Encoder (VAE) to normalize and denoise the image latent distribution,
which can help the discriminator better distinguish synthesized images from
real images. DNM also adopts a Distribution Adversarial Loss (DAL) to guide the
generator to align with normalized real image distributions in the latent
space. Extensive experiments on two public datasets demonstrated that our
DR-GAN achieved a competitive performance in the Text-to-Image task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrated In-vehicle Monitoring System Using 3D Human Pose Estimation and Seat Belt Segmentation. (arXiv:2204.07946v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07946">
<div class="article-summary-box-inner">
<span><p>Recently, along with interest in autonomous vehicles, the importance of
monitoring systems for both drivers and passengers inside vehicles has been
increasing. This paper proposes a novel in-vehicle monitoring system the
combines 3D pose estimation, seat-belt segmentation, and seat-belt status
classification networks. Our system outputs various information necessary for
monitoring by accurately considering the data characteristics of the in-vehicle
environment. Specifically, the proposed 3D pose estimation directly estimates
the absolute coordinates of keypoints for a driver and passengers, and the
proposed seat-belt segmentation is implemented by applying a structure based on
the feature pyramid. In addition, we propose a classification task to
distinguish between normal and abnormal states of wearing a seat belt using
results that combine 3D pose estimation with seat-belt segmentation. These
tasks can be learned simultaneously and operate in real-time. Our method was
evaluated on a private dataset we newly created and annotated. The experimental
results show that our method has significantly high performance that can be
applied directly to real in-vehicle monitoring systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Signatures. (arXiv:2204.07953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07953">
<div class="article-summary-box-inner">
<span><p>In this work we investigate the use of the Signature Transform in the context
of Learning. Under this assumption, we advance a supervised framework that
provides state-of-the-art classification accuracy with the use of very few
labels without the need of credit assignment and with minimal or no
overfitting. We leverage tools from harmonic analysis by the use of the
signature and log-signature and use as a score function RMSE and MAE Signature
and log-signature. We develop a closed-form equation to compute probably good
optimal scale factors. Classification is performed at the CPU level orders of
magnitude faster than other methods. We report results on AFHQ dataset, Four
Shapes, MNIST and CIFAR10 achieving 100% accuracy on all tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2204.07955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07955">
<div class="article-summary-box-inner">
<span><p>As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment
Analysis (MABSA) has attracted increasing attention in recent years. However,
previous approaches either (i) use separately pre-trained visual and textual
models, which ignore the crossmodal alignment or (ii) use vision-language
models pre-trained with general pre-training tasks, which are inadequate to
identify finegrained aspects, opinions, and their alignments across modalities.
To tackle these limitations, we propose a task-specific Vision-Language
Pre-training framework for MABSA (VLPMABSA), which is a unified multimodal
encoder-decoder architecture for all the pretraining and downstream tasks. We
further design three types of task-specific pre-training tasks from the
language, vision, and multimodal modalities, respectively. Experimental results
show that our approach generally outperforms the state-of-the-art approaches on
three MABSA subtasks. Further analysis demonstrates the effectiveness of each
pretraining task. The source code is publicly released at
https://github.com/NUSTM/VLP-MABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Extendable, Efficient and Effective Transformer-based Object Detector. (arXiv:2204.07962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07962">
<div class="article-summary-box-inner">
<span><p>Transformers have been widely used in numerous vision problems especially for
visual recognition and detection. Detection transformers are the first fully
end-to-end learning systems for object detection, while vision transformers are
the first fully transformer-based architecture for image classification. In
this paper, we integrate Vision and Detection Transformers (ViDT) to construct
an effective and efficient object detector. ViDT introduces a reconfigured
attention module to extend the recent Swin Transformer to be a standalone
object detector, followed by a computationally efficient transformer decoder
that exploits multi-scale features and auxiliary techniques essential to boost
the detection performance without much increase in computational load. In
addition, we extend it to ViDT+ to support joint-task learning for object
detection and instance segmentation. Specifically, we attach an efficient
multi-scale feature fusion layer and utilize two more auxiliary training
losses, IoU-aware loss and token labeling loss. Extensive evaluation results on
the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP
and latency trade-off among existing fully transformer-based object detectors,
and its extended ViDT+ achieves 53.2AP owing to its high scalability for large
models. The source code and trained models are available at
https://github.com/naver-ai/vidt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFSC: Adaptive Fourier Space Compression for Anomaly Detection. (arXiv:2204.07963v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07963">
<div class="article-summary-box-inner">
<span><p>Anomaly Detection (AD) on medical images enables a model to recognize any
type of anomaly pattern without lesion-specific supervised learning. Data
augmentation based methods construct pseudo-healthy images by "pasting" fake
lesions on real healthy ones, and a network is trained to predict healthy
images in a supervised manner. The lesion can be found by difference between
the unhealthy input and pseudo-healthy output. However, using only manually
designed fake lesions fail to approximate to irregular real lesions, hence
limiting the model generalization. We assume by exploring the intrinsic data
property within images, we can distinguish previously unseen lesions from
healthy regions in an unhealthy image. In this study, we propose an Adaptive
Fourier Space Compression (AFSC) module to distill healthy feature for AD. The
compression of both magnitude and phase in frequency domain addresses the hyper
intensity and diverse position of lesions. Experimental results on the BraTS
and MS-SEG datasets demonstrate an AFSC baseline is able to produce promising
detection results, and an AFSC module can be effectively embedded into existing
AD methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Target-Relevant Knowledge Preservation for Multi-Source Domain Adaptive Object Detection. (arXiv:2204.07964v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07964">
<div class="article-summary-box-inner">
<span><p>Domain adaptive object detection (DAOD) is a promising way to alleviate
performance drop of detectors in new scenes. Albeit great effort made in single
source domain adaptation, a more generalized task with multiple source domains
remains not being well explored, due to knowledge degradation during their
combination. To address this issue, we propose a novel approach, namely
target-relevant knowledge preservation (TRKP), to unsupervised multi-source
DAOD. Specifically, TRKP adopts the teacher-student framework, where the
multi-head teacher network is built to extract knowledge from labeled source
domains and guide the student network to learn detectors in unlabeled target
domain. The teacher network is further equipped with an adversarial
multi-source disentanglement (AMSD) module to preserve source domain-specific
knowledge and simultaneously perform cross-domain alignment. Besides, a
holistic target-relevant mining (HTRM) scheme is developed to re-weight the
source images according to the source-target relevance. By this means, the
teacher network is enforced to capture target-relevant knowledge, thus
benefiting decreasing domain shift when mentoring object detection in the
target domain. Extensive experiments are conducted on various widely used
benchmarks with new state-of-the-art scores reported, highlighting the
effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint. (arXiv:2204.07965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07965">
<div class="article-summary-box-inner">
<span><p>Active learning is a promising alternative to alleviate the issue of high
annotation cost in the computer vision tasks by consciously selecting more
informative samples to label. Active learning for object detection is more
challenging and existing efforts on it are relatively rare. In this paper, we
propose a novel hybrid approach to address this problem, where the
instance-level uncertainty and diversity are jointly considered in a bottom-up
manner. To balance the computational complexity, the proposed approach is
designed as a two-stage procedure. At the first stage, an Entropy-based
Non-Maximum Suppression (ENMS) is presented to estimate the uncertainty of
every image, which performs NMS according to the entropy in the feature space
to remove predictions with redundant information gains. At the second stage, a
diverse prototype (DivProto) strategy is explored to ensure the diversity
across images by progressively converting it into the intra-class and
inter-class diversities of the entropy-based class-specific prototypes.
Extensive experiments are conducted on MS COCO and Pascal VOC, and the proposed
approach achieves state of the art results and significantly outperforms the
other counterparts, highlighting its superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmentation Invariance and Adaptive Sampling in Semantic Segmentation of Agricultural Aerial Images. (arXiv:2204.07969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07969">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the problem of Semantic Segmentation for
agricultural aerial imagery. We observe that the existing methods used for this
task are designed without considering two characteristics of the aerial data:
(i) the top-down perspective implies that the model cannot rely on a fixed
semantic structure of the scene, because the same scene may be experienced with
different rotations of the sensor; (ii) there can be a strong imbalance in the
distribution of semantic classes because the relevant objects of the scene may
appear at extremely different scales (e.g., a field of crops and a small
vehicle). We propose a solution to these problems based on two ideas: (i) we
use together a set of suitable augmentation and a consistency loss to guide the
model to learn semantic representations that are invariant to the photometric
and geometric shifts typical of the top-down perspective (Augmentation
Invariance); (ii) we use a sampling method (Adaptive Sampling) that selects the
training images based on a measure of pixel-wise distribution of classes and
actual network confidence. With an extensive set of experiments conducted on
the Agriculture-Vision dataset, we demonstrate that our proposed strategies
improve the performance of the current state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic spinal curvature measurement on ultrasound spine images using Faster R-CNN. (arXiv:2204.07988v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07988">
<div class="article-summary-box-inner">
<span><p>Ultrasound spine imaging technique has been applied to the assessment of
spine deformity. However, manual measurements of scoliotic angles on ultrasound
images are time-consuming and heavily rely on raters experience. The objectives
of this study are to construct a fully automatic framework based on Faster
R-CNN for detecting vertebral lamina and to measure the fitting spinal curves
from the detected lamina pairs. The framework consisted of two closely linked
modules: 1) the lamina detector for identifying and locating each lamina pairs
on ultrasound coronal images, and 2) the spinal curvature estimator for
calculating the scoliotic angles based on the chain of detected lamina. Two
hundred ultrasound images obtained from AIS patients were identified and used
for the training and evaluation of the proposed method. The experimental
results showed the 0.76 AP on the test set, and the Mean Absolute Difference
(MAD) between automatic and manual measurement which was within the clinical
acceptance error. Meanwhile the correlation between automatic measurement and
Cobb angle from radiographs was 0.79. The results revealed that our proposed
technique could provide accurate and reliable automatic curvature measurements
on ultrasound spine images for spine deformities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PiouCrypt: Decentralized Lattice-based Method for Visual Symmetric Cryptography. (arXiv:2204.08017v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08017">
<div class="article-summary-box-inner">
<span><p>In recent years, establishing secure visual communications has turned into
one of the essential problems for security engineers and researchers. However,
only limited novel solutions are provided for image encryption, and limiting
the visual cryptography to only limited schemes can bring up negative
consequences, especially with emerging quantum computational systems. This
paper presents a novel algorithm for establishing secure private visual
communication. The proposed method has a layered architecture with several
cohesive components, and corresponded with an NP-hard problem, despite its
symmetric structure. This two-step technique is not limited to gray-scale
pictures, and furthermore, utilizing a lattice structure causes to proposed
method has optimal resistance for the post-quantum era, and is relatively
secure from the theoretical dimension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VDTR: Video Deblurring with Transformer. (arXiv:2204.08023v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08023">
<div class="article-summary-box-inner">
<span><p>Video deblurring is still an unsolved problem due to the challenging
spatio-temporal modeling process. While existing convolutional neural
network-based methods show a limited capacity for effective spatial and
temporal modeling for video deblurring. This paper presents VDTR, an effective
Transformer-based model that makes the first attempt to adapt Transformer for
video deblurring. VDTR exploits the superior long-range and relation modeling
capabilities of Transformer for both spatial and temporal modeling. However, it
is challenging to design an appropriate Transformer-based model for video
deblurring due to the complicated non-uniform blurs, misalignment across
multiple frames and the high computational costs for high-resolution spatial
modeling. To address these problems, VDTR advocates performing attention within
non-overlapping windows and exploiting the hierarchical structure for
long-range dependencies modeling. For frame-level spatial modeling, we propose
an encoder-decoder Transformer that utilizes multi-scale features for
deblurring. For multi-frame temporal modeling, we adapt Transformer to fuse
multiple spatial features efficiently. Compared with CNN-based methods, the
proposed method achieves highly competitive results on both synthetic and
real-world video deblurring benchmarks, including DVD, GOPRO, REDS and BSD. We
hope such a Transformer-based architecture can serve as a powerful alternative
baseline for video deblurring and other video restoration tasks. The source
code will be available at \url{https://github.com/ljzycmd/VDTR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Z-axis, X-axis, Weight and Disambiguation Methods for Constructing Local Reference Frame in 3D Registration: An Evaluation. (arXiv:2204.08024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08024">
<div class="article-summary-box-inner">
<span><p>The local reference frame (LRF), as an independent coordinate system
generated on a local 3D surface, is widely used in 3D local feature descriptor
construction and 3D transformation estimation which are two key steps in the
local method-based surface matching. There are numerous LRF methods have been
proposed in literatures. In these methods, the x- and z-axis are commonly
generated by different methods or strategies, and some x-axis methods are
implemented on the basis of a z-axis being given. In addition, the weight and
disambiguation methods are commonly used in these LRF methods. In existing
evaluations of LRF, each LRF method is evaluated with a complete form. However,
the merits and demerits of the z-axis, x-axis, weight and disambiguation
methods in LRF construction are unclear. In this paper, we comprehensively
analyze the z-axis, x-axis, weight and disambiguation methods in existing LRFs,
and obtain six z-axis and eight x-axis, five weight and two disambiguation
methods. The performance of these methods are comprehensively evaluated on six
standard datasets with different application scenarios and nuisances.
Considering the evaluation outcomes, the merits and demerits of different
weight, disambiguation, z- and x-axis methods are analyzed and summarized. The
experimental result also shows that some new designed LRF axes present superior
performance compared with the state-of-the-art ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Mechanism based Cognition-level Scene Understanding. (arXiv:2204.08027v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08027">
<div class="article-summary-box-inner">
<span><p>Given a question-image input, the Visual Commonsense Reasoning (VCR) model
can predict an answer with the corresponding rationale, which requires
inference ability from the real world. The VCR task, which calls for exploiting
the multi-source information as well as learning different levels of
understanding and extensive commonsense knowledge, is a cognition-level scene
understanding task. The VCR task has aroused researchers' interest due to its
wide range of applications, including visual question answering, automated
vehicle systems, and clinical decision support. Previous approaches to solving
the VCR task generally rely on pre-training or exploiting memory with long
dependency relationship encoded models. However, these approaches suffer from a
lack of generalizability and losing information in long sequences. In this
paper, we propose a parallel attention-based cognitive VCR network PAVCR, which
fuses visual-textual information efficiently and encodes semantic information
in parallel to enable the model to capture rich information for cognition-level
inference. Extensive experiments show that the proposed model yields
significant improvements over existing methods on the benchmark VCR dataset.
Moreover, the proposed model provides intuitive interpretation into visual
commonsense reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning based Automatic Detection of Dicentric Chromosome. (arXiv:2204.08029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08029">
<div class="article-summary-box-inner">
<span><p>Automatic detection of dicentric chromosomes is an essential step to estimate
radiation exposure and development of end to end emergency bio dosimetry
systems. During accidents, a large amount of data is required to be processed
for extensive testing to formulate a medical treatment plan for the masses,
which requires this process to be automated. Current approaches require human
adjustments according to the data and therefore need a human expert to
calibrate the system. This paper proposes a completely data driven framework
which requires minimum intervention of field experts and can be deployed in
emergency cases with relative ease. Our approach involves YOLOv4 to detect the
chromosomes and remove the debris in each image, followed by a classifier that
differentiates between an analysable chromosome and a non-analysable one.
Images are extracted from YOLOv4 based on the protocols described by
WHO-BIODOSNET. The analysable chromosome is classified as Monocentric or
Dicentric and an image is accepted for consideration of dose estimation based
on the analysable chromosome count. We report an accuracy in dicentric
identification of 94.33% on a 1:1 split of Dicentric and Monocentric
Chromosomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Adaptive Task-Related Component Analysis Method for SSVEP recognition. (arXiv:2204.08030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08030">
<div class="article-summary-box-inner">
<span><p>Steady-state visual evoked potential (SSVEP) recognition methods are equipped
with learning from the subject's calibration data, and they can achieve extra
high performance in the SSVEP-based brain-computer interfaces (BCIs), however
their performance deteriorate drastically if the calibration trials are
insufficient. This study develops a new method to learn from limited
calibration data and it proposes and evaluates a novel adaptive data-driven
spatial filtering approach for enhancing SSVEPs detection. The spatial filter
learned from each stimulus utilizes temporal information from the corresponding
EEG trials. To introduce the temporal information into the overall procedure,
an multitask learning approach, based on the bayesian framework, is adopted.
The performance of the proposed method was evaluated into two publicly
available benchmark datasets, and the results demonstrated that our method
outperform competing methods by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NICO++: Towards Better Benchmarking for Domain Generalization. (arXiv:2204.08040v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08040">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable performance that modern deep neural networks have
achieved on independent and identically distributed (I.I.D.) data, they can
crash under distribution shifts. Most current evaluation methods for domain
generalization (DG) adopt the leave-one-out strategy as a compromise on the
limited number of domains. We propose a large-scale benchmark with extensive
labeled domains named NICO++{\ddag} along with more rational evaluation methods
for comprehensively evaluating DG algorithms. To evaluate DG datasets, we
propose two metrics to quantify covariate shift and concept shift,
respectively. Two novel generalization bounds from the perspective of data
construction are proposed to prove that limited concept shift and significant
covariate shift favor the evaluation capability for generalization. Through
extensive experiments, NICO++ shows its superior evaluation capability compared
with current DG datasets and its contribution in alleviating unfairness caused
by the leak of oracle knowledge in model selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Hippocampus Segmentation with Transformers. (arXiv:2204.08043v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08043">
<div class="article-summary-box-inner">
<span><p>In clinical settings, where acquisition conditions and patient populations
change over time, continual learning is key for ensuring the safe use of deep
neural networks. Yet most existing work focuses on convolutional architectures
and image classification. Instead, radiologists prefer to work with
segmentation models that outline specific regions-of-interest, for which
Transformer-based architectures are gaining traction. The self-attention
mechanism of Transformers could potentially mitigate catastrophic forgetting,
opening the way for more robust medical image segmentation. In this work, we
explore how recently-proposed Transformer mechanisms for semantic segmentation
behave in sequential learning scenarios, and analyse how best to adapt
continual learning strategies for this setting. Our evaluation on hippocampus
segmentation shows that Transformer mechanisms mitigate catastrophic forgetting
for medical image segmentation compared to purely convolutional architectures,
and demonstrates that regularising ViT modules should be done with caution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration. (arXiv:2204.08058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08058">
<div class="article-summary-box-inner">
<span><p>Multimodal video-audio-text understanding and generation can benefit from
datasets that are narrow but rich. The narrowness allows bite-sized challenges
that the research community can make progress on. The richness ensures we are
making progress along the core challenges. To this end, we present a
large-scale video-audio-text dataset MUGEN, collected using the open-sourced
platform game CoinRun [11]. We made substantial modifications to make the game
richer by introducing audio and enabling new interactions. We trained RL agents
with different objectives to navigate the game and interact with 13 objects and
characters. This allows us to automatically extract a large collection of
diverse videos and associated audio. We sample 375K video clips (3.2s each) and
collect text descriptions from human annotators. Each video has additional
annotations that are extracted automatically from the game engine, such as
accurate semantic maps for each frame and templated textual descriptions.
Altogether, MUGEN can help progress research in many tasks in multimodal
understanding and generation. We benchmark representative approaches on tasks
involving video-audio-text retrieval and generation. Our dataset and code are
released at: https://mugen-org.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network. (arXiv:2204.08084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08084">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework to learn 3D point cloud semantics from 2D
multi-view image observations containing pose error. On the one hand, directly
learning from the massive, unstructured and unordered 3D point cloud is
computationally and algorithmically more difficult than learning from
compactly-organized and context-rich 2D RGB images. On the other hand, both
LiDAR point cloud and RGB images are captured in standard automated-driving
datasets. This motivates us to conduct a "task transfer" paradigm so that 3D
semantic segmentation benefits from aggregating 2D semantic cues, albeit pose
noises are contained in 2D image observations. Among all difficulties, pose
noise and erroneous prediction from 2D semantic segmentation approaches are the
main challenges for the task transfer. To alleviate the influence of those
factor, we perceive each 3D point using multi-view images and for each single
image a patch observation is associated. Moreover, the semantic labels of a
block of neighboring 3D points are predicted simultaneously, enabling us to
exploit the point structure prior to further improve the performance. A
hierarchical full attention network~(HiFANet) is designed to sequentially
aggregates patch, bag-of-frames and inter-point semantic cues, with
hierarchical attention mechanism tailored for different level of semantic cues.
Also, each preceding attention block largely reduces the feature size before
feeding to the next attention block, making our framework slim. Experiment
results on Semantic-KITTI show that the proposed framework outperforms existing
3D point cloud based methods significantly, it requires much less training data
and exhibits tolerance to pose noise. The code is available at
https://github.com/yuhanghe01/HiFANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Compositional Representations for Effective Low-Shot Generalization. (arXiv:2204.08090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08090">
<div class="article-summary-box-inner">
<span><p>We propose Recognition as Part Composition (RPC), an image encoding approach
inspired by human cognition. It is based on the cognitive theory that humans
recognize complex objects by components, and that they build a small compact
vocabulary of concepts to represent each instance with. RPC encodes images by
first decomposing them into salient parts, and then encoding each part as a
mixture of a small number of prototypes, each representing a certain concept.
We find that this type of learning inspired by human cognition can overcome
hurdles faced by deep convolutional networks in low-shot generalization tasks,
like zero-shot learning, few-shot learning and unsupervised domain adaptation.
Furthermore, we find a classifier using an RPC image encoder is fairly robust
to adversarial attacks, that deep neural networks are known to be prone to.
Given that our image encoding principle is based on human cognition, one would
expect the encodings to be interpretable by humans, which we find to be the
case via crowd-sourcing experiments. Finally, we propose an application of
these interpretable encodings in the form of generating synthetic attribute
annotations for evaluating zero-shot learning methods on new datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset for Analyzing Various Gaze Zones and Distracted Behaviors of a Driver. (arXiv:2204.08096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08096">
<div class="article-summary-box-inner">
<span><p>This article presents a synthetic dataset for machine learning models to
detect and analyze drivers' various distracted behavior and different gaze
zones. We collected the data in a stationary vehicle using three in-vehicle
cameras positioned at locations: on the dashboard, near the rearview mirror,
and on the top right-side window corner. The dataset contains two activity
types: distracted activities, and gaze zones for each participant and each
activity type has two sets: without appearance blocks and with appearance
blocks such as wearing a hat or sunglasses. The order and duration of each
activity for each participant are random. In addition, the dataset contains
manual annotations for each activity, having its start and end time annotated.
Researchers could use this dataset to evaluate the performance of machine
learning algorithms for the classification of various distracting activities
and gaze zones of drivers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction. (arXiv:2204.08107v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08107">
<div class="article-summary-box-inner">
<span><p>In this paper we present a novel method for a naive agent to detect novel
objects it encounters in an interaction. We train a reinforcement learning
policy on a stacking task given a known object type, and then observe the
results of the agent attempting to stack various other objects based on the
same trained policy. By extracting embedding vectors from a convolutional
neural net trained over the results of the aforementioned stacking play, we can
determine the similarity of a given object to known object types, and determine
if the given object is likely dissimilar enough to the known types to be
considered a novel class of object. We present the results of this method on
two datasets gathered using two different policies and demonstrate what
information the agent needs to extract from its environment to make these
novelty judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Dense Video Captioning as Sequence Generation. (arXiv:2204.08121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08121">
<div class="article-summary-box-inner">
<span><p>Dense video captioning aims to identify the events of interest in an input
video, and generate descriptive captions for each event. Previous approaches
usually follow a two-stage generative process, which first proposes a segment
for each event, then renders a caption for each identified segment. Recent
advances in large-scale sequence generation pretraining have seen great success
in unifying task formulation for a great variety of tasks, but so far, more
complex tasks such as dense video captioning are not able to fully utilize this
powerful paradigm. In this work, we show how to model the two subtasks of dense
video captioning jointly as one sequence generation task, and simultaneously
predict the events and the corresponding descriptions. Experiments on YouCook2
and ViTT show encouraging results and indicate the feasibility of training
complex tasks such as end-to-end dense video captioning integrated into
large-scale pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Network with Channel Attention and Post-Processing for Carotid Arteries Vulnerable Plaque Segmentation in Ultrasound Images. (arXiv:2204.08127v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08127">
<div class="article-summary-box-inner">
<span><p>Carotid arteries vulnerable plaques are a crucial factor in the screening of
atherosclerosis by ultrasound technique. However, the plaques are contaminated
by various noises such as artifact, speckle noise, and manual segmentation may
be time-consuming. This paper proposes an automatic convolutional neural
network (CNN) method for plaque segmentation in carotid ultrasound images using
a small dataset. First, a parallel network with three independent scale
decoders is utilized as our base segmentation network, pyramid dilation
convolutions are used to enlarge receptive fields in the three segmentation
sub-networks. Subsequently, the three decoders are merged to be rectified in
channels by SENet. Thirdly, in test stage, the initially segmented plaque is
refined by the max contour morphology post-processing to obtain the final
plaque. Moreover, three loss function Dice loss, SSIM loss and cross-entropy
loss are compared to segment plaques. Test results show that the proposed
method with dice loss function yields a Dice value of 0.820, an IoU of 0.701,
Acc of 0.969, and modified Hausdorff distance (MHD) of 1.43 for 30 vulnerable
cases of plaques, it outperforms some of the conventional CNN-based methods on
these metrics. Additionally, we apply an ablation experiment to show the
validity of each proposed module. Our study provides some reference for similar
researches and may be useful in actual applications for plaque segmentation of
ultrasound carotid arteries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understanding. (arXiv:2204.08129v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08129">
<div class="article-summary-box-inner">
<span><p>Understanding animals' behaviors is significant for a wide range of
applications. However, existing animal behavior datasets have limitations in
multiple aspects, including limited numbers of animal classes, data samples and
provided tasks, and also limited variations in environmental conditions and
viewpoints. To address these limitations, we create a large and diverse
dataset, Animal Kingdom, that provides multiple annotated tasks to enable a
more thorough understanding of natural animal behaviors. The wild animal
footages used in our dataset record different times of the day in extensive
range of environments containing variations in backgrounds, viewpoints,
illumination and weather conditions. More specifically, our dataset contains 50
hours of annotated videos to localize relevant animal behavior segments in long
videos for the video grounding task, 30K video sequences for the fine-grained
multi-label action recognition task, and 33K frames for the pose estimation
task, which correspond to a diverse range of animals with 850 species across 6
major animal classes. Such a challenging and comprehensive dataset shall be
able to facilitate the community to develop, adapt, and evaluate various types
of advanced methods for animal behavior analysis. Moreover, we propose a
Collaborative Action Recognition (CARe) model that learns general and specific
features for action recognition with unseen new animals. This method achieves
promising performance in our experiments. Our dataset can be found at
https://sutdcv.github.io/Animal-Kingdom.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Weakly-supervised Multiple 3D Hand Mesh Reconstruction from Single Image. (arXiv:2204.08154v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08154">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the challenging task of simultaneously locating
and recovering multiple hands from single 2D image. Previous studies either
focus on single hand reconstruction or solve this problem in a multi-stage way.
Moreover, the conventional two-stage pipeline firstly detects hand areas, and
then estimates 3D hand pose from each cropped patch. To reduce the
computational redundancy in preprocessing and feature extraction, we propose a
concise but efficient single-stage pipeline. Specifically, we design a
multi-head auto-encoder structure for multi-hand reconstruction, where each
head network shares the same feature map and outputs the hand center, pose and
texture, respectively. Besides, we adopt a weakly-supervised scheme to
alleviate the burden of expensive 3D real-world data annotations. To this end,
we propose a series of losses optimized by a stage-wise training scheme, where
a multi-hand dataset with 2D annotations is generated based on the publicly
available single hand datasets. In order to further improve the accuracy of the
weakly supervised model, we adopt several feature consistency constraints in
both single and multiple hand settings. Specifically, the keypoints of each
hand estimated from local features should be consistent with the re-projected
points predicted from global features. Extensive experiments on public
benchmarks including FreiHAND, HO3D, InterHand2.6M and RHD demonstrate that our
method outperforms the state-of-the-art model-based methods in both
weakly-supervised and fully-supervised manners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TOD-CNN: An Effective Convolutional Neural Network for Tiny Object Detection in Sperm Videos. (arXiv:2204.08166v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08166">
<div class="article-summary-box-inner">
<span><p>The detection of tiny objects in microscopic videos is a problematic point,
especially in large-scale experiments. For tiny objects (such as sperms) in
microscopic videos, current detection methods face challenges in fuzzy,
irregular, and precise positioning of objects. In contrast, we present a
convolutional neural network for tiny object detection (TOD-CNN) with an
underlying data set of high-quality sperm microscopic videos (111 videos, $&gt;$
278,000 annotated objects), and a graphical user interface (GUI) is designed to
employ and test the proposed model effectively. TOD-CNN is highly accurate,
achieving $85.60\%$ AP$_{50}$ in the task of real-time sperm detection in
microscopic videos. To demonstrate the importance of sperm detection technology
in sperm quality analysis, we carry out relevant sperm quality evaluation
metrics and compare them with the diagnosis results from medical doctors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-world Deep Local Motion Deblurring. (arXiv:2204.08179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08179">
<div class="article-summary-box-inner">
<span><p>Most existing deblurring methods focus on removing global blur caused by
camera shake, while they cannot well handle local blur caused by object
movements. To fill the vacancy of local deblurring in real scenes, we establish
the first real local motion blur dataset (ReLoBlur), which is captured by a
synchronized beam-splitting photographing system and corrected by a
post-progressing pipeline. Based on ReLoBlur, we propose a Local Blur-Aware
Gated network (LBAG) and several local blur-aware techniques to bridge the gap
between global and local deblurring: 1) a blur detection approach based on
background subtraction to localize blurred regions; 2) a gate mechanism to
guide our network to focus on blurred regions; and 3) a blur-aware patch
cropping strategy to address data imbalance problem. Extensive experiments
prove the reliability of ReLoBlur dataset, and demonstrate that LBAG achieves
better performance than state-of-the-art global deblurring methods without our
proposed local blur-aware techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality-Balanced Embedding for Video Retrieval. (arXiv:2204.08182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08182">
<div class="article-summary-box-inner">
<span><p>Video search has become the main routine for users to discover videos
relevant to a text query on large short-video sharing platforms. During
training a query-video bi-encoder model using online search logs, we identify a
modality bias phenomenon that the video encoder almost entirely relies on text
matching, neglecting other modalities of the videos such as vision, audio. This
modality imbalanceresults from a) modality gap: the relevance between a query
and a video text is much easier to learn as the query is also a piece of text,
with the same modality as the video text; b) data bias: most training samples
can be solved solely by text matching. Here we share our practices to improve
the first retrieval stage including our solution for the modality imbalance
issue. We propose MBVR (short for Modality Balanced Video Retrieval) with two
key components: manually generated modality-shuffled (MS) samples and a dynamic
margin (DM) based on visual relevance. They can encourage the video encoder to
pay balanced attentions to each modality. Through extensive experiments on a
real world dataset, we show empirically that our method is both effective and
efficient in solving modality bias problem. We have also deployed our MBVR in a
large video platform and observed statistically significant boost over a highly
optimized baseline in an A/B test and manual GSB evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge. (arXiv:2204.08189v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08189">
<div class="article-summary-box-inner">
<span><p>Adversarial example attack endangers the mobile edge systems such as vehicles
and drones that adopt deep neural networks for visual sensing. This paper
presents {\em Sardino}, an active and dynamic defense approach that renews the
inference ensemble at run time to develop security against the adaptive
adversary who tries to exfiltrate the ensemble and construct the corresponding
effective adversarial examples. By applying consistency check and data fusion
on the ensemble's predictions, Sardino can detect and thwart adversarial
inputs. Compared with the training-based ensemble renewal, we use HyperNet to
achieve {\em one million times} acceleration and per-frame ensemble renewal
that presents the highest level of difficulty to the prerequisite exfiltration
attacks. Moreover, the robustness of the renewed ensembles against adversarial
examples is enhanced with adversarial learning for the HyperNet. We design a
run-time planner that maximizes the ensemble size in favor of security while
maintaining the processing frame rate. Beyond adversarial examples, Sardino can
also address the issue of out-of-distribution inputs effectively. This paper
presents extensive evaluation of Sardino's performance in counteracting
adversarial examples and applies it to build a real-time car-borne traffic sign
recognition system. Live on-road tests show the built system's effectiveness in
maintaining frame rate and detecting out-of-distribution inputs due to the
false positives of a preceding YOLO-based traffic sign detector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Super-Resolution. (arXiv:2204.08192v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08192">
<div class="article-summary-box-inner">
<span><p>Super-Resolution is the process of generating a high-resolution image from a
low-resolution image. A picture may be of lower resolution due to smaller
spatial resolution, poor camera quality, as a result of blurring, or due to
other possible degradations. Super-Resolution is the technique to improve the
quality of a low-resolution photo by boosting its plausible resolution. The
computer vision community has extensively explored the area of
Super-Resolution. However, the previous Super-Resolution methods require vast
amounts of data for training. This becomes problematic in domains where very
few low-resolution, high-resolution pairs might be available. One of such areas
is statistical downscaling, where super-resolution is increasingly being used
to obtain high-resolution climate information from low-resolution data.
Acquiring high-resolution climate data is extremely expensive and challenging.
To reduce the cost of generating high-resolution climate information,
Super-Resolution algorithms should be able to train with a limited number of
low-resolution, high-resolution pairs. This paper tries to solve the
aforementioned problem by introducing a semi-supervised way to perform
super-resolution that can generate sharp, high-resolution images with as few as
500 paired examples. The proposed semi-supervised technique can be used as a
plug-and-play module with any supervised GAN-based Super-Resolution method to
enhance its performance. We quantitatively and qualitatively analyze the
performance of the proposed model and compare it with completely supervised
methods as well as other unsupervised techniques. Comprehensive evaluations
show the superiority of our method over other methods on different metrics. We
also offer the applicability of our approach in statistical downscaling to
obtain high-resolution climate images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Arbitrary-Scale Point Clouds Upsampling via Implicit Neural Representation. (arXiv:2204.08196v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08196">
<div class="article-summary-box-inner">
<span><p>Point clouds upsampling is a challenging issue to generate dense and uniform
point clouds from the given sparse input. Most existing methods either take the
end-to-end supervised learning based manner, where large amounts of pairs of
sparse input and dense ground-truth are exploited as supervision information;
or treat up-scaling of different scale factors as independent tasks, and have
to build multiple networks to handle upsampling with varying factors. In this
paper, we propose a novel approach that achieves self-supervised and
magnification-flexible point clouds upsampling simultaneously. We formulate
point clouds upsampling as the task of seeking nearest projection points on the
implicit surface for seed points. To this end, we define two implicit neural
functions to estimate projection direction and distance respectively, which can
be trained by two pretext learning tasks. Experimental results demonstrate that
our self-supervised learning based scheme achieves competitive or even better
performance than supervised learning based state-of-the-art methods. The source
code is publicly available at https://github.com/xnowbzhao/sapcu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OMG: Observe Multiple Granularities for Natural Language-Based Vehicle Retrieval. (arXiv:2204.08209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08209">
<div class="article-summary-box-inner">
<span><p>Retrieving tracked-vehicles by natural language descriptions plays a critical
role in smart city construction. It aims to find the best match for the given
texts from a set of tracked vehicles in surveillance videos. Existing works
generally solve it by a dual-stream framework, which consists of a text
encoder, a visual encoder and a cross-modal loss function. Although some
progress has been made, they failed to fully exploit the information at various
levels of granularity. To tackle this issue, we propose a novel framework for
the natural language-based vehicle retrieval task, OMG, which Observes Multiple
Granularities with respect to visual representation, textual representation and
objective functions. For the visual representation, target features, context
features and motion features are encoded separately. For the textual
representation, one global embedding, three local embeddings and a color-type
prompt embedding are extracted to represent various granularities of semantic
features. Finally, the overall framework is optimized by a cross-modal
multi-granularity contrastive loss function. Experiments demonstrate the
effectiveness of our method. Our OMG significantly outperforms all previous
methods and ranks the 9th on the 6th AI City Challenge Track2. The codes are
available at https://github.com/dyhBUPT/OMG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Evaluation and Theoretical Analysis for Representation Learning: A Survey. (arXiv:2204.08226v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08226">
<div class="article-summary-box-inner">
<span><p>Representation learning enables us to automatically extract generic feature
representations from a dataset to solve another machine learning task.
Recently, extracted feature representations by a representation learning
algorithm and a simple predictor have exhibited state-of-the-art performance on
several machine learning tasks. Despite its remarkable progress, there exist
various ways to evaluate representation learning algorithms depending on the
application because of the flexibility of representation learning. To
understand the current representation learning, we review evaluation methods of
representation learning algorithms and theoretical analyses. On the basis of
our evaluation survey, we also discuss the future direction of representation
learning. Note that this survey is the extended version of Nozawa and Sato
(2022).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training. (arXiv:2204.08227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08227">
<div class="article-summary-box-inner">
<span><p>The self-supervised Masked Image Modeling (MIM) schema, following
"mask-and-reconstruct" pipeline of recovering contents from masked image, has
recently captured the increasing interest in the multimedia community, owing to
the excellent ability of learning visual representation from unlabeled data.
Aiming at learning representations with high semantics abstracted, a group of
works attempts to reconstruct non-semantic pixels with large-ratio masking
strategy, which may suffer from "over-smoothing" problem, while others directly
infuse semantics into targets in off-line way requiring extra data. Different
from them, we shift the perspective to the Fourier domain which naturally has
global perspective and present a new Masked Image Modeling (MIM), termed
Geminated Gestalt Autoencoder (Ge$^2$-AE) for visual pre-training.
Specifically, we equip our model with geminated decoders in charge of
reconstructing image contents from both pixel and frequency space, where each
other serves as not only the complementation but also the reciprocal
constraints. Through this way, more robust representations can be learned in
the pre-trained encoders, of which the effectiveness is confirmed by the
juxtaposing experimental results on downstream recognition tasks. We also
conduct several quantitative and qualitative experiments to investigate the
learning behavior of our method. To our best knowledge, this is the first MIM
work to solve the visual pre-training through the lens of frequency domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Multi-view Unsupervised Feature Selection and Graph Learning. (arXiv:2204.08247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08247">
<div class="article-summary-box-inner">
<span><p>Despite the recent progress, the existing multi-view unsupervised feature
selection methods mostly suffer from two limitations. First, they generally
utilize either cluster structure or similarity structure to guide the feature
selection, neglecting the possibility of a joint formulation with mutual
benefits. Second, they often learn the similarity structure by either global
structure learning or local structure learning, lacking the capability of graph
learning with both global and local structural awareness. In light of this,
this paper presents a joint multi-view unsupervised feature selection and graph
learning (JMVFG) approach. Particularly, we formulate the multi-view feature
selection with orthogonal decomposition, where each target matrix is decomposed
into a view-specific basis matrix and a view-consistent cluster indicator.
Cross-space locality preservation is incorporated to bridge the cluster
structure learning in the projected space and the similarity learning (i.e.,
graph learning) in the original space. Further, a unified objective function is
presented to enable the simultaneous learning of the cluster structure, the
global and local similarity structures, and the multi-view consistency and
inconsistency, upon which an alternating optimization algorithm is developed
with theoretically proved convergence. Extensive experiments demonstrate the
superiority of our approach for both multi-view feature selection and graph
learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visio-Linguistic Brain Encoding. (arXiv:2204.08261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08261">
<div class="article-summary-box-inner">
<span><p>Enabling effective brain-computer interfaces requires understanding how the
human brain encodes stimuli across modalities such as visual, language (or
text), etc. Brain encoding aims at constructing fMRI brain activity given a
stimulus. There exists a plethora of neural encoding models which study brain
encoding for single mode stimuli: visual (pretrained CNNs) or text (pretrained
language models). Few recent papers have also obtained separate visual and text
representation models and performed late-fusion using simple heuristics.
However, previous work has failed to explore: (a) the effectiveness of image
Transformer models for encoding visual stimuli, and (b) co-attentive
multi-modal modeling for visual and text reasoning. In this paper, we
systematically explore the efficacy of image Transformers (ViT, DEiT, and BEiT)
and multi-modal Transformers (VisualBERT, LXMERT, and CLIP) for brain encoding.
Extensive experiments on two popular datasets, BOLD5000 and Pereira, provide
the following insights. (1) To the best of our knowledge, we are the first to
investigate the effectiveness of image and multi-modal Transformers for brain
encoding. (2) We find that VisualBERT, a multi-modal Transformer, significantly
outperforms previously proposed single-mode CNNs, image Transformers as well as
other previously proposed multi-modal models, thereby establishing new
state-of-the-art. The supremacy of visio-linguistic models raises the question
of whether the responses elicited in the visual regions are affected implicitly
by linguistic processing even when passively viewing images. Future fMRI tasks
can verify this computational insight in an appropriate experimental setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised domain adaptation and super resolution on drone images for autonomous dry herbage biomass estimation. (arXiv:2204.08271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08271">
<div class="article-summary-box-inner">
<span><p>Herbage mass yield and composition estimation is an important tool for dairy
farmers to ensure an adequate supply of high quality herbage for grazing and
subsequently milk production. By accurately estimating herbage mass and
composition, targeted nitrogen fertiliser application strategies can be
deployed to improve localised regions in a herbage field, effectively reducing
the negative impacts of over-fertilization on biodiversity and the environment.
In this context, deep learning algorithms offer a tempting alternative to the
usual means of sward composition estimation, which involves the destructive
process of cutting a sample from the herbage field and sorting by hand all
plant species in the herbage. The process is labour intensive and time
consuming and so not utilised by farmers. Deep learning has been successfully
applied in this context on images collected by high-resolution cameras on the
ground. Moving the deep learning solution to drone imaging, however, has the
potential to further improve the herbage mass yield and composition estimation
task by extending the ground-level estimation to the large surfaces occupied by
fields/paddocks. Drone images come at the cost of lower resolution views of the
fields taken from a high altitude and requires further herbage ground-truth
collection from the large surfaces covered by drone images. This paper proposes
to transfer knowledge learned on ground-level images to raw drone images in an
unsupervised manner. To do so, we use unpaired image style translation to
enhance the resolution of drone images by a factor of eight and modify them to
appear closer to their ground-level counterparts. We then ...
~\url{www.github.com/PaulAlbert31/Clover_SSL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heavy Rain Face Image Restoration: Integrating Physical Degradation Model and Facial Component Guided Adversarial Learning. (arXiv:2204.08307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08307">
<div class="article-summary-box-inner">
<span><p>With the recent increase in intelligent CCTVs for visual surveillance, a new
image degradation that integrates resolution conversion and synthetic rain
models is required. For example, in heavy rain, face images captured by CCTV
from a distance have significant deterioration in both visibility and
resolution. Unlike traditional image degradation models (IDM), such as rain
removal and superresolution, this study addresses a new IDM referred to as a
scale-aware heavy rain model and proposes a method for restoring
high-resolution face images (HR-FIs) from low-resolution heavy rain face images
(LRHR-FI). To this end, a 2-stage network is presented. The first stage
generates low-resolution face images (LR-FIs), from which heavy rain has been
removed from the LRHR-FIs to improve visibility. To realize this, an
interpretable IDM-based network is constructed to predict physical parameters,
such as rain streaks, transmission maps, and atmospheric light. In addition,
the image reconstruction loss is evaluated to enhance the estimates of the
physical parameters. For the second stage, which aims to reconstruct the HR-FIs
from the LR-FIs outputted in the first stage, facial component guided
adversarial learning (FCGAL) is applied to boost facial structure expressions.
To focus on informative facial features and reinforce the authenticity of
facial components, such as the eyes and nose, a face-parsing-guided generator
and facial local discriminators are designed for FCGAL. The experimental
results verify that the proposed approach based on physical-based network
design and FCGAL can remove heavy rain and increase the resolution and
visibility simultaneously. Moreover, the proposed heavy-rain face image
restoration outperforms state-of-the-art models of heavy rain removal,
image-to-image translation, and superresolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency in Augmented Reality. (arXiv:2204.08308v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08308">
<div class="article-summary-box-inner">
<span><p>With the rapid development of multimedia technology, Augmented Reality (AR)
has become a promising next-generation mobile platform. The primary theory
underlying AR is human visual confusion, which allows users to perceive the
real-world scenes and augmented contents (virtual-world scenes) simultaneously
by superimposing them together. To achieve good Quality of Experience (QoE), it
is important to understand the interaction between two scenarios, and
harmoniously display AR contents. However, studies on how this superimposition
will influence the human visual attention are lacking. Therefore, in this
paper, we mainly analyze the interaction effect between background (BG) scenes
and AR contents, and study the saliency prediction problem in AR. Specifically,
we first construct a Saliency in AR Dataset (SARD), which contains 450 BG
images, 450 AR images, as well as 1350 superimposed images generated by
superimposing BG and AR images in pair with three mixing levels. A large-scale
eye-tracking experiment among 60 subjects is conducted to collect eye movement
data. To better predict the saliency in AR, we propose a vector quantized
saliency prediction method and generalize it for AR saliency prediction. For
comparison, three benchmark methods are proposed and evaluated together with
our proposed method on our SARD. Experimental results demonstrate the
superiority of our proposed method on both of the common saliency prediction
problem and the AR saliency prediction problem over benchmark methods. Our data
collection methodology, dataset, benchmark methods, and proposed saliency
models will be publicly available to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking monocular camera pose and deformation for SLAM inside the human body. (arXiv:2204.08309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08309">
<div class="article-summary-box-inner">
<span><p>Monocular SLAM in deformable scenes will open the way to multiple medical
applications like computer-assisted navigation in endoscopy, automatic drug
delivery or autonomous robotic surgery. In this paper we propose a novel method
to simultaneously track the camera pose and the 3D scene deformation, without
any assumption about environment topology or shape. The method uses an
illumination-invariant photometric method to track image features and estimates
camera motion and deformation combining reprojection error with spatial and
temporal regularization of deformations. Our results in simulated colonoscopies
show the method's accuracy and robustness in complex scenes under increasing
levels of deformation. Our qualitative results in human colonoscopies from
Endomapper dataset show that the method is able to successfully cope with the
challenges of real endoscopies: deformations, low texture and strong
illumination changes. We also compare with previous tracking methods in simpler
scenarios from Hamlyn dataset where we obtain competitive performance, without
needing any topological assumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Transfer Learning and Ensemble Learning in Image-level Classification for Breast Histopathology. (arXiv:2204.08311v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08311">
<div class="article-summary-box-inner">
<span><p>Background: Breast cancer has the highest prevalence in women globally. The
classification and diagnosis of breast cancer and its histopathological images
have always been a hot spot of clinical concern. In Computer-Aided Diagnosis
(CAD), traditional classification models mostly use a single network to extract
features, which has significant limitations. On the other hand, many networks
are trained and optimized on patient-level datasets, ignoring the application
of lower-level data labels.
</p>
<p>Method: This paper proposes a deep ensemble model based on image-level labels
for the binary classification of benign and malignant lesions of breast
histopathological images. First, the BreakHis dataset is randomly divided into
a training, validation and test set. Then, data augmentation techniques are
used to balance the number of benign and malignant samples. Thirdly,
considering the performance of transfer learning and the complementarity
between each network, VGG-16, Xception, Resnet-50, DenseNet-201 are selected as
the base classifiers.
</p>
<p>Result: In the ensemble network model with accuracy as the weight, the
image-level binary classification achieves an accuracy of $98.90\%$. In order
to verify the capabilities of our method, the latest Transformer and Multilayer
Perception (MLP) models have been experimentally compared on the same dataset.
Our model wins with a $5\%-20\%$ advantage, emphasizing the ensemble model's
far-reaching significance in classification tasks.
</p>
<p>Conclusion: This research focuses on improving the model's classification
performance with an ensemble algorithm. Transfer learning plays an essential
role in small datasets, improving training speed and accuracy. Our model has
outperformed many existing approaches in accuracy, providing a method for the
field of auxiliary medical diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A high-resolution canopy height model of the Earth. (arXiv:2204.08322v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08322">
<div class="article-summary-box-inner">
<span><p>The worldwide variation in vegetation height is fundamental to the global
carbon cycle and central to the functioning of ecosystems and their
biodiversity. Geospatially explicit and, ideally, highly resolved information
is required to manage terrestrial ecosystems, mitigate climate change, and
prevent biodiversity loss. Here, we present the first global, wall-to-wall
canopy height map at 10 m ground sampling distance for the year 2020. No single
data source meets these requirements: dedicated space missions like GEDI
deliver sparse height data, with unprecedented coverage, whereas optical
satellite images like Sentinel-2 offer dense observations globally, but cannot
directly measure vertical structures. By fusing GEDI with Sentinel-2, we have
developed a probabilistic deep learning model to retrieve canopy height from
Sentinel-2 images anywhere on Earth, and to quantify the uncertainty in these
estimates. The presented approach reduces the saturation effect commonly
encountered when estimating canopy height from satellite images, allowing to
resolve tall canopies with likely high carbon stocks. According to our map,
only 5% of the global landmass is covered by trees taller than 30 m. Such data
play an important role for conservation, e.g., we find that only 34% of these
tall canopies are located within protected areas. Our model enables consistent,
uncertainty-informed worldwide mapping and supports an ongoing monitoring to
detect change and inform decision making. The approach can serve ongoing
efforts in forest conservation, and has the potential to foster advances in
climate, carbon, and biodiversity modelling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Optimal Transport for Comparing Histopathology Datasets. (arXiv:2204.08324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08324">
<div class="article-summary-box-inner">
<span><p>Scarcity of labeled histopathology data limits the applicability of deep
learning methods to under-profiled cancer types and labels. Transfer learning
allows researchers to overcome the limitations of small datasets by
pre-training machine learning models on larger datasets \emph{similar} to the
small target dataset. However, similarity between datasets is often determined
heuristically. In this paper, we propose a principled notion of distance
between histopathology datasets based on a hierarchical generalization of
optimal transport distances. Our method does not require any training, is
agnostic to model type, and preserves much of the hierarchical structure in
histopathology datasets imposed by tiling. We apply our method to H\&amp;E stained
slides from The Cancer Genome Atlas from six different cancer types. We show
that our method outperforms a baseline distance in a cancer-type prediction
task. Our results also show that our optimal transport distance predicts
difficulty of transferability in a tumor vs.~normal prediction setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey on Data-Efficient GANs in Image Generation. (arXiv:2204.08329v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08329">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have achieved remarkable achievements
in image synthesis. These successes of GANs rely on large scale datasets,
requiring too much cost. With limited training data, how to stable the training
process of GANs and generate realistic images have attracted more attention.
The challenges of Data-Efficient GANs (DE-GANs) mainly arise from three
aspects: (i) Mismatch Between Training and Target Distributions, (ii)
Overfitting of the Discriminator, and (iii) Imbalance Between Latent and Data
Spaces. Although many augmentation and pre-training strategies have been
proposed to alleviate these issues, there lacks a systematic survey to
summarize the properties, challenges, and solutions of DE-GANs. In this paper,
we revisit and define DE-GANs from the perspective of distribution
optimization. We conclude and analyze the challenges of DE-GANs. Meanwhile, we
propose a taxonomy, which classifies the existing methods into three
categories: Data Selection, GANs Optimization, and Knowledge Sharing. Last but
not the least, we attempt to highlight the current problems and the future
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment. (arXiv:2204.08332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08332">
<div class="article-summary-box-inner">
<span><p>This work addresses the Burst Super-Resolution (BurstSR) task using a new
architecture, which requires restoring a high-quality image from a sequence of
noisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in
BurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can
significantly improve the capability of extracting inter-frame information and
reconstruction. To achieve this goal, we propose a Pyramid Flow-Guided
Deformable Convolution Network (Pyramid FG-DCN) and incorporate Swin
Transformer Blocks and Groups as our main backbone. More specifically, we
combine optical flows and deformable convolutions, hence our BSRT can handle
misalignment and aggregate the potential texture information in multi-frames
more efficiently. In addition, our Transformer-based structure can capture
long-range dependency to further improve the performance. The evaluation on
both synthetic and real-world tracks demonstrates that our approach achieves a
new state-of-the-art in BurstSR task. Further, our BSRT wins the championship
in the NTIRE2022 Burst Super-Resolution Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Migrating Face Swap to Mobile Devices: A lightweight Framework and A Supervised Training Solution. (arXiv:2204.08339v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08339">
<div class="article-summary-box-inner">
<span><p>Existing face swap methods rely heavily on large-scale networks for adequate
capacity to generate visually plausible results, which inhibits its
applications on resource-constraint platforms. In this work, we propose
MobileFSGAN, a novel lightweight GAN for face swap that can run on mobile
devices with much fewer parameters while achieving competitive performance. A
lightweight encoder-decoder structure is designed especially for image
synthesis tasks, which is only 10.2MB and can run on mobile devices at a
real-time speed. To tackle the unstability of training such a small network, we
construct the FSTriplets dataset utilizing facial attribute editing techniques.
FSTriplets provides source-target-result training triplets, yielding
pixel-level labels thus for the first time making the training process
supervised. We also designed multi-scale gradient losses for efficient
back-propagation, resulting in faster and better convergence. Experimental
results show that our model reaches comparable performance towards
state-of-the-art methods, while significantly reducing the number of network
parameters. Codes and the dataset have been released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHSCNet: A Multimodal Hierarchical Shot-aware Convolutional Network for Video Summarization. (arXiv:2204.08352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08352">
<div class="article-summary-box-inner">
<span><p>Video summarization intends to produce a concise video summary by effectively
capturing and combining the most informative parts of the whole content.
Existing approaches for video summarization regard the task as a frame-wise
keyframe selection problem and generally construct the frame-wise
representation by combining the long-range temporal dependency with the
unimodal or bimodal information. However, the optimal video summaries need to
reflect the most valuable keyframe with its own information, and one with
semantic power of the whole content. Thus, it is critical to construct a more
powerful and robust frame-wise representation and predict the frame-level
importance score in a fair and comprehensive manner. To tackle the above
issues, we propose a multimodal hierarchical shot-aware convolutional network,
denoted as MHSCNet, to enhance the frame-wise representation via combining the
comprehensive available multimodal information. Specifically, we design a
hierarchical ShotConv network to incorporate the adaptive shot-aware
frame-level representation by considering the short-range and long-range
temporal dependency. Based on the learned shot-aware representations, MHSCNet
can predict the frame-level importance score in the local and global view of
the video. Extensive experiments on two standard video summarization datasets
demonstrate that our proposed method consistently outperforms state-of-the-art
baselines. Source code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting, Tracking and Counting Motorcycle Rider Traffic Violations on Unconstrained Roads. (arXiv:2204.08364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08364">
<div class="article-summary-box-inner">
<span><p>In many Asian countries with unconstrained road traffic conditions, driving
violations such as not wearing helmets and triple-riding are a significant
source of fatalities involving motorcycles. Identifying and penalizing such
riders is vital in curbing road accidents and improving citizens' safety. With
this motivation, we propose an approach for detecting, tracking, and counting
motorcycle riding violations in videos taken from a vehicle-mounted dashboard
camera. We employ a curriculum learning-based object detector to better tackle
challenging scenarios such as occlusions. We introduce a novel trapezium-shaped
object boundary representation to increase robustness and tackle the
rider-motorcycle association. We also introduce an amodal regressor that
generates bounding boxes for the occluded riders. Experimental results on a
large-scale unconstrained driving dataset demonstrate the superiority of our
approach compared to existing approaches and other ablative variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Deepfakes with Self-Blended Images. (arXiv:2204.08376v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08376">
<div class="article-summary-box-inner">
<span><p>In this paper, we present novel synthetic training data called self-blended
images (SBIs) to detect deepfakes. SBIs are generated by blending pseudo source
and target images from single pristine images, reproducing common forgery
artifacts (e.g., blending boundaries and statistical inconsistencies between
source and target images). The key idea behind SBIs is that more general and
hardly recognizable fake samples encourage classifiers to learn generic and
robust representations without overfitting to manipulation-specific artifacts.
We compare our approach with state-of-the-art methods on FF++, CDF, DFD, DFDC,
DFDCP, and FFIW datasets by following the standard cross-dataset and
cross-manipulation protocols. Extensive experiments show that our method
improves the model generalization to unknown manipulations and scenes. In
particular, on DFDC and DFDCP where existing methods suffer from the domain gap
between the training and test sets, our approach outperforms the baseline by
4.90% and 11.78% points in the cross-dataset evaluation, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple-environment Self-adaptive Network for Aerial-view Geo-localization. (arXiv:2204.08381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08381">
<div class="article-summary-box-inner">
<span><p>Aerial-view geo-localization tends to determine an unknown position through
matching the drone-view image with the geo-tagged satellite-view image. This
task is mostly regarded as an image retrieval problem. The key underpinning
this task is to design a series of deep neural networks to learn discriminative
image descriptors. However, existing methods meet large performance drops under
realistic weather, such as rain and fog, since they do not take the domain
shift between the training data and multiple test environments into
consideration. To minor this domain gap, we propose a Multiple-environment
Self-adaptive Network (MuSe-Net) to dynamically adjust the domain shift caused
by environmental changing. In particular, MuSe-Net employs a two-branch neural
network containing one multiple-environment style extraction network and one
self-adaptive feature extraction network. As the name implies, the
multiple-environment style extraction network is to extract the
environment-related style information, while the self-adaptive feature
extraction network utilizes an adaptive modulation module to dynamically
minimize the environment-related style gap. Extensive experiments on two
widely-used benchmarks, i.e., University-1652 and CVUSA, demonstrate that the
proposed MuSe-Net achieves a competitive result for geo-localization in
multiple environments. Furthermore, we observe that the proposed method also
shows great potential to the unseen extreme weather, such as mixing the fog,
rain and snow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subspace Nonnegative Matrix Factorization for Feature Representation. (arXiv:2204.08382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08382">
<div class="article-summary-box-inner">
<span><p>Traditional nonnegative matrix factorization (NMF) learns a new feature
representation on the whole data space, which means treating all features
equally. However, a subspace is often sufficient for accurate representation in
practical applications, and redundant features can be invalid or even harmful.
For example, if a camera has some sensors destroyed, then the corresponding
pixels in the photos from this camera are not helpful to identify the content,
which means only the subspace consisting of remaining pixels is worthy of
attention. This paper proposes a new NMF method by introducing adaptive weights
to identify key features in the original space so that only a subspace involves
generating the new representation. Two strategies are proposed to achieve this:
the fuzzier weighted technique and entropy regularized weighted technique, both
of which result in an iterative solution with a simple form. Experimental
results on several real-world datasets demonstrated that the proposed methods
can generate a more accurate feature representation than existing methods. The
code developed in this study is available at
https://github.com/WNMF1/FWNMF-ERWNMF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHSA-Net: Multi-Head Self-Attention Network for Occluded Person Re-Identification. (arXiv:2008.04015v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.04015">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel person re-identification model, named Multi-Head
Self-Attention Network (MHSA-Net), to prune unimportant information and capture
key local information from person images. MHSA-Net contains two main novel
components: Multi-Head Self-Attention Branch (MHSAB) and Attention Competition
Mechanism (ACM). The MHSAB adaptively captures key local person information,
and then produces effective diversity embeddings of an image for the person
matching. The ACM further helps filter out attention noise and non-key
information. Through extensive ablation studies, we verified that the
Multi-Head Self-Attention Branch (MHSAB) and Attention Competition Mechanism
(ACM) both contribute to the performance improvement of the MHSA-Net. Our
MHSA-Net achieves competitive performance in the standard and occluded person
Re-ID tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can You Spot the Chameleon? Adversarially Camouflaging Images from Co-Salient Object Detection. (arXiv:2009.09258v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09258">
<div class="article-summary-box-inner">
<span><p>Co-salient object detection (CoSOD) has recently achieved significant
progress and played a key role in retrieval-related tasks. However, it
inevitably poses an entirely new safety and security issue, i.e., highly
personal and sensitive content can potentially be extracting by powerful CoSOD
methods. In this paper, we address this problem from the perspective of
adversarial attacks and identify a novel task: adversarial co-saliency attack.
Specially, given an image selected from a group of images containing some
common and salient objects, we aim to generate an adversarial version that can
mislead CoSOD methods to predict incorrect co-salient regions. Note that,
compared with general white-box adversarial attacks for classification, this
new task faces two additional challenges: (1) low success rate due to the
diverse appearance of images in the group; (2) low transferability across CoSOD
methods due to the considerable difference between CoSOD pipelines. To address
these challenges, we propose the very first black-box joint adversarial
exposure and noise attack (Jadena), where we jointly and locally tune the
exposure and additive perturbations of the image according to a newly designed
high-feature-level contrast-sensitive loss function. Our method, without any
information on the state-of-the-art CoSOD methods, leads to significant
performance degradation on various co-saliency detection datasets and makes the
co-salient objects undetectable. This can have strong practical benefits in
properly securing the large number of personal photos currently shared on the
Internet. Moreover, our method is potential to be utilized as a metric for
evaluating the robustness of CoSOD methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Neural Networks via Orthogonal Diversity. (arXiv:2010.12190v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12190">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the
images generated by adversarial attacks, which raises researches on the
adversarial robustness of DNNs. A series of methods represented by the
adversarial training and its variants have proven as one of the most effective
techniques in enhancing the DNN robustness. Generally, adversarial training
focuses on enriching the training data by involving perturbed data. Despite of
the efficiency in defending specific attacks, adversarial training is benefited
from the data augmentation, which does not contribute to the robustness of DNN
itself and usually suffers from accuracy drop on clean data as well as
inefficiency in unknown attacks. Towards the robustness of DNN itself, we
propose a novel defense that aims at augmenting the model in order to learn
features adaptive to diverse inputs, including adversarial examples.
Specifically, we introduce multiple paths to augment the network, and impose
orthogonality constraints on these paths. In addition, a margin-maximization
loss is designed to further boost DIversity via Orthogonality (DIO). Extensive
empirical results on various data sets, architectures, and attacks demonstrate
the adversarial robustness of the proposed DIO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stretchable Cells Help DARTS Search Better. (arXiv:2011.09300v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09300">
<div class="article-summary-box-inner">
<span><p>Differentiable neural architecture search (DARTS) has gained much success in
discovering flexible and diverse cell types. To reduce the evaluation gap, the
supernet is expected to have identical layers with the target network. However,
even for this consistent search, the searched cells often suffer from poor
performance, especially for the supernet with fewer layers, as current DARTS
methods are prone to wide and shallow cells, and this topology collapse induces
sub-optimal searched cells. In this paper, we alleviate this issue by endowing
the cells with explicit stretchability, so the search can be directly
implemented on our stretchable cells for both operation type and topology
simultaneously. Concretely, we introduce a set of topological variables and a
combinatorial probabilistic distribution to explicitly model the target
topology. With more diverse and complex topologies, our method adapts well for
various layer numbers. Extensive experiments on CIFAR-10 and ImageNet show that
our stretchable cells obtain better performance with fewer layers and
parameters. For example, our method can improve DARTS by 0.28\% accuracy on
CIFAR-10 dataset with 45\% parameters reduced or 2.9\% with similar FLOPs on
ImageNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Layout Manipulation with High-Resolution Sparse Attention. (arXiv:2012.07288v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07288">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of semantic image layout manipulation, which aims to
manipulate an input image by editing its semantic label map. A core problem of
this task is how to transfer visual details from the input images to the new
semantic layout while making the resulting image visually realistic. Recent
work on learning cross-domain correspondence has shown promising results for
global layout transfer with dense attention-based warping. However, this method
tends to lose texture details due to the resolution limitation and the lack of
smoothness constraint of correspondence. To adapt this paradigm for the layout
manipulation task, we propose a high-resolution sparse attention module that
effectively transfers visual details to new layouts at a resolution up to
512x512. To further improve visual quality, we introduce a novel generator
architecture consisting of a semantic encoder and a two-stage decoder for
coarse-to-fine synthesis. Experiments on the ADE20k and Places365 datasets
demonstrate that our proposed approach achieves substantial improvements over
the existing inpainting and layout manipulation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss. (arXiv:2101.11952v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11952">
<div class="article-summary-box-inner">
<span><p>Boundary discontinuity and its inconsistency to the final detection metric
have been the bottleneck for rotating detection regression loss design. In this
paper, we propose a novel regression loss based on Gaussian Wasserstein
distance as a fundamental approach to solve the problem. Specifically, the
rotated bounding box is converted to a 2-D Gaussian distribution, which enables
to approximate the indifferentiable rotational IoU induced loss by the Gaussian
Wasserstein distance (GWD) which can be learned efficiently by gradient
back-propagation. GWD can still be informative for learning even there is no
overlapping between two rotating bounding boxes which is often the case for
small object detection. Thanks to its three unique properties, GWD can also
elegantly solve the boundary discontinuity and square-like problem regardless
how the bounding box is defined. Experiments on five datasets using different
detectors show the effectiveness of our approach. Codes are available at
https://github.com/yangxue0827/RotationDetection and
https://github.com/open-mmlab/mmrotate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Deep ML Architecture by Integrating Visual Simultaneous Localization and Mapping (vSLAM) into Mask R-CNN for Real-time Surgical Video Analysis. (arXiv:2103.16847v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16847">
<div class="article-summary-box-inner">
<span><p>Seven million people suffer surgical complications each year, but with
sufficient surgical training and review, 50\% of these complications could be
prevented. To improve surgical performance, existing research uses various deep
learning (DL) technologies including convolutional neural networks (CNN) and
recurrent neural networks (RNN) to automate surgical tool and workflow
detection. However, there is room to improve accuracy; real-time analysis is
also minimal due to the complexity of CNN. In this research, a novel DL
architecture is proposed to integrate visual simultaneous localization and
mapping (vSLAM) into Mask R-CNN. This architecture, vSLAM-CNN (vCNN), for the
first time, integrates the best of both worlds, inclusive of (1) vSLAM for
object detection, by focusing on geometric information for region proposals,
and (2) CNN for object recognition, by focusing on semantic information for
image classification, combining them into one joint end-to-end training
process. This method, using spatio-temporal information in addition to visual
features, is evaluated on M2CAI 2016 challenge datasets, achieving the
state-of-the-art results with 96.8 mAP for tool detection and 97.5 mean Jaccard
score for workflow detection, surpassing all previous works, and reaching a 50
FPS performance, 10x faster than the region-based CNN. A region proposal module
(RPM) replaces the region proposal network (RPN) in Mask R-CNN, accurately
placing bounding boxes and lessening the annotation requirement. Furthermore, a
Microsoft HoloLens 2 application is developed to provide an augmented reality
(AR)-based solution for surgical training and assistance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIH: Towards More Accurate Face Alignment via Heatmap in Heatmap. (arXiv:2104.03100v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03100">
<div class="article-summary-box-inner">
<span><p>Heatmap-based regression overcomes the lack of spatial and contextual
information of direct coordinate regression, and has revolutionized the task of
face alignment. Yet it suffers from quantization errors caused by neglecting
subpixel coordinates in image resizing and network downsampling. In this paper,
we first quantitatively analyze the quantization error on benchmarks, which
accounts for more than 1/3 of the whole prediction errors for state-of-the-art
methods. To tackle this problem, we propose a novel Heatmap In Heatmap(HIH)
representation and a coordinate soft-classification (CSC) method, which are
seamlessly integrated into the classic hourglass network. The HIH
representation utilizes nested heatmaps to jointly represent the coordinate
label: one heatmap called integer heatmap stands for the integer coordinate,
and the other heatmap named decimal heatmap represents the subpixel coordinate.
The range of a decimal heatmap makes up one pixel in the corresponding integer
heatmap. Besides, we transfer the offset regression problem to an interval
classification task, and CSC regards the confidence of the pixel as the
probability of the interval. Meanwhile, CSC applying the distribution loss
leverage the soft labels generated from the Gaussian distribution function to
guide the offset heatmap training, which makes it easier to learn the
distribution of coordinate offsets. Extensive experiments on challenging
benchmark datasets demonstrate that our HIH can achieve state-of-the-art
results. In particular, our HIH reaches 4.08 NME (Normalized Mean Error) on
WFLW, and 3.21 on COFW, which exceeds previous methods by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving. (arXiv:2105.00373v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00373">
<div class="article-summary-box-inner">
<span><p>The past few years have witnessed an increasing interest in improving the
perception performance of LiDARs on autonomous vehicles. While most of the
existing works focus on developing new deep learning algorithms or model
architectures, we study the problem from the physical design perspective, i.e.,
how different placements of multiple LiDARs influence the learning-based
perception. To this end, we introduce an easy-to-compute information-theoretic
surrogate metric to quantitatively and fast evaluate LiDAR placement for 3D
detection of different types of objects. We also present a new data collection,
detection model training and evaluation framework in the realistic CARLA
simulator to evaluate disparate multi-LiDAR configurations. Using several
prevalent placements inspired by the designs of self-driving companies, we show
the correlation between our surrogate metric and object detection performance
of different representative algorithms on KITTI through extensive experiments,
validating the effectiveness of our LiDAR placement evaluation approach. Our
results show that sensor placement is non-negligible in 3D point cloud-based
object detection, which will contribute up to 10% performance discrepancy in
terms of average precision in challenging 3D object detection settings. We
believe that this is one of the first studies to quantitatively investigate the
influence of LiDAR placement on perception performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Objects in Clutter. (arXiv:2105.03053v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03053">
<div class="article-summary-box-inner">
<span><p>This paper identifies and addresses a serious design bias of existing salient
object detection (SOD) datasets, which unrealistically assume that each image
should contain at least one clear and uncluttered salient object. This design
bias has led to a saturation in performance for state-of-the-art SOD models
when evaluated on existing datasets. However, these models are still far from
satisfactory when applied to real-world scenes. Based on our analyses, we
propose a new high-quality dataset and update the previous saliency benchmark.
Specifically, our dataset, called Salient Objects in Clutter~\textbf{(SOC)},
includes images with both salient and non-salient objects from several common
object categories. In addition to object category annotations, each salient
image is accompanied by attributes that reflect common challenges in common
scenes, which can help provide deeper insight into the SOD problem. Further,
with a given saliency encoder, e.g., the backbone network, existing saliency
models are designed to achieve mapping from the training image set to the
training ground-truth set. We, therefore, argue that improving the dataset can
yield higher performance gains than focusing only on the decoder design. With
this in mind, we investigate several dataset-enhancement strategies, including
label smoothing to implicitly emphasize salient boundaries, random image
augmentation to adapt saliency models to various scenarios, and self-supervised
learning as a regularization strategy to learn from small datasets. Our
extensive results demonstrate the effectiveness of these tricks. We also
provide a comprehensive benchmark for SOD, which can be found in our
repository: https://github.com/DengPingFan/SODBenchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCN-MIF: Graph Convolutional Network with Multi-Information Fusion for Low-dose CT Denoising. (arXiv:2105.07146v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07146">
<div class="article-summary-box-inner">
<span><p>Being low-level radiation exposure and less harmful to health, low-dose
computed tomography (LDCT) has been widely adopted in the early screening of
lung cancer and COVID-19. LDCT images inevitably suffer from the degradation
problem caused by complex noises. It was reported that deep learning (DL)-based
LDCT denoising methods using convolutional neural network (CNN) achieved
impressive denoising performance. Although most existing DL-based methods
(e.g., encoder-decoder framework) can implicitly utilize non-local and
contextual information via downsampling operator and 3D CNN, the explicit
multi-information (i.e., local, non-local, and contextual) integration may not
be explored enough. To address this issue, we propose a novel graph
convolutional network-based LDCT denoising model, namely GCN-MIF, to explicitly
perform multi-information fusion for denoising purpose. Concretely, by
constructing intra- and inter-slice graph, the graph convolutional network is
introduced to leverage the non-local and contextual relationships among pixels.
The traditional CNN is adopted for the extraction of local information.
Finally, the proposed GCN-MIF model fuses all the extracted local, non-local,
and contextual information. Extensive experiments show the effectiveness of our
proposed GCN-MIF model by quantitative and visualized results. Furthermore, a
double-blind reader study on a public clinical dataset is also performed to
validate the usability of denoising results in terms of the structural
fidelity, the noise suppression, and the overall score. Models and code are
available at https://github.com/tonyckc/GCN-MIF_demo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic segmentation of multispectral photoacoustic images using deep learning. (arXiv:2105.09624v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09624">
<div class="article-summary-box-inner">
<span><p>Photoacoustic (PA) imaging has the potential to revolutionize functional
medical imaging in healthcare due to the valuable information on tissue
physiology contained in multispectral photoacoustic measurements. Clinical
translation of the technology requires conversion of the high-dimensional
acquired data into clinically relevant and interpretable information. In this
work, we present a deep learning-based approach to semantic segmentation of
multispectral photoacoustic images to facilitate image interpretability.
Manually annotated photoacoustic {and ultrasound} imaging data are used as
reference and enable the training of a deep learning-based segmentation
algorithm in a supervised manner. Based on a validation study with
experimentally acquired data from 16 healthy human volunteers, we show that
automatic tissue segmentation can be used to create powerful analyses and
visualizations of multispectral photoacoustic images. Due to the intuitive
representation of high-dimensional information, such a preprocessing algorithm
could be a valuable means to facilitate the clinical translation of
photoacoustic imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D U-NetR: Low Dose Computed Tomography Reconstruction via Deep Learning and 3 Dimensional Convolutions. (arXiv:2105.14130v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14130">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduced a novel deep learning-based reconstruction
technique for low-dose CT imaging using 3 dimensional convolutions to include
the sagittal information unlike the existing 2 dimensional networks which
exploits correlation only in transverse plane. In the proposed reconstruction
technique, sparse and noisy sinograms are back-projected to the image domain
with FBP operation, then the denoising process is applied with a U-Net like
3-dimensional network called 3D U-NetR. The proposed network is trained with
synthetic and real chest CT images, and 2D U-Net is also trained with the same
dataset to show the importance of the third dimension in terms of recovering
the fine details. The proposed network shows better quantitative performance on
SSIM and PSNR, especially in the real chest CT data. More importantly, 3D
U-NetR captures medically critical visual details that cannot be visualized by
a 2D network on the reconstruction of real CT images with 1/10 of the normal
dose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01883">
<div class="article-summary-box-inner">
<span><p>Existing rotated object detectors are mostly inherited from the horizontal
detection paradigm, as the latter has evolved into a well-developed area.
However, these detectors are difficult to perform prominently in high-precision
detection due to the limitation of current regression loss design, especially
for objects with large aspect ratios. Taking the perspective that horizontal
detection is a special case for rotated object detection, in this paper, we are
motivated to change the design of rotation regression loss from induction
paradigm to deduction methodology, in terms of the relation between rotation
and horizontal detection. We show that one essential challenge is how to
modulate the coupled parameters in the rotation regression loss, as such the
estimated parameters can influence to each other during the dynamic joint
optimization, in an adaptive and synergetic way. Specifically, we first convert
the rotated bounding box into a 2-D Gaussian distribution, and then calculate
the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the
regression loss. By analyzing the gradient of each parameter, we show that KLD
(and its derivatives) can dynamically adjust the parameter gradients according
to the characteristics of the object. It will adjust the importance (gradient
weight) of the angle parameter according to the aspect ratio. This mechanism
can be vital for high-precision detection as a slight angle error would cause a
serious accuracy drop for large aspect ratios objects. More importantly, we
have proved that KLD is scale invariant. We further show that the KLD loss can
be degenerated into the popular $l_{n}$-norm loss for horizontal detection.
Experimental results on seven datasets using different detectors show its
consistent superiority, and codes are available at
https://github.com/yangxue0827/RotationDetection and
https://github.com/open-mmlab/mmrotate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stream Consensus Network: Submission to HACS Challenge 2021 Weakly-Supervised Learning Track. (arXiv:2106.10829v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10829">
<div class="article-summary-box-inner">
<span><p>This technical report presents our solution to the HACS Temporal Action
Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of
weakly-supervised temporal action localization is to temporally locate and
classify action of interest in untrimmed videos given only video-level labels.
We adopt the two-stream consensus network (TSCN) as the main framework in this
challenge. The TSCN consists of a two-stream base model training procedure and
a pseudo ground truth learning procedure. The base model training encourages
the model to predict reliable predictions based on single modality (i.e., RGB
or optical flow), based on the fusion of which a pseudo ground truth is
generated and in turn used as supervision to train the base models. On the HACS
v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our
method achieves 22.20% on the validation set and 21.68% on the testing set in
terms of average mAP. Our solution ranked the 2nd in this challenge, and we
hope our method can serve as a baseline for future academic research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Zero-Shot" Point Cloud Upsampling. (arXiv:2106.13765v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13765">
<div class="article-summary-box-inner">
<span><p>Recent supervised point cloud upsampling methods are restricted by the size
of training data and are limited in terms of covering all object shapes.
Besides the challenges faced due to data acquisition, the networks also
struggle to generalize on unseen records. In this paper, we present an internal
point cloud upsampling approach at a holistic level referred to as "Zero-Shot"
Point Cloud Upsampling (ZSPU). Our approach is data agnostic and relies solely
on the internal information provided by a particular point cloud without
patching in both self-training and testing phases. This single-stream design
significantly reduces the training time by learning the relation between low
resolution (LR) point clouds and their high (original) resolution (HR)
counterparts. This association will then provide super resolution (SR) outputs
when original point clouds are loaded as input. ZSPU achieves
competitive/superior quantitative and qualitative performances on benchmark
datasets when compared with other upsampling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Cycle Pruning: Pruning ConvNets Under a Tight Training Budget. (arXiv:2107.02086v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02086">
<div class="article-summary-box-inner">
<span><p>Introducing sparsity in a neural network has been an efficient way to reduce
its complexity while keeping its performance almost intact. Most of the time,
sparsity is introduced using a three-stage pipeline: 1) train the model to
convergence, 2) prune the model according to some criterion, 3) fine-tune the
pruned model to recover performance. The last two steps are often performed
iteratively, leading to reasonable results but also to a time-consuming and
complex process. In our work, we propose to get rid of the first step of the
pipeline and to combine the two other steps in a single pruning-training cycle,
allowing the model to jointly learn for the optimal weights while being pruned.
We do this by introducing a novel pruning schedule, named One-Cycle Pruning,
which starts pruning from the beginning of the training, and until its very
end. Adopting such a schedule not only leads to better performing pruned models
but also drastically reduces the training budget required to prune a model.
Experiments are conducted on a variety of architectures (VGG-16 and ResNet-18)
and datasets (CIFAR-10, CIFAR-100 and Caltech-101), and for relatively high
sparsity values (80%, 90%, 95% of weights removed). Our results show that
One-Cycle Pruning consistently outperforms commonly used pruning schedules such
as One-Shot Pruning, Iterative Pruning and Automated Gradual Pruning, on a
fixed training budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Large Circular Kernels into CNNs through Neural Architecture Search. (arXiv:2107.02451v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02451">
<div class="article-summary-box-inner">
<span><p>The square kernel is a standard unit for contemporary CNNs, as it fits well
on the tensor computation for convolution operation. However, the retinal
ganglion cells in the biological visual system have approximately concentric
receptive fields. Motivated by this observation, we propose to use circular
kernel with a concentric and isotropic receptive field as an option for the
convolution operation. We first propose a simple yet efficient implementation
of the convolution using circular kernels, and empirically show the significant
advantages of large circular kernels over the counterpart square kernels. We
then expand the operation space of several typical Neural Architecture Search
(NAS) methods with the convolutions of large circular kernels. The searched new
neural architectures do contain large circular kernels and outperform the
original searched models considerably. Our additional analysis also reveals
that large circular kernels could help the model to be more robust to the
rotated or sheared images due to their better rotation invariance. Our work
shows the potential of designing new convolutional kernels for CNNs, bringing
up the prospect of expanding the search space of NAS with new variants of
convolutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Classifiers Based Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. (arXiv:2108.00610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00610">
<div class="article-summary-box-inner">
<span><p>Adversarial training based on the maximum classifier discrepancy between two
classifier structures has achieved great success in unsupervised domain
adaptation tasks for image classification. The approach adopts the structure of
two classifiers, though simple and intuitive, the learned classification
boundary may not well represent the data property in the new domain. In this
paper, we propose to extend the structure to multiple classifiers to further
boost its performance. To this end, we develop a very straightforward approach
to adding more classifiers. We employ the principle that the classifiers are
different from each other to construct a discrepancy loss function for multiple
classifiers. The proposed construction method of loss function makes it
possible to add any number of classifiers to the original framework. The
proposed approach is validated through extensive experimental evaluations. We
demonstrate that, on average, adopting the structure of three classifiers
normally yields the best performance as a trade-off between accuracy and
efficiency. With minimum extra computational costs, the proposed approach can
significantly improve the performance of the original algorithm. The source
code of the proposed approach can be downloaded from
\url{https://github.com/rucv/MMCD\_DA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Research on Gender-related Fingerprint Features. (arXiv:2108.08233v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08233">
<div class="article-summary-box-inner">
<span><p>Fingerprint is an important biological feature of human body, which contains
abundant gender information. At present, the academic research of fingerprint
gender characteristics is generally at the level of understanding, while the
standardization research is quite limited. In this work, we propose a more
robust method, Dense Dilated Convolution ResNet (DDC-ResNet) to extract valid
gender information from fingerprints. By replacing the normal convolution
operations with the atrous convolution in the backbone, prior knowledge is
provided to keep the edge details and the global reception field can be
extended. We explored the results in 3 ways: 1) The efficiency of the
DDC-ResNet. 6 typical methods of automatic feature extraction coupling with 9
mainstream classifiers are evaluated in our dataset with fair implementation
details. Experimental results demonstrate that the combination of our approach
outperforms other combinations in terms of average accuracy and separate-gender
accuracy. It reaches 96.5% for average and 0.9752 (males)/0.9548 (females) for
separate-gender accuracy. 2) The effect of fingers. It is found that the best
performance of classifying gender with separate fingers is achieved by the
right ring finger. 3) The effect of specific features. Based on the
observations of the concentrations of fingerprints visualized by our approach,
it can be inferred that loops and whorls (level 1), bifurcations (level 2), as
well as line shapes (level 3) are connected with gender. Finally, we will open
source the dataset that contains 6000 fingerprint images
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12043">
<div class="article-summary-box-inner">
<span><p>Disentangled representation learning has been proposed as an approach to
learning general representations even in the absence of, or with limited,
supervision. A good general representation can be fine-tuned for new target
tasks using modest amounts of data, or used directly in unseen domains
achieving remarkable performance in the corresponding task. This alleviation of
the data and annotation requirements offers tantalising prospects for
applications in computer vision and healthcare. In this tutorial paper, we
motivate the need for disentangled representations, revisit key concepts, and
describe practical building blocks and criteria for learning such
representations. We survey applications in medical imaging emphasising choices
made in exemplar key works, and then discuss links to computer vision
applications. We conclude by presenting limitations, challenges, and
opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05265">
<div class="article-summary-box-inner">
<span><p>Stereoscopy exposits a natural perception of distance in a scene, and its
manifestation in 3D world understanding is an intuitive phenomenon. However, an
innate rigid calibration of binocular vision sensors is crucial for accurate
depth estimation. Alternatively, a monocular camera alleviates the limitation
at the expense of accuracy in estimating depth, and the challenge exacerbates
in harsh environmental conditions. Moreover, an optical sensor often fails to
acquire vital signals in harsh environments, and radar is used instead, which
gives coarse but more accurate signals. This work explores the utility of
coarse signals from radar when fused with fine-grained data from a monocular
camera for depth estimation in harsh environmental conditions. A variant of
feature pyramid network (FPN) extensively operates on fine-grained image
features at multiple scales with a fewer number of parameters. FPN feature maps
are fused with sparse radar features extracted with a Convolutional neural
network. The concatenated hierarchical features are used to predict the depth
with ordinal regression. We performed experiments on the nuScenes dataset, and
the proposed architecture stays on top in quantitative evaluations with reduced
parameters and faster inference. The depth estimation results suggest that the
proposed techniques can be used as an alternative to stereo depth estimation in
critical applications in robotics and self-driving cars. The source code will
be available in the following: \url{https://github.com/MI-Hussain/RVMDE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Non-Line-of-Sight Photography. (arXiv:2109.07783v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07783">
<div class="article-summary-box-inner">
<span><p>Non-line-of-sight (NLOS) imaging is based on capturing the multi-bounce
indirect reflections from the hidden objects. Active NLOS imaging systems rely
on the capture of the time of flight of light through the scene, and have shown
great promise for the accurate and robust reconstruction of hidden scenes
without the need for specialized scene setups and prior assumptions. Despite
that existing methods can reconstruct 3D geometries of the hidden scene with
excellent depth resolution, accurately recovering object textures and
appearance with high lateral resolution remains an challenging problem. In this
work, we propose a new problem formulation, called NLOS photography, to
specifically address this deficiency. Rather than performing an intermediate
estimate of the 3D scene geometry, our method follows a data-driven approach
and directly reconstructs 2D images of a NLOS scene that closely resemble the
pictures taken with a conventional camera from the location of the relay wall.
This formulation largely simplifies the challenging reconstruction problem by
bypassing the explicit modeling of 3D geometry, and enables the learning of a
deep model with a relatively small training dataset. The results are NLOS
reconstructions of unprecedented lateral resolution and image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Instance Segmentation with Automotive Radar Detection Points. (arXiv:2110.01775v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01775">
<div class="article-summary-box-inner">
<span><p>Automotive radar provides reliable environmental perception in all-weather
conditions with affordable cost, but it hardly supplies semantic and geometry
information due to the sparsity of radar detection points. With the development
of automotive radar technologies in recent years, instance segmentation becomes
possible by using automotive radar. Its data contain contexts such as radar
cross section and micro-Doppler effects, and sometimes can provide detection
when the field of view is obscured. The outcome from instance segmentation
could be potentially used as the input of trackers for tracking targets. The
existing methods often utilize a clustering-based classification framework,
which fits the need of real-time processing but has limited performance due to
minimum information provided by sparse radar detection points. In this paper,
we propose an efficient method based on clustering of estimated semantic
information to achieve instance segmentation for the sparse radar detection
points. In addition, we show that the performance of the proposed approach can
be further enhanced by incorporating the visual multi-layer perceptron. The
effectiveness of the proposed method is verified by experimental results on the
popular RadarScenes dataset, achieving 89.53% mean coverage and 86.97% mean
average precision with the IoU threshold of 0.5, which is superior to other
approaches in the literature. More significantly, the consumed memory is around
1MB, and the inference time is less than 40ms, indicating that our proposed
algorithm is storage and time efficient. These two criteria ensure the
practicality of the proposed method in real-world systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimMIM: A Simple Framework for Masked Image Modeling. (arXiv:2111.09886v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09886">
<div class="article-summary-box-inner">
<span><p>This paper presents SimMIM, a simple framework for masked image modeling. We
simplify recently proposed related approaches without special designs such as
block-wise masking and tokenization via discrete VAE or clustering. To study
what let the masked image modeling task learn good representations, we
systematically study the major components in our framework, and find that
simple designs of each component have revealed very strong representation
learning performance: 1) random masking of the input image with a moderately
large masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting
raw pixels of RGB values by direct regression performs no worse than the patch
classification approaches with complex designs; 3) the prediction head can be
as light as a linear layer, with no worse performance than heavier ones. Using
ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by
pre-training also on this dataset, surpassing previous best approach by +0.6%.
When applied on a larger model of about 650 million parameters, SwinV2-H, it
achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We
also leverage this approach to facilitate the training of a 3B model
(SwinV2-G), that by $40\times$ less data than that in previous practice, we
achieve the state-of-the-art on four representative vision benchmarks. The code
and models will be publicly available at https://github.com/microsoft/SimMIM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion. (arXiv:2111.10332v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10332">
<div class="article-summary-box-inner">
<span><p>Point cloud processing is a challenging task due to its sparsity and
irregularity. Prior works introduce delicate designs on either local feature
aggregator or global geometric architecture, but few combine both advantages.
We propose Dual-Scale Point Cloud Recognition with High-frequency Fusion
(DSPoint) to extract local-global features by concurrently operating on voxels
and points. We reverse the conventional design of applying convolution on
voxels and attention to points. Specifically, we disentangle point features
through channel dimension for dual-scale processing: one by point-wise
convolution for fine-grained geometry parsing, the other by voxel-wise global
attention for long-range structural exploration. We design a co-attention
fusion module for feature alignment to blend local-global modalities, which
conducts inter-scale cross-modality interaction by communicating high-frequency
coordinates information. Experiments and ablations on widely-adopted
ModelNet40, ShapeNet, and S3DIS demonstrate the state-of-the-art performance of
our DSPoint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GreedyNASv2: Greedier Search with a Greedy Path Filter. (arXiv:2111.12609v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12609">
<div class="article-summary-box-inner">
<span><p>Training a good supernet in one-shot NAS methods is difficult since the
search space is usually considerably huge (e.g., $13^{21}$). In order to
enhance the supernet's evaluation ability, one greedy strategy is to sample
good paths, and let the supernet lean towards the good ones and ease its
evaluation burden as a result. However, in practice the search can be still
quite inefficient since the identification of good paths is not accurate enough
and sampled paths still scatter around the whole search space. In this paper,
we leverage an explicit path filter to capture the characteristics of paths and
directly filter those weak ones, so that the search can be thus implemented on
the shrunk space more greedily and efficiently. Concretely, based on the fact
that good paths are much less than the weak ones in the space, we argue that
the label of "weak paths" will be more confident and reliable than that of
"good paths" in multi-path sampling. In this way, we thus cast the training of
path filter in the positive and unlabeled (PU) learning paradigm, and also
encourage a \textit{path embedding} as better path/operation representation to
enhance the identification capacity of the learned filter. By dint of this
embedding, we can further shrink the search space by aggregating similar
operations with similar embeddings, and the search can be more efficient and
accurate. Extensive experiments validate the effectiveness of the proposed
method GreedyNASv2. For example, our obtained GreedyNASv2-L achieves $81.1\%$
Top-1 accuracy on ImageNet dataset, significantly outperforming the ResNet-50
strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling. (arXiv:2111.12681v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12681">
<div class="article-summary-box-inner">
<span><p>A great challenge in video-language (VidL) modeling lies in the disconnection
between fixed video representations extracted from image/video understanding
models and downstream VidL data. Recent studies try to mitigate this
disconnection via end-to-end training. To make it computationally feasible,
prior works tend to "imagify" video inputs, i.e., a handful of sparsely sampled
frames are fed into a 2D CNN, followed by a simple mean-pooling or
concatenation to obtain the overall video representations. Although achieving
promising results, such simple approaches may lose temporal information that is
essential for performing downstream VidL tasks. In this work, we present
VIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video
transformer to explicitly model the temporal dynamics of video inputs. Further,
unlike previous studies that found pre-training tasks on video inputs (e.g.,
masked frame modeling) not very effective, we design a new pre-training task,
Masked Visual-token Modeling (MVM), for better video modeling. Specifically,
the original video frame patches are "tokenized" into discrete visual tokens,
and the goal is to recover the original visual tokens based on the masked
patches. Comprehensive analysis demonstrates the effectiveness of both explicit
temporal modeling via video transformer and MVM. As a result, VIOLET achieves
new state-of-the-art performance on 5 video question answering tasks and 4
text-to-video retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Low-Cost and Efficient Malaria Detection. (arXiv:2111.13656v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13656">
<div class="article-summary-box-inner">
<span><p>Malaria, a fatal but curable disease claims hundreds of thousands of lives
every year. Early and correct diagnosis is vital to avoid health complexities,
however, it depends upon the availability of costly microscopes and trained
experts to analyze blood-smear slides. Deep learning-based methods have the
potential to not only decrease the burden of experts but also improve
diagnostic accuracy on low-cost microscopes. However, this is hampered by the
absence of a reasonable size dataset. One of the most challenging aspects is
the reluctance of the experts to annotate the dataset at low magnification on
low-cost microscopes. We present a dataset to further the research on malaria
microscopy over the low-cost microscopes at low magnification. Our large-scale
dataset consists of images of blood-smear slides from several malaria-infected
patients, collected through microscopes at two different cost spectrums and
multiple magnifications. Malarial cells are annotated for the localization and
life-stage classification task on the images collected through the high-cost
microscope at high magnification. We design a mechanism to transfer these
annotations from the high-cost microscope at high magnification to the low-cost
microscope, at multiple magnifications. Multiple object detectors and domain
adaptation methods are presented as the baselines. Furthermore, a partially
supervised domain adaptation method is introduced to adapt the object-detector
to work on the images collected from the low-cost microscope. The dataset will
be made publicly available after publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification. (arXiv:2111.14271v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14271">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has led to substantial improvements in the quality of
learned embedding representations for tasks such as image classification.
However, a key drawback of existing contrastive augmentation methods is that
they may lead to the modification of the image content which can yield
undesired alterations of its semantics. This can affect the performance of the
model on downstream tasks. Hence, in this paper, we ask whether we can augment
image data in contrastive learning such that the task-relevant semantic content
of an image is preserved. For this purpose, we propose to leverage
saliency-based explanation methods to create content-preserving masked
augmentations for contrastive learning. Our novel explanation-driven supervised
contrastive learning (ExCon) methodology critically serves the dual goals of
encouraging nearby image embeddings to have similar content and explanation. To
quantify the impact of ExCon, we conduct experiments on the CIFAR-100 and the
Tiny ImageNet datasets. We demonstrate that ExCon outperforms vanilla
supervised contrastive learning in terms of classification, explanation
quality, adversarial robustness as well as probabilistic calibration in the
context of distributional shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00246">
<div class="article-summary-box-inner">
<span><p>Perceiving and interacting with 3D articulated objects, such as cabinets,
doors, and faucets, pose particular challenges for future home-assistant robots
performing daily tasks in human environments. Besides parsing the articulated
parts and joint parameters, researchers recently advocate learning manipulation
affordance over the input shape geometry which is more task-aware and
geometrically fine-grained. However, taking only passive observations as
inputs, these methods ignore many hidden but important kinematic constraints
(e.g., joint location and limits) and dynamic factors (e.g., joint friction and
restitution), therefore losing significant accuracy for test cases with such
uncertainties. In this paper, we propose a novel framework, named AdaAfford,
that learns to perform very few test-time interactions for quickly adapting the
affordance priors to more accurate instance-specific posteriors. We conduct
large-scale experiments using the PartNet-Mobility dataset and prove that our
system performs better than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving GAN Equilibrium by Raising Spatial Awareness. (arXiv:2112.00718v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00718">
<div class="article-summary-box-inner">
<span><p>The success of Generative Adversarial Networks (GANs) is largely built upon
the adversarial training between a generator (G) and a discriminator (D). They
are expected to reach a certain equilibrium where D cannot distinguish the
generated images from the real ones. However, such an equilibrium is rarely
achieved in practical GAN training, instead, D almost always surpasses G. We
attribute one of its sources to the information asymmetry between D and G. We
observe that D learns its own visual attention when determining whether an
image is real or fake, but G has no explicit clue on which regions to focus on
for a particular synthesis. To alleviate the issue of D dominating the
competition in GANs, we aim to raise the spatial awareness of G. Randomly
sampled multi-level heatmaps are encoded into the intermediate layers of G as
an inductive bias. Thus G can purposefully improve the synthesis of certain
image regions. We further propose to align the spatial awareness of G with the
attention map induced from D. Through this way we effectively lessen the
information gap between D and G. Extensive results show that our method pushes
the two-player game in GANs closer to the equilibrium, leading to a better
synthesis performance. As a byproduct, the introduced spatial awareness
facilitates interactive editing over the output synthesis. Demo video and code
are available at https://genforce.github.io/eqgan-sa/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BA-Net: Bridge Attention for Deep Convolutional Neural Networks. (arXiv:2112.04150v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04150">
<div class="article-summary-box-inner">
<span><p>In recent years, channel attention mechanism has been widely investigated due
to its great potential in improving the performance of deep convolutional
neural networks (CNNs) in many vision tasks. However, in most of the existing
methods, only the output of the adjacent convolution layer is fed into the
attention layer for calculating the channel weights. Information from other
convolution layers has been ignored. With these observations, a simple
strategy, named Bridge Attention Net (BA-Net), is proposed in this paper for
better performance with channel attention mechanisms. The core idea of this
design is to bridge the outputs of the previous convolution layers through skip
connections for channel weights generation. Based on our experiment and theory
analysis, we find that features from previous layers also contribute to the
weights significantly. The Comprehensive evaluation demonstrates that the
proposed approach achieves state-of-the-art(SOTA) performance compared with the
existing methods in accuracy and speed. which shows that Bridge Attention
provides a new perspective on the design of neural network architectures with
great potential in improving performance. The code is available at
https://github.com/zhaoy376/Bridge-Attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel multi-view deep learning approach for BI-RADS and density assessment of mammograms. (arXiv:2112.04490v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04490">
<div class="article-summary-box-inner">
<span><p>Advanced deep learning (DL) algorithms may predict the patient's risk of
developing breast cancer based on the Breast Imaging Reporting and Data System
(BI-RADS) and density standards. Recent studies have suggested that the
combination of multi-view analysis improved the overall breast exam
classification. In this paper, we propose a novel multi-view DL approach for
BI-RADS and density assessment of mammograms. The proposed approach first
deploys deep convolutional networks for feature extraction on each view
separately. The extracted features are then stacked and fed into a Light
Gradient Boosting Machine (LightGBM) classifier to predict BI-RADS and density
scores. We conduct extensive experiments on both the internal mammography
dataset and the public dataset Digital Database for Screening Mammography
(DDSM). The experimental results demonstrate that the proposed approach
outperforms the single-view classification approach on two benchmark datasets
by huge F1-score margins (+5% on the internal dataset and +10% on the DDSM
dataset). These results highlight the vital role of combining multi-view
information to improve the performance of breast cancer risk prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Vision Transformers for Incremental Learning. (arXiv:2112.06103v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06103">
<div class="article-summary-box-inner">
<span><p>This paper proposes a working recipe of using Vision Transformer (ViT) in
class incremental learning. Although this recipe only combines existing
techniques, developing the combination is not trivial. Firstly, naive
application of ViT to replace convolutional neural networks (CNNs) in
incremental learning results in serious performance degradation. Secondly, we
nail down three issues of naively using ViT: (a) ViT has very slow convergence
when the number of classes is small, (b) more bias towards new classes is
observed in ViT than CNN-based architectures, and (c) the conventional learning
rate of ViT is too low to learn a good classifier layer. Finally, our solution,
named ViTIL (ViT for Incremental Learning) achieves new state-of-the-art on
both CIFAR and ImageNet datasets for all three class incremental learning
setups by a clear margin. We believe this advances the knowledge of transformer
in the incremental learning community. Code will be publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Magnifying Networks for Images with Billions of Pixels. (arXiv:2112.06121v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06121">
<div class="article-summary-box-inner">
<span><p>The shift towards end-to-end deep learning has brought unprecedented advances
in many areas of computer vision. However, deep neural networks are trained on
images with resolutions that rarely exceed $1,000 \times 1,000$ pixels. The
growing use of scanners that create images with extremely high resolutions
(average can be $100,000 \times 100,000$ pixels) thereby presents novel
challenges to the field. Most of the published methods preprocess
high-resolution images into a set of smaller patches, imposing an a priori
belief on the best properties of the extracted patches (magnification, field of
view, location, etc.). Herein, we introduce Magnifying Networks (MagNets) as an
alternative deep learning solution for gigapixel image analysis that does not
rely on a preprocessing stage nor requires the processing of billions of
pixels. MagNets can learn to dynamically retrieve any part of a gigapixel
image, at any magnification level and field of view, in an end-to-end fashion
with minimal ground truth (a single global, slide-level label). Our results on
the publicly available Camelyon16 and Camelyon17 datasets corroborate to the
effectiveness and efficiency of MagNets and the proposed optimization framework
for whole slide image classification. Importantly, MagNets process far less
patches from each slide than any of the existing approaches ($10$ to $300$
times less).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVIP: Sequence VerIfication for Procedures in Videos. (arXiv:2112.06447v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06447">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel sequence verification task that aims to
distinguish positive video pairs performing the same action sequence from
negative ones with step-level transformations but still conducting the same
task. Such a challenging task resides in an open-set setting without prior
action detection or segmentation that requires event-level or even frame-level
annotations. To that end, we carefully reorganize two publicly available
action-related datasets with step-procedure-task structure. To fully
investigate the effectiveness of any method, we collect a scripted video
dataset enumerating all kinds of step-level transformations in chemical
experiments. Besides, a novel evaluation metric Weighted Distance Ratio is
introduced to ensure equivalence for different step-level transformations
during evaluation. In the end, a simple but effective baseline based on the
transformer encoder with a novel sequence alignment loss is introduced to
better characterize long-term dependency between steps, which outperforms other
action recognition methods. Codes and data will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Value Retrieval with Arbitrary Queries for Form-like Documents. (arXiv:2112.07820v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07820">
<div class="article-summary-box-inner">
<span><p>We propose value retrieval with arbitrary queries for form-like documents to
reduce human effort of processing forms. Unlike previous methods that only
address a fixed set of field items, our method predicts target value for an
arbitrary query based on the understanding of the layout and semantics of a
form. To further boost model performance, we propose a simple document language
modeling (SimpleDLM) strategy to improve document understanding on large-scale
model pre-training. Experimental results show that our method outperforms
previous designs significantly and the SimpleDLM further improves our
performance on value retrieval by around 17% F1 score compared with the
state-of-the-art pre-training method. Code is available at
https://github.com/salesforce/QVR-SimpleDLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPTS: Single-Point Text Spotting. (arXiv:2112.07917v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07917">
<div class="article-summary-box-inner">
<span><p>Existing scene text spotting (i.e., end-to-end text detection and
recognition) methods rely on costly bounding box annotations (e.g., text-line,
word-level, or character-level bounding boxes). For the first time, we
demonstrate that training scene text spotting models can be achieved with an
extremely low-cost annotation of a single-point for each instance. We propose
an end-to-end scene text spotting method that tackles scene text spotting as a
sequence prediction task. Given an image as input, we formulate the desired
detection and recognition results as a sequence of discrete tokens and use an
auto-regressive Transformer to predict the sequence. The proposed method is
simple yet effective, which can achieve state-of-the-art results on widely used
benchmarks. Most significantly, we show that the performance is not very
sensitive to the positions of the point annotation, meaning that it can be much
easier to be annotated or even be automatically generated than the bounding box
that requires precise positions. We believe that such a pioneer attempt
indicates a significant opportunity for scene text spotting applications of a
much larger scale than previously possible. The code will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition. (arXiv:2112.09690v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09690">
<div class="article-summary-box-inner">
<span><p>Semi-supervised action recognition is a challenging but important task due to
the high cost of data annotation. A common approach to this problem is to
assign unlabeled data with pseudo-labels, which are then used as additional
supervision in training. Typically in recent work, the pseudo-labels are
obtained by training a model on the labeled data, and then using confident
predictions from the model to teach itself. In this work, we propose a more
effective pseudo-labeling scheme, called Cross-Model Pseudo-Labeling (CMPL).
Concretely, we introduce a lightweight auxiliary network in addition to the
primary backbone, and ask them to predict pseudo-labels for each other. We
observe that, due to their different structural biases, these two models tend
to learn complementary representations from the same video clips. Each model
can thus benefit from its counterpart by utilizing cross-model predictions as
supervision. Experiments on different data partition protocols demonstrate the
significant improvement of our framework over existing alternatives. For
example, CMPL achieves $17.6\%$ and $25.1\%$ Top-1 accuracy on Kinetics-400 and
UCF-101 using only the RGB modality and $1\%$ labeled data, outperforming our
baseline model, FixMatch, by $9.0\%$ and $10.3\%$, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distill and De-bias: Mitigating Bias in Face Verification using Knowledge Distillation. (arXiv:2112.09786v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09786">
<div class="article-summary-box-inner">
<span><p>Face recognition networks generally demonstrate bias with respect to
sensitive attributes like gender, skintone etc. For gender and skintone, we
observe that the regions of the face that a network attends to vary by the
category of an attribute. This might contribute to bias. Building on this
intuition, we propose a novel distillation-based approach called Distill and
De-bias (D&amp;D) to enforce a network to attend to similar face regions,
irrespective of the attribute category. In D&amp;D, we train a teacher network on
images from one category of an attribute; e.g. light skintone. Then distilling
information from the teacher, we train a student network on images of the
remaining category; e.g., dark skintone. A feature-level distillation loss
constrains the student network to generate teacher-like representations. This
allows the student network to attend to similar face regions for all attribute
categories and enables it to reduce bias. We also propose a second distillation
step on top of D&amp;D, called D&amp;D++. Here, we distill the `un-biasedness' of the
D&amp;D network into a new student network, the D&amp;D++ network, while training this
new network on all attribute categories; e.g., both light and dark skintones.
This helps us train a network that is less biased for an attribute, while
obtaining higher face verification performance than D&amp;D. We show that D&amp;D++
outperforms existing baselines in reducing gender and skintone bias on the
IJB-C dataset, while obtaining higher face verification performance than
existing adversarial de-biasing methods. We evaluate the effectiveness of our
proposed methods on two state-of-the-art face recognition networks: ArcFace and
Crystalface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-aware Image Synthesis via Learning Structural and Textural Representations. (arXiv:2112.10759v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10759">
<div class="article-summary-box-inner">
<span><p>Making generative models 3D-aware bridges the 2D image space and the 3D
physical world yet remains challenging. Recent attempts equip a Generative
Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D
coordinates to pixel values, as a 3D prior. However, the implicit function in
NeRF has a very local receptive field, making the generator hard to become
aware of the global structure. Meanwhile, NeRF is built on volume rendering
which can be too costly to produce high-resolution results, increasing the
optimization difficulty. To alleviate these two problems, we propose a novel
framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis,
through explicitly learning a structural representation and a textural
representation. We first learn a feature volume to represent the underlying
structure, which is then converted to a feature field using a NeRF-like model.
The feature field is further accumulated into a 2D feature map as the textural
representation, followed by a neural renderer for appearance synthesis. Such a
design enables independent control of the shape and the appearance. Extensive
experiments on a wide range of datasets show that our approach achieves
sufficiently higher image quality and better 3D control than the previous
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape from Polarization for Complex Scenes in the Wild. (arXiv:2112.11377v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11377">
<div class="article-summary-box-inner">
<span><p>We present a new data-driven approach with physics-based priors to
scene-level normal estimation from a single polarization image. Existing shape
from polarization (SfP) works mainly focus on estimating the normal of a single
object rather than complex scenes in the wild. A key barrier to high-quality
scene-level SfP is the lack of real-world SfP data in complex scenes. Hence, we
contribute the first real-world scene-level SfP dataset with paired input
polarization images and ground-truth normal maps. Then we propose a
learning-based framework with a multi-head self-attention module and viewing
encoding, which is designed to handle increasing polarization ambiguities
caused by complex materials and non-orthographic projection in scene-level SfP.
Our trained model can be generalized to far-field outdoor scenes as the
relationship between polarized light and surface normals is not affected by
distance. Experimental results demonstrate that our approach significantly
outperforms existing SfP models on two datasets. Our dataset and source code
will be publicly available at https://github.com/ChenyangLEI/sfp-wild
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BDG-Net: Boundary Distribution Guided Network for Accurate Polyp Segmentation. (arXiv:2201.00767v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00767">
<div class="article-summary-box-inner">
<span><p>Colorectal cancer (CRC) is one of the most common fatal cancer in the world.
Polypectomy can effectively interrupt the progression of adenoma to
adenocarcinoma, thus reducing the risk of CRC development. Colonoscopy is the
primary method to find colonic polyps. However, due to the different sizes of
polyps and the unclear boundary between polyps and their surrounding mucosa, it
is challenging to segment polyps accurately. To address this problem, we design
a Boundary Distribution Guided Network (BDG-Net) for accurate polyp
segmentation. Specifically, under the supervision of the ideal Boundary
Distribution Map (BDM), we use Boundary Distribution Generate Module (BDGM) to
aggregate high-level features and generate BDM. Then, BDM is sent to the
Boundary Distribution Guided Decoder (BDGD) as complementary spatial
information to guide the polyp segmentation. Moreover, a multi-scale feature
interaction strategy is adopted in BDGD to improve the segmentation accuracy of
polyps with different sizes. Extensive quantitative and qualitative evaluations
demonstrate the effectiveness of our model, which outperforms state-of-the-art
models remarkably on five public polyp datasets while maintaining low
computational complexity. Code: https://github.com/zihuanqiu/BDG-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Empirical Study of Vision-Language Pre-trained Model for Supervised Cross-Modal Retrieval. (arXiv:2201.02772v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02772">
<div class="article-summary-box-inner">
<span><p>Cross-Modal Retrieval (CMR) is an important research topic across multimodal
computing and information retrieval, which takes one type of data as the query
to retrieve relevant data of another type. It has been widely used in many
real-world applications. Recently, the vision-language pre-trained models
represented by CLIP demonstrate its superiority in learning the visual and
textual representations and gain impressive performance on various vision and
language related tasks. Although CLIP as well as the previous pre-trained
models have shown great performance improvement in the unsupervised CMR, the
performance and impact of these pre-trained models on the supervised CMR were
rarely explored due to the lack of common representation for the multimodal
class-level associations. In this paper, we take CLIP as the current
representative vision-language pre-trained model to conduct a comprehensive
empirical study. We evaluate its performance and impact on the supervised CMR,
and attempt to answer several key research questions. To this end, we first
propose a novel model CLIP4CMR (CLIP enhanced network for Cross-Modal
Retrieval) that employs the pre-trained CLIP as backbone network to perform the
supervised CMR. Then by means of the CLIP4CMR framework, we revisit the design
of different learning objectives in current CMR methods to provide new insights
on model design. Moreover, we investigate the most concerned aspects in
applying CMR, including the robustness to modality imbalance and sensitivity to
hyper-parameters, to provide new perspectives for practical applications.
Through extensive experiments, we show that CLIP4CMR achieves the SOTA results
with prominent improvements on the benchmark datasets, and can be used as a
fundamental framework to empirically study the key research issues of the
supervised CMR, with significant implications for model design and practical
considerations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore and Match: A New Paradigm for Temporal Video Grounding with Natural Language. (arXiv:2201.10168v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10168">
<div class="article-summary-box-inner">
<span><p>Temporal Video Grounding (TVG) aims to localize time segments in an untrimmed
video according to natural language queries. In this work, we present a new
paradigm named Explore-and-Match for TVG that seamlessly unifies two streams of
TVG methods: proposal-free and proposal-based; the former explores the search
space to find segments directly, and the latter matches the predefined
proposals with ground truths. To achieve this goal, we view TVG as a set
prediction problem and design an end-to-end trainable Language Video
Transformer (LVTR) that utilizes the architectural strengths of rich
contextualization and parallel decoding for set prediction. The overall
training schedule is balanced by two key losses that play different roles,
namely temporal localization loss and set guidance loss. These two losses allow
each proposal to regress the target segment and identify the target query. More
specifically, LVTR first explores the search space to diversify the initial
proposals, and then matches the proposals to the corresponding targets to align
them in a fine-grained manner. The Explore-and-Match scheme successfully
combines the strengths of two complementary methods without encoding prior
knowledge (e.g., non-maximum suppression) into the TVG pipeline. As a result,
LVTR sets new state-of-the-art results on two TVG benchmarks (ActivityCaptions
and Charades-STA) with double the inference speed. Code is available at
https://github.com/sangminwoo/Explore-and-Match.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossRectify: Leveraging Disagreement for Semi-supervised Object Detection. (arXiv:2201.10734v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10734">
<div class="article-summary-box-inner">
<span><p>Semi-supervised object detection has recently achieved substantial progress.
As a mainstream solution, the self-labeling-based methods train the detector on
both labeled data and unlabeled data with pseudo labels predicted by the
detector itself, but their performances are always limited. Through
experimental analysis, we reveal the underlying reason is that the detector is
misguided by the incorrect pseudo labels predicted by itself (dubbed
self-errors). These self-errors can hurt performance even worse than
random-errors, and can be neither discerned nor rectified during the
self-labeling process. In this paper, we propose an effective detection
framework named CrossRectify, to obtain accurate pseudo labels by
simultaneously training two detectors with different initial parameters.
Specifically, the proposed approach leverages the disagreements between
detectors to discern the self-errors and refines the pseudo label quality by
the proposed cross-rectifying mechanism. Extensive experiments show that
CrossRectify achieves outperforming performances over various detector
structures on 2D and 3D detection benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Compressible and Learnable Image Transformation with Secret Key and Its Applications. (arXiv:2201.11006v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11006">
<div class="article-summary-box-inner">
<span><p>This article presents an overview of image transformation with a secret key
and its applications. Image transformation with a secret key enables us not
only to protect visual information on plain images but also to embed unique
features controlled with a key into images. In addition, numerous encryption
methods can generate encrypted images that are compressible and learnable for
machine learning. Various applications of such transformation have been
developed by using these properties. In this paper, we focus on a class of
image transformation referred to as learnable image encryption, which is
applicable to privacy-preserving machine learning and adversarially robust
defense. Detailed descriptions of both transformation algorithms and
performances are provided. Moreover, we discuss robustness against various
attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11114">
<div class="article-summary-box-inner">
<span><p>Some neurons in deep networks specialize in recognizing highly specific
perceptual, structural, or semantic features of inputs. In computer vision,
techniques exist for identifying neurons that respond to individual concept
categories like colors, textures, and object classes. But these techniques are
limited in scope, labeling only a small subset of neurons and behaviors in any
network. Is a richer characterization of neuron-level computation possible? We
introduce a procedure (called MILAN, for mutual-information-guided linguistic
annotation of neurons) that automatically labels neurons with open-ended,
compositional, natural language descriptions. Given a neuron, MILAN generates a
description by searching for a natural language string that maximizes pointwise
mutual information with the image regions in which the neuron is active. MILAN
produces fine-grained descriptions that capture categorical, relational, and
logical structure in learned features. These descriptions obtain high agreement
with human-generated feature descriptions across a diverse set of model
architectures and tasks, and can aid in understanding and controlling learned
models. We highlight three applications of natural language neuron
descriptions. First, we use MILAN for analysis, characterizing the distribution
and importance of neurons selective for attribute, category, and relational
information in vision models. Second, we use MILAN for auditing, surfacing
neurons sensitive to human faces in datasets designed to obscure them. Finally,
we use MILAN for editing, improving robustness in an image classifier by
deleting neurons sensitive to text features spuriously correlated with class
labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Feature based Cross-slide Registration. (arXiv:2202.09971v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09971">
<div class="article-summary-box-inner">
<span><p>Cross-slide image analysis provides additional information by analysing the
expression of different biomarkers as compared to a single slide analysis.
These biomarker stained slides are analysed side by side, revealing unknown
relations between them. During the slide preparation, a tissue section may be
placed at an arbitrary orientation as compared to other sections of the same
tissue block. The problem is compounded by the fact that tissue contents are
likely to change from one section to the next and there may be unique artefacts
on some of the slides. This makes registration of each section to a reference
section of the same tissue block an important pre-requisite task before any
cross-slide analysis. We propose a deep feature based registration (DFBR)
method which utilises data-driven features to estimate the rigid
transformation. We adopted a multi-stage strategy for improving the quality of
registration. We also developed a visualisation tool to view registered pairs
of WSIs at different magnifications. With the help of this tool, one can apply
a transformation on the fly without the need to generate transformed source WSI
in a pyramidal form. We compared the performance of data-driven features with
that of hand-crafted features on the COMET dataset. Our approach can align the
images with low registration errors. Generally, the success of non-rigid
registration is dependent on the quality of rigid registration. To evaluate the
efficacy of the DFBR method, the first two steps of the ANHIR winner's
framework are replaced with our DFBR to register challenge provided image
pairs. The modified framework produces comparable results to that of challenge
winning team.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning. (arXiv:2202.12588v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12588">
<div class="article-summary-box-inner">
<span><p>The expensive annotation cost is notoriously known as the main constraint for
the development of the point cloud semantic segmentation technique. Active
learning methods endeavor to reduce such cost by selecting and labeling only a
subset of the point clouds, yet previous attempts ignore the spatial-structural
diversity of the selected samples, inducing the model to select clustered
candidates with similar shapes in a local area while missing other
representative ones in the global environment. In this paper, we propose a new
3D region-based active learning method to tackle this problem. Dubbed SSDR-AL,
our method groups the original point clouds into superpoints and incrementally
selects the most informative and representative ones for label acquisition. We
achieve the selection mechanism via a graph reasoning network that considers
both the spatial and structural diversities of superpoints. To deploy SSDR-AL
in a more practical scenario, we design a noise-aware iterative labeling
strategy to confront the "noisy annotation" problem introduced by the previous
"dominant labeling" strategy in superpoints. Extensive experiments on two point
cloud benchmarks demonstrate the effectiveness of SSDR-AL in the semantic
segmentation task. Particularly, SSDR-AL significantly outperforms the baseline
method and reduces the annotation cost by up to 63.0% and 24.0% when achieving
90% performance of fully supervised learning, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGMR-Net: Attention Guided Multiscale Recovery framework for stroke segmentation. (arXiv:2202.13687v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13687">
<div class="article-summary-box-inner">
<span><p>Automatic and accurate lesion segmentation is critical for clinically
estimating the lesion statuses of stroke diseases and developing appropriate
diagnostic systems. Although existing methods have achieved remarkable results,
further adoption of the models is hindered by: (1) inter-class indistinction,
the normal brain tissue resembles the lesion in appearance. (2) intra-class
inconsistency, large variability exists between different areas of the lesion.
To solve these challenges in stroke segmentation, we propose a novel method,
namely Attention Guided Multiscale Recovery framework (AGMR-Net) in this paper.
Firstly, a coarse-grained patch attention module in the encoding is adopted to
get a patch-based coarse-grained attention map in a multi-stage explicitly
supervised way, enabling target spatial context saliency representation with a
patch-based weighting technique that eliminates the effect of intra-class
inconsistency. Secondly, to obtain a more detailed boundary partitioning to
solve the challenge of the inter-class indistinction, a newly designed
cross-dimensional feature fusion module is used to capture global contextual
information to further guide the selective aggregation of 2D and 3D features,
which can compensate for the lack of boundary learning capability of 2D
convolution. Lastly, in the decoding stage, an innovative designed multi-scale
deconvolution upsampling instead of linear interpolation enhances the recovery
of target space and boundary information. The AGMR-Net is evaluated on the open
dataset Anatomical Tracings of Lesions-After-Stroke (ATLAS), achieving the
highest dice similarity coefficient (DSC) score of 0.594, Hausdorff distance of
27.005 mm, and average symmetry surface distance of 7.137 mm, which demonstrate
that our proposed method outperforms other state-of-the-art methods and has
great potential in the diagnosis of stroke.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers. (arXiv:2203.00156v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00156">
<div class="article-summary-box-inner">
<span><p>As technology advances, the need for safe, efficient, and collaborative
human-robot-teams has become increasingly important. One of the most
fundamental collaborative tasks in any setting is the object handover.
Human-to-robot handovers can take either of two approaches: (1) direct
hand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach
ensures minimal contact between the human and robot but can also result in
increased idle time due to having to wait for the object to first be placed
down on a surface. To minimize such idle time, the robot must preemptively
predict the human intent of where the object will be placed. Furthermore, for
the robot to preemptively act in any sort of productive manner, predictions and
motion planning must occur in real-time. We introduce a novel
prediction-planning pipeline that allows the robot to preemptively move towards
the human agent's intended placement location using gaze and gestures as model
inputs. In this paper, we investigate the performance and drawbacks of our
early intent predictor-planner as well as the practical benefits of using such
a pipeline through a human-robot case study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Semantic and Instance Segmentation for Colon Nuclei Identification and Counting. (arXiv:2203.00157v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00157">
<div class="article-summary-box-inner">
<span><p>We address the problem of automated nuclear segmentation, classification, and
quantification from Haematoxylin and Eosin stained histology images, which is
of great relevance for several downstream computational pathology applications.
In this work, we present a solution framed as a simultaneous semantic and
instance segmentation framework. Our solution is part of the Colon Nuclei
Identification and Counting (CoNIC) Challenge. We first train a semantic and
instance segmentation model separately. Our framework uses as backbone HoverNet
and Cascade Mask-RCNN models. We then ensemble the results with a custom
Non-Maximum Suppression embedding (NMS). In our framework, the semantic model
computes a class prediction for the cells whilst the instance model provides a
refined segmentation. We demonstrate, through our experimental results, that
our model outperforms the provided baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning based Prediction of MSI using MMR Markers in Colorectal Cancer. (arXiv:2203.00449v2 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00449">
<div class="article-summary-box-inner">
<span><p>The accurate diagnosis and molecular profiling of colorectal cancers are
critical for planning the best treatment options for patients. Microsatellite
instability (MSI) or mismatch repair (MMR) status plays a vital role
inappropriate treatment selection, has prognostic implications and is used to
investigate the possibility of patients having underlying genetic disorders
(Lynch syndrome). NICE recommends that all CRC patients should be offered
MMR/microsatellite instability (MSI) testing. Immunohistochemistry is commonly
used to assess MMR status with subsequent molecular testing performed as
required. This incurs significant extra costs and requires additional
resources. The introduction of automated methods that can predict MSI or MMR
status from a target image could substantially reduce the cost associated with
MMR testing. Unlike previous studies on MSI prediction involving training a CNN
using coarse labels (Microsatellite Instable vs Microsatellite Stable), we have
utilised fine-grain MMR labels for training purposes. In this paper, we present
our work on predicting MSI status in a two-stage process using a single target
slide either stained with CK8/18 or H\&amp;E. First, we trained a multi-headed
convolutional neural network model where each head was responsible for
predicting one of the MMR protein expressions. To this end, we performed the
registration of MMR stained slides to the target slide as a pre-processing
step. In the second stage, statistical features computed from the MMR
prediction maps were used for the final MSI prediction. Our results
demonstrated that MSI classification can be improved by incorporating
fine-grained MMR labels in comparison to the previous approaches in which only
coarse labels were utilised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04961">
<div class="article-summary-box-inner">
<span><p>Early detection of breast cancer in mammography screening via deep-learning
based computer-aided detection systems shows promising potential in improving
the curability and mortality rates of breast cancer. However, many clinical
centres are restricted in the amount and heterogeneity of available data to
train such models to (i) achieve promising performance and to (ii) generalise
well across acquisition protocols and domains. As sharing data between centres
is restricted due to patient privacy concerns, we propose a potential solution:
sharing trained generative models between centres as substitute for real
patient data. In this work, we use three well known mammography datasets to
simulate three different centres, where one centre receives the trained
generator of Generative Adversarial Networks (GANs) from the two remaining
centres in order to augment the size and heterogeneity of its training dataset.
We evaluate the utility of this approach on mammography patch classification on
the test set of the GAN-receiving centre using two different classification
models, (a) a convolutional neural network and (b) a transformer neural
network. Our experiments demonstrate that shared GANs notably increase the
performance of both transformer and convolutional classification models and
highlight this approach as a viable alternative to inter-centre data sharing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PseudoProp: Robust Pseudo-Label Generation for Semi-Supervised Object Detection in Autonomous Driving Systems. (arXiv:2203.05983v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05983">
<div class="article-summary-box-inner">
<span><p>Semi-supervised object detection methods are widely used in autonomous
driving systems, where only a fraction of objects are labeled. To propagate
information from the labeled objects to the unlabeled ones, pseudo-labels for
unlabeled objects must be generated. Although pseudo-labels have proven to
improve the performance of semi-supervised object detection significantly, the
applications of image-based methods to video frames result in numerous miss or
false detections using such generated pseudo-labels. In this paper, we propose
a new approach, PseudoProp, to generate robust pseudo-labels by leveraging
motion continuity in video frames. Specifically, PseudoProp uses a novel
bidirectional pseudo-label propagation approach to compensate for misdetection.
A feature-based fusion technique is also used to suppress inference noise.
Extensive experiments on the large-scale Cityscapes dataset demonstrate that
our method outperforms the state-of-the-art semi-supervised object detection
methods by 7.4% on mAP75.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision and Deep Learning for Fish Classification in Underwater Habitats: A Survey. (arXiv:2203.06951v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06951">
<div class="article-summary-box-inner">
<span><p>Marine scientists use remote underwater video recording to survey fish
species in their natural habitats. This helps them understand and predict how
fish respond to climate change, habitat degradation, and fishing pressure. This
information is essential for developing sustainable fisheries for human
consumption, and for preserving the environment. However, the enormous volume
of collected videos makes extracting useful information a daunting and
time-consuming task for a human. A promising method to address this problem is
the cutting-edge Deep Learning (DL) technology.DL can help marine scientists
parse large volumes of video promptly and efficiently, unlocking niche
information that cannot be obtained using conventional manual monitoring
methods. In this paper, we provide an overview of the key concepts of DL, while
presenting a survey of literature on fish habitat monitoring with a focus on
underwater fish classification. We also discuss the main challenges faced when
developing DL for underwater image processing and propose approaches to address
them. Finally, we provide insights into the marine habitat monitoring research
domain and shed light on what the future of DL for underwater image processing
may hold. This paper aims to inform a wide range of readers from marine
scientists who would like to apply DL in their research to computer scientists
who would like to survey state-of-the-art DL-based underwater fish habitat
monitoring literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Unsupervised Hashing with Latent Semantic Components. (arXiv:2203.09420v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09420">
<div class="article-summary-box-inner">
<span><p>Deep unsupervised hashing has been appreciated in the regime of image
retrieval. However, most prior arts failed to detect the semantic components
and their relationships behind the images, which makes them lack discriminative
power. To make up the defect, we propose a novel Deep Semantic Components
Hashing (DSCH), which involves a common sense that an image normally contains a
bunch of semantic components with homology and co-occurrence relationships.
Based on this prior, DSCH regards the semantic components as latent variables
under the Expectation-Maximization framework and designs a two-step iterative
algorithm with the objective of maximum likelihood of training data. Firstly,
DSCH constructs a semantic component structure by uncovering the fine-grained
semantics components of images with a Gaussian Mixture Modal~(GMM), where an
image is represented as a mixture of multiple components, and the semantics
co-occurrence are exploited. Besides, coarse-grained semantics components, are
discovered by considering the homology relationships between fine-grained
components, and the hierarchy organization is then constructed. Secondly, DSCH
makes the images close to their semantic component centers at both fine-grained
and coarse-grained levels, and also makes the images share similar semantic
components close to each other. Extensive experiments on three benchmark
datasets demonstrate that the proposed hierarchical semantic components indeed
facilitate the hashing model to achieve superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training. (arXiv:2203.11947v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11947">
<div class="article-summary-box-inner">
<span><p>Recent image inpainting methods have made great progress but often struggle
to generate plausible image structures when dealing with large holes in complex
images. This is partially due to the lack of effective network structures that
can capture both the long-range dependency and high-level semantics of an
image. To address these problems, we propose cascaded modulation GAN (CM-GAN),
a new network design consisting of an encoder with Fourier convolution blocks
that extract multi-scale feature representations from the input image with
holes and a StyleGAN-like decoder with a novel cascaded global-spatial
modulation block at each scale level. In each decoder block, global modulation
is first applied to perform coarse semantic-aware structure synthesis, then
spatial modulation is applied on the output of global modulation to further
adjust the feature map in a spatially adaptive fashion. In addition, we design
an object-aware training scheme to prevent the network from hallucinating new
objects inside holes, fulfilling the needs of object removal tasks in
real-world scenarios. Extensive experiments are conducted to show that our
method significantly outperforms existing methods in both quantitative and
qualitative evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lymphocyte Classification in Hyperspectral Images of Ovarian Cancer Tissue Biopsy Samples. (arXiv:2203.12112v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12112">
<div class="article-summary-box-inner">
<span><p>Current methods for diagnosing the progression of multiple types of cancer
within patients rely on interpreting stained needle biopsies. This process is
time-consuming and susceptible to error throughout the paraffinization,
Hematoxylin and Eosin (H&amp;E) staining, deparaffinization, and annotation stages.
Fourier Transform Infrared (FTIR) imaging has been shown to be a promising
alternative to staining for appropriately annotating biopsy cores without the
need for deparaffinization or H&amp;E staining with the use of Fourier Transform
Infrared (FTIR) images when combined with machine learning to interpret the
dense spectral information. We present a machine learning pipeline to segment
white blood cell (lymphocyte) pixels in hyperspectral images of biopsy cores.
These cells are clinically important for diagnosis, but some prior work has
struggled to incorporate them due to difficulty obtaining precise pixel labels.
Evaluated methods include Support Vector Machine (SVM), Gaussian Naive Bayes,
and Multilayer Perceptron (MLP), as well as analyzing the comparatively modern
convolutional neural network (CNN).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Multimodal Information Fusion for Facial Expression Analysis. (arXiv:2203.12367v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12367">
<div class="article-summary-box-inner">
<span><p>Human affective behavior analysis has received much attention in
human-computer interaction (HCI). In this paper, we introduce our submission to
the CVPR 2022 Competition on Affective Behavior Analysis in-the-wild (ABAW). To
fully exploit affective knowledge from multiple views, we utilize the
multimodal features of spoken words, speech prosody, and facial expression,
which are extracted from the video clips in the Aff-Wild2 dataset. Based on
these features, we propose a unified transformer-based multimodal framework for
Action Unit detection and also expression recognition. Specifically, the static
vision feature is first encoded from the current frame image. At the same time,
we clip its adjacent frames by a sliding window and extract three kinds of
multimodal features from the sequence of images, audio, and text. Then, we
introduce a transformer-based fusion module that integrates the static vision
features and the dynamic multimodal features. The cross-attention module in the
fusion module makes the output integrated features focus on the crucial parts
that facilitate the downstream detection tasks. We also leverage some data
balancing techniques, data augmentation techniques, and postprocessing methods
to further improve the model performance. In the official test of ABAW3
Competition, our model ranks first in the EXPR and AU tracks. The extensive
quantitative evaluations, as well as ablation studies on the Aff-Wild2 dataset,
prove the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2203.13254v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13254">
<div class="article-summary-box-inner">
<span><p>Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is
a long-standing problem in computer vision. Driven by end-to-end deep learning,
recent studies suggest interpreting PnP as a differentiable layer, so that
2D-3D point correspondences can be partly learned by backpropagating the
gradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D
points from scratch fails to converge with existing approaches, since the
deterministic pose is inherently non-differentiable. In this paper, we propose
the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,
which outputs a distribution of pose on the SE(3) manifold, essentially
bringing categorical Softmax to the continuous domain. The 2D-3D coordinates
and corresponding weights are treated as intermediate variables learned by
minimizing the KL divergence between the predicted and target pose
distribution. The underlying principle unifies the existing approaches and
resembles the attention mechanism. EPro-PnP significantly outperforms
competitive baselines, closing the gap between PnP-based method and the
task-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D
object detection benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FocalClick: Towards Practical Interactive Image Segmentation. (arXiv:2204.02574v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02574">
<div class="article-summary-box-inner">
<span><p>Interactive segmentation allows users to extract target masks by making
positive/negative clicks. Although explored by many previous works, there is
still a gap between academic approaches and industrial needs: first, existing
models are not efficient enough to work on low power devices; second, they
perform poorly when used to refine preexisting masks as they could not avoid
destroying the correct part. FocalClick solves both issues at once by
predicting and updating the mask in localized areas. For higher efficiency, we
decompose the slow prediction on the entire image into two fast inferences on
small crops: a coarse segmentation on the Target Crop, and a local refinement
on the Focus Crop. To make the model work with preexisting masks, we formulate
a sub-task termed Interactive Mask Correction, and propose Progressive Merge as
the solution. Progressive Merge exploits morphological information to decide
where to preserve and where to update, enabling users to refine any preexisting
mask effectively. FocalClick achieves competitive results against SOTA methods
with significantly smaller FLOPs. It also shows significant superiority when
making corrections on preexisting masks. Code and data will be released at
github.com/XavierCHEN34/ClickSEG
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks. (arXiv:2204.02824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02824">
<div class="article-summary-box-inner">
<span><p>Face inpainting aims to complete the corrupted regions of the face images,
which requires coordination between the completed areas and the non-corrupted
areas. Recently, memory-oriented methods illustrate great prospects in the
generation related tasks by introducing an external memory module to improve
image coordination. However, such methods still have limitations in restoring
the consistency and continuity for specificfacial semantic parts. In this
paper, we propose the coarse-to-fine Memory-Disentangled Refinement Networks
(MDRNets) for coordinated face inpainting, in which two collaborative modules
are integrated, Disentangled Memory Module (DMM) and Mask-Region Enhanced
Module (MREM). Specifically, the DMM establishes a group of disentangled memory
blocks to store the semantic-decoupled face representations, which could
provide the most relevant information to refine the semantic-level
coordination. The MREM involves a masked correlation mining mechanism to
enhance the feature relationships into the corrupted regions, which could also
make up for the correlation loss caused by memory disentanglement. Furthermore,
to better improve the inter-coordination between the corrupted and
non-corrupted regions and enhance the intra-coordination in corrupted regions,
we design InCo2 Loss, a pair of similarity based losses to constrain the
feature consistency. Eventually, extensive experiments conducted on CelebA-HQ
and FFHQ datasets demonstrate the superiority of our MDRNets compared with
previous State-Of-The-Art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Dose CT Denoising via Sinogram Inner-Structure Transformer. (arXiv:2204.03163v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03163">
<div class="article-summary-box-inner">
<span><p>Low-Dose Computed Tomography (LDCT) technique, which reduces the radiation
harm to human bodies, is now attracting increasing interest in the medical
imaging field. As the image quality is degraded by low dose radiation, LDCT
exams require specialized reconstruction methods or denoising algorithms.
However, most of the recent effective methods overlook the inner-structure of
the original projection data (sinogram) which limits their denoising ability.
The inner-structure of the sinogram represents special characteristics of the
data in the sinogram domain. By maintaining this structure while denoising, the
noise can be obviously restrained. Therefore, we propose an LDCT denoising
network namely Sinogram Inner-Structure Transformer (SIST) to reduce the noise
by utilizing the inner-structure in the sinogram domain. Specifically, we study
the CT imaging mechanism and statistical characteristics of sinogram to design
the sinogram inner-structure loss including the global and local
inner-structure for restoring high-quality CT images. Besides, we propose a
sinogram transformer module to better extract sinogram features. The
transformer architecture using a self-attention mechanism can exploit
interrelations between projections of different view angles, which achieves an
outstanding performance in sinogram denoising. Furthermore, in order to improve
the performance in the image domain, we propose the image reconstruction module
to complementarily denoise both in the sinogram and image domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Transformer. A sparse-aware solution for efficient event data processing. (arXiv:2204.03355v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03355">
<div class="article-summary-box-inner">
<span><p>Event cameras are sensors of great interest for many applications that run in
low-resource and challenging environments. They log sparse illumination changes
with high temporal resolution and high dynamic range, while they present
minimal power consumption. However, top-performing methods often ignore
specific event-data properties, leading to the development of generic but
computationally expensive algorithms. Efforts toward efficient solutions
usually do not achieve top-accuracy results for complex tasks. This work
proposes a novel framework, Event Transformer (EvT), that effectively takes
advantage of event-data properties to be highly efficient and accurate. We
introduce a new patch-based event representation and a compact transformer-like
architecture to process it. EvT is evaluated on different event-based
benchmarks for action and gesture recognition. Evaluation results show better
or comparable accuracy to the state-of-the-art while requiring significantly
less computation resources, which makes EvT able to work with minimal latency
both on GPU and CPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Trajectory-Aware Transformer for Video Super-Resolution. (arXiv:2204.04216v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04216">
<div class="article-summary-box-inner">
<span><p>Video super-resolution (VSR) aims to restore a sequence of high-resolution
(HR) frames from their low-resolution (LR) counterparts. Although some progress
has been made, there are grand challenges to effectively utilize temporal
dependency in entire video sequences. Existing approaches usually align and
aggregate video frames from limited adjacent frames (e.g., 5 or 7 frames),
which prevents these approaches from satisfactory results. In this paper, we
take one step further to enable effective spatio-temporal learning in videos.
We propose a novel Trajectory-aware Transformer for Video Super-Resolution
(TTVSR). In particular, we formulate video frames into several pre-aligned
trajectories which consist of continuous visual tokens. For a query token,
self-attention is only learned on relevant visual tokens along spatio-temporal
trajectories. Compared with vanilla vision Transformers, such a design
significantly reduces the computational cost and enables Transformers to model
long-range features. We further propose a cross-scale feature tokenization
module to overcome scale-changing problems that often occur in long-range
videos. Experimental results demonstrate the superiority of the proposed TTVSR
over state-of-the-art models, by extensive quantitative and qualitative
evaluations in four widely-used video super-resolution benchmarks. Both code
and pre-trained models can be downloaded at
https://github.com/researchmm/TTVSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OutfitTransformer: Learning Outfit Representations for Fashion Recommendation. (arXiv:2204.04812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04812">
<div class="article-summary-box-inner">
<span><p>Learning an effective outfit-level representation is critical for predicting
the compatibility of items in an outfit, and retrieving complementary items for
a partial outfit. We present a framework, OutfitTransformer, that uses the
proposed task-specific tokens and leverages the self-attention mechanism to
learn effective outfit-level representations encoding the compatibility
relationships between all items in the entire outfit for addressing both
compatibility prediction and complementary item retrieval tasks. For
compatibility prediction, we design an outfit token to capture a global outfit
representation and train the framework using a classification loss. For
complementary item retrieval, we design a target item token that additionally
takes the target item specification (in the form of a category or text
description) into consideration. We train our framework using a proposed
set-wise outfit ranking loss to generate a target item embedding given an
outfit, and a target item specification as inputs. The generated target item
embedding is then used to retrieve compatible items that match the rest of the
outfit. Additionally, we adopt a pre-training approach and a curriculum
learning strategy to improve retrieval performance. Since our framework learns
at an outfit-level, it allows us to learn a single embedding capturing
higher-order relations among multiple items in the outfit more effectively than
pairwise methods. Experiments demonstrate that our approach outperforms
state-of-the-art methods on compatibility prediction, fill-in-the-blank, and
complementary item retrieval tasks. We further validate the quality of our
retrieval results with a user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Machine Learning Model Evaluation in Pathology. (arXiv:2204.05205v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05205">
<div class="article-summary-box-inner">
<span><p>Machine Learning has been applied to pathology images in research and
clinical practice with promising outcomes. However, standard ML models often
lack the rigorous evaluation required for clinical decisions. Machine learning
techniques for natural images are ill-equipped to deal with pathology images
that are significantly large and noisy, require expensive labeling, are hard to
interpret, and are susceptible to spurious correlations. We propose a set of
practical guidelines for ML evaluation in pathology that address the above
concerns. The paper includes measures for setting up the evaluation framework,
effectively dealing with variability in labels, and a recommended suite of
tests to address issues related to domain shift, robustness, and confounding
variables. We hope that the proposed framework will bridge the gap between ML
researchers and domain experts, leading to wider adoption of ML techniques in
pathology and improving patient outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">medXGAN: Visual Explanations for Medical Classifiers through a Generative Latent Space. (arXiv:2204.05376v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05376">
<div class="article-summary-box-inner">
<span><p>Despite the surge of deep learning in the past decade, some users are
skeptical to deploy these models in practice due to their black-box nature.
Specifically, in the medical space where there are severe potential
repercussions, we need to develop methods to gain confidence in the models'
decisions. To this end, we propose a novel medical imaging generative
adversarial framework, medXGAN (medical eXplanation GAN), to visually explain
what a medical classifier focuses on in its binary predictions. By encoding
domain knowledge of medical images, we are able to disentangle anatomical
structure and pathology, leading to fine-grained visualization through latent
interpolation. Furthermore, we optimize the latent space such that
interpolation explains how the features contribute to the classifier's output.
Our method outperforms baselines such as Gradient-Weighted Class Activation
Mapping (Grad-CAM) and Integrated Gradients in localization and explanatory
ability. Additionally, a combination of the medXGAN with Integrated Gradients
can yield explanations more robust to noise. The code is available at:
https://avdravid.github.io/medXGAN_page/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpoofGAN: Synthetic Fingerprint Spoof Images. (arXiv:2204.06498v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06498">
<div class="article-summary-box-inner">
<span><p>A major limitation to advances in fingerprint spoof detection is the lack of
publicly available, large-scale fingerprint spoof datasets, a problem which has
been compounded by increased concerns surrounding privacy and security of
biometric data. Furthermore, most state-of-the-art spoof detection algorithms
rely on deep networks which perform best in the presence of a large amount of
training data. This work aims to demonstrate the utility of synthetic (both
live and spoof) fingerprints in supplying these algorithms with sufficient data
to improve the performance of fingerprint spoof detection algorithms beyond the
capabilities when training on a limited amount of publicly available real
datasets. First, we provide details of our approach in modifying a
state-of-the-art generative architecture to synthesize high quality live and
spoof fingerprints. Then, we provide quantitative and qualitative analysis to
verify the quality of our synthetic fingerprints in mimicking the distribution
of real data samples. We showcase the utility of our synthetic live and spoof
fingerprints in training a deep network for fingerprint spoof detection, which
dramatically boosts the performance across three different evaluation datasets
compared to an identical model trained on real data alone. Finally, we
demonstrate that only 25% of the original (real) dataset is required to obtain
similar detection performance when augmenting the training dataset with
synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07439">
<div class="article-summary-box-inner">
<span><p>Binary Neural Networks (BNNs) have emerged as a promising solution for
reducing the memory footprint and compute costs of deep neural networks. BNNs,
on the other hand, suffer from information loss because binary activations are
limited to only two values, resulting in reduced accuracy. To improve the
accuracy, previous studies have attempted to control the distribution of binary
activation by manually shifting the threshold of the activation function or
making the shift amount trainable. During the process, they usually depended on
statistical information computed from a batch. We argue that using statistical
data from a batch fails to capture the crucial information for each input
instance in BNN computations, and the differences between statistical
information computed from each instance need to be considered when determining
the binary activation threshold of each instance. Based on the concept, we
propose the Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN),
which decides the activation threshold value considering the difference between
statistical data computed from a batch and each instance. The proposed
INSTA-BNN outperforms the baseline by 2.5% and 2.3% on the ImageNet
classification task with comparable computing cost, achieving 68.0% and 71.7%
top-1 accuracy on ResNet-18 and MobileNetV1 based models, respectively.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-19 23:07:33.821312847 UTC">2022-04-19 23:07:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>