<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-09T01:30:00Z">03-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperMixer: An MLP-based Green AI Alternative to Transformers. (arXiv:2203.03691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03691">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures are the model of choice for natural language
understanding, but they come at a significant cost, as they have quadratic
complexity in the input length and can be difficult to tune. In the pursuit of
Green AI, we investigate simple MLP-based architectures. We find that existing
architectures such as MLPMixer, which achieves token mixing through a static
MLP applied to each feature independently, are too detached from the inductive
biases required for natural language understanding. In this paper, we propose a
simple variant, HyperMixer, which forms the token mixing MLP dynamically using
hypernetworks. Empirically, we demonstrate that our model performs better than
alternative MLP-based models, and on par with Transformers. In contrast to
Transformers, HyperMixer achieves these results at substantially lower costs in
terms of processing time, training data, and hyperparameter tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IT5: Large-scale Text-to-text Pretraining for Italian Language Understanding and Generation. (arXiv:2203.03759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03759">
<div class="article-summary-box-inner">
<span><p>The T5 model and its unified text-to-text paradigm contributed in advancing
the state-of-the-art for many natural language processing tasks. While some
multilingual variants of the T5 model have recently been introduced, their
performances were found to provide suboptimal performances for languages other
than English if compared to monolingual variants. We are motivated by these
findings to introduce IT5, the first family of encoder-decoder transformer
models pretrained specifically on Italian. We perform a thorough cleaning of a
web-crawled Italian corpus including more than 40 billion words and use it to
pretrain three IT5 models of different sizes. The performance of IT5 models and
their multilingual counterparts is then evaluated on a broad range of natural
language understanding and generation benchmarks for Italian. We find the
monolingual IT5 models to provide the best scale-to-performance ratio across
tested models, consistently outperforming their multilingual counterparts and
setting a new state-of-the-art for most Italian conditional language generation
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Preserving Linguistic Steganography by Pivot Translation and Semantic-Aware Bins Coding. (arXiv:2203.03795v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03795">
<div class="article-summary-box-inner">
<span><p>Linguistic steganography (LS) aims to embed secret information into a highly
encoded text for covert communication. It can be roughly divided to two main
categories, i.e., modification based LS (MLS) and generation based LS (GLS).
Unlike MLS that hides secret data by slightly modifying a given text without
impairing the meaning of the text, GLS uses a trained language model to
directly generate a text carrying secret data. A common disadvantage for MLS
methods is that the embedding payload is very low, whose return is well
preserving the semantic quality of the text. In contrast, GLS allows the data
hider to embed a high payload, which has to pay the high price of
uncontrollable semantics. In this paper, we propose a novel LS method to modify
a given text by pivoting it between two different languages and embed secret
data by applying a GLS-like information encoding strategy. Our purpose is to
alter the expression of the given text, enabling a high payload to be embedded
while keeping the semantic information unchanged. Experimental results have
shown that the proposed work not only achieves a high embedding payload, but
also shows superior performance in maintaining the semantic consistency and
resisting linguistic steganalysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Iterative Revision from Human-Written Text. (arXiv:2203.03802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03802">
<div class="article-summary-box-inner">
<span><p>Writing is, by nature, a strategic, adaptive, and more importantly, an
iterative process. A crucial part of writing is editing and revising the text.
Previous works on text revision have focused on defining edit intention
taxonomies within a single domain or developing computational models with a
single level of edit granularity, such as sentence-level edits, which differ
from human's revision cycles. This work describes IteraTeR: the first
large-scale, multi-domain, edit-intention annotated corpus of iteratively
revised text. In particular, IteraTeR is collected based on a new framework to
comprehensively model the iterative text revisions that generalize to various
domains of formal writing, edit intentions, revision depths, and granularities.
When we incorporate our annotated edit intentions, both generative and
edit-based text revision models significantly improve automatic evaluations.
Through our work, we better understand the text revision process, making vital
connections between edit intentions and writing quality, enabling the creation
of diverse corpora to support computational modeling of iterative text
revisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Search with Text Feedback by Additive Attention Compositional Learning. (arXiv:2203.03809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03809">
<div class="article-summary-box-inner">
<span><p>Effective image retrieval with text feedback stands to impact a range of
real-world applications, such as e-commerce. Given a source image and text
feedback that describes the desired modifications to that image, the goal is to
retrieve the target images that resemble the source yet satisfy the given
modifications by composing a multi-modal (image-text) query. We propose a novel
solution to this problem, Additive Attention Compositional Learning (AACL),
that uses a multi-modal transformer-based architecture and effectively models
the image-text contexts. Specifically, we propose a novel image-text
composition module based on additive attention that can be seamlessly plugged
into deep neural networks. We also introduce a new challenging benchmark
derived from the Shopping100k dataset. AACL is evaluated on three large-scale
datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong
baselines. Extensive experiments show that AACL achieves new state-of-the-art
results on all three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Variational Hierarchical Model for Neural Cross-Lingual Summarization. (arXiv:2203.03820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03820">
<div class="article-summary-box-inner">
<span><p>The goal of the cross-lingual summarization (CLS) is to convert a document in
one language (e.g., English) to a summary in another one (e.g., Chinese).
Essentially, the CLS task is the combination of machine translation (MT) and
monolingual summarization (MS), and thus there exists the hierarchical
relationship between MT\&amp;MS and CLS. Existing studies on CLS mainly focus on
utilizing pipeline methods or jointly training an end-to-end model through an
auxiliary MT or MS objective. However, it is very challenging for the model to
directly conduct CLS as it requires both the abilities to translate and
summarize. To address this issue, we propose a hierarchical model for the CLS
task, based on the conditional variational auto-encoder. The hierarchical model
contains two kinds of latent variables at the local and global levels,
respectively. At the local level, there are two latent variables, one for
translation and the other for summarization. As for the global level, there is
another latent variable for cross-lingual summarization conditioned on the two
local-level variables. Experiments on two language directions (English-Chinese)
verify the effectiveness and superiority of the proposed approach. In addition,
we show that our model is able to generate better cross-lingual summaries than
comparison models in the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Framework of Medical Information Annotation and Extraction for Chinese Clinical Text. (arXiv:2203.03823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03823">
<div class="article-summary-box-inner">
<span><p>Medical information extraction consists of a group of natural language
processing (NLP) tasks, which collaboratively convert clinical text to
pre-defined structured formats. Current state-of-the-art (SOTA) NLP models are
highly integrated with deep learning techniques and thus require massive
annotated linguistic data. This study presents an engineering framework of
medical entity recognition, relation extraction and attribute extraction, which
are unified in annotation, modeling and evaluation. Specifically, the
annotation scheme is comprehensive, and compatible between tasks, especially
for the medical relations. The resulted annotated corpus includes 1,200 full
medical records (or 18,039 broken-down documents), and achieves inter-annotator
agreements (IAAs) of 94.53%, 73.73% and 91.98% F 1 scores for the three tasks.
Three task-specific neural network models are developed within a shared
structure, and enhanced by SOTA NLP techniques, i.e., pre-trained language
models. Experimental results show that the system can retrieve medical
entities, relations and attributes with F 1 scores of 93.47%, 67.14% and
90.89%, respectively. This study, in addition to our publicly released
annotation scheme and code, provides solid and practical engineering experience
of developing an integrated medical information extraction system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification. (arXiv:2203.03825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03825">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification is a challenging subtask of multi-label
classification due to its complex label hierarchy. Existing methods encode text
and label hierarchy separately and mix their representations for
classification, where the hierarchy remains unchanged for all input text.
Instead of modeling them separately, in this work, we propose Hierarchy-guided
Contrastive Learning (HGCLR) to directly embed the hierarchy into a text
encoder. During training, HGCLR constructs positive samples for input text
under the guidance of the label hierarchy. By pulling together the input text
and its positive sample, the text encoder can learn to generate the
hierarchy-aware text representation independently. Therefore, after training,
the HGCLR enhanced text encoder can dispense with the redundant hierarchy.
Extensive experiments on three benchmark datasets verify the effectiveness of
HGCLR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Building an Open-Domain Dialogue System Incorporated with Internet Memes. (arXiv:2203.03835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03835">
<div class="article-summary-box-inner">
<span><p>In recent years, Internet memes have been widely used in online chatting.
Compared with text-based communication, conversations become more expressive
and attractive when Internet memes are incorporated. This paper presents our
solutions for the Meme incorporated Open-domain Dialogue (MOD) Challenge of
DSTC10, where three tasks are involved: text response modeling, meme retrieval,
and meme emotion classification. Firstly, we leverage a large-scale pre-trained
dialogue model for coherent and informative response generation. Secondly,
based on interaction-based text-matching, our approach can retrieve appropriate
memes with good generalization ability. Thirdly, we propose to model the
emotion flow (EF) in conversations and introduce an auxiliary task of emotion
description prediction (EDP) to boost the performance of meme emotion
classification. Experimental results on the MOD dataset demonstrate that our
methods can incorporate Internet memes into dialogue systems effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniXcoder: Unified Cross-Modal Pre-training for Code Representation. (arXiv:2203.03850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03850">
<div class="article-summary-box-inner">
<span><p>Pre-trained models for programming languages have recently demonstrated great
success on code intelligence. To support both code-related understanding and
generation tasks, recent works attempt to pre-train unified encoder-decoder
models. However, such encoder-decoder framework is sub-optimal for
auto-regressive tasks, especially code completion that requires a decoder-only
manner for efficient inference. In this paper, we present UniXcoder, a unified
cross-modal pre-trained model for programming language. The model utilizes mask
attention matrices with prefix adapters to control the behavior of the model
and leverages cross-modal contents like AST and code comment to enhance code
representation. To encode AST that is represented as a tree in parallel, we
propose a one-to-one mapping method to transform AST in a sequence structure
that retains all structural information from the tree. Furthermore, we propose
to utilize multi-modal contents to learn representation of code fragment with
contrastive learning, and then align representations among programming
languages using a cross-modal generation task. We evaluate UniXcoder on five
code-related tasks over nine datasets. To further evaluate the performance of
code fragment representation, we also construct a dataset for a new task,
called zero-shot code-to-code search. Results show that our model achieves
state-of-the-art performance on most tasks and analysis reveals that comment
and AST can both enhance UniXcoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Does the Performance Improvement Come From? - A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03853">
<div class="article-summary-box-inner">
<span><p>This paper seeks to provide the information retrieval community with some
reflections on the current improvements of retrieval learning through the
analysis of the reproducibility aspects of image-text retrieval models. For the
latter part of the past decade, image-text retrieval has gradually become a
major research direction in the field of information retrieval because of the
growth of multi-modal data. Many researchers use benchmark datasets like
MS-COCO and Flickr30k to train and assess the performance of image-text
retrieval algorithms. Research in the past has mostly focused on performance,
with several state-of-the-art methods being proposed in various ways. According
to their claims, these approaches achieve better modal interactions and thus
better multimodal representations with greater precision. In contrast to those
previous works, we focus on the repeatability of the approaches and the overall
examination of the elements that lead to improved performance by pretrained and
nonpretrained models in retrieving images and text. To be more specific, we
first examine the related reproducibility concerns and why the focus is on
image-text retrieval tasks, and then we systematically summarize the current
paradigm of image-text retrieval models and the stated contributions of those
approaches. Second, we analyze various aspects of the reproduction of
pretrained and nonpretrained retrieval models. Based on this, we conducted
ablation experiments and obtained some influencing factors that affect
retrieval recall more than the improvement claimed in the original paper.
Finally, we also present some reflections and issues that should be considered
by the retrieval community in the future. Our code is freely available at
https://github.com/WangFei-2019/Image-text-Retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2203.03856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03856">
<div class="article-summary-box-inner">
<span><p>The task of joint dialog sentiment classification (DSC) and act recognition
(DAR) aims to simultaneously predict the sentiment label and act label for each
utterance in a dialog. In this paper, we put forward a new framework which
models the explicit dependencies via integrating \textit{prediction-level
interactions} other than semantics-level interactions, more consistent with
human intuition. Besides, we propose a speaker-aware temporal graph (SATG) and
a dual-task relational temporal graph (DRTG) to introduce \textit{temporal
relations} into dialog understanding and dual-task reasoning. To implement our
framework, we propose a novel model dubbed DARER, which first generates the
context-, speaker- and temporal-sensitive utterance representations via
modeling SATG, then conducts recurrent dual-task relational reasoning on DRTG,
in which process the estimated label distributions act as key clues in
prediction-level interactions. Experiment results show that DARER outperforms
existing models by large margins while requiring much less computation resource
and costing less training time. Remarkably, on DSC task in Mastodon, DARER
gains a relative improvement of about 25% over previous best model in terms of
F1, with less than 50% parameters and about only 60% required GPU memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks. (arXiv:2203.03878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03878">
<div class="article-summary-box-inner">
<span><p>The workflow of pretraining and fine-tuning has emerged as a popular paradigm
for solving various NLP and V&amp;L (Vision-and-Language) downstream tasks. With
the capacity of pretrained models growing rapidly, how to perform
parameter-efficient fine-tuning has become fairly important for quick transfer
learning and deployment. In this paper, we design a novel unified
parameter-efficient transfer learning framework that works effectively on both
pure language and V&amp;L tasks. In particular, we use a shared hypernetwork that
takes trainable hyper-embeddings as input, and outputs weights for fine-tuning
different small modules in a pretrained language model, such as tuning the
parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and
feed-forward blocks (i.e., adapter-tuning). We define a set of embeddings
(e.g., layer, block, task and visual embeddings) as the key components to
calculate hyper-embeddings, which thus can support both pure language and V&amp;L
tasks. Our proposed framework adds fewer trainable parameters in multi-task
learning while achieving superior performances and transfer ability compared to
state-of-the-art methods. Empirical results on the GLUE benchmark and multiple
V&amp;L tasks confirm the effectiveness of our framework on both textual and visual
modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Mixup for Robust Fine-tuning. (arXiv:2203.03897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03897">
<div class="article-summary-box-inner">
<span><p>Pre-trained large-scale models provide a transferable embedding, and they
show comparable performance on the diverse downstream task. However, the
transferability of multi-modal learning is restricted, and the analysis of
learned embedding has not been explored well. This paper provides a perspective
to understand the multi-modal embedding in terms of uniformity and alignment.
We newly find that the representation learned by multi-modal learning models
such as CLIP has a two separated representation space for each heterogeneous
dataset with less alignment. Besides, there are unexplored large intermediate
areas between two modalities with less uniformity. Less robust embedding might
restrict the transferability of the representation for the downstream task.
This paper provides a new end-to-end fine-tuning method for robust
representation that encourages better uniformity and alignment score. First, we
propose a multi-modal Mixup, $m^{2}$-Mix that mixes the representation of image
and text to generate the hard negative samples. Second, we fine-tune the
multi-modal model on a hard negative sample as well as normal negative and
positive samples with contrastive learning. Our multi-modal Mixup provides a
robust representation, and we validate our methods on classification,
retrieval, and structure-awareness task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InstructionNER: A Multi-Task Instruction-Based Generative Framework for Few-shot NER. (arXiv:2203.03903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03903">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-based methods have achieved significant performance in
few-shot learning scenarios by bridging the gap between language model
pre-training and fine-tuning for downstream tasks. However, existing prompt
templates are mostly designed for sentence-level tasks and are inappropriate
for sequence labeling objectives. To address the above issue, we propose a
multi-task instruction-based generative framework, named InstructionNER, for
low-resource named entity recognition. Specifically, we reformulate the NER
task as a generation problem, which enriches source sentences with
task-specific instructions and answer options, then inferences the entities and
types in natural language. We further propose two auxiliary tasks, including
entity extraction and entity typing, which enable the model to capture more
boundary information of entities and deepen the understanding of entity type
semantics, respectively. Experimental results show that our method consistently
outperforms other baselines on five datasets in few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation. (arXiv:2203.03910v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03910">
<div class="article-summary-box-inner">
<span><p>Neural networks tend to gradually forget the previously learned knowledge
when learning multiple tasks sequentially from dynamic data distributions. This
problem is called \textit{catastrophic forgetting}, which is a fundamental
challenge in the continual learning of neural networks. In this work, we
observe that catastrophic forgetting not only occurs in continual learning but
also affects the traditional static training. Neural networks, especially
neural machine translation models, suffer from catastrophic forgetting even if
they learn from a static training set. To be specific, the final model pays
imbalanced attention to training samples, where recently exposed samples
attract more attention than earlier samples. The underlying cause is that
training samples do not get balanced training in each model update, so we name
this problem \textit{imbalanced training}. To alleviate this problem, we
propose Complementary Online Knowledge Distillation (COKD), which uses
dynamically updated teacher models trained on specific data orders to
iteratively provide complementary knowledge to the student model. Experimental
results on multiple machine translation tasks show that our method successfully
alleviates the problem of imbalanced training and achieves substantial
improvements over strong baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapt$\mathcal{O}$r: Objective-Centric Adaptation Framework for Language Models. (arXiv:2203.03989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03989">
<div class="article-summary-box-inner">
<span><p>Progress in natural language processing research is catalyzed by the
possibilities given by the widespread software frameworks. This paper
introduces Adaptor library that transposes the traditional model-centric
approach composed of pre-training + fine-tuning steps to objective-centric
approach, composing the training process by applications of selected
objectives. We survey research directions that can benefit from enhanced
objective-centric experimentation in multitask training, custom objectives
development, dynamic training curricula, or domain adaptation. Adaptor aims to
ease reproducibility of these research directions in practice. Finally, we
demonstrate the practical applicability of Adaptor in selected unsupervised
domain adaptation scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration. (arXiv:2203.04006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04006">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation (VLN) is a challenging task due to its large
searching space in the environment. To address this problem, previous works
have proposed some methods of fine-tuning a large model that pretrained on
large-scale datasets. However, the conventional fine-tuning methods require
extra human-labeled navigation data and lack self-exploration capabilities in
environments, which hinders their generalization of unseen scenes. To improve
the ability of fast cross-domain adaptation, we propose Prompt-based
Environmental Self-exploration (ProbES), which can self-explore the
environments by sampling trajectories and automatically generates structured
instructions via a large-scale cross-modal pretrained model (CLIP). Our method
fully utilizes the knowledge learned from CLIP to build an in-domain dataset by
self-exploration without human labeling. Unlike the conventional approach of
fine-tuning, we introduce prompt-based learning to achieve fast adaptation for
language embeddings, which substantially improves the learning efficiency by
leveraging prior knowledge. By automatically synthesizing
trajectory-instruction pairs in any environment without human supervision and
efficient prompt-based learning, our model can adapt to diverse vision-language
navigation tasks, including VLN and REVERIE. Both qualitative and quantitative
results show that our ProbES significantly improves the generalization ability
of the navigation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Generalized Models for Task-oriented Dialogue Modeling on Spoken Conversations. (arXiv:2203.04045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04045">
<div class="article-summary-box-inner">
<span><p>Building robust and general dialogue models for spoken conversations is
challenging due to the gap in distributions of spoken and written data. This
paper presents our approach to build generalized models for the
Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations
Challenge of DSTC-10. In order to mitigate the discrepancies between spoken and
written text, we mainly employ extensive data augmentation strategies on
written data, including artificial error injection and round-trip text-speech
transformation. To train robust models for spoken conversations, we improve
pre-trained language models, and apply ensemble algorithms for each sub-task.
Typically, for the detection task, we fine-tune \roberta and ELECTRA, and run
an error-fixing ensemble algorithm. For the selection task, we adopt a
two-stage framework that consists of entity tracking and knowledge ranking, and
propose a multi-task learning method to learn multi-level semantic information
by domain classification and entity selection. For the generation task, we
adopt a cross-validation data process to improve pre-trained generative
language models, followed by a consensus decoding algorithm, which can add
arbitrary features like relative \rouge metric, and tune associated feature
weights toward \bleu directly. Our approach ranks third on the objective
evaluation and second on the final official human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Distillation Guided Salient Object Detection. (arXiv:2203.04076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04076">
<div class="article-summary-box-inner">
<span><p>Most existing CNN-based salient object detection methods can identify local
segmentation details like hair and animal fur, but often misinterpret the real
saliency due to the lack of global contextual information caused by the
subjectiveness of the SOD task and the locality of convolution layers.
Moreover, due to the unrealistically expensive labeling costs, the current
existing SOD datasets are insufficient to cover the real data distribution. The
limitation and bias of the training data add additional difficulty to fully
exploring the semantic association between object-to-object and
object-to-environment in a given image. In this paper, we propose a semantic
distillation guided SOD (SDG-SOD) method that produces accurate results by
fusing semantically distilled knowledge from generated image captioning into
the Vision-Transformer-based SOD framework. SDG-SOD can better uncover
inter-objects and object-to-environment saliency and cover the gap between the
subjective nature of SOD and its expensive labeling. Comprehensive experiments
on five benchmark datasets demonstrate that the SDG-SOD outperforms the
state-of-the-art approaches on four evaluation metrics, and largely improves
the model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plumeria at SemEval-2022 Task 6: Robust Approaches for Sarcasm Detection for English and Arabic Using Transformers and Data Augmentation. (arXiv:2203.04111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04111">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to SemEval-2022 Task 6 on sarcasm
detection and its five subtasks for English and Arabic. Sarcasm conveys a
meaning which contradicts the literal meaning, and it is mainly found on social
networks. It has a significant role in understanding the intention of the user.
For detecting sarcasm, we used deep learning techniques based on transformers
due to its success in the field of Natural Language Processing (NLP) without
the need for feature engineering. The datasets were taken from tweets. We
created new datasets by augmenting with external data or by using word
embeddings and repetition of instances. Experiments were done on the datasets
with different types of preprocessing because it is crucial in this task. The
rank of our team was consistent across four subtasks (fourth rank in three
subtasks and sixth rank in one subtask); whereas other teams might be in the
top ranks for some subtasks but rank drastically less in other subtasks. This
implies the robustness and stability of the models and the techniques we used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Mixing of Contextual Information in the Transformer. (arXiv:2203.04212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04212">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture aggregates input information through the
self-attention mechanism, but there is no clear understanding of how this
information is mixed across the entire model. Additionally, recent works have
demonstrated that attention weights alone are not enough to describe the flow
of information. In this paper, we consider the whole attention block
--multi-head attention, residual connection, and layer normalization-- and
define a metric to measure token-to-token interactions within each layer,
considering the characteristics of the representation space. Then, we aggregate
layer-wise interpretations to provide input attribution scores for model
predictions. Experimentally, we show that our method, ALTI (Aggregation of
Layer-wise Token-to-token Interactions), provides faithful explanations and
outperforms similar aggregation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Bidirectional Translation between Descriptions and Actions with Small Paired Data. (arXiv:2203.04218v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04218">
<div class="article-summary-box-inner">
<span><p>This study achieved bidirectional translation between descriptions and
actions using small paired data. The ability to mutually generate descriptions
and actions is essential for robots to collaborate with humans in their daily
lives. The robot is required to associate real-world objects with linguistic
expressions, and large-scale paired data are required for machine learning
approaches. However, a paired dataset is expensive to construct and difficult
to collect. This study proposes a two-stage training method for bidirectional
translation. In the proposed method, we train recurrent autoencoders (RAEs) for
descriptions and actions with a large amount of non-paired data. Then, we
fine-tune the entire model to bind their intermediate representations using
small paired data. Because the data used for pre-training do not require
pairing, behavior-only data or a large language corpus can be used. We
experimentally evaluated our method using a paired dataset consisting of
motion-captured actions and descriptions. The results showed that our method
performed well, even when the amount of paired data to train was small. The
visualization of the intermediate representations of each RAE showed that
similar actions were encoded in a clustered position and the corresponding
feature vectors well aligned.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-agnostic BERT Sentence Embedding. (arXiv:2007.01852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01852">
<div class="article-summary-box-inner">
<span><p>While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning
(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have
yet to be explored. We systematically investigate methods for learning
multilingual sentence embeddings by combining the best methods for learning
monolingual and cross-lingual representations including: masked language
modeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),
dual encoder translation ranking (Guo et al., 2018), and additive margin
softmax (Yang et al., 2019a). We show that introducing a pre-trained
multilingual language model dramatically reduces the amount of parallel
training data required to achieve good performance by 80%. Composing the best
of these methods produces a model that achieves 83.7% bi-text retrieval
accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by
Artetxe and Schwenk (2019b), while still performing competitively on
monolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuatRE: Relation-Aware Quaternions for Knowledge Graph Embeddings. (arXiv:2009.12517v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.12517">
<div class="article-summary-box-inner">
<span><p>We propose a simple yet effective embedding model to learn quaternion
embeddings for entities and relations in knowledge graphs. Our model aims to
enhance correlations between head and tail entities given a relation within the
Quaternion space with Hamilton product. The model achieves this goal by further
associating each relation with two relation-aware rotations, which are used to
rotate quaternion embeddings of the head and tail entities, respectively.
Experimental results show that our proposed model produces state-of-the-art
performances on well-known benchmark datasets for knowledge graph completion.
Our code is available at: \url{https://github.com/daiquocnguyen/QuatRE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Annotation of Chest Abdomen Pelvis Computed Tomography Text Reports Using Deep Learning. (arXiv:2102.02959v5 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02959">
<div class="article-summary-box-inner">
<span><p>Purpose: To develop high throughput multi-label annotators for body (chest,
abdomen, and pelvis) Computed Tomography (CT) reports that can be applied
across a variety of abnormalities, organs, and disease states.
</p>
<p>Approach: We used a dictionary approach to develop rule-based algorithms
(RBA) for extraction of disease labels from radiology text reports. We targeted
three organ systems (lungs/pleura, liver/gallbladder, kidneys/ureters) with
four diseases per system based on their prevalence in our dataset. To expand
the algorithms beyond pre-defined keywords, attention-guided recurrent neural
networks (RNN) were trained using the RBA-extracted labels to classify reports
as being positive for one or more diseases or normal for each organ system.
Confounding effects on model performance were evaluated using random
initialization or pre-trained embedding as well as different sizes of training
datasets. Performance was evaluated using the receiver operating characteristic
(ROC) area under the curve (AUC) against 2,158 manually obtained labels.
</p>
<p>Results: Our models extracted disease labels from 261,229 radiology reports
of 112,501 unique subjects. Pre-trained models outperformed random
initialization across all diseases. As the training dataset size was reduced,
performance was robust except for a few diseases with relatively small number
of cases. Pre-trained classification AUCs achieved &gt; 0.95 for all five disease
outcomes across all three organ systems.
</p>
<p>Conclusions: Our label-extracting pipeline was able to encompass a variety of
cases and diseases by generalizing beyond strict rules with exceptional
accuracy. This method can be easily adapted to enable automated labeling of
hospital-scale medical data sets for training image-based disease classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByT5: Towards a token-free future with pre-trained byte-to-byte models. (arXiv:2105.13626v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13626">
<div class="article-summary-box-inner">
<span><p>Most widely-used pre-trained language models operate on sequences of tokens
corresponding to word or subword units. By comparison, token-free models that
operate directly on raw text (bytes or characters) have many benefits: they can
process text in any language out of the box, they are more robust to noise, and
they minimize technical debt by removing complex and error-prone text
preprocessing pipelines. Since byte or character sequences are longer than
token sequences, past work on token-free models has often introduced new model
architectures designed to amortize the cost of operating directly on raw text.
In this paper, we show that a standard Transformer architecture can be used
with minimal modifications to process byte sequences. We characterize the
trade-offs in terms of parameter count, training FLOPs, and inference speed,
and show that byte-level models are competitive with their token-level
counterparts. We also demonstrate that byte-level models are significantly more
robust to noise and perform better on tasks that are sensitive to spelling and
pronunciation. As part of our contribution, we release a new set of pre-trained
byte-level Transformer models based on the T5 architecture, as well as all code
and data used in our experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Integrated Gradients and Constituency Parse Trees to explain Linguistic Acceptability learnt by BERT. (arXiv:2106.07349v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07349">
<div class="article-summary-box-inner">
<span><p>Linguistic Acceptability is the task of determining whether a sentence is
grammatical or ungrammatical. It has applications in several use cases like
Question-Answering, Natural Language Generation, Neural Machine Translation,
where grammatical correctness is crucial. In this paper we aim to understand
the decision-making process of BERT (Devlin et al., 2019) in distinguishing
between Linguistically Acceptable sentences (LA) and Linguistically
Unacceptable sentences (LUA). We leverage Layer Integrated Gradients
Attribution Scores (LIG) to explain the Linguistic Acceptability criteria that
are learnt by BERT on the Corpus of Linguistic Acceptability (CoLA) (Warstadt
et al., 2018) benchmark dataset. Our experiments on 5 categories of sentences
lead to the following interesting findings: 1) LIG for LA are significantly
smaller in comparison to LUA, 2) There are specific subtrees of the
Constituency Parse Tree (CPT) for LA and LUA which contribute larger LIG, 3)
Across the different categories of sentences we observed around 88% to 100% of
the Correctly classified sentences had positive LIG, indicating a strong
positive relationship to the prediction confidence of the model, and 4) Around
43% of the Misclassified sentences had negative LIG, which we believe can
become correctly classified sentences if the LIG are parameterized in the loss
function of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text. (arXiv:2107.01294v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01294">
<div class="article-summary-box-inner">
<span><p>Modern neural language models can produce remarkably fluent and grammatical
text. So much, in fact, that recent work by Clark et al. (2021) has reported
that conventional crowdsourcing can no longer reliably distinguish between
machine-authored (GPT-3) and human-authored writing. As errors in machine
generations become ever subtler and harder to spot, it poses a new challenge to
the research community for robust machine text evaluation. We propose a new
framework called Scarecrow for scrutinizing machine text via crowd annotation.
To support the broad range of real machine errors that can be identified by
laypeople, the ten error categories of Scarecrow -- such as redundancy,
commonsense errors, and incoherence -- are identified through several rounds of
crowd annotation experiments without a predefined ontology. We then use
Scarecrow to collect over 41k error spans in human-written and
machine-generated paragraphs of English language news text. We isolate factors
for detailed analysis, including parameter count, training data, and various
decoding-time configurations. Our approach successfully quantifies measurable
gaps between human authored text and generations from models of several sizes,
including fourteen configurations of GPT-3. In addition, our analysis unveils
new insights, with detailed rationales provided by laypeople, e.g., that the
commonsense capabilities have been improving with larger models while math
capabilities have not, and that the choices of simple decoding hyperparameters
can make remarkable differences on the perceived quality of machine text. We
release our training material, annotation toolkit and dataset at
https://yao-dou.github.io/scarecrow/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEAP-FAKED: Knowledge Graph based Approach for Fake News Detection. (arXiv:2107.10648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10648">
<div class="article-summary-box-inner">
<span><p>Fake News on social media platforms has attracted a lot of attention in
recent times, primarily for events related to politics (2016 US Presidential
elections), healthcare (infodemic during COVID-19), to name a few. Various
methods have been proposed for detecting Fake News. The approaches span from
exploiting techniques related to network analysis, Natural Language Processing
(NLP), and the usage of Graph Neural Networks (GNNs). In this work, we propose
DEAP-FAKED, a knowleDgE grAPh FAKe nEws Detection framework for identifying
Fake News. Our approach is a combination of the NLP -- where we encode the news
content, and the GNN technique -- where we encode the Knowledge Graph (KG). A
variety of these encodings provides a complementary advantage to our detector.
We evaluate our framework using two publicly available datasets containing
articles from domains such as politics, business, technology, and healthcare.
As part of dataset pre-processing, we also remove the bias, such as the source
of the articles, which could impact the performance of the models. DEAP-FAKED
obtains an F1-score of 88% and 78% for the two datasets, which is an
improvement of 21%, and 3% respectively, which shows the effectiveness of the
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSTL - From English Requirements to Signal Temporal Logic. (arXiv:2109.10294v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10294">
<div class="article-summary-box-inner">
<span><p>Formal methods provide very powerful tools and techniques for the design and
analysis of complex systems. Their practical application remains however
limited, due to the widely accepted belief that formal methods require
extensive expertise and a steep learning curve. Writing correct formal
specifications in form of logical formulas is still considered to be a
difficult and error prone task.
</p>
<p>In this paper we propose DeepSTL, a tool and technique for the translation of
informal requirements, given as free English sentences, into Signal Temporal
Logic (STL), a formal specification language for cyber-physical systems, used
both by academia and advanced research labs in industry. A major challenge to
devise such a translator is the lack of publicly available informal
requirements and formal specifications. We propose a two-step workflow to
address this challenge. We first design a grammar-based generation technique of
synthetic data, where each output is a random STL formula and its associated
set of possible English translations. In the second step, we use a
state-of-the-art transformer-based neural translation technique, to train an
accurate attentional translator of English to STL. The experimental results
show high translation quality for patterns of English requirements that have
been well trained, making this workflow promising to be extended for processing
more complex translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EntQA: Entity Linking as Question Answering. (arXiv:2110.02369v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02369">
<div class="article-summary-box-inner">
<span><p>A conventional approach to entity linking is to first find mentions in a
given document and then infer their underlying entities in the knowledge base.
A well-known limitation of this approach is that it requires finding mentions
without knowing their entities, which is unnatural and difficult. We present a
new model that does not suffer from this limitation called EntQA, which stands
for Entity linking as Question Answering. EntQA first proposes candidate
entities with a fast retrieval module, and then scrutinizes the document to
find mentions of each candidate with a powerful reader module. Our approach
combines progress in entity linking with that in open-domain question answering
and capitalizes on pretrained models for dense entity retrieval and reading
comprehension. Unlike in previous works, we do not rely on a mention-candidates
dictionary or large-scale weak supervision. EntQA achieves strong results on
the GERBIL benchmarking platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Combating Hype, Proceed with Caution. (arXiv:2110.08300v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08300">
<div class="article-summary-box-inner">
<span><p>In an effort to avoid reinforcing widespread hype about the capabilities of
state-of-the-art language technology, researchers have developed practices in
framing and citation that serve to deemphasize the field's successes. Though
well-meaning, these practices often yield misleading or even false claims about
the limits of our best technology. This is a problem, and it may be more
serious than it looks: It limits our ability to mitigate short-term harms from
NLP deployments and it limits our ability to prepare for the potentially
enormous impacts of more distant future advances. This paper urges researchers
to be careful about these claims and suggests some research directions and
communication strategies that will make it easier to avoid or rebut them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Labeling for Massively Multilingual Speech Recognition. (arXiv:2111.00161v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00161">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning through pseudo-labeling has become a staple of
state-of-the-art monolingual speech recognition systems. In this work, we
extend pseudo-labeling to massively multilingual speech recognition with 60
languages. We propose a simple pseudo-labeling recipe that works well even with
low-resource languages: train a supervised multilingual model, fine-tune it
with semi-supervised learning on a target language, generate pseudo-labels for
that language, and train a final model using pseudo-labels for all languages,
either from scratch or by fine-tuning. Experiments on the labeled Common Voice
and unlabeled VoxPopuli datasets show that our recipe can yield a model with
better performance for many languages that also transfers well to LibriSpeech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Scene Imagination for Generative Commonsense Reasoning. (arXiv:2112.06318v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06318">
<div class="article-summary-box-inner">
<span><p>Humans use natural language to compose common concepts from their environment
into plausible, day-to-day scene descriptions. However, such generative
commonsense reasoning (GCSR) skills are lacking in state-of-the-art text
generation methods. Descriptive sentences about arbitrary concepts generated by
neural text generation models (e.g., pre-trained text-to-text Transformers) are
often grammatically fluent but may not correspond to human common sense,
largely due to their lack of mechanisms to capture concept relations, to
identify implicit concepts, and to perform generalizable reasoning about unseen
concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&amp;V)
method, which learns to imagine a relational scene knowledge graph (SKG) with
relations between the input concepts, and leverage the SKG as a constraint when
generating a plausible scene description. We collect and harmonize a set of
knowledge resources from different domains and modalities, providing a rich
auxiliary supervision signal for I&amp;V. The experiments demonstrate the
effectiveness of I&amp;V in improving language models on both concept-to-sentence
and concept-to-story generation tasks, while enabling the model to learn well
from fewer task examples and generate SKGs that make common sense to human
annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViNMT: Neural Machine Translation Toolkit. (arXiv:2112.15272v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15272">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolkit for neural machine translation (NMT). The
new toolkit is mainly based on vaulted Transformer (Vaswani et al., 2017) along
with many other improvements detailed below, in order to create a
self-contained, simple to use, consistent and comprehensive framework for
Machine Translation tasks of various domains. It is tooled to support both
bilingual and multilingual translation tasks, starting from building the model
from respective corpora, to inferring new predictions or packaging the model to
serving-capable JIT format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. (arXiv:2201.07207v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07207">
<div class="article-summary-box-inner">
<span><p>Can world knowledge learned by large language models (LLMs) be used to act in
interactive environments? In this paper, we investigate the possibility of
grounding high-level tasks, expressed in natural language (e.g. "make
breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While
prior work focused on learning from explicit step-by-step examples of how to
act, we surprisingly find that if pre-trained LMs are large enough and prompted
appropriately, they can effectively decompose high-level tasks into mid-level
plans without any further training. However, the plans produced naively by LLMs
often cannot map precisely to admissible actions. We propose a procedure that
conditions on existing demonstrations and semantically translates the plans to
admissible actions. Our evaluation in the recent VirtualHome environment shows
that the resulting method substantially improves executability over the LLM
baseline. The conducted human evaluation reveals a trade-off between
executability and correctness but shows a promising sign towards extracting
actionable knowledge from language models. Website at
https://huangwl18.github.io/language-planner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset. (arXiv:2202.11684v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11684">
<div class="article-summary-box-inner">
<span><p>Misinformation is becoming increasingly prevalent on social media and in news
articles. It has become so widespread that we require algorithmic assistance
utilising machine learning to detect such content. Training these machine
learning models require datasets of sufficient scale, diversity and quality.
However, datasets in the field of automatic misinformation detection are
predominantly monolingual, include a limited amount of modalities and are not
of sufficient scale and quality. Addressing this, we develop a data collection
and linking system (MuMiN-trawl), to build a public misinformation graph
dataset (MuMiN), containing rich social media data (tweets, replies, users,
images, articles, hashtags) spanning 21 million tweets belonging to 26 thousand
Twitter threads, each of which have been semantically linked to 13 thousand
fact-checked claims across dozens of topics, events and domains, in 41
different languages, spanning more than a decade. The dataset is made available
as a heterogeneous graph via a Python package (mumin). We provide baseline
results for two node classification tasks related to the veracity of a claim
involving social media, and demonstrate that these are challenging tasks, with
the highest macro-average F1-score being 62.55% and 61.45% for the two tasks,
respectively. The MuMiN ecosystem is available at
https://mumin-dataset.github.io/, including the data, documentation, tutorials
and leaderboards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion. (arXiv:2202.13785v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13785">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs store a large number of factual triples while they are still
incomplete, inevitably. The previous knowledge graph completion (KGC) models
predict missing links between entities merely relying on fact-view data,
ignoring the valuable commonsense knowledge. The previous knowledge graph
embedding (KGE) techniques suffer from invalid negative sampling and the
uncertainty of fact-view link prediction, limiting KGC's performance. To
address the above challenges, we propose a novel and scalable Commonsense-Aware
Knowledge Embedding (CAKE) framework to automatically extract commonsense from
factual triples with entity concepts. The generated commonsense augments
effective self-supervision to facilitate both high-quality negative sampling
(NS) and joint commonsense and fact-view link prediction. Experimental results
on the KGC task demonstrate that assembling our framework could enhance the
performance of the original KGE models, and the proposed commonsense-aware NS
module is superior to other NS techniques. Besides, our proposed framework
could be easily adaptive to various KGE models and explain the predicted
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPPAC: Entity Pre-typing Relation Classification with Prompt AnswerCentralizing. (arXiv:2203.00193v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00193">
<div class="article-summary-box-inner">
<span><p>Relation classification (RC) aims to predict the relationship between a pair
of subject and object in a given context. Recently, prompt tuning approaches
have achieved high performance in RC. However, existing prompt tuning
approaches have the following issues: (1) numerous categories decrease RC
performance; (2) manually designed prompts require intensive labor. To address
these issues, a novel paradigm, Entity Pre-typing Relation Classification with
Prompt Answer Centralizing(EPPAC) is proposed in this paper. The entity
pre-tying in EPPAC is presented to address the first issue using a double-level
framework that pre-types entities before RC and prompt answer centralizing is
proposed to address the second issue. Extensive experiments show that our
proposed EPPAC outperformed state-of-the-art approaches on TACRED and TACREV by
14.4% and 11.1%, respectively. The code is provided in the Supplementary
Materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discontinuous Constituency and BERT: A Case Study of Dutch. (arXiv:2203.01063v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01063">
<div class="article-summary-box-inner">
<span><p>In this paper, we set out to quantify the syntactic capacity of BERT in the
evaluation regime of non-context free patterns, as occurring in Dutch. We
devise a test suite based on a mildly context-sensitive formalism, from which
we derive grammars that capture the linguistic phenomena of control verb
nesting and verb raising. The grammars, paired with a small lexicon, provide us
with a large collection of naturalistic utterances, annotated with verb-subject
pairings, that serve as the evaluation test bed for an attention-based span
selection probe. Our results, backed by extensive analysis, suggest that the
models investigated fail in the implicit acquisition of the dependencies
examined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02932">
<div class="article-summary-box-inner">
<span><p>Huge volumes of patient queries are daily generated on online health forums,
rendering manual doctor allocation a labor-intensive task. To better help
patients, this paper studies a novel task of doctor recommendation to enable
automatic pairing of a patient to a doctor with relevant expertise. While most
prior work in recommendation focuses on modeling target users from their past
behavior, we can only rely on the limited words in a query to infer a patient's
needs for privacy reasons. For doctor modeling, we study the joint effects of
their profiles and previous dialogues with other patients and explore their
interactions via self-learning. The learned doctor embeddings are further
employed to estimate their capabilities of handling a patient query with a
multi-head attention mechanism. For experiments, a large-scale dataset is
collected from Chunyu Yisheng, a Chinese online health forum, where our model
exhibits the state-of-the-art results, outperforming baselines only consider
profiles and past dialogues to characterize a doctor.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Mammograms Classification: A Review. (arXiv:2203.03618v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03618">
<div class="article-summary-box-inner">
<span><p>An advanced reliable low-cost form of screening method, Digital mammography
has been used as an effective imaging method for breast cancer detection. With
an increased focus on technologies to aid healthcare, Mammogram images have
been utilized in developing computer-aided diagnosis systems that will
potentially help in clinical diagnosis. Researchers have proved that artificial
intelligence with its emerging technologies can be used in the early detection
of the disease and improve radiologists' performance in assessing breast
cancer. In this paper, we review the methods developed for mammogram mass
classification in two categories. The first one is classifying manually
provided cropped region of interests (ROI) as either malignant or benign, and
the second one is the classification of automatically segmented ROIs as either
malignant or benign. We also provide an overview of datasets and evaluation
metrics used in the classification task. Finally, we compare and discuss the
deep learning approach to classical image processing and learning approach in
this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Cross-Layer Attention for Image Restoration. (arXiv:2203.03619v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03619">
<div class="article-summary-box-inner">
<span><p>Non-local attention module has been proven to be crucial for image
restoration. Conventional non-local attention processes features of each layer
separately, so it risks missing correlation between features among different
layers. To address this problem, we propose Cross-Layer Attention (CLA) module
in this paper. Instead of finding correlated key pixels within the same layer,
each query pixel can attend to key pixels at previous layers of the network. In
order to further enhance the learning capability and reduce the inference cost
of CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two
adaptive designs are proposed for ACLA: 1) adaptively selecting the keys for
non-local attention at each layer; 2) automatically searching for the insertion
locations for ACLA modules. By these two adaptive designs, ACLA dynamically
selects the number of keys to be aggregated for non-local attention at layer.
In addition, ACLA searches for the optimal insert positions of ACLA modules by
a neural architecture search method to render a compact neural network with
compelling performance. Extensive experiments on image restoration tasks,
including single image super-resolution, image denoising, image demosaicing,
and image compression artifacts reduction, validate the effectiveness and
efficiency of ACLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Triple Motion Estimation and Frame Interpolation based on Adaptive Threshold for Frame Rate Up-Conversion. (arXiv:2203.03621v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03621">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel motion-compensated frame rate up-conversion
(MC-FRUC) algorithm. The proposed algorithm creates interpolated frames by
first estimating motion vectors using unilateral (jointing forward and
backward) and bilateral motion estimation. Then motion vectors are combined
based on adaptive threshold, in order to creates high-quality interpolated
frames and reduce block artifacts. Since motion-compensated frame interpolation
along unilateral motion trajectories yields holes, a new algorithm is
introduced to resolve this problem. The experimental results show that the
quality of the interpolated frames using the proposed algorithm is much higher
than the existing algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement. (arXiv:2203.03622v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03622">
<div class="article-summary-box-inner">
<span><p>A stroke occurs when an artery in the brain ruptures and bleeds or when the
blood supply to the brain is cut off. Blood and oxygen cannot reach the brain's
tissues due to the rupture or obstruction resulting in tissue death. The Middle
cerebral artery (MCA) is the largest cerebral artery and the most commonly
damaged vessel in stroke. The quick onset of a focused neurological deficit
caused by interruption of blood flow in the territory supplied by the MCA is
known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is
used to estimate the extent of early ischemic changes in patients with MCA
stroke. This study proposes a deep learning-based method to score the CT scan
for ASPECTS. Our work has three highlights. First, we propose a novel method
for medical image segmentation for stroke detection. Second, we show the
effectiveness of AI solution for fully-automated ASPECT scoring with reduced
diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a
dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72
for the infarcts segmentation. Lastly, we show that our model's performance is
inline with inter-reader variability between radiologists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction. (arXiv:2203.03623v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03623">
<div class="article-summary-box-inner">
<span><p>We propose a novel and unified method, measurement-conditioned denoising
diffusion probabilistic model (MC-DDPM), for under-sampled medical image
reconstruction based on DDPM. Different from previous works, MC-DDPM is defined
in measurement domain (e.g. k-space in MRI reconstruction) and conditioned on
under-sampling mask. We apply this method to accelerate MRI reconstruction and
the experimental results show excellent performance, outperforming full
supervision baseline and the state-of-the-art score-based reconstruction
method. Due to its generative nature, MC-DDPM can also quantify the uncertainty
of reconstruction. Our code is available on github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusion-Correction Network for Single-Exposure Correction and Multi-Exposure Fusion. (arXiv:2203.03624v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03624">
<div class="article-summary-box-inner">
<span><p>The photographs captured by digital cameras usually suffer from over-exposure
or under-exposure problems. The Single-Exposure Correction (SEC) and
Multi-Exposure Fusion (MEF) are two widely studied image processing tasks for
image exposure enhancement. However, current SEC and MEF methods ignore the
internal correlation between SEC and MEF, and are proposed under distinct
frameworks. What's more, most MEF methods usually fail at processing a sequence
containing only under-exposed or over-exposed images. To alleviate these
problems, in this paper, we develop an integrated framework to simultaneously
tackle the SEC and MEF tasks. Built upon the Laplacian Pyramid (LP)
decomposition, we propose a novel Fusion-Correction Network (FCNet) to fuse and
correct an image sequence sequentially in a multi-level scheme. In each LP
level, the image sequence is feed into a Fusion block and a Correction block
for consecutive image fusion and exposure correction. The corrected image is
upsampled and re-composed with the high-frequency detail components in
next-level, producing the base sequence for the next-level blocks. Experiments
on the benchmark dataset demonstrate that our FCNet is effective on both the
SEC and MEF tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coordinate Translator for Learning Deformable Medical Image Registration. (arXiv:2203.03626v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03626">
<div class="article-summary-box-inner">
<span><p>The majority of deep learning (DL) based deformable image registration
methods use convolutional neural networks (CNNs) to estimate displacement
fields from pairs of moving and fixed images. This, however, requires the
convolutional kernels in the CNN to not only extract intensity features from
the inputs but also understand image coordinate systems. We argue that the
latter task is challenging for traditional CNNs, limiting their performance in
registration tasks. To tackle this problem, we first introduce Coordinate
Translator (CoTr), a differentiable module that identifies matched features
between the fixed and moving image and outputs their coordinate correspondences
without the need for training. It unloads the burden of understanding image
coordinate systems for CNNs, allowing them to focus on feature extraction. We
then propose a novel deformable registration network, im2grid, that uses
multiple CoTr's with the hierarchical features extracted from a CNN encoder and
outputs a deformation field in a coarse-to-fine fashion. We compared im2grid
with the state-of-the-art DL and non-DL methods for unsupervised 3D magnetic
resonance image registration. Our experiments show that im2grid outperforms
these methods both qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-channel deep convolutional neural networks for multi-classifying thyroid disease. (arXiv:2203.03627v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03627">
<div class="article-summary-box-inner">
<span><p>Thyroid disease instances have been continuously increasing since the 1990s,
and thyroid cancer has become the most rapidly rising disease among all the
malignancies in recent years. Most existing studies focused on applying deep
convolutional neural networks for detecting thyroid cancer. Despite their
satisfactory performance on binary classification tasks, limited studies have
explored multi-class classification of thyroid disease types; much less is
known of the diagnosis of co-existence situation for different types of thyroid
diseases. Therefore, this study proposed a novel multi-channel convolutional
neural network (CNN) architecture to address the multi-class classification
task of thyroid disease. The multi-channel CNN merits from computed tomography
to drive a comprehensive diagnostic decision for the overall thyroid gland,
emphasizing the disease co-existence circumstance. Moreover, this study also
examined alternative strategies to enhance the diagnostic accuracy of CNN
models through concatenation of different scales of feature maps. Benchmarking
experiments demonstrate the improved performance of the proposed multi-channel
CNN architecture compared with the standard single-channel CNN architecture.
More specifically, the multi-channel CNN achieved an accuracy of 0.909,
precision of 0.944, recall of 0.896, specificity of 0.994, and F1 of 0.917, in
contrast to the single-channel CNN, which obtained 0.902, 0.892, 0.909, 0.993,
0.898, respectively. In addition, the proposed model was evaluated in different
gender groups; it reached a diagnostic accuracy of 0.908 for the female group
and 0.901 for the male group. Collectively, the results highlight that the
proposed multi-channel CNN has excellent generalization and has the potential
to be deployed to provide computational decision support in clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Student Become Decathlon Master in Retinal Vessel Segmentation via Dual-teacher Multi-target Domain Adaptation. (arXiv:2203.03631v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03631">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation has been proposed recently to tackle the
so-called domain shift between training data and test data with different
distributions. However, most of them only focus on single-target domain
adaptation and cannot be applied to the scenario with multiple target domains.
In this paper, we propose RVms, a novel unsupervised multi-target domain
adaptation approach to segment retinal vessels (RVs) from multimodal and
multicenter retinal images. RVms mainly consists of a style augmentation and
transfer (SAT) module and a dual-teacher knowledge distillation (DTKD) module.
SAT augments and clusters images into source-similar domains and
source-dissimilar domains via B\'ezier and Fourier transformations. DTKD
utilizes the augmented and transformed data to train two teachers, one for
source-similar domains and the other for source-dissimilar domains. Afterwards,
knowledge distillation is performed to iteratively distill different domain
knowledge from teachers to a generic student. The local relative intensity
transformation is employed to characterize RVs in a domain invariant manner and
promote the generalizability of teachers and student models. Moreover, we
construct a new multimodal and multicenter vascular segmentation dataset from
existing publicly-available datasets, which can be used to benchmark various
domain adaptation and domain generalization methods. Through extensive
experiments, RVms is found to be very close to the target-trained Oracle in
terms of segmenting the RVs, largely outperforming other state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InsightNet: non-contact blood pressure measuring network based on face video. (arXiv:2203.03634v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03634">
<div class="article-summary-box-inner">
<span><p>Blood pressure indicates cardiac function and peripheral vascular resistance
and is critical for disease diagnosis. Traditionally, blood pressure data are
mainly acquired through contact sensors, which require high maintenance and may
be inconvenient and unfriendly to some people (e.g., burn patients). In this
paper, an efficient non-contact blood pressure measurement network based on
face videos is proposed for the first time. An innovative oversampling training
strategy is proposed to handle the unbalanced data distribution. The input
video sequences are first normalized and converted to our proposed YUVT color
space. Then, the Spatio-temporal slicer encodes it into a multi-domain
Spatio-temporal mapping. Finally, the neural network computation module, used
for high-dimensional feature extraction of the multi-domain spatial feature
mapping, after which the extracted high-dimensional features are used to
enhance the time-domain feature association using LSTM, is computed by the
blood pressure classifier to obtain the blood pressure measurement intervals.
Combining the output of feature extraction and the result after classification,
the blood pressure calculator, calculates the blood pressure measurement
values. The solution uses a blood pressure classifier to calculate blood
pressure intervals, which can help the neural network distinguish between the
high-dimensional features of different blood pressure intervals and alleviate
the overfitting phenomenon. It can also locate the blood pressure intervals,
correct the final blood pressure values and improve the network performance.
Experimental results on two datasets show that the network outperforms existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stepwise Feature Fusion: Local Guides Global. (arXiv:2203.03635v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03635">
<div class="article-summary-box-inner">
<span><p>Colonoscopy, currently the most efficient and recognized colon polyp
detection technology, is necessary for early screening and prevention of
colorectal cancer. However, due to the varying size and complex morphological
features of colonic polyps as well as the indistinct boundary between polyps
and mucosa, accurate segmentation of polyps is still challenging. Deep learning
has become popular for accurate polyp segmentation tasks with excellent
results. However, due to the structure of polyps image and the varying shapes
of polyps, it easy for existing deep learning models to overfitting the current
dataset. As a result, the model may not process unseen colonoscopy data. To
address this, we propose a new State-Of-The-Art model for medical image
segmentation, the SSFormer, which uses a pyramid Transformer encoder to improve
the generalization ability of models. Specifically, our proposed Progressive
Locality Decoder can be adapted to the pyramid Transformer backbone to
emphasize local features and restrict attention dispersion. The SSFormer
achieves statet-of-the-art performance in both learning and generalization
assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering and classification of low-dimensional data in explicit feature map domain: intraoperative pixel-wise diagnosis of adenocarcinoma of a colon in a liver. (arXiv:2203.03636v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03636">
<div class="article-summary-box-inner">
<span><p>Application of artificial intelligence in medicine brings in highly accurate
predictions achieved by complex models, the reasoning of which is hard to
interpret. Their generalization ability can be reduced because of the lack of
pixel wise annotated images that occurs in frozen section tissue analysis. To
partially overcome this gap, this paper explores the approximate explicit
feature map (aEFM) transform of low-dimensional data into a low-dimensional
subspace in Hilbert space. There, with a modest increase in computational
complexity, linear algorithms yield improved performance and keep
interpretability. They remain amenable to incremental learning that is not a
trivial issue for some nonlinear algorithms. We demonstrate proposed
methodology on a very large-scale problem related to intraoperative pixel-wise
semantic segmentation and clustering of adenocarcinoma of a colon in a liver.
Compared to the results in the input space, logistic classifier achieved
statistically significant performance improvements in micro balanced accuracy
and F1 score in the amounts of 12.04% and 12.58%, respectively. Support vector
machine classifier yielded the increase of 8.04% and 9.41%. For clustering,
increases of 0.79% and 0.85% are obtained with ultra large-scale spectral
clustering algorithm. Results are supported by a discussion of interpretability
using Shapely additive explanation values for predictions of linear classifier
in input space and aEFM induced space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image Registration Towards Enhancing Performance and Explainability in Cardiac And Brain Image Analysis. (arXiv:2203.03638v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03638">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance Imaging (MRI) typically recruits multiple sequences
(defined here as "modalities"). As each modality is designed to offer different
anatomical and functional clinical information, there are evident disparities
in the imaging content across modalities. Inter- and intra-modality affine and
non-rigid image registration is an essential medical image analysis process in
clinical imaging, as for example before imaging biomarkers need to be derived
and clinically evaluated across different MRI modalities, time phases and
slices. Although commonly needed in real clinical scenarios, affine and
non-rigid image registration is not extensively investigated using a single
unsupervised model architecture. In our work, we present an un-supervised deep
learning registration methodology which can accurately model affine and
non-rigid trans-formations, simultaneously. Moreover, inverse-consistency is a
fundamental inter-modality registration property that is not considered in deep
learning registration algorithms. To address inverse-consistency, our
methodology performs bi-directional cross-modality image synthesis to learn
modality-invariant latent rep-resentations, while involves two factorised
transformation networks and an inverse-consistency loss to learn
topology-preserving anatomical transformations. Overall, our model (named
"FIRE") shows improved performances against the reference standard baseline
method on multi-modality brain 2D and 3D MRI and intra-modality cardiac 4D MRI
data experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conquering Data Variations in Resolution: A Slice-Aware Multi-Branch Decoder Network. (arXiv:2203.03640v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03640">
<div class="article-summary-box-inner">
<span><p>Fully convolutional neural networks have made promising progress in joint
liver and liver tumor segmentation. Instead of following the debates over 2D
versus 3D networks (for example, pursuing the balance between large-scale 2D
pretraining and 3D context), in this paper, we novelly identify the wide
variation in the ratio between intra- and inter-slice resolutions as a crucial
obstacle to the performance. To tackle the mismatch between the intra- and
inter-slice information, we propose a slice-aware 2.5D network that emphasizes
extracting discriminative features utilizing not only in-plane semantics but
also out-of-plane coherence for each separate slice. Specifically, we present a
slice-wise multi-input multi-output architecture to instantiate such a design
paradigm, which contains a Multi-Branch Decoder (MD) with a Slice-centric
Attention Block (SAB) for learning slice-specific features and a Densely
Connected Dice (DCD) loss to regularize the inter-slice predictions to be
coherent and continuous. Based on the aforementioned innovations, we achieve
state-of-the-art results on the MICCAI 2017 Liver Tumor Segmentation (LiTS)
dataset. Besides, we also test our model on the ISBI 2019 Segmentation of
THoracic Organs at Risk (SegTHOR) dataset, and the result proves the robustness
and generalizability of the proposed method in other segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation. (arXiv:2203.03664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03664">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of retinal fluids in 3D Optical Coherence Tomography
images is key for diagnosis and personalized treatment of eye diseases. While
deep learning has been successful at this task, trained supervised models often
fail for images that do not resemble labeled examples, e.g. for images acquired
using different devices. We hereby propose a novel semi-supervised learning
framework for segmentation of volumetric images from new unlabeled domains. We
jointly use supervised and contrastive learning, also introducing a contrastive
pairing scheme that leverages similarity between nearby slices in 3D. In
addition, we propose channel-wise aggregation as an alternative to conventional
spatial-pooling aggregation for contrastive feature map projection. We evaluate
our methods for domain adaptation from a (labeled) source domain to an
(unlabeled) target domain, each containing images acquired with different
acquisition devices. In the target domain, our method achieves a Dice
coefficient 13.8% higher than SimCLR (a state-of-the-art contrastive
framework), and leads to results comparable to an upper bound with supervised
training in that domain. In the source domain, our model also improves the
results by 5.4% Dice, by successfully leveraging information from many
unlabeled images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-centric and memory-guided normality reconstruction for video anomaly detection. (arXiv:2203.03677v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03677">
<div class="article-summary-box-inner">
<span><p>This paper addresses video anomaly detection problem for videosurveillance.
Due to the inherent rarity and heterogeneity of abnormal events, the problem is
viewed as a normality modeling strategy, in which our model learns
object-centric normal patterns without seeing anomalous samples during
training. The main contributions consist in coupling pretrained object-level
action features prototypes with a cosine distance-based anomaly estimation
function, therefore extending previous methods by introducing additional
constraints to the mainstream reconstruction-based strategy. Our framework
leverages both appearance and motion information to learn object-level behavior
and captures prototypical patterns within a memory module. Experiments on
several well-known datasets demonstrate the effectiveness of our method as it
outperforms current state-of-the-art on most relevant spatio-temporal
evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers. (arXiv:2203.03682v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03682">
<div class="article-summary-box-inner">
<span><p>In this work, we consider the problem of learning a perception model for
monocular robot navigation using few annotated images. Using a Vision
Transformer (ViT) pretrained with a label-free self-supervised method, we
successfully train a coarse image segmentation model for the Duckietown
environment using 70 training images. Our model performs coarse image
segmentation at the 8x8 patch level, and the inference resolution can be
adjusted to balance prediction granularity and real-time perception
constraints. We study how best to adapt a ViT to our task and environment, and
find that some lightweight architectures can yield good single-image
segmentations at a usable frame rate, even on CPU. The resulting perception
model is used as the backbone for a simple yet robust visual servoing agent,
which we deploy on a differential drive mobile robot to perform two tasks: lane
following and obstacle avoidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WaveMix: Resource-efficient Token Mixing for Images. (arXiv:2203.03689v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03689">
<div class="article-summary-box-inner">
<span><p>Although certain vision transformer (ViT) and CNN architectures generalize
well on vision tasks, it is often impractical to use them on green, edge, or
desktop computing due to their computational requirements for training and even
testing. We present WaveMix as an alternative neural architecture that uses a
multi-scale 2D discrete wavelet transform (DWT) for spatial token mixing.
Unlike ViTs, WaveMix neither unrolls the image nor requires self-attention of
quadratic complexity. Additionally, DWT introduces another inductive bias --
besides convolutional filtering -- to utilize the 2D structure of an image to
improve generalization. The multi-scale nature of the DWT also reduces the
requirement for a deeper architecture compared to the CNNs, as the latter
relies on pooling for partial spatial mixing. WaveMix models show
generalization that is competitive with ViTs, CNNs, and token mixers on several
datasets while requiring lower GPU RAM (training and testing), number of
computations, and storage. WaveMix have achieved State-of-the-art (SOTA)
results in EMNIST Byclass and EMNIST Balanced datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometric recognition: why not massively adopted yet?. (arXiv:2203.03719v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03719">
<div class="article-summary-box-inner">
<span><p>Although there has been a dramatically reduction on the prices of capturing
devices and an increase on computing power in the last decade, it seems that
biometric systems are still far from massive adoption for civilian
applications. This paper deals with the causes of this phenomenon, as well as
some misconceptions regarding biometric identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Barlow constrained optimization for Visual Question Answering. (arXiv:2203.03727v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03727">
<div class="article-summary-box-inner">
<span><p>Visual question answering is a vision-and-language multimodal task, that aims
at predicting answers given samples from the question and image modalities.
Most recent methods focus on learning a good joint embedding space of images
and questions, either by improving the interaction between these two
modalities, or by making it a more discriminant space. However, how informative
this joint space is, has not been well explored. In this paper, we propose a
novel regularization for VQA models, Constrained Optimization using Barlow's
theory (COB), that improves the information content of the joint space by
minimizing the redundancy. It reduces the correlation between the learned
feature components and thereby disentangles semantic concepts. Our model also
aligns the joint space with the answer embedding space, where we consider the
answer and image+question as two different `views' of what in essence is the
same semantic information. We propose a constrained optimization policy to
balance the categorical and redundancy minimization forces. When built on the
state-of-the-art GGE model, the resulting model improves VQA accuracy by 1.4%
and 4% on the VQA-CP v2 and VQA v2 datasets respectively. The model also
exhibits better interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrowdFormer: Weakly-supervised Crowd counting with Improved Generalizability. (arXiv:2203.03768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03768">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have dominated the field of computer
vision for nearly a decade due to their strong ability to learn local features.
However, due to their limited receptive field, CNNs fail to model the global
context. On the other hand, transformer, an attention-based architecture can
model the global context easily. Despite this, there are limited studies that
investigate the effectiveness of transformers in crowd counting. In addition,
the majority of the existing crowd counting methods are based on the regression
of density maps which requires point-level annotation of each person present in
the scene. This annotation task is laborious and also error-prone. This has led
to increased focus on weakly-supervised crowd counting methods which require
only the count-level annotations. In this paper, we propose a weakly-supervised
method for crowd counting using a pyramid vision transformer. We have conducted
extensive evaluations to validate the effectiveness of the proposed method. Our
method is comparable to the state-of-the-art on the benchmark crowd datasets.
More importantly, it shows remarkable generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAMI-AD: An Activity Detector Exploiting Part-attention and Motion Information in Surveillance Videos. (arXiv:2203.03796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03796">
<div class="article-summary-box-inner">
<span><p>Activity detection in surveillance videos is a challenging task caused by
small objects, complex activity categories, its untrimmed nature, etc. In this
work, we propose an effective activity detection system for person-only and
vehicle-only activities in untrimmed surveillance videos, named PAMI-AD. It
consists of four modules, i.e., multi-object tracking, background modeling,
activity classifier and post-processing. In particular, we propose a novel
part-attention mechanism for person-only activities and a simple but strong
motion information encoding method for vehicle-only activities. Our proposed
system achieves the best results on the VIRAT dataset. Furthermore, our team
won the 1st place in the TRECVID 2021 ActEV challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unknown-Aware Object Detection: Learning What You Don't Know from Videos in the Wild. (arXiv:2203.03800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03800">
<div class="article-summary-box-inner">
<span><p>Building reliable object detectors that can detect out-of-distribution (OOD)
objects is critical yet underexplored. One of the key challenges is that models
lack supervision signals from unknown data, producing overconfident predictions
on OOD objects. We propose a new unknown-aware object detection framework
through Spatial-Temporal Unknown Distillation (STUD), which distills unknown
objects from videos in the wild and meaningfully regularizes the model's
decision boundary. STUD first identifies the unknown candidate object proposals
in the spatial dimension, and then aggregates the candidates across multiple
video frames to form a diverse set of unknown objects near the decision
boundary. Alongside, we employ an energy-based uncertainty regularization loss,
which contrastively shapes the uncertainty space between the in-distribution
and distilled unknown objects. STUD establishes the state-of-the-art
performance on OOD detection tasks for object detection, reducing the FPR95
score by over 10% compared to the previous best method. Code is available at
https://github.com/deeplearning-wisc/stud.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoramic Human Activity Recognition. (arXiv:2203.03806v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03806">
<div class="article-summary-box-inner">
<span><p>To obtain a more comprehensive activity understanding for a crowded scene, in
this paper, we propose a new problem of panoramic human activity recognition
(PAR), which aims to simultaneous achieve the individual action, social group
activity, and global activity recognition. This is a challenging yet practical
problem in real-world applications. For this problem, we develop a novel
hierarchical graph neural network to progressively represent and model the
multi-granularity human activities and mutual social relations for a crowd of
people. We further build a benchmark to evaluate the proposed method and other
existing related methods. Experimental results verify the rationality of the
proposed PAR problem, the effectiveness of our method and the usefulness of the
benchmark. We will release the source code and benchmark to the public for
promoting the study on this problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Search with Text Feedback by Additive Attention Compositional Learning. (arXiv:2203.03809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03809">
<div class="article-summary-box-inner">
<span><p>Effective image retrieval with text feedback stands to impact a range of
real-world applications, such as e-commerce. Given a source image and text
feedback that describes the desired modifications to that image, the goal is to
retrieve the target images that resemble the source yet satisfy the given
modifications by composing a multi-modal (image-text) query. We propose a novel
solution to this problem, Additive Attention Compositional Learning (AACL),
that uses a multi-modal transformer-based architecture and effectively models
the image-text contexts. Specifically, we propose a novel image-text
composition module based on additive attention that can be seamlessly plugged
into deep neural networks. We also introduce a new challenging benchmark
derived from the Shopping100k dataset. AACL is evaluated on three large-scale
datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong
baselines. Extensive experiments show that AACL achieves new state-of-the-art
results on all three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating 3D Bio-Printable Patches Using Wound Segmentation and Reconstruction to Treat Diabetic Foot Ulcers. (arXiv:2203.03814v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03814">
<div class="article-summary-box-inner">
<span><p>We introduce AiD Regen, a novel system that generates 3D wound models
combining 2D semantic segmentation with 3D reconstruction so that they can be
printed via 3D bio-printers during the surgery to treat diabetic foot ulcers
(DFUs). AiD Regen seamlessly binds the full pipeline, which includes RGB-D
image capturing, semantic segmentation, boundary-guided point-cloud processing,
3D model reconstruction, and 3D printable G-code generation, into a single
system that can be used out of the box. We developed a multi-stage data
preprocessing method to handle small and unbalanced DFU image datasets. AiD
Regen's human-in-the-loop machine learning interface enables clinicians to not
only create 3D regenerative patches with just a few touch interactions but also
customize and confirm wound boundaries. As evidenced by our experiments, our
model outperforms prior wound segmentation models and our reconstruction
algorithm is capable of generating 3D wound models with compelling accuracy. We
further conducted a case study on a real DFU patient and demonstrated the
effectiveness of AiD Regen in treating DFU wounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon. (arXiv:2203.03818v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03818">
<div class="article-summary-box-inner">
<span><p>Estimating the risk level of adversarial examples is essential for safely
deploying machine learning models in the real world. One popular approach for
physical-world attacks is to adopt the "sticker-pasting" strategy, which
however suffers from some limitations, including difficulties in access to the
target or printing by valid colors. A new type of non-invasive attacks emerged
recently, which attempt to cast perturbation onto the target by optics based
tools, such as laser beam and projector. However, the added optical patterns
are artificial but not natural. Thus, they are still conspicuous and
attention-grabbed, and can be easily noticed by humans. In this paper, we study
a new type of optical adversarial examples, in which the perturbations are
generated by a very common natural phenomenon, shadow, to achieve naturalistic
and stealthy physical-world adversarial attack under the black-box setting. We
extensively evaluate the effectiveness of this new attack on both simulated and
real-world environments. Experimental results on traffic sign recognition
demonstrate that our algorithm can generate adversarial examples effectively,
reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets
respectively, while continuously misleading a moving camera over 95% of the
time in real-world scenarios. We also offer discussions about the limitations
and the defense mechanism of this attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Table Structure Recognition with Conditional Attention. (arXiv:2203.03819v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03819">
<div class="article-summary-box-inner">
<span><p>Tabular data in digital documents is widely used to express compact and
important information for readers. However, it is challenging to parse tables
from unstructured digital documents, such as PDFs and images, into
machine-readable format because of the complexity of table structures and the
missing of meta-information. Table Structure Recognition (TSR) problem aims to
recognize the structure of a table and transform the unstructured tables into a
structured and machine-readable format so that the tabular data can be further
analysed by the down-stream tasks, such as semantic modeling and information
retrieval. In this study, we hypothesize that a complicated table structure can
be represented by a graph whose vertices and edges represent the cells and
association between cells, respectively. Then we define the table structure
recognition problem as a cell association classification problem and propose a
conditional attention network (CATT-Net). The experimental results demonstrate
the superiority of our proposed method over the state-of-the-art methods on
various datasets. Besides, we investigate whether the alignment of a cell
bounding box or a text-focused approach has more impact on the model
performance. Due to the lack of public dataset annotations based on these two
approaches, we further annotate the ICDAR2013 dataset providing both types of
bounding boxes, which can be a new benchmark dataset for evaluating the methods
in this field. Experimental results show that the alignment of a cell bounding
box can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the
Macro-average F1 score from 0.787 to 0.923.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Vision Transformer. (arXiv:2203.03821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03821">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have made many breakthroughs in computer vision
tasks. However, considerable redundancy arises in the spatial dimension of an
input image, leading to massive computational costs. Therefore, We propose a
coarse-to-fine vision transformer (CF-ViT) to relieve computational burden
while retaining performance in this paper. Our proposed CF-ViT is motivated by
two important observations in modern ViT models: (1) The coarse-grained patch
splitting can locate informative regions of an input image. (2) Most images can
be well recognized by a ViT model in a small-length token sequence. Therefore,
our CF-ViT implements network inference in a two-stage manner. At coarse
inference stage, an input image is split into a small-length patch sequence for
a computationally economical classification. If not well recognized, the
informative patches are identified and further re-split in a fine-grained
granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For
example, without any compromise on performance, CF-ViT reduces 53% FLOPs of
LV-ViT, and also achieves 2.01x throughput.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03831">
<div class="article-summary-box-inner">
<span><p>Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant
irregular boundaries. To deal with this problem, existing image rectangling
methods devote to searching an initial mesh and optimizing a target mesh to
form the mesh deformation in two stages. Then rectangular images can be
generated by warping stitched images. However, these solutions only work for
images with rich linear structures, leading to noticeable distortions for
portraits and landscapes with non-linear objects. In this paper, we address
these issues by proposing the first deep learning solution to image
rectangling. Concretely, we predefine a rigid target mesh and only estimate an
initial mesh to form the mesh deformation, contributing to a compact one-stage
solution. The initial mesh is predicted using a fully convolutional network
with a residual progressive regression strategy. To obtain results with high
content fidelity, a comprehensive objective function is proposed to
simultaneously encourage the boundary rectangular, mesh shape-preserving, and
content perceptually natural. Besides, we build the first image stitching
rectangling dataset with a large diversity in irregular boundaries and scenes.
Experiments demonstrate our superiority over traditional methods both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point Clouds for Closing Domain Gap. (arXiv:2203.03833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03833">
<div class="article-summary-box-inner">
<span><p>Semantic analyses of object point clouds are largely driven by releasing of
benchmarking datasets, including synthetic ones whose instances are sampled
from object CAD models. However, learning from synthetic data may not
generalize to practical scenarios, where point clouds are typically incomplete,
non-uniformly distributed, and noisy. Such a challenge of Simulation-to-Real
(Sim2Real) domain gap could be mitigated via learning algorithms of domain
adaptation; however, we argue that generation of synthetic point clouds via
more physically realistic rendering is a powerful alternative, as systematic
non-uniform noise patterns can be captured. To this end, we propose an
integrated scheme consisting of physically realistic synthesis of object point
clouds via rendering stereo images via projection of speckle patterns onto CAD
models and a novel quasi-balanced self-training designed for more balanced data
distribution by sparsity-driven selection of pseudo labeled samples for long
tailed classes. Experiment results can verify the effectiveness of our method
as well as both of its modules for unsupervised domain adaptation on point
cloud classification, achieving the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scale Self-Contrastive Learning with Hard Negative Mining for Weakly-Supervised Query-based Video Grounding. (arXiv:2203.03838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03838">
<div class="article-summary-box-inner">
<span><p>Query-based video grounding is an important yet challenging task in video
understanding, which aims to localize the target segment in an untrimmed video
according to a sentence query. Most previous works achieve significant progress
by addressing this task in a fully-supervised manner with segment-level labels,
which require high labeling cost. Although some recent efforts develop
weakly-supervised methods that only need the video-level knowledge, they
generally match multiple pre-defined segment proposals with query and select
the best one, which lacks fine-grained frame-level details for distinguishing
frames with high repeatability and similarity within the entire video. To
alleviate the above limitations, we propose a self-contrastive learning
framework to address the query-based video grounding task under a
weakly-supervised setting. Firstly, instead of utilizing redundant segment
proposals, we propose a new grounding scheme that learns frame-wise matching
scores referring to the query semantic to predict the possible foreground
frames by only using the video-level annotations. Secondly, since some
predicted frames (i.e., boundary frames) are relatively coarse and exhibit
similar appearance to their adjacent frames, we propose a coarse-to-fine
contrastive learning paradigm to learn more discriminative frame-wise
representations for distinguishing the false positive frames. In particular, we
iteratively explore multi-scale hard negative samples that are close to
positive samples in the representation space for distinguishing fine-grained
frame-wise details, thus enforcing more accurate segment grounding. Extensive
experiments on two challenging benchmarks demonstrate the superiority of our
proposed method compared with the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Social Relation Representation for Human Group Detection. (arXiv:2203.03843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03843">
<div class="article-summary-box-inner">
<span><p>Human group detection, which splits crowd of people into groups, is an
important step for video-based human social activity analysis. The core of
human group detection is the human social relation representation and
division.In this paper, we propose a new two-stage multi-head framework for
human group detection. In the first stage, we propose a human behavior
simulator head to learn the social relation feature embedding, which is
self-supervisely trained by leveraging the socially grounded multi-person
behavior relationship. In the second stage, based on the social relation
embedding, we develop a self-attention inspired network for human group
detection. Remarkable performance on two state-of-the-art large-scale
benchmarks, i.e., PANDA and JRDB-Group, verifies the effectiveness of the
proposed framework. Benefiting from the self-supervised social relation
embedding, our method can provide promising results with very few (labeled)
training data. We will release the source code to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03844">
<div class="article-summary-box-inner">
<span><p>Light-weight super-resolution (SR) models have received considerable
attention for their serviceability in mobile devices. Many efforts employ
network quantization to compress SR models. However, these methods suffer from
severe performance degradation when quantizing the SR models to ultra-low
precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In
this paper, we identify that the performance drop comes from the contradiction
between the layer-wise symmetric quantizer and the highly asymmetric activation
distribution in SR models. This discrepancy leads to either a waste on the
quantization levels or detail loss in reconstructed images. Therefore, we
propose a novel activation quantizer, referred to as Dynamic Dual Trainable
Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically,
DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower
bounds to tackle the highly asymmetric activations. 2) A dynamic gate
controller to adaptively adjust the upper and lower bounds at runtime to
overcome the drastically varying activation ranges over different samples.To
reduce the extra overhead, the dynamic gate controller is quantized to 2-bit
and applied to only part of the SR networks according to the introduced dynamic
intensity. Extensive experiments demonstrate that our DDTB exhibits significant
performance improvements in ultra-low precision. For example, our DDTB achieves
a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and
scaling up output images to x4. Code is at
\url{https://github.com/zysxmu/DDTB}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Does the Performance Improvement Come From? - A Reproducibility Concern about Image-Text Retrieval. (arXiv:2203.03853v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03853">
<div class="article-summary-box-inner">
<span><p>This paper seeks to provide the information retrieval community with some
reflections on the current improvements of retrieval learning through the
analysis of the reproducibility aspects of image-text retrieval models. For the
latter part of the past decade, image-text retrieval has gradually become a
major research direction in the field of information retrieval because of the
growth of multi-modal data. Many researchers use benchmark datasets like
MS-COCO and Flickr30k to train and assess the performance of image-text
retrieval algorithms. Research in the past has mostly focused on performance,
with several state-of-the-art methods being proposed in various ways. According
to their claims, these approaches achieve better modal interactions and thus
better multimodal representations with greater precision. In contrast to those
previous works, we focus on the repeatability of the approaches and the overall
examination of the elements that lead to improved performance by pretrained and
nonpretrained models in retrieving images and text. To be more specific, we
first examine the related reproducibility concerns and why the focus is on
image-text retrieval tasks, and then we systematically summarize the current
paradigm of image-text retrieval models and the stated contributions of those
approaches. Second, we analyze various aspects of the reproduction of
pretrained and nonpretrained retrieval models. Based on this, we conducted
ablation experiments and obtained some influencing factors that affect
retrieval recall more than the improvement claimed in the original paper.
Finally, we also present some reflections and issues that should be considered
by the retrieval community in the future. Our code is freely available at
https://github.com/WangFei-2019/Image-text-Retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New 27 Class Sign Language Dataset Collected from 173 Individuals. (arXiv:2203.03859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03859">
<div class="article-summary-box-inner">
<span><p>After the interviews, it has been comprehended that speech-impaired
individuals who use sign languages have difficulty communicating with other
people who do not know sign language. Due to the communication problems, the
sense of independence of speech-impaired individuals could be damaged and lead
them to socialize less with society. To contribute to the development of
technologies, that can reduce the communication problems of speech-impaired
persons, a new dataset was presented with this paper. The dataset was created
by processing American Sign Language-based photographs collected from 173
volunteers, published as 27 Class Sign Language Dataset on the Kaggle Datasets
web page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Semantic Segmentation using Out-of-Distribution Data. (arXiv:2203.03860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03860">
<div class="article-summary-box-inner">
<span><p>Weakly supervised semantic segmentation (WSSS) methods are often built on
pixel-level localization maps obtained from a classifier. However, training on
class labels only, classifiers suffer from the spurious correlation between
foreground and background cues (e.g. train and rail), fundamentally bounding
the performance of WSSS. There have been previous endeavors to address this
issue with additional supervision. We propose a novel source of information to
distinguish foreground from the background: Out-of-Distribution (OoD) data, or
images devoid of foreground object classes. In particular, we utilize the hard
OoDs that the classifier is likely to make false-positive predictions. These
samples typically carry key visual features on the background (e.g. rail) that
the classifiers often confuse as foreground (e.g. train), so these cues let
classifiers correctly suppress spurious background cues. Acquiring such hard
OoDs does not require an extensive amount of annotation efforts; it only incurs
a few additional image-level labeling costs on top of the original efforts to
collect class labels. We propose a method, W-OoD, for utilizing the hard OoDs.
W-OoD achieves state-of-the-art performance on Pascal VOC 2012.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03871">
<div class="article-summary-box-inner">
<span><p>This work simultaneously considers the discriminability and transferability
properties of deep representations in the typical supervised learning task,
i.e., image classification. By a comprehensive temporal analysis, we observe a
trade-off between these two properties. The discriminability keeps increasing
with the training progressing while the transferability intensely diminishes in
the later training period.
</p>
<p>From the perspective of information-bottleneck theory, we reveal that the
incompatibility between discriminability and transferability is attributed to
the over-compression of input information. More importantly, we investigate why
and how the InfoNCE loss can alleviate the over-compression, and further
present a learning framework, named contrastive temporal coding~(CTC), to
counteract the over-compression and alleviate the incompatibility. Extensive
experiments validate that CTC successfully mitigates the incompatibility,
yielding discriminative and transferable representations. Noticeable
improvements are achieved on the image classification task and challenging
transfer learning tasks. We hope that this work will raise the significance of
the transferability property in the conventional supervised learning setting.
Code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual anomaly detection in video by variational autoencoder. (arXiv:2203.03872v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03872">
<div class="article-summary-box-inner">
<span><p>Video anomalies detection is the intersection of anomaly detection and visual
intelligence. It has commercial applications in surveillance, security,
self-driving cars and crop monitoring. Videos can capture a variety of
anomalies. Due to efforts needed to label training data, unsupervised
approaches to train anomaly detection models for videos is more practical An
autoencoder is a neural network that is trained to recreate its input using
latent representation of input also called a bottleneck layer. Variational
autoencoder uses distribution (mean and variance) as compared to latent vector
as bottleneck layer and can have better regularization effect. In this paper we
have demonstrated comparison between performance of convolutional LSTM versus a
variation convolutional LSTM autoencoder
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels. (arXiv:2203.03884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03884">
<div class="article-summary-box-inner">
<span><p>The crux of semi-supervised semantic segmentation is to assign adequate
pseudo-labels to the pixels of unlabeled images. A common practice is to select
the highly confident predictions as the pseudo ground-truth, but it leads to a
problem that most pixels may be left unused due to their unreliability. We
argue that every pixel matters to the model training, even its prediction is
ambiguous. Intuitively, an unreliable prediction may get confused among the top
classes (i.e., those with the highest probabilities), however, it should be
confident about the pixel not belonging to the remaining classes. Hence, such a
pixel can be convincingly treated as a negative sample to those most unlikely
categories. Based on this insight, we develop an effective pipeline to make
sufficient use of unlabeled data. Concretely, we separate reliable and
unreliable pixels via the entropy of predictions, push each unreliable pixel to
a category-wise queue that consists of negative samples, and manage to train
the model with all candidate pixels. Considering the training evolution, where
the prediction becomes more and more accurate, we adaptively adjust the
threshold for the reliable-unreliable partition. Experimental results on
various benchmarks and training settings demonstrate the superiority of our
approach over the state-of-the-art alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Mask R-CNN Performance for Long, Thin Forensic Traces with Pre-Segmentation and IoU Region Merging. (arXiv:2203.03886v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03886">
<div class="article-summary-box-inner">
<span><p>Mask R-CNN has recently achieved great success in the field of instance
segmentation. However, weaknesses of the algorithm have been repeatedly pointed
out as well, especially in the segmentation of long, sparse objects whose
orientation is not exclusively horizontal or vertical. We present here an
approach that significantly improves the performance of the algorithm by first
pre-segmenting the images with a PSPNet algorithm. To further improve its
prediction, we have developed our own cost functions and heuristics in the form
of training strategies, which can prevent so-called (early) overfitting and
achieve a more targeted convergence. Furthermore, due to the high variance of
the images, especially for PSPNet, we aimed to develop strategies for a high
robustness and generalization, which are also presented here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ART-Point: Improving Rotation Robustness of Point Cloud Classifiers via Adversarial Rotation. (arXiv:2203.03888v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03888">
<div class="article-summary-box-inner">
<span><p>Point cloud classifiers with rotation robustness have been widely discussed
in the 3D deep learning community. Most proposed methods either use rotation
invariant descriptors as inputs or try to design rotation equivariant networks.
However, robust models generated by these methods have limited performance
under clean aligned datasets due to modifications on the original classifiers
or input space. In this study, for the first time, we show that the rotation
robustness of point cloud classifiers can also be acquired via adversarial
training with better performance on both rotated and clean datasets.
Specifically, our proposed framework named ART-Point regards the rotation of
the point cloud as an attack and improves rotation robustness by training the
classifier on inputs with Adversarial RoTations. We contribute an axis-wise
rotation attack that uses back-propagated gradients of the pre-trained model to
effectively find the adversarial rotations. To avoid model over-fitting on
adversarial inputs, we construct rotation pools that leverage the
transferability of adversarial rotations among samples to increase the
diversity of training data. Moreover, we propose a fast one-step optimization
to efficiently reach the final robust model. Experiments show that our proposed
rotation attack achieves a high success rate and ART-Point can be used on most
existing classifiers to improve the rotation robustness while showing better
performance on clean datasets than state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClearPose: Large-scale Transparent Object Dataset and Benchmark. (arXiv:2203.03890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03890">
<div class="article-summary-box-inner">
<span><p>Transparent objects are ubiquitous in household settings and pose distinct
challenges for visual sensing and perception systems. The optical properties of
transparent objects leave conventional 3D sensors alone unreliable for object
depth and pose estimation. These challenges are highlighted by the shortage of
large-scale RGB-Depth datasets focusing on transparent objects in real-world
settings. In this work, we contribute a large-scale real-world RGB-Depth
transparent object dataset named ClearPose to serve as a benchmark dataset for
segmentation, scene-level depth completion and object-centric pose estimation
tasks. The ClearPose dataset contains over 350K labeled real-world RGB-Depth
frames and 4M instance annotations covering 63 household objects. The dataset
includes object categories commonly used in daily life under various lighting
and occluding conditions as well as challenging test scenarios such as cases of
occlusion by opaque or translucent objects, non-planar orientations, presence
of liquids, etc. We benchmark several state-of-the-art depth completion and
object pose estimation deep neural networks on ClearPose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Mixup for Robust Fine-tuning. (arXiv:2203.03897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03897">
<div class="article-summary-box-inner">
<span><p>Pre-trained large-scale models provide a transferable embedding, and they
show comparable performance on the diverse downstream task. However, the
transferability of multi-modal learning is restricted, and the analysis of
learned embedding has not been explored well. This paper provides a perspective
to understand the multi-modal embedding in terms of uniformity and alignment.
We newly find that the representation learned by multi-modal learning models
such as CLIP has a two separated representation space for each heterogeneous
dataset with less alignment. Besides, there are unexplored large intermediate
areas between two modalities with less uniformity. Less robust embedding might
restrict the transferability of the representation for the downstream task.
This paper provides a new end-to-end fine-tuning method for robust
representation that encourages better uniformity and alignment score. First, we
propose a multi-modal Mixup, $m^{2}$-Mix that mixes the representation of image
and text to generate the hard negative samples. Second, we fine-tune the
multi-modal model on a hard negative sample as well as normal negative and
positive samples with contrastive learning. Our multi-modal Mixup provides a
robust representation, and we validate our methods on classification,
retrieval, and structure-awareness task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end system for object detection from sub-sampled radar data. (arXiv:2203.03905v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03905">
<div class="article-summary-box-inner">
<span><p>Robust and accurate sensing is of critical importance for advancing
autonomous automotive systems. The need to acquire situational awareness in
complex urban conditions using sensors such as radar has motivated research on
power and latency-efficient signal acquisition methods. In this paper, we
present an end-to-end signal processing pipeline, capable of operating in
extreme weather conditions, that relies on sub-sampled radar data to perform
object detection in vehicular settings. The results of the object detection are
further utilized to sub-sample forthcoming radar data, which stands in contrast
to prior work where the sub-sampling relies on image information. We show
robust detection based on radar data reconstructed using 20% of samples under
extreme weather conditions such as snow or fog, and on low-illuminated nights.
Additionally, we generate 20% sampled radar data in a fine-tuning set and show
1.1% gain in AP50 across scenes and 3% AP50 gain in motorway condition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Matters: A Weakly Supervised Pre-training Approach for Scene Text Detection and Spotting. (arXiv:2203.03911v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03911">
<div class="article-summary-box-inner">
<span><p>Recently, Vision-Language Pre-training (VLP) techniques have greatly
benefited various vision-language tasks by jointly learning visual and textual
representations, which intuitively helps in Optical Character Recognition (OCR)
tasks due to the rich visual and textual information in scene text images.
However, these methods cannot well cope with OCR tasks because of the
difficulty in both instance-level text encoding and image-text pair acquisition
(i.e. images and captured texts in them). This paper presents a weakly
supervised pre-training method that can acquire effective scene text
representations by jointly learning and aligning visual and textual
information. Our network consists of an image encoder and a character-aware
text encoder that extract visual and textual features, respectively, as well as
a visual-textual decoder that models the interaction among textual and visual
features for learning effective scene text representations. With the learning
of textual features, the pre-trained model can attend texts in images well with
character awareness. Besides, these designs enable the learning from weakly
annotated texts (i.e. partial texts in images without text bounding boxes)
which mitigates the data annotation constraint greatly. Experiments over the
weakly annotated images in ICDAR2019-LSVT show that our pre-trained model
improves F-score by +2.5% and +4.8% while transferring its weights to other
text detection and spotting networks, respectively. In addition, the proposed
method outperforms existing pre-training techniques consistently across
multiple public datasets (e.g., +3.2% and +1.3% for Total-Text and CTW1500).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globally-Optimal Event Camera Motion Estimation. (arXiv:2203.03914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03914">
<div class="article-summary-box-inner">
<span><p>Event cameras are bio-inspired sensors that perform well in HDR conditions
and have high temporal resolution. However, different from traditional
frame-based cameras, event cameras measure asynchronous pixel-level brightness
changes and return them in a highly discretised format, hence new algorithms
are needed. The present paper looks at fronto-parallel motion estimation of an
event camera. The flow of the events is modeled by a general homographic
warping in a space-time volume, and the objective is formulated as a
maximisation of contrast within the image of unwarped events. However, in stark
contrast to prior art, we derive a globally optimal solution to this generally
non-convex problem, and thus remove the dependency on a good initial guess. Our
algorithm relies on branch-and-bound optimisation for which we derive novel,
recursive upper and lower bounds for six different contrast estimation
functions. The practical validity of our approach is supported by a highly
successful application to AGV motion estimation with a downward facing event
camera, a challenging scenario in which the sensor experiences fronto-parallel
motion in front of noisy, fast moving textures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Part-Aware Self-Supervised Pre-Training for Person Re-Identification. (arXiv:2203.03931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03931">
<div class="article-summary-box-inner">
<span><p>In person re-identification (ReID), very recent researches have validated
pre-training the models on unlabelled person images is much better than on
ImageNet. However, these researches directly apply the existing self-supervised
learning (SSL) methods designed for image classification to ReID without any
adaption in the framework. These SSL methods match the outputs of local views
(e.g., red T-shirt, blue shorts) to those of the global views at the same time,
losing lots of details. In this paper, we propose a ReID-specific pre-training
method, Part-Aware Self-Supervised pre-training (PASS), which can generate
part-level features to offer fine-grained information and is more suitable for
ReID. PASS divides the images into several local areas, and the local views
randomly cropped from each area are assigned with a specific learnable [PART]
token. On the other hand, the [PART]s of all local areas are also appended to
the global views. PASS learns to match the output of the local views and global
views on the same [PART]. That is, the learned [PART] of the local views from a
local area is only matched with the corresponding [PART] learned from the
global views. As a result, each [PART] can focus on a specific local area of
the image and extracts fine-grained information of this area. Experiments show
PASS sets the new state-of-the-art performances on Market1501 and MSMT17 on
various ReID tasks, e.g., vanilla ViT-S/16 pre-trained by PASS achieves
92.2\%/90.2\%/88.5\% mAP accuracy on Market1501 for supervised/UDA/USL ReID.
Our codes are available at https://github.com/CASIA-IVA-Lab/PASS-reID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03937">
<div class="article-summary-box-inner">
<span><p>Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by each query
attending to all keys/values, various methods have constrained the range of
attention within local regions, where each query only attends to keys/values
within a hand-crafted window. However, these hand-crafted window partition
mechanisms are data-agnostic and ignore their input content, so it is likely
that one query maybe attends to irrelevant keys/values. To address this issue,
we propose a Dynamic Group Attention (DG-Attention), which dynamically divides
all queries into multiple groups and selects the most relevant keys/values for
each group. Our DG-Attention can flexibly model more relevant dependencies
without any spatial constraint that is used in hand-crafted window based
attention. Built on the DG-Attention, we develop a general vision transformer
backbone named Dynamic Group Transformer (DGT). Extensive experiments show that
our models can outperform the state-of-the-art methods on multiple common
vision tasks, including image classification, semantic segmentation, object
detection, and instance segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Online Semantic Mapping System for Extending and Enhancing Visual SLAM. (arXiv:2203.03944v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03944">
<div class="article-summary-box-inner">
<span><p>We present a real-time semantic mapping approach for mobile vision systems
with a 2D to 3D object detection pipeline and rapid data association for
generated landmarks. Besides the semantic map enrichment the associated
detections are further introduced as semantic constraints into a simultaneous
localization and mapping (SLAM) system for pose correction purposes. This way,
we are able generate additional meaningful information that allows to achieve
higher-level tasks, while simultaneously leveraging the view-invariance of
object detections to improve the accuracy and the robustness of the odometry
estimation. We propose tracklets of locally associated object observations to
handle ambiguous and false predictions and an uncertainty-based greedy
association scheme for an accelerated processing time. Our system reaches
real-time capabilities with an average iteration duration of 65~ms and is able
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a
public dataset. Additionally, we implemented our approach as a modular ROS
package that makes it straightforward for integration in arbitrary graph-based
SLAM methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering. (arXiv:2203.03949v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03949">
<div class="article-summary-box-inner">
<span><p>Finding accurate correspondences among different views is the Achilles' heel
of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the
assumption that corresponding pixels share similar photometric features.
However, multi-view images in real scenarios observe non-Lambertian surfaces
and experience occlusions. In this work, we propose a novel approach with
neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences
among views. Specifically, we impose a depth rendering consistency loss to
constrain the geometry features close to the object surface to alleviate
occlusions. Concurrently, we introduce a reference view synthesis loss to
generate consistent supervision, even for non-Lambertian surfaces. Extensive
experiments on DTU and Tanks\&amp;Temples benchmarks demonstrate that our RC-MVSNet
approach achieves state-of-the-art performance over unsupervised MVS frameworks
and competitive performance to many supervised methods.The trained models and
code will be released at https://github.com/Boese0601/RC-MVSNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient and Accurate Hyperspectral Pansharpening Using 3D VolumeNet and 2.5D Texture Transfer. (arXiv:2203.03951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03951">
<div class="article-summary-box-inner">
<span><p>Recently, convolutional neural networks (CNN) have obtained promising results
in single-image SR for hyperspectral pansharpening. However, enhancing CNNs'
representation ability with fewer parameters and a shorter prediction time is a
challenging and critical task. In this paper, we propose a novel multi-spectral
image fusion method using a combination of the previously proposed 3D CNN model
VolumeNet and 2.5D texture transfer method using other modality high resolution
(HR) images. Since a multi-spectral (MS) image consists of several bands and
each band is a 2D image slice, MS images can be seen as 3D data. Thus, we use
the previously proposed VolumeNet to fuse HR panchromatic (PAN) images and
bicubic interpolated MS images. Because the proposed 3D VolumeNet can
effectively improve the accuracy by expanding the receptive field of the model,
and due to its lightweight structure, we can achieve better performance against
the existing method without purchasing a large number of remote sensing images
for training. In addition, VolumeNet can restore the high-frequency information
lost in the HR MR image as much as possible, reducing the difficulty of feature
extraction in the following step: 2.5D texture transfer. As one of the latest
technologies, deep learning-based texture transfer has been demonstrated to
effectively and efficiently improve the visual performance and quality
evaluation indicators of image reconstruction. Different from the texture
transfer processing of RGB image, we use HR PAN images as the reference images
and perform texture transfer for each frequency band of MS images, which is
named 2.5D texture transfer. The experimental results show that the proposed
method outperforms the existing methods in terms of objective accuracy
assessment, method efficiency, and visual subjective evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers. (arXiv:2203.03952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03952">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers started to show impressive results which
outperform large convolution based models significantly. However, in the area
of small models for mobile or resource constrained devices, ConvNet still has
its own advantages in both performance and model complexity. We propose
EdgeFormer, a pure ConvNet based backbone model that further strengthens these
advantages by fusing the merits of vision transformers into ConvNets.
Specifically, we propose global circular convolution (GCC) with position
embeddings, a light-weight convolution op which boasts a global receptive field
while producing location sensitive features as in local convolutions. We
combine the GCCs and squeeze-exictation ops to form a meta-former like model
block, which further has the attention mechanism like transformers. The
aforementioned block can be used in plug-and-play manner to replace relevant
blocks in ConvNets or transformers. Experiment results show that the proposed
EdgeFormer achieves better performance than popular light-weight ConvNets and
vision transformer based models in common vision tasks and datasets, while
having fewer parameters and faster inference speed. For classification on
ImageNet-1k, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million
parameters, saving 11% parameters and 13% computational cost but gaining 0.2%
higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288)
compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7%
accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC
segmentation tasks, EdgeFormer also shows better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Cooperative Learning for Unsupervised Video Anomaly Detection. (arXiv:2203.03962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03962">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection is well investigated in weakly-supervised and
one-class classification (OCC) settings. However, unsupervised video anomaly
detection methods are quite sparse, likely because anomalies are less frequent
in occurrence and usually not well-defined, which when coupled with the absence
of ground truth supervision, could adversely affect the performance of the
learning algorithms. This problem is challenging yet rewarding as it can
completely eradicate the costs of obtaining laborious annotations and enable
such systems to be deployed without human intervention. To this end, we propose
a novel unsupervised Generative Cooperative Learning (GCL) approach for video
anomaly detection that exploits the low frequency of anomalies towards building
a cross-supervision between a generator and a discriminator. In essence, both
networks get trained in a cooperative fashion, thereby allowing unsupervised
learning. We conduct extensive experiments on two large-scale video anomaly
detection datasets, UCF crime, and ShanghaiTech. Consistent improvement over
the existing state-of-the-art unsupervised and OCC methods corroborate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaitStrip: Gait Recognition via Effective Strip-based Feature Representations and Multi-Level Framework. (arXiv:2203.03966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03966">
<div class="article-summary-box-inner">
<span><p>Many gait recognition methods first partition the human gait into N-parts and
then combine them to establish part-based feature representations. Their gait
recognition performance is often affected by partitioning strategies, which are
empirically chosen in different datasets. However, we observe that strips as
the basic component of parts are agnostic against different partitioning
strategies. Motivated by this observation, we present a strip-based multi-level
gait recognition network, named GaitStrip, to extract comprehensive gait
information at different levels. To be specific, our high-level branch explores
the context of gait sequences and our low-level one focuses on detailed posture
changes. We introduce a novel StriP-Based feature extractor (SPB) to learn the
strip-based feature representations by directly taking each strip of the human
body as the basic unit. Moreover, we propose a novel multi-branch structure,
called Enhanced Convolution Module (ECM), to extract different representations
of gaits. ECM consists of the Spatial-Temporal feature extractor (ST), the
Frame-Level feature extractor (FL) and SPB, and has two obvious advantages:
First, each branch focuses on a specific representation, which can be used to
improve the robustness of the network. Specifically, ST aims to extract
spatial-temporal features of gait sequences, while FL is used to generate the
feature representation of each frame. Second, the parameters of the ECM can be
reduced in test by introducing a structural re-parameterization technique.
Extensive experimental results demonstrate that our GaitStrip achieves
state-of-the-art performance in both normal walking and complex conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Generalizing Beyond Domains in Cross-Domain Continual Learning. (arXiv:2203.03970v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03970">
<div class="article-summary-box-inner">
<span><p>Humans have the ability to accumulate knowledge of new tasks in varying
conditions, but deep neural networks often suffer from catastrophic forgetting
of previously learned knowledge after learning a new task. Many recent methods
focus on preventing catastrophic forgetting under the assumption of train and
test data following similar distributions. In this work, we consider a more
realistic scenario of continual learning under domain shifts where the model
must generalize its inference to an unseen domain. To this end, we encourage
learning semantically meaningful features by equipping the classifier with
class similarity metrics as learning parameters which are obtained through
Mahalanobis similarity computations. Learning of the backbone representation
along with these extra parameters is done seamlessly in an end-to-end manner.
In addition, we propose an approach based on the exponential moving average of
the parameters for better knowledge distillation. We demonstrate that, to a
great extent, existing continual learning algorithms fail to handle the
forgetting issue under multiple distributions, while our proposed approach
learns new tasks under domain shift with accuracy boosts up to 10% on
challenging datasets such as DomainNet and OfficeHome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Prototype Transport for Zero-Shot Action Recognition and Localization. (arXiv:2203.03971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03971">
<div class="article-summary-box-inner">
<span><p>This work addresses the problem of recognizing action categories in videos
for which no training examples are available. The current state-of-the-art
enables such a zero-shot recognition by learning universal mappings from videos
to a shared semantic space, either trained on large-scale seen actions or on
objects. While effective, we find that universal action and object mappings are
biased to their seen categories. Such biases are further amplified due to
biases between seen and unseen categories in the semantic space. The
compounding biases result in many unseen action categories simply never being
selected during inference, hampering zero-shot progress. We seek to address
this limitation and introduce universal prototype transport for zero-shot
action recognition. The main idea is to re-position the semantic prototypes of
unseen actions through transduction, i.e. by using the distribution of the
unlabelled test set. For universal action models, we first seek to find a
hyperspherical optimal transport mapping from unseen action prototypes to the
set of all projected test videos. We then define a target prototype for each
unseen action as the weighted Fr\'echet mean over the transport couplings.
Equipped with a target prototype, we propose to re-position unseen action
prototypes along the geodesic spanned by the original and target prototypes,
acting as a form of semantic regularization. For universal object models, we
outline a variant that defines target prototypes based on an optimal transport
between unseen action prototypes and semantic object prototypes. Empirically,
we show that universal prototype transport diminishes the biased selection of
unseen action prototypes and boosts both universal action and object models,
resulting in state-of-the-art performance for zero-shot classification and
spatio-temporal localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaitEdge: Beyond Plain End-to-end Gait Recognition for Better Practicality. (arXiv:2203.03972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03972">
<div class="article-summary-box-inner">
<span><p>Gait is one of the most promising biometrics to identify individuals at a
long distance. Although most previous methods have focused on recognizing the
silhouettes, several end-to-end methods that extract gait features directly
from RGB images perform better. However, we argue that these end-to-end methods
inevitably suffer from the gait-unrelated noises, i.e., low-level texture and
colorful information. Experimentally, we design both the cross-domain
evaluation and visualization to stand for this view. In this work, we propose a
novel end-to-end framework named GaitEdge which can effectively block
gait-unrelated information and release end-to-end training potential.
Specifically, GaitEdge synthesizes the output of the pedestrian segmentation
network and then feeds it to the subsequent recognition network, where the
synthetic silhouettes consist of trainable edges of bodies and fixed interiors
to limit the information that the recognition network receives. Besides,
GaitAlign for aligning silhouettes is embedded into the GaitEdge without loss
of differentiability. Experimental results on CASIA-B and our newly built
TTG-200 indicate that GaitEdge significantly outperforms the previous methods
and provides a more practical end-to-end paradigm for blocking RGB noises
effectively. All the source code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Multiple Instance Learning with Gradient Accumulation. (arXiv:2203.03981v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03981">
<div class="article-summary-box-inner">
<span><p>Being able to learn on weakly labeled data, and provide interpretability, are
two of the main reasons why attention-based deep multiple instance learning
(ABMIL) methods have become particularly popular for classification of
histopathological images. Such image data usually come in the form of
gigapixel-sized whole-slide-images (WSI) that are cropped into smaller patches
(instances). However, the sheer size of the data makes training of ABMIL models
challenging. All the instances from one WSI cannot be processed at once by
conventional GPUs. Existing solutions compromise training by relying on
pre-trained models, strategic sampling or selection of instances, or
self-supervised learning. We propose a training strategy based on gradient
accumulation that enables direct end-to-end training of ABMIL models without
being limited by GPU memory. We conduct experiments on both QMNIST and
Imagenette to investigate the performance and training time, and compare with
the conventional memory-expensive baseline and a recent sampled-based approach.
This memory-efficient approach, although slower, reaches performance
indistinguishable from the memory-expensive baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild. (arXiv:2203.03984v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03984">
<div class="article-summary-box-inner">
<span><p>Talking face generation with great practical significance has attracted more
attention in recent audio-visual studies. How to achieve accurate lip
synchronization is a long-standing challenge to be further investigated.
Motivated by xxx, in this paper, an AttnWav2Lip model is proposed by
incorporating spatial attention module and channel attention module into
lip-syncing strategy. Rather than focusing on the unimportant regions of the
face image, the proposed AttnWav2Lip model is able to pay more attention on the
lip region reconstruction. To our limited knowledge, this is the first attempt
to introduce attention mechanism to the scheme of talking face generation. An
extensive experiments have been conducted to evaluate the effectiveness of the
proposed model. Compared to the baseline measured by LSE-D and LSE-C metrics, a
superior performance has been demonstrated on the benchmark lip synthesis
datasets, including LRW, LRS2 and LRS3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimpleTrack: Rethinking and Improving the JDE Approach for Multi-Object Tracking. (arXiv:2203.03985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03985">
<div class="article-summary-box-inner">
<span><p>Joint detection and embedding (JDE) based methods usually estimate bounding
boxes and embedding features of objects with a single network in Multi-Object
Tracking (MOT). In the tracking stage, JDE-based methods fuse the target motion
information and appearance information by applying the same rule, which could
fail when the target is briefly lost or blocked. To overcome this problem, we
propose a new association matrix, the Embedding and Giou matrix, which combines
embedding cosine distance and Giou distance of objects. To further improve the
performance of data association, we develop a simple, effective tracker named
SimpleTrack, which designs a bottom-up fusion method for Re-identity and
proposes a new tracking strategy based on our EG matrix. The experimental
results indicate that SimpleTrack has powerful data association capability,
e.g., 61.6 HOTA and 76.3 IDF1 on MOT17. In addition, we apply the EG matrix to
5 different state-of-the-art JDE-based methods and achieve significant
improvements in IDF1, HOTA and IDsw metrics, and increase the tracking speed of
these methods by about 20%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skating-Mixer: Multimodal MLP for Scoring Figure Skating. (arXiv:2203.03990v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03990">
<div class="article-summary-box-inner">
<span><p>Figure skating scoring is a challenging task because it requires judging
players' technical moves as well as coordination with the background music.
Prior learning-based work cannot solve it well for two reasons: 1) each move in
figure skating changes quickly, hence simply applying traditional frame
sampling will lose a lot of valuable information, especially in a 3-5 minutes
lasting video, so an extremely long-range representation learning is necessary;
2) prior methods rarely considered the critical audio-visual relationship in
their models. Thus, we introduce a multimodal MLP architecture, named
Skating-Mixer. It extends the MLP-Mixer-based framework into a multimodal
fashion and effectively learns long-term representations through our designed
memory recurrent unit (MRU). Aside from the model, we also collected a
high-quality audio-visual FS1000 dataset, which contains over 1000 videos on 8
types of programs with 7 different rating metrics, overtaking other datasets in
both quantity and diversity. Experiments show the proposed method outperforms
SOTAs over all major metrics on the public Fis-V and our FS1000 dataset. In
addition, we include an analysis applying our method to recent competitions
that occurred in Beijing 2022 Winter Olympic Games, proving our method has
strong robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeltaCNN: End-to-End CNN Inference of Sparse Frame Differences in Videos. (arXiv:2203.03996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03996">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network inference on video data requires powerful
hardware for real-time processing. Given the inherent coherence across
consecutive frames, large parts of a video typically change little. By skipping
identical image regions and truncating insignificant pixel updates,
computational redundancy can in theory be reduced significantly. However, these
theoretical savings have been difficult to translate into practice, as sparse
updates hamper computational consistency and memory access coherence; which are
key for efficiency on real hardware. With DeltaCNN, we present a sparse
convolutional neural network framework that enables sparse frame-by-frame
updates to accelerate video inference in practice. We provide sparse
implementations for all typical CNN layers and propagate sparse feature updates
end-to-end - without accumulating errors over time. DeltaCNN is applicable to
all convolutional neural networks without retraining. To the best of our
knowledge, we are the first to significantly outperform the dense reference,
cuDNN, in practical settings, achieving speedups of up to 7x with only marginal
differences in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration. (arXiv:2203.04006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04006">
<div class="article-summary-box-inner">
<span><p>Vision-language navigation (VLN) is a challenging task due to its large
searching space in the environment. To address this problem, previous works
have proposed some methods of fine-tuning a large model that pretrained on
large-scale datasets. However, the conventional fine-tuning methods require
extra human-labeled navigation data and lack self-exploration capabilities in
environments, which hinders their generalization of unseen scenes. To improve
the ability of fast cross-domain adaptation, we propose Prompt-based
Environmental Self-exploration (ProbES), which can self-explore the
environments by sampling trajectories and automatically generates structured
instructions via a large-scale cross-modal pretrained model (CLIP). Our method
fully utilizes the knowledge learned from CLIP to build an in-domain dataset by
self-exploration without human labeling. Unlike the conventional approach of
fine-tuning, we introduce prompt-based learning to achieve fast adaptation for
language embeddings, which substantially improves the learning efficiency by
leveraging prior knowledge. By automatically synthesizing
trajectory-instruction pairs in any environment without human supervision and
efficient prompt-based learning, our model can adapt to diverse vision-language
navigation tasks, including VLN and REVERIE. Both qualitative and quantitative
results show that our ProbES significantly improves the generalization ability
of the navigation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuMLP-Pin: A Dual-MLP-dot-product Permutation-invariant Network for Set Feature Extraction. (arXiv:2203.04007v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04007">
<div class="article-summary-box-inner">
<span><p>Existing permutation-invariant methods can be divided into two categories
according to the aggregation scope, i.e. global aggregation and local one.
Although the global aggregation methods, e. g., PointNet and Deep Sets, get
involved in simpler structures, their performance is poorer than the local
aggregation ones like PointNet++ and Point Transformer. It remains an open
problem whether there exists a global aggregation method with a simple
structure, competitive performance, and even much fewer parameters. In this
paper, we propose a novel global aggregation permutation-invariant network
based on dual MLP dot-product, called DuMLP-Pin, which is capable of being
employed to extract features for set inputs, including unordered or
unstructured pixel, attribute, and point cloud data sets. We strictly prove
that any permutation-invariant function implemented by DuMLP-Pin can be
decomposed into two or more permutation-equivariant ones in a dot-product way
as the cardinality of the given input set is greater than a threshold. We also
show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints
under certain conditions. The performance of DuMLP-Pin is evaluated on several
different tasks with diverse data sets. The experimental results demonstrate
that our DuMLP-Pin achieves the best results on the two classification problems
for pixel sets and attribute sets. On both the point cloud classification and
the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far
best-performing local aggregation method with only a 1-2% difference, while the
number of required parameters is significantly reduced by more than 85% in
classification and 69% in segmentation, respectively. The code is publicly
available on https://github.com/JaronTHU/DuMLP-Pin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evolutionary Neural Cascade Search across Supernetworks. (arXiv:2203.04011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04011">
<div class="article-summary-box-inner">
<span><p>To achieve excellent performance with modern neural networks, having the
right network architecture is important. Neural Architecture Search (NAS)
concerns the automatic discovery of task-specific network architectures. Modern
NAS approaches leverage supernetworks whose subnetworks encode candidate neural
network architectures. These subnetworks can be trained simultaneously,
removing the need to train each network from scratch, thereby increasing the
efficiency of NAS. A recent method called Neural Architecture Transfer (NAT)
further improves the efficiency of NAS for computer vision tasks by using a
multi-objective evolutionary algorithm to find high-quality subnetworks of a
supernetwork pretrained on ImageNet. Building upon NAT, we introduce ENCAS -
Evolutionary Neural Cascade Search. ENCAS can be used to search over multiple
pretrained supernetworks to achieve a trade-off front of cascades of different
neural network architectures, maximizing accuracy while minimizing FLOPS count.
We test ENCAS on common computer vision benchmarks (CIFAR-10, CIFAR-100,
ImageNet) and achieve Pareto dominance over previous state-of-the-art NAS
models up to 1.5 GFLOPS. Additionally, applying ENCAS to a pool of 518 publicly
available ImageNet classifiers leads to Pareto dominance in all computation
regimes and to increasing the maximum accuracy from 88.6% to 89.0%, accompanied
by an 18\% decrease in computation effort from 362 to 296 GFLOPS. Our code is
available at https://github.com/AwesomeLemon/ENCAS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Contrastive Learning to Disentangle Whole Slide Image Representations for Glioma Grading. (arXiv:2203.04013v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04013">
<div class="article-summary-box-inner">
<span><p>Whole slide images (WSI) provide valuable phenotypic information for
histological assessment and malignancy grading of tumors. The WSI-based
computational pathology promises to provide rapid diagnostic support and
facilitate digital health. The most commonly used WSI are derived from
formalin-fixed paraffin-embedded (FFPE) and frozen sections. Currently, the
majority of automatic tumor grading models are developed based on FFPE
sections, which could be affected by the artifacts introduced by tissue
processing. Here we propose a mutual contrastive learning scheme to integrate
FFPE and frozen sections and disentangle cross-modality representations for
glioma grading. We first design a mutual learning scheme to jointly optimize
the model training based on FFPE and frozen sections. Further, we develop a
multi-modality domain alignment mechanism to ensure semantic consistency in the
backbone model training. We finally design a sphere normalized
temperature-scaled cross-entropy loss (NT-Xent), which could promote
cross-modality representation disentangling of FFPE and frozen sections. Our
experiments show that the proposed scheme achieves better performance than the
model trained based on each single modality or mixed modalities. The sphere
NT-Xent loss outperforms other typical metrics loss functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data augmentation with mixtures of max-entropy transformations for filling-level classification. (arXiv:2203.04027v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04027">
<div class="article-summary-box-inner">
<span><p>We address the problem of distribution shifts in test-time data with a
principled data augmentation scheme for the task of content-level
classification. In such a task, properties such as shape or transparency of
test-time containers (cup or drinking glass) may differ from those represented
in the training data. Dealing with such distribution shifts using standard
augmentation schemes is challenging and transforming the training images to
cover the properties of the test-time instances requires sophisticated image
manipulations. We therefore generate diverse augmentations using a family of
max-entropy transformations that create samples with new shapes, colors and
spectral characteristics. We show that such a principled augmentation scheme,
alone, can replace current approaches that use transfer learning or can be used
in combination with transfer learning to improve its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stage-Aware Feature Alignment Network for Real-Time Semantic Segmentation of Street Scenes. (arXiv:2203.04031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04031">
<div class="article-summary-box-inner">
<span><p>Over the past few years, deep convolutional neural network-based methods have
made great progress in semantic segmentation of street scenes. Some recent
methods align feature maps to alleviate the semantic gap between them and
achieve high segmentation accuracy. However, they usually adopt the feature
alignment modules with the same network configuration in the decoder and thus
ignore the different roles of stages of the decoder during feature aggregation,
leading to a complex decoder structure. Such a manner greatly affects the
inference speed. In this paper, we present a novel Stage-aware Feature
Alignment Network (SFANet) based on the encoder-decoder structure for real-time
semantic segmentation of street scenes. Specifically, a Stage-aware Feature
Alignment module (SFA) is proposed to align and aggregate two adjacent levels
of feature maps effectively. In the SFA, by taking into account the unique role
of each stage in the decoder, a novel stage-aware Feature Enhancement Block
(FEB) is designed to enhance spatial details and contextual information of
feature maps from the encoder. In this way, we are able to address the
misalignment problem with a very simple and efficient multi-branch decoder
structure. Moreover, an auxiliary training strategy is developed to explicitly
alleviate the multi-scale object problem without bringing additional
computational costs during the inference phase. Experimental results show that
the proposed SFANet exhibits a good balance between accuracy and speed for
real-time semantic segmentation of street scenes. In particular, based on
ResNet-18, SFANet respectively obtains 78.1% and 74.7% mean of class-wise
Intersection-over-Union (mIoU) at inference speeds of 37 FPS and 96 FPS on the
challenging Cityscapes and CamVid test datasets by using only a single GTX
1080Ti GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pretrained StyleGAN. (arXiv:2203.04036v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04036">
<div class="article-summary-box-inner">
<span><p>One-shot talking face generation aims at synthesizing a high-quality talking
face video from an arbitrary portrait image, driven by a video or an audio
segment. One challenging quality factor is the resolution of the output video:
higher resolution conveys more details. In this work, we investigate the latent
feature space of a pre-trained StyleGAN and discover some excellent spatial
transformation properties. Upon the observation, we explore the possibility of
using a pre-trained StyleGAN to break through the resolution limit of training
datasets. We propose a novel unified framework based on a pre-trained StyleGAN
that enables a set of powerful functionalities, i.e., high-resolution video
generation, disentangled control by driving video or audio, and flexible face
editing. Our framework elevates the resolution of the synthesized talking face
to 1024*1024 for the first time, even though the training dataset has a lower
resolution. We design a video-based motion generation module and an audio-based
one, which can be plugged into the framework either individually or jointly to
drive the video generation. The predicted motion is used to transform the
latent features of StyleGAN for visual animation. To compensate for the
transformation distortion, we propose a calibration network as well as a domain
loss to refine the features. Moreover, our framework allows two types of facial
editing, i.e., global editing via GAN inversion and intuitive editing based on
3D morphable models. Comprehensive experiments show superior video quality,
flexible controllability, and editability over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multi-Branch Aggregation Network for Real-Time Semantic Segmentation in Street Scenes. (arXiv:2203.04037v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04037">
<div class="article-summary-box-inner">
<span><p>Real-time semantic segmentation, which aims to achieve high segmentation
accuracy at real-time inference speed, has received substantial attention over
the past few years. However, many state-of-the-art real-time semantic
segmentation methods tend to sacrifice some spatial details or contextual
information for fast inference, thus leading to degradation in segmentation
quality. In this paper, we propose a novel Deep Multi-branch Aggregation
Network (called DMA-Net) based on the encoder-decoder structure to perform
real-time semantic segmentation in street scenes. Specifically, we first adopt
ResNet-18 as the encoder to efficiently generate various levels of feature maps
from different stages of convolutions. Then, we develop a Multi-branch
Aggregation Network (MAN) as the decoder to effectively aggregate different
levels of feature maps and capture the multi-scale information. In MAN, a
lattice enhanced residual block is designed to enhance feature representations
of the network by taking advantage of the lattice structure. Meanwhile, a
feature transformation block is introduced to explicitly transform the feature
map from the neighboring branch before feature aggregation. Moreover, a global
context block is used to exploit the global contextual information. These key
components are tightly combined and jointly optimized in a unified network.
Extensive experimental results on the challenging Cityscapes and CamVid
datasets demonstrate that our proposed DMA-Net respectively obtains 77.0% and
73.6% mean Intersection over Union (mIoU) at the inference speed of 46.7 FPS
and 119.8 FPS by only using a single NVIDIA GTX 1080Ti GPU. This shows that
DMA-Net provides a good tradeoff between segmentation quality and speed for
semantic segmentation in street scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gait Recognition with Mask-based Regularization. (arXiv:2203.04038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04038">
<div class="article-summary-box-inner">
<span><p>Most gait recognition methods exploit spatial-temporal representations from
static appearances and dynamic walking patterns. However, we observe that many
part-based methods neglect representations at boundaries. In addition, the
phenomenon of overfitting on training data is relatively common in gait
recognition, which is perhaps due to insufficient data and low-informative gait
silhouettes. Motivated by these observations, we propose a novel mask-based
regularization method named ReverseMask. By injecting perturbation on the
feature map, the proposed regularization method helps convolutional
architecture learn the discriminative representations and enhances
generalization. Also, we design an Inception-like ReverseMask Block, which has
three branches composed of a global branch, a feature dropping branch, and a
feature scaling branch. Precisely, the dropping branch can extract fine-grained
representations when partial activations are zero-outed. Meanwhile, the scaling
branch randomly scales the feature map, keeping structural information of
activations and preventing overfitting. The plug-and-play Inception-like
ReverseMask block is simple and effective to generalize networks, and it also
improves the performance of many state-of-the-art methods. Extensive
experiments demonstrate that the ReverseMask regularization help baseline
achieves higher accuracy and better generalization. Moreover, the baseline with
Inception-like Block significantly outperforms state-of-the-art methods on the
two most popular datasets, CASIA-B and OUMVLP. The source code will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape-invariant 3D Adversarial Point Clouds. (arXiv:2203.04041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04041">
<div class="article-summary-box-inner">
<span><p>Adversary and invisibility are two fundamental but conflict characters of
adversarial perturbations. Previous adversarial attacks on 3D point cloud
recognition have often been criticized for their noticeable point outliers,
since they just involve an "implicit constrain" like global distance loss in
the time-consuming optimization to limit the generated noise. While point cloud
is a highly structured data format, it is hard to metric and constrain its
perturbation with a simple loss properly. In this paper, we propose a novel
Point-Cloud Sensitivity Map to boost both the efficiency and imperceptibility
of point perturbations. This map reveals the vulnerability of point cloud
recognition models when encountering shape-invariant adversarial noises. These
noises are designed along the shape surface with an "explicit constrain"
instead of extra distance loss. Specifically, we first apply a reversible
coordinate transformation on each point of the point cloud input, to reduce one
degree of point freedom and limit its movement on the tangent plane. Then we
calculate the best attacking direction with the gradients of the transformed
point cloud obtained on the white-box model. Finally we assign each point with
a non-negative score to construct the sensitivity map, which benefits both
white-box adversarial invisibility and black-box query-efficiency extended in
our work. Extensive evaluations prove that our method can achieve the superior
performance on various point cloud recognition models, with its satisfying
adversarial imperceptibility and strong resistance to different point cloud
defense settings. Our code is available at: https://github.com/shikiw/SI-Adv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Erase the Bayer-Filter to See in the Dark. (arXiv:2203.04042v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04042">
<div class="article-summary-box-inner">
<span><p>Low-light image enhancement - a pervasive but challenging problem, plays a
central role in enhancing the visibility of an image captured in a poor
illumination environment. Due to the fact that not all photons can pass the
Bayer-Filter on the sensor of the color camera, in this work, we first present
a De-Bayer-Filter simulator based on deep neural networks to generate a
monochrome raw image from the colored raw image. Next, a fully convolutional
network is proposed to achieve the low-light image enhancement by fusing
colored raw data with synthesized monochrome raw data. Channel-wise attention
is also introduced to the fusion process to establish a complementary
interaction between features from colored and monochrome raw images. To train
the convolutional networks, we propose a dataset with monochrome and color raw
pairs named Mono-Colored Raw paired dataset (MCR) collected by using a
monochrome camera without Bayer-Filter and a color camera with Bayer-Filter.
The proposed pipeline take advantages of the fusion of the virtual monochrome
and the color raw images and our extensive experiments indicate that
significant improvement can be achieved by leveraging raw sensor data and
data-driven learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Attention Transformer Network for Multi-Label Image Classification. (arXiv:2203.04049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04049">
<div class="article-summary-box-inner">
<span><p>Multi-label classification aims to recognize multiple objects or attributes
from images. However, it is challenging to learn from proper label graphs to
effectively characterize such inter-label correlations or dependencies. Current
methods often use the co-occurrence probability of labels based on the training
set as the adjacency matrix to model this correlation, which is greatly limited
by the dataset and affects the model's generalization ability. In this paper,
we propose a Graph Attention Transformer Network (GATN), a general framework
for multi-label image classification that can effectively mine complex
inter-label relationships. First, we use the cosine similarity based on the
label word embedding as the initial correlation matrix, which can represent
rich semantic information. Subsequently, we design the graph attention
transformer layer to transfer this adjacency matrix to adapt to the current
domain. Our extensive experiments have demonstrated that our proposed methods
can achieve state-of-the-art performance on three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs. (arXiv:2203.04050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04050">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation in bird's eye view (BEV) is an important task for
autonomous driving. Though this task has attracted a large amount of research
efforts, it is still challenging to flexibly cope with arbitrary (single or
multiple) camera sensors equipped on the autonomous vehicle. In this paper, we
present BEVSegFormer, an effective transformer-based method for BEV semantic
segmentation from arbitrary camera rigs. Specifically, our method first encodes
image features from arbitrary cameras with a shared backbone. These image
features are then enhanced by a deformable transformer-based encoder. Moreover,
we introduce a BEV transformer decoder module to parse BEV semantic
segmentation results. An efficient multi-camera deformable attention unit is
designed to carry out the BEV-to-image view transformation. Finally, the
queries are reshaped according the layout of grids in the BEV, and upsampled to
produce the semantic segmentation result in a supervised manner. We evaluate
the proposed algorithm on the public nuScenes dataset and a self-collected
dataset. Experimental results show that our method achieves promising
performance on BEV semantic segmentation from arbitrary camera rigs. We also
demonstrate the effectiveness of each component via ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counting with Adaptive Auxiliary Learning. (arXiv:2203.04061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04061">
<div class="article-summary-box-inner">
<span><p>This paper proposes an adaptive auxiliary task learning based approach for
object counting problems. Unlike existing auxiliary task learning based
methods, we develop an attention-enhanced adaptively shared backbone network to
enable both task-shared and task-tailored features learning in an end-to-end
manner. The network seamlessly combines standard Convolution Neural Network
(CNN) and Graph Convolution Network (GCN) for feature extraction and feature
reasoning among different domains of tasks. Our approach gains enriched
contextual information by iteratively and hierarchically fusing the features
across different task branches of the adaptive CNN backbone. The whole
framework pays special attention to the objects' spatial locations and varied
density levels, informed by object (or crowd) segmentation and density level
segmentation auxiliary tasks. In particular, thanks to the proposed dilated
contrastive density loss function, our network benefits from individual and
regional context supervision in terms of pixel-independent and pixel-dependent
feature learning mechanisms, along with strengthened robustness. Experiments on
seven challenging multi-domain datasets demonstrate that our method achieves
superior performance to the state-of-the-art auxiliary task learning based
counting methods. Our code is made publicly available at:
https://github.com/smallmax00/Counting_With_Adaptive_Auxiliary
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing General-Purpose Deep-Learning Detection and Segmentation Models with Images from a Lidar as a Camera Sensor. (arXiv:2203.04064v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04064">
<div class="article-summary-box-inner">
<span><p>Over the last decade, robotic perception algorithms have significantly
benefited from the rapid advances in deep learning (DL). Indeed, a significant
amount of the autonomy stack of different commercial and research platforms
relies on DL for situational awareness, especially vision sensors. This work
explores the potential of general-purpose DL perception algorithms,
specifically detection and segmentation neural networks, for processing
image-like outputs of advanced lidar sensors. Rather than processing the
three-dimensional point cloud data, this is, to the best of our knowledge, the
first work to focus on low-resolution images with 360\textdegree field of view
obtained with lidar sensors by encoding either depth, reflectivity, or
near-infrared light in the image pixels. We show that with adequate
preprocessing, general-purpose DL models can process these images, opening the
door to their usage in environmental conditions where vision sensors present
inherent limitations. We provide both a qualitative and quantitative analysis
of the performance of a variety of neural network architectures. We believe
that using DL models built for visual cameras offers significant advantages due
to the much wider availability and maturity compared to point cloud-based
perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lane Detection with Versatile AtrousFormer and Local Semantic Guidance. (arXiv:2203.04067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04067">
<div class="article-summary-box-inner">
<span><p>Lane detection is one of the core functions in autonomous driving and has
aroused widespread attention recently. The networks to segment lane instances,
especially with bad appearance, must be able to explore lane distribution
properties. Most existing methods tend to resort to CNN-based techniques. A few
have a try on incorporating the recent adorable, the seq2seq Transformer
\cite{transformer}. However, their innate drawbacks of weak global information
collection ability and exorbitant computation overhead prohibit a wide range of
the further applications. In this work, we propose Atrous Transformer
(AtrousFormer) to solve the problem. Its variant local AtrousFormer is
interleaved into feature extractor to enhance extraction. Their collecting
information first by rows and then by columns in a dedicated manner finally
equips our network with stronger information gleaning ability and better
computation efficiency. To further improve the performance, we also propose a
local semantic guided decoder to delineate the identities and shapes of lanes
more accurately, in which the predicted Gaussian map of the starting point of
each lane serves to guide the process. Extensive results on three challenging
benchmarks (CULane, TuSimple, and BDD100K) show that our network performs
favorably against the state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2EC: An End-to-End Contour-based Method for High-Quality High-Speed Instance Segmentation. (arXiv:2203.04074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04074">
<div class="article-summary-box-inner">
<span><p>Contour-based instance segmentation methods have developed rapidly recently
but feature rough and hand-crafted front-end contour initialization, which
restricts the model performance, and an empirical and fixed backend
predicted-label vertex pairing, which contributes to the learning difficulty.
In this paper, we introduce a novel contour-based method, named E2EC, for
high-quality instance segmentation. Firstly, E2EC applies a novel learnable
contour initialization architecture instead of hand-crafted contour
initialization. This consists of a contour initialization module for
constructing more explicit learning goals and a global contour deformation
module for taking advantage of all of the vertices' features better. Secondly,
we propose a novel label sampling scheme, named multi-direction alignment, to
reduce the learning difficulty. Thirdly, to improve the quality of the boundary
details, we dynamically match the most appropriate predicted-ground truth
vertex pairs and propose the corresponding loss function named dynamic matching
loss. The experiments showed that E2EC can achieve a state-of-the-art
performance on the KITTI INStance (KINS) dataset, the Semantic Boundaries
Dataset (SBD), the Cityscapes and the COCO dataset. E2EC is also efficient for
use in real-time applications, with an inference speed of 36 fps for 512*512
images on an NVIDIA A6000 GPU. Code will be released at
https://github.com/zhang-tao-whu/e2ec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Distillation Guided Salient Object Detection. (arXiv:2203.04076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04076">
<div class="article-summary-box-inner">
<span><p>Most existing CNN-based salient object detection methods can identify local
segmentation details like hair and animal fur, but often misinterpret the real
saliency due to the lack of global contextual information caused by the
subjectiveness of the SOD task and the locality of convolution layers.
Moreover, due to the unrealistically expensive labeling costs, the current
existing SOD datasets are insufficient to cover the real data distribution. The
limitation and bias of the training data add additional difficulty to fully
exploring the semantic association between object-to-object and
object-to-environment in a given image. In this paper, we propose a semantic
distillation guided SOD (SDG-SOD) method that produces accurate results by
fusing semantically distilled knowledge from generated image captioning into
the Vision-Transformer-based SOD framework. SDG-SOD can better uncover
inter-objects and object-to-environment saliency and cover the gap between the
subjective nature of SOD and its expensive labeling. Comprehensive experiments
on five benchmark datasets demonstrate that the SDG-SOD outperforms the
state-of-the-art approaches on four evaluation metrics, and largely improves
the model performance on DUTS, ECSSD, DUT, HKU-IS, and PASCAL-S datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploration of Various Deep Learning Models for Increased Accuracy in Automatic Polyp Detection. (arXiv:2203.04093v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04093">
<div class="article-summary-box-inner">
<span><p>This paper is created to explore deep learning models and algorithms that
results in highest accuracy in detecting polyp on colonoscopy images. Previous
studies implemented deep learning using convolution neural network (CNN)
algorithm in detecting polyp and non-polyp. Other studies used dropout, and
data augmentation algorithm but mostly not checking the overfitting, thus,
include more than four-layer modelss. Rulei Yu et.al from the Institute of
Software, Chinese Academy of Sciences said that transfer learning is better
talking about performance or improving the previous used algorithm. Most
especially in applying the transfer learning in feature extraction. Series of
experiments were conducted with only a minimum of 4 CNN layers applying
previous used models and identified the model that produce the highest
percentage accuracy of 98% among the other models that apply transfer learning.
Further studies could use different optimizer to a different CNN modelsto
increase accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Enhancement Using Latent Prototype for Few-Shot Segmentation. (arXiv:2203.04095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04095">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation enables the model to recognize unseen classes with few
annotated examples. Most existing methods adopt prototype learning
architecture, where support prototype vectors are expanded and concatenated
with query features to perform conditional segmentation. However, such
framework potentially focuses more on query features while may neglect the
similarity between support and query features. This paper proposes a
contrastive enhancement approach using latent prototypes to leverage latent
classes and raise the utilization of similarity information between prototype
and query features. Specifically, a latent prototype sampling module is
proposed to generate pseudo-mask and novel prototypes based on features
similarity. The module conveniently conducts end-to-end learning and has no
strong dependence on clustering numbers like cluster-based method. Besides, a
contrastive enhancement module is developed to drive models to provide
different predictions with the same query features. Our method can be used as
an auxiliary module to flexibly integrate into other baselines for a better
segmentation performance. Extensive experiments show our approach remarkably
improves the performance of state-of-the-art methods for 1-shot and 5-shot
segmentation, especially outperforming baseline by 5.9% and 7.3% for 5-shot
task on Pascal-5^i and COCO-20^i. Source code is available at
https://github.com/zhaoxiaoyu1995/CELP-Pytorch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer. (arXiv:2203.04099v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04099">
<div class="article-summary-box-inner">
<span><p>This paper presents an audio-visual approach for voice separation which
outperforms state-of-the-art methods at a low latency in two scenarios: speech
and singing voice. The model is based on a two-stage network. Motion cues are
obtained with a lightweight graph convolutional network that processes face
landmarks. Then, both audio and motion features are fed to an audio-visual
transformer which produces a fairly good estimation of the isolated target
source. In a second stage, the predominant voice is enhanced with an audio-only
network. We present different ablation studies and comparison to
state-of-the-art methods. Finally, we explore the transferability of models
trained for speech separation in the task of singing voice separation. The
demos, code, and weights will be made publicly available at
https://ipcv.github.io/VoViT/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing representations of biological data learned with different AI paradigms, augmenting and cropping strategies. (arXiv:2203.04107v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04107">
<div class="article-summary-box-inner">
<span><p>Recent advances in computer vision and robotics enabled automated large-scale
biological image analysis. Various machine learning approaches have been
successfully applied to phenotypic profiling. However, it remains unclear how
they compare in terms of biological feature extraction. In this study, we
propose a simple CNN architecture and implement 4 different representation
learning approaches. We train 16 deep learning setups on the 770k cancer cell
images dataset under identical conditions, using different augmenting and
cropping strategies. We compare the learned representations by evaluating
multiple metrics for each of three downstream tasks: i) distance-based
similarity analysis of known drugs, ii) classification of drugs versus
controls, iii) clustering within cell lines. We also compare training times and
memory usage. Among all tested setups, multi-crops and random augmentations
generally improved performance across tasks, as expected. Strikingly,
self-supervised (implicit contrastive learning) models showed competitive
performance being up to 11 times faster to train. Self-supervised regularized
learning required the most of memory and computation to deliver arguably the
most informative features. We observe that no single combination of augmenting
and cropping strategies consistently results in top performance across tasks
and recommend prospective research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Classifiers by Constructing Familiar Concepts. (arXiv:2203.04109v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04109">
<div class="article-summary-box-inner">
<span><p>Interpreting a large number of neurons in deep learning is difficult. Our
proposed `CLAssifier-DECoder' architecture (ClaDec) facilitates the
understanding of the output of an arbitrary layer of neurons or subsets
thereof. It uses a decoder that transforms the incomprehensible representation
of the given neurons to a representation that is more similar to the domain a
human is familiar with. In an image recognition problem, one can recognize what
information (or concepts) a layer maintains by contrasting reconstructed images
of ClaDec with those of a conventional auto-encoder(AE) serving as reference.
An extension of ClaDec allows trading comprehensibility and fidelity. We
evaluate our approach for image classification using convolutional neural
networks. We show that reconstructed visualizations using encodings from a
classifier capture more relevant classification information than conventional
AEs. This holds although AEs contain more information on the original input.
Our user study highlights that even non-experts can identify a diverse set of
concepts contained in images that are relevant (or irrelevant) for the
classifier. We also compare against saliency based methods that focus on pixel
relevance rather than concepts. We show that ClaDec tends to highlight more
relevant input areas to classification though outcomes depend on classifier
architecture. Code is at \url{https://github.com/JohnTailor/ClaDec}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantification of Occlusion Handling Capability of a 3D Human Pose Estimation Framework. (arXiv:2203.04113v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04113">
<div class="article-summary-box-inner">
<span><p>3D human pose estimation using monocular images is an important yet
challenging task. Existing 3D pose detection methods exhibit excellent
performance under normal conditions however their performance may degrade due
to occlusion. Recently some occlusion aware methods have also been proposed,
however, the occlusion handling capability of these networks has not yet been
thoroughly investigated. In the current work, we propose an occlusion-guided 3D
human pose estimation framework and quantify its occlusion handling capability
by using different protocols. The proposed method estimates more accurate 3D
human poses using 2D skeletons with missing joints as input. Missing joints are
handled by introducing occlusion guidance that provides extra information about
the absence or presence of a joint. Temporal information has also been
exploited to better estimate the missing joints. A large number of experiments
are performed for the quantification of occlusion handling capability of the
proposed method on three publicly available datasets in various settings
including random missing joints, fixed body parts missing, and complete frames
missing, using mean per joint position error criterion. In addition to that,
the quality of the predicted 3D poses is also evaluated using action
classification performance as a criterion. 3D poses estimated by the proposed
method achieved significantly improved action recognition performance in the
presence of missing joints. Our experiments demonstrate the effectiveness of
the proposed framework for handling the missing joints as well as
quantification of the occlusion handling capability of the deep neural
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A study on joint modeling and data augmentation of multi-modalities for audio-visual scene classification. (arXiv:2203.04114v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04114">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose two techniques, namely joint modeling and data
augmentation, to improve system performances for audio-visual scene
classification (AVSC). We employ pre-trained networks trained only on image
data sets to extract video embedding; whereas for audio embedding models, we
decide to train them from scratch. We explore different neural network
architectures for joint modeling to effectively combine the video and audio
modalities. Moreover, data augmentation strategies are investigated to increase
audio-visual training set size. For the video modality the effectiveness of
several operations in RandAugment is verified. An audio-video joint mixup
scheme is proposed to further improve AVSC performances. Evaluated on the
development set of TAU Urban Audio Visual Scenes 2021, our final system can
achieve the best accuracy of 94.2% among all single AVSC systems submitted to
DCASE 2021 Task 1b.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Polyp Segmentation Network. (arXiv:2203.04118v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04118">
<div class="article-summary-box-inner">
<span><p>Cancer is a disease that occurs as a result of uncontrolled division and
proliferation of cells. The number of cancer cases has been on the rise over
the recent years.. Colon cancer is one of the most common types of cancer in
the world. Polyps that can be seen in the large intestine can cause cancer if
not removed with early intervention. Deep learning and image segmentation
techniques are used to minimize the number of polyps that goes unnoticed by the
experts during the diagnosis. Although these techniques give good results, they
require too many parameters. We propose a new model to solve this problem. Our
proposed model includes less parameters as well as outperforming the success of
the state of the art models. In the proposed model, a partial decoder is used
to reduce the number of parameters while maintaning success. EfficientNetB0,
which gives successfull results as well as requiring few parameters, is used in
the encoder part. Since polyps have variable aspect and aspect ratios, an
asymetric convolution block was used instead of using classic convolution
block. Kvasir and CVC-ClinicDB datasets were seperated as training, validation
and testing, and CVC-ColonDB, ETIS and Endoscene datasets were used for
testing. According to the dice metric, our model had the best results with
%71.8 in the ColonDB test dataset, %89.3 in the EndoScene test dataset and
%74.8 in the ETIS test dataset. Our model requires a total of 2.626.337
parameters. When we compare it in the literature, according to similar studies,
the model that requires the least parameters is U-Net++ with 9.042.177
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment. (arXiv:2203.04121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04121">
<div class="article-summary-box-inner">
<span><p>Training a generative adversarial network (GAN) with limited data has been a
challenging task. A feasible solution is to start with a GAN well-trained on a
large scale source domain and adapt it to the target domain with a few samples,
termed as few shot generative model adaption. However, existing methods are
prone to model overfitting and collapse in extremely few shot setting (less
than 10). To solve this problem, we propose a relaxed spatial structural
alignment method to calibrate the target generative models during the adaption.
We design a cross-domain spatial structural consistency loss comprising the
self-correlation and disturbance correlation consistency loss. It helps align
the spatial structural information between the synthesis image pairs of the
source and target domains. To relax the cross-domain alignment, we compress the
original latent space of generative models to a subspace. Image pairs generated
from the subspace are pulled closer. Qualitative and quantitative experiments
show that our method consistently surpasses the state-of-the-art methods in few
shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YouTube-GDD: A challenging gun detection dataset with rich contextual information. (arXiv:2203.04129v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04129">
<div class="article-summary-box-inner">
<span><p>An automatic gun detection system can detect potential gun-related violence
at an early stage that is of paramount importance for citizens security. In the
whole system, object detection algorithm is the key to perceive the environment
so that the system can detect dangerous objects such as pistols and rifles.
However, mainstream deep learning-based object detection algorithms depend
heavily on large-scale high-quality annotated samples, and the existing gun
datasets are characterized by low resolution, little contextual information and
little data volume. To promote the development of security, this work presents
a new challenging dataset called YouTube Gun Detection Dataset (YouTube-GDD).
Our dataset is collected from 343 high-definition YouTube videos and contains
5000 well-chosen images, in which 16064 instances of gun and 9046 instances of
person are annotated. Compared to other datasets, YouTube-GDD is "dynamic",
containing rich contextual information and recording shape changes of the gun
during shooting. To build a baseline for gun detection, we evaluate YOLOv5 on
YouTube-GDD and analyze the influence of additional related annotated
information on gun detection. YouTube-GDD and subsequent updates will be
released at https://github.com/UCAS-GYX/YouTube-GDD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeReF: Neural Refractive Field for Fluid Surface Reconstruction and Implicit Representation. (arXiv:2203.04130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04130">
<div class="article-summary-box-inner">
<span><p>Existing neural reconstruction schemes such as Neural Radiance Field (NeRF)
are largely focused on modeling opaque objects. We present a novel neural
refractive field(NeReF) to recover wavefront of transparent fluids by
simultaneously estimating the surface position and normal of the fluid front.
Unlike prior arts that treat the reconstruction target as a single layer of the
surface, NeReF is specifically formulated to recover a volumetric normal field
with its corresponding density field. A query ray will be refracted by NeReF
according to its accumulated refractive point and normal, and we employ the
correspondences and uniqueness of refracted ray for NeReF optimization. We show
NeReF, as a global optimization scheme, can more robustly tackle refraction
distortions detrimental to traditional methods for correspondence matching.
Furthermore, the continuous NeReF representation of wavefront enables view
synthesis as well as normal integration. We validate our approach on both
synthetic and real data and show it is particularly suitable for sparse
multi-view acquisition. We hence build a small light field array and experiment
on various surface shapes to demonstrate high fidelity NeReF reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04132">
<div class="article-summary-box-inner">
<span><p>Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic motions
and corresponding confidence values for each mode. Our model aims to be tightly
integrated with the robotic planning-control-interaction loop; outputting
physically feasible human motions and being computationally efficient. We
demonstrate the performance of our model on several challenging real-world
motion forecasting datasets, outperforming a wide array of generative methods
while providing state-of-the-art deterministic motions if required. Both using
significantly less computational power than state-of-the art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy Ensemble: Simple Deep Ensemble Learning for Sensor-Based Human Activity Recognition. (arXiv:2203.04153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04153">
<div class="article-summary-box-inner">
<span><p>Sensor-based human activity recognition (HAR) is a paramount technology in
the Internet of Things services. HAR using representation learning, which
automatically learns a feature representation from raw data, is the mainstream
method because it is difficult to interpret relevant information from raw
sensor data to design meaningful features. Ensemble learning is a robust
approach to improve generalization performance; however, deep ensemble learning
requires various procedures, such as data partitioning and training multiple
models, which are time-consuming and computationally expensive. In this study,
we propose Easy Ensemble (EE) for HAR, which enables the easy implementation of
deep ensemble learning in a single model. In addition, we propose input masking
as a method for diversifying the input for EE. Experiments on a benchmark
dataset for HAR demonstrated the effectiveness of EE and input masking and
their characteristics compared with conventional ensemble learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Local Preserving and Global Aligning Network for Adversarial Domain Adaptation. (arXiv:2203.04156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04156">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) requires source domain samples with
clean ground truth labels during training. Accurately labeling a large number
of source domain samples is time-consuming and laborious. An alternative is to
utilize samples with noisy labels for training. However, training with noisy
labels can greatly reduce the performance of UDA. In this paper, we address the
problem that learning UDA models only with access to noisy labels and propose a
novel method called robust local preserving and global aligning network
(RLPGA). RLPGA improves the robustness of the label noise from two aspects. One
is learning a classifier by a robust informative-theoretic-based loss function.
The other is constructing two adjacency weight matrices and two negative weight
matrices by the proposed local preserving module to preserve the local topology
structures of input data. We conduct theoretical analysis on the robustness of
the proposed RLPGA and prove that the robust informative-theoretic-based loss
and the local preserving module are beneficial to reduce the empirical risk of
the target domain. A series of empirical studies show the effectiveness of our
proposed RLPGA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding person identification via gait. (arXiv:2203.04179v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04179">
<div class="article-summary-box-inner">
<span><p>Gait recognition is the process of identifying humans from their bipedal
locomotion such as walking or running. As such gait data is privacy sensitive
information and should be anonymized. With the rise of more and higher quality
gait recording techniques, such as depth cameras or motion capture suits, an
increasing amount of high-quality gait data becomes available which requires
anonymization. As a first step towards developing anonymization techniques for
high-quality gait data, we study different aspects of movement data to quantify
their contribution to the gait recognition process. We first extract categories
of features from the literature on human gait perception and then design
computational experiments for each of the categories which we run against a
gait recognition system. Our results show that gait anonymization is a
challenging process as the data is highly redundant and interdependent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP). (arXiv:2203.04180v1 [math.NA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04180">
<div class="article-summary-box-inner">
<span><p>Purpose: To develop a tuning-free method for multi-coil compressed sensing
MRI that performs competitively with algorithms with an optimally tuned sparse
parameter.
</p>
<p>Theory: The Parallel Variable Density Approximate Message Passing (P-VDAMP)
algorithm is proposed. For Bernoulli random variable density sampling, P-VDAMP
obeys a "state evolution", where the intermediate per-iteration image estimate
is distributed according to the ground truth corrupted by a Gaussian vector
with approximately known covariance. State evolution is leveraged to
automatically tune sparse parameters on-the-fly with Stein's Unbiased Risk
Estimate (SURE).
</p>
<p>Methods: P-VDAMP is evaluated on brain, knee and angiogram datasets at
acceleration factors 5 and 10 and compared with four variants of the Fast
Iterative Shrinkage-Thresholding algorithm (FISTA), including two tuning-free
variants from the literature.
</p>
<p>Results: The proposed method is found to have a similar reconstruction
quality and time to convergence as FISTA with an optimally tuned sparse
weighting.
</p>
<p>Conclusions: P-VDAMP is an efficient, robust and principled method for
on-the-fly parameter tuning that is competitive with optimally tuned FISTA and
offers substantial robustness and reconstruction quality improvements over
competing tuning-free methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective-Supervised Contrastive Learning with Noisy Labels. (arXiv:2203.04181v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04181">
<div class="article-summary-box-inner">
<span><p>Deep networks have strong capacities of embedding data into latent
representations and finishing following tasks. However, the capacities largely
come from high-quality annotated labels, which are expensive to collect. Noisy
labels are more affordable, but result in corrupted representations, leading to
poor generalization performance. To learn robust representations and handle
noisy labels, we propose selective-supervised contrastive learning (Sel-CL) in
this paper. Specifically, Sel-CL extend supervised contrastive learning
(Sup-CL), which is powerful in representation learning, but is degraded when
there are noisy labels. Sel-CL tackles the direct cause of the problem of
Sup-CL. That is, as Sup-CL works in a \textit{pair-wise} manner, noisy pairs
built by noisy labels mislead representation learning. To alleviate the issue,
we select confident pairs out of noisy ones for Sup-CL without knowing noise
rates. In the selection process, by measuring the agreement between learned
representations and given labels, we first identify confident examples that are
exploited to build confident pairs. Then, the representation similarity
distribution in the built confident pairs is exploited to identify more
confident pairs out of noisy pairs. All obtained confident pairs are finally
used for Sup-CL to enhance representations. Experiments on multiple noisy
datasets demonstrate the robustness of the learned representations by our
method, following the state-of-the-art performance. Source codes are available
at https://github.com/ShikunLi/Sel-CL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLSeg: Image and Video Segmentation as Multi-Label Classification and Selected-Label Pixel Classification. (arXiv:2203.04187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04187">
<div class="article-summary-box-inner">
<span><p>For a long period of time, research studies on segmentation have typically
formulated the task as pixel classification that predicts a class for each
pixel from a set of predefined, fixed number of semantic categories. Yet
standard architectures following this formulation will inevitably encounter
various challenges under more realistic settings where the total number of
semantic categories scales up (e.g., beyond $1\rm{k}$ classes). On the other
hand, a standard image or video usually contains only a small number of
semantic categories from the entire label set. Motivated by this intuition, in
this paper, we propose to decompose segmentation into two sub-problems: (i)
image-level or video-level multi-label classification and (ii) pixel-level
selected-label classification. Given an input image or video, our framework
first conducts multi-label classification over the large complete label set and
selects a small set of labels according to the class confidence scores. Then
the follow-up pixel-wise classification is only performed among the selected
subset of labels. Our approach is conceptually general and can be applied to
various existing segmentation frameworks by simply adding a lightweight
multi-label classification branch. We demonstrate the effectiveness of our
framework with competitive experimental results across four tasks including
image semantic segmentation, image panoptic segmentation, video instance
segmentation, and video semantic segmentation. Especially, with our MLSeg,
Mask$2$Former gains +$0.8\%$/+$0.7\%$/+$0.7\%$ on ADE$20$K panoptic
segmentation/YouTubeVIS $2019$ video instance segmentation/VSPW video semantic
segmentation benchmarks respectively. Code will be available
at:https://github.com/openseg-group/MLSeg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Gating Model for Bias Calibration in Generalized Zero-shot Learning. (arXiv:2203.04195v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04195">
<div class="article-summary-box-inner">
<span><p>Generalized zero-shot learning (GZSL) aims at training a model that can
generalize to unseen class data by only using auxiliary information. One of the
main challenges in GZSL is a biased model prediction toward seen classes caused
by overfitting on only available seen class data during training. To overcome
this issue, we propose a two-stream autoencoder-based gating model for GZSL.
Our gating model predicts whether the query data is from seen classes or unseen
classes, and utilizes separate seen and unseen experts to predict the class
independently from each other. This framework avoids comparing the biased
prediction scores for seen classes with the prediction scores for unseen
classes. In particular, we measure the distance between visual and attribute
representations in the latent space and the cross-reconstruction space of the
autoencoder. These distances are utilized as complementary features to
characterize unseen classes at different levels of data abstraction. Also, the
two-stream autoencoder works as a unified framework for the gating model and
the unseen expert, which makes the proposed method computationally efficient.
We validate our proposed method in four benchmark image recognition datasets.
In comparison with other state-of-the-art methods, we achieve the best harmonic
mean accuracy in SUN and AWA2, and the second best in CUB and AWA1.
Furthermore, our base model requires at least 20% less number of model
parameters than state-of-the-art methods relying on generative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trustable Co-label Learning from Multiple Noisy Annotators. (arXiv:2203.04199v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04199">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning depends on massive accurately annotated examples,
which is usually impractical in many real-world scenarios. A typical
alternative is learning from multiple noisy annotators. Numerous earlier works
assume that all labels are noisy, while it is usually the case that a few
trusted samples with clean labels are available. This raises the following
important question: how can we effectively use a small amount of trusted data
to facilitate robust classifier learning from multiple annotators? This paper
proposes a data-efficient approach, called \emph{Trustable Co-label Learning}
(TCL), to learn deep classifiers from multiple noisy annotators when a small
set of trusted data is available. This approach follows the coupled-view
learning manner, which jointly learns the data classifier and the label
aggregator. It effectively uses trusted data as a guide to generate trustable
soft labels (termed co-labels). A co-label learning can then be performed by
alternately reannotating the pseudo labels and refining the classifiers. In
addition, we further improve TCL for a special complete data case, where each
instance is labeled by all annotators and the label aggregator is represented
by multilayer neural networks to enhance model capacity. Extensive experiments
on synthetic and real datasets clearly demonstrate the effectiveness and
robustness of the proposed approach. Source code is available at
https://github.com/ShikunLi/TCL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04203">
<div class="article-summary-box-inner">
<span><p>A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as "how
can I run the microwave for 1 minute?". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos and scripts to guide the user
step-by-step. To support the task, we constructed AssistQ, a new dataset
comprising 529 question-answer samples derived from 100 newly filmed
first-person videos. Each question should be completed with multi-step
guidances by inferring from visual details (e.g., buttons' position) and
textural details (e.g., actions like press/turn). To address this unique task,
we developed a Question-to-Actions (Q2A) model that significantly outperforms
several baseline methods while still having large room for improvement. We
expect our task and dataset to advance Egocentric AI Assistant's development.
Our project page is available at: https://showlab.github.io/assistq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Monocular Depth Estimation through Guided Decoding. (arXiv:2203.04206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04206">
<div class="article-summary-box-inner">
<span><p>We present a lightweight encoder-decoder archi- tecture for monocular depth
estimation, specifically designed for embedded platforms. Our main contribution
is the Guided Upsampling Block (GUB) for building the decoder of our model.
Motivated by the concept of guided image filtering, GUB relies on the image to
guide the decoder on upsampling the feature representation and the depth map
reconstruction, achieving high resolution results with fine-grained details.
Based on multiple GUBs, our model outperforms the related methods on the NYU
Depth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the
NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on
the KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano
and 102.9 fps on the Xavier NX. Our code and models are made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Universal Texture Synthesis by Combining Texton Broadcasting with Noise Injection in StyleGAN-2. (arXiv:2203.04221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04221">
<div class="article-summary-box-inner">
<span><p>We present a new approach for universal texture synthesis by incorporating a
multi-scale texton broadcasting module in the StyleGAN-2 framework. The texton
broadcasting module introduces an inductive bias, enabling generation of
broader range of textures, from those with regular structures to completely
stochastic ones. To train and evaluate the proposed approach, we construct a
comprehensive high-resolution dataset that captures the diversity of natural
textures as well as stochastic variations within each perceptually uniform
texture. Experimental results demonstrate that the proposed approach yields
significantly better quality textures than the state of the art. The ultimate
goal of this work is a comprehensive understanding of texture space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Face Identification in a 2D Wireframe Projection of a Manifold Object. (arXiv:2203.04229v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04229">
<div class="article-summary-box-inner">
<span><p>In computer-aided design (CAD) systems, 2D line drawings are commonly used to
illustrate 3D object designs. To reconstruct the 3D models depicted by a single
2D line drawing, an important key is finding the edge loops in the line drawing
which correspond to the actual faces of the 3D object. In this paper, we
approach the classical problem of face identification from a novel data-driven
point of view. We cast it as a sequence generation problem: starting from an
arbitrary edge, we adopt a variant of the popular Transformer model to predict
the edges associated with the same face in a natural order. This allows us to
avoid searching the space of all possible edge loops with various hand-crafted
rules and heuristics as most existing methods do, deal with challenging cases
such as curved surfaces and nested edge loops, and leverage additional cues
such as face types. We further discuss how possibly imperfect predictions can
be used for 3D object reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Lightweight and Detector-free 3D Single Object Tracker on Point Clouds. (arXiv:2203.04232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04232">
<div class="article-summary-box-inner">
<span><p>Recent works on 3D single object tracking treat the tracking as a
target-specific 3D detection task, where an off-the-shelf 3D detector is
commonly employed for tracking. However, it is non-trivial to perform accurate
target-specific detection since the point cloud of objects in raw LiDAR scans
is usually sparse and incomplete. In this paper, we address this issue by
explicitly leveraging temporal motion cues and propose DMT, a Detector-free
Motion prediction based 3D Tracking network that totally removes the usage of
complicated 3D detectors, which is lighter, faster, and more accurate than
previous trackers. Specifically, the motion prediction module is firstly
introduced to estimate a potential target center of the current frame in a
point-cloud free way. Then, an explicit voting module is proposed to directly
regress the 3D box from the estimated target center. Extensive experiments on
KITTI and NuScenes datasets demonstrate that our DMT, without applying any
complicated 3D detectors, can still achieve better performance (~10%
improvement on the NuScenes dataset) and faster tracking speed (i.e., 72 FPS)
than state-of-the-art approaches. Our codes will be released publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Semi-Supervised Learning for Video Action Detection. (arXiv:2203.04251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04251">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on semi-supervised learning for video action detection
which utilizes both labeled as well as unlabeled data. We propose a simple
end-to-end consistency based approach which effectively utilizes the unlabeled
data. Video action detection requires both, action class prediction as well as
a spatio-temporal localization of actions. Therefore, we investigate two types
of constraints, classification consistency, and spatio-temporal consistency.
The presence of predominant background and static regions in a video makes it
challenging to utilize spatio-temporal consistency for action detection. To
address this, we propose two novel regularization constraints for
spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.
Both these aspects exploit the temporal continuity of action in videos and are
found to be effective for utilizing unlabeled videos for action detection. We
demonstrate the effectiveness of the proposed approach on two different action
detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show
the effectiveness of the proposed approach for video object segmentation on the
Youtube-VOS dataset which demonstrates its generalization capability to other
tasks. The proposed approach achieves competitive performance by using merely
20% of annotations on UCF101-24 when compared with recent fully supervised
methods. On UCF101-24, it improves the score by +8.9% and +11% at 0.5 f-mAP and
v-mAP respectively, compared to supervised approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Multi-Task Learning and Online Refinement for Spacecraft Pose Estimation across Domain Gap. (arXiv:2203.04275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04275">
<div class="article-summary-box-inner">
<span><p>This work presents Spacecraft Pose Network v2 (SPNv2), a Convolutional Neural
Network (CNN) for pose estimation of noncooperative spacecraft across domain
gap. SPNv2 is a multi-scale, multi-task CNN which consists of a shared
multi-scale feature encoder and multiple prediction heads that perform
different tasks on a shared feature output. These tasks are all related to
detection and pose estimation of a target spacecraft from an image, such as
prediction of pre-defined satellite keypoints, direct pose regression, and
binary segmentation of the satellite foreground. It is shown that by jointly
training on different yet related tasks with extensive data augmentations on
synthetic images only, the shared encoder learns features that are common
across image domains that have fundamentally different visual characteristics
compared to synthetic images. This work also introduces Online Domain
Refinement (ODR) which refines the parameters of the normalization layers of
SPNv2 on the target domain images online at deployment. Specifically, ODR
performs self-supervised entropy minimization of the predicted satellite
foreground, thereby improving the CNN's performance on the target domain images
without their pose labels and with minimal computational efforts. The GitHub
repository for SPNv2 will be made available in the near future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences. (arXiv:2203.04279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04279">
<div class="article-summary-box-inner">
<span><p>We propose Probabilistic Warp Consistency, a weakly-supervised learning
objective for semantic matching. Our approach directly supervises the dense
matching scores predicted by the network, encoded as a conditional probability
distribution. We first construct an image triplet by applying a known warp to
one of the images in a pair depicting different instances of the same object
class. Our probabilistic learning objectives are then derived using the
constraints arising from the resulting image triplet. We further account for
occlusion and background clutter present in real image pairs by extending our
probabilistic output space with a learnable unmatched state. To supervise it,
we design an objective between image pairs depicting different object classes.
We validate our method by applying it to four recent semantic matching
architectures. Our weakly-supervised approach sets a new state-of-the-art on
four challenging semantic matching benchmarks. Lastly, we demonstrate that our
objective also brings substantial improvements in the strongly-supervised
regime, when combined with keypoint annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proximal PanNet: A Model-Based Deep Network for Pansharpening. (arXiv:2203.04286v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04286">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning techniques have been extensively studied for
pansharpening, which aims to generate a high resolution multispectral (HRMS)
image by fusing a low resolution multispectral (LRMS) image with a high
resolution panchromatic (PAN) image. However, existing deep learning-based
pansharpening methods directly learn the mapping from LRMS and PAN to HRMS.
These network architectures always lack sufficient interpretability, which
limits further performance improvements. To alleviate this issue, we propose a
novel deep network for pansharpening by combining the model-based methodology
with the deep learning method. Firstly, we build an observation model for
pansharpening using the convolutional sparse coding (CSC) technique and design
a proximal gradient algorithm to solve this model. Secondly, we unfold the
iterative algorithm into a deep network, dubbed as Proximal PanNet, by learning
the proximal operators using convolutional neural networks. Finally, all the
learnable modules can be automatically learned in an end-to-end manner.
Experimental results on some benchmark datasets show that our network performs
better than other advanced methods both quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation. (arXiv:2203.04287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04287">
<div class="article-summary-box-inner">
<span><p>This paper proposes a simple transfer learning baseline for sign language
translation. Existing sign language datasets (e.g. PHOENIX-2014T, CSL-Daily)
contain only about 10K-20K pairs of sign videos, gloss annotations and texts,
which are an order of magnitude smaller than typical parallel data for training
spoken language translation models. Data is thus a bottleneck for training
effective sign language translation models. To mitigate this problem, we
propose to progressively pretrain the model from general-domain datasets that
include a large amount of external supervision to within-domain datasets.
Concretely, we pretrain the sign-to-gloss visual network on the general domain
of human actions and the within-domain of a sign-to-gloss dataset, and pretrain
the gloss-to-text translation network on the general domain of a multilingual
corpus and the within-domain of a gloss-to-text corpus. The joint model is
fine-tuned with an additional module named the visual-language mapper that
connects the two networks. This simple baseline surpasses the previous
state-of-the-art results on two sign language translation benchmarks,
demonstrating the effectiveness of transfer learning. With its simplicity and
strong performance, this approach can serve as a solid baseline for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Semi-Supervised Framework for Automatic Pixel-Wise Breast Cancer Grading of Histological Images. (arXiv:1907.01696v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.01696">
<div class="article-summary-box-inner">
<span><p>Throughout the world, breast cancer is one of the leading causes of female
death. Recently, deep learning methods are developed to automatically grade
breast cancer of histological slides. However, the performance of existing deep
learning models is limited due to the lack of large annotated biomedical
datasets. One promising way to relieve the annotating burden is to leverage the
unannotated datasets to enhance the trained model. In this paper, we first
apply active learning method in breast cancer grading, and propose a
semi-supervised framework based on expectation maximization (EM) model. The
proposed EM approach is based on the collaborative filtering among the
annotated and unannotated datasets. The collaborative filtering method
effectively extracts useful and credible datasets from the unannotated images.
Results of pixel-wise prediction of whole-slide images (WSI) demonstrate that
the proposed method not only outperforms state-of-art methods, but also
significantly reduces the annotation cost by over 70%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Graph Transformer Self-Attention Networks. (arXiv:1909.11855v13 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.11855">
<div class="article-summary-box-inner">
<span><p>We introduce a transformer-based GNN model, named UGformer, to learn graph
representations. In particular, we present two UGformer variants, wherein the
first variant (publicized in September 2019) is to leverage the transformer on
a set of sampled neighbors for each input node, while the second (publicized in
May 2021) is to leverage the transformer on all input nodes. Experimental
results demonstrate that the first UGformer variant achieves state-of-the-art
accuracies on benchmark datasets for graph classification in both inductive
setting and unsupervised transductive setting; and the second UGformer variant
obtains state-of-the-art accuracies for inductive text classification. The code
is available at: \url{https://github.com/daiquocnguyen/Graph-Transformer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geography-Aware Self-Supervised Learning. (arXiv:2011.09980v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09980">
<div class="article-summary-box-inner">
<span><p>Contrastive learning methods have significantly narrowed the gap between
supervised and unsupervised learning on computer vision tasks. In this paper,
we explore their application to geo-located datasets, e.g. remote sensing,
where unlabeled data is often abundant but labeled data is scarce. We first
show that due to their different characteristics, a non-trivial gap persists
between contrastive and supervised learning on standard benchmarks. To close
the gap, we propose novel training methods that exploit the spatio-temporal
structure of remote sensing data. We leverage spatially aligned images over
time to construct temporal positive pairs in contrastive learning and
geo-location to design pre-text tasks. Our experiments show that our proposed
method closes the gap between contrastive and supervised learning on image
classification, object detection and semantic segmentation for remote sensing.
Moreover, we demonstrate that the proposed method can also be applied to
geo-tagged ImageNet images, improving downstream performance on various tasks.
Project Webpage can be found at this link geography-aware-ssl.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forecasting Characteristic 3D Poses of Human Actions. (arXiv:2011.15079v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.15079">
<div class="article-summary-box-inner">
<span><p>We propose the task of forecasting characteristic 3d poses: from a short
sequence observation of a person, predict a future 3d pose of that person in a
likely action-defining, characteristic pose -- for instance, from observing a
person picking up an apple, predict the pose of the person eating the apple.
Prior work on human motion prediction estimates future poses at fixed time
intervals. Although easy to define, this frame-by-frame formulation confounds
temporal and intentional aspects of human action. Instead, we define a
semantically meaningful pose prediction task that decouples the predicted pose
from time, taking inspiration from goal-directed behavior. To predict
characteristic poses, we propose a probabilistic approach that models the
possible multi-modality in the distribution of likely characteristic poses. We
then sample future pose hypotheses from the predicted distribution in an
autoregressive fashion to model dependencies between joints. To evaluate our
method, we construct a dataset of manually annotated characteristic 3d poses.
Our experiments with this dataset suggest that our proposed probabilistic
approach outperforms state-of-the-art methods by 26% on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble deep learning: A review. (arXiv:2104.02395v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02395">
<div class="article-summary-box-inner">
<span><p>Ensemble learning combines several individual models to obtain better
generalization performance. Currently, deep learning models with multilayer
processing architecture is showing better performance as compared to the
shallow or traditional classification models. Deep ensemble learning models
combine the advantages of both the deep learning models as well as the ensemble
learning such that the final model has better generalization performance. This
paper reviews the state-of-art deep ensemble models and hence serves as an
extensive summary for the researchers. The ensemble models are broadly
categorised into ensemble models like bagging, boosting and stacking, negative
correlation based deep ensemble models, explicit/implicit ensembles,
homogeneous /heterogeneous ensemble, decision fusion strategies, unsupervised,
semi-supervised, reinforcement learning and online/incremental, multilabel
based deep ensemble models. Application of deep ensemble models in different
domains is also briefly discussed. Finally, we conclude this paper with some
future recommendations and research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02320">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation aims to train a segmentation model that can fast adapt
to novel classes with few exemplars. The conventional training paradigm is to
learn to make predictions on query images conditioned on the features from
support images. Previous methods only utilized the semantic-level prototypes of
support images as conditional information. These methods cannot utilize all
pixel-wise support information for the query predictions, which is however
critical for the segmentation task. In this paper, we focus on utilizing
pixel-wise relationships between support and query images to facilitate the
few-shot segmentation task. We design a novel Cycle-Consistent TRansformer
(CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR
performs cross-attention between features from different images, i.e. support
and query images. We observe that there may exist unexpected irrelevant
pixel-level support features. Directly performing cross-attention may aggregate
these features from support to query and bias the query features. Thus, we
propose using a novel cycle-consistent attention mechanism to filter out
possible harmful support features and encourage query features to attend to the
most informative pixels from support images. Experiments on all few-shot
segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable
improvement compared to previous state-of-the-art methods. Specifically, on
Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 67.5% and 45.6% mIoU for
5-shot segmentation, outperforming previous state-of-the-art methods by 5.6%
and 7.1% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving Deep into the Generalization of Vision Transformers under Distribution Shifts. (arXiv:2106.07617v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07617">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) have achieved impressive performance on various
vision tasks, yet their generalization under distribution shifts (DS) is rarely
understood. In this work, we comprehensively study the out-of-distribution
(OOD) generalization of ViTs. For systematic investigation, we first present a
taxonomy of DS. We then perform extensive evaluations of ViT variants under
different DS and compare their generalization with Convolutional Neural Network
(CNN) models. Important observations are obtained: 1) ViTs learn weaker biases
on backgrounds and textures, while they are equipped with stronger inductive
biases towards shapes and structures, which is more consistent with human
cognitive traits. Therefore, ViTs generalize better than CNNs under DS. With
the same or less amount of parameters, ViTs are ahead of corresponding CNNs by
more than 5% in top-1 accuracy under most types of DS. 2) As the model scale
increases, ViTs strengthen these biases and thus gradually narrow the
in-distribution and OOD performance gap. To further improve the generalization
of ViTs, we design the Generalization-Enhanced ViTs (GE-ViTs) from the
perspectives of adversarial learning, information theory, and self-supervised
learning. By comprehensively investigating these GE-ViTs and comparing with
their corresponding CNN models, we observe: 1) For the enhanced model, larger
ViTs still benefit more for the OOD generalization. 2) GE-ViTs are more
sensitive to the hyper-parameters than their corresponding CNN models. We
design a smoother learning strategy to achieve a stable training process and
obtain performance improvements on OOD data by 4% from vanilla ViTs. We hope
our comprehensive study could shed light on the design of more generalizable
learning architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RPR-Net: A Point Cloud-based Rotation-aware Large Scale Place Recognition Network. (arXiv:2108.12790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12790">
<div class="article-summary-box-inner">
<span><p>Point cloud-based large scale place recognition is an important but
challenging task for many applications such as Simultaneous Localization and
Mapping (SLAM). Taking the task as a point cloud retrieval problem, previous
methods have made delightful achievements. However, how to deal with
catastrophic collapse caused by rotation problems is still under-explored. In
this paper, to tackle the issue, we propose a novel Point Cloud-based
Rotation-aware Large Scale Place Recognition Network (RPR-Net). In particular,
to solve the problem, we propose to learn rotation-invariant features in three
steps. First, we design three kinds of novel Rotation-Invariant Features
(RIFs), which are low-level features that can hold the rotation-invariant
property. Second, using these RIFs, we design an attentive module to learn
rotation-invariant kernels. Third, we apply these kernels to previous point
cloud features to generate new features, which is the well-known SO(3) mapping
process. By doing so, high-level scene-specific rotation-invariant features can
be learned. We call the above process an Attentive Rotation-Invariant
Convolution (ARIConv). To achieve the place recognition goal, we build RPR-Net,
which takes ARIConv as a basic unit to construct a dense network architecture.
Then, powerful global descriptors used for retrieval-based place recognition
can be sufficiently extracted from RPR-Net. Experimental results on prevalent
datasets show that our method achieves comparable results to existing
state-of-the-art place recognition models and significantly outperforms other
rotation-invariant baseline models when solving rotation problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAM-loss: Towards Learning Spatially Discriminative Feature Representations. (arXiv:2109.01359v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01359">
<div class="article-summary-box-inner">
<span><p>The backbone of traditional CNN classifier is generally considered as a
feature extractor, followed by a linear layer which performs the
classification. We propose a novel loss function, termed as CAM-loss, to
constrain the embedded feature maps with the class activation maps (CAMs) which
indicate the spatially discriminative regions of an image for particular
categories. CAM-loss drives the backbone to express the features of target
category and suppress the features of non-target categories or background, so
as to obtain more discriminative feature representations. It can be simply
applied in any CNN architecture with neglectable additional parameters and
calculations. Experimental results show that CAM-loss is applicable to a
variety of network structures and can be combined with mainstream
regularization methods to improve the performance of image classification. The
strong generalization ability of CAM-loss is validated in the transfer learning
and few shot learning tasks. Based on CAM-loss, we also propose a novel
CAAM-CAM matching knowledge distillation method. This method directly uses the
CAM generated by the teacher network to supervise the CAAM generated by the
student network, which effectively improves the accuracy and convergence rate
of the student network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05021">
<div class="article-summary-box-inner">
<span><p>Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early
signs of diabetic retinopathy (DR). The automatic detection of MAs and HMs on
retinal fundus images is a challenging task. Most of the existing methods
detect either only MAs or only HMs because of the difference in their texture,
sizes, and morphology. Though some methods detect both MAs and HMs, they suffer
from the curse of dimensionality of shape and colors features and fail to
detect all shape variations of HMs such as flame-shaped HM. Leveraging the
progress in deep learning, we proposed a two-stream red lesions detection
system dealing simultaneously with small and large red lesions. For this
system, we introduced a new ROIs candidates generation method for large red
lesions fundus images; it is based on blood vessel segmentation and
morphological operations, and reduces the computational complexity, and
enhances the detection accuracy by generating a small number of potential
candidates. For detection, we adapted the Faster RCNN framework with two
streams. We used pre-trained VGGNet as a bone model and carried out several
extensive experiments to tune it for vessels segmentation and candidates
generation, and finally learning the appropriate mapping, which yields better
detection of the red lesions comparing with the state-of-the-art methods. The
experimental results validated the effectiveness of the system in the detection
of both MAs and HMs; the method yields higher performance for per lesion
detection according to sensitivity under 4 FPIs on DiaretDB1-MA and
DiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state
of the art methods w.r.t. various evaluation metrics. For DR screening, the
system outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05205">
<div class="article-summary-box-inner">
<span><p>The high efficiency in computation and storage makes hashing (including
binary hashing and quantization) a common strategy in large-scale retrieval
systems. To alleviate the reliance on expensive annotations, unsupervised deep
hashing becomes an important research problem. This paper provides a novel
solution to unsupervised deep quantization, namely Contrastive Quantization
with Code Memory (MeCoQ). Different from existing reconstruction-based
strategies, we learn unsupervised binary descriptors by contrastive learning,
which can better capture discriminative visual semantics. Besides, we uncover
that codeword diversity regularization is critical to prevent contrastive
learning-based quantization from model degeneration. Moreover, we introduce a
novel quantization code memory module that boosts contrastive learning with
lower feature drift than conventional feature memories. Extensive experiments
on benchmark datasets show that MeCoQ outperforms state-of-the-art methods.
Code and configurations are publicly available at
https://github.com/gimpong/AAAI22-MeCoQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05565">
<div class="article-summary-box-inner">
<span><p>This paper addresses the deep face recognition problem under an open-set
protocol, where ideal face features are expected to have smaller maximal
intra-class distance than minimal inter-class distance under a suitably chosen
metric space. To this end, hyperspherical face recognition, as a promising line
of research, has attracted increasing attention and gradually become a major
focus in face recognition research. As one of the earliest works in
hyperspherical face recognition, SphereFace explicitly proposed to learn face
embeddings with large inter-class angular margin. However, SphereFace still
suffers from severe training instability which limits its application in
practice. In order to address this problem, we introduce a unified framework to
understand large angular margin in hyperspherical face recognition. Under this
framework, we extend the study of SphereFace and propose an improved variant
with substantially better training stability -- SphereFace-R. Specifically, we
propose two novel ways to implement the multiplicative margin, and study
SphereFace-R under three different feature normalization schemes (no feature
normalization, hard feature normalization and soft feature normalization). We
also propose an implementation strategy -- "characteristic gradient detachment"
-- to stabilize training. Extensive experiments on SphereFace-R show that it is
consistently better than or competitive with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Instance Segmentation with High-Resolution Automotive Radar. (arXiv:2110.01775v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01775">
<div class="article-summary-box-inner">
<span><p>Automotive radar has been widely used in the modern advanced driver
assistance systems (ADAS) and autonomous driving system as it provides reliable
environmental perception in all-weather conditions with affordable cost.
However, automotive radar usually only plays as an auxiliary sensor since it
hardly supplies semantic and geometry information due to the sparsity of radar
detection points. Nonetheless, as development of high-resolution automotive
radar in recent years, more advanced perception functionality like instance
segmentation which has only been well explored using Lidar point clouds,
becomes possible by using automotive radar. Its data comes with rich contexts
such as Radar Cross Section (RCS) and micro-doppler effects which may
potentially be pertinent, and sometimes can even provide detection when the
field of view is completely obscured. Therefore, the effective utilization of
radar detection points data is an integral part of automotive perception. The
outcome from instance segmentation could be seen as comparable result of
clustering, and could be potentially used as the input of tracker for tracking
the targets. In this paper, we propose two efficient methods for instance
segmentation with radar detection points, one is implemented in an end-to-end
deep learning driven fashion using PointNet++ framework, and the other is based
on clustering of the radar detection points with semantic information. Both
approaches can be further improved by implementing visual multi-layer
perceptron (MLP). The effectiveness of the proposed methods is verified using
experimental results on the recent RadarScenes dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim2Air - Synthetic aerial dataset for UAV monitoring. (arXiv:2110.05145v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05145">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a novel approach to generate a synthetic aerial
dataset for application in UAV monitoring. We propose to accentuate shape-based
object representation by applying texture randomization. A diverse dataset with
photorealism in all parameters such as shape, pose, lighting, scale, viewpoint,
etc. except for atypical textures is created in a 3D modelling software
Blender. Our approach specifically targets two conditions in aerial images
where texture of objects is difficult to detect, namely challenging
illumination and objects occupying only a small portion of the image.
Experimental evaluation of YOLO and Faster R-CNN detectors trained on synthetic
data with randomized textures confirmed our approach by increasing the mAP
value (17 and 3.7 percentage points for YOLO; 20 and 1.1 percentage points for
Faster R-CNN) on two test datasets of real images, both containing UAV-to-UAV
images with motion blur. Testing on different domains, we conclude that the
more the generalisation ability is put to the test, the more apparent are the
advantages of the shape-based representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Contrastive Learning for Domain Adaptation. (arXiv:2111.06021v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06021">
<div class="article-summary-box-inner">
<span><p>Recent feature contrastive learning (FCL) has shown promising performance in
self-supervised representation learning. For domain adaptation, however, FCL
cannot show overwhelming gains since the class weights are not involved during
optimization, which does not guarantee the produced features to be clustered
around the class weights learned from source data. To tackle this issue, we
propose a novel probability contrastive learning (PCL) in this paper, which not
only produces compact features but also enforces them to be distributed around
the class weights. Specifically, we propose to use the output probabilities
after softmax to perform contrastive learning instead of the extracted features
and remove the $\ell_{2}$ normalization in the traditional FCL. In this way,
the probability will approximate the one-hot form, thereby narrowing the
distance between the features and the class weights. Our proposed PCL is simple
and effective. We conduct extensive experiments on two domain adaptation tasks,
i.e., unsupervised domain adaptation and semi-supervised domain adaptation. The
results on multiple datasets demonstrate that our PCL can consistently get
considerable gains and achieves the state-of-the-art performance. In addition,
our method also obtains considerable gains on semi-supervised tasks when
labeled data is scarce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization. (arXiv:2111.09136v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09136">
<div class="article-summary-box-inner">
<span><p>Learning to synthesize data has emerged as a promising direction in zero-shot
quantization (ZSQ), which represents neural networks by low-bit integer without
accessing any of the real data. In this paper, we observe an interesting
phenomenon of intra-class heterogeneity in real data and show that existing
methods fail to retain this property in their synthetic images, which causes a
limited performance increase. To address this issue, we propose a novel
zero-shot quantization method referred to as IntraQ. First, we propose a local
object reinforcement that locates the target objects at different scales and
positions of the synthetic images. Second, we introduce a marginal distance
constraint to form class-related features distributed in a coarse area. Lastly,
we devise a soft inception loss which injects a soft prior label to prevent the
synthetic images from being overfitting to a fixed object. Our IntraQ is
demonstrated to well retain the intra-class heterogeneity in the synthetic
images and also observed to perform state-of-the-art. For example, compared to
the advanced ZSQ, our IntraQ obtains 9.17\% increase of the top-1 accuracy on
ImageNet when all layers of MobileNetV1 are quantized to 4-bit. Code is at
https://github.com/zysxmu/IntraQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Vocabulary Object Detection with Pseudo Bounding-Box Labels. (arXiv:2111.09452v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09452">
<div class="article-summary-box-inner">
<span><p>Despite great progress in object detection, most existing methods work only
on a limited set of object categories, due to the tremendous human effort
needed for instance-level bounding-box annotations of training data. To
alleviate the problem, recent open vocabulary and zero-shot detection methods
attempt to detect novel object categories beyond those seen during training.
They achieve this goal by training on a pre-defined base categories to induce
generalization to novel objects. However, their potential is still constrained
by the small set of base categories available for training. To enlarge the set
of base classes, we propose a method to automatically generate pseudo
bounding-box annotations of diverse objects from large-scale image-caption
pairs. Our method leverages the localization ability of pre-trained
vision-language models to generate pseudo bounding-box labels and then directly
uses them for training object detectors. Experimental results show that our
method outperforms the state-of-the-art (SOTA) open vocabulary object detector
by 8% AP on COCO novel categories, by 6.3% AP on PASCAL VOC, by 2.3% AP on
Objects365 and by 2.8% AP on LVIS. Code is available:
https://github.com/salesforce/PB-OVD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Positional Encoder Graph Neural Networks for Geographic Data. (arXiv:2111.10144v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10144">
<div class="article-summary-box-inner">
<span><p>Graph neural networks (GNNs) provide a powerful and scalable solution for
modeling continuous spatial data. However, in the absence of further context on
the geometric structure of the data, they often rely on Euclidean distances to
construct the input graphs. This assumption can be improbable in many
real-world settings, where the spatial structure is more complex and explicitly
non-Euclidean (e.g., road networks). In this paper, we propose PE-GNN, a new
framework that incorporates spatial context and correlation explicitly into the
models. Building on recent advances in geospatial auxiliary task learning and
semantic spatial embeddings, our proposed method (1) learns a context-aware
vector encoding of the geographic coordinates and (2) predicts spatial
autocorrelation in the data in parallel with the main task. On spatial
regression tasks, we show the effectiveness of our approach, improving
performance over different state-of-the-art GNN approaches. We also test our
approach for spatial interpolation, i.e., spatial regression without node
features, a task that GNNs are currently not competitive at. We observe that
our approach not only vastly improves over the GNN baselines, but can match
Gaussian processes, the most commonly utilized method for spatial interpolation
problems. The code for this study can be accessed via:
https://github.com/konstantinklemmer/pe-gnn
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Do Sparse Imagenet Models Transfer?. (arXiv:2111.13445v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13445">
<div class="article-summary-box-inner">
<span><p>Transfer learning is a classic paradigm by which models pretrained on large
"upstream" datasets are adapted to yield good results on "downstream,"
specialized datasets. Generally, it is understood that more accurate models on
the "upstream" dataset will provide better transfer accuracy "downstream". In
this work, we perform an in-depth investigation of this phenomenon in the
context of convolutional neural networks (CNNs) trained on the ImageNet
dataset, which have been pruned - that is, compressed by sparsifiying their
connections. Specifically, we consider transfer using unstructured pruned
models obtained by applying several state-of-the-art pruning methods, including
magnitude-based, second-order, re-growth and regularization approaches, in the
context of twelve standard transfer tasks. In a nutshell, our study shows that
sparse models can match or even outperform the transfer performance of dense
models, even at high sparsities, and, while doing so, can lead to significant
inference and even training speedups. At the same time, we observe and analyze
significant differences in the behaviour of different pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration. (arXiv:2112.01740v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01740">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection has attracted increasing attention and rapidly
progressed in recent years. However, the requirement of an exhaustive offline
fine-tuning stage in existing methods is time-consuming and significantly
hinders their usage in online applications such as autonomous exploration of
low-power robots. We find that their major limitation is that the little but
valuable information from a few support images is not fully exploited. To solve
this problem, we propose a brand new architecture, AirDet, and surprisingly
find that, by learning class-agnostic relation with the support images in all
modules, including cross-scale object proposal network, shots aggregation
module, and localization network, AirDet without fine-tuning achieves
comparable or even better results than the exhaustively fine-tuned methods,
reaching up to 30-40% improvements. We also present solid results of onboard
tests on real-world exploration data from the DARPA Subterranean Challenge,
which strongly validate the feasibility of AirDet in robotics. To the best of
our knowledge, AirDet is the first feasible few-shot detection method for
autonomous exploration of low-power robots. The source code, pre-trained
models, along with the real-world data for exploration, will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeing BDD100K in dark: Single-Stage Night-time Object Detection via Continual Fourier Contrastive Learning. (arXiv:2112.02891v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02891">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the lesser explored avenue of object detection at
night-time. An object detector trained on abundant labeled daytime images often
fails to perform well on night images, due to domain gap. As collecting more
labeled data from night-time is expensive, unpaired generative image
translation techniques seek to synthesize night-time images. However,
unrealistic artifacts often arise on the synthetic images. Illuminating
night-time inference images also does not work well in practice, as shown in
our paper. To address these issues, we suggest a novel technique for enhancing
the object detector via Contrastive Learning, which tries to group together
embeddings of similar images. To provide anchor-positive image pairs for
Contrastive Learning, we leverage Fourier Transformation, which is naturally
good at preserving the semantics of an image. For practical benefits in
real-time applications, we choose the recently proposed YOLOF single-stage
detector, which provides a simple and clean encoder-decoder segregation of the
detector network. However, merely trying to teach the encoder to perform well
on the auxiliary Contrastive Learning task may lead to catastrophic forgetting
of the knowledge essential for object detection. Hence, we train the encoder in
a Continual Learning fashion. Our novel method by an elegant training framework
achieves state-of-the-art performance on the large scale BDD100K dataset, in an
uniform setting, chosen, to the best of our knowledge, for the first time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASTAF: A Model-Agnostic Spatio-Temporal Attention Fusion Network for Few-shot Video Classification. (arXiv:2112.04585v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04585">
<div class="article-summary-box-inner">
<span><p>We propose MASTAF, a Model-Agnostic Spatio-Temporal Attention Fusion network
for few-shot video classification. MASTAF takes input from a general video
spatial and temporal representation,e.g., using 2D CNN, 3D CNN, and video
Transformer. Then, to make the most of such representations, we use self- and
cross-attention models to highlight the critical spatio-temporal region to
increase the inter-class distance and decrease the intra-class distance. Last,
MASTAF applies a lightweight fusion network and a nearest neighbor classifier
to classify each query video. We demonstrate that MASTAF improves the
state-of-the-art performance on three few-shot video classification
benchmarks(UCF101, HMDB51, and Something-Something-V2), e.g., by up to 91.6%,
69.5%, and 60.7% for five-way one-shot video classification, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. (arXiv:2112.10741v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10741">
<div class="article-summary-box-inner">
<span><p>Diffusion models have recently been shown to generate high-quality synthetic
images, especially when paired with a guidance technique to trade off diversity
for fidelity. We explore diffusion models for the problem of text-conditional
image synthesis and compare two different guidance strategies: CLIP guidance
and classifier-free guidance. We find that the latter is preferred by human
evaluators for both photorealism and caption similarity, and often produces
photorealistic samples. Samples from a 3.5 billion parameter text-conditional
diffusion model using classifier-free guidance are favored by human evaluators
to those from DALL-E, even when the latter uses expensive CLIP reranking.
Additionally, we find that our models can be fine-tuned to perform image
inpainting, enabling powerful text-driven image editing. We train a smaller
model on a filtered dataset and release the code and weights at
https://github.com/openai/glide-text2im.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04756">
<div class="article-summary-box-inner">
<span><p>In this paper, we developed the solution of roadside LiDAR object detection
using a combination of two unsupervised learning algorithms. The 3D point
clouds are firstly converted into spherical coordinates and filled into the
elevation-azimuth matrix using a hash function. After that, the raw LiDAR data
were rearranged into new data structures to store the information of range,
azimuth, and intensity. Then, the Dynamic Mode Decomposition method is applied
to decompose the LiDAR data into low-rank backgrounds and sparse foregrounds
based on intensity channel pattern recognition. The Coarse Fine Triangle
Algorithm (CFTA) automatically finds the dividing value to separate the moving
targets from static background according to range information. After intensity
and range background subtraction, the foreground moving objects will be
detected using a density-based detector and encoded into the state-space model
for tracking. The output of the proposed solution includes vehicle trajectories
that can enable many mobility and safety applications. The method was validated
at both path and point levels and outperformed the state-of-the-art. In
contrast to the previous methods that process directly on the scattered and
discrete point clouds, the dynamic classification method can establish the less
sophisticated linear relationship of the 3D measurement data, which captures
the spatial-temporal structure that we often desire.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Singapore Trusted Media Challenge Dataset. (arXiv:2201.04788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04788">
<div class="article-summary-box-inner">
<span><p>The development of powerful deep learning technologies has brought about some
negative effects to both society and individuals. One such issue is the
emergence of fake media. To tackle the issue, we have organized the Trusted
Media Challenge (TMC) to explore how Artificial Intelligence (AI) technologies
could be leveraged to combat fake media.
</p>
<p>To enable further research, we are releasing the dataset that we had prepared
from the TMC challenge, consisting of 4,380 fake and 2,563 real videos, with
various video and/or audio manipulation methods employed to produce different
types of fake media. All the videos in the TMC dataset are accompanied with
audios and have a minimum resolution of 360p. The videos have various
durations, background, illumination, and may contain perturbations that mimic
transmission errors and compression.
</p>
<p>We have also carried out a user study to demonstrate the quality of the TMC
dataset and to compare the performance of humans and AI models. The results
showed that the TMC dataset can fool human participants in many cases, and the
winning AI models of the Trusted Media Challenge outperformed humans.
</p>
<p>The TMC dataset is available for research purpose upon request via
tmc-dataset@aisingapore.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Style Image Super-Resolution using Conditional Objective. (arXiv:2201.04898v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04898">
<div class="article-summary-box-inner">
<span><p>Recent studies have significantly enhanced the performance of single-image
super-resolution (SR) using convolutional neural networks (CNNs). While there
can be many high-resolution (HR) solutions for a given input, most existing
CNN-based methods do not explore alternative solutions during the inference. A
typical approach to obtaining alternative SR results is to train multiple SR
models with different loss weightings and exploit the combination of these
models. Instead of using multiple models, we present a more efficient method to
train a single adjustable SR model on various combinations of losses by taking
advantage of multi-task learning. Specifically, we optimize an SR model with a
conditional objective during training, where the objective is a weighted sum of
multiple perceptual losses at different feature levels. The weights vary
according to given conditions, and the set of weights is defined as a style
controller. Also, we present an architecture appropriate for this training
scheme, which is the Residual-in-Residual Dense Block equipped with spatial
feature transformation layers. At the inference phase, our trained model can
generate locally different outputs conditioned on the style control map.
Extensive experiments show that the proposed SR model produces various
desirable reconstructions without artifacts and yields comparable quantitative
performance to state-of-the-art SR methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. (arXiv:2201.07207v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07207">
<div class="article-summary-box-inner">
<span><p>Can world knowledge learned by large language models (LLMs) be used to act in
interactive environments? In this paper, we investigate the possibility of
grounding high-level tasks, expressed in natural language (e.g. "make
breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While
prior work focused on learning from explicit step-by-step examples of how to
act, we surprisingly find that if pre-trained LMs are large enough and prompted
appropriately, they can effectively decompose high-level tasks into mid-level
plans without any further training. However, the plans produced naively by LLMs
often cannot map precisely to admissible actions. We propose a procedure that
conditions on existing demonstrations and semantically translates the plans to
admissible actions. Our evaluation in the recent VirtualHome environment shows
that the resulting method substantially improves executability over the LLM
baseline. The conducted human evaluation reveals a trade-off between
executability and correctness but shows a promising sign towards extracting
actionable knowledge from language models. Website at
https://huangwl18.github.io/language-planner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08845">
<div class="article-summary-box-inner">
<span><p>Volumetric neural rendering methods like NeRF generate high-quality view
synthesis results but are optimized per-scene leading to prohibitive
reconstruction time. On the other hand, deep multi-view stereo methods can
quickly reconstruct scene geometry via direct network inference. Point-NeRF
combines the advantages of these two approaches by using neural 3D point
clouds, with associated neural features, to model a radiance field. Point-NeRF
can be rendered efficiently by aggregating neural point features near scene
surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can
be initialized via direct inference of a pre-trained deep network to produce a
neural point cloud; this point cloud can be finetuned to surpass the visual
quality of NeRF with 30X faster training time. Point-NeRF can be combined with
other 3D reconstruction methods and handles the errors and outliers in such
methods via a novel pruning and growing mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Object Counting with Similarity-Aware Feature Enhancement. (arXiv:2201.08959v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08959">
<div class="article-summary-box-inner">
<span><p>This work studies the problem of few-shot object counting, which counts the
number of exemplar objects (i.e., described by one or several support images)
occurring in the query image. The major challenge lies in that the target
objects can be densely packed in the query image, making it hard to recognize
every single one. To tackle the obstacle, we propose a novel learning block,
equipped with a similarity comparison module (SCM) and a feature enhancement
module (FEM). Concretely, given a support image and a query image, we first
derive a score map by comparing their projected features at every spatial
position. The score maps regarding all support images are collected together
and normalized across both the exemplar dimension and the spatial dimensions,
producing a reliable similarity map. We then enhance the query feature with the
support features by employing the developed point-wise similarities as the
weighting coefficients. Such a design encourages the model to inspect the query
image by focusing more on the regions akin to the support images, leading to
much clearer boundaries between different objects. Extensive experiments on
various benchmarks and training setups suggest that our method surpasses the
state-of-the-art approaches by a sufficiently large margin. For instance, on
the very recent large-scale FSC-147 dataset, we beat the second competitor by
improving the mean absolute counting error from 22.08 to 14.32 (35%
$\uparrow$). Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Writer Recognition Using Off-line Handwritten Single Block Characters. (arXiv:2201.10665v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10665">
<div class="article-summary-box-inner">
<span><p>Block characters are often used when filling paper forms for a variety of
purposes. We investigate if there is biometric information contained within
individual digits of handwritten text. In particular, we use personal identity
numbers consisting of the six digits of the date of birth, DoB. We evaluate two
recognition approaches, one based on handcrafted features that compute contour
directional measurements, and another based on deep features from a ResNet50
model. We use a self-captured database of 317 individuals and 4920 written DoBs
in total. Results show the presence of identity-related information in a piece
of handwritten information as small as six digits with the DoB. We also analyze
the impact of the amount of enrolment samples, varying its number between one
and ten. Results with such small amount of data are promising. With ten
enrolment samples, the Top-1 accuracy with deep features is around 94%, and
reaches nearly 100% by Top-10. The verification accuracy is more modest, with
EER&gt;20%with any given feature and enrolment set size, showing that there is
still room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Issues of TrueDepth Sensor Data for Computer Vision Tasks Across Different iPad Generations. (arXiv:2201.10865v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10865">
<div class="article-summary-box-inner">
<span><p>In 2017 Apple introduced the TrueDepth sensor with the iPhone X release.
Although its primary use case is biometric face recognition, the exploitation
of accurate depth data for other computer vision tasks like segmentation,
portrait image generation and metric 3D reconstruction seems natural and lead
to the development of various applications. In this report, we investigate the
reliability of TrueDepth data - accessed through two different APIs - on
various devices including different iPhone and iPad generations and reveal two
different and significant issues on all tested iPads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVP-Net: Multiple View Pointwise Semantic Segmentation of Large-Scale Point Clouds. (arXiv:2201.12769v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12769">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of 3D point cloud is an essential task for autonomous
driving environment perception. The pipeline of most pointwise point cloud
semantic segmentation methods includes points sampling, neighbor searching,
feature aggregation, and classification. Neighbor searching method like
K-nearest neighbors algorithm, KNN, has been widely applied. However, the
complexity of KNN is always a bottleneck of efficiency. In this paper, we
propose an end-to-end neural architecture, Multiple View Pointwise Net,
MVP-Net, to efficiently and directly infer large-scale outdoor point cloud
without KNN or any complex pre/postprocessing. Instead, assumption-based space
filling curves and multi-rotation of point cloud methods are introduced to
point feature aggregation and receptive field expanding. Numerical experiments
show that the proposed MVP-Net is 11 times faster than the most efficient
pointwise semantic segmentation method RandLA-Net and achieves the same
accuracy on the large-scale benchmark SemanticKITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge-Selective Feature Weaving for Point Cloud Matching. (arXiv:2202.02149v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02149">
<div class="article-summary-box-inner">
<span><p>This paper tackles the problem of accurately matching the points of two 3D
point clouds. Most conventional methods improve their performance by extracting
representative features from each point via deep-learning-based algorithms. On
the other hand, the correspondence calculation between the extracted features
has not been examined in depth, and non-trainable algorithms (e.g. the Sinkhorn
algorithm) are frequently applied. As a result, the extracted features may be
forcibly fitted to a non-trainable algorithm. Furthermore, the extracted
features frequently contain stochastically unavoidable errors, which degrades
the matching accuracy. In this paper, instead of using a non-trainable
algorithm, we propose a differentiable matching network that can be jointly
optimized with the feature extraction procedure. Our network first constructs
graphs with edges connecting the points of each point cloud and then extracts
discriminative edge features by using two main components: a shared set-encoder
and an edge-selective cross-concatenation. These components enable us to
symmetrically consider two point clouds and to extract discriminative edge
features, respectively. By using the extracted discriminative edge features,
our network can accurately calculate the correspondence between points. Our
experimental results show that the proposed network can significantly improve
the performance of point cloud matching. Our code is available at
https://github.com/yanarin/ESFW
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Network. (arXiv:2202.09741v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09741">
<div class="article-summary-box-inner">
<span><p>While originally designed for natural language processing tasks, the
self-attention mechanism has recently taken various computer vision areas by
storm. However, the 2D nature of images brings three challenges for applying
self-attention in computer vision. (1) Treating images as 1D sequences neglects
their 2D structures. (2) The quadratic complexity is too expensive for
high-resolution images. (3) It only captures spatial adaptability but ignores
channel adaptability. In this paper, we propose a novel large kernel attention
(LKA) module to enable self-adaptive and long-range correlations in
self-attention while avoiding the above issues. We further introduce a novel
neural network based on LKA, namely Visual Attention Network (VAN). While
extremely simple, VAN outperforms the state-of-the-art vision transformers and
convolutional neural networks with a large margin in extensive experiments,
including image classification, object detection, semantic segmentation,
instance segmentation, etc. Code is available at
https://github.com/Visual-Attention-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Effective and Robust Neural Trojan Defenses via Input Filtering. (arXiv:2202.12154v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12154">
<div class="article-summary-box-inner">
<span><p>Trojan attacks on deep neural networks are both dangerous and surreptitious.
Over the past few years, Trojan attacks have advanced from using only a single
input-agnostic trigger and targeting only one class to using multiple,
input-specific triggers and targeting multiple classes. However, Trojan
defenses have not caught up with this development. Most defense methods still
make out-of-date assumptions about Trojan triggers and target classes, thus,
can be easily circumvented by modern Trojan attacks. To deal with this problem,
we propose two novel "filtering" defenses called Variational Input Filtering
(VIF) and Adversarial Input Filtering (AIF) which leverage lossy data
compression and adversarial learning respectively to effectively purify all
potential Trojan triggers in the input at run time without making assumptions
about the number of triggers/target classes or the input dependence property of
triggers. In addition, we introduce a new defense mechanism called
"Filtering-then-Contrasting" (FtC) which helps avoid the drop in classification
accuracy on clean data caused by "filtering", and combine it with VIF/AIF to
derive new defenses of this kind. Extensive experimental results and ablation
studies show that our proposed defenses significantly outperform well-known
baseline defenses in mitigating five advanced Trojan attacks including two
recent state-of-the-art while being quite robust to small amounts of training
data and large-norm triggers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Transferable Reward for Query Object Localization with Policy Adaptation. (arXiv:2202.12403v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12403">
<div class="article-summary-box-inner">
<span><p>We propose a reinforcement learning based approach to query object
localization, for which an agent is trained to localize objects of interest
specified by a small exemplary set. We learn a transferable reward signal
formulated using the exemplary set by ordinal metric learning. Our proposed
method enables test-time policy adaptation to new environments where the reward
signals are not readily available, and outperforms fine-tuning approaches that
are limited to annotated images. In addition, the transferable reward allows
repurposing the trained agent from one specific class to another class.
Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffeomorphic Image Registration with Neural Velocity Field. (arXiv:2202.12498v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12498">
<div class="article-summary-box-inner">
<span><p>Diffeomorphic image registration is a crucial task in medical image analysis.
Recent learning-based image registration methods utilize convolutional neural
networks (CNN) to learn the spatial transformation between image pairs and
achieve a fast inference speed. However, these methods often require a large
number of training data to improve their generalization abilities. During the
test time, learning-based methods might fail to provide a good registration
result, which is likely because of the model overfitting on the training
dataset. In this paper, we propose a neural representation of continuous
velocity field (NeVF) to describe the deformations across two images.
Specifically, this neural velocity field assigns a velocity vector to each
point in the space, which has higher flexibility in modeling the complex
deformation field. Furthermore, we propose a simple sparse-sampling strategy to
reduce the memory consumption for the diffeomorphic registration. The proposed
NeVF can also incorporate with a pre-trained learning-based model whose
predicted deformation is taken as an initial state for optimization. Extensive
experiments conducted on two large-scale 3D MR brain scan datasets demonstrate
that our proposed method outperforms the state-of-the-art registration methods
by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Effective Subnetworks with Gumebel-Softmax. (arXiv:2202.12986v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12986">
<div class="article-summary-box-inner">
<span><p>Large and performant neural networks are often overparameterized and can be
drastically reduced in size and complexity thanks to pruning. Pruning is a
group of methods, which seeks to remove redundant or unnecessary weights or
groups of weights in a network. These techniques allow the creation of
lightweight networks, which are particularly critical in embedded or mobile
applications. In this paper, we devise an alternative pruning method that
allows extracting effective subnetworks from larger untrained ones. Our method
is stochastic and extracts subnetworks by exploring different topologies which
are sampled using Gumbel Softmax. The latter is also used to train probability
distributions which measure the relevance of weights in the sampled topologies.
The resulting subnetworks are further enhanced using a highly efficient
rescaling mechanism that reduces training time and improves performance.
Extensive experiments conducted on CIFAR10 show the outperformance of our
subnetwork extraction method against the related work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence. (arXiv:2203.00911v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00911">
<div class="article-summary-box-inner">
<span><p>Deep learning based single image super-resolution models have been widely
studied and superb results are achieved in upscaling low-resolution images with
fixed scale factor and downscaling degradation kernel. To improve real world
applicability of such models, there are growing interests to develop models
optimized for arbitrary upscaling factors. Our proposed method is the first to
treat arbitrary rescaling, both upscaling and downscaling, as one unified
process. Using joint optimization of both directions, the proposed model is
able to learn upscaling and downscaling simultaneously and achieve
bidirectional arbitrary image rescaling. It improves the performance of current
arbitrary upscaling models by a large margin while at the same time learns to
maintain visual perception quality in downscaled images. The proposed model is
further shown to be robust in cycle idempotence test, free of severe
degradations in reconstruction accuracy when the downscaling-to-upscaling cycle
is applied repetitively. This robustness is beneficial for image rescaling in
the wild when this cycle could be applied to one image for multiple times. It
also performs well on tests with arbitrary large scales and asymmetric scales,
even when the model is not trained with such tasks. Extensive experiments are
conducted to demonstrate the superior performance of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovering 3D Human Mesh from Monocular Images: A Survey. (arXiv:2203.01923v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01923">
<div class="article-summary-box-inner">
<span><p>Estimating human pose and shape from monocular images is a long-standing
problem in computer vision. Since the release of statistical body models, 3D
human mesh recovery has been drawing broader attention. With the same goal of
obtaining well-aligned and physically plausible mesh results, two paradigms
have been developed to overcome challenges in the 2D-to-3D lifting process: i)
an optimization-based paradigm, where different data terms and regularization
terms are exploited as optimization objectives; and ii) a regression-based
paradigm, where deep learning techniques are embraced to solve the problem in
an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving
the quality of 3D mesh labels for a wide range of datasets. Though remarkable
progress has been achieved in the past decade, the task is still challenging
due to flexible body motions, diverse appearances, complex environments, and
insufficient in-the-wild annotations. To the best of our knowledge, this is the
first survey to focus on the task of monocular 3D human mesh recovery. We start
with the introduction of body models and then elaborate recovery frameworks and
training objectives by providing in-depth analyses of their strengths and
weaknesses. We also summarize datasets, evaluation metrics, and benchmark
results. Open issues and future directions are discussed in the end, hoping to
motivate researchers and facilitate their research in this area. A regularly
updated project page can be found at https://github.com/tinatiansjz/hmr-survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated and Generalized Person Re-identification through Domain and Feature Hallucinating. (arXiv:2203.02689v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02689">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of federated domain generalization
(FedDG) for person re-identification (re-ID), which aims to learn a generalized
model with multiple decentralized labeled source domains. An empirical method
(FedAvg) trains local models individually and averages them to obtain the
global model for further local fine-tuning or deploying in unseen target
domains. One drawback of FedAvg is neglecting the data distributions of other
clients during local training, making the local model overfit local data and
producing a poorly-generalized global model. To solve this problem, we propose
a novel method, called "Domain and Feature Hallucinating (DFH)", to produce
diverse features for learning generalized local and global models.
Specifically, after each model aggregation process, we share the Domain-level
Feature Statistics (DFS) among different clients without violating data
privacy. During local training, the DFS are used to synthesize novel domain
statistics with the proposed domain hallucinating, which is achieved by
re-weighting DFS with random weights. Then, we propose feature hallucinating to
diversify local features by scaling and shifting them to the distribution of
the obtained novel domain. The synthesized novel features retain the original
pair-wise similarities, enabling us to utilize them to optimize the model in a
supervised manner. Extensive experiments verify that the proposed DFH can
effectively improve the generalization ability of the global model. Our method
achieves the state-of-the-art performance for FedDG on four large-scale re-ID
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02925">
<div class="article-summary-box-inner">
<span><p>Weakly supervised temporal action localization aims to localize temporal
boundaries of actions and simultaneously identify their categories with only
video-level category labels. Many existing methods seek to generate pseudo
labels for bridging the discrepancy between classification and localization,
but usually only make use of limited contextual information for pseudo label
generation. To alleviate this problem, we propose a representative snippet
summarization and propagation framework. Our method seeks to mine the
representative snippets in each video for propagating information between video
snippets to generate better pseudo labels. For each video, its own
representative snippets and the representative snippets from a memory bank are
propagated to update the input features in an intra- and inter-video manner.
The pseudo labels are generated from the temporal class activation maps of the
updated features to rectify the predictions of the main branch. Our method
obtains superior performance in comparison to the existing methods on two
benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in
terms of average mAP on THUMOS14.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Texture for Fooling Person Detectors in the Physical World. (arXiv:2203.03373v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03373">
<div class="article-summary-box-inner">
<span><p>Nowadays, cameras equipped with AI systems can capture and analyze images to
detect people automatically. However, the AI system can make mistakes when
receiving deliberately designed patterns in the real world, i.e., physical
adversarial examples. Prior works have shown that it is possible to print
adversarial patches on clothes to evade DNN-based person detectors. However,
these adversarial examples could have catastrophic drops in the attack success
rate when the viewing angle (i.e., the camera's angle towards the object)
changes. To perform a multi-angle attack, we propose Adversarial Texture
(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people
wearing such clothes can hide from person detectors from different viewing
angles. We propose a generative method, named Toroidal-Cropping-based
Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive
structures. We printed several pieces of cloth with AdvTexure and then made
T-shirts, skirts, and dresses in the physical world. Experiments showed that
these clothes could fool person detectors in the physical world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Networks for Image Classification and Reinforcement Learning using Graph representations. (arXiv:2203.03457v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03457">
<div class="article-summary-box-inner">
<span><p>In this paper, we will evaluate the performance of graph neural networks in
two distinct domains: computer vision and reinforcement learning. In the
computer vision section, we seek to learn whether a novel non-redundant
representation for images as graphs can improve performance over trivial pixel
to node mapping on a graph-level prediction graph, specifically image
classification. For the reinforcement learning section, we seek to learn if
explicitly modeling solving a Rubik's cube as a graph problem can improve
performance over a standard model-free technique with no inductive bias.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-09 23:07:39.626025371 UTC">2022-03-09 23:07:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>