<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-07T01:30:00Z">06-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentences as connection paths: A neural language architecture of sentence structure in the brain. (arXiv:2206.01725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01725">
<div class="article-summary-box-inner">
<span><p>This article presents a neural language architecture of sentence structure in
the brain, in which sentences are temporal connection paths that interconnect
neural structures underlying their words. Words remain 'in-situ', hence they
are always content-addressable. Arbitrary and novel sentences (with novel
words) can be created with 'neural blackboards' for words and sentences. Hence,
the unlimited productivity of natural language can be achieved with a 'fixed'
small world like network structure. The article focuses on the neural
blackboard for sentences. The architecture uses only one 'connection matrix'
for binding all structural relations between words in sentences. Its ability to
represent arbitrary (English) sentences is discussed in detail, based on a
comprehensive analysis of them. The architecture simulates intra-cranial brain
activity observed during sentence processing and fMRI observations related to
sentence complexity and ambiguity. The simulations indicate that the observed
effects relate to global control over the architecture, not to the sentence
structures involved, which predicts higher activity differences related to
complexity and ambiguity with higher comprehension capacity. Other aspects
discussed are the 'intrinsic' sentence structures provided by connection paths
and their relation to scope and inflection, the use of a dependency parser for
control of binding, long-distance dependencies and gaps, question answering,
ambiguity resolution based on backward processing without explicit
backtracking, garden paths, and performance difficulties related to embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[Re] Badder Seeds: Reproducing the Evaluation of Lexical Methods for Bias Measurement. (arXiv:2206.01767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01767">
<div class="article-summary-box-inner">
<span><p>Combating bias in NLP requires bias measurement. Bias measurement is almost
always achieved by using lexicons of seed terms, i.e. sets of words specifying
stereotypes or dimensions of interest. This reproducibility study focuses on
the original authors' main claim that the rationale for the construction of
these lexicons needs thorough checking before usage, as the seeds used for bias
measurement can themselves exhibit biases. The study aims to evaluate the
reproducibility of the quantitative and qualitative results presented in the
paper and the conclusions drawn thereof. We reproduce most of the results
supporting the original authors' general claim: seed sets often suffer from
biases that affect their performance as a baseline for bias metrics. Generally,
our results mirror the original paper's. They are slightly different on select
occasions, but not in ways that undermine the paper's general intent to show
the fragility of seed sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QAGCN: A Graph Convolutional Network-based Multi-Relation Question Answering System. (arXiv:2206.01818v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01818">
<div class="article-summary-box-inner">
<span><p>Answering multi-relation questions over knowledge graphs is a challenging
task as it requires multi-step reasoning over a huge number of possible paths.
Reasoning-based methods with complex reasoning mechanisms, such as
reinforcement learning-based sequential decision making, have been regarded as
the default pathway for this task. However, these mechanisms are difficult to
implement and train, which hampers their reproducibility and transferability to
new domains. In this paper, we propose QAGCN - a simple but effective and novel
model that leverages attentional graph convolutional networks that can perform
multi-step reasoning during the encoding of knowledge graphs. As a consequence,
complex reasoning mechanisms are avoided. In addition, to improve efficiency,
we retrieve answers using highly-efficient embedding computations and, for
better interpretability, we extract interpretable paths for returned answers.
On widely adopted benchmark datasets, the proposed model has been demonstrated
competitive against state-of-the-art methods that rely on complex reasoning
mechanisms. We also conducted extensive experiments to scrutinize the
efficiency and contribution of each component of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric. (arXiv:2206.01823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01823">
<div class="article-summary-box-inner">
<span><p>In this work, we evaluate various existing dialogue relevance metrics, find
strong dependency on the dataset, often with poor correlation with human scores
of relevance, and propose modifications to reduce data requirements and domain
sensitivity while improving correlation. Our proposed metric achieves
state-of-the-art performance on the HUMOD dataset while reducing measured
sensitivity to dataset by 37%-66%. We achieve this without fine-tuning a
pretrained language model, and using only 3,750 unannotated human dialogues and
a single negative example. Despite these limitations, we demonstrate
competitive performance on four datasets from different domains. Our code,
including our metric and experiments, is open sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kallima: A Clean-label Framework for Textual Backdoor Attacks. (arXiv:2206.01832v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01832">
<div class="article-summary-box-inner">
<span><p>Although Deep Neural Network (DNN) has led to unprecedented progress in
various natural language processing (NLP) tasks, research shows that deep
models are extremely vulnerable to backdoor attacks. The existing backdoor
attacks mainly inject a small number of poisoned samples into the training
dataset with the labels changed to the target one. Such mislabeled samples
would raise suspicion upon human inspection, potentially revealing the attack.
To improve the stealthiness of textual backdoor attacks, we propose the first
clean-label framework Kallima for synthesizing mimesis-style backdoor samples
to develop insidious textual backdoor attacks. We modify inputs belonging to
the target class with adversarial perturbations, making the model rely more on
the backdoor trigger. Our framework is compatible with most existing backdoor
triggers. The experimental results on three benchmark datasets demonstrate the
effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning. (arXiv:2206.01843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01843">
<div class="article-summary-box-inner">
<span><p>People say, "A picture is worth a thousand words". Then how can we get the
rich information out of the image? We argue that by using visual clues to
bridge large pretrained vision foundation models and language models, we can do
so without any extra cross-modal training. Thanks to the strong zero-shot
capability of foundation models, we start by constructing a rich semantic
representation of the image (e.g., image tags, object attributes / locations,
captions) as a structured textual prompt, called visual clues, using a vision
foundation model. Based on visual clues, we use large language model to produce
a series of comprehensive descriptions for the visual content, which is then
verified by the vision model again to select the candidate that aligns best
with the image. We evaluate the quality of generated descriptions by
quantitative and qualitative measurement. The results demonstrate the
effectiveness of such a structured semantic representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extreme Compression for Pre-trained Transformers Made Simple and Efficient. (arXiv:2206.01859v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01859">
<div class="article-summary-box-inner">
<span><p>Extreme compression, particularly ultra-low bit precision (binary/ternary)
quantization, has been proposed to fit large NLP models on resource-constraint
devices. However, to preserve the accuracy for such aggressive compression
schemes, cutting-edge methods usually introduce complicated compression
pipelines, e.g., multi-stage expensive knowledge distillation with extensive
hyperparameter tuning. Also, they oftentimes focus less on smaller transformer
models that have already been heavily compressed via knowledge distillation and
lack a systematic study to show the effectiveness of their methods. In this
paper, we perform a very comprehensive systematic study to measure the impact
of many key hyperparameters and training strategies from previous works. As a
result, we find out that previous baselines for ultra-low bit precision
quantization are significantly under-trained. Based on our study, we propose a
simple yet effective compression pipeline for extreme compression, named XTC.
XTC demonstrates that (1) we can skip the pre-training knowledge distillation
to obtain a 5-layer BERT while achieving better performance than previous
state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization
plus layer reduction is able to reduce the model size by 50x, resulting in new
state-of-the-art results on GLUE tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. (arXiv:2206.01861v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01861">
<div class="article-summary-box-inner">
<span><p>How to efficiently serve ever-larger trained natural language models in
practice has become exceptionally challenging even for powerful cloud servers
due to their prohibitive memory/computation requirements. In this work, we
present an efficient and affordable post-training quantization approach to
compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an
end-to-end quantization and inference pipeline with three main components: (1)
a fine-grained hardware-friendly quantization scheme for both weight and
activations; (2) a novel affordable layer-by-layer knowledge distillation
algorithm (LKD) even without the access to the original training data; (3) a
highly-optimized quantization system backend support to remove the
quantization/dequantization overhead. As such, we are able to show that: (1)
ZeroQuant can reduce the precision for weights and activations to INT8 in a
cost-free way for both BERT and GPT3-style models with minimal accuracy impact,
which leads to up to 5.19x/4.16x speedup on those models compared to FP16
inference; (2) ZeroQuant plus LKD affordably quantize the weights in the
fully-connected module to INT4 along with INT8 weights in the attention module
and INT8 activations, resulting in 3x memory footprint reduction compared to
the FP16 model; (3) ZeroQuant can be directly applied to two of the largest
open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our
INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x
better efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Initial Study into Application of Feature Density and Linguistically-backed Embedding to Improve Machine Learning-based Cyberbullying Detection. (arXiv:2206.01889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01889">
<div class="article-summary-box-inner">
<span><p>In this research, we study the change in the performance of machine learning
(ML) classifiers when various linguistic preprocessing methods of a dataset
were used, with the specific focus on linguistically-backed embeddings in
Convolutional Neural Networks (CNN). Moreover, we study the concept of Feature
Density and confirm its potential to comparatively predict the performance of
ML classifiers, including CNN. The research was conducted on a Formspring
dataset provided in a Kaggle competition on automatic cyberbullying detection.
The dataset was re-annotated by objective experts (psychologists), as the
importance of professional annotation in cyberbullying research has been
indicated multiple times. The study confirmed the effectiveness of Neural
Networks in cyberbullying detection and the correlation between classifier
performance and Feature Density while also proposing a new approach of training
various linguistically-backed embeddings for Convolutional Neural Networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Audio Captioning with Epochal Difficult Captions for Curriculum Learning. (arXiv:2206.01918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01918">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an algorithm, Epochal Difficult Captions, to
supplement the training of any model for the Automated Audio Captioning task.
Epochal Difficult Captions is an elegant evolution to the keyword estimation
task that previous work have used to train the encoder of the AAC model.
Epochal Difficult Captions modifies the target captions based on a curriculum
and a difficulty level determined as a function of current epoch. Epochal
Difficult Captions can be used with any model architecture and is a lightweight
function that does not increase training time. We test our results on three
systems and show that using Epochal Difficult Captions consistently improves
performance
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Potential of Feature Density in Estimating Machine Learning Classifier Performance with Application to Cyberbullying Detection. (arXiv:2206.01949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01949">
<div class="article-summary-box-inner">
<span><p>In this research. we analyze the potential of Feature Density (HD) as a way
to comparatively estimate machine learning (ML) classifier performance prior to
training. The goal of the study is to aid in solving the problem of
resource-intensive training of ML models which is becoming a serious issue due
to continuously increasing dataset sizes and the ever rising popularity of Deep
Neural Networks (DNN). The issue of constantly increasing demands for more
powerful computational resources is also affecting the environment, as training
large-scale ML models are causing alarmingly-growing amounts of CO2, emissions.
Our approach 1s to optimize the resource-intensive training of ML models for
Natural Language Processing to reduce the number of required experiments
iterations. We expand on previous attempts on improving classifier training
efficiency with FD while also providing an insight to the effectiveness of
various linguistically-backed feature preprocessing methods for dialog
classification, specifically cyberbullying detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Performance of Different Linguistically-Backed Word Embeddings for Cyberbullying Detection. (arXiv:2206.01950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01950">
<div class="article-summary-box-inner">
<span><p>In most cases, word embeddings are learned only from raw tokens or in some
cases, lemmas. This includes pre-trained language models like BERT. To
investigate on the potential of capturing deeper relations between lexical
items and structures and to filter out redundant information, we propose to
preserve the morphological, syntactic and other types of linguistic information
by combining them with the raw tokens or lemmas. This means, for example,
including parts-of-speech or dependency information within the used lexical
features. The word embeddings can then be trained on the combinations instead
of just raw tokens. It is also possible to later apply this method to the
pre-training of huge language models and possibly enhance their performance.
This would aid in tackling problems which are more sophisticated from the point
of view of linguistic representation, such as detection of cyberbullying.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-wise Prompt Tuning for Pretrained Language Models. (arXiv:2206.01958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01958">
<div class="article-summary-box-inner">
<span><p>Prompt Learning has recently gained great popularity in bridging the gap
between pretraining tasks and various downstream tasks. It freezes Pretrained
Language Models (PLMs) and only tunes a few task-related parameters (prompts)
for downstream tasks, greatly reducing the cost of tuning giant models. The key
enabler of this is the idea of querying PLMs with task-specific knowledge
implicated in prompts. This paper reveals a major limitation of existing
methods that the indiscriminate prompts for all input data in a task ignore the
intrinsic knowledge from input data, resulting in sub-optimal performance. We
introduce Instance-wise Prompt Tuning (IPT), the first prompt learning paradigm
that injects knowledge from the input data instances to the prompts, thereby
providing PLMs with richer and more concrete context information. We devise a
series of strategies to produce instance-wise prompts, addressing various
concerns like model quality and cost-efficiency. Across multiple tasks and
resource settings, IPT significantly outperforms task-based prompt learning
methods, and achieves comparable performance to conventional finetuning with
only 0.5% - 1.5% of tuned parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Openness of CLIP. (arXiv:2206.01986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01986">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) has demonstrated great
potential in realizing open-vocabulary image classification in a matching
style, because of its holistic use of natural language supervision that covers
unconstrained real-world visual concepts. However, it is, in turn, also
difficult to evaluate and analyze the openness of CLIP-like models, since they
are in theory open to any vocabulary but the actual accuracy varies. To address
the insufficiency of conventional studies on openness, we resort to an
incremental view and define the extensibility, which essentially approximates
the model's ability to deal with new visual concepts, by evaluating openness
through vocabulary expansions. Our evaluation based on extensibility shows that
CLIP-like models are hardly truly open and their performances degrade as the
vocabulary expands to different degrees. Further analysis reveals that the
over-estimation of openness is not because CLIP-like models fail to capture the
general similarity of image and text features of novel visual concepts, but
because of the confusion among competing text features, that is, they are not
stable with respect to the vocabulary. In light of this, we propose to improve
the openness of CLIP from the perspective of feature space by enforcing the
distinguishability of text features. Our method retrieves relevant texts from
the pre-training corpus to enhance prompts for inference, which boosts the
extensibility and stability of CLIP even without fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Atypical lexical abbreviations identification in Russian medical texts. (arXiv:2206.01987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01987">
<div class="article-summary-box-inner">
<span><p>Abbreviation is a method of word formation that aims to construct the
shortened term from the first letters of the initial phrase. Implicit
abbreviations frequently cause the comprehension difficulties for unprepared
readers. In this paper, we propose an efficient ML-based algorithm which allows
to identify the abbreviations in Russian texts. The method achieves ROC AUC
score 0.926 and F1 score 0.706 which are confirmed as competitive in comparison
with the baselines. Along with the pipeline, we also establish first to our
knowledge Russian dataset that is relevant for the desired task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Actuarial Applications of Natural Language Processing Using Transformers: Case Studies for Using Text Features in an Actuarial Context. (arXiv:2206.02014v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02014">
<div class="article-summary-box-inner">
<span><p>This tutorial demonstrates workflows to incorporate text data into actuarial
classification and regression tasks. The main focus is on methods employing
transformer-based models. A dataset of car accident descriptions with an
average length of 400 words, available in English and German, and a dataset
with short property insurance claims descriptions are used to demonstrate these
techniques. The case studies tackle challenges related to a multi-lingual
setting and long input sequences. They also show ways to interpret model
output, to assess and improve model performance, by fine-tuning the models to
the domain of application or to a specific prediction task. Finally, the
tutorial provides practical approaches to handle classification tasks in
situations with no or only few labeled data. The results achieved by using the
language-understanding skills of off-the-shelf natural language processing
(NLP) models with only minimal pre-processing and fine-tuning clearly
demonstrate the power of transfer learning for practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders. (arXiv:2206.02079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02079">
<div class="article-summary-box-inner">
<span><p>Recent work in multilingual translation advances translation quality
surpassing bilingual baselines using deep transformer models with increased
capacity. However, the extra latency and memory costs introduced by this
approach may make it unacceptable for efficiency-constrained applications. It
has recently been shown for bilingual translation that using a deep encoder and
shallow decoder (DESD) can reduce inference latency while maintaining
translation quality, so we study similar speed-accuracy trade-offs for
multilingual translation. We find that for many-to-one translation we can
indeed increase decoder speed without sacrificing quality using this approach,
but for one-to-many translation, shallow decoders cause a clear quality drop.
To ameliorate this drop, we propose a deep encoder with multiple shallow
decoders (DEMSD) where each shallow decoder is responsible for a disjoint
subset of target languages. Specifically, the DEMSD model with 2-layer decoders
is able to obtain a 1.8x speedup on average compared to a standard transformer
model with no drop in translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAE: Language-Aware Encoder for Monolingual and Multilingual ASR. (arXiv:2206.02093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02093">
<div class="article-summary-box-inner">
<span><p>Despite the rapid progress in automatic speech recognition (ASR) research,
recognizing multilingual speech using a unified ASR system remains highly
challenging. Previous works on multilingual speech recognition mainly focus on
two directions: recognizing multiple monolingual speech or recognizing
code-switched speech that uses different languages interchangeably within a
single utterance. However, a pragmatic multilingual recognizer is expected to
be compatible with both directions. In this work, a novel language-aware
encoder (LAE) architecture is proposed to handle both situations by
disentangling language-specific information and generating frame-level
language-aware representations during encoding. In the LAE, the primary
encoding is implemented by the shared block while the language-specific blocks
are used to extract specific representations for each language. To learn
language-specific information discriminatively, a language-aware training
method is proposed to optimize the language-specific blocks in LAE. Experiments
conducted on Mandarin-English code-switched speech suggest that the proposed
LAE is capable of discriminating different languages in frame-level and shows
superior performance on both monolingual and multilingual ASR tasks. With
either a real-recorded or simulated code-switched dataset, the proposed LAE
achieves statistically significant improvements on both CTC and neural
transducer systems. Code is released
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Detection Task Against Asian Hate: BERT the Central, While Data-Centric Studies the Crucial. (arXiv:2206.02114v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02114">
<div class="article-summary-box-inner">
<span><p>With the epidemic continuing, hatred against Asians is intensifying in
countries outside Asia, especially among the Chinese. Thus, there is an urgent
need to detect and prevent hate speech toward Asians effectively. In this work,
we first create COVID-HATE-2022, an annotated dataset that is an extension of
the anti-Asian hate speech dataset on Twitter, including 2,035 annotated tweets
fetched in early February 2022, which are labeled based on specific criteria,
and we present the comprehensive collection of scenarios of hate and non-hate
tweets in the dataset. Second, we fine-tune the BERT models based on the
relevant datasets, and demonstrate strategies including 1) cleaning the
hashtags, usernames being @, URLs, and emojis before the fine-tuning process,
and 2) training with the data while validating with the "clean" data (and the
opposite) are not effective for improving performance. Third, we investigate
the performance of advanced fine-tuning strategies with 1) model-centric
approaches, such as discriminative fine-tuning, gradual unfreezing, and warmup
steps, and 2) data-centric approaches, which incorporate data trimming and data
augmenting, and show that both strategies generally improve the performance,
while data-centric ones outperform the others, which demonstrate the
feasibility and effectiveness of the data-centric approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal Corpus for Emotion Recognition in Sarcasm. (arXiv:2206.02119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02119">
<div class="article-summary-box-inner">
<span><p>While sentiment and emotion analysis have been studied extensively, the
relationship between sarcasm and emotion has largely remained unexplored. A
sarcastic expression may have a variety of underlying emotions. For example, "I
love being ignored" belies sadness, while "my mobile is fabulous with a battery
backup of only 15 minutes!" expresses frustration. Detecting the emotion behind
a sarcastic expression is non-trivial yet an important task. We undertake the
task of detecting the emotion in a sarcastic statement, which to the best of
our knowledge, is hitherto unexplored. We start with the recently released
multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions.
We identify and correct 343 incorrect emotion labels (out of 690). We double
the size of the dataset, label it with emotions along with valence and arousal
which are important indicators of emotional intensity. Finally, we label each
sarcastic utterance with one of the four sarcasm types-Propositional, Embedded,
Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection
research. Exhaustive experimentation with multimodal (text, audio, and video)
fusion models establishes a benchmark for exact emotion recognition in sarcasm
and outperforms the state-of-art sarcasm detection. We release the dataset
enriched with various annotations and the code for research purposes:
https://github.com/apoorva-nunna/MUStARD_Plus_Plus
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech. (arXiv:2206.02147v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02147">
<div class="article-summary-box-inner">
<span><p>Polyphone disambiguation aims to capture accurate pronunciation knowledge
from natural text sequences for reliable Text-to-speech (TTS) systems. However,
previous approaches require substantial annotated training data and additional
efforts from language experts, making it difficult to extend high-quality
neural TTS systems to out-of-domain daily conversations and countless languages
worldwide. This paper tackles the polyphone disambiguation problem from a
concise and novel perspective: we propose Dict-TTS, a semantic-aware generative
text-to-speech model with an online website dictionary (the existing prior
information in the natural language). Specifically, we design a
semantics-to-pronunciation attention (S2PA) module to match the semantic
patterns between the input text sequence and the prior semantics in the
dictionary and obtain the corresponding pronunciations; The S2PA module can be
easily trained with the end-to-end TTS model without any annotated phoneme
labels. Experimental results in three languages show that our model outperforms
several strong baseline models in terms of pronunciation accuracy and improves
the prosody modeling of TTS systems. Further extensive analyses with different
linguistic encoders demonstrate that each design in Dict-TTS is effective.
Audio samples are available at \url{https://dicttts.github.io/DictTTS-Demo/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis of Online Travel Reviews Based on Capsule Network and Sentiment Lexicon. (arXiv:2206.02160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02160">
<div class="article-summary-box-inner">
<span><p>With the development of online travel services, it has great application
prospects to timely mine users' evaluation emotions for travel services and use
them as indicators to guide the improvement of online travel service quality.
In this paper, we study the text sentiment classification of online travel
reviews based on social media online comments and propose the SCCL model based
on capsule network and sentiment lexicon. SCCL model aims at the lack of
consideration of local features and emotional semantic features of the text in
the language model that can efficiently extract text context features like BERT
and GRU. Then make the following improvements to their shortcomings. On the one
hand, based on BERT-BiGRU, the capsule network is introduced to extract local
features while retaining good context features. On the other hand, the
sentiment lexicon is introduced to extract the emotional sequence of the text
to provide richer emotional semantic features for the model. To enhance the
universality of the sentiment lexicon, the improved SO-PMI algorithm based on
TF-IDF is used to expand the lexicon, so that the lexicon can also perform well
in the field of online travel reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Near-Term Advances in Quantum Natural Language Processing. (arXiv:2206.02171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02171">
<div class="article-summary-box-inner">
<span><p>This paper describes experiments showing that some problems in natural
language processing can already be addressed using quantum computers. The
examples presented here include topic classification using both a quantum
support vector machine and a bag-of-words approach, bigram modeling that can be
applied to sequences of words and formal concepts, and ambiguity resolution in
verb-noun composition.
</p>
<p>While the datasets used are still small, the systems described have been run
on physical quantum computers. These implementations and their results are
described along with the algorithms and mathematical approaches used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Meta-learning Paradigm for Zero-shot Intent Classification with Mixture Attention Mechanism. (arXiv:2206.02179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02179">
<div class="article-summary-box-inner">
<span><p>Zero-shot intent classification is a vital and challenging task in dialogue
systems, which aims to deal with numerous fast-emerging unacquainted intents
without annotated training data. To obtain more satisfactory performance, the
crucial points lie in two aspects: extracting better utterance features and
strengthening the model generalization ability. In this paper, we propose a
simple yet effective meta-learning paradigm for zero-shot intent
classification. To learn better semantic representations for utterances, we
introduce a new mixture attention mechanism, which encodes the pertinent word
occurrence patterns by leveraging the distributional signature attention and
multi-layer perceptron attention simultaneously. To strengthen the transfer
ability of the model from seen classes to unseen classes, we reformulate
zero-shot intent classification with a meta-learning strategy, which trains the
model by simulating multiple zero-shot classification tasks on seen categories,
and promotes the model generalization ability with a meta-adapting procedure on
mimic unseen categories. Extensive experiments on two real-world dialogue
datasets in different languages show that our model outperforms other strong
baselines on both standard and generalized zero-shot intent classification
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance Comparison of Simple Transformer and Res-CNN-BiLSTM for Cyberbullying Classification. (arXiv:2206.02206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02206">
<div class="article-summary-box-inner">
<span><p>The task of text classification using Bidirectional based LSTM architectures
is computationally expensive and time consuming to train. For this,
transformers were discovered which effectively give good performance as
compared to the traditional deep learning architectures. In this paper we
present a performance based comparison between simple transformer based network
and Res-CNN-BiLSTM based network for cyberbullying text classification problem.
The results obtained show that transformer we trained with 0.65 million
parameters has significantly being able to beat the performance of
Res-CNN-BiLSTM with 48.82 million parameters for faster training speeds and
more generalized metrics. The paper also compares the 1-dimensional character
level embedding network and 100-dimensional glove embedding network with
transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stylistic Fingerprints, POS-tags and Inflected Languages: A Case Study in Polish. (arXiv:2206.02208v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02208">
<div class="article-summary-box-inner">
<span><p>In stylometric investigations, frequencies of the most frequent words (MFWs)
and character n-grams outperform other style-markers, even if their performance
varies significantly across languages. In inflected languages, word endings
play a prominent role, and hence different word forms cannot be recognized
using generic text tokenization. Countless inflected word forms make
frequencies sparse, making most statistical procedures complicated. Presumably,
applying one of the NLP techniques, such as lemmatization and/or parsing, might
increase the performance of classification. The aim of this paper is to examine
the usefulness of grammatical features (as assessed via POS-tag n-grams) and
lemmatized forms in recognizing authorial profiles, in order to address the
underlying issue of the degree of freedom of choice within lexis and grammar.
Using a corpus of Polish novels, we performed a series of supervised authorship
attribution benchmarks, in order to compare the classification accuracy for
different types of lexical and syntactic style-markers. Even if the performance
of POS-tags as well as lemmatized forms was notoriously worse than that of
lexical markers, the difference was not substantial and never exceeded ca. 15%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. (arXiv:2206.02211v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02211">
<div class="article-summary-box-inner">
<span><p>The success of deep learning comes from its ability to capture the
hierarchical structure of data by learning high-level representations defined
in terms of low-level ones. In this paper we explore self-supervised learning
of hierarchical representations of speech by applying multiple levels of
Contrastive Predictive Coding (CPC). We observe that simply stacking two CPC
models does not yield significant improvements over single-level architectures.
Inspired by the fact that speech is often described as a sequence of discrete
units unevenly distributed in time, we propose a model in which the output of a
low-level CPC module is non-uniformly downsampled to directly minimize the loss
of a high-level CPC module. The latter is designed to also enforce a prior of
separability and discreteness in its representations by enforcing dissimilarity
of successive high-level representations through focused negative sampling, and
by quantization of the prediction targets. Accounting for the structure of the
speech signal improves upon single-level CPC features and enhances the
disentanglement of the learned representations, as measured by downstream
speech recognition tasks, while resulting in a meaningful segmentation of the
signal that closely resembles phone boundaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning a Kalaallisut-English machine translation system using web-crawled data. (arXiv:2206.02230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02230">
<div class="article-summary-box-inner">
<span><p>West Greenlandic, known by native speakers as Kalaallisut, is an extremely
low-resource polysynthetic language spoken by around 56,000 people in
Greenland. Here, we attempt to finetune a pretrained Kalaallisut-to-English
neural machine translation (NMT) system using web-crawled pseudoparallel
sentences from around 30 multilingual websites. We compile a corpus of over
93,000 Kalaallisut sentences and over 140,000 Danish sentences, then use
cross-lingual sentence embeddings and approximate nearest-neighbors search in
an attempt to mine near-translations from these corpora. Finally, we translate
the Danish sentence to English to obtain a synthetic Kalaallisut-English
aligned corpus. Although the resulting dataset is too small and noisy to
improve the pretrained MT model, we believe that with additional resources, we
could construct a better pseudoparallel corpus and achieve more promising
results on MT. We also note other possible uses of the monolingual Kalaallisut
data and discuss directions for future work. We make the code and data for our
experiments publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Cross-lingual Textual Style Transfer with Large Multilingual Language Models. (arXiv:2206.02252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02252">
<div class="article-summary-box-inner">
<span><p>Detoxification is a task of generating text in polite style while preserving
meaning and fluency of the original toxic text. Existing detoxification methods
are designed to work in one exact language. This work investigates multilingual
and cross-lingual detoxification and the behavior of large multilingual models
like in this setting. Unlike previous works we aim to make large language
models able to perform detoxification without direct fine-tuning in given
language. Experiments show that multilingual models are capable of performing
multilingual style transfer. However, models are not able to perform
cross-lingual detoxification and direct fine-tuning on exact language is
inevitable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future. (arXiv:2206.02280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02280">
<div class="article-summary-box-inner">
<span><p>Annotated data is an essential ingredient in natural language processing for
training and evaluating machine learning models. It is therefore very desirable
for the annotations to be of high quality. Recent work, however, has shown that
several popular datasets contain a surprising amount of annotation errors or
inconsistencies. To alleviate this issue, many methods for annotation error
detection have been devised over the years. While researchers show that their
approaches work well on their newly introduced datasets, they rarely compare
their methods to previous work or on the same datasets. This raises strong
concerns on methods' general performance and makes it difficult to asses their
strengths and weaknesses. We therefore reimplement 18 methods for detecting
potential annotation errors and evaluate them on 9 English datasets for text
classification as well as token and span labeling. In addition, we define a
uniform evaluation setup including a new formalization of the annotation error
detection task, evaluation protocol and general best practices. To facilitate
future research and reproducibility, we release our datasets and
implementations in an easy-to-use and open source software package.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretrained Models for Multilingual Federated Learning. (arXiv:2206.02291v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02291">
<div class="article-summary-box-inner">
<span><p>Since the advent of Federated Learning (FL), research has applied these
methods to natural language processing (NLP) tasks. Despite a plethora of
papers in FL for NLP, no previous works have studied how multilingual text
impacts FL algorithms. Furthermore, multilingual text provides an interesting
avenue to examine the impact of non-IID text (e.g. different languages) on FL
in naturally occurring data. We explore three multilingual language tasks,
language modeling, machine translation, and text classification using differing
federated and non-federated learning algorithms. Our results show that using
pretrained models reduces the negative effects of FL, helping them to perform
near or better than centralized (no privacy) learning, even when using non-IID
partitioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Advance of Making Language Models Better Reasoners. (arXiv:2206.02336v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02336">
<div class="article-summary-box-inner">
<span><p>Large language models such as GPT-3 and PaLM have shown remarkable
performance in few-shot learning. However, they still struggle with reasoning
tasks such as the arithmetic benchmark GSM8K. Recent advances deliberately
guide the language model to generate a chain of reasoning steps before
producing the final answer, successfully boosting the GSM8K benchmark from
17.9% to 58.1% in terms of problem solving rate. In this paper, we propose a
new approach, DiVeRSe (Diverse Verifier on Reasoning Step), to further advance
their reasoning capability. DiVeRSe first explores different prompts to enhance
the diversity in reasoning paths. Second, DiVeRSe introduces a verifier to
distinguish good answers from bad answers for a better weighted voting.
Finally, DiVeRSe verifies the correctness of each single step rather than all
the steps in a whole. We conduct extensive experiments using the latest
language model code-davinci-002 and demonstrate that DiVeRSe can achieve new
state-of-the-art performance on six out of eight reasoning benchmarks (e.g.,
GSM8K 74.4% to 83.2%), outperforming the PaLM model with 540B parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Automated Fact-Checking. (arXiv:2108.11896v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11896">
<div class="article-summary-box-inner">
<span><p>Fact-checking has become increasingly important due to the speed with which
both information and misinformation can spread in the modern media ecosystem.
Therefore, researchers have been exploring how fact-checking can be automated,
using techniques based on natural language processing, machine learning,
knowledge representation, and databases to automatically predict the veracity
of claims. In this paper, we survey automated fact-checking stemming from
natural language processing, and discuss its connections to related tasks and
disciplines. In this process, we present an overview of existing datasets and
models, aiming to unify the various definitions given and identify common
concepts. Finally, we highlight challenges for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N24News: A New Dataset for Multimodal News Classification. (arXiv:2108.13327v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13327">
<div class="article-summary-box-inner">
<span><p>Current news datasets merely focus on text features on the news and rarely
leverage the feature of images, excluding numerous essential features for news
classification. In this paper, we propose a new dataset, N24News, which is
generated from New York Times with 24 categories and contains both text and
image information in each news. We use a multitask multimodal method and the
experimental results show multimodal news classification performs better than
text-only news classification. Depending on the length of the text, the
classification accuracy can be increased by up to 8.11%. Our research reveals
the relationship between the performance of a multimodal classifier and its
sub-classifiers, and also the possible improvements when applying multimodal in
news classification. N24News is shown to have great potential to prompt the
multimodal news studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text analysis and deep learning: A network approach. (arXiv:2110.04151v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04151">
<div class="article-summary-box-inner">
<span><p>Much information available to applied researchers is contained within written
language or spoken text. Deep language models such as BERT have achieved
unprecedented success in many applications of computational linguistics.
However, much less is known about how these models can be used to analyze
existing text. We propose a novel method that combines transformer models with
network analysis to form a self-referential representation of language use
within a corpus of interest. Our approach produces linguistic relations
strongly consistent with the underlying model as well as mathematically
well-defined operations on them, while reducing the amount of discretionary
choices of representation and distance measures. It represents, to the best of
our knowledge, the first unsupervised method to extract semantic networks
directly from deep language models. We illustrate our approach in a semantic
analysis of the term "founder". Using the entire corpus of Harvard Business
Review from 1980 to 2020, we find that ties in our network track the semantics
of discourse over time, and across contexts, identifying and relating clusters
of semantic and syntactic relations. Finally, we discuss how this method can
also complement and inform analyses of the behavior of deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. (arXiv:2110.04330v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04330">
<div class="article-summary-box-inner">
<span><p>Current Open-Domain Question Answering (ODQA) model paradigm often contains a
retrieving module and a reading module. Given an input question, the reading
module predicts the answer from the relevant passages which are retrieved by
the retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on
top of the pretrained generative model T5, achieves the state-of-the-art
performance in the reading module. Although being effective, it remains
constrained by inefficient attention on all retrieved passages which contain a
lot of noise. In this work, we propose a novel method KG-FiD, which filters
noisy passages by leveraging the structural relationship among the retrieved
passages with a knowledge graph. We initiate the passage node embedding from
the FiD encoder and then use graph neural network (GNN) to update the
representation for reranking. To improve the efficiency, we build the GNN on
top of the intermediate layer output of the FiD encoder and only pass a few top
reranked passages into the higher layers of encoder and decoder for answer
generation. We also apply the proposed GNN based reranking method to enhance
the passage retrieval results in the retrieving module. Extensive experiments
on common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate
that KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score
and achieve comparable performance with FiD with only 40% of computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting to the Long Tail: A Meta-Analysis of Transfer Learning Research for Language Understanding Tasks. (arXiv:2111.01340v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01340">
<div class="article-summary-box-inner">
<span><p>Natural language understanding (NLU) has made massive progress driven by
large benchmarks, but benchmarks often leave a long tail of infrequent
phenomena underrepresented. We reflect on the question: have transfer learning
methods sufficiently addressed the poor performance of benchmark-trained models
on the long tail? We conceptualize the long tail using macro-level dimensions
(e.g., underrepresented genres, topics, etc.), and perform a qualitative
meta-analysis of 100 representative papers on transfer learning research for
NLU. Our analysis asks three questions: (i) Which long tail dimensions do
transfer learning studies target? (ii) Which properties of adaptation methods
help improve performance on the long tail? (iii) Which methodological gaps have
greatest negative impact on long tail performance? Our answers highlight major
avenues for future research in transfer learning for the long tail. Lastly,
using our meta-analysis framework, we perform a case study comparing the
performance of various adaptation methods on clinical narratives, which
provides interesting insights that may enable us to make progress along these
future avenues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR. (arXiv:2111.06799v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06799">
<div class="article-summary-box-inner">
<span><p>We present a method for cross-lingual training an ASR system using absolutely
no transcribed training data from the target language, and with no phonetic
knowledge of the language in question. Our approach uses a novel application of
a decipherment algorithm, which operates given only unpaired speech and text
data from the target language. We apply this decipherment to phone sequences
generated by a universal phone recogniser trained on out-of-language speech
corpora, which we follow with flat-start semi-supervised training to obtain an
acoustic model for the new language. To the best of our knowledge, this is the
first practical approach to zero-resource cross-lingual ASR which does not rely
on any hand-crafted phonetic information. We carry out experiments on read
speech from the GlobalPhone corpus, and show that it is possible to learn a
decipherment model on just 20 minutes of data from the target language. When
used to generate pseudo-labels for semi-supervised training, we obtain WERs
that range from 32.5% to just 1.9% absolute worse than the equivalent fully
supervised models trained on the same data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Effects on Pre-trained Models for Language Processing Tasks. (arXiv:2111.12790v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12790">
<div class="article-summary-box-inner">
<span><p>Keeping the performance of language technologies optimal as time passes is of
great practical interest. We study temporal effects on model performance on
downstream language tasks, establishing a nuanced terminology for such
discussion and identifying factors essential to conduct a robust study. We
present experiments for several tasks in English where the label correctness is
not dependent on time and demonstrate the importance of distinguishing between
temporal model deterioration and temporal domain adaptation for systems using
pre-trained representations. We find that depending on the task, temporal model
deterioration is not necessarily a concern. Temporal domain adaptation however
is beneficial in all cases, with better performance for a given time period
possible when the system is trained on temporally more recent data. Therefore,
we also examine the efficacy of two approaches for temporal domain adaptation
without human annotations on new data. Self-labeling shows consistent
improvement and notably, for named entity recognition, leads to better temporal
adaptation than even human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Distillation for Language Models. (arXiv:2112.02505v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02505">
<div class="article-summary-box-inner">
<span><p>Distillation efforts have led to language models that are more compact and
efficient without serious drops in performance. The standard approach to
distillation trains a student model against two objectives: a task-specific
objective (e.g., language modeling) and an imitation objective that encourages
the hidden states of the student model to be similar to those of the larger
teacher model. In this paper, we show that it is beneficial to augment
distillation with a third objective that encourages the student to imitate the
causal computation process of the teacher through interchange intervention
training(IIT). IIT pushes the student model to become a causal abstraction of
the teacher model - a simpler model with the same causal structure. IIT is
fully differentiable, easily implemented, and combines flexibly with other
objectives. Compared with standard distillation of BERT, distillation via IIT
results in lower perplexity on Wikipedia (masked language modeling) and marked
improvements on the GLUE benchmark (natural language understanding), SQuAD
(question answering), and CoNLL-2003 (named entity recognition).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01337">
<div class="article-summary-box-inner">
<span><p>Traditional text classification approaches often require a good amount of
labeled data, which is difficult to obtain, especially in restricted domains or
less widespread languages. This lack of labeled data has led to the rise of
low-resource methods, that assume low data availability in natural language
processing. Among them, zero-shot learning stands out, which consists of
learning a classifier without any previously labeled data. The best results
reported with this approach use language models such as Transformers, but fall
into two problems: high execution time and inability to handle long texts as
input. This paper proposes a new model, ZeroBERTo, which leverages an
unsupervised clustering step to obtain a compressed data representation before
the classification task. We show that ZeroBERTo has better performance for long
inputs and shorter execution time, outperforming XLM-R by about 12% in the F1
score in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,
Zero-Shot Learning, Topic Modeling, Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Representative Keywords Selection: A Probabilistic Approach. (arXiv:2203.10365v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10365">
<div class="article-summary-box-inner">
<span><p>We propose a probabilistic approach to select a subset of a \textit{target
domain representative keywords} from a candidate set, contrasting with a
context domain. Such a task is crucial for many downstream tasks in natural
language processing. To contrast the target domain and the context domain, we
adapt the \textit{two-component mixture model} concept to generate a
distribution of candidate keywords. It provides more importance to the
\textit{distinctive} keywords of the target domain than common keywords
contrasting with the context domain. To support the \textit{representativeness}
of the selected keywords towards the target domain, we introduce an
\textit{optimization algorithm} for selecting the subset from the generated
candidate distribution. We have shown that the optimization algorithm can be
efficiently implemented with a near-optimal approximation guarantee. Finally,
extensive experiments on multiple domains demonstrate the superiority of our
approach over other baselines for the tasks of keyword summary generation and
trending keywords selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Transformations in Contrastive Self-Supervised Learning: A Review. (arXiv:2203.12000v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12000">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning has become a prominent technique in
representation learning. The main step in these methods is to contrast
semantically similar and dissimilar pairs of samples. However, in the domain of
Natural Language Processing (NLP), the augmentation methods used in creating
similar pairs with regard to contrastive learning (CL) assumptions are
challenging. This is because, even simply modifying a word in the input might
change the semantic meaning of the sentence, and hence, would violate the
distributional hypothesis. In this review paper, we formalize the contrastive
learning framework, emphasize the considerations that need to be addressed in
the data transformation step, and review the state-of-the-art methods and
evaluations for contrastive representation learning in NLP. Finally, we
describe some challenges and potential directions for learning better text
representations using contrastive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12667">
<div class="article-summary-box-inner">
<span><p>A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations. (arXiv:2204.09781v3 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09781">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has been severely impacting global society since
December 2019. Massive research has been undertaken to understand the
characteristics of the virus and design vaccines and drugs. The related
findings have been reported in biomedical literature at a rate of about 10,000
articles on COVID-19 per month. Such rapid growth significantly challenges
manual curation and interpretation. For instance, LitCovid is a literature
database of COVID-19-related articles in PubMed, which has accumulated more
than 200,000 articles with millions of accesses each month by users worldwide.
One primary curation task is to assign up to eight topics (e.g., Diagnosis and
Treatment) to the articles in LitCovid. Despite the continuing advances in
biomedical text mining methods, few have been dedicated to topic annotations in
COVID-19 literature. To close the gap, we organized the BioCreative LitCovid
track to call for a community effort to tackle automated topic annotation for
COVID-19 literature. The BioCreative LitCovid dataset, consisting of over
30,000 articles with manually reviewed topics, was created for training and
testing. It is one of the largest multilabel classification datasets in
biomedical scientific literature. 19 teams worldwide participated and made 80
submissions in total. Most teams used hybrid systems based on transformers. The
highest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro
F1-score, micro F1-score, and instance-based F1-score, respectively. The level
of participation and results demonstrate a successful track and help close the
gap between dataset curation and method development. The dataset is publicly
available via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/ for
benchmarking and further development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Document Re-ranking with Modular Re-ranker. (arXiv:2205.04275v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04275">
<div class="article-summary-box-inner">
<span><p>Long document re-ranking has been a challenging problem for neural re-rankers
based on deep language models like BERT. Early work breaks the documents into
short passage-like chunks. These chunks are independently mapped to scalar
scores or latent vectors, which are then pooled into a final relevance score.
These encode-and-pool methods however inevitably introduce an information
bottleneck: the low dimension representations. In this paper, we propose
instead to model full query-to-document interaction, leveraging the attention
operation and modular Transformer re-ranker framework. First, document chunks
are encoded independently with an encoder module. An interaction module then
encodes the query and performs joint attention from the query to all document
chunk representations. We demonstrate that the model can use this new degree of
freedom to aggregate important information from the entire document. Our
experiments show that this design produces effective re-ranking on two
classical IR collections Robust04 and ClueWeb09, and a large-scale supervised
collection MS-MARCO document ranking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities. (arXiv:2205.10312v2 [cs.DB] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10312">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA) aims at finding equivalent entities in different
knowledge graphs (KGs). Embedding-based approaches have dominated the EA task
in recent years. Those methods face problems that come from the geometric
properties of embedding vectors, including hubness and isolation. To solve
these geometric problems, many normalization approaches have been adopted for
EA. However, the increasing scale of KGs renders it hard for EA models to adopt
the normalization processes, thus limiting their usage in real-world
applications. To tackle this challenge, we present ClusterEA, a general
framework that is capable of scaling up EA models and enhancing their results
by leveraging normalization methods on mini-batches with a high entity
equivalent rate. ClusterEA contains three components to align entities between
large-scale KGs, including stochastic training, ClusterSampler, and
SparseFusion. It first trains a large-scale Siamese GNN for EA in a stochastic
fashion to produce entity embeddings. Based on the embeddings, a novel
ClusterSampler strategy is proposed for sampling highly overlapped
mini-batches. Finally, ClusterEA incorporates SparseFusion, which normalizes
local and global similarity and then fuses all similarity matrices to obtain
the final similarity matrix. Extensive experiments with real-life datasets on
EA benchmarks offer insight into the proposed framework, and suggest that it is
capable of outperforming the state-of-the-art scalable EA framework by up to 8
times in terms of Hits@1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12986">
<div class="article-summary-box-inner">
<span><p>Sentence scoring aims at measuring the likelihood score of a sentence and is
widely used in many natural language processing scenarios, like reranking,
which is to select the best sentence from multiple candidates. Previous works
on sentence scoring mainly adopted either causal language modeling (CLM) like
GPT or masked language modeling (MLM) like BERT, which have some limitations:
1) CLM only utilizes unidirectional information for the probability estimation
of a sentence without considering bidirectional context, which affects the
scoring quality; 2) MLM can only estimate the probability of partial tokens at
a time and thus requires multiple forward passes to estimate the probability of
the whole sentence, which incurs large computation and time cost. In this
paper, we propose \textit{Transcormer} -- a Transformer model with a novel
\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,
our SLM adopts a triple-stream self-attention mechanism to estimate the
probability of all tokens in a sentence with bidirectional context and only
requires a single forward pass. SLM can avoid the limitations of CLM (only
unidirectional context) and MLM (multiple forward passes) and inherit their
advantages, and thus achieve high effectiveness and efficiency in scoring.
Experimental results on multiple tasks demonstrate that our method achieves
better performance than other language modelings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Causal Inference for Explainable Automatic Program Repair. (arXiv:2205.13342v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13342">
<div class="article-summary-box-inner">
<span><p>Deep learning models have made significant progress in automatic program
repair. However, the black-box nature of these methods has restricted their
practical applications. To address this challenge, this paper presents an
interpretable approach for program repair based on sequence-to-sequence models
with causal inference and our method is called CPR, short for causal program
repair. Our CPR can generate explanations in the process of decision making,
which consists of groups of causally related input-output tokens. Firstly, our
method infers these relations by querying the model with inputs disturbed by
data augmentation. Secondly, it generates a graph over tokens from the
responses and solves a partitioning problem to select the most relevant
components. The experiments on four programming languages (Java, C, Python, and
JavaScript) show that CPR can generate causal graphs for reasonable
interpretations and boost the performance of bug fixing in automatic program
repair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection. (arXiv:2206.00962v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00962">
<div class="article-summary-box-inner">
<span><p>We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of machine learning approaches, challenges and prospects for computational tumor pathology. (arXiv:2206.01728v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01728">
<div class="article-summary-box-inner">
<span><p>Computational pathology is part of precision oncology medicine. The
integration of high-throughput data including genomics, transcriptomics,
proteomics, metabolomics, pathomics, and radiomics into clinical practice
improves cancer treatment plans, treatment cycles, and cure rates, and helps
doctors open up innovative approaches to patient prognosis. In the past decade,
rapid advances in artificial intelligence, chip design and manufacturing, and
mobile computing have facilitated research in computational pathology and have
the potential to provide better-integrated solutions for whole-slide images,
multi-omics data, and clinical informatics. However, tumor computational
pathology now brings some challenges to the application of tumour screening,
diagnosis and prognosis in terms of data integration, hardware processing,
network sharing bandwidth and machine learning technology. This review
investigates image preprocessing methods in computational pathology from a
pathological and technical perspective, machine learning-based methods, and
applications of computational pathology in breast, colon, prostate, lung, and
various tumour disease scenarios. Finally, the challenges and prospects of
machine learning in computational pathology applications are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs. (arXiv:2206.01731v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01731">
<div class="article-summary-box-inner">
<span><p>In this work, we present an empirical study of DCGANs for synthetic
generation of fetal head ultrasound, consisting of hyperparameter heuristics
and image quality assessment. We present experiments to show the impact of
different image sizes, epochs, data size input, and learning rates for quality
image assessment on four metrics: mutual information (MI), fr\'echet inception
distance (FID), peak-signal-to-noise ratio (PSNR), and local binary pattern
vector (LBPv). The results show that FID and LBPv have stronger relationship
with clinical image quality scores. The resources to reproduce this work are
available at \url{https://github.com/xfetus/miua2022}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline. (arXiv:2206.01733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01733">
<div class="article-summary-box-inner">
<span><p>Deep learning technologies have become the backbone for the development of
computer vision. With further explorations, deep neural networks have been
found vulnerable to well-designed adversarial attacks. Most of the vision
devices are equipped with image signal processing (ISP) pipeline to implement
RAW-to-RGB transformations and embedded into data preprocessing module for
efficient image processing. Actually, ISP pipeline can introduce adversarial
behaviors to post-capture images while data preprocessing may destroy attack
patterns. However, none of the existing adversarial attacks takes into account
the impacts of both ISP pipeline and data preprocessing. In this paper, we
develop an image-scaling attack targeting on ISP pipeline, where the crafted
adversarial RAW can be transformed into attack image that presents entirely
different appearance once being scaled to a specific-size image. We first
consider the gradient-available ISP pipeline, i.e., the gradient information
can be directly used in the generation process of adversarial RAW to launch the
attack. To make the adversarial attack more applicable, we further consider the
gradient-unavailable ISP pipeline, in which a proxy model that well learns the
RAW-to-RGB transformations is proposed as the gradient oracles. Extensive
experiments show that the proposed adversarial attacks can craft adversarial
RAW data against the target ISP pipelines with high attack rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using UAS Imagery and Computer Vision to Support Site-Specific Weed Control in Corn. (arXiv:2206.01734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01734">
<div class="article-summary-box-inner">
<span><p>Currently, weed control in a corn field is performed by a blanket application
of herbicides that do not consider spatial distribution information of weeds
and also uses an extensive amount of chemical herbicides. To reduce the amount
of chemicals, we used drone-based high-resolution imagery and computer-vision
techniques to perform site-specific weed control in corn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining the behaviour of state-of-the-art convolutional neural networks for brain tumor detection with and without transfer learning. (arXiv:2206.01735v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01735">
<div class="article-summary-box-inner">
<span><p>Distinguishing normal from malignant and determining the tumor type are
critical components of brain tumor diagnosis. Two different kinds of dataset
are investigated using state-of-the-art CNN models in this research work. One
dataset(binary) has images of normal and tumor types, while
another(multi-class) provides all images of tumors classified as glioma,
meningioma, or pituitary. The experiments were conducted in these dataset with
transfer learning from pre-trained weights from ImageNet as well as
initializing the weights randomly. The experimental environment is equivalent
for all models in this study in order to make a fair comparison. For both of
the dataset, the validation set are same for all the models where train data is
60% while the rest is 40% for validation. With the proposed techniques in this
research, the EfficientNet-B5 architecture outperforms all the state-of-the-art
models in the binary-classification dataset with the accuracy of 99.75% and
98.61% accuracy for the multi-class dataset. This research also demonstrates
the behaviour of convergence of validation loss in different weight
initialization techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection. (arXiv:2206.01736v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01736">
<div class="article-summary-box-inner">
<span><p>Recent methods based on Deep Neural Networks (DNNs) have reached high
accuracy for medical image analysis, including the three basic tasks:
segmentation, landmark detection, and object detection. It is known that DNNs
are vulnerable to adversarial attacks, and the adversarial robustness of DNNs
could be improved by adding adversarial noises to training data (i.e.,
adversarial training). In this study, we show that the standard adversarial
training (SAT) method has a severe issue that limits its practical use: it
generates a fixed level of noise for DNN training, and it is difficult for the
user to choose an appropriate noise level, because a high noise level may lead
to a large reduction in model performance, and a low noise level may have
little effect. To resolve this issue, we have designed a novel adaptive-margin
adversarial training (AMAT) method that generates adaptive adversarial noises
for DNN training, which are dynamically tailored for each individual training
sample. We have applied our AMAT method to state-of-the-art DNNs for the three
basic tasks, using five publicly available datasets. The experimental results
demonstrate that our AMAT method outperforms the SAT method in adversarial
robustness on noisy data and prediction accuracy on clean data. Please contact
the author for the source code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation. (arXiv:2206.01737v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01737">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have achieved remarkable segmentation
accuracy on benchmark datasets where training and test sets are from the same
domain, yet their performance can degrade significantly on unseen domains,
which hinders the deployment of CNNs in many clinical scenarios. Most existing
works improve model out-of-domain (OOD) robustness by collecting multi-domain
datasets for training, which is expensive and may not always be feasible due to
privacy and logistical issues. In this work, we focus on improving model
robustness using a single-domain dataset only. We propose a novel data
augmentation framework called MaxStyle, which maximizes the effectiveness of
style augmentation for model OOD performance. It attaches an auxiliary
style-augmented image decoder to a segmentation network for robust feature
learning and data augmentation. Importantly, MaxStyle augments data with
improved image style diversity and hardness, by expanding the style space with
noise and searching for the worst-case style composition of latent features via
adversarial training. With extensive experiments on multiple public cardiac and
prostate MR datasets, we demonstrate that MaxStyle leads to significantly
improved out-of-distribution robustness against unseen corruptions as well as
common distribution shifts across multiple, different, unseen sites and unknown
image sequences under both low- and high-training data settings. The code can
be found at https://github.com/cherise215/MaxStyle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding. (arXiv:2206.01738v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01738">
<div class="article-summary-box-inner">
<span><p>Lidars are depth measuring sensors widely used in autonomous driving and
augmented reality. However, the large volume of data produced by lidars can
lead to high costs in data storage and transmission. While lidar data can be
represented as two interchangeable representations: 3D point clouds and range
images, most previous work focus on compressing the generic 3D point clouds. In
this work, we show that directly compressing the range images can leverage the
lidar scanning pattern, compared to compressing the unprojected point clouds.
We propose a novel data-driven range image compression algorithm, named RIDDLE
(Range Image Deep DeLta Encoding). At its core is a deep model that predicts
the next pixel value in a raster scanning order, based on contextual laser
shots from both the current and past scans (represented as a 4D point cloud of
spherical coordinates and time). The deltas between predictions and original
values can then be compressed by entropy encoding. Evaluated on the Waymo Open
Dataset and KITTI, our method demonstrates significant improvement in the
compression rate (under the same distortion) compared to widely used point
cloud and range image compression algorithms as well as recent deep methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual- and Self- Prototype Alignment for Semi-supervised Medical Image Segmentation. (arXiv:2206.01739v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01739">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning methods have been explored in medical image
segmentation tasks due to the scarcity of pixel-level annotation in the real
scenario. Proto-type alignment based consistency constraint is an intuitional
and plausible solu-tion to explore the useful information in the unlabeled
data. In this paper, we propose a mutual- and self- prototype alignment (MSPA)
framework to better utilize the unlabeled data. In specific, mutual-prototype
alignment enhances the information interaction between labeled and unlabeled
data. The mutual-prototype alignment imposes two consistency constraints in
reverse directions between the unlabeled and labeled data, which enables the
consistent embedding and model discriminability on unlabeled data. The proposed
self-prototype alignment learns more stable region-wise features within
unlabeled images, which optimizes the classification margin in semi-supervised
segmentation by boosting the intra-class compactness and inter-class separation
on the feature space. Extensive experimental results on three medical datasets
demonstrate that with a small amount of labeled data, MSPA achieves large
improvements by leveraging the unlabeled data. Our method also outperforms
seven state-of-the-art semi-supervised segmentation methods on all three
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoising Fast X-Ray Fluorescence Raster Scans of Paintings. (arXiv:2206.01740v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01740">
<div class="article-summary-box-inner">
<span><p>Macro x-ray fluorescence (XRF) imaging of cultural heritage objects, while a
popular non-invasive technique for providing elemental distribution maps, is a
slow acquisition process in acquiring high signal-to-noise ratio XRF volumes.
Typically on the order of tenths of a second per pixel, a raster scanning probe
counts the number of photons at different energies emitted by the object under
x-ray illumination. In an effort to reduce the scan times without sacrificing
elemental map and XRF volume quality, we propose using dictionary learning with
a Poisson noise model as well as a color image-based prior to restore noisy,
rapidly acquired XRF data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation. (arXiv:2206.01741v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01741">
<div class="article-summary-box-inner">
<span><p>We present a new encoder-decoder Vision Transformer architecture, Patcher,
for medical image segmentation. Unlike standard Vision Transformers, it employs
Patcher blocks that segment an image into large patches, each of which is
further divided into small patches. Transformers are applied to the small
patches within a large patch, which constrains the receptive field of each
pixel. We intentionally make the large patches overlap to enhance intra-patch
communication. The encoder employs a cascade of Patcher blocks with increasing
receptive fields to extract features from local to global levels. This design
allows Patcher to benefit from both the coarse-to-fine feature extraction
common in CNNs and the superior spatial relationship modeling of Transformers.
We also propose a new mixture-of-experts (MoE) based decoder, which treats the
feature maps from the encoder as experts and selects a suitable set of expert
features to predict the label for each pixel. The use of MoE enables better
specializations of the expert features and reduces interference between them
during inference. Extensive experiments demonstrate that Patcher outperforms
state-of-the-art Transformer- and CNN-based approaches significantly on stroke
lesion segmentation and polyp segmentation. Code for Patcher will be released
with publication to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Probabilistic Structural Representation for Biomedical Image Segmentation. (arXiv:2206.01742v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01742">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of various fine-scale structures from biomedical images
is a very important yet challenging problem. Existing methods use topological
information as an additional training loss, but are ultimately learning a
pixel-wise representation. In this paper, we propose the first deep learning
method to learn a structural representation. We use discrete Morse theory and
persistent homology to construct an one-parameter family of structures as the
structural representation space. Furthermore, we learn a probabilistic model
that can do inference tasks on such a structural representation space. We
empirically demonstrate the strength of our method, i.e., generating true
structures rather than pixel-maps with better topological integrity, and
facilitating a human-in-the-loop annotation pipeline using the sampling of
structures and structure-aware uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orthogonal Transform based Generative Adversarial Network for Image Dehazing. (arXiv:2206.01743v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01743">
<div class="article-summary-box-inner">
<span><p>Image dehazing has become one of the crucial preprocessing steps for any
computer vision task. Most of the dehazing methods try to estimate the
transmission map along with the atmospheric light to get the dehazed image in
the image domain. In this paper, we propose a novel end-to-end architecture
that directly estimates dehazed image in Krawtchouk transform domain. For this
a customized Krawtchouk Convolution Layer (KCL) in the architecture is added.
KCL is constructed using Krawtchouk basis functions which converts the image
from the spatial domain to the Krawtchouk transform domain. Another convolution
layer is added at the end of the architecture named as Inverse Krawtchouk
Convolution Layer (IKCL) which converts the image back to the spatial domain
from the transform domain. It has been observed that the haze is mainly present
in lower frequencies of hazy images, wherein the Krawtchouk transform helps to
analyze the high and low frequencies of the images separately. We have divided
our architecture into two branches, the upper branch deals with the higher
frequencies while the lower branch deals with the lower frequencies of the
image. The lower branch is made deeper in terms of the layers as compared to
the upper branch to address the haze present in the lower frequencies. Using
the proposed Orthogonal Transform based Generative Adversarial Network (OTGAN)
architecture for image dehazing, we were able to achieve competitive results
when compared to the present state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques. (arXiv:2206.01745v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01745">
<div class="article-summary-box-inner">
<span><p>Background: Artificial intelligence techniques have demonstrated great
potential in cardiology, especially to detect imperceptible patterns for the
human eye. In this sense, these techniques seem to be adequate to identify
patterns in the myocardial texture which could lead to characterize and
quantify fibrosis. Purpose: The aim of this study was to postulate a new
artificial intelligence method to identify fibrosis in cine cardiac magnetic
resonance (CMR) imaging. Methods: A retrospective observational study was
carried out in a population of 75 subjects from a clinical center of San Carlos
de Bariloche. The proposed method analyzes the myocardial texture in cine CMR
images using a convolutional neural network to determine local myocardial
tissue damage. Results: An accuracy of 89% for quantifying local tissue damage
was observed for the validation data set and 70% for the test set. In addition,
the qualitative analysis showed a high spatial correlation in lesion location.
Conclusions: The postulated method enables to spatially identify fibrosis using
only the information from cine nuclear magnetic resonance studies,
demonstrating the potential of this technique to quantify myocardial viability
in the future or to study the lesions etiology
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach. (arXiv:2206.01746v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01746">
<div class="article-summary-box-inner">
<span><p>Background: Artificial intelligence techniques have shown great potential in
cardiology, especially in quantifying cardiac biventricular function, volume,
mass, and ejection fraction (EF). However, its use in clinical practice is not
straightforward due to its poor reproducibility with cases from daily practice,
among other reasons. Objectives: To validate a new artificial intelligence tool
in order to quantify the cardiac biventricular function (volume, mass, and EF).
To analyze its robustness in the clinical area, and the computational times
compared with conventional methods. Methods: A total of 189 patients were
analyzed: 89 from a regional center and 100 from a public center. The method
proposes two convolutional networks that include anatomical information of the
heart to reduce classification errors. Results: A high concordance (Pearson
coefficient) was observed between manual quantification and the proposed
quantification of cardiac function (0.98, 0.92, 0.96 and 0.8 for volumes and
biventricular EF) in about 5 seconds per study. Conclusions: This method
quantifies biventricular function and volumes in seconds with an accuracy
equivalent to that of a specialist.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radar Guided Dynamic Visual Attention for Resource-Efficient RGB Object Detection. (arXiv:2206.01772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01772">
<div class="article-summary-box-inner">
<span><p>An autonomous system's perception engine must provide an accurate
understanding of the environment for it to make decisions. Deep learning based
object detection networks experience degradation in the performance and
robustness for small and far away objects due to a reduction in object's
feature map as we move to higher layers of the network. In this work, we
propose a novel radar-guided spatial attention for RGB images to improve the
perception quality of autonomous vehicles operating in a dynamic environment.
In particular, our method improves the perception of small and long range
objects, which are often not detected by the object detectors in RGB mode. The
proposed method consists of two RGB object detectors, namely the Primary
detector and a lightweight Secondary detector. The primary detector takes a
full RGB image and generates primary detections. Next, the radar proposal
framework creates regions of interest (ROIs) for object proposals by projecting
the radar point cloud onto the 2D RGB image. These ROIs are cropped and fed to
the secondary detector to generate secondary detections which are then fused
with the primary detections via non-maximum suppression. This method helps in
recovering the small objects by preserving the object's spatial features
through an increase in their receptive field. We evaluate our fusion method on
the challenging nuScenes dataset and show that our fusion method with SSD-lite
as primary and secondary detector improves the baseline primary yolov3
detector's recall by 14% while requiring three times fewer computational
resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monkeypox Image Data collection. (arXiv:2206.01774v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01774">
<div class="article-summary-box-inner">
<span><p>This paper explains the initial Monkeypox Open image data collection
procedure. It was created by assembling images collected from websites,
newspapers, and online portals and currently contains around 1905 images after
data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Super-Resolution for Real-World Images on Mobile Devices. (arXiv:2206.01777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01777">
<div class="article-summary-box-inner">
<span><p>Image Super-Resolution (ISR), which aims at recovering High-Resolution (HR)
images from the corresponding Low-Resolution (LR) counterparts. Although recent
progress in ISR has been remarkable. However, they are way too computationally
intensive to be deployed on edge devices, since most of the recent approaches
are deep learning-based. Besides, these methods always fail in real-world
scenes, since most of them adopt a simple fixed "ideal" bicubic downsampling
kernel from high-quality images to construct LR/HR training pairs which may
lose track of frequency-related details. In this work, an approach for
real-time ISR on mobile devices is presented, which is able to deal with a wide
range of degradations in real-world scenarios. Extensive experiments on
traditional super-resolution datasets (Set5, Set14, BSD100, Urban100, Manga109,
DIV2K) and real-world images with a variety of degradations demonstrate that
our method outperforms the state-of-art methods, resulting in higher PSNR and
SSIM, lower noise and better visual quality. Most importantly, our method
achieves real-time performance on mobile or edge devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2U++: A Multiscale Recurrent Residual U-Net with Dense Skip Connections for Medical Image Segmentation. (arXiv:2206.01793v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01793">
<div class="article-summary-box-inner">
<span><p>U-Net is a widely adopted neural network in the domain of medical image
segmentation. Despite its quick embracement by the medical imaging community,
its performance suffers on complicated datasets. The problem can be ascribed to
its simple feature extracting blocks: encoder/decoder, and the semantic gap
between encoder and decoder. Variants of U-Net (such as R2U-Net) have been
proposed to address the problem of simple feature extracting blocks by making
the network deeper, but it does not deal with the semantic gap problem. On the
other hand, another variant UNET++ deals with the semantic gap problem by
introducing dense skip connections but has simple feature extraction blocks. To
overcome these issues, we propose a new U-Net based medical image segmentation
architecture R2U++. In the proposed architecture, the adapted changes from
vanilla U-Net are: (1) the plain convolutional backbone is replaced by a deeper
recurrent residual convolution block. The increased field of view with these
blocks aids in extracting crucial features for segmentation which is proven by
improvement in the overall performance of the network. (2) The semantic gap
between encoder and decoder is reduced by dense skip pathways. These pathways
accumulate features coming from multiple scales and apply concatenation
accordingly. The modified architecture has embedded multi-depth models, and an
ensemble of outputs taken from varying depths improves the performance on
foreground objects appearing at various scales in the images. The performance
of R2U++ is evaluated on four distinct medical imaging modalities: electron
microscopy (EM), X-rays, fundus, and computed tomography (CT). The average gain
achieved in IoU score is 1.5+-0.37% and in dice score is 0.9+-0.33% over
UNET++, whereas, 4.21+-2.72 in IoU and 3.47+-1.89 in dice score over R2U-Net
across different medical imaging segmentation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Additive MIL: Intrinsic Interpretability for Pathology. (arXiv:2206.01794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01794">
<div class="article-summary-box-inner">
<span><p>Multiple Instance Learning (MIL) has been widely applied in pathology towards
solving critical problems such as automating cancer diagnosis and grading,
predicting patient prognosis, and therapy response. Deploying these models in a
clinical setting requires careful inspection of these black boxes during
development and deployment to identify failures and maintain physician trust.
In this work, we propose a simple formulation of MIL models, which enables
interpretability while maintaining similar predictive performance. Our Additive
MIL models enable spatial credit assignment such that the contribution of each
region in the image can be exactly computed and visualized. We show that our
spatial credit assignment coincides with regions used by pathologists during
diagnosis and improves upon classical attention heatmaps from attention MIL
models. We show that any existing MIL model can be made additive with a simple
change in function composition. We also show how these models can debug model
failures, identify spurious features, and highlight class-wise regions of
interest, enabling their use in high-stakes environments such as clinical
decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata. (arXiv:2206.01813v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01813">
<div class="article-summary-box-inner">
<span><p>Most camera images are rendered and saved in the standard RGB (sRGB) format
by the camera's hardware. Due to the in-camera photo-finishing routines,
nonlinear sRGB images are undesirable for computer vision tasks that assume a
direct relationship between pixel values and scene radiance. For such
applications, linear raw-RGB sensor images are preferred. Saving images in
their raw-RGB format is still uncommon due to the large storage requirement and
lack of support by many imaging applications. Several "raw reconstruction"
methods have been proposed that utilize specialized metadata sampled from the
raw-RGB image at capture time and embedded in the sRGB image. This metadata is
used to parameterize a mapping function to de-render the sRGB image back to its
original raw-RGB format when needed. Existing raw reconstruction methods rely
on simple sampling strategies and global mapping to perform the de-rendering.
This paper shows how to improve the de-rendering results by jointly learning
sampling and reconstruction. Our experiments show that our learned sampling can
adapt to the image content to produce better raw reconstructions than existing
methods. We also describe an online fine-tuning strategy for the reconstruction
network to improve results further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EAANet: Efficient Attention Augmented Convolutional Networks. (arXiv:2206.01821v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01821">
<div class="article-summary-box-inner">
<span><p>Humans can effectively find salient regions in complex scenes. Self-attention
mechanisms were introduced into Computer Vision (CV) to achieve this. Attention
Augmented Convolutional Network (AANet) is a mixture of convolution and
self-attention, which increases the accuracy of a typical ResNet. However, The
complexity of self-attention is O(n2) in terms of computation and memory usage
with respect to the number of input tokens. In this project, we propose EAANet:
Efficient Attention Augmented Convolutional Networks, which incorporates
efficient self-attention mechanisms in a convolution and self-attention hybrid
architecture to reduce the model's memory footprint. Our best model show
performance improvement over AA-Net and ResNet18. We also explore different
methods to augment Convolutional Network with self-attention mechanisms and
show the difficulty of training those methods compared to ResNet. Finally, we
show that augmenting efficient self-attention mechanisms with ResNet scales
better with input size than normal self-attention mechanisms. Therefore, our
EAANet is more capable of working with high-resolution images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Gamma Generalized Normal Distribution: A Descriptor of SAR Imagery. (arXiv:2206.01826v1 [stat.ME])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01826">
<div class="article-summary-box-inner">
<span><p>We propose a new four-parameter distribution for modeling synthetic aperture
radar (SAR) imagery named the gamma generalized normal (GGN) by combining the
gamma and generalized normal distributions. A mathematical characterization of
the new distribution is provided by identifying the limit behavior and by
calculating the density and moment expansions. The GGN model performance is
evaluated on both synthetic and actual data and, for that, maximum likelihood
estimation and random number generation are discussed. The proposed
distribution is compared with the beta generalized normal distribution (BGN),
which has already shown to appropriately represent SAR imagery. The performance
of these two distributions are measured by means of statistics which provide
evidence that the GGN can outperform the BGN distribution in some contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drawing out of Distribution with Neuro-Symbolic Generative Models. (arXiv:2206.01829v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01829">
<div class="article-summary-box-inner">
<span><p>Learning general-purpose representations from perceptual inputs is a hallmark
of human intelligence. For example, people can write out numbers or characters,
or even draw doodles, by characterizing these tasks as different instantiations
of the same generic underlying process -- compositional arrangements of
different forms of pen strokes. Crucially, learning to do one task, say
writing, implies reasonable competence at another, say drawing, on account of
this shared process. We present Drawing out of Distribution (DooD), a
neuro-symbolic generative model of stroke-based drawing that can learn such
general-purpose representations. In contrast to prior work, DooD operates
directly on images, requires no supervision or expensive test-time inference,
and performs unsupervised amortised inference with a symbolic stroke model that
better enables both interpretability and generalization. We evaluate DooD on
its ability to generalise across both data and tasks. We first perform
zero-shot transfer from one dataset (e.g. MNIST) to another (e.g. Quickdraw),
across five different datasets, and show that DooD clearly outperforms
different baselines. An analysis of the learnt representations further
highlights the benefits of adopting a symbolic stroke model. We then adopt a
subset of the Omniglot challenge tasks, and evaluate its ability to generate
new exemplars (both unconditionally and conditionally), and perform one-shot
classification, showing that DooD matches the state of the art. Taken together,
we demonstrate that DooD does indeed capture general-purpose representations
across both data and task, and takes a further step towards building general
and robust concept-learning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Feature Mapping for 6DoF Object Pose Estimation. (arXiv:2206.01831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01831">
<div class="article-summary-box-inner">
<span><p>This work aims to estimate 6Dof (6D) object pose in background clutter.
Considering the strong occlusion and background noise, we propose to utilize
the spatial structure for better tackling this challenging task. Observing that
the 3D mesh can be naturally abstracted by a graph, we build the graph using 3D
points as vertices and mesh connections as edges. We construct the
corresponding mapping from 2D image features to 3D points for filling the graph
and fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network
(GCN) is applied to help the feature exchange among objects' points in 3D
space. To address the problem of rotation symmetry ambiguity for objects, a
spherical convolution is utilized and the spherical features are combined with
the convolutional features that are mapped to the graph. Predefined 3D
keypoints are voted and the 6DoF pose is obtained via the fitting optimization.
Two scenarios of inference, one with the depth information and the other
without it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the
experiments demonstrate the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coffee Roast Intelligence. (arXiv:2206.01841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01841">
<div class="article-summary-box-inner">
<span><p>As the coffee industry has grown, there would be more demand for roasted
coffee beans, as well as increased rivalry for selling coffee and attracting
customers. As the flavor of each variety of coffee is dependent on the degree
of roasting of the coffee beans, it is vital to maintain a consistent quality
related to the degree of roasting. Each barista has their own method for
determining the degree of roasting. However, extrinsic circumstances such as
light, fatigue, and other factors may alter their judgment. As a result, the
quality of the coffee cannot be controlled. The Coffee Roast Intelligence
application is a machine learning-based study of roasted coffee bean degrees
classification produced as an Android application platform that identifies the
color of coffee beans by photographing or uploading them while roasting. This
application displays the text showing at what level the coffee beans have been
roasted, as well as informs the percent chance of class prediction to the
consumers. Users may also keep track of the result of the predictions related
to the roasting level of coffee beans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning. (arXiv:2206.01843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01843">
<div class="article-summary-box-inner">
<span><p>People say, "A picture is worth a thousand words". Then how can we get the
rich information out of the image? We argue that by using visual clues to
bridge large pretrained vision foundation models and language models, we can do
so without any extra cross-modal training. Thanks to the strong zero-shot
capability of foundation models, we start by constructing a rich semantic
representation of the image (e.g., image tags, object attributes / locations,
captions) as a structured textual prompt, called visual clues, using a vision
foundation model. Based on visual clues, we use large language model to produce
a series of comprehensive descriptions for the visual content, which is then
verified by the vision model again to select the candidate that aligns best
with the image. We evaluate the quality of generated descriptions by
quantitative and qualitative measurement. The results demonstrate the
effectiveness of such a structured semantic representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image. (arXiv:2206.01856v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01856">
<div class="article-summary-box-inner">
<span><p>Image enhancement approaches often assume that the noise is signal
independent, and approximate the degradation model as zero-mean additive
Gaussian noise. However, this assumption does not hold for biomedical imaging
systems where sensor-based sources of noise are proportional to signal
strengths, and the noise is better represented as a Poisson process. In this
work, we explore a sparsity and dictionary learning-based approach and present
a novel self-supervised learning method for single-image denoising where the
noise is approximated as a Poisson process, requiring no clean ground-truth
data. Specifically, we approximate traditional iterative optimization
algorithms for image denoising with a recurrent neural network which enforces
sparsity with respect to the weights of the network. Since the sparse
representations are based on the underlying image, it is able to suppress the
spurious components (noise) in the image patches, thereby introducing implicit
regularization for denoising task through the network structure. Experiments on
two bio-imaging datasets demonstrate that our method outperforms the
state-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results
demonstrate that, in addition to higher performance on standard quantitative
metrics, we are able to recover much more subtle details than other compared
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Data collection and implementation of deep learning-based model in detecting Monkeypox disease using modified VGG16. (arXiv:2206.01862v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01862">
<div class="article-summary-box-inner">
<span><p>While the world is still attempting to recover from the damage caused by the
broad spread of COVID-19, the Monkeypox virus poses a new threat of becoming a
global pandemic. Although the Monkeypox virus itself is not deadly and
contagious as COVID-19, still every day, new patients case has been reported
from many nations. Therefore, it will be no surprise if the world ever faces
another global pandemic due to the lack of proper precautious steps. Recently,
Machine learning (ML) has demonstrated huge potential in image-based diagnoses
such as cancer detection, tumor cell identification, and COVID-19 patient
detection. Therefore, a similar application can be adopted to diagnose the
Monkeypox-related disease as it infected the human skin, which image can be
acquired and further used in diagnosing the disease. Considering this
opportunity, in this work, we introduce a newly developed "Monkeypox2022"
dataset that is publicly available to use and can be obtained from our shared
GitHub repository. The dataset is created by collecting images from multiple
open-source and online portals that do not impose any restrictions on use, even
for commercial purposes, hence giving a safer path to use and disseminate such
data when constructing and deploying any type of ML model. Further, we propose
and evaluate a modified VGG16 model, which includes two distinct studies: Study
One and Two. Our exploratory computational results indicate that our suggested
model can identify Monkeypox patients with an accuracy of $97\pm1.8\%$
(AUC=97.2) and $88\pm0.8\%$ (AUC=0.867) for Study One and Two, respectively.
Additionally, we explain our model's prediction and feature extraction
utilizing Local Interpretable Model-Agnostic Explanations (LIME) help to a
deeper insight into specific features that characterize the onset of the
Monkeypox virus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Image Registration using Mutual Attention based Network. (arXiv:2206.01863v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01863">
<div class="article-summary-box-inner">
<span><p>Image registration is an important task in medical imaging which estimates
the spatial transformation between different images. Many previous studies have
used learning-based methods for multi-stage registration to perform 3D image
registration to improve performance. The performance of the multi-stage
approach, however, is limited by the size of the receptive field where complex
motion does not occur at a single spatial scale. We propose a new registration
network combining recursive network architecture and mutual attention mechanism
to overcome these limitations. Compared with the previous deep learning
methods, our network based on the recursive structure achieves the highest
accuracy in lung Computed Tomography (CT) data set (Dice score of 92\% and
average surface distance of 3.8mm for lungs) and one of the most accurate
results in abdominal CT data set with 9 organs of various sizes (Dice score of
55\% and average surface distance of 7.8mm). We also showed that adding 3
recursive networks is sufficient to achieve the state-of-the-art results
without a significant increase in the inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPGNet: Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional Space. (arXiv:2206.01867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01867">
<div class="article-summary-box-inner">
<span><p>We propose a method SPGNet for 3D human pose estimation that mixes
multi-dimensional re-projection into supervised learning. In this method, the
2D-to-3D-lifting network predicts the global position and coordinates of the 3D
human pose. Then, we re-project the estimated 3D pose back to the 2D key points
along with spatial adjustments. The loss functions compare the estimated 3D
pose with the 3D pose ground truth, and re-projected 2D pose with the input 2D
pose. In addition, we propose a kinematic constraint to restrict the predicted
target with constant human bone length. Based on the estimation results for the
dataset Human3.6M, our approach outperforms many state-of-the-art methods both
qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Recognition Accuracy Across Demographics: Shining a Light Into the Problem. (arXiv:2206.01881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01881">
<div class="article-summary-box-inner">
<span><p>This is the first work that we are aware of to explore how the level of
brightness of the skin region in a pair of face images impacts face recognition
accuracy. Image pairs with both images having mean face skin brightness in an
upper-middle range of brightness are found to have the highest matching
accuracy across demographics and matchers. Image pairs with both images having
mean face skin brightness that is too dark or too light are found to have an
increased false match rate (FMR). Image pairs with strongly different face skin
brightness are found to have decreased FMR and increased false non-match rate
(FNMR). Using a brightness information metric that captures the variation in
brightness in the face skin region, the variation in matching accuracy is shown
to correlate with the level of information available in the face skin region.
For operational scenarios where image acquisition is controlled, we propose
acquiring images with lighting adjusted to yield face skin brightness in a
narrow range.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Superimposed Divide-and-Conquer Image Recognition Method for SEM Images of Nanoparticles on The Surface of Monocrystalline silicon with High Aggregation Degree. (arXiv:2206.01884v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01884">
<div class="article-summary-box-inner">
<span><p>The nanoparticle size and distribution information in the SEM images of
silicon crystals are generally counted by manual methods. The realization of
automatic machine recognition is significant in materials science. This paper
proposed a superposition partitioning image recognition method to realize
automatic recognition and information statistics of silicon crystal
nanoparticle SEM images. Especially for the complex and highly aggregated
characteristics of silicon crystal particle size, an accurate recognition step
and contour statistics method based on morphological processing are given. This
method has technical reference value for the recognition of Monocrystalline
silicon surface nanoparticle images under different SEM shooting conditions.
Besides, it outperforms other methods in terms of recognition accuracy and
algorithm efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling of Textures to Predict Immune Cell Status and Survival of Brain Tumour Patients. (arXiv:2206.01897v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01897">
<div class="article-summary-box-inner">
<span><p>Radiomics has shown a capability for different types of cancers such as
glioma to predict the clinical outcome. It can have a non-invasive means of
evaluating the immunotherapy response prior to treatment. However, the use of
deep convolutional neural networks (CNNs)-based radiomics requires large
training image sets. To avoid this problem, we investigate a new imaging
features that model distribution with a Gaussian mixture model (GMM) of learned
3D CNN features. Using these deep radiomic features (DRFs), we aim to predict
the immune marker status (low versus high) and overall survival for glioma
patients. We extract the DRFs by aggregating the activation maps of a
pre-trained 3D-CNN within labeled tumor regions of MRI scans that corresponded
immune markers of 151 patients. Our experiments are performed to assess the
relationship between the proposed DRFs, three immune cell markers (Macrophage
M1, Neutrophils and T Cells Follicular Helper), and measure their association
with overall survival. Using the random forest (RF) model, DRFs was able to
predict the immune marker status with area under the ROC curve (AUC) of 78.67,
83.93 and 75.67\% for Macrophage M1, Neutrophils and T Cells Follicular Helper,
respectively. Combined the immune markers with DRFs and clinical variables,
Kaplan-Meier estimator and Log-rank test achieved the most significant
difference between predicted groups of patients (short-term versus long-term
survival) with p\,=\,4.31$\times$10$^{-7}$ compared to p\,=\,0.03 for Immune
cell markers, p\,=\,0.07 for clinical variables , and
p\,=\,1.45$\times$10$^{-5}$ for DRFs. Our findings indicate that the proposed
features (DRFs) used in RF models may significantly consider prognosticating
patients with brain tumour prior to surgery through regularly acquired imaging
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency Attack: Towards Imperceptible Black-box Adversarial Attack. (arXiv:2206.01898v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01898">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to adversarial examples, even in the
black-box setting where the attacker is only accessible to the model output.
Recent studies have devised effective black-box attacks with high query
efficiency. However, such performance is often accompanied by compromises in
attack imperceptibility, hindering the practical use of these approaches. In
this paper, we propose to restrict the perturbations to a small salient region
to generate adversarial examples that can hardly be perceived. This approach is
readily compatible with many existing black-box attacks and can significantly
improve their imperceptibility with little degradation in attack success rate.
Further, we propose the Saliency Attack, a new black-box attack aiming to
refine the perturbations in the salient region to achieve even better
imperceptibility. Extensive experiments show that compared to the
state-of-the-art black-box attacks, our approach achieves much better
imperceptibility scores, including most apparent distortion (MAD), $L_0$ and
$L_2$ distances, and also obtains significantly higher success rates judged by
a human-like threshold on MAD. Importantly, the perturbations generated by our
approach are interpretable to some extent. Finally, it is also demonstrated to
be robust to different detection-based defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Radiomic Analysis for Predicting Coronavirus Disease 2019 in Computerized Tomography and X-ray Images. (arXiv:2206.01903v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01903">
<div class="article-summary-box-inner">
<span><p>This paper proposes to encode the distribution of features learned from a
convolutional neural network using a Gaussian Mixture Model. These parametric
features, called GMM-CNN, are derived from chest computed tomography and X-ray
scans of patients with Coronavirus Disease 2019. We use the proposed GMM-CNN
features as input to a robust classifier based on random forests to
differentiate between COVID-19 and other pneumonia cases. Our experiments
assess the advantage of GMM-CNN features compared to standard CNN
classification on test images. Using a random forest classifier (80\% samples
for training; 20\% samples for testing), GMM-CNN features encoded with two
mixture components provided a significantly better performance than standard
CNN classification (p\,$&lt;$\,0.05). Specifically, our method achieved an
accuracy in the range of 96.00\,--\,96.70\% and an area under the ROC curve in
the range of 99.29\,--\,99.45\%, with the best performance obtained by
combining GMM-CNN features from both computed tomography and X-ray images. Our
results suggest that the proposed GMM-CNN features could improve the prediction
of COVID-19 in chest computed tomography and X-ray scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based Human-Object Interaction Detection from Tubelet Tokens. (arXiv:2206.01908v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01908">
<div class="article-summary-box-inner">
<span><p>We present a novel vision Transformer, named TUTOR, which is able to learn
tubelet tokens, served as highly-abstracted spatiotemporal representations, for
video-based human-object interaction (V-HOI) detection. The tubelet tokens
structurize videos by agglomerating and linking semantically-related patch
tokens along spatial and temporal domains, which enjoy two benefits: 1)
Compactness: each tubelet token is learned by a selective attention mechanism
to reduce redundant spatial dependencies from others; 2) Expressiveness: each
tubelet token is enabled to align with a semantic instance, i.e., an object or
a human, across frames, thanks to agglomeration and linking. The effectiveness
and efficiency of TUTOR are verified by extensive experiments. Results shows
our method outperforms existing works by large margins, with a relative mAP
gain of $16.14\%$ on VidHOI and a 2 points gain on CAD-120 as well as a $4
\times$ speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition. (arXiv:2206.01910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01910">
<div class="article-summary-box-inner">
<span><p>Action recognition is an exciting research avenue for artificial intelligence
since it may be a game changer in the emerging industrial fields such as
robotic visions and automobiles. However, current deep learning faces major
challenges for such applications because of the huge computational cost and the
inefficient learning. Hence, we develop a novel brain-inspired Spiking Neural
Network (SNN) based system titled Spiking Gating Flow (SGF) for online action
learning. The developed system consists of multiple SGF units which assembled
in a hierarchical manner. A single SGF unit involves three layers: a feature
extraction layer, an event-driven layer and a histogram-based training layer.
To demonstrate the developed system capabilities, we employ a standard Dynamic
Vision Sensor (DVS) gesture classification as a benchmark. The results indicate
that we can achieve 87.5% accuracy which is comparable with Deep Learning (DL),
but at smaller training/inference data number ratio 1.5:1. And only a single
training epoch is required during the learning process. Meanwhile, to the best
of our knowledge, this is the highest accuracy among the non-backpropagation
algorithm based SNNs. At last, we conclude the few-shot learning paradigm of
the developed network: 1) a hierarchical structure-based network design
involves human prior knowledge; 2) SNNs for content based global dynamic
feature detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nerfels: Renderable Neural Codes for Improved Camera Pose Estimation. (arXiv:2206.01916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01916">
<div class="article-summary-box-inner">
<span><p>This paper presents a framework that combines traditional keypoint-based
camera pose optimization with an invertible neural rendering mechanism. Our
proposed 3D scene representation, Nerfels, is locally dense yet globally
sparse. As opposed to existing invertible neural rendering systems which
overfit a model to the entire scene, we adopt a feature-driven approach for
representing scene-agnostic, local 3D patches with renderable codes. By
modelling a scene only where local features are detected, our framework
effectively generalizes to unseen local regions in the scene via an optimizable
code conditioning mechanism in the neural renderer, all while maintaining the
low memory footprint of a sparse 3D map representation. Our model can be
incorporated to existing state-of-the-art hand-crafted and learned local
feature pose estimators, yielding improved performance when evaluating on
ScanNet for wide camera baseline scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Pixels to Objects: Cubic Visual Attention for Visual Question Answering. (arXiv:2206.01923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01923">
<div class="article-summary-box-inner">
<span><p>Recently, attention-based Visual Question Answering (VQA) has achieved great
success by utilizing question to selectively target different visual areas that
are related to the answer. Existing visual attention models are generally
planar, i.e., different channels of the last conv-layer feature map of an image
share the same weight. This conflicts with the attention mechanism because CNN
features are naturally spatial and channel-wise. Also, visual attention models
are usually conducted on pixel-level, which may cause region discontinuous
problems. In this paper, we propose a Cubic Visual Attention (CVA) model by
successfully applying a novel channel and spatial attention on object regions
to improve VQA task. Specifically, instead of attending to pixels, we first
take advantage of the object proposal networks to generate a set of object
candidates and extract their associated conv features. Then, we utilize the
question to guide channel attention and spatial attention calculation based on
the con-layer feature map. Finally, the attended visual features and the
question are combined to infer the answer. We assess the performance of our
proposed CVA on three public image QA datasets, including COCO-QA, VQA and
Visual7W. Experimental results show that our proposed method significantly
outperforms the state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occlusion-Resistant Instance Segmentation of Piglets in Farrowing Pens Using Center Clustering Network. (arXiv:2206.01942v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01942">
<div class="article-summary-box-inner">
<span><p>Computer vision enables the development of new approaches to monitor the
behavior, health, and welfare of animals. Instance segmentation is a
high-precision method in computer vision for detecting individual animals of
interest. This method can be used for in-depth analysis of animals, such as
examining their subtle interactive behaviors, from videos and images. However,
existing deep-learning-based instance segmentation methods have been mostly
developed based on public datasets, which largely omit heavy occlusion
problems; therefore, these methods have limitations in real-world applications
involving object occlusions, such as farrowing pen systems used on pig farms in
which the farrowing crates often impede the sow and piglets. In this paper, we
propose a novel occlusion-resistant Center Clustering Network for instance
segmentation, dubbed as CClusnet-Inseg. Specifically, CClusnet-Inseg uses each
pixel to predict object centers and trace these centers to form masks based on
clustering results, which consists of a network for segmentation and center
offset vector map, Density-Based Spatial Clustering of Applications with Noise
(DBSCAN) algorithm, Centers-to-Mask (C2M) and Remain-Centers-to-Mask (RC2M)
algorithms, and a pseudo-occlusion generator (POG). In all, 4,600 images were
extracted from six videos collected from six farrowing pens to train and
validate our method. CClusnet-Inseg achieves a mean average precision (mAP) of
83.6; it outperformed YOLACT++ and Mask R-CNN, which had mAP values of 81.2 and
74.7, respectively. We conduct comprehensive ablation studies to demonstrate
the advantages and effectiveness of core modules of our method. In addition, we
apply CClusnet-Inseg to multi-object tracking for animal monitoring, and the
predicted object center that is a conjunct output could serve as an
occlusion-resistant representation of the location of an object.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C$^3$Fusion: Consistent Contrastive Colon Fusion, Towards Deep SLAM in Colonoscopy. (arXiv:2206.01961v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01961">
<div class="article-summary-box-inner">
<span><p>3D colon reconstruction from Optical Colonoscopy (OC) to detect non-examined
surfaces remains an unsolved problem. The challenges arise from the nature of
optical colonoscopy data, characterized by highly reflective low-texture
surfaces, drastic illumination changes and frequent tracking loss. Recent
methods demonstrate compelling results, but suffer from: (1) frangible
frame-to-frame (or frame-to-model) pose estimation resulting in many tracking
failures; or (2) rely on point-based representations at the cost of scan
quality. In this paper, we propose a novel reconstruction framework that
addresses these issues end to end, which result in both quantitatively and
qualitatively accurate and robust 3D colon reconstruction. Our SLAM approach,
which employs correspondences based on contrastive deep features, and deep
consistent depth maps, estimates globally optimized poses, is able to recover
from frequent tracking failures, and estimates a global consistent 3D model;
all within a single framework. We perform an extensive experimental evaluation
on multiple synthetic and real colonoscopy videos, showing high-quality results
and comparisons against relevant baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Openness of CLIP. (arXiv:2206.01986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01986">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) has demonstrated great
potential in realizing open-vocabulary image classification in a matching
style, because of its holistic use of natural language supervision that covers
unconstrained real-world visual concepts. However, it is, in turn, also
difficult to evaluate and analyze the openness of CLIP-like models, since they
are in theory open to any vocabulary but the actual accuracy varies. To address
the insufficiency of conventional studies on openness, we resort to an
incremental view and define the extensibility, which essentially approximates
the model's ability to deal with new visual concepts, by evaluating openness
through vocabulary expansions. Our evaluation based on extensibility shows that
CLIP-like models are hardly truly open and their performances degrade as the
vocabulary expands to different degrees. Further analysis reveals that the
over-estimation of openness is not because CLIP-like models fail to capture the
general similarity of image and text features of novel visual concepts, but
because of the confusion among competing text features, that is, they are not
stable with respect to the vocabulary. In light of this, we propose to improve
the openness of CLIP from the perspective of feature space by enforcing the
distinguishability of text features. Our method retrieves relevant texts from
the pre-training corpus to enhance prompts for inference, which boosts the
extensibility and stability of CLIP even without fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation. (arXiv:2206.01988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01988">
<div class="article-summary-box-inner">
<span><p>Automatic generation of ophthalmic reports using data-driven neural networks
has great potential in clinical practice. When writing a report,
ophthalmologists make inferences with prior clinical knowledge. This knowledge
has been neglected in prior medical report generation methods. To endow models
with the capability of incorporating expert knowledge, we propose a Cross-modal
clinical Graph Transformer (CGT) for ophthalmic report generation (ORG), in
which clinical relation triples are injected into the visual features as prior
knowledge to drive the decoding procedure. However, two major common Knowledge
Noise (KN) issues may affect models' effectiveness. 1) Existing general
biomedical knowledge bases such as the UMLS may not align meaningfully to the
specific context and language of the report, limiting their utility for
knowledge injection. 2) Incorporating too much knowledge may divert the visual
features from their correct meaning. To overcome these limitations, we design
an automatic information extraction scheme based on natural language processing
to obtain clinical entities and relations directly from in-domain training
reports. Given a set of ophthalmic images, our CGT first restores a sub-graph
from the clinical graph and injects the restored triples into visual features.
Then visible matrix is employed during the encoding procedure to limit the
impact of knowledge. Finally, reports are predicted by the encoded cross-modal
features via a Transformer decoder. Extensive experiments on the large-scale
FFA-IR benchmark demonstrate that the proposed CGT is able to outperform
previous benchmark methods and achieve state-of-the-art performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01992">
<div class="article-summary-box-inner">
<span><p>Detection of object anomalies is crucial in industrial processes, but
unsupervised anomaly detection and localization is particularly important due
to the difficulty of obtaining a large number of defective samples and the
unpredictable types of anomalies in real life. Among the existing unsupervised
anomaly detection and localization methods, the NF-based scheme has achieved
better results. However, the two subnets (complex functions) si(ui) and ti(ui)
in NF are usually multilayer perceptrons, which need to squeeze the input
visual features from 2D flattening to 1D, destroying the spatial location
relationship in the feature map and losing the spatial structure information.
In order to retain and effectively extract spatial structure information, we
design in this study a complex function model with alternating CBAM embedded in
a stacked 3*3 full convolution, which is able to retain and effectively extract
spatial structure information in the normalized flow model. Extensive
experimental results on the MVTec AD dataset show that CAINNFlow achieves
advanced levels of accuracy and inference efficiency based on CNN and
Transformer backbone networks as feature extractors, and CAINNFlow achieves a
pixel-level AUC of 98.76\% for anomaly detection in MVTec AD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSR: Making Self-supervised learning Robust to Aggressive Augmentations. (arXiv:2206.01999v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01999">
<div class="article-summary-box-inner">
<span><p>Most recent self-supervised learning methods learn visual representation by
contrasting different augmented views of images. Compared with supervised
learning, more aggressive augmentations have been introduced to further improve
the diversity of training pairs. However, aggressive augmentations may distort
images' structures leading to a severe semantic shift problem that augmented
views of the same image may not share the same semantics, thus degrading the
transfer performance. To address this problem, we propose a new SSL paradigm,
which counteracts the impact of semantic shift by balancing the role of weak
and aggressively augmented pairs. Specifically, semantically inconsistent pairs
are of minority and we treat them as noisy pairs. Note that deep neural
networks (DNNs) have a crucial memorization effect that DNNs tend to first
memorize clean (majority) examples before overfitting to noisy (minority)
examples. Therefore, we set a relatively large weight for aggressively
augmented data pairs at the early learning stage. With the training going on,
the model begins to overfit noisy pairs. Accordingly, we gradually reduce the
weights of aggressively augmented pairs. In doing so, our method can better
embrace the aggressive augmentations and neutralize the semantic shift problem.
Experiments show that our model achieves 73.1% top-1 accuracy on ImageNet-1K
with ResNet-50 for 200 epochs, which is a 2.5% improvement over BYOL. Moreover,
experiments also demonstrate that the learned representations can transfer well
for various downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVNets: High Performance Library for Computer Vision. (arXiv:2206.02002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02002">
<div class="article-summary-box-inner">
<span><p>We introduce CVNets, a high-performance open-source library for training deep
neural networks for visual recognition tasks, including classification,
detection, and segmentation. CVNets supports image and video understanding
tools, including data loading, data transformations, novel data sampling
methods, and implementations of several standard networks with similar or
better performance than previous studies.
</p>
<p>Our source code is available at: \url{https://github.com/apple/ml-cvnets}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APES: Articulated Part Extraction from Sprite Sheets. (arXiv:2206.02015v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02015">
<div class="article-summary-box-inner">
<span><p>Rigged puppets are one of the most prevalent representations to create 2D
character animations. Creating these puppets requires partitioning characters
into independently moving parts. In this work, we present a method to
automatically identify such articulated parts from a small set of character
poses shown in a sprite sheet, which is an illustration of the character that
artists often draw before puppet creation. Our method is trained to infer
articulated parts, e.g. head, torso and limbs, that can be re-assembled to best
reconstruct the given poses. Our results demonstrate significantly better
performance than alternatives qualitatively and quantitatively.Our project page
https://zhan-xu.github.io/parts/ includes our code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Neural Representation for Mesh-Free Inverse Obstacle Scattering. (arXiv:2206.02027v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02027">
<div class="article-summary-box-inner">
<span><p>Implicit representation of shapes as level sets of multilayer perceptrons has
recently flourished in different shape analysis, compression, and
reconstruction tasks. In this paper, we introduce an implicit neural
representation-based framework for solving the inverse obstacle scattering
problem in a mesh-free fashion. We efficiently express the obstacle shape as
the zero-level set of a signed distance function which is implicitly determined
by a small number of network parameters. To solve the direct scattering
problem, we implement the implicit boundary integral method. It uses
projections of the grid points in the tubular neighborhood onto the boundary to
compute the PDE solution instead of a grid-size-dependent extraction method of
surface points such as Marching Cubes. The implicit representation conveniently
handles the shape perturbation in the optimization process. To update the
shape, we use PyTorch's automatic differentiation to backpropagate the loss
function w.r.t. the network parameters, allowing us to avoid complex and
error-prone manual derivation of the shape derivative. The proposed framework
makes the inverse scattering problem more tractable with fewer parameters to
optimize in comparison to the memory-inefficient grid-based approaches and
outputs high-quality reconstruction results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guided Deep Metric Learning. (arXiv:2206.02029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02029">
<div class="article-summary-box-inner">
<span><p>Deep Metric Learning (DML) methods have been proven relevant for visual
similarity learning. However, they sometimes lack generalization properties
because they are trained often using an inappropriate sample selection strategy
or due to the difficulty of the dataset caused by a distributional shift in the
data. These represent a significant drawback when attempting to learn the
underlying data manifold. Therefore, there is a pressing need to develop better
ways of obtaining generalization and representation of the underlying manifold.
In this paper, we propose a novel approach to DML that we call Guided Deep
Metric Learning, a novel architecture oriented to learning more compact
clusters, improving generalization under distributional shifts in DML. This
novel architecture consists of two independent models: A multi-branch master
model, inspired from a Few-Shot Learning (FSL) perspective, generates a reduced
hypothesis space based on prior knowledge from labeled data, which guides or
regularizes the decision boundary of a student model during training under an
offline knowledge distillation scheme. Experiments have shown that the proposed
method is capable of a better manifold generalization and representation to up
to 40% improvement (Recall@1, CIFAR10), using guidelines suggested by Musgrave
et al. to perform a more fair and realistic comparison, which is currently
absent in the literature
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Speaker-specific Lip-to-Speech Generation. (arXiv:2206.02050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02050">
<div class="article-summary-box-inner">
<span><p>Understanding the lip movement and inferring the speech from it is
notoriously difficult for the common person. The task of accurate lip-reading
gets help from various cues of the speaker and its contextual or environmental
setting. Every speaker has a different accent and speaking style, which can be
inferred from their visual and speech features. This work aims to understand
the correlation/mapping between speech and the sequence of lip movement of
individual speakers in an unconstrained and large vocabulary. We model the
frame sequence as a prior to the transformer in an auto-encoder setting and
learned a joint embedding that exploits temporal properties of both audio and
video. We learn temporal synchronization using deep metric learning, which
guides the decoder to generate speech in sync with input lip movements. The
predictive posterior thus gives us the generated speech in speaker speaking
style. We have trained our model on the Grid and Lip2Wav Chemistry lecture
dataset to evaluate single speaker natural speech generation tasks from lip
movement in an unconstrained natural setting. Extensive evaluation using
various qualitative and quantitative metrics with human evaluation also shows
that our method outperforms the Lip2Wav Chemistry dataset(large vocabulary in
an unconstrained setting) by a good margin across almost all evaluation metrics
and marginally outperforms the state-of-the-art on GRID dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIDNet: A Real-time Semantic Segmentation Network Inspired from PID Controller. (arXiv:2206.02066v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02066">
<div class="article-summary-box-inner">
<span><p>Two-branch network architecture has shown its efficiency and effectiveness
for real-time semantic segmentation tasks. However, direct fusion of low-level
details and high-level semantics will lead to a phenomenon that the detailed
features are easily overwhelmed by surrounding contextual information, namely
overshoot in this paper, which limits the improvement of the accuracy of
existed two-branch models. In this paper, we bridge a connection between
Convolutional Neural Network (CNN) and Proportional-Integral-Derivative (PID)
controller and reveal that the two-branch network is nothing but a
Proportional-Integral (PI) controller, which inherently suffers from the
similar overshoot issue. To alleviate this issue, we propose a novel
three-branch network architecture: PIDNet, which possesses three branches to
parse the detailed, context and boundary information (derivative of semantics),
respectively, and employs boundary attention to guide the fusion of detailed
and context branches in final stage. The family of PIDNets achieve the best
trade-off between inference speed and accuracy and their test accuracy
surpasses all the existed models with similar inference speed on Cityscapes,
CamVid and COCO-Stuff datasets. Especially, PIDNet-S achieves 78.6% mIOU with
inference speed of 93.2 FPS on Cityscapes test set and 81.6% mIOU with speed of
153.7 FPS on CamVid test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All One Needs to Know about Priors for Deep Image Restoration and Enhancement: A Survey. (arXiv:2206.02070v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02070">
<div class="article-summary-box-inner">
<span><p>Image restoration and enhancement is a process of improving the image quality
by removing degradations, such as noise, blur, and resolution degradation. Deep
learning (DL) has recently been applied to image restoration and enhancement.
Due to its ill-posed property, plenty of works have explored priors to
facilitate training deep neural networks (DNNs). However, the importance of
priors has not been systematically studied and analyzed by far in the research
community. Therefore, this paper serves as the first study that provides a
comprehensive overview of recent advancements of priors for deep image
restoration and enhancement. Our work covers five primary contents: (1) A
theoretical analysis of priors for deep image restoration and enhancement; (2)
A hierarchical and structural taxonomy of priors commonly used in the DL-based
methods; (3) An insightful discussion on each prior regarding its principle,
potential, and applications; (4) A summary of crucial problems by highlighting
the potential future directions to spark more research in the community; (5) An
open-source repository that provides a taxonomy of all mentioned works and code
links.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval. (arXiv:2206.02082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02082">
<div class="article-summary-box-inner">
<span><p>Multi-channel video-language retrieval require models to understand
information from different modalities (e.g. video+question, video+speech) and
real-world knowledge to correctly link a video with a textual response or
query. Fortunately, multimodal contrastive models have been shown to be highly
effective at aligning entities in images/videos and text, e.g., CLIP; text
contrastive models have been extensively studied recently for their strong
ability of producing discriminative sentence embeddings, e.g., SimCSE. Their
abilities are exactly needed by multi-channel video-language retrieval.
However, it is not clear how to quickly adapt these two lines of models to
multi-channel video-language retrieval-style tasks. In this paper, we identify
a principled model design space with two axes: how to represent videos and how
to fuse video and text information. Based on categorization of recent methods,
we investigate the options of representing videos using continuous feature
vectors or discrete text tokens; for the fusion method, we explore a multimodal
transformer or a pretrained contrastive text model. We extensively evaluate the
four combinations on five video-language datasets. We surprisingly find that
discrete text tokens coupled with a pretrained contrastive text model yields
the best performance. This combination can even outperform state-of-the-art on
the iVQA dataset without the additional training on millions of video-language
data. Further analysis shows that this is because representing videos as text
tokens captures the key visual information with text tokens that are naturally
aligned with text models and the text models obtained rich knowledge during
contrastive pretraining process. All the empirical analysis we obtain for the
four variants establishes a solid foundation for future research on leveraging
the rich knowledge of pretrained contrastive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Creation of a Nutrition and Food Group Based Image Database. (arXiv:2206.02086v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02086">
<div class="article-summary-box-inner">
<span><p>Food classification is critical to the analysis of nutrients comprising foods
reported in dietary assessment. Advances in mobile and wearable sensors,
combined with new image based methods, particularly deep learning based
approaches, have shown great promise to improve the accuracy of food
classification to assess dietary intake. However, these approaches are
data-hungry and their performances are heavily reliant on the quantity and
quality of the available datasets for training the food classification model.
Existing food image datasets are not suitable for fine-grained food
classification and the following nutrition analysis as they lack fine-grained
and transparently derived food group based identification which are often
provided by trained dietitians with expert domain knowledge. In this paper, we
propose a framework to create a nutrition and food group based image database
that contains both visual and hierarchical food categorization information to
enhance links to the nutrient profile of each food. We design a protocol for
linking food group based food codes in the U.S. Department of Agriculture's
(USDA) Food and Nutrient Database for Dietary Studies (FNDDS) to a food image
dataset, and implement a web-based annotation tool for efficient deployment of
this protocol.Our proposed method is used to build a nutrition and food group
based image database including 16,114 food images representing the 74 most
frequently consumed What We Eat in America (WWEIA) food sub-categories in the
United States with 1,865 USDA food code matched to a nutrient database, the
USDA FNDDS nutrient database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Scoliosis Vertebral Landmark Localization on X-ray Images via Shape-constrained Multi-stage Cascaded CNNs. (arXiv:2206.02087v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02087">
<div class="article-summary-box-inner">
<span><p>Vertebral landmark localization is a crucial step for variant spine-related
clinical applications, which requires detecting the corner points of 17
vertebrae. However, the neighbor landmarks often disturb each other for the
homogeneous appearance of vertebrae, which makes vertebral landmark
localization extremely difficult. In this paper, we propose multi-stage
cascaded convolutional neural networks (CNNs) to split the single task into two
sequential steps, i.e., center point localization to roughly locate 17 center
points of vertebrae, and corner point localization to find 4 corner points for
each vertebra without distracted by others. Landmarks in each step are located
gradually from a set of initialized points by regressing offsets via cascaded
CNNs. Principal Component Analysis (PCA) is employed to preserve a shape
constraint in offset regression to resist the mutual attraction of vertebrae.
We evaluate our method on the AASCE dataset that consists of 609 tight spinal
anterior-posterior X-ray images and each image contains 17 vertebrae composed
of the thoracic and lumbar spine for spinal shape characterization.
Experimental results demonstrate our superior performance of vertebral landmark
localization over other state-of-the-arts with the relative error decreasing
from 3.2e-3 to 7.2e-4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation. (arXiv:2206.02099v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02099">
<div class="article-summary-box-inner">
<span><p>This article addresses the problem of distilling knowledge from a large
teacher model to a slim student network for LiDAR semantic segmentation.
Directly employing previous distillation approaches yields inferior results due
to the intrinsic challenges of point cloud, i.e., sparsity, randomness and
varying density. To tackle the aforementioned problems, we propose the
Point-to-Voxel Knowledge Distillation (PVD), which transfers the hidden
knowledge from both point level and voxel level. Specifically, we first
leverage both the pointwise and voxelwise output distillation to complement the
sparse supervision signals. Then, to better exploit the structural information,
we divide the whole point cloud into several supervoxels and design a
difficulty-aware sampling strategy to more frequently sample supervoxels
containing less-frequent classes and faraway objects. On these supervoxels, we
propose inter-point and inter-voxel affinity distillation, where the similarity
information between points and voxels can help the student model better capture
the structural information of the surrounding environment. We conduct extensive
experiments on two popular LiDAR segmentation benchmarks, i.e., nuScenes and
SemanticKITTI. On both benchmarks, our PVD consistently outperforms previous
distillation approaches by a large margin on three representative backbones,
i.e., Cylinder3D, SPVNAS and MinkowskiNet. Notably, on the challenging nuScenes
and SemanticKITTI datasets, our method can achieve roughly 75% MACs reduction
and 2x speedup on the competitive Cylinder3D model and rank 1st on the
SemanticKITTI leaderboard among all published algorithms. Our code is available
at https://github.com/cardwing/Codes-for-PVKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUTM Flow: Atomic Unrestricted Time Machine for Monotonic Normalizing Flows. (arXiv:2206.02102v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02102">
<div class="article-summary-box-inner">
<span><p>Nonlinear monotone transformations are used extensively in normalizing flows
to construct invertible triangular mappings from simple distributions to
complex ones. In existing literature, monotonicity is usually enforced by
restricting function classes or model parameters and the inverse transformation
is often approximated by root-finding algorithms as a closed-form inverse is
unavailable. In this paper, we introduce a new integral-based approach termed
"Atomic Unrestricted Time Machine (AUTM)", equipped with unrestricted
integrands and easy-to-compute explicit inverse. AUTM offers a versatile and
efficient way to the design of normalizing flows with explicit inverse and
unrestricted function classes or parameters. Theoretically, we present a
constructive proof that AUTM is universal: all monotonic normalizing flows can
be viewed as limits of AUTM flows. We provide a concrete example to show how to
approximate any given monotonic normalizing flow using AUTM flows with
guaranteed convergence. The result implies that AUTM can be used to transform
an existing flow into a new one equipped with explicit inverse and unrestricted
parameters. The performance of the new approach is evaluated on high
dimensional density estimation, variational inference and image generation.
Experiments demonstrate superior speed and memory efficiency of AUTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContraCLIP: Interpretable GAN generation driven by pairs of contrasting sentences. (arXiv:2206.02104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02104">
<div class="article-summary-box-inner">
<span><p>This work addresses the problem of discovering non-linear interpretable paths
in the latent space of pre-trained GANs in a model-agnostic manner. In the
proposed method, the discovery is driven by a set of pairs of natural language
sentences with contrasting semantics, named semantic dipoles, that serve as the
limits of the interpretation that we require by the trainable latent paths to
encode. By using the pre-trained CLIP encoder, the sentences are projected into
the vision-language space, where they serve as dipoles, and where RBF-based
warping functions define a set of non-linear directional paths, one for each
semantic dipole, allowing in this way traversals from one semantic pole to the
other. By defining an objective that discovers paths in the latent space of
GANs that generate changes along the desired paths in the vision-language
embedding space, we provide an intuitive way of controlling the underlying
generative factors and address some of the limitations of the state-of-the-art
works, namely, that a) they are typically tailored to specific GAN
architectures (i.e., StyleGAN), b) they disregard the relative position of the
manipulated and the original image in the image embedding and the relative
position of the image and the text embeddings, and c) they lead to abrupt image
manipulations and quickly arrive at regions of low density and, thus, low image
quality, providing limited control of the generative factors. We provide
extensive qualitative and quantitative results that demonstrate our claims with
two pre-trained GANs, and make the code and the pre-trained models publicly
available at: https://github.com/chi0tzp/ContraCLIP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision-based Characterization of Large-scale Jet Flames using a Synthetic Infrared Image Generation Approach. (arXiv:2206.02110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02110">
<div class="article-summary-box-inner">
<span><p>Among the different kinds of fire accidents that can occur during industrial
activities that involve hazardous materials, jet fires are one of the
lesser-known types. This is because they are often involved in a process that
generates a sequence of other accidents of greater magnitude, known as domino
effect. Flame impingement usually causes domino effects, and jet fires present
specific features that can significantly increase the probability of this
happening. These features become relevant from a risk analysis perspective,
making their proper characterization a crucial task. Deep Learning approaches
have become extensively used for tasks such as jet fire characterization;
however, these methods are heavily dependent on the amount of data and the
quality of the labels. Data acquisition of jet fires involve expensive
experiments, especially so if infrared imagery is used. Therefore, this paper
proposes the use of Generative Adversarial Networks to produce plausible
infrared images from visible ones, making experiments less expensive and
allowing for other potential applications. The results suggest that it is
possible to realistically replicate the results for experiments carried out
using both visible and infrared cameras. The obtained results are compared with
some previous experiments, and it is shown that similar results were obtained.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cannot See the Forest for the Trees: Aggregating Multiple Viewpoints to Better Classify Objects in Videos. (arXiv:2206.02116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02116">
<div class="article-summary-box-inner">
<span><p>Recently, both long-tailed recognition and object tracking have made great
advances individually. TAO benchmark presented a mixture of the two,
long-tailed object tracking, in order to further reflect the aspect of the
real-world. To date, existing solutions have adopted detectors showing
robustness in long-tailed distributions, which derive per-frame results. Then,
they used tracking algorithms that combine the temporally independent
detections to finalize tracklets. However, as the approaches did not take
temporal changes in scenes into account, inconsistent classification results in
videos led to low overall performance. In this paper, we present a set
classifier that improves accuracy of classifying tracklets by aggregating
information from multiple viewpoints contained in a tracklet. To cope with
sparse annotations in videos, we further propose augmentation of tracklets that
can maximize data efficiency. The set classifier is plug-and-playable to
existing object trackers, and highly improves the performance of long-tailed
object tracking. By simply attaching our method to QDTrack on top of
ResNet-101, we achieve the new state-of-the-art, 19.9% and 15.7% TrackAP_50 on
TAO validation and test sets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation. (arXiv:2206.02118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02118">
<div class="article-summary-box-inner">
<span><p>Cardiac segmentation is an essential step for the diagnosis of cardiovascular
diseases. However, pixel-wise dense labeling is both costly and time-consuming.
Scribble, as a form of sparse annotation, is more accessible than full
annotations. However, it's particularly challenging to train a segmentation
network with weak supervision from scribbles. To tackle this problem, we
propose a new scribble-guided method for cardiac segmentation, based on the
Positive-Unlabeled (PU) learning framework and global consistency
regularization, and termed as ShapePU. To leverage unlabeled pixels via PU
learning, we first present an Expectation-Maximization (EM) algorithm to
estimate the proportion of each class in the unlabeled pixels. Given the
estimated ratios, we then introduce the marginal probability maximization to
identify the classes of unlabeled pixels. To exploit shape knowledge, we apply
cutout operations to training images, and penalize the inconsistent
segmentation results. Evaluated on two open datasets, i.e, ACDC and MSCMRseg,
our scribble-supervised ShapePU surpassed the fully supervised approach
respectively by 1.4% and 9.8% in average Dice, and outperformed the
state-of-the-art weakly supervised and PU learning methods by large margins.
Our code is available at https://github.com/BWGZK/ShapePU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPANet: Multi-Patch Attention For Infrared Small Target object Detection. (arXiv:2206.02120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02120">
<div class="article-summary-box-inner">
<span><p>Infrared small target detection (ISTD) has attracted widespread attention and
been applied in various fields. Due to the small size of infrared targets and
the noise interference from complex backgrounds, the performance of ISTD using
convolutional neural networks (CNNs) is restricted. Moreover, the constriant
that long-distance dependent features can not be encoded by the vanilla CNNs
also impairs the robustness of capturing targets' shapes and locations in
complex scenarios. To this end, a multi-patch attention network (MPANet) based
on the axial-attention encoder and the multi-scale patch branch (MSPB)
structure is proposed. Specially, an axial-attention-improved encoder
architecture is designed to highlight the effective features of small targets
and suppress background noises. Furthermore, the developed MSPB structure fuses
the coarse-grained and fine-grained features from different semantic scales.
Extensive experiments on the SIRST dataset show the superiority performance and
effectiveness of the proposed MPANet compared to the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Adversarial Training with Transformers. (arXiv:2206.02131v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02131">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) has emerged to enable global model training over
distributed clients' data while preserving its privacy. However, the global
trained model is vulnerable to the evasion attacks especially, the adversarial
examples (AEs), carefully crafted samples to yield false classification.
Adversarial training (AT) is found to be the most promising approach against
evasion attacks and it is widely studied for convolutional neural network
(CNN). Recently, vision transformers have been found to be effective in many
computer vision tasks. To the best of the authors' knowledge, there is no work
that studied the feasibility of AT in a FL process for vision transformers.
This paper investigates such feasibility with different federated model
aggregation methods and different vision transformer models with different
tokenization and classification head techniques. In order to improve the robust
accuracy of the models with the not independent and identically distributed
(Non-IID), we propose an extension to FedAvg aggregation method, called
FedWAvg. By measuring the similarities between the last layer of the global
model and the last layer of the client updates, FedWAvg calculates the weights
to aggregate the local models updates. The experiments show that FedWAvg
improves the robust accuracy when compared with other state-of-the-art
aggregation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LDRNet: Enabling Real-time Document Localization on Mobile Devices. (arXiv:2206.02136v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02136">
<div class="article-summary-box-inner">
<span><p>While Identity Document Verification (IDV) technology on mobile devices
becomes ubiquitous in modern business operations, the risk of identity theft
and fraud is increasing. The identity document holder is normally required to
participate in an online video interview to circumvent impostors. However, the
current IDV process depends on an additional human workforce to support online
step-by-step guidance which is inefficient and expensive. The performance of
existing AI-based approaches cannot meet the real-time and lightweight demands
of mobile devices. In this paper, we address those challenges by designing an
edge intelligence-assisted approach for real-time IDV. Aiming at improving the
responsiveness of the IDV process, we propose a new document localization model
for mobile devices, LDRNet, to Localize the identity Document in Real-time. On
the basis of a lightweight backbone network, we build three prediction branches
for LDRNet, the corner points prediction, the line borders prediction and the
document classification. We design novel supplementary targets, the
equal-division points, and use a new loss function named Line Loss, to improve
the speed and accuracy of our approach. In addition to the IDV process, LDRNet
is an efficient and reliable document localization alternative for all kinds of
mobile applications. As a matter of proof, we compare the performance of LDRNet
with other popular approaches on localizing general document datasets. The
experimental results show that LDRNet runs at a speed up to 790 FPS which is
47x faster, while still achieving comparable Jaccard Index(JI) in single-model
and single-scale tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Video Restoration Transformer with Guided Deformable Attention. (arXiv:2206.02146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02146">
<div class="article-summary-box-inner">
<span><p>Video restoration aims at restoring multiple high-quality frames from
multiple low-quality frames. Existing video restoration methods generally fall
into two extreme cases, i.e., they either restore all frames in parallel or
restore the video frame by frame in a recurrent way, which would result in
different merits and drawbacks. Typically, the former has the advantage of
temporal information fusion. However, it suffers from large model size and
intensive memory consumption; the latter has a relatively small model size as
it shares parameters across frames; however, it lacks long-range dependency
modeling ability and parallelizability. In this paper, we attempt to integrate
the advantages of the two cases by proposing a recurrent video restoration
transformer, namely RVRT. RVRT processes local neighboring frames in parallel
within a globally recurrent framework which can achieve a good trade-off
between model size, effectiveness, and efficiency. Specifically, RVRT divides
the video into multiple clips and uses the previously inferred clip feature to
estimate the subsequent clip feature. Within each clip, different frame
features are jointly updated with implicit feature aggregation. Across
different clips, the guided deformable attention is designed for clip-to-clip
alignment, which predicts multiple relevant locations from the whole inferred
clip and aggregates their features by the attention mechanism. Extensive
experiments on video super-resolution, deblurring, and denoising show that the
proposed RVRT achieves state-of-the-art performance on benchmark datasets with
balanced model size, testing memory and runtime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPGNN: Using Hierarchical Graph Neural Networks for Outdoor Point Cloud Processing. (arXiv:2206.02153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02153">
<div class="article-summary-box-inner">
<span><p>Inspired by recent improvements in point cloud processing for autonomous
navigation, we focus on using hierarchical graph neural networks for processing
and feature learning over large-scale outdoor LiDAR point clouds. We observe
that existing GNN based methods fail to overcome challenges of scale and
irregularity of points in outdoor datasets. Addressing the need to preserve
structural details while learning over a larger volume efficiently, we propose
Hierarchical Point Graph Neural Network (HPGNN). It learns node features at
various levels of graph coarseness to extract information. This enables to
learn over a large point cloud while retaining fine details that existing
point-level graph networks struggle to achieve. Connections between multiple
levels enable a point to learn features in multiple scales, in a few
iterations. We design HPGNN as a purely GNN-based approach, so that it offers
modular expandability as seen with other point-based and Graph network
baselines. To illustrate the improved processing capability, we compare
previous point based and GNN models for semantic segmentation with our HPGNN,
achieving a significant improvement for GNNs (+36.7 mIoU) on the SemanticKITTI
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vanilla Feature Distillation for Improving the Accuracy-Robustness Trade-Off in Adversarial Training. (arXiv:2206.02158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02158">
<div class="article-summary-box-inner">
<span><p>Adversarial training has been widely explored for mitigating attacks against
deep models. However, most existing works are still trapped in the dilemma
between higher accuracy and stronger robustness since they tend to fit a model
towards robust features (not easily tampered with by adversaries) while
ignoring those non-robust but highly predictive features. To achieve a better
robustness-accuracy trade-off, we propose the Vanilla Feature Distillation
Adversarial Training (VFD-Adv), which conducts knowledge distillation from a
pre-trained model (optimized towards high accuracy) to guide adversarial
training towards higher accuracy, i.e., preserving those non-robust but
predictive features. More specifically, both adversarial examples and their
clean counterparts are forced to be aligned in the feature space by distilling
predictive representations from the pre-trained/clean model, while previous
works barely utilize predictive features from clean models. Therefore, the
adversarial training model is updated towards maximally preserving the accuracy
as gaining robustness. A key advantage of our method is that it can be
universally adapted to and boost existing works. Exhaustive experiments on
various datasets, classification models, and adversarial training algorithms
demonstrate the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotionCNN: A Strong Baseline for Motion Prediction in Autonomous Driving. (arXiv:2206.02163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02163">
<div class="article-summary-box-inner">
<span><p>To plan a safe and efficient route, an autonomous vehicle should anticipate
future motions of other agents around it. Motion prediction is an extremely
challenging task that recently gained significant attention within the research
community. In this work, we present a simple and yet very strong baseline for
multimodal motion prediction based purely on Convolutional Neural Networks.
While being easy-to-implement, the proposed approach achieves competitive
performance compared to the state-of-the-art methods and ranks 3rd on the 2021
Waymo Open Dataset Motion Prediction Challenge. Our source code is publicly
available at GitHub
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning for Mars Imagery Classification and Segmentation. (arXiv:2206.02180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02180">
<div class="article-summary-box-inner">
<span><p>With the progress of Mars exploration, numerous Mars image data are collected
and need to be analyzed. However, due to the imbalance and distortion of
Martian data, the performance of existing computer vision models is
unsatisfactory. In this paper, we introduce a semi-supervised framework for
machine vision on Mars and try to resolve two specific tasks: classification
and segmentation. Contrastive learning is a powerful representation learning
technique. However, there is too much information overlap between Martian data
samples, leading to a contradiction between contrastive learning and Martian
data. Our key idea is to reconcile this contradiction with the help of
annotations and further take advantage of unlabeled data to improve
performance. For classification, we propose to ignore inner-class pairs on
labeled data as well as neglect negative pairs on unlabeled data, forming
supervised inter-class contrastive learning and unsupervised similarity
learning. For segmentation, we extend supervised inter-class contrastive
learning into an element-wise mode and use online pseudo labels for supervision
on unlabeled areas. Experimental results show that our learning strategies can
improve the classification and segmentation models by a large margin and
outperform state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Functional Ensemble Distillation. (arXiv:2206.02183v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02183">
<div class="article-summary-box-inner">
<span><p>Bayesian models have many desirable properties, most notable is their ability
to generalize from limited data and to properly estimate the uncertainty in
their predictions. However, these benefits come at a steep computational cost
as Bayesian inference, in most cases, is computationally intractable. One
popular approach to alleviate this problem is using a Monte-Carlo estimation
with an ensemble of models sampled from the posterior. However, this approach
still comes at a significant computational cost, as one needs to store and run
multiple models at test time. In this work, we investigate how to best distill
an ensemble's predictions using an efficient model. First, we argue that
current approaches that simply return distribution over predictions cannot
compute important properties, such as the covariance between predictions, which
can be valuable for further processing. Second, in many limited data settings,
all ensemble members achieve nearly zero training loss, namely, they produce
near-identical predictions on the training set which results in sub-optimal
distilled models. To address both problems, we propose a novel and general
distillation approach, named Functional Ensemble Distillation (FED), and we
investigate how to best distill an ensemble in this setting. We find that
learning the distilled model via a simple augmentation scheme in the form of
mixup augmentation significantly boosts the performance. We evaluated our
method on several tasks and showed that it achieves superior results in both
accuracy and uncertainty estimation compared to current approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation. (arXiv:2206.02187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02187">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition in Conversations (ERC) is crucial in developing
sympathetic human-machine interaction. In conversational videos, emotion can be
present in multiple modalities, i.e., audio, video, and transcript. However,
due to the inherent characteristics of these modalities, multi-modal ERC has
always been considered a challenging undertaking. Existing ERC research focuses
mainly on using text information in a discussion, ignoring the other two
modalities. We anticipate that emotion recognition accuracy can be improved by
employing a multi-modal approach. Thus, in this study, we propose a Multi-modal
Fusion Network (M2FNet) that extracts emotion-relevant features from visual,
audio, and text modality. It employs a multi-head attention-based fusion
mechanism to combine emotion-rich latent representations of the input data. We
introduce a new feature extractor to extract latent features from the audio and
visual modality. The proposed feature extractor is trained with a novel
adaptive margin-based triplet loss function to learn emotion-relevant features
from the audio and visual data. In the domain of ERC, the existing methods
perform well on one benchmark dataset but not on others. Our results show that
the proposed M2FNet architecture outperforms all other methods in terms of
weighted average F1 score on well-known MELD and IEMOCAP datasets and sets a
new state-of-the-art performance in ERC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOF: Learning Fourier Occupancy Field for Monocular Real-time Human Reconstruction. (arXiv:2206.02194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02194">
<div class="article-summary-box-inner">
<span><p>The advent of deep learning has led to significant progress in monocular
human reconstruction. However, existing representations, such as parametric
models, voxel grids, meshes and implicit neural representations, have
difficulties achieving high-quality results and real-time speed at the same
time. In this paper, we propose Fourier Occupancy Field (FOF), a novel
powerful, efficient and flexible 3D representation, for monocular real-time and
accurate human reconstruction. The FOF represents a 3D object with a 2D field
orthogonal to the view direction where at each 2D position the occupancy field
of the object along the view direction is compactly represented with the first
few terms of Fourier series, which retains the topology and neighborhood
relation in the 2D domain. A FOF can be stored as a multi-channel image, which
is compatible with 2D convolutional neural networks and can bridge the gap
between 3D geometries and 2D images. The FOF is very flexible and extensible,
e.g., parametric models can be easily integrated into a FOF as a prior to
generate more robust results. Based on FOF, we design the first 30+FPS
high-fidelity real-time monocular human reconstruction framework. We
demonstrate the potential of FOF on both public dataset and real captured data.
The code will be released for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GridShift: A Faster Mode-seeking Algorithm for Image Segmentation and Object Tracking. (arXiv:2206.02200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02200">
<div class="article-summary-box-inner">
<span><p>In machine learning and computer vision, mean shift (MS) qualifies as one of
the most popular mode-seeking algorithms used for clustering and image
segmentation. It iteratively moves each data point to the weighted mean of its
neighborhood data points. The computational cost required to find the neighbors
of each data point is quadratic to the number of data points. Consequently, the
vanilla MS appears to be very slow for large-scale datasets. To address this
issue, we propose a mode-seeking algorithm called GridShift, with significant
speedup and principally based on MS. To accelerate, GridShift employs a
grid-based approach for neighbor search, which is linear in the number of data
points. In addition, GridShift moves the active grid cells (grid cells
associated with at least one data point) in place of data points towards the
higher density, a step that provides more speedup. The runtime of GridShift is
linear in the number of active grid cells and exponential in the number of
features. Therefore, it is ideal for large-scale low-dimensional applications
such as object tracking and image segmentation. Through extensive experiments,
we showcase the superior performance of GridShift compared to other MS-based as
well as state-of-the-art algorithms in terms of accuracy and runtime on
benchmark datasets for image segmentation. Finally, we provide a new
object-tracking algorithm based on GridShift and show promising results for
object tracking compared to CamShift and meanshift++.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Convolutional with Attention for Action Recognition. (arXiv:2206.02203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02203">
<div class="article-summary-box-inner">
<span><p>Human action recognition is one of the challenging tasks in computer vision.
The current action recognition methods use computationally expensive models for
learning spatio-temporal dependencies of the action. Models utilizing RGB
channels and optical flow separately, models using a two-stream fusion
technique, and models consisting of both convolutional neural network (CNN) and
long-short term memory (LSTM) network are few examples of such complex models.
Moreover, fine-tuning such complex models is computationally expensive as well.
This paper proposes a deep neural network architecture for learning such
dependencies consisting of a 3D convolutional layer, fully connected (FC)
layers, and attention layer, which is simpler to implement and gives a
competitive performance on the UCF-101 dataset. The proposed method first
learns spatial and temporal features of actions through 3D-CNN, and then the
attention mechanism helps the model to locate attention to essential features
for recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U(1) Symmetry-breaking Observed in Generic CNN Bottleneck Layers. (arXiv:2206.02220v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02220">
<div class="article-summary-box-inner">
<span><p>We report on a significant discovery linking deep convolutional neural
networks (CNN) to biological vision and fundamental particle physics. A model
of information propagation in a CNN is proposed via an analogy to an optical
system, where bosonic particles (i.e. photons) are concentrated as the 2D
spatial resolution of the image collapses to a focal point $1\times 1=1$. A 3D
space $(x,y,t)$ is defined by $(x,y)$ coordinates in the image plane and CNN
layer $t$, where a principal ray $(0,0,t)$ runs in the direction of information
propagation through both the optical axis and the image center pixel located at
$(x,y)=(0,0)$, about which the sharpest possible spatial focus is limited to a
circle of confusion in the image plane. Our novel insight is to model the
principal optical ray $(0,0,t)$ as geometrically equivalent to the medial
vector in the positive orthant $I(x,y) \in R^{N+}$ of a $N$-channel activation
space, e.g. along the greyscale (or luminance) vector $(t,t,t)$ in $RGB$ colour
space. Information is thus concentrated into an energy potential
$E(x,y,t)=\|I(x,y,t)\|^2$, which, particularly for bottleneck layers $t$ of
generic CNNs, is highly concentrated and symmetric about the spatial origin
$(0,0,t)$ and exhibits the well-known "Sombrero" potential of the boson
particle. This symmetry is broken in classification, where bottleneck layers of
generic pre-trained CNN models exhibit a consistent class-specific bias towards
an angle $\theta \in U(1)$ defined simultaneously in the image plane and in
activation feature space. Initial observations validate our hypothesis from
generic pre-trained CNN activation maps and a bare-bones memory-based
classification scheme, with no training or tuning. Training from scratch using
a random $U(1)$ class label the leads to improved classification in all cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography. (arXiv:2206.02225v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02225">
<div class="article-summary-box-inner">
<span><p>Displacement estimation is a critical step of virtually all Ultrasound
Elastography (USE) techniques. Two main features make this task unique compared
to the general optical flow problem: the high-frequency nature of ultrasound
radio-frequency (RF) data and the governing laws of physics on the displacement
field. Recently, the architecture of the optical flow networks has been
modified to be able to use RF data. Also, semi-supervised and unsupervised
techniques have been employed for USE by considering prior knowledge of
displacement continuity in the form of the first- and second-derivative
regularizers. Despite these attempts, no work has considered the tissue
compression pattern, and displacements in axial and lateral directions have
been assumed to be independent. However, tissue motion pattern is governed by
laws of physics in USE, rendering the axial and the lateral displacements
highly correlated. In this paper, we propose Physically Inspired ConsTraint for
Unsupervised Regularized Elastography (PICTURE), where we impose constraints on
the Poisson's ratio to improve lateral displacement estimates. Experiments on
phantom and in vivo data show that PICTURE substantially improves the quality
of the lateral displacement estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Decades of Bengali Handwritten Digit Recognition: A Survey. (arXiv:2206.02234v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02234">
<div class="article-summary-box-inner">
<span><p>Handwritten Digit Recognition (HDR) is one of the most challenging tasks in
the domain of Optical Character Recognition (OCR). Irrespective of language,
there are some inherent challenges of HDR, which mostly arise due to the
variations in writing styles across individuals, writing medium and
environment, inability to maintain the same strokes while writing any digit
repeatedly, etc. In addition to that, the structural complexities of the digits
of a particular language may lead to ambiguous scenarios of HDR. Over the
years, researchers have developed numerous offline and online HDR pipelines,
where different image processing techniques are combined with traditional
Machine Learning (ML)-based and/or Deep Learning (DL)-based architectures.
Although evidence of extensive review studies on HDR exists in the literature
for languages, such as: English, Arabic, Indian, Farsi, Chinese, etc., few
surveys on Bengali HDR (BHDR) can be found, which lack a comprehensive analysis
of the challenges, the underlying recognition process, and possible future
directions. In this paper, the characteristics and inherent ambiguities of
Bengali handwritten digits along with a comprehensive insight of two decades of
the state-of-the-art datasets and approaches towards offline BHDR have been
analyzed. Furthermore, several real-life application-specific studies, which
involve BHDR, have also been discussed in detail. This paper will also serve as
a compendium for researchers interested in the science behind offline BHDR,
instigating the exploration of newer avenues of relevant research that may
further lead to better offline recognition of Bengali handwritten digits in
different application areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Annotation and Learning for 3D Hand Pose Estimation: A Survey. (arXiv:2206.02257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02257">
<div class="article-summary-box-inner">
<span><p>In this survey, we present comprehensive analysis of 3D hand pose estimation
from the perspective of efficient annotation and learning. In particular, we
study recent approaches for 3D hand pose annotation and learning methods with
limited annotated data. In 3D hand pose estimation, collecting 3D hand pose
annotation is a key step in developing hand pose estimators and their
applications, such as video understanding, AR/VR, and robotics. However,
acquiring annotated 3D hand poses is cumbersome, e.g., due to the difficulty of
accessing 3D information and occlusion. Motivated by elucidating how recent
works address the annotation issue, we investigated annotation methods
classified as manual, synthetic-model-based, hand-sensor-based, and
computational approaches. Since these annotation methods are not always
available on a large scale, we examined methods of learning 3D hand poses when
we do not have enough annotated data, namely self-supervised pre-training,
semi-supervised learning, and domain adaptation. Based on the analysis of these
efficient annotation and learning, we further discuss limitations and possible
future directions of this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SealID: Saimaa ringed seal re-identification dataset. (arXiv:2206.02260v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02260">
<div class="article-summary-box-inner">
<span><p>Wildlife camera traps and crowd-sourced image material provide novel
possibilities to monitor endangered animal species. However, massive image
volumes that these methods produce are overwhelming for researchers to go
through manually which calls for automatic systems to perform the analysis. The
analysis task that has gained the most attention is the re-identification of
individuals, as it allows, for example, to study animal migration or to
estimate the population size. The Saimaa ringed seal (Pusa hispida saimensis)
is an endangered subspecies only found in the Lake Saimaa, Finland, and is one
of the few existing freshwater seal species. Ringed seals have permanent pelage
patterns that are unique to each individual which can be used for the
identification of individuals. Large variation in poses further exacerbated by
the deformable nature of seals together with varying appearance and low
contrast between the ring pattern and the rest of the pelage makes the Saimaa
ringed seal re-identification task very challenging, providing a good benchmark
to evaluate state-of-the-art re-identification methods. Therefore, we make our
Saimaa ringed seal image (SealID) dataset (N=57) publicly available for
research purposes. In this paper, the dataset is described, the evaluation
protocol for re-identification methods is proposed, and the results for two
baseline methods HotSpotter and NORPPA are provided. The SealID dataset has
been made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02261">
<div class="article-summary-box-inner">
<span><p>This paper combines deep learning techniques for species detection, 3D model
fitting, and metric learning in one pipeline to perform individual animal
identification from photographs by exploiting unique coat patterns. This is the
first work to attempt this and, compared to traditional 2D bounding box or
segmentation based CNN identification pipelines, the approach provides
effective and explicit view-point normalisation and allows for a straight
forward visualisation of the learned biometric population space. Note that due
to the use of metric learning the pipeline is also readily applicable to open
set and zero shot re-identification scenarios. We apply the proposed approach
to individual Grevy's zebra (Equus grevyi) identification and show in a small
study on the SMALST dataset that the use of 3D model fitting can indeed benefit
performance. In particular, back-projected textures from 3D fitted models
improve identification accuracy from 48.0% to 56.8% compared to 2D bounding box
approaches for the dataset. Whilst the study is far too small accurately to
estimate the full performance potential achievable in larger-scale real-world
application settings and in comparisons against polished tools, our work lays
the conceptual and practical foundations for a next step in animal biometrics
towards deep metric learning driven, fully 3D-aware animal identification in
open population settings. We publish network weights and relevant facilitating
source code with this paper for full reproducibility and as inspiration for
further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Building Energy Efficiency From Street View Imagery, Aerial Imagery, and Land Surface Temperature Data. (arXiv:2206.02270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02270">
<div class="article-summary-box-inner">
<span><p>In the race towards carbon neutrality, the building sector has fallen behind
and bears the potential to endanger the progress made across other industries.
This is because buildings exhibit a life span of several decades which creates
substantial inertia in the face of climate change. This inertia is further
exacerbated by the scale of the existing building stock. With several billion
operational buildings around the globe, working towards a carbon-neutral
building sector requires solutions which enable stakeholders to accurately
identify and retrofit subpar buildings at scale. However, improving the energy
efficiency of the existing building stock through retrofits in a targeted and
efficient way remains challenging. This is because, as of today, the energy
efficiency of buildings is generally determined by on-site visits of certified
energy auditors which makes the process slow, costly, and geographically
incomplete. In order to accelerate the identification of promising retrofit
targets, this work proposes a new method which can estimate a building's energy
efficiency using purely remotely sensed data such as street view and aerial
imagery, OSM-derived footprint areas, and satellite-borne land surface
temperature (LST) measurements. We find that in the binary setting of
distinguishing efficient from inefficient buildings, our end-to-end deep
learning model achieves a macro-averaged F1-score of 62.06\%. As such, this
work shows the potential and complementary nature of remotely sensed data in
predicting building attributes such as energy efficiency and opens up new
opportunities for future work to integrate additional data sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive Model for Multi-Pass SAR Change Detection Based on Image Stacks. (arXiv:2206.02278v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02278">
<div class="article-summary-box-inner">
<span><p>Change detection is an important synthetic aperture radar (SAR) application,
usually used to detect changes on the ground scene measurements in different
moments in time. Traditionally, change detection algorithm (CDA) is mainly
designed for two synthetic aperture radar (SAR) images retrieved at different
instants. However, more images can be used to improve the algorithms
performance, witch emerges as a research topic on SAR change detection. Image
stack information can be treated as a data series over time and can be modeled
by autoregressive (AR) models. Thus, we present some initial findings on SAR
change detection based on image stack considering AR models. Applying AR model
for each pixel position in the image stack, we obtained an estimated image of
the ground scene which can be used as a reference image for CDA. The
experimental results reveal that ground scene estimates by the AR models is
accurate and can be used for change detection applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E^2VTS: Energy-Efficient Video Text Spotting from Unmanned Aerial Vehicles. (arXiv:2206.02281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02281">
<div class="article-summary-box-inner">
<span><p>Unmanned Aerial Vehicles (UAVs) based video text spotting has been
extensively used in civil and military domains. UAV's limited battery capacity
motivates us to develop an energy-efficient video text spotting solution. In
this paper, we first revisit RCNN's crop &amp; resize training strategy and
empirically find that it outperforms aligned RoI sampling on a real-world video
text dataset captured by UAV. To reduce energy consumption, we further propose
a multi-stage image processor that takes videos' redundancy, continuity, and
mixed degradation into account. Lastly, the model is pruned and quantized
before deployed on Raspberry Pi. Our proposed energy-efficient video text
spotting solution, dubbed as E^2VTS, outperforms all previous methods by
achieving a competitive tradeoff between energy efficiency and performance. All
our codes and pre-trained models are available at
https://github.com/wuzhenyusjtu/LPCVC20-VideoTextSpotting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tagged-MRI2Audio with Attention Guided Heterogeneous Translator. (arXiv:2206.02284v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02284">
<div class="article-summary-box-inner">
<span><p>Understanding the underlying relationship between tongue and oropharyngeal
muscle deformation seen in tagged-MRI and intelligible speech plays an
important role in advancing speech motor control theories and treatment of
speech related-disorders. Because of their heterogeneous representations,
however, direct mapping between the two modalities -- i.e., two-dimensional
(mid-sagittal slice) plus time tagged-MRI sequence and its corresponding
one-dimensional waveform -- is not straightforward. Instead, we resort to
two-dimensional spectrograms as an intermediate representation, which contains
both pitch and resonance, from which to develop an end-to-end deep learning
framework to translate from a sequence of tagged-MRI to its corresponding audio
waveform with limited dataset size. Our framework is based on a novel fully
convolutional asymmetry translator with guidance of a self residual attention
strategy to specifically exploit the moving muscular structures during speech.
In addition, we leverage a pairwise correlation of the samples with the same
utterances with a latent space representation disentanglement strategy.
Furthermore, we incorporate an adversarial training approach with generative
adversarial networks to offer improved realism on our generated spectrograms.
Our experimental results, carried out with a total of 63 tagged-MRI sequences
alongside speech acoustics, showed that our framework enabled the generation of
clear audio waveforms from a sequence of tagged-MRI, surpassing competing
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugLoss: A Learning Methodology for Real-World Dataset Corruption. (arXiv:2206.02286v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02286">
<div class="article-summary-box-inner">
<span><p>Deep Learning (DL) models achieve great successes in many domains. However,
DL models increasingly face safety and robustness concerns, including noisy
labeling in the training stage and feature distribution shifts in the testing
stage. Previous works made significant progress in addressing these problems,
but the focus has largely been on developing solutions for only one problem at
a time. For example, recent work has argued for the use of tunable robust loss
functions to mitigate label noise, and data augmentation (e.g., AugMix) to
combat distribution shifts. As a step towards addressing both problems
simultaneously, we introduce AugLoss, a simple but effective methodology that
achieves robustness against both train-time noisy labeling and test-time
feature distribution shifts by unifying data augmentation and robust loss
functions. We conduct comprehensive experiments in varied settings of
real-world dataset corruption to showcase the gains achieved by AugLoss
compared to previous state-of-the-art methods. Lastly, we hope this work will
open new directions for designing more robust and reliable DL models under
real-world corruptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training. (arXiv:2206.02288v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02288">
<div class="article-summary-box-inner">
<span><p>We aim to develop semi-supervised domain adaptation (SSDA) for medical image
segmentation, which is largely underexplored. We propose to exploit both
labeled source and target domain data, in addition to unlabeled target data in
a unified manner. Specifically, we present a novel asymmetric co-training (ACT)
framework to integrate these subsets and avoid the domination of the source
domain data. Following a divide-and-conquer strategy, we explicitly decouple
the label supervisions in SSDA into two asymmetric sub-tasks, including
semi-supervised learning (SSL) and UDA, and leverage different knowledge from
two segmentors to take into account the distinction between the source and
target label supervisions. The knowledge learned in the two modules is then
adaptively integrated with ACT, by iteratively teaching each other, based on
the confidence-aware pseudo-label. In addition, pseudo label noise is
well-controlled with an exponential MixUp decay scheme for smooth propagation.
Experiments on cross-modality brain tumor MRI segmentation tasks using the
BraTS18 database showed, even with limited labeled target samples, ACT yielded
marked improvements over UDA and state-of-the-art SSDA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIFI-Net: A Novel Network for Enhancement to Underwater Images. (arXiv:2206.02295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02295">
<div class="article-summary-box-inner">
<span><p>A novel network for enhancement to underwater images is proposed in this
paper. It contains a Reinforcement Fusion Module for Haar wavelet images
(RFM-Haar) based on Reinforcement Fusion Unit (RFU), which is used to fuse an
original image and some important information within it. Fusion is achieved for
better enhancement. As this network make "Haar Images into Fusion Images", it
is called HIFI-Net. The experimental results show the proposed HIFI-Net
performs best among many state-of-the-art methods on three datasets at three
normal metrics and a new metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02307">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has shown great promise over annotation scarcity
problems in the context of medical image segmentation. Existing approaches
typically assume a balanced class distribution for both labeled and unlabeled
medical images. However, medical image data in reality is commonly imbalanced
(i.e., multi-class label imbalance), which naturally yields blurry contours and
usually incorrectly labels rare objects. Moreover, it remains unclear whether
all negative samples are equally negative. In this work, we present ACTION, an
Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised
medical image segmentation. Specifically, we first develop an iterative
contrastive distillation algorithm by softly labeling the negatives rather than
binary supervision between positive and negative pairs. We also capture more
semantically similar features from the randomly chosen negative set compared to
the positives to enforce the diversity of the sampled data. Second, we raise a
more important question: Can we really handle imbalanced samples to yield
better performance? Hence, the key innovation in ACTION is to learn global
semantic relationship across the entire dataset and local anatomical features
among the neighbouring pixels with minimal additional memory footprint. During
the training, we introduce anatomical contrast by actively sampling a sparse
set of hard negative pixels, which can generate smoother segmentation
boundaries and more accurate predictions. Extensive experiments across two
benchmark datasets and different unlabeled settings show that ACTION performs
comparable or better than the current state-of-the-art supervised and
semi-supervised methods. Our code and models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation-oriented Knowledge Distillation for Deep Face Recognition. (arXiv:2206.02325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02325">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is a widely-used technique that utilizes large
networks to improve the performance of compact models. Previous KD approaches
usually aim to guide the student to mimic the teacher's behavior completely in
the representation space. However, such one-to-one corresponding constraints
may lead to inflexible knowledge transfer from the teacher to the student,
especially those with low model capacities. Inspired by the ultimate goal of KD
methods, we propose a novel Evaluation oriented KD method (EKD) for deep face
recognition to directly reduce the performance gap between the teacher and
student models during training. Specifically, we adopt the commonly used
evaluation metrics in face recognition, i.e., False Positive Rate (FPR) and
True Positive Rate (TPR) as the performance indicator. According to the
evaluation protocol, the critical pair relations that cause the TPR and FPR
difference between the teacher and student models are selected. Then, the
critical relations in the student are constrained to approximate the
corresponding ones in the teacher by a novel rank-based loss function, giving
more flexibility to the student with low capacity. Extensive experimental
results on popular benchmarks demonstrate the superiority of our EKD over
state-of-the-art competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JigsawHSI: a network for Hyperspectral Image classification. (arXiv:2206.02327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02327">
<div class="article-summary-box-inner">
<span><p>This article describes the performance of JigsawHSI,a convolutional neural
network (CNN) based on Inception but tailored for geoscientific analyses, on
classification with the Indian Pines, Pavia University and Salinas
hyperspectral image data sets. The network is compared against HybridSN, a
spectral-spatial 3D-CNN followed by 2D-CNN that achieves state-of-the-art
results in the datasets. This short article proves that JigsawHSI is able to
meet or exceed HybridSN performance in all three cases. Additionally, the code
and toolkit are made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASNet:Improve Performance of Siamese Networks with Mutual-attention for Remote Sensing Change Detection Tasks. (arXiv:2206.02331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02331">
<div class="article-summary-box-inner">
<span><p>Siamese networks are widely used for remote sensing change detection tasks. A
vanilla siamese network has two identical feature extraction branches which
share weights, these two branches work independently and the feature maps are
not fused until about to be sent to a decoder head. However we find that it is
critical to exchange information between two feature extraction branches at
early stage for change detection task. In this work we present Mutual-Attention
Siamese Network (MASNet), a general siamese network with mutual-attention
plug-in, so to exchange information between the two feature extraction
branches. We show that our modification improve the performance of siamese
networks on multi change detection datasets, and it works for both
convolutional neural network and visual transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression. (arXiv:2206.02338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02338">
<div class="article-summary-box-inner">
<span><p>This paper presents a language-powered paradigm for ordinal regression.
Existing methods usually treat each rank as a category and employ a set of
weights to learn these concepts. These methods are easy to overfit and usually
attain unsatisfactory performance as the learned concepts are mainly derived
from the training set. Recent large pre-trained vision-language models like
CLIP have shown impressive performance on various visual tasks. In this paper,
we propose to learn the rank concepts from the rich semantic CLIP latent space.
Specifically, we reformulate this task as an image-language matching problem
with a contrastive objective, which regards labels as text and obtains a
language prototype from a text encoder for each rank. While prompt engineering
for CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable
prompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists
of learnable context tokens and learnable rank embeddings; The learnable rank
embeddings are constructed by explicitly modeling numerical continuity,
resulting in well-ordered, compact language prototypes in the CLIP space. Once
learned, we can only save the language prototypes and discard the huge language
model, resulting in zero additional computational overhead compared with the
linear head counterpart. Experimental results show that our paradigm achieves
competitive performance in general ordinal regression tasks, and gains
improvements in few-shot and distribution shift settings for age estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WHU-Stereo: A Challenging Benchmark for Stereo Matching of High-Resolution Satellite Images. (arXiv:2206.02342v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02342">
<div class="article-summary-box-inner">
<span><p>Stereo matching of high-resolution satellite images (HRSI) is still a
fundamental but challenging task in the field of photogrammetry and remote
sensing. Recently, deep learning (DL) methods, especially convolutional neural
networks (CNNs), have demonstrated tremendous potential for stereo matching on
public benchmark datasets. However, datasets for stereo matching of satellite
images are scarce. To facilitate further research, this paper creates and
publishes a challenging dataset, termed WHU-Stereo, for stereo matching DL
network training and testing. This dataset is created by using airborne LiDAR
point clouds and high-resolution stereo imageries taken from the Chinese
GaoFen-7 satellite (GF-7). The WHU-Stereo dataset contains more than 1700
epipolar rectified image pairs, which cover six areas in China and includes
various kinds of landscapes. We have assessed the accuracy of ground-truth
disparity maps, and it is proved that our dataset achieves comparable precision
compared with existing state-of-the-art stereo matching datasets. To verify its
feasibility, in experiments, the hand-crafted SGM stereo matching algorithm and
recent deep learning networks have been tested on the WHU-Stereo dataset.
Experimental results show that deep learning networks can be well trained and
achieves higher performance than hand-crafted SGM algorithm, and the dataset
has great potential in remote sensing application. The WHU-Stereo dataset can
serve as a challenging benchmark for stereo matching of high-resolution
satellite images, and performance evaluation of deep learning models. Our
dataset is available at https://github.com/Sheng029/WHU-Stereo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Graph Multimodal Model for Text Classification in Videos. (arXiv:2206.02343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02343">
<div class="article-summary-box-inner">
<span><p>The extraction of text information in videos serves as a critical step
towards semantic understanding of videos. It usually involved in two steps: (1)
text recognition and (2) text classification. To localize texts in videos, we
can resort to large numbers of text recognition methods based on OCR
technology. However, to our knowledge, there is no existing work focused on the
second step of video text classification, which will limit the guidance to
downstream tasks such as video indexing and browsing. In this paper, we are the
first to address this new task of video text classification by fusing
multimodal information to deal with the challenging scenario where different
types of video texts may be confused with various colors, unknown fonts and
complex layouts. In addition, we tailor a specific module called CorrelationNet
to reinforce feature representation by explicitly extracting layout
information. Furthermore, contrastive learning is utilized to explore inherent
connections between samples using plentiful unlabeled videos. Finally, we
construct a new well-defined industrial dataset from the news domain, called
TI-News, which is dedicated to building and evaluating video text recognition
and classification applications. Extensive experiments on TI-News demonstrate
the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection with Test Time Augmentation and Consistency Evaluation. (arXiv:2206.02345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02345">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are known to be vulnerable to unseen data: they may
wrongly assign high confidence stcores to out-distribuion samples. Recent works
try to solve the problem using representation learning methods and specific
metrics. In this paper, we propose a simple, yet effective post-hoc anomaly
detection algorithm named Test Time Augmentation Anomaly Detection (TTA-AD),
inspired by a novel observation. Specifically, we observe that in-distribution
data enjoy more consistent predictions for its original and augmented versions
on a trained network than out-distribution data, which separates
in-distribution and out-distribution samples. Experiments on various
high-resolution image benchmark datasets demonstrate that TTA-AD achieves
comparable or better detection performance under dataset-vs-dataset anomaly
detection settings with a 60%~90\% running time reduction of existing
classifier-based algorithms. We provide empirical verification that the key to
TTA-AD lies in the remaining classes between augmented features, which has long
been partially ignored by previous works. Additionally, we use RUNS as a
surrogate to analyze our algorithm theoretically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Binary Neural Networks for Image Recognition. (arXiv:1909.09934v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.09934">
<div class="article-summary-box-inner">
<span><p>We propose methods to train convolutional neural networks (CNNs) with both
binarized weights and activations, leading to quantized models that are
specifically friendly to mobile devices with limited power capacity and
computation resources. Previous works on quantizing CNNs often seek to
approximate the floating-point information using a set of discrete values,
which we call value approximation, typically assuming the same architecture as
the full-precision networks. Here we take a novel "structure approximation"
view of quantization -- it is very likely that different architectures designed
for low-bit networks may be better for achieving good performance. In
particular, we propose a "network decomposition" strategy, termed Group-Net, in
which we divide the network into groups. Thus, each full-precision group can be
effectively reconstructed by aggregating a set of homogeneous binary branches.
In addition, we learn effective connections among groups to improve the
representation capability. Moreover, the proposed Group-Net shows strong
generalization to other tasks. For instance, we extend Group-Net for accurate
semantic segmentation by embedding rich context into the binary structure.
Furthermore, for the first time, we apply binary neural networks to object
detection. Experiments on both classification, semantic segmentation and object
detection tasks demonstrate the superior performance of the proposed methods
over various quantized networks in the literature. Our methods outperform the
previous best binary neural networks in terms of accuracy and computation
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Train Your Dragon: Tamed Warping Network for Semantic Video Segmentation. (arXiv:2005.01344v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.01344">
<div class="article-summary-box-inner">
<span><p>Real-time semantic segmentation on high-resolution videos is challenging due
to the strict requirements of speed. Recent approaches have utilized the
inter-frame continuity to reduce redundant computation by warping the feature
maps across adjacent frames, greatly speeding up the inference phase. However,
their accuracy drops significantly owing to the imprecise motion estimation and
error accumulation. In this paper, we propose to introduce a simple and
effective correction stage right after the warping stage to form a framework
named Tamed Warping Network (TWNet), aiming to improve the accuracy and
robustness of warping-based models. The experimental results on the Cityscapes
dataset show that with the correction, the accuracy (mIoU) significantly
increases from 67.3% to 71.6%, and the speed edges down from 65.5 FPS to 61.8
FPS. For non-rigid categories such as "human" and "object", the improvements of
IoU are even higher than 18 percentage points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using Limited Dataset. (arXiv:2005.07225v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.07225">
<div class="article-summary-box-inner">
<span><p>While deep learning approaches have shown remarkable performance in many
imaging tasks, most of these methods rely on availability of large quantities
of data. Medical image data, however, is scarce and fragmented. Generative
Adversarial Networks (GANs) have recently been very effective in handling such
datasets by generating more data. If the datasets are very small, however, GANs
cannot learn the data distribution properly, resulting in less diverse or
low-quality results. One such limited dataset is that for the concurrent gain
of 19 and 20 chromosomes (19/20 co-gain), a mutation with positive prognostic
value in Glioblastomas (GBM). In this paper, we detect imaging biomarkers for
the mutation to streamline the extensive and invasive prognosis pipeline. Since
this mutation is relatively rare, i.e. small dataset, we propose a novel
generative framework - the Sequential Attribute GEnerator (SAGE), that
generates detailed tumor imaging features while learning from a limited
dataset. Experiments show that not only does SAGE generate high quality tumors
when compared to standard Deep Convolutional GAN (DC-GAN) and Wasserstein GAN
with Gradient Penalty (WGAN-GP), it also captures the imaging biomarkers
accurately.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Unlabeled Data for Increasing Low-Shot Classification Accuracy of Relevant and Open-Set Irrelevant Images. (arXiv:2010.00721v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00721">
<div class="article-summary-box-inner">
<span><p>In search, exploration, and reconnaissance tasks performed with autonomous
ground vehicles, an image classification capability is needed for specifically
identifying targeted objects (relevant classes) and at the same time recognize
when a candidate image does not belong to anyone of the relevant classes
(irrelevant images). In this paper, we present an open-set low-shot classifier
that uses, during its training, a modest number (less than 40) of labeled
images for each relevant class, and unlabeled irrelevant images that are
randomly selected at each epoch of the training process. The new classifier is
capable of identifying images from the relevant classes, determining when a
candidate image is irrelevant, and it can further recognize categories of
irrelevant images that were not included in the training (unseen). The proposed
low-shot classifier can be attached as a top layer to any pre-trained feature
extractor when constructing a Convolutional Neural Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Far Can We Get with Neural Networks Straight from JPEG?. (arXiv:2012.14426v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14426">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have achieved astonishing advances over
the past decade, defining state-of-the-art in several computer vision tasks.
CNNs are capable of learning robust representations of the data directly from
the RGB pixels. However, most image data are usually available in compressed
format, from which the JPEG is the most widely used due to transmission and
storage purposes demanding a preliminary decoding process that have a high
computational load and memory usage. For this reason, deep learning methods
capable of leaning directly from the compressed domain have been gaining
attention in recent years. These methods adapt typical CNNs to work on the
compressed domain, but the common architectural modifications lead to an
increase in computational complexity and the number of parameters. In this
paper, we investigate the usage of CNNs that are designed to work directly with
the DCT coefficients available in JPEG compressed images, proposing a
handcrafted and data-driven techniques for reducing the computational
complexity and the number of parameters for these models in order to keep their
computational cost similar to their RGB baselines. We make initial ablation
studies on a subset of ImageNet in order to analyse the impact of different
frequency ranges, image resolution, JPEG quality and classification task
difficulty on the performance of the models. Then, we evaluate the models on
the complete ImageNet dataset. Our results indicate that DCT models are capable
of obtaining good performance, and that it is possible to reduce the
computational complexity and the number of parameters from these models while
retaining a similar classification accuracy through the use of our proposed
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-point dimensionality reduction to improve projection layout reliability. (arXiv:2101.06224v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06224">
<div class="article-summary-box-inner">
<span><p>In ordinary Dimensionality Reduction (DR), each data instance in a high
dimensional space (original space), or on a distance matrix denoting original
space distances, is mapped to (projected onto) one point in a low dimensional
space (visual space), building a layout of projected points trying to preserve
as much as possible some property of data such as distances, neighbourhood
relationships, and/or topology structures, with the ultimate goal of
approximating semantic properties of data with preserved geometric properties
or topology structures in visual space. In this paper, the concept of
Multi-point Dimensionality Reduction is elaborated on where each data instance
can be mapped to (projected onto) possibly more than one point in visual space
by providing the first general solution (algorithm) for it as a move in the
direction of improving reliablity, usability and interpretability of
dimensionality reduction. Furthermore by allowing the points in visual space to
be split into two layers while maintaining the possibility of having more than
one projection (mapping) per data instance , the benefit of separating more
reliable points from less reliable points is dicussed notwithstanding the
effort to improve less reliable points. The proposed solution (algorithm) in
this paper, named Layered Vertex Splitting Data Embedding (LVSDE), is built
upon and extends a combination of ordinary DR and graph drawing techniques.
Based on the experiments of this paper on some data sets, the particular
proposed algorithm (LVSDE) practically outperforms popular ordinary DR methods
visually (semantics, group separation, subgroup detection or combinational
group detection) in a way that is easily explainable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Depth Estimation through Virtual-world Supervision and Real-world SfM Self-Supervision. (arXiv:2103.12209v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12209">
<div class="article-summary-box-inner">
<span><p>Depth information is essential for on-board perception in autonomous driving
and driver assistance. Monocular depth estimation (MDE) is very appealing since
it allows for appearance and depth being on direct pixelwise correspondence
without further calibration. Best MDE models are based on Convolutional Neural
Networks (CNNs) trained in a supervised manner, i.e., assuming pixelwise ground
truth (GT). Usually, this GT is acquired at training time through a calibrated
multi-modal suite of sensors. However, also using only a monocular system at
training time is cheaper and more scalable. This is possible by relying on
structure-from-motion (SfM) principles to generate self-supervision.
Nevertheless, problems of camouflaged objects, visibility changes,
static-camera intervals, textureless areas, and scale ambiguity, diminish the
usefulness of such self-supervision. In this paper, we perform monocular depth
estimation by virtual-world supervision (MonoDEVS) and real-world SfM
self-supervision. We compensate the SfM self-supervision limitations by
leveraging virtual-world images with accurate semantic and depth supervision
and addressing the virtual-to-real domain gap. Our MonoDEVSNet outperforms
previous MDE CNNs trained on monocular and even stereo sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-level Image Retrieval using Reranking Transformers. (arXiv:2103.12236v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12236">
<div class="article-summary-box-inner">
<span><p>Instance-level image retrieval is the task of searching in a large database
for images that match an object in a query image. To address this task, systems
usually rely on a retrieval step that uses global image descriptors, and a
subsequent step that performs domain-specific refinements or reranking by
leveraging operations such as geometric verification based on local features.
In this work, we propose Reranking Transformers (RRTs) as a general model to
incorporate both local and global features to rerank the matching images in a
supervised fashion and thus replace the relatively expensive process of
geometric verification. RRTs are lightweight and can be easily parallelized so
that reranking a set of top matching results can be performed in a single
forward-pass. We perform extensive experiments on the Revisited Oxford and
Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs
outperform previous reranking approaches while using much fewer local
descriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs
can be optimized jointly with the feature extractor, which can lead to feature
representations tailored to downstream tasks and further accuracy improvements.
The code and trained models are publicly available at
https://github.com/uvavision/RerankingTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLAN: Multi-Level Adversarial Network for Domain Adaptive Semantic Segmentation. (arXiv:2103.12991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12991">
<div class="article-summary-box-inner">
<span><p>Recent progresses in domain adaptive semantic segmentation demonstrate the
effectiveness of adversarial learning (AL) in unsupervised domain adaptation.
However, most adversarial learning based methods align source and target
distributions at a global image level but neglect the inconsistency around
local image regions. This paper presents a novel multi-level adversarial
network (MLAN) that aims to address inter-domain inconsistency at both global
image level and local region level optimally. MLAN has two novel designs,
namely, region-level adversarial learning (RL-AL) and co-regularized
adversarial learning (CR-AL). Specifically, RL-AL models prototypical regional
context-relations explicitly in the feature space of a labelled source domain
and transfers them to an unlabelled target domain via adversarial learning.
CR-AL fuses region-level AL and image-level AL optimally via mutual
regularization. In addition, we design a multi-level consistency map that can
guide domain adaptation in both input space ($i.e.$, image-to-image
translation) and output space ($i.e.$, self-training) effectively. Extensive
experiments show that MLAN outperforms the state-of-the-art with a large margin
consistently across multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02904">
<div class="article-summary-box-inner">
<span><p>Object detection with multimodal inputs can improve many safety-critical
systems such as autonomous vehicles (AVs). Motivated by AVs that operate in
both day and night, we study multimodal object detection with RGB and thermal
cameras, since the latter provides much stronger object signatures under poor
illumination. We explore strategies for fusing information from different
modalities. Our key contribution is a probabilistic ensembling technique,
ProbEn, a simple non-learned method that fuses together detections from
multi-modalities. We derive ProbEn from Bayes' rule and first principles that
assume conditional independence across modalities. Through probabilistic
marginalization, ProbEn elegantly handles missing modalities when detectors do
not fire on the same object. Importantly, ProbEn also notably improves
multimodal detection even when the conditional independence assumption does not
hold, e.g., fusing outputs from other fusion methods (both off-the-shelf and
trained in-house). We validate ProbEn on two benchmarks containing both aligned
(KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms
prior work by more than 13% in relative performance!
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The art of defense: letting networks fool the attacker. (arXiv:2104.02963v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02963">
<div class="article-summary-box-inner">
<span><p>Robust environment perception is critical for autonomous cars, and
adversarial defenses are the most effective and widely studied ways to improve
the robustness of environment perception. However, all of previous defense
methods decrease the natural accuracy, and the nature of the DNNs itself has
been overlooked. To this end, in this paper, we propose a novel adversarial
defense for 3D point cloud classifier that makes full use of the nature of the
DNNs. Due to the disorder of point cloud, all point cloud classifiers have the
property of permutation invariant to the input point cloud. Based on this
nature, we design invariant transformations defense (IT-Defense). We show that,
even after accounting for obfuscated gradients, our IT-Defense is a resilient
defense against state-of-the-art (SOTA) 3D attacks. Moreover, IT-Defense do not
hurt clean accuracy compared to previous SOTA 3D defenses. Our code is
available at: {\footnotesize{\url{https://github.com/cuge1995/IT-Defense}}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Very Lightweight Photo Retouching Network with Conditional Sequential Modulation. (arXiv:2104.06279v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06279">
<div class="article-summary-box-inner">
<span><p>Photo retouching aims at improving the aesthetic visual quality of images
that suffer from photographic defects, especially for poor contrast, over/under
exposure, and inharmonious saturation. In practice, photo retouching can be
accomplished by a series of image processing operations. As most commonly-used
retouching operations are pixel-independent, i.e., the manipulation on one
pixel is uncorrelated with its neighboring pixels, we can take advantage of
this property and design a specialized algorithm for efficient global photo
retouching. We analyze these global operations and find that they can be
mathematically formulated by a Multi-Layer Perceptron (MLP). Based on this
observation, we propose an extremely lightweight framework -- Conditional
Sequential Retouching Network (CSRNet). Benefiting from the utilization of
$1\times1$ convolution, CSRNet only contains less than 37K trainable
parameters, which are orders of magnitude smaller than existing learning-based
methods. Experiments show that our method achieves state-of-the-art performance
on the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. In
addition to achieve global photo retouching, the proposed framework can be
easily extended to learn local enhancement effects. The extended model, namely
CSRNet-L, also achieves competitive results in various local enhancement tasks.
Codes are available at https://github.com/lyh-18/CSRNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13353">
<div class="article-summary-box-inner">
<span><p>We present a novel approach for unsupervised activity segmentation which uses
video frame clustering as a pretext task and simultaneously performs
representation learning and online clustering. This is in contrast with prior
works where representation learning and clustering are often performed
sequentially. We leverage temporal information in videos by employing temporal
optimal transport. In particular, we incorporate a temporal regularization term
which preserves the temporal order of the activity into the standard optimal
transport module for computing pseudo-label cluster assignments. The temporal
optimal transport module enables our approach to learn effective
representations for unsupervised activity segmentation. Furthermore, previous
methods require storing learned features for the entire dataset before
clustering them in an offline manner, whereas our approach processes one
mini-batch at a time in an online manner. Extensive evaluations on three public
datasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,
i.e., Desktop Assembly, show that our approach performs on par with or better
than previous methods, despite having significantly less memory constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03485">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) defy the classical bias-variance trade-off:
adding parameters to a DNN that interpolates its training data will typically
improve its generalization performance. Explaining the mechanism behind this
``benign overfitting'' in deep networks remains an outstanding challenge. Here,
we study the last hidden layer representations of various state-of-the-art
convolutional neural networks and find that if the last hidden representation
is wide enough, its neurons tend to split into groups that carry identical
information, and differ from each other only by statistically independent
noise. The number of such groups increases linearly with the width of the
layer, but only if the width is above a critical value. We show that redundant
neurons appear only when the training process reaches interpolation and the
training error is zero.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Unsupervised Domain Adaptation for Visual Recognition. (arXiv:2106.06112v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06112">
<div class="article-summary-box-inner">
<span><p>Though unsupervised domain adaptation (UDA) has achieved very impressive
progress recently, it remains a great challenge due to missing target
annotations and the rich discrepancy between source and target distributions.
We propose Spectral UDA (SUDA), an effective and efficient UDA technique that
works in the spectral space and can generalize across different visual
recognition tasks. SUDA addresses the UDA challenges from two perspectives.
First, it introduces a spectrum transformer (ST) that mitigates inter-domain
discrepancies by enhancing domain-invariant spectra while suppressing
domain-variant spectra of source and target samples simultaneously. Second, it
introduces multi-view spectral learning that learns useful unsupervised
representations by maximizing mutual information among multiple ST-generated
spectral views of each target sample. Extensive experiments show that SUDA
achieves superior accuracy consistently across different visual tasks in object
detection, semantic segmentation and image classification. Additionally, SUDA
also works with the transformer-based network and achieves state-of-the-art
performance on object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning of feature points without additional supervision improves reinforcement learning from images. (arXiv:2106.07995v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07995">
<div class="article-summary-box-inner">
<span><p>In many control problems that include vision, optimal controls can be
inferred from the location of the objects in the scene. This information can be
represented using feature points, which is a list of spatial locations in
learned feature maps of an input image. Previous works show that feature points
learned using unsupervised pre-training or human supervision can provide good
features for control tasks. In this paper, we show that it is possible to learn
efficient feature point representations end-to-end, without the need for
unsupervised pre-training, decoders, or additional losses. Our proposed
architecture consists of a differentiable feature point extractor that feeds
the coordinates of the estimated feature points directly to a soft actor-critic
agent. The proposed algorithm yields performance competitive to the
state-of-the art on DeepMind Control Suite tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07791">
<div class="article-summary-box-inner">
<span><p>We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07917">
<div class="article-summary-box-inner">
<span><p>A formal autism diagnosis can be an inefficient and lengthy process. Families
may wait months or longer before receiving a diagnosis for their child despite
evidence that earlier intervention leads to better treatment outcomes. Digital
technologies which detect the presence of behaviors related to autism can scale
access to pediatric diagnoses. This work aims to demonstrate the feasibility of
deep learning technologies for detecting hand flapping from unstructured home
videos as a first step towards validating whether models and digital
technologies can be leveraged to aid with autism diagnoses. We used the
Self-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand
flapping, head banging, and spinning exhibited by children. From all the hand
flapping videos, we extracted 100 positive and control videos of hand flapping,
each between 2 to 5 seconds in duration. Utilizing both
landmark-driven-approaches and MobileNet V2's pretrained convolutional layers,
our highest performing model achieved a testing F1 score of 84% (90% precision
and 80% recall) when evaluating with 5-fold cross validation 100 times. This
work provides the first step towards developing precise deep learning methods
for activity detection of autism-related behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards A Fairer Landmark Recognition Dataset. (arXiv:2108.08874v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08874">
<div class="article-summary-box-inner">
<span><p>We introduce a new landmark recognition dataset, which is created with a
focus on fair worldwide representation. While previous work proposes to collect
as many images as possible from web repositories, we instead argue that such
approaches can lead to biased data. To create a more comprehensive and
equitable dataset, we start by defining the fair relevance of a landmark to the
world population. These relevances are estimated by combining anonymized Google
Maps user contribution statistics with the contributors' demographic
information. We present a stratification approach and analysis which leads to a
much fairer coverage of the world, compared to existing datasets. The resulting
datasets are used to evaluate computer vision models as part of the the Google
Landmark Recognition and RetrievalChallenges 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLVIP: A Visible-infrared Paired Dataset for Low-light Vision. (arXiv:2108.10831v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10831">
<div class="article-summary-box-inner">
<span><p>It is very challenging for various visual tasks such as image fusion,
pedestrian detection and image-to-image translation in low light conditions due
to the loss of effective target areas. In this case, infrared and visible
images can be used together to provide both rich detail information and
effective target areas. In this paper, we present LLVIP, a visible-infrared
paired dataset for low-light vision. This dataset contains 30976 images, or
15488 pairs, most of which were taken at very dark scenes, and all of the
images are strictly aligned in time and space. Pedestrians in the dataset are
labeled. We compare the dataset with other visible-infrared datasets and
evaluate the performance of some popular visual algorithms including image
fusion, pedestrian detection and image-to-image translation on the dataset. The
experimental results demonstrate the complementary effect of fusion on image
information, and find the deficiency of existing algorithms of the three visual
tasks in very low-light conditions. We believe the LLVIP dataset will
contribute to the community of computer vision by promoting image fusion,
pedestrian detection and image-to-image translation in very low-light
applications. The dataset is being released in
https://bupt-ai-cz.github.io/LLVIP. Raw data is also provided for further
research such as image registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03374">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation aims to align a labeled source domain and an
unlabeled target domain, but it requires to access the source data which often
raises concerns in data privacy, data portability and data transmission
efficiency. We study unsupervised model adaptation (UMA), or called
Unsupervised Domain Adaptation without Source Data, an alternative setting that
aims to adapt source-trained models towards target distributions without
accessing source data. To this end, we design an innovative historical
contrastive learning (HCL) technique that exploits historical source hypothesis
to make up for the absence of source data in UMA. HCL addresses the UMA
challenge from two perspectives. First, it introduces historical contrastive
instance discrimination (HCID) that learns from target samples by contrasting
their embeddings which are generated by the currently adapted model and the
historical models. With the historical models, HCID encourages UMA to learn
instance-discriminative target representations while preserving the source
hypothesis. Second, it introduces historical contrastive category
discrimination (HCCD) that pseudo-labels target samples to learn
category-discriminative target representations. Specifically, HCCD re-weights
pseudo labels according to their prediction consistency across the current and
historical models. Extensive experiments show that HCL outperforms and
state-of-the-art methods consistently across a variety of visual tasks and
setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04627">
<div class="article-summary-box-inner">
<span><p>Pretraining language models with next-token prediction on massive text
corpora has delivered phenomenal zero-shot, few-shot, transfer learning and
multi-tasking capabilities on both generative and discriminative language
tasks. Motivated by this success, we explore a Vector-quantized Image Modeling
(VIM) approach that involves pretraining a Transformer to predict rasterized
image tokens autoregressively. The discrete image tokens are encoded from a
learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple
improvements over vanilla VQGAN from architecture to codebook learning,
yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN
further improves vector-quantized image modeling tasks, including
unconditional, class-conditioned image generation and unsupervised
representation learning. When trained on ImageNet at \(256\times256\)
resolution, we achieve Inception Score (IS) of 175.1 and Fr'echet Inception
Distance (FID) of 4.17, a dramatic improvement over the vanilla VQGAN, which
obtains 70.6 and 17.04 for IS and FID, respectively. Based on ViT-VQGAN and
unsupervised pretraining, we further evaluate the pretrained Transformer by
averaging intermediate features, similar to Image GPT (iGPT). This
ImageNet-pretrained VIM-L significantly beats iGPT-L on linear-probe accuracy
from 60.3% to 73.2% for a similar model size. VIM-L also outperforms iGPT-XL
which is trained with extra web image data and larger model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Class Cell Detection Using Spatial Context Representation. (arXiv:2110.04886v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04886">
<div class="article-summary-box-inner">
<span><p>In digital pathology, both detection and classification of cells are
important for automatic diagnostic and prognostic tasks. Classifying cells into
subtypes, such as tumor cells, lymphocytes or stromal cells is particularly
challenging. Existing methods focus on morphological appearance of individual
cells, whereas in practice pathologists often infer cell classes through their
spatial context. In this paper, we propose a novel method for both detection
and classification that explicitly incorporates spatial contextual information.
We use the spatial statistical function to describe local density in both a
multi-class and a multi-scale manner. Through representation learning and deep
clustering techniques, we learn advanced cell representation with both
appearance and spatial context. On various benchmarks, our method achieves
better performance than state-of-the-arts, especially on the classification
task. We also create a new dataset for multi-class cell detection and
classification in breast cancer and we make both our code and data publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways. (arXiv:2110.11048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11048">
<div class="article-summary-box-inner">
<span><p>Lane detection is a critical function for autonomous driving. With the recent
development of deep learning and the publication of camera lane datasets and
benchmarks, camera lane detection networks (CLDNs) have been remarkably
developed. Unfortunately, CLDNs rely on camera images which are often distorted
near the vanishing line and prone to poor lighting condition. This is in
contrast with Lidar lane detection networks (LLDNs), which can directly extract
the lane lines on the bird's eye view (BEV) for motion planning and operate
robustly under various lighting conditions. However, LLDNs have not been
actively studied, mostly due to the absence of large public lidar lane
datasets. In this paper, we introduce KAIST-Lane (K-Lane), the world's first
and the largest public urban road and highway lane dataset for Lidar. K-Lane
has more than 15K frames and contains annotations of up to six lanes under
various road and traffic conditions, e.g., occluded roads of multiple occlusion
levels, roads at day and night times, merging (converging and diverging) and
curved lanes. We also provide baseline networks we term Lidar lane detection
networks utilizing global feature correlator (LLDN-GFC). LLDN-GFC exploits the
spatial characteristics of lane lines on the point cloud, which are sparse,
thin, and stretched along the entire ground plane of the point cloud. From
experimental results, LLDN-GFC achieves the state-of-the-art performance with
an F1- score of 82.1%, on the K-Lane. Moreover, LLDN-GFC shows strong
performance under various lighting conditions, which is unlike CLDNs, and also
robust even in the case of severe occlusions, unlike LLDNs using the
conventional CNN. The K-Lane, LLDN-GFC training code, pre-trained models, and
complete development kits including evaluation, visualization and annotation
tools are available at https://github.com/kaist-avelab/k-lane.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention. (arXiv:2110.12372v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12372">
<div class="article-summary-box-inner">
<span><p>Since radiologists have different training and clinical experiences, they may
provide various segmentation annotations for a lung nodule. Conventional
studies choose a single annotation as the learning target by default, but they
waste valuable information of consensus or disagreements ingrained in the
multiple annotations. This paper proposes an Uncertainty-Guided Segmentation
Network (UGS-Net), which learns the rich visual features from the regions that
may cause segmentation uncertainty and contributes to a better segmentation
result. With an Uncertainty-Aware Module, this network can provide a
Multi-Confidence Mask (MCM), pointing out regions with different segmentation
uncertainty levels. Moreover, this paper introduces a Feature-Aware Attention
Module to enhance the learning of the nodule boundary and density differences.
Experimental results show that our method can predict the nodule regions with
different uncertainty levels and achieve superior performance in LIDC-IDRI
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14051">
<div class="article-summary-box-inner">
<span><p>Machine learning models often encounter samples that are diverged from the
training distribution. Failure to recognize an out-of-distribution (OOD)
sample, and consequently assign that sample to an in-class label significantly
compromises the reliability of a model. The problem has gained significant
attention due to its importance for safety deploying models in open-world
settings. Detecting OOD samples is challenging due to the intractability of
modeling all possible unknown distributions. To date, several research domains
tackle the problem of detecting unfamiliar samples, including anomaly
detection, novelty detection, one-class learning, open set recognition, and
out-of-distribution detection. Despite having similar and shared concepts,
out-of-distribution, open-set, and anomaly detection have been investigated
independently. Accordingly, these research avenues have not cross-pollinated,
creating research barriers. While some surveys intend to provide an overview of
these approaches, they seem to only focus on a specific domain without
examining the relationship between different domains. This survey aims to
provide a cross-domain and comprehensive review of numerous eminent works in
respective areas while identifying their commonalities. Researchers can benefit
from the overview of research advances in different fields and develop future
methodology synergistically. Furthermore, to the best of our knowledge, while
there are surveys in anomaly detection or one-class learning, there is no
comprehensive or up-to-date survey on out-of-distribution detection, which our
survey covers extensively. Finally, having a unified cross-domain perspective,
we discuss and shed light on future lines of research, intending to bring these
fields closer together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodEx: A Modular Framework for Joint Temporal De-blurring and Tomographic Reconstruction. (arXiv:2111.06069v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06069">
<div class="article-summary-box-inner">
<span><p>In many computed tomography (CT) imaging applications, it is important to
rapidly collect data from an object that is moving or changing with time.
Tomographic acquisition is generally assumed to be step-and-shoot, where the
object is rotated to each desired angle, and a view is taken. However,
step-and-shoot acquisition is slow and can waste photons, so in practice
fly-scanning is done where the object is continuously rotated while collecting
data. However, this can result in motion-blurred views and consequently
reconstructions with severe motion artifacts.
</p>
<p>In this paper, we introduce CodEx, a modular framework for joint de-blurring
and tomographic reconstruction that can effectively invert the motion blur
introduced in sparse view fly-scanning. The method is a synergistic combination
of a novel acquisition method with a novel non-convex Bayesian reconstruction
algorithm. CodEx works by encoding the acquisition with a known binary code
that the reconstruction algorithm then inverts. Using a well chosen binary code
to encode the measurements can improve the accuracy of the inversion process.
The CodEx reconstruction method uses the alternating direction method of
multipliers (ADMM) to split the inverse problem into iterative deblurring and
reconstruction sub-problems, making reconstruction practical to implement. We
present reconstruction results on both simulated and binned experimental data
to demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Re-renderable Facial Albedo Reconstruction from Single Image. (arXiv:2111.08282v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08282">
<div class="article-summary-box-inner">
<span><p>Reconstructing high-fidelity 3D facial texture from a single image is a quite
challenging task due to the lack of complete face information and the domain
gap between the 3D face and 2D image. Further, obtaining re-renderable 3D faces
has become a strongly desired property in many applications, where the term
're-renderable' demands the facial texture to be spatially complete and
disentangled with environmental illumination. In this paper, we propose a new
self-supervised deep learning framework for reconstructing high-quality and
re-renderable facial albedos from single-view images in-the-wild. Our main idea
is to first utilize a prior generation module based on the 3DMM proxy model to
produce an unwrapped texture and a globally parameterized prior albedo. Then we
apply a detail refinement module to synthesize the final texture with both
high-frequency details and completeness. To further make facial textures
disentangled with illumination, we propose a novel detailed illumination
representation which is reconstructed with the detailed albedo together. We
also design several novel regularization losses on both the albedo and
illumination maps to facilitate the disentanglement of these two factors.
Finally, by leveraging a differentiable renderer, each face attribute can be
jointly trained in a self-supervised manner without requiring ground-truth
facial reflectance. Extensive comparisons and ablation studies on challenging
datasets demonstrate that our framework outperforms state-of-the-art
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring dual-attention mechanism with multi-scale feature extraction scheme for skin lesion segmentation. (arXiv:2111.08708v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08708">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of skin lesions from dermoscopic images is a
challenging task due to the irregular lesion boundaries, poor contrast between
the lesion and the background, and the presence of artifacts. In this work, a
new convolutional neural network-based approach is proposed for skin lesion
segmentation. In this work, a novel multi-scale feature extraction module is
proposed for extracting more discriminative features for dealing with the
challenges related to complex skin lesions; this module is embedded in the
UNet, replacing the convolutional layers in the standard architecture. Further
in this work, two different attention mechanisms refine the feature extracted
by the encoder and the post-upsampled features. This work was evaluated using
the two publicly available datasets, including ISBI2017 and ISIC2018 datasets.
The proposed method reported an accuracy, recall, and JSI of 97.5%, 94.29%,
91.16% on the ISBI2017 dataset and 95.92%, 95.37%, 91.52% on the ISIC2018
dataset. It outperformed the existing methods and the top-ranked models in the
respective competitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Object Detection via Association and DIscrimination. (arXiv:2111.11656v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11656">
<div class="article-summary-box-inner">
<span><p>Object detection has achieved substantial progress in the last decade.
However, detecting novel classes with only few samples remains challenging,
since deep learning under low data regime usually leads to a degraded feature
space. Existing works employ a holistic fine-tuning paradigm to tackle this
problem, where the model is first pre-trained on all base classes with abundant
samples, and then it is used to carve the novel class feature space.
Nonetheless, this paradigm is still imperfect. Durning fine-tuning, a novel
class may implicitly leverage the knowledge of multiple base classes to
construct its feature space, which induces a scattered feature space, hence
violating the inter-class separability. To overcome these obstacles, we propose
a two-step fine-tuning framework, Few-shot object detection via Association and
DIscrimination (FADI), which builds up a discriminative feature space for each
novel class with two integral steps. 1) In the association step, in contrast to
implicitly leveraging multiple base classes, we construct a compact novel class
feature space via explicitly imitating a specific base class feature space.
Specifically, we associate each novel class with a base class according to
their semantic similarity. After that, the feature space of a novel class can
readily imitate the well-trained feature space of the associated base class. 2)
In the discrimination step, to ensure the separability between the novel
classes and associated base classes, we disentangle the classification branches
for base and novel classes. To further enlarge the inter-class separability
between all classes, a set-specialized margin loss is imposed. Extensive
experiments on Pascal VOC and MS-COCO datasets demonstrate FADI achieves new
SOTA performance, significantly improving the baseline in any shot/split by
+18.7. Notably, the advantage is most announced on extremely few-shot
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling. (arXiv:2111.14819v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14819">
<div class="article-summary-box-inner">
<span><p>We present Point-BERT, a new paradigm for learning Transformers to generalize
the concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked
Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically,
we first divide a point cloud into several local point patches, and a point
cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to
generate discrete point tokens containing meaningful local information. Then,
we randomly mask out some patches of input point clouds and feed them into the
backbone Transformers. The pre-training objective is to recover the original
point tokens at the masked locations under the supervision of point tokens
obtained by the Tokenizer. Extensive experiments demonstrate that the proposed
BERT-style pre-training strategy significantly improves the performance of
standard point cloud Transformers. Equipped with our pre-training strategy, we
show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40
and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully
designed point cloud models with much fewer hand-made designs. We also
demonstrate that the representations learned by Point-BERT transfer well to new
tasks and domains, where our models largely advance the state-of-the-art of
few-shot point cloud classification task. The code and pre-trained models are
available at https://github.com/lulutang0608/Point-BERT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Flow Transformation Network for Deformable Image Registration with Region Consistency Constraint. (arXiv:2112.02249v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02249">
<div class="article-summary-box-inner">
<span><p>Deformable image registration is able to achieve fast and accurate alignment
between a pair of images and thus plays an important role in many medical image
studies. The current deep learning (DL)-based image registration approaches
directly learn the spatial transformation from one image to another by
leveraging a convolutional neural network, requiring ground truth or similarity
metric. Nevertheless, these methods only use a global similarity energy
function to evaluate the similarity of a pair of images, which ignores the
similarity of regions of interest (ROIs) within images. Moreover, DL-based
methods often estimate global spatial transformations of image directly, which
never pays attention to region spatial transformations of ROIs within images.
In this paper, we present a novel dual-flow transformation network with region
consistency constraint which maximizes the similarity of ROIs within a pair of
images and estimates both global and region spatial transformations
simultaneously. Experiments on four public 3D MRI datasets show that the
proposed method achieves the best registration performance in accuracy and
generalization compared with other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03860">
<div class="article-summary-box-inner">
<span><p>Deep generative models such as GANs and normalizing flows are powerful
priors. They can regularize inverse problems to reduce ill-posedness and attain
high-quality results. However, the latent vector of such deep generative models
can fall out of the desired high-dimensional standard Gaussian distribution
during an inversion, particularly in the presence of noise in data or
inaccurate forward models. In such a case, deep generative models are
ineffective in attaining high-fidelity solutions. To address this issue, we
propose to reparameterize and Gaussianize the latent vector using novel
differentiable data-dependent layers wherein custom operators are defined by
solving optimization problems. These proposed layers constrain an inversion to
find feasible in-distribution solutions. We tested and validated our technique
on three inversion tasks: compressive-sensing MRI, image deblurring, and
eikonal tomography (a nonlinear PDE-constrained inverse problem), using two
representative deep generative models: StyleGAN2 and Glow, and achieved
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shaping Visual Representations with Attributes for Few-Shot Recognition. (arXiv:2112.06398v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06398">
<div class="article-summary-box-inner">
<span><p>Few-shot recognition aims to recognize novel categories under low-data
regimes. Some recent few-shot recognition methods introduce auxiliary semantic
modality, i.e., category attribute information, into representation learning,
which enhances the feature discrimination and improves the recognition
performance. Most of these existing methods only consider the attribute
information of support set while ignoring the query set, resulting in a
potential loss of performance. In this letter, we propose a novel
attribute-shaped learning (ASL) framework, which can jointly perform query
attributes generation and discriminative visual representation learning for
few-shot recognition. Specifically, a visual-attribute predictor (VAP) is
constructed to predict the attributes of queries. By leveraging the attributes
information, an attribute-visual attention module (AVAM) is designed, which can
adaptively utilize attributes and visual representations to learn more
discriminative features. Under the guidance of attribute modality, our method
can learn enhanced semantic-aware representation for classification.
Experiments demonstrate that our method can achieve competitive results on CUB
and SUN benchmarks. Our source code is available at:
\url{https://github.com/chenhaoxing/ASL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-Aware Image Segmentation with Homotopy Warping. (arXiv:2112.07812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07812">
<div class="article-summary-box-inner">
<span><p>Besides per-pixel accuracy, topological correctness is also crucial for the
segmentation of images with fine-scale structures, e.g., satellite images and
biomedical images. In this paper, by leveraging the theory of digital topology,
we identify locations in an image that are critical for topology. By focusing
on these critical locations, we propose a new homotopy warping loss to train
deep image segmentation networks for better topological accuracy. To
efficiently identity these topologically critical locations, we propose a new
algorithm exploiting the distance transform. The proposed algorithm, as well as
the loss function, naturally generalize to different topological structures in
both 2D and 3D settings. The proposed loss function helps deep nets achieve
better performance in terms of topology-aware metrics, outperforming
state-of-the-art structure/topology-aware segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation Detection. (arXiv:2112.08935v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08935">
<div class="article-summary-box-inner">
<span><p>As manipulating images by copy-move, splicing and/or inpainting may lead to
misinterpretation of the visual content, detecting these sorts of manipulations
is crucial for media forensics. Given the variety of possible attacks on the
content, devising a generic method is nontrivial. Current deep learning based
methods are promising when training and test data are well aligned, but perform
poorly on independent tests. Moreover, due to the absence of authentic test
images, their image-level detection specificity is in doubt. The key question
is how to design and train a deep neural network capable of learning
generalizable features sensitive to manipulations in novel data, whilst
specific to prevent false alarms on the authentic. We propose multi-view
feature learning to jointly exploit tampering boundary artifacts and the noise
view of the input image. As both clues are meant to be semantic-agnostic, the
learned features are thus generalizable. For effectively learning from
authentic images, we train with multi-scale (pixel / edge / image) supervision.
We term the new network MVSS-Net and its enhanced version MVSS-Net++.
Experiments are conducted in both within-dataset and cross-dataset scenarios,
showing that MVSS-Net++ performs the best, and exhibits better robustness
against JPEG compression, Gaussian blur and screenshot based image
re-capturing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Referring Expression Grounding via Bidirectional Cross-Modal Matching. (arXiv:2201.06686v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06686">
<div class="article-summary-box-inner">
<span><p>Referring expression grounding is an important and challenging task in
computer vision. To avoid the laborious annotation in conventional referring
grounding, unpaired referring grounding is introduced, where the training data
only contains a number of images and queries without correspondences. The few
existing solutions to unpaired referring grounding are still preliminary, due
to the challenges of learning image-text matching and lack of the top-down
guidance with unpaired data. In this paper, we propose a novel bidirectional
cross-modal matching (BiCM) framework to address these challenges.
Particularly, we design a query-aware attention map (QAM) module that
introduces top-down perspective via generating query-specific visual attention
maps. A cross-modal object matching (COM) module is further introduced, which
exploits the recently emerged image-text matching pretrained model, CLIP, to
predict the target objects from a bottom-up perspective. The top-down and
bottom-up predictions are then integrated via a similarity funsion (SF) module.
We also propose a knowledge adaptation matching (KAM) module that leverages
unpaired training data to adapt pretrained knowledge to the target dataset and
task. Experiments show that our framework outperforms previous works by 6.55%
and 9.94% on two popular grounding datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Kernelized Dense Geometric Matching. (arXiv:2202.00667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00667">
<div class="article-summary-box-inner">
<span><p>Geometric matching is a challenging computer vision task that involves
finding correspondences between two views of a 3D scene. Dense geometric
matching, i.e., finding every matching pixel, is an appealing approach due to,
among other things, the capacity for sub-pixel accuracy and low-texture
robustness. While previous results have shown that sparse and semi-sparse
methods are better suited than dense approaches for geometry estimation, we
propose a novel dense method that outperforms them. We accomplish this by
formulating dense global matching as a probabilistic regression task using deep
kernels, in contrast to typical correlation volume processing. Furthermore, we
show that replacing local correlation with warped feature stacking in the
refinement stage further boosts performance. Finally, we observe that a
systematic attenuation of the model confidence improves geometry estimation
results. Our full approach, $\textbf{D}$eep $\textbf{K}$ernelized Dense
Geometric $\textbf{M}$atching, sets a new state-of-the-art on the competitive
HPatches, YFCC100m, MegaDepth-1500, and ScanNet-1500 geometry estimation
benchmarks. We provide code for all our experiments, instructions for
downloading datasets, and pretrained models, at https://github.com/Parskatt/dkm
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Student Dangerous Behavior Detection in School. (arXiv:2202.09550v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09550">
<div class="article-summary-box-inner">
<span><p>Video surveillance systems have been installed to ensure the student safety
in schools. However, discovering dangerous behaviors, such as fighting and
falling down, usually depends on untimely human observations. In this paper, we
focus on detecting dangerous behaviors of students automatically, which faces
numerous challenges, such as insufficient datasets, confusing postures,
keyframes detection and prompt response. To address these challenges, we first
build a danger behavior dataset with locations and labels from surveillance
videos, and transform action recognition of long videos to an object detection
task that avoids keyframes detection. Then, we propose a novel end-to-end
dangerous behavior detection method, named DangerDet, that combines multi-scale
body features and keypoints-based pose features. We could improve the accuracy
of behavior classification due to the highly correlation between pose and
behavior. On our dataset, DangerDet achieves 71.0\% mAP with about 11 FPS. It
keeps a better balance between the accuracy and time cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in Medical Image Analysis: A Review. (arXiv:2202.12165v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12165">
<div class="article-summary-box-inner">
<span><p>Transformers have dominated the field of natural language processing, and
recently impacted the computer vision area. In the field of medical image
analysis, Transformers have also been successfully applied to full-stack
clinical applications, including image synthesis/reconstruction, registration,
segmentation, detection, and diagnosis. Our paper aims to promote awareness and
application of Transformers in the field of medical image analysis.
Specifically, we first overview the core concepts of the attention mechanism
built into Transformers and other basic components. Second, we review various
Transformer architectures tailored for medical image applications and discuss
their limitations. Within this review, we investigate key challenges revolving
around the use of Transformers in different learning paradigms, improving the
model efficiency, and their coupling with other techniques. We hope this review
can give a comprehensive picture of Transformers to the readers in the field of
medical image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Point Cloud Representation Learning with Deep Neural Networks: A Survey. (arXiv:2202.13589v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13589">
<div class="article-summary-box-inner">
<span><p>Point cloud data have been widely explored due to its superior accuracy and
robustness under various adverse situations. Meanwhile, deep neural networks
(DNNs) have achieved very impressive success in various applications such as
surveillance and autonomous driving. The convergence of point cloud and DNNs
has led to many deep point cloud models, largely trained under the supervision
of large-scale and densely-labelled point cloud data. Unsupervised point cloud
representation learning, which aims to learn general and useful point cloud
representations from unlabelled point cloud data, has recently attracted
increasing attention due to the constraint in large-scale point cloud
labelling. This paper provides a comprehensive review of unsupervised point
cloud representation learning using DNNs. It first describes the motivation,
general pipelines as well as terminologies of the recent studies. Relevant
background including widely adopted point cloud datasets and DNN architectures
is then briefly presented. This is followed by an extensive discussion of
existing unsupervised point cloud representation learning methods according to
their technical approaches. We also quantitatively benchmark and discuss the
reviewed methods over multiple widely adopted point cloud datasets. Finally, we
share our humble opinion about several challenges and problems that could be
pursued in future research in unsupervised point cloud representation learning.
A project associated with this survey has been built at
https://github.com/xiaoaoran/3d_url_survey.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation. (arXiv:2203.01502v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01502">
<div class="article-summary-box-inner">
<span><p>Estimating the accurate depth from a single image is challenging since it is
inherently ambiguous and ill-posed. While recent works design increasingly
complicated and powerful networks to directly regress the depth map, we take
the path of CRFs optimization. Due to the expensive computation, CRFs are
usually performed between neighborhoods rather than the whole graph. To
leverage the potential of fully-connected CRFs, we split the input into windows
and perform the FC-CRFs optimization within each window, which reduces the
computation complexity and makes FC-CRFs feasible. To better capture the
relationships between nodes in the graph, we exploit the multi-head attention
mechanism to compute a multi-head potential function, which is fed to the
networks to output an optimized depth map. Then we build a bottom-up-top-down
structure, where this neural window FC-CRFs module serves as the decoder, and a
vision transformer serves as the encoder. The experiments demonstrate that our
method significantly improves the performance across all metrics on both the
KITTI and NYUv2 datasets, compared to previous methods. Furthermore, the
proposed method can be directly applied to panorama images and outperforms all
previous panorama methods on the MatterPort3D dataset. Project page:
https://weihaosky.github.io/newcrfs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Independent Depth Completion via Least Square Estimation. (arXiv:2203.03317v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03317">
<div class="article-summary-box-inner">
<span><p>The depth completion task aims to complete a per-pixel dense depth map from a
sparse depth map. In this paper, we propose an efficient least square based
depth-independent method to complete the sparse depth map utilizing the RGB
image and the sparse depth map in two independent stages. In this way can we
decouple the neural network and the sparse depth input, so that when some
features of the sparse depth map change, such as the sparsity, our method can
still produce a promising result. Moreover, due to the positional encoding and
linear procession in our pipeline, we can easily produce a super-resolution
dense depth map of high quality. We also test the generalization of our method
on different datasets compared to some state-of-the-art algorithms. Experiments
on the benchmark show that our method produces competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Cross-Domain Open World Recognition. (arXiv:2203.09257v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09257">
<div class="article-summary-box-inner">
<span><p>The ability to evolve is fundamental for any valuable autonomous agent whose
knowledge cannot remain limited to that injected by the manufacturer. Consider
for example a home assistant robot: it should be able to incrementally learn
new object categories when requested, but also to recognize the same objects in
different environments (rooms) and poses (hand-held/on the floor/above
furniture), while rejecting unknown ones. Despite its importance, this scenario
has started to raise interest in the robotic community only recently and the
related research is still in its infancy, with existing experimental testbeds
but no tailored methods. With this work, we propose the first learning approach
that deals with all the previously mentioned challenges at once by exploiting a
single contrastive objective. We show how it learns a feature space perfectly
suitable to incrementally include new classes and is able to capture knowledge
which generalizes across a variety of visual domains. Our method is endowed
with a tailored effective stopping criterion for each learning episode and
exploits a self-paced thresholding strategy that provides the classifier with a
reliable rejection option. Both these novel contributions are based on the
observation of the data statistics and do not need manual tuning. An extensive
experimental analysis confirms the effectiveness of the proposed approach in
establishing the new state-of-the-art. The code is available at
https://github.com/FrancescoCappio/Contrastive_Open_World.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Operator Sketching for Deep Unrolling Networks. (arXiv:2203.11156v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11156">
<div class="article-summary-box-inner">
<span><p>In this work we propose a new paradigm for designing efficient deep unrolling
networks using operator sketching. The deep unrolling networks are currently
the state-of-the-art solutions for imaging inverse problems. However, for
high-dimensional imaging tasks, especially the 3D cone-beam X-ray CT and 4D MRI
imaging, the deep unrolling schemes typically become inefficient both in terms
of memory and computation, due to the need of computing multiple times the
high-dimensional forward and adjoint operators. Recently researchers have found
that such limitations can be partially addressed by stochastic unrolling with
subsets of operators, inspired by the success of stochastic first-order
optimization. In this work, we propose a further acceleration upon stochastic
unrolling, using sketching techniques to approximate products in the
high-dimensional image space. The operator sketching can be jointly applied
with stochastic unrolling for the best acceleration and compression
performance. Our numerical experiments on X-ray CT image reconstruction
demonstrate the remarkable effectiveness of our sketched unrolling schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12667">
<div class="article-summary-box-inner">
<span><p>A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15118">
<div class="article-summary-box-inner">
<span><p>3D object detection is a central task for applications such as autonomous
driving, in which the system needs to localize and classify surrounding traffic
agents, even in the presence of adverse weather. In this paper, we address the
problem of LiDAR-based 3D object detection under snowfall. Due to the
difficulty of collecting and annotating training data in this setting, we
propose a physically based method to simulate the effect of snowfall on real
clear-weather LiDAR point clouds. Our method samples snow particles in 2D space
for each LiDAR line and uses the induced geometry to modify the measurement for
each LiDAR beam accordingly. Moreover, as snowfall often causes wetness on the
ground, we also simulate ground wetness on LiDAR point clouds. We use our
simulation to generate partially synthetic snowy LiDAR data and leverage these
data for training 3D object detection models that are robust to snowfall. We
conduct an extensive evaluation using several state-of-the-art 3D object
detection methods and show that our simulation consistently yields significant
performance gains on the real snowy STF dataset compared to clear-weather
baselines and competing simulation approaches, while not sacrificing
performance in clear weather. Our code is available at
www.github.com/SysCV/LiDAR_snow_sim.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16616">
<div class="article-summary-box-inner">
<span><p>Knowledge-based entity prediction (KEP) is a novel task that aims to improve
machine perception in autonomous systems. KEP leverages relational knowledge
from heterogeneous sources in predicting potentially unrecognized entities. In
this paper, we provide a formal definition of KEP as a knowledge completion
task. Three potential solutions are then introduced, which employ several
machine learning and data mining techniques. Finally, the applicability of KEP
is demonstrated on two autonomous systems from different domains; namely,
autonomous driving and smart manufacturing. We argue that in complex real-world
systems, the use of KEP would significantly improve machine perception while
pushing the current technology one step closer to achieving full autonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00833">
<div class="article-summary-box-inner">
<span><p>Pixel synthesis is a promising research paradigm for image generation, which
can well exploit pixel-wise prior knowledge for generation. However, existing
methods still suffer from excessive memory footprint and computation overhead.
In this paper, we propose a progressive pixel synthesis network towards
efficient image generation, coined as PixelFolder. Specifically, PixelFolder
formulates image generation as a progressive pixel regression problem and
synthesizes images by a multi-stage paradigm, which can greatly reduce the
overhead caused by large tensor transformations. In addition, we introduce
novel pixel folding operations to further improve model efficiency while
maintaining pixel-wise prior knowledge for end-to-end regression. With these
innovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,
reducing 90% computation and 57% parameters compared to the latest pixel
synthesis method called CIPS. To validate our approach, we conduct extensive
experiments on two benchmark datasets, namely FFHQ and LSUN Church. The
experimental results show that with much less expenditure, PixelFolder obtains
new state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77
FID and 2.45 FID on FFHQ and LSUN Church, respectively. Meanwhile, PixelFolder
is also more efficient than the SOTA methods like StyleGAN2, reducing about 74%
computation and 36% parameters, respectively. These results greatly validate
the effectiveness of the proposed PixelFolder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E^2TAD: An Energy-Efficient Tracking-based Action Detector. (arXiv:2204.04416v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04416">
<div class="article-summary-box-inner">
<span><p>Video action detection (spatio-temporal action localization) is usually the
starting point for human-centric intelligent analysis of videos nowadays. It
has high practical impacts for many applications across robotics, security,
healthcare, etc. The two-stage paradigm of Faster R-CNN inspires a standard
paradigm of video action detection in object detection, i.e., firstly
generating person proposals and then classifying their actions. However, none
of the existing solutions could provide fine-grained action detection to the
"who-when-where-what" level. This paper presents a tracking-based solution to
accurately and efficiently localize predefined key actions spatially (by
predicting the associated target IDs and locations) and temporally (by
predicting the time in exact frame indices). This solution won first place in
the UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09914">
<div class="article-summary-box-inner">
<span><p>LiDAR semantic segmentation essential for advanced autonomous driving is
required to be accurate, fast, and easy-deployed on mobile platforms. Previous
point-based or sparse voxel-based methods are far away from real-time
applications since time-consuming neighbor searching or sparse 3D convolution
are employed. Recent 2D projection-based methods, including range view and
multi-view fusion, can run in real time, but suffer from lower accuracy due to
information loss during the 2D projection. Besides, to improve the performance,
previous methods usually adopt test time augmentation (TTA), which further
slows down the inference process. To achieve a better speed-accuracy trade-off,
we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both
effectiveness and efficiency mainly by the following two techniques: 1) the
novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D
projected grid for efficiency, while summarizes both 2D and 3D features on 3D
point for minimal information loss; 2) the proposed transformation consistency
loss narrows the gap between the single-time model inference and TTA. The
experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the
CPGNet without ensemble models or TTA is comparable with the state-of-the-art
RPVNet, while it runs 4.7 times faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10962">
<div class="article-summary-box-inner">
<span><p>Visual attention helps achieve robust perception under noise, corruption, and
distribution shifts in human vision, which are areas where modern neural
networks still fall short. We present VARS, Visual Attention from Recurrent
Sparse reconstruction, a new attention formulation built on two prominent
features of the human visual attention mechanism: recurrency and sparsity.
Related features are grouped together via recurrent connections between
neurons, with salient objects emerging via sparse regularization. VARS adopts
an attractor network with recurrent connections that converges toward a stable
pattern over time. Network layers are represented as ordinary differential
equations (ODEs), formulating attention as a recurrent attractor network that
equivalently optimizes the sparse reconstruction of input using a dictionary of
"templates" encoding underlying patterns of data. We show that self-attention
is a special case of VARS with a single-step optimization and no sparsity
constraint. VARS can be readily used as a replacement for self-attention in
popular vision transformers, consistently improving their robustness across
various benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Efficient Backdoor Attacks. (arXiv:2204.12281v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12281">
<div class="article-summary-box-inner">
<span><p>Recent studies have proven that deep neural networks are vulnerable to
backdoor attacks. Specifically, by mixing a small number of poisoned samples
into the training set, the behavior of the trained model can be maliciously
controlled. Existing attack methods construct such adversaries by randomly
selecting some clean data from the benign set and then embedding a trigger into
them. However, this selection strategy ignores the fact that each poisoned
sample contributes inequally to the backdoor injection, which reduces the
efficiency of poisoning. In this paper, we formulate improving the poisoned
data efficiency by the selection as an optimization problem and propose a
Filtering-and-Updating Strategy (FUS) to solve it. The experimental results on
CIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the
same attack success rate can be achieved with only 47% to 75% of the poisoned
sample volume compared to the random selection strategy. More importantly, the
adversaries selected according to one setting can generalize well to other
settings, exhibiting strong transferability. The prototype code of our method
is now available at https://github.com/xpf/Data-Efficient-Backdoor-Attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Knowledge Distillation in Multi-attention Tasks: The Impact of a DCT-driven Loss. (arXiv:2205.01997v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01997">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is a strategy for the definition of a set of
transferability gangways to improve the efficiency of Convolutional Neural
Networks. Feature-based Knowledge Distillation is a subfield of KD that relies
on intermediate network representations, either unaltered or depth-reduced via
maximum activation maps, as the source knowledge. In this paper, we propose and
analyse the use of a 2D frequency transform of the activation maps before
transferring them. We pose that\textemdash by using global image cues rather
than pixel estimates, this strategy enhances knowledge transferability in tasks
such as scene recognition, defined by strong spatial and contextual
relationships between multiple and varied concepts. To validate the proposed
method, an extensive evaluation of the state-of-the-art in scene recognition is
presented. Experimental results provide strong evidences that the proposed
strategy enables the student network to better focus on the relevant image
areas learnt by the teacher network, hence leading to better descriptive
features and higher transferred performance than every other state-of-the-art
alternative. We publicly release the training and evaluation framework used
along this paper at
<a href="http://www-vpu.eps.uam.es/publications/DCTBasedKDForSceneRecognition.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Rubbing Restoration Using Generative Adversarial Networks. (arXiv:2205.03743v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03743">
<div class="article-summary-box-inner">
<span><p>Rubbing restorations are significant for preserving world cultural history.
In this paper, we propose the RubbingGAN model for restoring incomplete rubbing
characters. Specifically, we collect characters from the Zhang Menglong Bei and
build up the first rubbing restoration dataset. We design the first generative
adversarial network for rubbing restoration. Based on the dataset we collect,
we apply the RubbingGAN to learn the Zhang Menglong Bei font style and restore
the characters. The results of experiments show that RubbingGAN can repair both
slightly and severely incomplete rubbing characters fast and effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03860">
<div class="article-summary-box-inner">
<span><p>Vision-language pre-training (VLP) on large-scale datasets has shown premier
performance on various downstream tasks. A complete and fair benchmark (i.e.,
including large-scale pre-training datasets and diverse downstream tasks) is
essential for VLP. While there are plenty of benchmarks with English corpus,
building a rich benchmark for VLP with other languages, such as Chinese,
remains a critical problem. To this end, we build a large-scale Chinese
cross-modal benchmark called Zero for the research community to fairly compare
VLP models. We release two pre-training datasets and five fine-tuning datasets
for downstream tasks. Alongside, we propose a novel pre-training framework of
pre-Ranking + Ranking for cross-modal learning. Specifically, we apply global
contrastive pre-ranking to learn the individual representations of images and
texts, respectively. We then fuse the representations in a fine-grained ranking
manner via an image-text cross encoder and a text-image cross encoder. To
further enhance the capability of the model, we propose a two-way distillation
strategy consisting of target-guided Distillation and feature-guided
Distillation. For brevity, we name our model R2D2. We achieve state-of-the-art
performance on four public cross-modal datasets and the proposed five
downstream datasets. When conducting zero-shot tasks on Flickr30k-CN, COCO-CN,
and MUGE, R2D2 pre-trained on a 250 million dataset achieves significant
improvements of 4.7%, 5.4%, and 6.3% in mean recall compared to the
state-of-the-art. The datasets, models, and codes are available at
https://github.com/yuxie11/R2D2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Invariant Masked Autoencoders for Self-supervised Learning from Multi-domains. (arXiv:2205.04771v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04771">
<div class="article-summary-box-inner">
<span><p>Generalizing learned representations across significantly different visual
domains is a fundamental yet crucial ability of the human visual system. While
recent self-supervised learning methods have achieved good performances with
evaluation set on the same domain as the training set, they will have an
undesirable performance decrease when tested on a different domain. Therefore,
the self-supervised learning from multiple domains task is proposed to learn
domain-invariant features that are not only suitable for evaluation on the same
domain as the training set but also can be generalized to unseen domains. In
this paper, we propose a Domain-invariant Masked AutoEncoder (DiMAE) for
self-supervised learning from multi-domains, which designs a new pretext task,
\emph{i.e.,} the cross-domain reconstruction task, to learn domain-invariant
features. The core idea is to augment the input image with style noise from
different domains and then reconstruct the image from the embedding of the
augmented image, regularizing the encoder to learn domain-invariant features.
To accomplish the idea, DiMAE contains two critical designs, 1)
content-preserved style mix, which adds style information from other domains to
input while persevering the content in a parameter-free manner, and 2) multiple
domain-specific decoders, which recovers the corresponding domain style of
input to the encoded domain-invariant features for reconstruction. Experiments
on PACS and DomainNet illustrate that DiMAE achieves considerable gains
compared with recent state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10667">
<div class="article-summary-box-inner">
<span><p>Traditionally, extracting patterns from eye movement data relies on
statistics of different macro-events such as fixations and saccades. This
requires an additional preprocessing step to separate the eye movement
subtypes, often with a number of parameters on which the classification results
depend. Besides that, definitions of such macro events are formulated in
different ways by different researchers.
</p>
<p>We propose an application of a new class of features to the quantitative
analysis of personal eye movement trajectories structure. This new class of
features based on algebraic topology allows extracting patterns from different
modalities of gaze such as time series of coordinates and amplitudes, heatmaps,
and point clouds in a unified way at all scales from micro to macro. We
experimentally demonstrate the competitiveness of the new class of features
with the traditional ones and their significant synergy while being used
together for the person authentication task on the recently published eye
movement trajectories dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems. (arXiv:2205.12755v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12755">
<div class="article-summary-box-inner">
<span><p>Multitask learning assumes that models capable of learning from multiple
tasks can achieve better quality and efficiency via knowledge transfer, a key
feature of human learning. Though, state of the art ML models rely on high
customization for each task and leverage size and data scale rather than
scaling the number of tasks. Also, continual learning, that adds the temporal
aspect to multitask, is often focused to the study of common pitfalls such as
catastrophic forgetting instead of being studied at a large scale as a critical
component to build the next generation artificial intelligence. We propose an
evolutionary method that can generate a large scale multitask model, and can
support the dynamic and continuous addition of new tasks. The generated
multitask model is sparsely activated and integrates a task-based routing that
guarantees bounded compute cost and fewer added parameters per task as the
model expands. The proposed method relies on a knowledge compartmentalization
technique to achieve immunity against catastrophic forgetting and other common
pitfalls such as gradient interference and negative transfer. We empirically
show that the proposed method can jointly solve and achieve competitive results
on 69image classification tasks, for example achieving the best test accuracy
reported fora model trained only on public data for competitive tasks such as
cifar10: 99.43%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strengthening Skeletal Action Recognizers via Leveraging Temporal Patterns. (arXiv:2205.14405v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14405">
<div class="article-summary-box-inner">
<span><p>Skeleton sequences are compact and lightweight. Numerous skeleton-based
action recognizers have been proposed to classify human behaviors. In this
work, we aim to incorporate components that are compatible with existing models
and further improve their accuracy. To this end, we design two temporal
accessories: discrete cosine encoding (DCE) and chronological loss (CRL). DCE
facilitates models to analyze motion patterns from the frequency domain and
meanwhile alleviates the influence of signal noise. CRL guides networks to
explicitly capture the sequence's chronological order. These two components
consistently endow many recently-proposed action recognizers with accuracy
boosts, achieving new state-of-the-art (SOTA) accuracy on two large benchmark
datasets (NTU60 and NTU120).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guided Diffusion Model for Adversarial Purification. (arXiv:2205.14969v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14969">
<div class="article-summary-box-inner">
<span><p>With wider application of deep neural networks (DNNs) in various algorithms
and frameworks, security threats have become one of the concerns. Adversarial
attacks disturb DNN-based image classifiers, in which attackers can
intentionally add imperceptible adversarial perturbations on input images to
fool the classifiers. In this paper, we propose a novel purification approach,
referred to as guided diffusion model for purification (GDMP), to help protect
classifiers from adversarial attacks. The core of our approach is to embed
purification into the diffusion denoising process of a Denoised Diffusion
Probabilistic Model (DDPM), so that its diffusion process could submerge the
adversarial perturbations with gradually added Gaussian noises, and both of
these noises can be simultaneously removed following a guided denoising
process. On our comprehensive experiments across various datasets, the proposed
GDMP is shown to reduce the perturbations raised by adversarial attacks to a
shallow range, thereby significantly improving the correctness of
classification. GDMP improves the robust accuracy by 5%, obtaining 90.1% under
PGD attack on the CIFAR10 dataset. Moreover, GDMP achieves 70.94% robustness on
the challenging ImageNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes. (arXiv:2205.15723v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15723">
<div class="article-summary-box-inner">
<span><p>Modeling dynamic scenes is important for many applications such as virtual
reality and telepresence. Despite achieving unprecedented fidelity for novel
view synthesis in dynamic scenes, existing methods based on Neural Radiance
Fields (NeRF) suffer from slow convergence (i.e., model training time measured
in days). In this paper, we present DeVRF, a novel representation to accelerate
learning dynamic radiance fields. The core of DeVRF is to model both the 3D
canonical space and 4D deformation field of a dynamic, non-rigid scene with
explicit and discrete voxel-based representations. However, it is quite
challenging to train such a representation which has a large number of model
parameters, often resulting in overfitting issues. To overcome this challenge,
we devise a novel static-to-dynamic learning paradigm together with a new data
capture setup that is convenient to deploy in practice. This paradigm unlocks
efficient learning of deformable radiance fields via utilizing the 3D
volumetric canonical space learnt from multi-view static images to ease the
learning of 4D voxel deformation field with only few-view dynamic sequences. To
further improve the efficiency of our DeVRF and its synthesized novel view's
quality, we conduct thorough explorations and identify a set of strategies. We
evaluate DeVRF on both synthetic and real-world dynamic scenes with different
types of deformation. Experiments demonstrate that DeVRF achieves two orders of
magnitude speedup (100x faster) with on-par high-fidelity results compared to
the previous state-of-the-art approaches. The code and dataset will be released
in https://github.com/showlab/DeVRF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Labeling Where Adapting Fails: Cross-Domain Semantic Segmentation with Point Supervision via Active Selection. (arXiv:2206.00181v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00181">
<div class="article-summary-box-inner">
<span><p>Training models dedicated to semantic segmentation requires a large amount of
pixel-wise annotated data. Due to their costly nature, these annotations might
not be available for the task at hand. To alleviate this problem, unsupervised
domain adaptation approaches aim at aligning the feature distributions between
the labeled source and the unlabeled target data. While these strategies lead
to noticeable improvements, their effectiveness remains limited. To guide the
domain adaptation task more efficiently, previous works attempted to include
human interactions in this process under the form of sparse single-pixel
annotations in the target data. In this work, we propose a new domain
adaptation framework for semantic segmentation with annotated points via active
selection. First, we conduct an unsupervised domain adaptation of the model;
from this adaptation, we use an entropy-based uncertainty measurement for
target points selection. Finally, to minimize the domain gap, we propose a
domain adaptation framework utilizing these target points annotated by human
annotators. Experimental results on benchmark datasets show the effectiveness
of our methods against existing unsupervised domain adaptation approaches. The
propose pipeline is generic and can be included as an extra module to existing
domain adaptation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly detection in surveillance videos using transformer based attention model. (arXiv:2206.01524v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01524">
<div class="article-summary-box-inner">
<span><p>Surveillance footage can catch a wide range of realistic anomalies. This
research suggests using a weakly supervised strategy to avoid annotating
anomalous segments in training videos, which is time consuming. In this
approach only video level labels are used to obtain frame level anomaly scores.
Weakly supervised video anomaly detection (WSVAD) suffers from the wrong
identification of abnormal and normal instances during the training process.
Therefore it is important to extract better quality features from the available
videos. WIth this motivation, the present paper uses better quality
transformer-based features named Videoswin Features followed by the attention
layer based on dilated convolution and self attention to capture long and short
range dependencies in temporal domain. This gives us a better understanding of
available videos. The proposed framework is validated on real-world dataset
i.e. ShanghaiTech Campus dataset which results in competitive performance than
current state-of-the-art methods. The model and the code are available at
https://github.com/kapildeshpande/Anomaly-Detection-in-Surveillance-Videos
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01612">
<div class="article-summary-box-inner">
<span><p>We introduce OmniXAI, an open-source Python library of eXplainable AI (XAI),
which offers omni-way explainable AI capabilities and various interpretable
machine learning techniques to address the pain points of understanding and
interpreting the decisions made by machine learning (ML) in practice. OmniXAI
aims to be a one-stop comprehensive library that makes explainable AI easy for
data scientists, ML researchers and practitioners who need explanation for
various types of data, models and explanation methods at different stages of ML
process (data exploration, feature engineering, model development, evaluation,
and decision-making, etc). In particular, our library includes a rich family of
explanation methods integrated in a unified interface, which supports multiple
data types (tabular data, images, texts, time-series), multiple types of ML
models (traditional ML in Scikit-learn and deep learning models in
PyTorch/TensorFlow), and a range of diverse explanation methods including
"model-specific" and "model-agnostic" ones (such as feature-attribution
explanation, counterfactual explanation, gradient-based explanation, etc). For
practitioners, the library provides an easy-to-use unified interface to
generate the explanations for their applications by only writing a few lines of
codes, and also a GUI dashboard for visualization of different explanations for
more insights about decisions. In this technical report, we present OmniXAI's
design principles, system architectures, and major functionalities, and also
demonstrate several example use cases across different types of data, tasks,
and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v4 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06932">
<div class="article-summary-box-inner">
<span><p>Most polyp segmentation methods use CNNs as their backbone, leading to two
key issues when exchanging information between the encoder and decoder: 1)
taking into account the differences in contribution between different-level
features; and 2) designing an effective mechanism for fusing these features.
Different from existing CNN-based methods, we adopt a transformer encoder,
which learns more powerful and robust representations. In addition, considering
the image acquisition influence and elusive properties of polyps, we introduce
three novel modules, including a cascaded fusion module (CFM), a camouflage
identification module (CIM), a and similarity aggregation module (SAM). Among
these, the CFM is used to collect the semantic and location information of
polyps from high-level features, while the CIM is applied to capture polyp
information disguised in low-level features. With the help of the SAM, we
extend the pixel features of the polyp area with high-level semantic position
information to the entire polyp area, thereby effectively fusing cross-level
features. The proposed model, named Polyp-PVT, effectively suppresses noises in
the features and significantly improves their expressive capabilities.
Extensive experiments on five widely adopted datasets show that the proposed
model is more robust to various challenging situations (e.g., appearance
changes, small objects) than existing methods, and achieves the new
state-of-the-art performance. The proposed model is available at
https://github.com/DengPingFan/Polyp-PVT.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-07 23:07:53.171319048 UTC">2022-06-07 23:07:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>