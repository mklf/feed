<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-18T01:30:00Z">05-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ScAN: Suicide Attempt and Ideation Events Dataset. (arXiv:2205.07872v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07872">
<div class="article-summary-box-inner">
<span><p>Suicide is an important public health concern and one of the leading causes
of death worldwide. Suicidal behaviors, including suicide attempts (SA) and
suicide ideations (SI), are leading risk factors for death by suicide.
Information related to patients' previous and current SA and SI are frequently
documented in the electronic health record (EHR) notes. Accurate detection of
such documentation may help improve surveillance and predictions of patients'
suicidal behaviors and alert medical professionals for suicide prevention
efforts. In this study, we first built Suicide Attempt and Ideation Events
(ScAN) dataset, a subset of the publicly available MIMIC III dataset spanning
over 12k+ EHR notes with 19k+ annotated SA and SI events information. The
annotations also contain attributes such as method of suicide attempt. We also
provide a strong baseline model ScANER (Suicide Attempt and Ideation Events
Retriever), a multi-task RoBERTa-based model with a retrieval module to extract
all the relevant suicidal behavioral evidences from EHR notes of an
hospital-stay and, and a prediction module to identify the type of suicidal
behavior (SA and SI) concluded during the patient's stay at the hospital.
ScANER achieved a macro-weighted F1-score of 0.83 for identifying suicidal
behavioral evidences and a macro F1-score of 0.78 and 0.60 for classification
of SA and SI for the patient's hospital-stay, respectively. ScAN and ScANER are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Diversity of Argument-Making in the Wild: from Assumptions and Definitions to Causation and Anecdote in Reddit's "Change My View". (arXiv:2205.07938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07938">
<div class="article-summary-box-inner">
<span><p>What kinds of arguments do people make, and what effect do they have on
others? Normative constraints on argument-making are as old as philosophy
itself, but little is known about the diversity of arguments made in practice.
We use NLP tools to extract patterns of argument-making from the Reddit site
"Change My View" (r/CMV). This reveals six distinct argument patterns: not just
the familiar deductive and inductive forms, but also arguments about
definitions, relevance, possibility and cause, and personal experience. Data
from r/CMV also reveal differences in efficacy: personal experience and, to a
lesser extent, arguments about causation and examples, are most likely to shift
a person's view, while arguments about relevance are the least. Finally, our
methods reveal a gradient of argument-making preferences among users: a
two-axis model, of "personal--impersonal" and "concrete--abstract", can account
for nearly 80% of the strategy variance between individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta AI at Arabic Hate Speech 2022: MultiTask Learning with Self-Correction for Hate Speech Classification. (arXiv:2205.07960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07960">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the Arabic Fine-Grained Hate Speech Detection shared
task and demonstrate significant improvements over reported baselines for its
three subtasks. The tasks are to predict if a tweet contains (1) Offensive
language; and whether it is considered (2) Hate Speech or not and if so, then
predict the (3) Fine-Grained Hate Speech label from one of six categories. Our
final solution is an ensemble of models that employs multitask learning and a
self-consistency correction method yielding 82.7% on the hate speech subtask --
reflecting a 3.4% relative improvement compared to previous work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Budge programming language. (arXiv:2205.07979v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07979">
<div class="article-summary-box-inner">
<span><p>We present a simple, esoteric programming language based on G\"odel numbering
and prime factorization, enhanced with explicit, scoped loops, allowing for
easy program composition. We will show the syntax and semantics and then
provide a few example programs and their evaluation. We will also provide a few
interpreter implementations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Debiasing Translation Artifacts. (arXiv:2205.08001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08001">
<div class="article-summary-box-inner">
<span><p>Cross-lingual natural language processing relies on translation, either by
humans or machines, at different levels, from translating training data to
translating test sets. However, compared to original texts in the same
language, translations possess distinct qualities referred to as
translationese. Previous research has shown that these translation artifacts
influence the performance of a variety of cross-lingual tasks. In this work, we
propose a novel approach to reducing translationese by extending an established
bias-removal technique. We use the Iterative Null-space Projection (INLP)
algorithm, and show by measuring classification accuracy before and after
debiasing, that translationese is reduced at both sentence and word level. We
evaluate the utility of debiasing translationese on a natural language
inference (NLI) task, and show that by reducing this bias, NLI accuracy
improves. To the best of our knowledge, this is the first study to debias
translationese as represented in latent embedding space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CascadER: Cross-Modal Cascading for Knowledge Graph Link Prediction. (arXiv:2205.08012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08012">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) link prediction is a fundamental task in artificial
intelligence, with applications in natural language processing, information
retrieval, and biomedicine. Recently, promising results have been achieved by
leveraging cross-modal information in KGs, using ensembles that combine
knowledge graph embeddings (KGEs) and contextual language models (LMs).
However, existing ensembles are either (1) not consistently effective in terms
of ranking accuracy gains or (2) impractically inefficient on larger datasets
due to the combinatorial explosion problem of pairwise ranking with deep
language models. In this paper, we propose a novel tiered ranking architecture
CascadER to maintain the ranking accuracy of full ensembling while improving
efficiency considerably. CascadER uses LMs to rerank the outputs of more
efficient base KGEs, relying on an adaptive subset selection scheme aimed at
invoking the LMs minimally while maximizing accuracy gain over the KGE.
Extensive experiments demonstrate that CascadER improves MRR by up to 9 points
over KGE baselines, setting new state-of-the-art performance on four benchmarks
while improving efficiency by one or more orders of magnitude over competitive
cross-modal baselines. Our empirical analyses reveal that diversity of models
across modalities and preservation of individual models' confidence signals
help explain the effectiveness of CascadER, and suggest promising directions
for cross-modal cascaded architectures. Code and pretrained models are
available at https://github.com/tsafavi/cascader.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing Multilingual Resources to Question Answering in Arabic. (arXiv:2205.08024v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08024">
<div class="article-summary-box-inner">
<span><p>The goal of the paper is to predict answers to questions given a passage of
Qur'an. The answers are always found in the passage, so the task of the model
is to predict where an answer starts and where it ends. As the initial data set
is rather small for training, we make use of multilingual BERT so that we can
augment the training data by using data available for languages other than
Arabic. Furthermore, we crawl a large Arabic corpus that is domain specific to
religious discourse. Our approach consists of two steps, first we train a BERT
model to predict a set of possible answers in a passage. Finally, we use
another BERT based model to rank the candidate answers produced by the first
BERT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"What makes a question inquisitive?" A Study on Type-Controlled Inquisitive Question Generation. (arXiv:2205.08056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08056">
<div class="article-summary-box-inner">
<span><p>We propose a type-controlled framework for inquisitive question generation.
We annotate an inquisitive question dataset with question types, train question
type classifiers, and finetune models for type-controlled question generation.
Empirical results demonstrate that we can generate a variety of questions that
adhere to specific types while drawing from the source texts. We also
investigate strategies for selecting a single question from a generated set,
considering both an informative vs.~inquisitive question classifier and a
pairwise ranker trained from a small set of expert annotations. Question
selection using the pairwise ranker yields strong results in automatic and
manual evaluation. Our human evaluation assesses multiple aspects of the
generated questions, finding that the ranker chooses questions with the best
syntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5,
even rivaling the performance of human-written questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unbiased Math Word Problems Benchmark for Mitigating Solving Bias. (arXiv:2205.08108v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08108">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit the solving bias when evaluating models on current
Math Word Problem (MWP) benchmarks. However, current solvers exist solving bias
which consists of data bias and learning bias due to biased dataset and
improper training strategy. Our experiments verify MWP solvers are easy to be
biased by the biased training datasets which do not cover diverse questions for
each problem narrative of all MWPs, thus a solver can only learn shallow
heuristics rather than deep semantics for understanding problems. Besides, an
MWP can be naturally solved by multiple equivalent equations while current
datasets take only one of the equivalent equations as ground truth, forcing the
model to match the labeled ground truth and ignoring other equivalent
equations. Here, we first introduce a novel MWP dataset named UnbiasedMWP which
is constructed by varying the grounded expressions in our collected data and
annotating them with corresponding multiple new questions manually. Then, to
further mitigate learning bias, we propose a Dynamic Target Selection (DTS)
Strategy to dynamically select more suitable target expressions according to
the longest prefix match between the current model output and candidate
equivalent equations which are obtained by applying commutative law during
training. The results show that our UnbiasedMWP has significantly fewer biases
than its original data and other datasets, posing a promising benchmark for
fairly evaluating the solvers' reasoning skills rather than matching nearest
neighbors. And the solvers trained with our DTS achieve higher accuracies on
multiple MWP benchmarks. The source code is available at
https://github.com/yangzhch6/UnbiasedMWP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning. (arXiv:2205.08124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08124">
<div class="article-summary-box-inner">
<span><p>Transfer learning (TL) in natural language processing (NLP) has seen a surge
of interest in recent years, as pre-trained models have shown an impressive
ability to transfer to novel tasks. Three main strategies have emerged for
making use of multiple supervised datasets during fine-tuning: training on an
intermediate task before training on the target task (STILTs), using multi-task
learning (MTL) to train jointly on a supplementary task and the target task
(pairwise MTL), or simply using MTL to train jointly on all available datasets
(MTL-ALL). In this work, we compare all three TL methods in a comprehensive
analysis on the GLUE dataset suite. We find that there is a simple heuristic
for when to use one of these techniques over the other: pairwise MTL is better
than STILTs when the target task has fewer instances than the supporting task
and vice versa. We show that this holds true in more than 92% of applicable
cases on the GLUE dataset and validate this hypothesis with experiments varying
dataset size. The simplicity and effectiveness of this heuristic is surprising
and warrants additional exploration by the TL community. Furthermore, we find
that MTL-ALL is worse than the pairwise methods in almost every case. We hope
this study will aid others as they choose between TL methods for NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection. (arXiv:2205.08159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08159">
<div class="article-summary-box-inner">
<span><p>Fake News Detection (FND) is an essential field in natural language
processing that aims to identify and check the truthfulness of major claims in
a news article to decide the news veracity. FND finds its uses in preventing
social, political and national damage caused due to misrepresentation of facts
which may harm a certain section of society. Further, with the explosive rise
in fake news dissemination over social media, including images and text, it has
become imperative to identify fake news faster and more accurately. To solve
this problem, this work investigates a novel multimodal stacked ensemble-based
approach (SEMIFND) to fake news detection. Focus is also kept on ensuring
faster performance with fewer parameters. Moreover, to improve multimodal
performance, a deep unimodal analysis is done on the image modality to identify
NasNet Mobile as the most appropriate model for the task. For text, an ensemble
of BERT and ELECTRA is used. The approach was evaluated on two datasets:
Twitter MediaEval and Weibo Corpus. The suggested framework offered accuracies
of 85.80% and 86.83% on the Twitter and Weibo datasets respectively. These
reported metrics are found to be superior when compared to similar recent
works. Further, we also report a reduction in the number of parameters used in
training when compared to recent relevant works. SEMI-FND offers an overall
parameter reduction of at least 20% with unimodal parametric reduction on text
being 60%. Therefore, based on the investigations presented, it is concluded
that the application of a stacked ensembling significantly improves FND over
other approaches while also improving speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level Cross-Lingual Speech Representation. (arXiv:2205.08180v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08180">
<div class="article-summary-box-inner">
<span><p>We propose the SAMU-XLSR: Semantically-Aligned Multimodal Utterance-level
Cross-Lingual Speech Representation learning framework. Unlike previous works
on speech representation learning, which learns multilingual contextual speech
embedding at the resolution of an acoustic frame (10-20ms), this work focuses
on learning multimodal (speech-text) multilingual speech embedding at the
resolution of a sentence (5-10s) such that the embedding vector space is
semantically aligned across different languages. We combine state-of-the-art
multilingual acoustic frame-level speech representation learning model XLS-R
with the Language Agnostic BERT Sentence Embedding (LaBSE) model to create an
utterance-level multimodal multilingual speech encoder SAMU-XLSR. Although we
train SAMU-XLSR with only multilingual transcribed speech data, cross-lingual
speech-text and speech-speech associations emerge in its learned representation
space. To substantiate our claims, we use SAMU-XLSR speech encoder in
combination with a pre-trained LaBSE text sentence encoder for cross-lingual
speech-to-text translation retrieval, and SAMU-XLSR alone for cross-lingual
speech-to-speech translation retrieval. We highlight these applications by
performing several cross-lingual text and speech translation retrieval tasks
across several datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SKILL: Structured Knowledge Infusion for Large Language Models. (arXiv:2205.08184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08184">
<div class="article-summary-box-inner">
<span><p>Large language models (LLMs) have demonstrated human-level performance on a
vast spectrum of natural language tasks. However, it is largely unexplored
whether they can better internalize knowledge from a structured data, such as a
knowledge graph, or from text. In this work, we propose a method to infuse
structured knowledge into LLMs, by directly training T5 models on factual
triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata
KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as
well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The
models pre-trained on factual triples compare competitively with the ones on
natural language sentences that contain the same knowledge. Trained on a
smaller size KG, WikiMovies, we saw 3x improvement of exact match score on
MetaQA task compared to T5 baseline. The proposed method has an advantage that
no alignment between the knowledge graph and text corpus is required in
curating training data. This makes our method particularly useful when working
with industry-scale knowledge graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning. (arXiv:2205.08221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08221">
<div class="article-summary-box-inner">
<span><p>Sentence compression reduces the length of text by removing non-essential
content while preserving important facts and grammaticality. Unsupervised
objective driven methods for sentence compression can be used to create
customized models without the need for ground-truth training data, while
allowing flexibility in the objective function(s) that are used for learning
and inference. Recent unsupervised sentence compression approaches use custom
objectives to guide discrete search; however, guided search is expensive at
inference time. In this work, we explore the use of reinforcement learning to
train effective sentence compression models that are also fast when generating
predictions. In particular, we cast the task as binary sequence labelling and
fine-tune a pre-trained transformer using a simple policy gradient approach.
Our approach outperforms other unsupervised models while also being more
efficient at inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning. (arXiv:2205.08232v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08232">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning models have made great progress in MWP solving on
answer accuracy. However, they are uninterpretable since they mainly rely on
shallow heuristics to achieve high performance without understanding and
reasoning the grounded math logic. To address this issue and make a step
towards interpretable MWP solving, we first construct a high-quality MWP
dataset named InterMWP which consists of 11,495 MWPs and annotates
interpretable logical formulas based on algebraic knowledge as the grounded
linguistic logic of each solution equation. Different from existing MWP
datasets, our InterMWP benchmark asks for a solver to not only output the
solution expressions but also predict the corresponding logical formulas. We
further propose a novel approach with logical prompt and interpretation
generation, called LogicSolver. For each MWP, our LogicSolver first retrieves
some highly-correlated algebraic knowledge and then passes them to the backbone
model as prompts to improve the semantic representations of MWPs. With these
improved semantic representations, our LogicSolver generates corresponding
solution expressions and interpretable knowledge formulas in accord with the
generated solution expressions, simultaneously. Experimental results show that
our LogicSolver has stronger logical formula-based interpretability than
baselines while achieving higher answer accuracy with the help of logical
prompts, simultaneously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Letters From the Past: Modeling Historical Sound Change Through Diachronic Character Embeddings. (arXiv:2205.08256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08256">
<div class="article-summary-box-inner">
<span><p>While a great deal of work has been done on NLP approaches to lexical
semantic change detection, other aspects of language change have received less
attention from the NLP community. In this paper, we address the detection of
sound change through historical spelling. We propose that a sound change can be
captured by comparing the relative distance through time between their
distributions using PPMI character embeddings. We verify this hypothesis in
synthetic data and then test the method's ability to trace the well-known
historical change of lenition of plosives in Danish historical sources. We show
that the models are able to identify several of the changes under consideration
and to uncover meaningful contexts in which they appeared. The methodology has
the potential to contribute to the study of open questions such as the relative
chronology of sound shifts and their geographical distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Math Word Problems with Fine-to-Coarse Abstracting and Reasoning. (arXiv:2205.08274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08274">
<div class="article-summary-box-inner">
<span><p>Math Word Problems (MWP) is an important task that requires the ability of
understanding and reasoning over mathematical text. Existing approaches mostly
formalize it as a generation task by adopting Seq2Seq or Seq2Tree models to
encode an input math problem in natural language as a global representation and
generate the output mathematical expression. Such approaches only learn shallow
heuristics and fail to capture fine-grained variations in inputs. In this
paper, we propose to model a math word problem in a fine-to-coarse manner to
capture both the local fine-grained information and the global logical
structure of it. Instead of generating a complete equation sequence or
expression tree from the global features, we iteratively combine low-level
operands to predict a higher-level operator, abstracting the problem and
reasoning about the solving operators from bottom to up. Our model is naturally
more sensitive to local variations and can better generalize to unseen problem
types. Extensive evaluations on Math23k and SVAMP datasets demonstrate the
accuracy and robustness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers. (arXiv:2205.08288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08288">
<div class="article-summary-box-inner">
<span><p>Prior to deep learning the semantic parsing community has been interested in
understanding and modeling the range of possible word alignments between
natural language sentences and their corresponding meaning representations.
Sequence-to-sequence models changed the research landscape suggesting that we
no longer need to worry about alignments since they can be learned
automatically by means of an attention mechanism. More recently, researchers
have started to question such premise. In this work we investigate whether
seq2seq models can handle both simple and complex alignments. To answer this
question we augment the popular Geo semantic parsing dataset with alignment
annotations and create Geo-Aligned. We then study the performance of standard
seq2seq models on the examples that can be aligned monotonically versus
examples that require more complex alignments. Our empirical study shows that
performance is significantly better over monotonic alignments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias and Fairness on Multimodal Emotion Detection Algorithms. (arXiv:2205.08383v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08383">
<div class="article-summary-box-inner">
<span><p>Numerous studies have shown that machine learning algorithms can latch onto
protected attributes such as race and gender and generate predictions that
systematically discriminate against one or more groups. To date the majority of
bias and fairness research has been on unimodal models. In this work, we
explore the biases that exist in emotion recognition systems in relationship to
the modalities utilized, and study how multimodal approaches affect system bias
and fairness. We consider audio, text, and video modalities, as well as all
possible multimodal combinations of those, and find that text alone has the
least bias, and accounts for the majority of the models' performances, raising
doubts about the worthiness of multimodal emotion recognition systems when bias
and fairness are desired alongside model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation Framework for Legal Document Summarization. (arXiv:2205.08478v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08478">
<div class="article-summary-box-inner">
<span><p>A law practitioner has to go through numerous lengthy legal case proceedings
for their practices of various categories, such as land dispute, corruption,
etc. Hence, it is important to summarize these documents, and ensure that
summaries contain phrases with intent matching the category of the case. To the
best of our knowledge, there is no evaluation metric that evaluates a summary
based on its intent. We propose an automated intent-based summarization metric,
which shows a better agreement with human evaluation as compared to other
automated metrics like BLEU, ROUGE-L etc. in terms of human satisfaction. We
also curate a dataset by annotating intent phrases in legal documents, and show
a proof of concept as to how this system can be automated. Additionally, all
the code and data to generate reproducible results is available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Aggregation in Zero-Shot Cross-Lingual Transfer Using Multilingual BERT. (arXiv:2205.08497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08497">
<div class="article-summary-box-inner">
<span><p>Multilingual BERT (mBERT), a language model pre-trained on large multilingual
corpora, has impressive zero-shot cross-lingual transfer capabilities and
performs surprisingly well on zero-shot POS tagging and Named Entity
Recognition (NER), as well as on cross-lingual model transfer. At present, the
mainstream methods to solve the cross-lingual downstream tasks are always using
the last transformer layer's output of mBERT as the representation of
linguistic information. In this work, we explore the complementary property of
lower layers to the last transformer layer of mBERT. A feature aggregation
module based on an attention mechanism is proposed to fuse the information
contained in different layers of mBERT. The experiments are conducted on four
zero-shot cross-lingual transfer datasets, and the proposed method obtains
performance improvements on key multilingual benchmark tasks XNLI (+1.5 %),
PAWS-X (+2.4 %), NER (+1.2 F1), and POS (+1.5 F1). Through the analysis of the
experimental results, we prove that the layers before the last layer of mBERT
can provide extra useful information for cross-lingual downstream tasks and
explore the interpretability of mBERT empirically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovering Private Text in Federated Learning of Language Models. (arXiv:2205.08514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08514">
<div class="article-summary-box-inner">
<span><p>Federated learning allows distributed users to collaboratively train a model
while keeping each user's data private. Recently, a growing body of work has
demonstrated that an eavesdropping attacker can effectively recover image data
from gradients transmitted during federated learning. However, little progress
has been made in recovering text data. In this paper, we present a novel attack
method FILM for federated learning of language models -- for the first time, we
show the feasibility of recovering text from large batch sizes of up to 128
sentences. Different from image-recovery methods which are optimized to match
gradients, we take a distinct approach that first identifies a set of words
from gradients and then directly reconstructs sentences based on beam search
and a prior-based reordering strategy. The key insight of our attack is to
leverage either prior knowledge in pre-trained language models or memorization
during training. Despite its simplicity, we demonstrate that FILM can work well
with several large-scale datasets -- it can extract single sentences with high
fidelity even for large batch sizes and recover multiple sentences from the
batch successfully if the attack is applied iteratively. We hope our results
can motivate future work in developing stronger attacks as well as new defense
methods for training language models in federated learning. Our code is
publicly available at https://github.com/Princeton-SysML/FILM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Plagiarism in Introductory Programming Course Assignments. (arXiv:2205.08520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08520">
<div class="article-summary-box-inner">
<span><p>Measuring plagiarism in programming assignments is an essential task to the
educational procedure. This paper discusses the methods of plagiarism and its
detection in introductory programming course assignments written in C++. A
small corpus of assignments is made publically available. A general framework
to compute the similarity between a solution pair is developed that uses the
three token-based similarity methods as features and predicts if the solution
is plagiarized. The importance of each feature is also measured, which in
return ranks the effectiveness of each method in use. Finally, the artificially
generated dataset improves the results compared to the original data. We
achieved an F1 score of 0.955 and 0.971 on original and synthetic datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Human Evaluation of Machine Translation across Language Pairs. (arXiv:2205.08533v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08533">
<div class="article-summary-box-inner">
<span><p>Obtaining meaningful quality scores for machine translation systems through
human evaluation remains a challenge given the high variability between human
evaluators, partly due to subjective expectations for translation quality for
different language pairs. We propose a new metric called XSTS that is more
focused on semantic equivalence and a cross-lingual calibration method that
enables more consistent assessment. We demonstrate the effectiveness of these
novel contributions in large scale evaluation studies across up to 14 language
pairs, with translation both into and out of English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11225">
<div class="article-summary-box-inner">
<span><p>Label noise and long-tailed distributions are two major challenges in
distantly supervised relation extraction. Recent studies have shown great
progress on denoising, but paid little attention to the problem of long-tailed
relations. In this paper, we introduce a constraint graph to model the
dependencies between relation labels. On top of that, we further propose a
novel constraint graph-based relation extraction framework(CGRE) to handle the
two challenges simultaneously. CGRE employs graph convolution networks to
propagate information from data-rich relation nodes to data-poor relation
nodes, and thus boosts the representation learning of long-tailed relations. To
further improve the noise immunity, a constraint-aware attention module is
designed in CGRE to integrate the constraint information. Extensive
experimental results indicate that CGRE achieves significant improvements over
the previous methods for both denoising and long-tailed relation extraction.
The pre-processed datasets and source code are publicly available at
https://github.com/tmliang/CGRE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KGAP: Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03861">
<div class="article-summary-box-inner">
<span><p>Identifying political perspectives in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
political ideologies. Previous approaches focus on textual content and leave
out the rich social and political context that is essential in the perspective
detection process. To address this limitation, we propose KGAP, a political
perspective detection method that incorporates external domain knowledge.
Specifically, we construct a political knowledge graph to serve as
domain-specific external knowledge. We then construct heterogeneous information
networks to represent news documents, which jointly model news text and
external knowledge. Finally, we adopt relational graph neural networks and
conduct political perspective detection as graph-level classification.
Extensive experiments demonstrate that our method consistently achieves the
best performance on two real-world perspective detection benchmarks. Ablation
studies further bear out the necessity of external knowledge and the
effectiveness of our graph-based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06838">
<div class="article-summary-box-inner">
<span><p>While large language models have shown exciting progress on several NLP
benchmarks, evaluating their ability for complex analogical reasoning remains
under-explored. Here, we introduce a high-quality crowdsourced dataset of
narratives for employing proverbs in context as a benchmark for abstract
language understanding. The dataset provides fine-grained annotation of aligned
spans between proverbs and narratives, and contains minimal lexical overlaps
between narratives and proverbs, ensuring that models need to go beyond
surface-level reasoning to succeed. We explore three tasks: (1) proverb
recommendation and alignment prediction, (2) narrative generation for a given
proverb and topic, and (3) identifying narratives with similar motifs. Our
experiments show that neural language models struggle on these tasks compared
to humans, and these tasks pose multiple learning challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10341">
<div class="article-summary-box-inner">
<span><p>Document-level neural machine translation (DocNMT) achieves coherent
translations by incorporating cross-sentence context. However, for most
language pairs there's a shortage of parallel documents, although parallel
sentences are readily available. In this paper, we study whether and how
contextual modeling in DocNMT is transferable via multilingual modeling. We
focus on the scenario of zero-shot transfer from teacher languages with
document level data to student languages with no documents but sentence level
data, and for the first time treat document-level translation as a transfer
learning problem. Using simple concatenation-based DocNMT, we explore the
effect of 3 factors on the transfer: the number of teacher languages with
document level data, the balance between document and sentence level data at
training, and the data condition of parallel documents (genuine vs.
backtranslated). Our experiments on Europarl-7 and IWSLT-10 show the
feasibility of multilingual transfer for DocNMT, particularly on
document-specific metrics. We observe that more teacher languages and adequate
data balance both contribute to better transfer quality. Surprisingly, the
transfer is less sensitive to the data condition, where multilingual DocNMT
delivers decent performance with either backtranslated or genuine document
pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Retrieve Passages without Supervision. (arXiv:2112.07708v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07708">
<div class="article-summary-box-inner">
<span><p>Dense retrievers for open-domain question answering (ODQA) have been shown to
achieve impressive performance by training on large datasets of
question-passage pairs. In this work we ask whether this dependence on labeled
data can be reduced via unsupervised pretraining that is geared towards ODQA.
We show this is in fact possible, via a novel pretraining scheme designed for
retrieval. Our "recurring span retrieval" approach uses recurring spans across
passages in a document to create pseudo examples for contrastive learning. Our
pretraining scheme directly controls for term overlap across pseudo queries and
relevant passages, thus allowing to model both lexical and semantic relations
between them. The resulting model, named Spider, performs surprisingly well
without any labeled training examples on a wide range of ODQA datasets.
Specifically, it significantly outperforms all other pretrained baselines in a
zero-shot setting, and is competitive with BM25, a strong sparse baseline.
Moreover, a hybrid retriever over Spider and BM25 improves over both, and is
often competitive with DPR models, which are trained on tens of thousands of
examples. Last, notable gains are observed when using Spider as an
initialization for supervised training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants. (arXiv:2112.09062v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09062">
<div class="article-summary-box-inner">
<span><p>In Dynamic Adversarial Data Collection (DADC), human annotators are tasked
with finding examples that models struggle to predict correctly. Models trained
on DADC-collected training data have been shown to be more robust in
adversarial and out-of-domain settings, and are considerably harder for humans
to fool. However, DADC is more time-consuming than traditional data collection
and thus more costly per annotated example. In this work, we examine whether we
can maintain the advantages of DADC, without incurring the additional cost. To
that end, we introduce Generative Annotation Assistants (GAAs),
generator-in-the-loop models that provide real-time suggestions that annotators
can either approve, modify, or reject entirely. We collect training datasets in
twenty experimental settings and perform a detailed analysis of this approach
for the task of extractive question answering (QA) for both standard and
adversarial data collection. We demonstrate that GAAs provide significant
efficiency benefits with over a 30% annotation speed-up, while leading to over
a 5x improvement in model fooling rates. In addition, we find that using
GAA-assisted training data leads to higher downstream model performance on a
variety of question answering tasks over adversarial data collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black-Box Tuning for Language-Model-as-a-Service. (arXiv:2201.03514v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03514">
<div class="article-summary-box-inner">
<span><p>Extremely large pre-trained language models (PTMs) such as GPT-3 are usually
released as a service. It allows users to design task-specific prompts to query
the PTMs through some black-box APIs. In such a scenario, which we call
Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually
unavailable. Can we optimize the task prompts by only accessing the model
inference APIs? This paper proposes the black-box tuning framework to optimize
the continuous prompt prepended to the input text via derivative-free
optimization. Instead of optimizing in the original high-dimensional prompt
space, which is intractable for traditional derivative-free optimization, we
perform optimization in a randomly generated subspace due to the low intrinsic
dimensionality of large PTMs. The experimental results show that the black-box
tuning with RoBERTa on a few labeled samples not only significantly outperforms
manual prompt and GPT-3's in-context learning, but also surpasses the
gradient-based counterparts, i.e., prompt tuning and full model tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models. (arXiv:2202.13392v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13392">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) cannot well recall rich factual knowledge
of entities exhibited in large-scale corpora, especially those rare entities.
In this paper, we propose to build a simple but effective Pluggable Entity
Lookup Table (PELT) on demand by aggregating the entity's output
representations of multiple occurrences in the corpora. PELT can be compatibly
plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared
to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation
with capability of acquiring knowledge from out-of-domain corpora for domain
adaptation scenario. The experiments on knowledge-related tasks demonstrate
that our method, PELT, can flexibly and effectively transfer entity knowledge
from related corpora into PLMs with different architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagonal State Spaces are as Effective as Structured State Spaces. (arXiv:2203.14343v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14343">
<div class="article-summary-box-inner">
<span><p>Modeling long range dependencies in sequential data is a fundamental step
towards attaining human-level performance in many modalities such as text,
vision, audio and video. While attention-based models are a popular and
effective choice in modeling short-range interactions, their performance on
tasks requiring long range reasoning has been largely inadequate. In an
exciting result, Gu et al. (ICLR 2022) proposed the $\textit{Structured State
Space}$ (S4) architecture delivering large gains over state-of-the-art models
on several long-range tasks across various modalities. The core proposition of
S4 is the parameterization of state matrices via a diagonal plus low rank
structure, allowing efficient computation. In this work, we show that one can
match the performance of S4 even without the low rank correction and thus
assuming the state matrices to be diagonal. Our $\textit{Diagonal State Space}$
(DSS) model matches the performance of S4 on Long Range Arena tasks, speech
classification on Speech Commands dataset, while being conceptually simpler and
straightforward to implement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Gender Debiasing Affects Internal Model Representations, and Why It Matters. (arXiv:2204.06827v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06827">
<div class="article-summary-box-inner">
<span><p>Common studies of gender bias in NLP focus either on extrinsic bias measured
by model performance on a downstream task or on intrinsic bias found in models'
internal representations. However, the relationship between extrinsic and
intrinsic bias is relatively unknown. In this work, we illuminate this
relationship by measuring both quantities together: we debias a model during
downstream fine-tuning, which reduces extrinsic bias, and measure the effect on
intrinsic bias, which is operationalized as bias extractability with
information-theoretic probing. Through experiments on two tasks and multiple
bias metrics, we show that our intrinsic bias metric is a better indicator of
debiasing than (a contextual adaptation of) the standard WEAT metric, and can
also expose cases of superficial debiasing. Our framework provides a
comprehensive perspective on bias in NLP models, which can be applied to deploy
NLP systems in a more informed manner. Our code and model checkpoints are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Few-Shot Learning with FASL. (arXiv:2204.09347v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09347">
<div class="article-summary-box-inner">
<span><p>Recent advances in natural language processing (NLP) have led to strong text
classification models for many tasks. However, still often thousands of
examples are needed to train models with good quality. This makes it
challenging to quickly develop and deploy new models for real world problems
and business needs. Few-shot learning and active learning are two lines of
research, aimed at tackling this problem. In this work, we combine both lines
into FASL, a platform that allows training text classification models using an
iterative and fast process. We investigate which active learning methods work
best in our few-shot setup. Additionally, we develop a model to predict when to
stop annotating. This is relevant as in a few-shot setup we do not have access
to a large validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero and Few-shot Learning for Author Profiling. (arXiv:2204.10543v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10543">
<div class="article-summary-box-inner">
<span><p>Author profiling classifies author characteristics by analyzing how language
is shared among people. In this work, we study that task from a low-resource
viewpoint: using little or no training data. We explore different zero and
few-shot models based on entailment and evaluate our systems on several
profiling tasks in Spanish and English. In addition, we study the effect of
both the entailment hypothesis and the size of the few-shot training sample. We
find that entailment-based models out-perform supervised text classifiers based
on roberta-XLM and that we can reach 80% of the accuracy of previous approaches
using less than 50\% of the training data on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data. (arXiv:2205.07557v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07557">
<div class="article-summary-box-inner">
<span><p>This paper shows how to use large-scale pre-trained language models to
extract character roles from narrative texts without training data. Queried
with a zero-shot question-answering prompt, GPT-3 can identify the hero,
villain, and victim in diverse domains: newspaper articles, movie plot
summaries, and political speeches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CQR-SQL: Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers. (arXiv:2205.07686v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07686">
<div class="article-summary-box-inner">
<span><p>Context-dependent text-to-SQL is the task of translating multi-turn questions
into database-related SQL queries. Existing methods typically focus on making
full use of history context or previously predicted SQL for currently SQL
parsing, while neglecting to explicitly comprehend the schema and
conversational dependency, such as co-reference, ellipsis and user focus
change. In this paper, we propose CQR-SQL, which uses auxiliary Conversational
Question Reformulation (CQR) learning to explicitly exploit schema and decouple
contextual dependency for SQL parsing. Specifically, we first present a schema
enhanced recursive CQR method to produce domain-relevant self-contained
questions. Secondly, we train CQR-SQL models to map the semantics of multi-turn
questions and auxiliary self-contained questions into the same latent space
through schema grounding consistency task and tree-structured SQL parsing
consistency task, which enhances the abilities of SQL parsing by adequately
contextual understanding. At the time of writing, our CQR-SQL achieves new
state-of-the-art results on two context-dependent text-to-SQL benchmarks SParC
and CoSQL.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Functional2Structural: Cross-Modality Brain Networks Representation Learning. (arXiv:2205.07854v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07854">
<div class="article-summary-box-inner">
<span><p>MRI-based modeling of brain networks has been widely used to understand
functional and structural interactions and connections among brain regions, and
factors that affect them, such as brain development and disease. Graph mining
on brain networks may facilitate the discovery of novel biomarkers for clinical
phenotypes and neurodegenerative diseases. Since brain networks derived from
functional and structural MRI describe the brain topology from different
perspectives, exploring a representation that combines these cross-modality
brain networks is non-trivial. Most current studies aim to extract a fused
representation of the two types of brain network by projecting the structural
network to the functional counterpart. Since the functional network is dynamic
and the structural network is static, mapping a static object to a dynamic
object is suboptimal. However, mapping in the opposite direction is not
feasible due to the non-negativity requirement of current graph learning
techniques. Here, we propose a novel graph learning framework, known as Deep
Signed Brain Networks (DSBN), with a signed graph encoder that, from an
opposite perspective, learns the cross-modality representations by projecting
the functional network to the structural counterpart. We validate our framework
on clinical phenotype and neurodegenerative disease prediction tasks using two
independent, publicly available datasets (HCP and OASIS). The experimental
results clearly demonstrate the advantages of our model compared to several
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Enhancement for Cloud-Based Few-Shot Learning. (arXiv:2205.07864v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07864">
<div class="article-summary-box-inner">
<span><p>Requiring less data for accurate models, few-shot learning has shown
robustness and generality in many application domains. However, deploying
few-shot models in untrusted environments may inflict privacy concerns, e.g.,
attacks or adversaries that may breach the privacy of user-supplied data. This
paper studies the privacy enhancement for the few-shot learning in an untrusted
environment, e.g., the cloud, by establishing a novel privacy-preserved
embedding space that preserves the privacy of data and maintains the accuracy
of the model. We examine the impact of various image privacy methods such as
blurring, pixelization, Gaussian noise, and differentially private pixelization
(DP-Pix) on few-shot image classification and propose a method that learns
privacy-preserved representation through the joint loss. The empirical results
show how privacy-performance trade-off can be negotiated for privacy-enhanced
few-shot learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Primal-Dual UNet for Sparse View Cone Beam Computed Tomography Volume Reconstruction. (arXiv:2205.07866v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07866">
<div class="article-summary-box-inner">
<span><p>In this paper, the Primal-Dual UNet for sparse view CT reconstruction is
modified to be applicable to cone beam projections and perform reconstructions
of entire volumes instead of slices. Experiments show that the PSNR of the
proposed method is increased by 10dB compared to the direct FDK reconstruction
and almost 3dB compared to the modified original Primal-Dual Network when using
only 23 projections. The presented network is not optimized wrt. memory
consumption or hyperparameters but merely serves as a proof of concept and is
limited to low resolution projections and volumes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Updates of a Pre-trained Model for Few-shot Learning. (arXiv:2205.07874v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07874">
<div class="article-summary-box-inner">
<span><p>Most of the recent few-shot learning algorithms are based on transfer
learning, where a model is pre-trained using a large amount of source data, and
the pre-trained model is updated using a small amount of target data afterward.
In transfer-based few-shot learning, sophisticated pre-training methods have
been widely studied for universal and improved representation. However, there
is little study on updating pre-trained models for few-shot learning. In this
paper, we compare the two popular updating methods, fine-tuning (i.e., updating
the entire network) and linear probing (i.e., updating only the linear
classifier), considering the distribution shift between the source and target
data. We find that fine-tuning is better than linear probing as the number of
samples increases, regardless of distribution shift. Next, we investigate the
effectiveness and ineffectiveness of data augmentation when pre-trained models
are fine-tuned. Our fundamental analyses demonstrate that careful
considerations of the details about updating pre-trained models are required
for better few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Driven Interpolation for Super-Scarce X-Ray Computed Tomography. (arXiv:2205.07888v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07888">
<div class="article-summary-box-inner">
<span><p>We address the problem of reconstructing X-Ray tomographic images from scarce
measurements by interpolating missing acquisitions using a self-supervised
approach. To do so, we train shallow neural networks to combine two
neighbouring acquisitions into an estimated measurement at an intermediate
angle. This procedure yields an enhanced sequence of measurements that can be
reconstructed using standard methods, or further enhanced using regularisation
approaches.
</p>
<p>Unlike methods that improve the sequence of acquisitions using an initial
deterministic interpolation followed by machine-learning enhancement, we focus
on inferring one measurement at once. This allows the method to scale to 3D,
the computation to be faster and crucially, the interpolation to be
significantly better than the current methods, when they exist. We also
establish that a sequence of measurements must be processed as such, rather
than as an image or a volume. We do so by comparing interpolation and
up-sampling methods, and find that the latter significantly under-perform.
</p>
<p>We compare the performance of the proposed method against deterministic
interpolation and up-sampling procedures and find that it outperforms them,
even when used jointly with a state-of-the-art projection-data enhancement
approach using machine-learning. These results are obtained for 2D and 3D
imaging, on large biomedical datasets, in both projection space and image
space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Apprenticeship Learning for Playing Games. (arXiv:2205.07959v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07959">
<div class="article-summary-box-inner">
<span><p>In the last decade, deep learning has achieved great success in machine
learning tasks where the input data is represented with different levels of
abstractions. Driven by the recent research in reinforcement learning using
deep neural networks, we explore the feasibility of designing a learning model
based on expert behaviour for complex, multidimensional tasks where reward
function is not available. We propose a novel method for apprenticeship
learning based on the previous research on supervised learning techniques in
reinforcement learning. Our method is applied to video frames from Atari games
in order to teach an artificial agent to play those games. Even though the
reported results are not comparable with the state-of-the-art results in
reinforcement learning, we demonstrate that such an approach has the potential
to achieve strong performance in the future and is worthwhile for further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Visual Counterfactual Explanations in Image Space. (arXiv:2205.07972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07972">
<div class="article-summary-box-inner">
<span><p>Visual counterfactual explanations (VCEs) in image space are an important
tool to understand decisions of image classifiers as they show under which
changes of the image the decision of the classifier would change. Their
generation in image space is challenging and requires robust models due to the
problem of adversarial examples. Existing techniques to generate VCEs in image
space suffer from spurious changes in the background. Our novel perturbation
model for VCEs together with its efficient optimization via our novel
Auto-Frank-Wolfe scheme yields sparse VCEs which are significantly more
object-centric. Moreover, we show that VCEs can be used to detect undesired
behavior of ImageNet classifiers due to spurious features in the ImageNet
dataset and discuss how estimates of the data-generating distribution can be
used for VCEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TOCH: Spatio-Temporal Object Correspondence to Hand for Motion Refinement. (arXiv:2205.07982v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07982">
<div class="article-summary-box-inner">
<span><p>We present TOCH, a method for refining incorrect 3D hand-object interaction
sequences using a data prior. Existing hand trackers, especially those that
rely on very few cameras, often produce visually unrealistic results with
hand-object intersection or missing contacts. Although correcting such errors
requires reasoning about temporal aspects of interaction, most previous work
focus on static grasps and contacts. The core of our method are TOCH fields, a
novel spatio-temporal representation for modeling correspondences between hands
and objects during interaction. The key component is a point-wise
object-centric representation which encodes the hand position relative to the
object. Leveraging this novel representation, we learn a latent manifold of
plausible TOCH fields with a temporal denoising auto-encoder. Experiments
demonstrate that TOCH outperforms state-of-the-art (SOTA) 3D hand-object
interaction models, which are limited to static grasps and contacts. More
importantly, our method produces smooth interactions even before and after
contact. Using a single trained TOCH model, we quantitatively and qualitatively
demonstrate its usefulness for 1) correcting erroneous reconstruction results
from off-the-shelf RGB/RGB-D hand-object reconstruction methods, 2) de-noising,
and 3) grasp transfer across objects. We will release our code and trained
model on our project page at <a href="http://virtualhumans.mpi-inf.mpg.de/toch/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-Time Adaptation with Shape Moments for Image Segmentation. (arXiv:2205.07983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07983">
<div class="article-summary-box-inner">
<span><p>Supervised learning is well-known to fail at generalization under
distribution shifts. In typical clinical settings, the source data is
inaccessible and the target distribution is represented with a handful of
samples: adaptation can only happen at test time on a few or even a single
subject(s). We investigate test-time single-subject adaptation for
segmentation, and propose a Shape-guided Entropy Minimization objective for
tackling this task. During inference for a single testing subject, our loss is
minimized with respect to the batch normalization's scale and bias parameters.
We show the potential of integrating various shape priors to guide adaptation
to plausible solutions, and validate our method in two challenging scenarios:
MRI-to-CT adaptation of cardiac segmentation and cross-site adaptation of
prostate segmentation. Our approach exhibits substantially better performances
than the existing test-time adaptation methods. Even more surprisingly, it
fares better than state-of-the-art domain adaptation methods, although it
forgoes training on additional target data during adaptation. Our results
question the usefulness of training on target data in segmentation adaptation,
and points to the substantial effect of shape priors on test-time inference.
Our framework can be readily used for integrating various priors and for
adapting any segmentation network, and our code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lost in Compression: the Impact of Lossy Image Compression on Variable Size Object Detection within Infrared Imagery. (arXiv:2205.08002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08002">
<div class="article-summary-box-inner">
<span><p>Lossy image compression strategies allow for more efficient storage and
transmission of data by encoding data to a reduced form. This is essential
enable training with larger datasets on less storage-equipped environments.
However, such compression can cause severe decline in performance of deep
Convolution Neural Network (CNN) architectures even when mild compression is
applied and the resulting compressed imagery is visually identical. In this
work, we apply the lossy JPEG compression method with six discrete levels of
increasing compression {95, 75, 50, 15, 10, 5} to infrared band (thermal)
imagery. Our study quantitatively evaluates the affect that increasing levels
of lossy compression has upon the performance of characteristically diverse
object detection architectures (Cascade-RCNN, FSAF and Deformable DETR) with
respect to varying sizes of objects present in the dataset. When training and
evaluating on uncompressed data as a baseline, we achieve maximal mean Average
Precision (mAP) of 0.823 with Cascade R-CNN across the FLIR dataset,
outperforming prior work. The impact of the lossy compression is more extreme
at higher compression levels (15, 10, 5) across all three CNN architectures.
However, re-training models on lossy compressed imagery notably ameliorated
performances for all three CNN models with an average increment of ~76% (at
higher compression level 5). Additionally, we demonstrate the relative
sensitivity of differing object areas {tiny, small, medium, large} with respect
to the compression level. We show that tiny and small objects are more
sensitive to compression than medium and large objects. Overall, Cascade R-CNN
attains the maximal mAP across most of the object area categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning on 3D point clouds with random compressed rehearsal. (arXiv:2205.08013v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08013">
<div class="article-summary-box-inner">
<span><p>Contemporary deep neural networks offer state-of-the-art results when applied
to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds
are important datatype for precise modeling of three-dimensional environments,
but effective processing of this type of data proves to be challenging. In the
world of large, heavily-parameterized network architectures and
continuously-streamed data, there is an increasing need for machine learning
models that can be trained on additional data. Unfortunately, currently
available models cannot fully leverage training on additional data without
losing their past knowledge. Combating this phenomenon, called catastrophic
forgetting, is one of the main objectives of continual learning. Continual
learning for deep neural networks has been an active field of research,
primarily in 2D computer vision, natural language processing, reinforcement
learning, and robotics. However, in 3D computer vision, there are hardly any
continual learning solutions specifically designed to take advantage of point
cloud structure. This work proposes a novel neural network architecture capable
of continual learning on 3D point cloud data. We utilize point cloud structure
properties for preserving a heavily compressed set of past data. By using
rehearsal and reconstruction as regularization methods of the learning process,
our approach achieves a significant decrease of catastrophic forgetting
compared to the existing solutions on several most popular point cloud datasets
considering two continual learning settings: when a task is known beforehand,
and in the challenging scenario of when task information is unknown to the
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection and Physical Interaction with Deformable Linear Objects. (arXiv:2205.08041v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08041">
<div class="article-summary-box-inner">
<span><p>Deformable linear objects (e.g., cables, ropes, and threads) commonly appear
in our everyday lives. However, perception of these objects and the study of
physical interaction with them is still a growing area. There have already been
successful methods to model and track deformable linear objects. However, the
number of methods that can automatically extract the initial conditions in
non-trivial situations for these methods has been limited, and they have been
introduced to the community only recently. On the other hand, while physical
interaction with these objects has been done with ground manipulators, there
have not been any studies on physical interaction and manipulation of the
deformable linear object with aerial robots.
</p>
<p>This workshop describes our recent work on detecting deformable linear
objects, which uses the segmentation output of the existing methods to provide
the initialization required by the tracking methods automatically. It works
with crossings and can fill the gaps and occlusions in the segmentation and
output the model desirable for physical interaction and simulation. Then we
present our work on using the method for tasks such as routing and manipulation
with the ground and aerial robots. We discuss our feasibility analysis on
extending the physical interaction with these objects to aerial manipulation
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Attention Memory Network for Video Object Segmentation. (arXiv:2205.08075v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08075">
<div class="article-summary-box-inner">
<span><p>Semi-supervised video object segmentation is a fundamental yet Challenging
task in computer vision. Embedding matching based CFBI series networks have
achieved promising results by foreground-background integration approach.
Despite its superior performance, these works exhibit distinct shortcomings,
especially the false predictions caused by little appearance instances in first
frame, even they could easily be recognized by previous frame. Moreover, they
suffer from object's occlusion and error drifts. In order to overcome the
shortcomings , we propose Collaborative Attention Memory Network with an
enhanced segmentation head. We introduce a object context scheme that
explicitly enhances the object information, which aims at only gathering the
pixels that belong to the same category as a given pixel as its context.
Additionally, a segmentation head with Feature Pyramid Attention(FPA) module is
adopted to perform spatial pyramid attention structure on high-level output.
Furthermore, we propose an ensemble network to combine STM network with all
these new refined CFBI network. Finally, we evaluated our approach on the 2021
Youtube-VOS challenge where we obtain 6th place with an overall score of
83.5\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers. (arXiv:2205.08078v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08078">
<div class="article-summary-box-inner">
<span><p>Vision transformers using self-attention or its proposed alternatives have
demonstrated promising results in many image related tasks. However, the
underpinning inductive bias of attention is not well understood. To address
this issue, this paper analyzes attention through the lens of convex duality.
For the non-linear dot-product self-attention, and alternative mechanisms such
as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent
finite-dimensional convex problems that are interpretable and solvable to
global optimality. The convex programs lead to {\it block nuclear-norm
regularization} that promotes low rank in the latent feature and token
dimensions. In particular, we show how self-attention networks implicitly
clusters the tokens, based on their latent similarity. We conduct experiments
for transferring a pre-trained transformer backbone for CIFAR-100
classification by fine-tuning a variety of convex attention heads. The results
indicate the merits of the bias induced by attention compared with the existing
MLP or linear heads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-Aware Metric Learning for Open World Semantic Segmentation via Meta-Channel Aggregation. (arXiv:2205.08083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08083">
<div class="article-summary-box-inner">
<span><p>As one of the most challenging and practical segmentation tasks, open-world
semantic segmentation requires the model to segment the anomaly regions in the
images and incrementally learn to segment out-of-distribution (OOD) objects,
especially under a few-shot condition. The current state-of-the-art (SOTA)
method, Deep Metric Learning Network (DMLNet), relies on pixel-level metric
learning, with which the identification of similar regions having different
semantics is difficult. Therefore, we propose a method called region-aware
metric learning (RAML), which first separates the regions of the images and
generates region-aware features for further metric learning. RAML improves the
integrity of the segmented anomaly regions. Moreover, we propose a novel
meta-channel aggregation (MCA) module to further separate anomaly regions,
forming high-quality sub-region candidates and thereby improving the model
performance for OOD objects. To evaluate the proposed RAML, we have conducted
extensive experiments and ablation studies on Lost And Found and Road Anomaly
datasets for anomaly segmentation and the CityScapes dataset for incremental
few-shot learning. The results show that the proposed RAML achieves SOTA
performance in both stages of open world segmentation. Our code and appendix
are available at https://github.com/czifan/RAML.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Stereo Depth Estimation for Pseudo LiDAR: A Self-Supervised Approach Based on Multi-Input ResNet Encoder. (arXiv:2205.08089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08089">
<div class="article-summary-box-inner">
<span><p>Perception and localization are essential for autonomous delivery vehicles,
mostly estimated from 3D LiDAR sensors due to their precise distance
measurement capability. This paper presents a strategy to obtain the real-time
pseudo point cloud instead of the laser sensor from the image sensor. We
propose an approach to use different depth estimators to obtain pseudo point
clouds like LiDAR to obtain better performance. Moreover, the training and
validating strategy of the depth estimator has adopted stereo imagery data to
estimate more accurate depth estimation as well as point cloud results. Our
approach to generating depth maps outperforms on KITTI benchmark while yielding
point clouds significantly faster than other approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Linear Comb Filter for Event Flicker Removal. (arXiv:2205.08090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08090">
<div class="article-summary-box-inner">
<span><p>Event cameras are bio-inspired sensors that capture per-pixel asynchronous
intensity change rather than the synchronous absolute intensity frames captured
by a classical camera sensor. Such cameras are ideal for robotics applications
since they have high temporal resolution, high dynamic range and low latency.
However, due to their high temporal resolution, event cameras are particularly
sensitive to flicker such as from fluorescent or LED lights. During every cycle
from bright to dark, pixels that image a flickering light source generate many
events that provide little or no useful information for a robot, swamping the
useful data in the scene. In this paper, we propose a novel linear filter to
preprocess event data to remove unwanted flicker events from an event stream.
The proposed algorithm achieves over 4.6 times relative improvement in the
signal-to-noise ratio when compared to the raw event stream due to the
effective removal of flicker from fluorescent lighting. Thus, it is ideally
suited to robotics applications that operate in indoor settings or scenes
illuminated by flickering light sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MATrIX -- Modality-Aware Transformer for Information eXtraction. (arXiv:2205.08094v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08094">
<div class="article-summary-box-inner">
<span><p>We present MATrIX - a Modality-Aware Transformer for Information eXtraction
in the Visual Document Understanding (VDU) domain. VDU covers information
extraction from visually rich documents such as forms, invoices, receipts,
tables, graphs, presentations, or advertisements. In these, text semantics and
visual information supplement each other to provide a global understanding of
the document. MATrIX is pre-trained in an unsupervised way with specifically
designed tasks that require the use of multi-modal information (spatial,
visual, or textual). We consider the spatial and text modalities all at once in
a single token set. To make the attention more flexible, we use a learned
modality-aware relative bias in the attention mechanism to modulate the
attention between the tokens of different modalities. We evaluate MATrIX on 3
different datasets each with strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08099">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep learning models have a parameter count that reaches
into the billions. Training, storing and transferring such models is energy and
time consuming, thus costly. A big part of these costs is caused by training
the network. Model compression lowers storage and transfer costs, and can
further make training more efficient by decreasing the number of computations
in the forward and/or backward pass. Thus, compressing networks also at
training time while maintaining a high performance is an important research
topic. This work is a survey on methods which reduce the number of trained
weights in deep learning models throughout the training. Most of the introduced
methods set network parameters to zero which is called pruning. The presented
pruning approaches are categorized into pruning at initialization, lottery
tickets and dynamic sparse training. Moreover, we discuss methods that freeze
parts of a network at its random initialization. By freezing weights, the
number of trainable parameters is shrunken which reduces gradient computations
and the dimensionality of the model's optimization space. In this survey we
first propose dimensionality reduced training as an underlying mathematical
model that covers pruning and freezing during training. Afterwards, we present
and discuss different dimensionality reduced training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computerized Tomography Pulmonary Angiography Image Simulation using Cycle Generative Adversarial Network from Chest CT imaging in Pulmonary Embolism Patients. (arXiv:2205.08106v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08106">
<div class="article-summary-box-inner">
<span><p>The purpose of this research is to develop a system that generates simulated
computed tomography pulmonary angiography (CTPA) images clinically for
pulmonary embolism diagnoses. Nowadays, CTPA images are the gold standard
computerized detection method to determine and identify the symptoms of
pulmonary embolism (PE), although performing CTPA is harmful for patients and
also expensive. Therefore, we aim to detect possible PE patients through CT
images. The system will simulate CTPA images with deep learning models for the
identification of PE patients' symptoms, providing physicians with another
reference for determining PE patients. In this study, the simulated CTPA image
generation system uses a generative antagonistic network to enhance the
features of pulmonary vessels in the CT images to strengthen the reference
value of the images and provide a basis for hospitals to judge PE patients. We
used the CT images of 22 patients from National Cheng Kung University Hospital
and the corresponding CTPA images as the training data for the task of
simulating CTPA images and generated them using two sets of generative
countermeasure networks. This study is expected to propose a new approach to
the clinical diagnosis of pulmonary embolism, in which a deep learning network
is used to assist in the complex screening process and to review the generated
simulated CTPA images, allowing physicians to assess whether a patient needs to
undergo detailed testing for CTPA, improving the speed of detection of
pulmonary embolism and significantly reducing the number of undetected
patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using artificial intelligence to detect chest X-rays with no significant findings in a primary health care setting in Oulu, Finland. (arXiv:2205.08123v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08123">
<div class="article-summary-box-inner">
<span><p>Objectives: To assess the use of artificial intelligence-based software in
ruling out chest X-ray cases, with no significant findings in a primary health
care setting.
</p>
<p>Methods: In this retrospective study, a commercially available artificial
intelligence (AI) software was used to analyse 10 000 chest X-rays of Finnish
primary health care patients. In studies with a mismatch between an AI normal
report and the original radiologist report, a consensus read by two
board-certified radiologists was conducted to make the final diagnosis.
</p>
<p>Results: After the exclusion of cases not meeting the study criteria, 9579
cases were analysed by AI. Of these cases, 4451 were considered normal in the
original radiologist report and 4644 after the consensus reading. The number of
cases correctly found nonsignificant by AI was 1692 (17.7% of all studies and
36.4% of studies with no significant findings). After the consensus read, there
were nine confirmed false-negative studies. These studies included four cases
of slightly enlarged heart size, four cases of slightly increased pulmonary
opacification and one case with a small unilateral pleural effusion. This gives
the AI a sensitivity of 99.8% (95% CI= 99.65-99.92) and specificity of 36.4 %
(95% CI= 35.05-37.84) for recognising significant pathology on a chest X-ray.
</p>
<p>Conclusions: AI was able to correctly rule out 36.4% of chest X-rays with no
significant findings of primary health care patients, with a minimal number of
false negatives that would lead to effectively no compromise on patient safety.
No critical findings were missed by the software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08129">
<div class="article-summary-box-inner">
<span><p>General-purpose robots require diverse repertoires of behaviors to complete
challenging tasks in real-world unstructured environments. To address this
issue, goal-conditioned reinforcement learning aims to acquire policies that
can reach configurable goals for a wide range of tasks on command. However,
such goal-conditioned policies are notoriously difficult and time-consuming to
train from scratch. In this paper, we propose Planning to Practice (PTP), a
method that makes it practical to train goal-conditioned policies for
long-horizon tasks that require multiple distinct types of interactions to
solve. Our approach is based on two key ideas. First, we decompose the
goal-reaching problem hierarchically, with a high-level planner that sets
intermediate subgoals using conditional subgoal generators in the latent space
for a low-level model-free policy. Second, we propose a hybrid approach which
first pre-trains both the conditional subgoal generator and the policy on
previously collected data through offline reinforcement learning, and then
fine-tunes the policy via online exploration. This fine-tuning process is
itself facilitated by the planned subgoals, which breaks down the original
target task into short-horizon goal-reaching tasks that are significantly
easier to learn. We conduct experiments in both the simulation and real world,
in which the policy is pre-trained on demonstrations of short primitive
behaviors and fine-tuned for temporally extended tasks that are unseen in the
offline data. Our experimental results show that PTP can generate feasible
sequences of subgoals that enable the policy to efficiently solve the target
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brachial Plexus Nerve Trunk Segmentation Using Deep Learning: A Comparative Study with Doctors' Manual Segmentation. (arXiv:2205.08143v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08143">
<div class="article-summary-box-inner">
<span><p>Ultrasound-guided nerve block anesthesia (UGNB) is a high-tech visual nerve
block anesthesia method that can observe the target nerve and its surrounding
structures, the puncture needle's advancement, and local anesthetics spread in
real-time. The key in UGNB is nerve identification. With the help of deep
learning methods, the automatic identification or segmentation of nerves can be
realized, assisting doctors in completing nerve block anesthesia accurately and
efficiently. Here, we establish a public dataset containing 320 ultrasound
images of brachial plexus (BP). Three experienced doctors jointly produce the
BP segmentation ground truth and label brachial plexus trunks. We design a
brachial plexus segmentation system (BPSegSys) based on deep learning. BPSegSys
achieves experienced-doctor-level nerve identification performance in various
experiments. We evaluate BPSegSys' performance in terms of
intersection-over-union (IoU), a commonly used performance measure for
segmentation experiments. Considering three dataset groups in our established
public dataset, the IoU of BPSegSys are 0.5238, 0.4715, and 0.5029,
respectively, which exceed the IoU 0.5205, 0.4704, and 0.4979 of experienced
doctors. In addition, we show that BPSegSys can help doctors identify brachial
plexus trunks more accurately, with IoU improvement up to 27%, which has
significant clinical application value.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairwise Comparison Network for Remote Sensing Scene Classification. (arXiv:2205.08147v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08147">
<div class="article-summary-box-inner">
<span><p>Remote sensing scene classification aims to assign a specific semantic label
to a remote sensing image. Recently, convolutional neural networks have greatly
improved the performance of remote sensing scene classification. However, some
confused images may be easily recognized as the incorrect category, which
generally degrade the performance. The differences between image pairs can be
used to distinguish image categories. This paper proposed a pairwise comparison
network, which contains two main steps: pairwise selection and pairwise
representation. The proposed network first selects similar image pairs, and
then represents the image pairs with pairwise representations. The
self-representation is introduced to highlight the informative parts of each
image itself, while the mutual-representation is proposed to capture the subtle
differences between image pairs. Comprehensive experimental results on two
challenging datasets (AID, NWPU-RESISC45) demonstrate the effectiveness of the
proposed network. The code are provided in
https://github.com/spectralpublic/PCNet.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender and Racial Bias in Visual Question Answering Datasets. (arXiv:2205.08148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08148">
<div class="article-summary-box-inner">
<span><p>Vision-and-language tasks have increasingly drawn more attention as a means
to evaluate human-like reasoning in machine learning models. A popular task in
the field is visual question answering (VQA), which aims to answer questions
about images. However, VQA models have been shown to exploit language bias by
learning the statistical correlations between questions and answers without
looking into the image content: e.g., questions about the color of a banana are
answered with yellow, even if the banana in the image is green. If societal
bias (e.g., sexism, racism, ableism, etc.) is present in the training data,
this problem may be causing VQA models to learn harmful stereotypes. For this
reason, we investigate gender and racial bias in five VQA datasets. In our
analysis, we find that the distribution of answers is highly different between
questions about women and men, as well as the existence of detrimental
gender-stereotypical samples. Likewise, we identify that specific race-related
attributes are underrepresented, whereas potentially discriminatory samples
appear in the analyzed datasets. Our findings suggest that there are dangers
associated to using VQA datasets without considering and dealing with the
potentially harmful stereotypes. We conclude the paper by proposing solutions
to alleviate the problem before, during, and after the dataset collection
process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnPWC-SVDLO: Multi-SVD on PointPWC for Unsupervised Lidar Odometry. (arXiv:2205.08150v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08150">
<div class="article-summary-box-inner">
<span><p>High-precision lidar odomety is an essential part of autonomous driving. In
recent years, deep learning methods have been widely used in lidar odomety
tasks, but most of the current methods only extract the global features of the
point clouds. It is impossible to obtain more detailed point-level features in
this way. In addition, only the fully connected layer is used to estimate the
pose. The fully connected layer has achieved obvious results in the
classification task, but the changes in pose are a continuous rather than
discrete process, high-precision pose estimation can not be obtained only by
using the fully connected layer. Our method avoids problems mentioned above. We
use PointPWC as our backbone network. PointPWC is originally used for scene
flow estimation. The scene flow estimation task has a strong correlation with
lidar odomety. Traget point clouds can be obtained by adding the scene flow and
source point clouds. We can achieve the pose directly through ICP algorithm
solved by SVD, and the fully connected layer is no longer used. PointPWC
extracts point-level features from point clouds with different sampling levels,
which solves the problem of too rough feature extraction. We conduct
experiments on KITTI, Ford Campus Vision and Lidar DataSe and Apollo-SouthBay
Dataset. Our result is comparable with the state-of-the-art unsupervised deep
learing method SelfVoxeLO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-based Network for Few-shot Image Classification. (arXiv:2205.08157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08157">
<div class="article-summary-box-inner">
<span><p>The transductive inference is an effective technique in the few-shot learning
task, where query sets update prototypes to improve themselves. However, these
methods optimize the model by considering only the classification scores of the
query instances as confidence while ignoring the uncertainty of these
classification scores. In this paper, we propose a novel method called
Uncertainty-Based Network, which models the uncertainty of classification
results with the help of mutual information. Specifically, we first data
augment and classify the query instance and calculate the mutual information of
these classification scores. Then, mutual information is used as uncertainty to
assign weights to classification scores, and the iterative update strategy
based on classification scores and uncertainties assigns the optimal weights to
query instances in prototype optimization. Extensive results on four benchmarks
show that Uncertainty-Based Network achieves comparable performance in
classification accuracy compared to state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CellTypeGraph: A New Geometric Computer Vision Benchmark. (arXiv:2205.08166v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08166">
<div class="article-summary-box-inner">
<span><p>Classifying all cells in an organ is a relevant and difficult problem from
plant developmental biology. We here abstract the problem into a new benchmark
for node classification in a geo-referenced graph. Solving it requires learning
the spatial layout of the organ including symmetries. To allow the convenient
testing of new geometrical learning methods, the benchmark of Arabidopsis
thaliana ovules is made available as a PyTorch data loader, along with a large
number of precomputed features. Finally, we benchmark eight recent graph neural
network architectures, finding that DeeperGCN currently works best on this
problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DynPL-SVO: A New Method Using Point and Line Features for Stereo Visual Odometry in Dynamic Scenes. (arXiv:2205.08207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08207">
<div class="article-summary-box-inner">
<span><p>Stereo visual odometry is widely used where a robot tracks its position and
orientation using stereo cameras. Most of the approaches recovered mobile
robotics motion based on the matching and tracking of point features along a
sequence of stereo images. But in low-textured and dynamic scenes, there are no
sufficient robust static point features for motion estimation, causing lots of
previous work to fail to reconstruct the robotic motion. However, line features
can be detected in such low-textured and dynamic scenes. In this paper, we
proposed DynPL-SVO, a stereo visual odometry with the $dynamic$ $grid$
algorithm and the cost function containing both vertical and horizontal
information of the line features. Stereo camera motion was obtained through
Levenberg-Marquard minimization of re-projection error of point and line
features. The experimental results on the KITTI and EuRoC MAV datasets showed
that the DynPL-SVO had a competitive performance when compared to other
state-of-the-art systems by producing more robust and accurate motion
estimation, especially in low-textured and dynamic scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08209">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks have proven to be remarkably effective in
semantic segmentation tasks. Most popular loss functions were introduced
targeting improved volumetric scores, such as the Sorensen Dice coefficient. By
design, DSC can tackle class imbalance; however, it does not recognize instance
imbalance within a class. As a result, a large foreground instance can dominate
minor instances and still produce a satisfactory Sorensen Dice coefficient.
Nevertheless, missing out on instances will lead to poor detection performance.
This represents a critical issue in applications such as disease progression
monitoring. For example, it is imperative to locate and surveil small-scale
lesions in the follow-up of multiple sclerosis patients. We propose a novel
family of loss functions, nicknamed blob loss, primarily aimed at maximizing
instance-level detection metrics, such as F1 score and sensitivity. Blob loss
is designed for semantic segmentation problems in which the instances are the
connected components within a class. We extensively evaluate a DSC-based blob
loss in five complex 3D semantic segmentation tasks featuring pronounced
instance heterogeneity in terms of texture and morphology. Compared to soft
Dice loss, we achieve 5 percent improvement for MS lesions, 3 percent
improvement for liver tumor, and an average 2 percent improvement for
Microscopy segmentation tasks considering F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAS-Net: Conditional Atlas Generation and Brain Segmentation for Fetal MRI. (arXiv:2205.08239v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08239">
<div class="article-summary-box-inner">
<span><p>Fetal Magnetic Resonance Imaging (MRI) is used in prenatal diagnosis and to
assess early brain development. Accurate segmentation of the different brain
tissues is a vital step in several brain analysis tasks, such as cortical
surface reconstruction and tissue thickness measurements. Fetal MRI scans,
however, are prone to motion artifacts that can affect the correctness of both
manual and automatic segmentation techniques. In this paper, we propose a novel
network structure that can simultaneously generate conditional atlases and
predict brain tissue segmentation, called CAS-Net. The conditional atlases
provide anatomical priors that can constrain the segmentation connectivity,
despite the heterogeneity of intensity values caused by motion or partial
volume effects. The proposed method is trained and evaluated on 253 subjects
from the developing Human Connectome Project (dHCP). The results demonstrate
that the proposed method can generate conditional age-specific atlas with sharp
boundary and shape variance. It also segment multi-category brain tissues for
fetal MRI with a high overall Dice similarity coefficient (DSC) of $85.2\%$ for
the selected 9 tissue labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Optimal Sequential Grouping for Video Scene Detection. (arXiv:2205.08249v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08249">
<div class="article-summary-box-inner">
<span><p>Video scene detection is the task of dividing videos into temporal semantic
chapters. This is an important preliminary step before attempting to analyze
heterogeneous video content. Recently, Optimal Sequential Grouping (OSG) was
proposed as a powerful unsupervised solution to solve a formulation of the
video scene detection problem. In this work, we extend the capabilities of OSG
to the learning regime. By giving the capability to both learn from examples
and leverage a robust optimization formulation, we can boost performance and
enhance the versatility of the technology. We present a comprehensive analysis
of incorporating OSG into deep learning neural networks under various
configurations. These configurations include learning an embedding in a
straight-forward manner, a tailored loss designed to guide the solution of OSG,
and an integrated model where the learning is performed through the OSG
pipeline. With thorough evaluation and analysis, we assess the benefits and
behavior of the various configurations, and show that our learnable OSG
approach exhibits desirable behavior and enhanced performance compared to the
state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection Masking for Improved OCR on Noisy Documents. (arXiv:2205.08257v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08257">
<div class="article-summary-box-inner">
<span><p>Optical Character Recognition (OCR), the task of extracting textual
information from scanned documents is a vital and broadly used technology for
digitizing and indexing physical documents. Existing technologies perform well
for clean documents, but when the document is visually degraded, or when there
are non-textual elements, OCR quality can be greatly impacted, specifically due
to erroneous detections. In this paper we present an improved detection network
with a masking system to improve the quality of OCR performed on documents. By
filtering non-textual elements from the image we can utilize document-level OCR
to incorporate contextual information to improve OCR results. We perform a
unified evaluation on a publicly available dataset demonstrating the usefulness
and broad applicability of our method. Additionally, we present and make
publicly available our synthetic dataset with a unique hard-negative component
specifically tuned to improve detection results, and evaluate the benefits that
can be gained from its usage
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MulT: An End-to-End Multitask Learning Transformer. (arXiv:2205.08303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08303">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end Multitask Learning Transformer framework, named
MulT, to simultaneously learn multiple high-level vision tasks, including depth
estimation, semantic segmentation, reshading, surface normal estimation, 2D
keypoint detection, and edge detection. Based on the Swin transformer model,
our framework encodes the input image into a shared representation and makes
predictions for each vision task using task-specific transformer-based decoder
heads. At the heart of our approach is a shared attention mechanism modeling
the dependencies across the tasks. We evaluate our model on several multitask
benchmarks, showing that our MulT framework outperforms both the state-of-the
art multitask convolutional neural network models and all the respective single
task transformer models. Our experiments further highlight the benefits of
sharing attention across all the tasks, and demonstrate that our MulT model is
robust and generalizes well to new domains. Our project website is at
https://ivrl.github.io/MulT/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation. (arXiv:2205.08316v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08316">
<div class="article-summary-box-inner">
<span><p>In recent years, policy learning methods using either reinforcement or
imitation have made significant progress. However, both techniques still suffer
from being computationally expensive and requiring large amounts of training
data. This problem is especially prevalent in real-world robotic manipulation
tasks, where access to ground truth scene features is not available and
policies are instead learned from raw camera observations. In this paper, we
demonstrate the efficacy of learning image keypoints via the Dense
Correspondence pretext task for downstream policy learning. Extending prior
work to challenging multi-object scenes, we show that our model can be trained
to deal with important problems in representation learning, primarily
scale-invariance and occlusion. We evaluate our approach on diverse robot
manipulation tasks, compare it to other visual representation learning
approaches, and demonstrate its flexibility and effectiveness for
sample-efficient policy learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Interactive Image Matting. (arXiv:2205.08324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08324">
<div class="article-summary-box-inner">
<span><p>Recent image matting studies are developing towards proposing trimap-free or
interactive methods for complete complex image matting tasks. Although avoiding
the extensive labors of trimap annotation, existing methods still suffer from
two limitations: (1) For the single image with multiple objects, it is
essential to provide extra interaction information to help determining the
matting target; (2) For transparent objects, the accurate regression of alpha
matte from RGB image is much more difficult compared with the opaque ones. In
this work, we propose a Unified Interactive image Matting method, named UIM,
which solves the limitations and achieves satisfying matting results for any
scenario. Specifically, UIM leverages multiple types of user interaction to
avoid the ambiguity of multiple matting targets, and we compare the pros and
cons of different annotation types in detail. To unify the matting performance
for transparent and opaque objects, we decouple image matting into two stages,
i.e., foreground segmentation and transparency prediction. Moreover, we design
a multi-scale attentive fusion module to alleviate the vagueness in the
boundary region. Experimental results demonstrate that UIM achieves
state-of-the-art performance on the Composition-1K test set and a synthetic
unified dataset. Our code and models will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphMapper: Efficient Visual Navigation by Scene Graph Generation. (arXiv:2205.08325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08325">
<div class="article-summary-box-inner">
<span><p>Understanding the geometric relationships between objects in a scene is a
core capability in enabling both humans and autonomous agents to navigate in
new environments. A sparse, unified representation of the scene topology will
allow agents to act efficiently to move through their environment, communicate
the environment state with others, and utilize the representation for diverse
downstream tasks. To this end, we propose a method to train an autonomous agent
to learn to accumulate a 3D scene graph representation of its environment by
simultaneously learning to navigate through said environment. We demonstrate
that our approach, GraphMapper, enables the learning of effective navigation
policies through fewer interactions with the environment than vision-based
systems alone. Further, we show that GraphMapper can act as a modular scene
encoder to operate alongside existing Learning-based solutions to not only
increase navigational efficiency but also generate intermediate scene
representations that are useful for other future tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Supervised Information Bottleneck Hashing for Cross-modal Retrieval based Computer-aided Diagnosis. (arXiv:2205.08365v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08365">
<div class="article-summary-box-inner">
<span><p>Mapping X-ray images, radiology reports, and other medical data as binary
codes in the common space, which can assist clinicians to retrieve
pathology-related data from heterogeneous modalities (i.e., hashing-based
cross-modal medical data retrieval), provides a new view to promot
computeraided diagnosis. Nevertheless, there remains a barrier to boost medical
retrieval accuracy: how to reveal the ambiguous semantics of medical data
without the distraction of superfluous information. To circumvent this
drawback, we propose Deep Supervised Information Bottleneck Hashing (DSIBH),
which effectively strengthens the discriminability of hash codes. Specifically,
the Deep Deterministic Information Bottleneck (Yu, Yu, and Principe 2021) for
single modality is extended to the cross-modal scenario. Benefiting from this,
the superfluous information is reduced, which facilitates the discriminability
of hash codes. Experimental results demonstrate the superior accuracy of the
proposed DSIBH compared with state-of-the-arts in cross-modal medical data
retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Velocity Picking Using Unsupervised Ensemble Learning. (arXiv:2205.08372v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08372">
<div class="article-summary-box-inner">
<span><p>In seismic data processing, accurate and efficient automatic velocity picking
algorithms can significantly accelerate the processing, and the main branch is
to use velocity spectra for velocity pickup. Recently, machine learning
algorithms have been widely used in automatic spectrum picking. Even though
deep learning methods can address the problem well in supervised cases, they
are often accompanied by expensive computational costs and low
interpretability. On the contrast, unsupervised learning methods based on the
physical knowledge have great potential to efficiently resolve the task. In
this paper, we propose an unsupervised ensemble learning (UEL) method to pick
the root mean square (RMS) velocities on the spectrum. In particular, UEL
utilizes the information of nearby velocity spectra and the nearest seed
velocity curve to assist the selection of effective and reasonable velocity
points. To increase the coherence of energy peaks, an information gain method
is developed by local normalization. In addition, we designed the attention
scale-space filter (ASSF) clustering method to incorporate the coherence
information into the picking process. Experiments on three datasets demonstrate
that compared to traditional clustering methods, UEL can recognize energy
clusters better, especially with smaller blobs. Moreover, the injection of
nearby spectra and interval velocity constraint in UEL significantly improves
the robustness and accuracy of picking results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias and Fairness on Multimodal Emotion Detection Algorithms. (arXiv:2205.08383v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08383">
<div class="article-summary-box-inner">
<span><p>Numerous studies have shown that machine learning algorithms can latch onto
protected attributes such as race and gender and generate predictions that
systematically discriminate against one or more groups. To date the majority of
bias and fairness research has been on unimodal models. In this work, we
explore the biases that exist in emotion recognition systems in relationship to
the modalities utilized, and study how multimodal approaches affect system bias
and fairness. We consider audio, text, and video modalities, as well as all
possible multimodal combinations of those, and find that text alone has the
least bias, and accounts for the majority of the models' performances, raising
doubts about the worthiness of multimodal emotion recognition systems when bias
and fairness are desired alongside model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08390">
<div class="article-summary-box-inner">
<span><p>Ultrasonography is an important routine examination for breast cancer
diagnosis, due to its non-invasive, radiation-free and low-cost properties.
However, it is still not the first-line screening test for breast cancer due to
its inherent limitations. It would be a tremendous success if we can precisely
diagnose breast cancer by breast ultrasound images (BUS). Many learning-based
computer-aided diagnostic methods have been proposed to achieve breast cancer
diagnosis/lesion classification. However, most of them require a pre-define ROI
and then classify the lesion inside the ROI. Conventional classification
backbones, such as VGG16 and ResNet50, can achieve promising classification
results with no ROI requirement. But these models lack interpretability, thus
restricting their use in clinical practice. In this study, we propose a novel
ROI-free model for breast cancer diagnosis in ultrasound images with
interpretable feature representations. We leverage the anatomical prior
knowledge that malignant and benign tumors have different spatial relationships
between different tissue layers, and propose a HoVer-Transformer to formulate
this prior knowledge. The proposed HoVer-Trans block extracts the inter- and
intra-layer spatial information horizontally and vertically. We conduct and
release an open dataset GDPH&amp;GYFYY for breast cancer diagnosis in BUS. The
proposed model is evaluated in three datasets by comparing with four CNN-based
models and two vision transformer models via a five-fold cross validation. It
achieves state-of-the-art classification performance with the best model
interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Building Footprint Generation with Feature and Output Consistency Training. (arXiv:2205.08416v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08416">
<div class="article-summary-box-inner">
<span><p>Accurate and reliable building footprint maps are vital to urban planning and
monitoring, and most existing approaches fall back on convolutional neural
networks (CNNs) for building footprint generation. However, one limitation of
these methods is that they require strong supervisory information from massive
annotated samples for network learning. State-of-the-art semi-supervised
semantic segmentation networks with consistency training can help to deal with
this issue by leveraging a large amount of unlabeled data, which encourages the
consistency of model output on data perturbation. Considering that rich
information is also encoded in feature maps, we propose to integrate the
consistency of both features and outputs in the end-to-end network training of
unlabeled samples, enabling to impose additional constraints. Prior
semi-supervised semantic segmentation networks have established the cluster
assumption, in which the decision boundary should lie in the vicinity of low
sample density. In this work, we observe that for building footprint
generation, the low-density regions are more apparent at the intermediate
feature representations within the encoder than the encoder's input or output.
Therefore, we propose an instruction to assign the perturbation to the
intermediate feature representations within the encoder, which considers the
spatial resolution of input remote sensing imagery and the mean size of
individual buildings in the study area. The proposed method is evaluated on
three datasets with different resolutions: Planet dataset (3 m/pixel),
Massachusetts dataset (1 m/pixel), and Inria dataset (0.3 m/pixel).
Experimental results show that the proposed approach can well extract more
complete building structures and alleviate omission errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Visual Servoing for Multi-Step Tasks. (arXiv:2205.08441v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08441">
<div class="article-summary-box-inner">
<span><p>Visual Servoing has been effectively used to move a robot into specific
target locations or to track a recorded demonstration. It does not require
manual programming, but it is typically limited to settings where one
demonstration maps to one environment state. We propose a modular approach to
extend visual servoing to scenarios with multiple demonstration sequences. We
call this conditional servoing, as we choose the next demonstration conditioned
on the observation of the robot. This method presents an appealing strategy to
tackle multi-step problems, as individual demonstrations can be combined
flexibly into a control policy. We propose different selection functions and
compare them on a shape-sorting task in simulation. With the reprojection error
yielding the best overall results, we implement this selection function on a
real robot and show the efficacy of the proposed conditional servoing. For
videos of our experiments, please check out our project page:
https://lmb.informatik.uni-freiburg.de/projects/conditional_servoing/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Graph Based Features in Computer Aided Diagnosis for Histopathological Image Classification of Gastric Cancer. (arXiv:2205.08467v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08467">
<div class="article-summary-box-inner">
<span><p>The gold standard for gastric cancer detection is gastric histopathological
image analysis, but there are certain drawbacks in the existing
histopathological detection and diagnosis. In this paper, based on the study of
computer aided diagnosis system, graph based features are applied to gastric
cancer histopathology microscopic image analysis, and a classifier is used to
classify gastric cancer cells from benign cells. Firstly, image segmentation is
performed, and after finding the region, cell nuclei are extracted using the
k-means method, the minimum spanning tree (MST) is drawn, and graph based
features of the MST are extracted. The graph based features are then put into
the classifier for classification. In this study, different segmentation
methods are compared in the tissue segmentation stage, among which are
Level-Set, Otsu thresholding, watershed, SegNet, U-Net and Trans-U-Net
segmentation; Graph based features, Red, Green, Blue features, Grey-Level
Co-occurrence Matrix features, Histograms of Oriented Gradient features and
Local Binary Patterns features are compared in the feature extraction stage;
Radial Basis Function (RBF) Support Vector Machine (SVM), Linear SVM,
Artificial Neural Network, Random Forests, k-NearestNeighbor, VGG16, and
Inception-V3 are compared in the classifier stage. It is found that using U-Net
to segment tissue areas, then extracting graph based features, and finally
using RBF SVM classifier gives the optimal results with 94.29%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation. (arXiv:2205.08473v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08473">
<div class="article-summary-box-inner">
<span><p>Identifying polyps is a challenging problem for automatic analysis of
endoscopic images in computer-aided clinical support systems. Models based on
convolutional networks (CNN), transformers, and combinations of them have been
proposed to segment polyps with promising results. However, those approaches
have limitations either in modeling the local appearance of the polyps only or
lack of multi-level features for spatial dependency in the decoding process.
This paper proposes a novel network, namely ColonFormer, to address these
limitations. ColonFormer is an encoder-decoder architecture with the capability
of modeling long-range semantic information at both encoder and decoder
branches. The encoder is a lightweight architecture based on transformers for
modeling global semantic relations at multi scales. The decoder is a
hierarchical network structure designed for learning multi-level features to
enrich feature representation. Besides, a refinement module is added with a new
skip connection technique to refine the boundary of polyp objects in the global
map for accurate segmentation. Extensive experiments have been conducted on
five popular benchmark datasets for polyp segmentation, including Kvasir,
CVC-Clinic DB, CVCColonDB, EndoScene, and ETIS. Experimental results show that
our ColonFormer achieve state-of-the-art performance on all benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A CLIP-Hitchhiker's Guide to Long Video Retrieval. (arXiv:2205.08508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08508">
<div class="article-summary-box-inner">
<span><p>Our goal in this paper is the adaptation of image-text models for long video
retrieval. Recent works have demonstrated state-of-the-art performance in video
retrieval by adopting CLIP, effectively hitchhiking on the image-text
representation for video tasks. However, there has been limited success in
learning temporal aggregation that outperform mean-pooling the image-level
representations extracted per frame by CLIP. We find that the simple yet
effective baseline of weighted-mean of frame embeddings via query-scoring is a
significant improvement above all prior temporal modelling attempts and
mean-pooling. In doing so, we provide an improved baseline for others to
compare to and demonstrate state-of-the-art performance of this simple baseline
on a suite of long video retrieval benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08515">
<div class="article-summary-box-inner">
<span><p>Self-supervised category-agnostic segmentation of real-world images into
objects is a challenging open problem in computer vision. Here, we show how to
learn static grouping priors from motion self-supervision, building on the
cognitive science notion of Spelke Objects: groupings of stuff that move
together. We introduce Excitatory-Inhibitory Segment Extraction Network
(EISEN), which learns from optical flow estimates to extract pairwise affinity
graphs for static scenes. EISEN then produces segments from affinities using a
novel graph propagation and competition mechanism. Correlations between
independent sources of motion (e.g. robot arms) and objects they move are
resolved into separate segments through a bootstrapping training process. We
show that EISEN achieves a substantial improvement in the state of the art for
self-supervised segmentation on challenging synthetic and real-world robotic
image datasets. We also present an ablation analysis illustrating the
importance of each element of the EISEN architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Neural Networks Compress Manifolds Optimally?. (arXiv:2205.08518v1 [cs.IT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08518">
<div class="article-summary-box-inner">
<span><p>Artificial Neural-Network-based (ANN-based) lossy compressors have recently
obtained striking results on several sources. Their success may be ascribed to
an ability to identify the structure of low-dimensional manifolds in
high-dimensional ambient spaces. Indeed, prior work has shown that ANN-based
compressors can achieve the optimal entropy-distortion curve for some such
sources. In contrast, we determine the optimal entropy-distortion tradeoffs for
two low-dimensional manifolds with circular structure and show that
state-of-the-art ANN-based compressors fail to optimally compress the sources,
especially at high rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Neural Articulated Shape and Appearance Models. (arXiv:2205.08525v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08525">
<div class="article-summary-box-inner">
<span><p>Learning geometry, motion, and appearance priors of object classes is
important for the solution of a large variety of computer vision problems.
While the majority of approaches has focused on static objects, dynamic
objects, especially with controllable articulation, are less explored. We
propose a novel approach for learning a representation of the geometry,
appearance, and motion of a class of articulated objects given only a set of
color images as input. In a self-supervised manner, our novel representation
learns shape, appearance, and articulation codes that enable independent
control of these semantic dimensions. Our model is trained end-to-end without
requiring any articulation annotations. Experiments show that our approach
performs well for different joint types, such as revolute and prismatic joints,
as well as different combinations of these joints. Compared to state of the art
that uses direct 3D supervision and does not output appearance, we recover more
faithful geometry and appearance from 2D observations only. In addition, our
representation enables a large variety of applications, such as few-shot
reconstruction, the generation of novel articulations, and novel
view-synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Adapter for Dense Predictions. (arXiv:2205.08534v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08534">
<div class="article-summary-box-inner">
<span><p>This work investigates a simple yet powerful adapter for Vision Transformer
(ViT). Unlike recent visual transformers that introduce vision-specific
inductive biases into their architectures, ViT achieves inferior performance on
dense prediction tasks due to lacking prior information of images. To solve
this issue, we propose a Vision Transformer Adapter (ViT-Adapter), which can
remedy the defects of ViT and achieve comparable performance to vision-specific
models by introducing inductive biases via an additional architecture.
Specifically, the backbone in our framework is a vanilla transformer that can
be pre-trained with multi-modal data. When fine-tuning on downstream tasks, a
modality-specific adapter is used to introduce the data and tasks' prior
information into the model, making it suitable for these tasks. We verify the
effectiveness of our ViT-Adapter on multiple downstream tasks, including object
detection, instance segmentation, and semantic segmentation. Notably, when
using HTC++, our ViT-Adapter-L yields 60.1 box AP and 52.1 mask AP on COCO
test-dev, surpassing Swin-L by 1.4 box AP and 1.0 mask AP. For semantic
segmentation, our ViT-Adapter-L establishes a new state-of-the-art of 60.5 mIoU
on ADE20K val, 0.6 points higher than SwinV2-G. We hope that the proposed
ViT-Adapter could serve as an alternative for vision-specific transformers and
facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. (arXiv:2205.08535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08535">
<div class="article-summary-box-inner">
<span><p>3D avatar creation plays a crucial role in the digital age. However, the
whole production process is prohibitively time-consuming and labor-intensive.
To democratize this technology to a larger audience, we propose AvatarCLIP, a
zero-shot text-driven framework for 3D avatar generation and animation. Unlike
professional software that requires expert knowledge, AvatarCLIP empowers
layman users to customize a 3D avatar with the desired shape and texture, and
drive the avatar with the described motions using solely natural languages. Our
key insight is to take advantage of the powerful vision-language model CLIP for
supervising neural human generation, in terms of 3D geometry, texture and
animation. Specifically, driven by natural language descriptions, we initialize
3D human geometry generation with a shape VAE network. Based on the generated
3D human shapes, a volume rendering model is utilized to further facilitate
geometry sculpting and texture generation. Moreover, by leveraging the priors
learned in the motion VAE, a CLIP-guided reference-based motion synthesis
method is proposed for the animation of the generated 3D avatar. Extensive
qualitative and quantitative experiments validate the effectiveness and
generalizability of AvatarCLIP on a wide range of avatars. Remarkably,
AvatarCLIP can generate unseen 3D avatars with novel animations, achieving
superior zero-shot capability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Visual Embeddings for Attributes and Objects. (arXiv:2205.08536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08536">
<div class="article-summary-box-inner">
<span><p>We study the problem of compositional zero-shot learning for object-attribute
recognition. Prior works use visual features extracted with a backbone network,
pre-trained for object classification and thus do not capture the subtly
distinct features associated with attributes. To overcome this challenge, these
studies employ supervision from the linguistic space, and use pre-trained word
embeddings to better separate and compose attribute-object pairs for
recognition. Analogous to linguistic embedding space, which already has unique
and agnostic embeddings for object and attribute, we shift the focus back to
the visual space and propose a novel architecture that can disentangle
attribute and object features in the visual space. We use visual decomposed
features to hallucinate embeddings that are representative for the seen and
novel compositions to better regularize the learning of our model. Extensive
experiments show that our method outperforms existing work with significant
margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created
based on VAW. The code, models, and dataset splits are publicly available at
https://github.com/nirat1606/OADis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signal2Image Modules in Deep Neural Networks for EEG Classification. (arXiv:1904.13216v8 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.13216">
<div class="article-summary-box-inner">
<span><p>Deep learning has revolutionized computer vision utilizing the increased
availability of big data and the power of parallel computational units such as
graphical processing units. The vast majority of deep learning research is
conducted using images as training data, however the biomedical domain is rich
in physiological signals that are used for diagnosis and prediction problems.
It is still an open research question how to best utilize signals to train deep
neural networks.
</p>
<p>In this paper we define the term Signal2Image (S2Is) as trainable or
non-trainable prefix modules that convert signals, such as
Electroencephalography (EEG), to image-like representations making them
suitable for training image-based deep neural networks defined as `base
models'. We compare the accuracy and time performance of four S2Is (`signal as
image', spectrogram, one and two layer Convolutional Neural Networks (CNNs))
combined with a set of `base models' (LeNet, AlexNet, VGGnet, ResNet, DenseNet)
along with the depth-wise and 1D variations of the latter. We also provide
empirical evidence that the one layer CNN S2I performs better in eleven out of
fifteen tested models than non-trainable S2Is for classifying EEG signals and
we present visual comparisons of the outputs of the S2Is.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsely Activated Networks. (arXiv:1907.06592v9 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.06592">
<div class="article-summary-box-inner">
<span><p>Previous literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features. However, this was done
without considering the description length of the learned representations which
is a direct and unbiased measure of the model complexity. In this paper, first
we introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined $\varphi$. We lastly present Sparsely Activated Networks
(SANs) that consist of kernels with shared weights that, during encoding, are
convolved with the input and then passed through a sparse activation function.
During decoding, the same weights are convolved with the sparse activation map
and subsequently the partial reconstructions from each weight are summed to
reconstruct the input. We compare SANs using the five previously defined
activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,
FMNIST) and show that models that are selected using $\varphi$ have small
description representation length and consist of interpretable kernels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occluded Video Instance Segmentation: A Benchmark. (arXiv:2102.01558v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01558">
<div class="article-summary-box-inner">
<span><p>Can our video understanding systems perceive objects when a heavy occlusion
exists in a scene?
</p>
<p>To answer this question, we collect a large-scale dataset called OVIS for
occluded video instance segmentation, that is, to simultaneously detect,
segment, and track instances in occluded scenes. OVIS consists of 296k
high-quality instance masks from 25 semantic categories, where object
occlusions usually occur. While our human vision systems can understand those
occluded instances by contextual reasoning and association, our experiments
suggest that current video understanding systems cannot. On the OVIS dataset,
the highest AP achieved by state-of-the-art algorithms is only 16.3, which
reveals that we are still at a nascent stage for understanding objects,
instances, and videos in a real-world scenario. We also present a simple
plug-and-play module that performs temporal feature calibration to complement
missing object cues caused by occlusion. Built upon MaskTrack R-CNN and
SipMask, we obtain a remarkable AP improvement on the OVIS dataset. The OVIS
dataset and project code are available at <a href="http://songbai.site/ovis">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence. (arXiv:2103.13512v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13512">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence systems cannot yet match human abilities to apply
knowledge to situations that vary from what they have been programmed for, or
trained for. In visual object recognition methods of inference exploiting
top-down information (from a model) have been shown to be effective for
recognising entities in difficult conditions. Here this type of inference,
called `projection', is shown to be a key mechanism to solve the problem of
applying knowledge to varied or challenging situations, across a range of AI
domains, such as vision, robotics, or language. Finally the relevance of
projection to tackling the commonsense knowledge problem is discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised 3D Human Pose Estimation with Cross-view U-shaped Graph Convolutional Network. (arXiv:2105.10882v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10882">
<div class="article-summary-box-inner">
<span><p>Although monocular 3D human pose estimation methods have made significant
progress, it is far from being solved due to the inherent depth ambiguity.
Instead, exploiting multi-view information is a practical way to achieve
absolute 3D human pose estimation. In this paper, we propose a simple yet
effective pipeline for weakly-supervised cross-view 3D human pose estimation.
By only using two camera views, our method can achieve state-of-the-art
performance in a weakly-supervised manner, requiring no 3D ground truth but
only 2D annotations. Specifically, our method contains two steps: triangulation
and refinement. First, given the 2D keypoints that can be obtained through any
classic 2D detection methods, triangulation is performed across two views to
lift the 2D keypoints into coarse 3D poses. Then, a novel cross-view U-shaped
graph convolutional network (CV-UGCN), which can explore the spatial
configurations and cross-view correlations, is designed to refine the coarse 3D
poses. In particular, the refinement progress is achieved through
weakly-supervised learning, in which geometric and structure-aware consistency
checks are performed. We evaluate our method on the standard benchmark dataset,
Human3.6M. The Mean Per Joint Position Error on the benchmark dataset is 27.4
mm, which outperforms existing state-of-the-art methods remarkably (27.4 mm vs
30.2 mm).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Face Anti-Spoofing: A Survey. (arXiv:2106.14948v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14948">
<div class="article-summary-box-inner">
<span><p>Face anti-spoofing (FAS) has lately attracted increasing attention due to its
vital role in securing face recognition systems from presentation attacks
(PAs). As more and more realistic PAs with novel types spring up, traditional
FAS methods based on handcrafted features become unreliable due to their
limited representation capacity. With the emergence of large-scale academic
datasets in the recent decade, deep learning based FAS achieves remarkable
performance and dominates this area. However, existing reviews in this field
mainly focus on the handcrafted features, which are outdated and uninspiring
for the progress of FAS community. In this paper, to stimulate future research,
we present the first comprehensive review of recent advances in deep learning
based FAS. It covers several novel and insightful components: 1) besides
supervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also
investigate recent methods with pixel-wise supervision (e.g., pseudo depth
map); 2) in addition to traditional intra-dataset evaluation, we collect and
analyze the latest methods specially designed for domain generalization and
open-set FAS; and 3) besides commercial RGB camera, we summarize the deep
learning applications under multi-modal (e.g., depth and infrared) or
specialized (e.g., light field and flash) sensors. We conclude this survey by
emphasizing current open issues and highlighting potential prospects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03152">
<div class="article-summary-box-inner">
<span><p>Domain adaptation (DA) has drawn high interest for its capacity to adapt a
model trained on labeled source data to perform well on unlabeled or weakly
labeled target data from a different domain. Most common DA techniques require
concurrent access to the input images of both the source and target domains.
However, in practice, privacy concerns often impede the availability of source
images in the adaptation phase. This is a very frequent DA scenario in medical
imaging, where, for instance, the source and target images could come from
different clinical sites. We introduce a source-free domain adaptation for
image segmentation. Our formulation is based on minimizing a label-free entropy
loss defined over target-domain data, which we further guide with a
domain-invariant prior on the segmentation regions. Many priors can be derived
from anatomical information. Here, a class ratio prior is estimated from
anatomical knowledge and integrated in the form of a Kullback Leibler (KL)
divergence in our overall loss function. Furthermore, we motivate our overall
loss with an interesting link to maximizing the mutual information between the
target images and their label predictions. We show the effectiveness of our
prior aware entropy minimization in a variety of domain-adaptation scenarios,
with different modalities and applications, including spine, prostate, and
cardiac segmentation. Our method yields comparable results to several state of
the art adaptation techniques, despite having access to much less information,
as the source images are entirely absent in our adaptation phase. Our
straightforward adaptation strategy uses only one network, contrary to popular
adversarial techniques, which are not applicable to a source-free DA setting.
Our framework can be readily used in a breadth of segmentation problems, and
our code is publicly available: https://github.com/mathilde-b/SFDA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Meta Pattern for Face Anti-Spoofing. (arXiv:2110.06753v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06753">
<div class="article-summary-box-inner">
<span><p>Face Anti-Spoofing (FAS) is essential to secure face recognition systems and
has been extensively studied in recent years. Although deep neural networks
(DNNs) for the FAS task have achieved promising results in intra-dataset
experiments with similar distributions of training and testing data, the DNNs'
generalization ability is limited under the cross-domain scenarios with
different distributions of training and testing data. To improve the
generalization ability, recent hybrid methods have been explored to extract
task-aware handcrafted features (e.g., Local Binary Pattern) as discriminative
information for the input of DNNs. However, the handcrafted feature extraction
relies on experts' domain knowledge, and how to choose appropriate handcrafted
features is underexplored. To this end, we propose a learnable network to
extract Meta Pattern (MP) in our learning-to-learn framework. By replacing
handcrafted features with the MP, the discriminative information from MP is
capable of learning a more generalized model. Moreover, we devise a two-stream
network to hierarchically fuse the input RGB image and the extracted MP by
using our proposed Hierarchical Fusion Module (HFM). We conduct comprehensive
experiments and show that our MP outperforms the compared handcrafted features.
Also, our proposed method with HFM and the MP can achieve state-of-the-art
performance on two different domain generalization evaluation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Strong Gravitational Lenses Through Self-Attention. (arXiv:2110.09202v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09202">
<div class="article-summary-box-inner">
<span><p>The upcoming large scale surveys like LSST are expected to find approximately
$10^5$ strong gravitational lenses by analysing data of many orders of
magnitude larger than those in contemporary astronomical surveys. In this case,
non-automated techniques will be highly challenging and time-consuming, even if
they are possible at all. We propose a new automated architecture based on the
principle of self-attention to find strong gravitational lenses. The advantages
of self-attention-based encoder models over convolution neural networks are
investigated, and ways to optimise the outcome of encoder models are analysed.
We constructed and trained 21 self-attention based encoder models and five
convolution neural networks to identify gravitational lenses from the Bologna
Lens Challenge. Each model was trained separately using 18,000 simulated
images, cross-validated using 2,000 images, and then applied to a test set with
100,000 images. We used four different metrics for evaluation: classification
accuracy, area under the receiver operating characteristic curve (AUROC), the
TPR$_0$ score and the TPR$_{10}$ score. The performances of
self-attention-based encoder models and CNNs participating in the challenge are
compared. They were able to surpass the CNN models that participated in the
Bologna Lens Challenge by a high margin for the $TPR_0$ and $TPR_${10}$.
Self-Attention based models have clear advantages compared to simpler CNNs.
They have highly competing performance in comparison to the currently used
residual neural networks. Compared to CNNs, self-attention based models can
identify highly confident lensing candidates and will be able to filter out
potential candidates from real data. Moreover, introducing the encoder layers
can also tackle the over-fitting problem present in the CNNs by acting as
effective filters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11430">
<div class="article-summary-box-inner">
<span><p>What constitutes an object? This has been a long-standing question in
computer vision. Towards this goal, numerous learning-free and learning-based
approaches have been developed to score objectness. However, they generally do
not scale well across new domains and novel objects. In this paper, we advocate
that existing methods lack a top-down supervision signal governed by
human-understandable semantics. For the first time in literature, we
demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned
image-text pairs can effectively bridge this gap. Our extensive experiments
across various domains and novel objects show the state-of-the-art performance
of MViTs to localize generic objects in images. Based on the observation that
existing MViTs do not include multi-scale feature processing and usually
require longer training schedules, we develop an efficient MViT architecture
using multi-scale deformable attention and late vision-language fusion. We show
the significance of MViT proposals in a diverse range of applications including
open-world object detection, salient and camouflage object detection,
supervised and self-supervised detection tasks. Further, MViTs can adaptively
generate proposals given a specific language query and thus offer enhanced
interactability. Code: \url{https://git.io/J1HPY}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attack on Facial Soft-biometric Privacy Enhancement. (arXiv:2111.12405v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12405">
<div class="article-summary-box-inner">
<span><p>In the recent past, different researchers have proposed privacy-enhancing
face recognition systems designed to conceal soft-biometric attributes at
feature level. These works have reported impressive results, but generally did
not consider specific attacks in their analysis of privacy protection. We
introduce an attack on said schemes based on two observations: (1) highly
similar facial representations usually originate from face images with similar
soft-biometric attributes; (2) to achieve high recognition accuracy, robustness
against intra-class variations within facial representations has to be retained
in their privacy-enhanced versions. The presented attack only requires the
privacy-enhancing algorithm as a black-box and a relatively small database of
face images with annotated soft-biometric attributes. Firstly, an intercepted
privacy-enhanced face representation is compared against the attacker's
database. Subsequently, the unknown attribute is inferred from the attributes
associated with the highest obtained similarity scores. In the experiments, the
attack is applied against two state-of-the-art approaches. The attack is shown
to circumvent the privacy enhancement to a considerable degree and is able to
correctly classify gender with an accuracy of up to approximately 90%. Future
works on privacy-enhancing face recognition are encouraged to include the
proposed attack in evaluations on the privacy protection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning. (arXiv:2111.13196v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13196">
<div class="article-summary-box-inner">
<span><p>The canonical approach to video captioning dictates a caption generation
model to learn from offline-extracted dense video features. These feature
extractors usually operate on video frames sampled at a fixed frame rate and
are often trained on image/video understanding tasks, without adaption to video
captioning data. In this work, we present SwinBERT, an end-to-end
transformer-based model for video captioning, which takes video frame patches
directly as inputs, and outputs a natural language description. Instead of
leveraging multiple 2D/3D feature extractors, our method adopts a video
transformer to encode spatial-temporal representations that can adapt to
variable lengths of video input without dedicated design for different frame
rates. Based on this model architecture, we show that video captioning can
benefit significantly from more densely sampled video frames as opposed to
previous successes with sparsely sampled video frames for video-and-language
understanding tasks (e.g., video question answering). Moreover, to avoid the
inherent redundancy in consecutive video frames, we propose adaptively learning
a sparse attention mask and optimizing it for task-specific performance
improvement through better long-range video sequence modeling. Through
extensive experiments on 5 video captioning datasets, we show that SwinBERT
achieves across-the-board performance improvements over previous methods, often
by a large margin. The learned sparse attention masks in addition push the
limit to new state of the arts, and can be transferred between different video
lengths and between different datasets. Code is available at
https://github.com/microsoft/SwinBERT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Generalization by Learning a Bridge Across Domains. (arXiv:2112.02300v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02300">
<div class="article-summary-box-inner">
<span><p>The ability to generalize learned representations across significantly
different visual domains, such as between real photos, clipart, paintings, and
sketches, is a fundamental capacity of the human visual system. In this paper,
different from most cross-domain works that utilize some (or full) source
domain supervision, we approach a relatively new and very practical
Unsupervised Domain Generalization (UDG) setup of having no training
supervision in neither source nor target domains. Our approach is based on
self-supervised learning of a Bridge Across Domains (BrAD) - an auxiliary
bridge domain accompanied by a set of semantics preserving visual
(image-to-image) mappings to BrAD from each of the training domains. The BrAD
and mappings to it are learned jointly (end-to-end) with a contrastive
self-supervised representation model that semantically aligns each of the
domains to its BrAD-projection, and hence implicitly drives all the domains
(seen or unseen) to semantically align to each other. In this work, we show how
using an edge-regularized BrAD our approach achieves significant gains across
multiple benchmarks and a range of tasks, including UDG, Few-shot UDA, and
unsupervised generalization across multi-domain datasets (including
generalization to unseen domains and classes).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Unsupervised Stain-To-Stain Translation using Self-Supervision and Meta-Learning. (arXiv:2112.08837v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08837">
<div class="article-summary-box-inner">
<span><p>In digital pathology, many image analysis tasks are challenged by the need
for large and time-consuming manual data annotations to cope with various
sources of variability in the image domain. Unsupervised domain adaptation
based on image-to-image translation is gaining importance in this field by
addressing variabilities without the manual overhead. Here, we tackle the
variation of different histological stains by unsupervised stain-to-stain
translation to enable a stain-independent applicability of a deep learning
segmentation model. We use CycleGANs for stain-to-stain translation in kidney
histopathology, and propose two novel approaches to improve translational
effectivity. First, we integrate a prior segmentation network into the CycleGAN
for a self-supervised, application-oriented optimization of translation through
semantic guidance, and second, we incorporate extra channels to the translation
output to implicitly separate artificial meta-information otherwise encoded for
tackling underdetermined reconstructions. The latter showed partially superior
performances to the unmodified CycleGAN, but the former performed best in all
stains providing instance-level Dice scores ranging between 78% and 92% for
most kidney structures, such as glomeruli, tubules, and veins. However,
CycleGANs showed only limited performance in the translation of other
structures, e.g. arteries. Our study also found somewhat lower performance for
all structures in all stains when compared to segmentation in the original
stain. Our study suggests that with current unsupervised technologies, it seems
unlikely to produce generally applicable simulated stains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V-LinkNet: Learning Contextual Inpainting Across Latent Space of Generative Adversarial Network. (arXiv:2201.00323v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00323">
<div class="article-summary-box-inner">
<span><p>Image inpainting is a key technique in image processing task to predict the
missing regions and generate realistic images. Given the advancement of
existing generative inpainting models with feature extraction, propagation and
reconstruction capabilities, there is lack of high-quality feature extraction
and transfer mechanisms in deeper layers to tackle persistent aberrations on
the generated inpainted regions. Our method, V-LinkNet, develops high-level
feature transference to deep level textural context of inpainted regions our
work, proposes a novel technique of combining encoders learning through a
recursive residual transition layer (RSTL). The RSTL layer easily adapts dual
encoders by increasing the unique semantic information through direct
communication. By collaborating the dual encoders structure with contextualised
feature representation loss function, our system gains the ability to inpaint
with high-level features. To reduce biases from random mask-image pairing, we
introduce a standard protocol with paired mask-image on the testing set of
CelebA-HQ, Paris Street View and Places2 datasets. Our results show V-LinkNet
performed better on CelebA-HQ and Paris Street View using this standard
protocol. We will share the standard protocol and our codes with the research
community upon acceptance of this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Quantum-Classical Algorithm for Robust Fitting. (arXiv:2201.10110v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10110">
<div class="article-summary-box-inner">
<span><p>Fitting geometric models onto outlier contaminated data is provably
intractable. Many computer vision systems rely on random sampling heuristics to
solve robust fitting, which do not provide optimality guarantees and error
bounds. It is therefore critical to develop novel approaches that can bridge
the gap between exact solutions that are costly, and fast heuristics that offer
no quality assurances. In this paper, we propose a hybrid quantum-classical
algorithm for robust fitting. Our core contribution is a novel robust fitting
formulation that solves a sequence of integer programs and terminates with a
global solution or an error bound. The combinatorial subproblems are amenable
to a quantum annealer, which helps to tighten the bound efficiently. While our
usage of quantum computing does not surmount the fundamental intractability of
robust fitting, by providing error bounds our algorithm is a practical
improvement over randomised heuristics. Moreover, our work represents a
concrete application of quantum computing in computer vision. We present
results obtained using an actual quantum computer (D-Wave Advantage) and via
simulation. Source code: https://github.com/dadung/HQC-robust-fitting
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images. (arXiv:2201.12785v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12785">
<div class="article-summary-box-inner">
<span><p>Transformer, benefiting from global (long-range) information modeling using
self-attention mechanism, has been successful in natural language processing
and computer vision recently. Convolutional Neural Networks, capable of
capturing local features, are difficult to model explicit long-distance
dependencies from global feature space. However, both local and global features
are crucial for dense prediction tasks, especially for 3D medical image
segmentation. In this paper, we present the further attempt to exploit
Transformer in 3D CNN for 3D medical image volumetric segmentation and propose
a novel network named TransBTSV2 based on the encoder-decoder structure.
Different from TransBTS, the proposed TransBTSV2 is not limited to brain tumor
segmentation (BTS) but focuses on general medical image segmentation, providing
a stronger and more efficient 3D baseline for volumetric segmentation of
medical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can
achieve accurate segmentation of medical images without any pre-training,
possessing the strong inductive bias as CNNs and powerful global context
modeling ability as Transformer. With the proposed insight to redesign the
internal structure of Transformer block and the introduced Deformable
Bottleneck Module to capture shape-aware local details, a highly efficient
architecture is achieved with superior performance. Extensive experimental
results on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and
KiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results
compared to the state-of-the-art methods for the segmentation of brain tumor,
liver tumor as well as kidney tumor. Code will be publicly available at
https://github.com/Wenxuan-1119/TransBTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines. (arXiv:2202.08340v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08340">
<div class="article-summary-box-inner">
<span><p>Early in development, children learn to extend novel category labels to
objects with the same shape, a phenomenon known as the shape bias. Inspired by
these findings, Geirhos et al. (2019) examined whether deep neural networks
show a shape or texture bias by constructing images with conflicting shape and
texture cues. They found that convolutional neural networks strongly preferred
to classify familiar objects based on texture as opposed to shape, suggesting a
texture bias. However, there are a number of differences between how the
networks were tested in this study versus how children are typically tested. In
this work, we re-examine the inductive biases of neural networks by adapting
the stimuli and procedure from Geirhos et al. (2019) to more closely follow the
developmental paradigm and test on a wide range of pre-trained neural networks.
Across three experiments, we find that deep neural networks exhibit a
preference for shape rather than texture when tested under conditions that more
closely replicate the developmental procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Roto-Translation Equivariant Super-Resolution of Two-Dimensional Flows Using Convolutional Neural Networks. (arXiv:2202.11099v3 [physics.flu-dyn] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11099">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) often apparently process vectors as
quantities that have no direction such as colors in images. This study
investigates the effect of considering vectors as geometric objects in terms of
super-resolution of velocity on two-dimensional fluids. Vector is distinguished
from scalar by the transformation law associated with a change in basis, which
can be incorporated as prior knowledge using the equivariant deep learning. The
existing CNNs are converted into equivariant ones by rendering each layer
equivariant with respect to rotation and translation. The training data in the
high- and low-resolution are generated with the fluid simulation and
downsampling, respectively. The inference of the equivariant CNNs is not highly
accurate or robust, compared with the conventional CNNs. The conventional CNNs
can learn the equivariance and recognize vector directions, adapting to the
symmetry of data. In contrast, the equivariant CNNs do not have this
flexibility and their inference can be sensitive to the method of data
generation. The main advantage of equivariant CNNs is the trainability with a
smaller size of data due to the reduction in the parameters. The conclusion of
this paper is negative toward the use of equivariant CNNs in super-resolution
tasks, which is in contrast to previous studies that apply equivariant neural
networks to other fluid mechanics tasks. The effect of incorporating the
geometric equivariance in neural networks has yet to be sufficiently explored.
It will be necessary to conduct super-resolution with more general
configurations such as flows on irregular grids.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoVISQ: Reduction of Video Service Quality via Adversarial Attacks on Deep Learning-based Video Compression. (arXiv:2203.10183v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10183">
<div class="article-summary-box-inner">
<span><p>Video compression plays a crucial role in video streaming and classification
systems by maximizing the end-user quality of experience (QoE) at a given
bandwidth budget. In this paper, we conduct the first systematic study for
adversarial attacks on deep learning-based video compression and downstream
classification systems. Our attack framework, dubbed RoVISQ, manipulates the
Rate-Distortion (R-D) relationship of a video compression model to achieve one
or both of the following goals: (1) increasing the network bandwidth, (2)
degrading the video quality for end-users. We further devise new objectives for
targeted and untargeted attacks to a downstream video classification service.
Finally, we design an input-invariant perturbation that universally disrupts
video compression and classification systems in real time. Unlike previously
proposed attacks on video classification, our adversarial perturbations are the
first to withstand compression. We empirically show the resilience of RoVISQ
attacks against various defenses, i.e., adversarial training, video denoising,
and JPEG compression. Our extensive experimental results on various video
datasets show RoVISQ attacks deteriorate peak signal-to-noise ratio by up to
5.6dB and the bit-rate by up to 2.4 times while achieving over 90% attack
success rate on a downstream classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality-Balanced Embedding for Video Retrieval. (arXiv:2204.08182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08182">
<div class="article-summary-box-inner">
<span><p>Video search has become the main routine for users to discover videos
relevant to a text query on large short-video sharing platforms. During
training a query-video bi-encoder model using online search logs, we identify a
modality bias phenomenon that the video encoder almost entirely relies on text
matching, neglecting other modalities of the videos such as vision, audio. This
modality imbalanceresults from a) modality gap: the relevance between a query
and a video text is much easier to learn as the query is also a piece of text,
with the same modality as the video text; b) data bias: most training samples
can be solved solely by text matching. Here we share our practices to improve
the first retrieval stage including our solution for the modality imbalance
issue. We propose MBVR (short for Modality Balanced Video Retrieval) with two
key components: manually generated modality-shuffled (MS) samples and a dynamic
margin (DM) based on visual relevance. They can encourage the video encoder to
pay balanced attentions to each modality. Through extensive experiments on a
real world dataset, we show empirically that our method is both effective and
efficient in solving modality bias problem. We have also deployed our MBVR in a
large video platform and observed statistically significant boost over a highly
optimized baseline in an A/B test and manual GSB evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision-language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 6th AI City Challenge. (arXiv:2204.10380v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10380">
<div class="article-summary-box-inner">
<span><p>The 6th edition of the AI City Challenge specifically focuses on problems in
two domains where there is tremendous unlocked potential at the intersection of
computer vision and artificial intelligence: Intelligent Traffic Systems (ITS),
and brick and mortar retail businesses. The four challenge tracks of the 2022
AI City Challenge received participation requests from 254 teams across 27
countries. Track 1 addressed city-scale multi-target multi-camera (MTMC)
vehicle tracking. Track 2 addressed natural-language-based vehicle track
retrieval. Track 3 was a brand new track for naturalistic driving analysis,
where the data were captured by several cameras mounted inside the vehicle
focusing on driver safety, and the task was to classify driver actions. Track 4
was another new track aiming to achieve retail store automated checkout using
only a single view camera. We released two leader boards for submissions based
on different methods, including a public leader board for the contest, where no
use of external data is allowed, and a general leader board for all submitted
results. The top performance of participating teams established strong
baselines and even outperformed the state-of-the-art in the proposed challenge
tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequencer: Deep LSTM for Image Classification. (arXiv:2205.01972v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01972">
<div class="article-summary-box-inner">
<span><p>In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating the Training of Video Super-Resolution Models. (arXiv:2205.05069v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05069">
<div class="article-summary-box-inner">
<span><p>Despite that convolution neural networks (CNN) have recently demonstrated
high-quality reconstruction for video super-resolution (VSR), efficiently
training competitive VSR models remains a challenging problem. It usually takes
an order of magnitude more time than training their counterpart image models,
leading to long research cycles. Existing VSR methods typically train models
with fixed spatial and temporal sizes from beginning to end. The fixed sizes
are usually set to large values for good performance, resulting to slow
training. However, is such a rigid training strategy necessary for VSR? In this
work, we show that it is possible to gradually train video models from small to
large spatial/temporal sizes, i.e., in an easy-to-hard manner. In particular,
the whole training is divided into several stages and the earlier stage has
smaller training spatial shape. Inside each stage, the temporal size also
varies from short to long while the spatial size remains unchanged. Training is
accelerated by such a multigrid training strategy, as most of computation is
performed on smaller spatial and shorter temporal shapes. For further
acceleration with GPU parallelization, we also investigate the large minibatch
training without the loss in accuracy. Extensive experiments demonstrate that
our method is capable of largely speeding up training (up to $6.2\times$
speedup in wall-clock training time) without performance drop for various VSR
models. The code is available at
https://github.com/TencentARC/Efficient-VSR-Training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Depth Completion: A Survey. (arXiv:2205.05335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05335">
<div class="article-summary-box-inner">
<span><p>Depth completion aims at predicting dense pixel-wise depth from a sparse map
captured from a depth sensor. It plays an essential role in various
applications such as autonomous driving, 3D reconstruction, augmented reality,
and robot navigation. Recent successes on the task have been demonstrated and
dominated by deep learning based solutions. In this article, for the first
time, we provide a comprehensive literature review that helps readers better
grasp the research trends and clearly understand the current advances. We
investigate the related studies from the design aspects of network
architectures, loss functions, benchmark datasets, and learning strategies with
a proposal of a novel taxonomy that categorizes existing methods. Besides, we
present a quantitative comparison of model performance on two widely used
benchmark datasets, including an indoor and an outdoor dataset. Finally, we
discuss the challenges of prior works and provide readers with some insights
for future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning. (arXiv:2205.06401v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06401">
<div class="article-summary-box-inner">
<span><p>Contrastive learning pre-trains an image encoder using a large amount of
unlabeled data such that the image encoder can be used as a general-purpose
feature extractor for various downstream tasks. In this work, we propose
PoisonedEncoder, a data poisoning attack to contrastive learning. In
particular, an attacker injects carefully crafted poisoning inputs into the
unlabeled pre-training data, such that the downstream classifiers built based
on the poisoned encoder for multiple target downstream tasks simultaneously
classify attacker-chosen, arbitrary clean inputs as attacker-chosen, arbitrary
classes. We formulate our data poisoning attack as a bilevel optimization
problem, whose solution is the set of poisoning inputs; and we propose a
contrastive-learning-tailored method to approximately solve it. Our evaluation
on multiple datasets shows that PoisonedEncoder achieves high attack success
rates while maintaining the testing accuracy of the downstream classifiers
built upon the poisoned encoder for non-attacker-chosen inputs. We also
evaluate five defenses against PoisonedEncoder, including one pre-processing,
three in-processing, and one post-processing defenses. Our results show that
these defenses can decrease the attack success rate of PoisonedEncoder, but
they also sacrifice the utility of the encoder or require a large clean
pre-training dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition. (arXiv:2205.07556v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07556">
<div class="article-summary-box-inner">
<span><p>We present an effective method for Intracranial Hemorrhage Detection (IHD)
which exceeds the performance of the winner solution in RSNA-IHD competition
(2019). Meanwhile, our model only takes quarter parameters and ten percent
FLOPs compared to the winner's solution. The IHD task needs to predict the
hemorrhage category of each slice for the input brain CT. We review the top-5
solutions for the IHD competition held by the Radiological Society of North
America(RSNA) in 2019. Nearly all the top solutions rely on 2D convolutional
networks and sequential models (Bidirectional GRU or LSTM) to extract
intra-slice and inter-slice features, respectively. All the top solutions
enhance the performance by leveraging the model ensemble, and the model number
varies from 7 to 31. In the past years, since much progress has been made in
the computer vision regime especially Transformer-based models, we introduce
the Transformer-based techniques to extract the features in both intra-slice
and inter-slice views for IHD tasks. Additionally, a semi-supervised method is
embedded into our workflow to further improve the performance. The code is
available in the manuscript.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-18 23:08:36.990895118 UTC">2022-05-18 23:08:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>