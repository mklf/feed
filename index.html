<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-15T01:30:00Z">10-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modelling via Learning to Rank. (arXiv:2110.06961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06961">
<div class="article-summary-box-inner">
<span><p>We consider language modelling (LM) as a multi-label structured prediction
task by re-framing training from solely predicting a single ground-truth word
to ranking a set of words which could continue a given context. To avoid
annotating top-$k$ ranks, we generate them using pre-trained LMs: GPT-2, BERT,
and Born-Again models. This leads to a rank-based form of knowledge
distillation (KD). We also develop a method using $N$-grams to create a
non-probabilistic teacher which generates the ranks without the need of a
pre-trained LM.
</p>
<p>We confirm the hypotheses that we can treat LMing as a ranking task and that
we can do so without the use of a pre-trained LM. We show that rank-based KD
generally improves perplexity (PPL), often with statistical significance, when
compared to Kullback-Leibler-based KD. Surprisingly, given the simplicity of
the method, $N$-grams act as competitive teachers and achieve similar
performance as using either BERT or a Born-Again model teachers. GPT-2 always
acts as the best teacher, though, and using it and a Transformer-XL student on
Wiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and
against a KL-based KD of 56.70.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Domain Question-Answering for COVID-19 and Other Emergent Domains. (arXiv:2110.06962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06962">
<div class="article-summary-box-inner">
<span><p>Since late 2019, COVID-19 has quickly emerged as the newest biomedical
domain, resulting in a surge of new information. As with other emergent
domains, the discussion surrounding the topic has been rapidly changing,
leading to the spread of misinformation. This has created the need for a public
space for users to ask questions and receive credible, scientific answers. To
fulfill this need, we turn to the task of open-domain question-answering, which
we can use to efficiently find answers to free-text questions from a large set
of documents. In this work, we present such a system for the emergent domain of
COVID-19. Despite the small data size available, we are able to successfully
train the system to retrieve answers from a large-scale corpus of published
COVID-19 scientific papers. Furthermore, we incorporate effective re-ranking
and question-answering techniques, such as document diversity and multiple
answer spans. Our open-domain question-answering system can further act as a
model for the quick development of similar systems that can be adapted and
modified for other developing emergent domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexiTerm: A more efficient implementation of flexible multi-word term recognition. (arXiv:2110.06981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06981">
<div class="article-summary-box-inner">
<span><p>Terms are linguistic signifiers of domain-specific concepts. Automated
recognition of multi-word terms (MWT) in free text is a sequence labelling
problem, which is commonly addressed using supervised machine learning methods.
Their need for manual annotation of training data makes it difficult to port
such methods across domains. FlexiTerm, on the other hand, is a fully
unsupervised method for MWT recognition from domain-specific corpora.
Originally implemented in Java as a proof of concept, it did not scale well,
thus offering little practical value in the context of big data. In this paper,
we describe its re-implementation in Python and compare the performance of
these two implementations. The results demonstrated major improvements in terms
of efficiency, which allow FlexiTerm to transition from the proof of concept to
the production-grade application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits. (arXiv:2110.06997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06997">
<div class="article-summary-box-inner">
<span><p>Training data for machine translation (MT) is often sourced from a multitude
of large corpora that are multi-faceted in nature, e.g. containing contents
from multiple domains or different levels of quality or complexity. Naturally,
these facets do not occur with equal frequency, nor are they equally important
for the test scenario at hand. In this work, we propose to optimize this
balance jointly with MT model parameters to relieve system developers from
manual schedule design. A multi-armed bandit is trained to dynamically choose
between facets in a way that is most beneficial for the MT system. We evaluate
it on three different multi-facet applications: balancing translationese and
natural training data, or data from multiple domains or multiple language
pairs. We find that bandit learning leads to competitive MT systems across
tasks, and our analysis provides insights into its learned strategies and the
underlying data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation. (arXiv:2110.07002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07002">
<div class="article-summary-box-inner">
<span><p>Text autoencoders are often used for unsupervised conditional text generation
by applying mappings in the latent space to change attributes to the desired
values. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these
mappings in the embedding space of an autoencoder. However, their method is
restricted to autoencoders with a single-vector embedding, which limits how
much information can be retained. We address this issue by extending their
method to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a
variable-size bag of vectors that grows with the size of the text, as in
attention-based models. This allows to encode and reconstruct much longer texts
than standard autoencoders. Analogous to conventional autoencoders, we propose
regularization techniques that facilitate learning meaningful operations in the
latent space. Finally, we adapt for a training scheme that learns to map an
input bag to an output bag, including a novel loss function and neural
architecture. Our experimental evaluations on unsupervised sentiment transfer
and sentence summarization show that our method performs substantially better
than a standard autoencoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of SVD and factorized TDNN approaches for speech to text. (arXiv:2110.07027v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07027">
<div class="article-summary-box-inner">
<span><p>This work concentrates on reducing the RTF and word error rate of a hybrid
HMM-DNN. Our baseline system uses an architecture with TDNN and LSTM layers. We
find this architecture particularly useful for lightly reverberated
environments. However, these models tend to demand more computation than is
desirable. In this work, we explore alternate architectures employing singular
value decomposition (SVD) is applied to the TDNN layers to reduce the RTF, as
well as to the affine transforms of every LSTM cell. We compare this approach
with specifying bottleneck layers similar to those introduced by SVD before
training. Additionally, we reduced the search space of the decoding graph to
make it a better fit to operate in real-time applications. We report -61.57%
relative reduction in RTF and almost 1% relative decrease in WER for our
architecture trained on Fisher data along with reverberated versions of this
dataset in order to match one of our target test distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Robustness to Variations of Objects and Instructions with a Neuro-Symbolic Approach for Interactive Instruction Following. (arXiv:2110.07031v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07031">
<div class="article-summary-box-inner">
<span><p>An interactive instruction following task has been proposed as a benchmark
for learning to map natural language instructions and first-person vision into
sequences of actions to interact with objects in a 3D simulated environment. We
find that an existing end-to-end neural model for this task is not robust to
variations of objects and language instructions. We assume that this problem is
due to the high sensitiveness of neural feature extraction to small changes in
vision and language inputs. To mitigate this problem, we propose a
neuro-symbolic approach that performs reasoning over high-level symbolic
representations that are robust to small changes in raw inputs. Our experiments
on the ALFRED dataset show that our approach significantly outperforms the
existing model by 18, 52, and 73 points in the success rate on the
ToggleObject, PickupObject, and SliceObject subtasks in unseen environments
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient NLP: A Standard Evaluation and A Strong Baseline. (arXiv:2110.07038v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07038">
<div class="article-summary-box-inner">
<span><p>Supersized pre-trained language models have pushed the accuracy of various
NLP tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless
SOTA accuracy, most works are pursuing improvement on other dimensions such as
efficiency, leading to "Pareto SOTA". Different from accuracy, the metric for
efficiency varies across different studies, making them hard to be fairly
compared. To that end, this work presents ELUE (Efficient Language
Understanding Evaluation), a standard evaluation, and a public leaderboard for
efficient NLP models. ELUE is dedicated to depicting the Pareto Front for
various language understanding tasks, such that it can tell whether and how
much a method achieves Pareto improvement. Along with the benchmark, we also
pre-train and release a strong baseline, ElasticBERT, whose elasticity is both
static and dynamic. ElasticBERT is static in that it allows reducing model
layers on demand. ElasticBERT is dynamic in that it selectively executes parts
of model layers conditioned on the input. We demonstrate the ElasticBERT,
despite its simplicity, outperforms or performs on par with SOTA compressed and
early exiting models. The ELUE benchmark is publicly available at
<a href="http://eluebenchmark.fastnlp.top/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning using lattice-free MMI for speech recognition. (arXiv:2110.07055v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07055">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL), or domain expansion, recently became a popular topic
for automatic speech recognition (ASR) acoustic modeling because practical
systems have to be updated frequently in order to work robustly on types of
speech not observed during initial training. While sequential adaptation allows
tuning a system to a new domain, it may result in performance degradation on
the old domains due to catastrophic forgetting. In this work we explore
regularization-based CL for neural network acoustic models trained with the
lattice-free maximum mutual information (LF-MMI) criterion. We simulate domain
expansion by incrementally adapting the acoustic model on different public
datasets that include several accents and speaking styles. We investigate two
well-known CL techniques, elastic weight consolidation (EWC) and learning
without forgetting (LWF), which aim to reduce forgetting by preserving model
weights or network outputs. We additionally introduce a sequence-level LWF
regularization, which exploits posteriors from the denominator graph of LF-MMI
to further reduce forgetting. Empirical results show that the proposed
sequence-level LWF can improve the best average word error rate across all
domains by up to 9.4% relative compared with using regular LWF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIMICause : Defining, identifying and predicting types of causal relationships between biomedical concepts from clinical notes. (arXiv:2110.07090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07090">
<div class="article-summary-box-inner">
<span><p>Understanding of causal narratives communicated in clinical notes can help
make strides towards personalized healthcare. In this work, MIMICause, we
propose annotation guidelines, develop an annotated corpus and provide baseline
scores to identify types and direction of causal relations between a pair of
biomedical concepts in clinical notes; communicated implicitly or explicitly,
identified either in a single sentence or across multiple sentences.
</p>
<p>We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2
shared task dataset and train four different language model based
architectures. Annotation based on our guidelines achieved a high
inter-annotator agreement i.e. Fleiss' kappa score of 0.72 and our model for
identification of causal relation achieved a macro F1 score of 0.56 on test
data. The high inter-annotator agreement for clinical text shows the quality of
our annotation guidelines while the provided baseline F1 score sets the
direction for future research towards understanding narratives in clinical
texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Introductions in Podcast Episodes from Automatically Generated Transcripts. (arXiv:2110.07096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07096">
<div class="article-summary-box-inner">
<span><p>As the volume of long-form spoken-word content such as podcasts explodes,
many platforms desire to present short, meaningful, and logically coherent
segments extracted from the full content. Such segments can be consumed by
users to sample content before diving in, as well as used by the platform to
promote and recommend content. However, little published work is focused on the
segmentation of spoken-word content, where the errors (noise) in transcripts
generated by automatic speech recognition (ASR) services poses many challenges.
Here we build a novel dataset of complete transcriptions of over 400 podcast
episodes, in which we label the position of introductions in each episode.
These introductions contain information about the episodes' topics, hosts, and
guests, providing a valuable summary of the episode content, as it is created
by the authors. We further augment our dataset with word substitutions to
increase the amount of available training data. We train three Transformer
models based on the pre-trained BERT and different augmentation strategies,
which achieve significantly better performance compared with a static embedding
model, showing that it is possible to capture generalized, larger-scale
structural information from noisy, loosely-organized speech data. This is
further demonstrated through an analysis of the models' inner architecture. Our
methods and dataset can be used to facilitate future work on the
structure-based segmentation of spoken-word content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A CLIP-Enhanced Method for Video-Language Understanding. (arXiv:2110.07137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07137">
<div class="article-summary-box-inner">
<span><p>This technical report summarizes our method for the Video-And-Language
Understanding Evaluation (VALUE) challenge
(https://value-benchmark.github.io/challenge\_2021.html). We propose a
CLIP-Enhanced method to incorporate the image-text pretrained knowledge into
downstream video-text tasks. Combined with several other improved designs, our
method outperforms the state-of-the-art by $2.4\%$ ($57.58$ to $60.00$)
Meta-Ave score on VALUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. (arXiv:2110.07139v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07139">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks and backdoor attacks are two common security threats that
hang over deep learning. Both of them harness task-irrelevant features of data
in their implementation. Text style is a feature that is naturally irrelevant
to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In
this paper, we make the first attempt to conduct adversarial and backdoor
attacks based on text style transfer, which is aimed at altering the style of a
sentence while preserving its meaning. We design an adversarial attack method
and a backdoor attack method, and conduct extensive experiments to evaluate
them. Experimental results show that popular NLP models are vulnerable to both
adversarial and backdoor attacks based on text style transfer -- the attack
success rates can exceed 90% without much effort. It reflects the limited
ability of NLP models to handle the feature of text style that has not been
widely realized. In addition, the style transfer-based adversarial and backdoor
attack methods show superiority to baselines in many aspects. All the code and
data of this paper can be obtained at https://github.com/thunlp/StyleAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">bert2BERT: Towards Reusable Pretrained Language Models. (arXiv:2110.07143v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07143">
<div class="article-summary-box-inner">
<span><p>In recent years, researchers tend to pre-train ever-larger language models to
explore the upper limit of deep models. However, large language model
pre-training costs intensive computational resources and most of the models are
trained from scratch without reusing the existing pre-trained models, which is
wasteful. In this paper, we propose bert2BERT, which can effectively transfer
the knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a
large model (e.g., BERT_LARGE) through parameter initialization and
significantly improve the pre-training efficiency of the large model.
Specifically, we extend the previous function-preserving on Transformer-based
language model, and further improve it by proposing advanced knowledge for
large model's initialization. In addition, a two-stage pre-training method is
proposed to further accelerate the training process. We did extensive
experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that
(1) our method can save a significant amount of training cost compared with
baselines including learning from scratch, StackBERT and MSLT; (2) our method
is generic and applicable to different types of pre-trained models. In
particular, bert2BERT saves about 45% and 47% computational cost of
pre-training BERT_BASE and GPT_BASE by reusing the models of almost their half
sizes. The source code will be publicly available upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual GenQA: A Language-Agnostic Generative Question Answering Approach for Open-Domain Question Answering. (arXiv:2110.07150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07150">
<div class="article-summary-box-inner">
<span><p>Open-Retrieval Generative Question Answering (GenQA) is proven to deliver
high-quality, natural-sounding answers in English. In this paper, we present
the first generalization of the GenQA approach for the multilingual
environment. To this end, we present the GenTyDiQA dataset, which extends the
TyDiQA evaluation data (Clark et al., 2020) with natural-sounding, well-formed
answers in Arabic, Bengali, English, Japanese, and Russian. For all these
languages, we show that a GenQA sequence-to-sequence-based model outperforms a
state-of-the-art Answer Sentence Selection model. We also show that a
multilingually-trained model competes with, and in some cases outperforms, its
monolingual counterparts. Finally, we show that our system can even compete
with strong baselines, even when fed with information from a variety of
languages. Essentially, our system is able to answer a question in any language
of our language set using information from many languages, making it the first
Language-Agnostic GenQA system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causally Estimating the Sensitivity of Neural NLP Models to Spurious Features. (arXiv:2110.07159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07159">
<div class="article-summary-box-inner">
<span><p>Recent work finds modern natural language processing (NLP) models relying on
spurious features for prediction. Mitigating such effects is thus important.
Despite this need, there is no quantitative measure to evaluate or compare the
effects of different forms of spurious features in NLP. We address this gap in
the literature by quantifying model sensitivity to spurious features with a
causal estimand, dubbed CENT, which draws on the concept of average treatment
effect from the causality literature. By conducting simulations with four
prominent NLP models -- TextRNN, BERT, RoBERTa and XLNet -- we rank the models
against their sensitivity to artificial injections of eight spurious features.
We further hypothesize and validate that models that are more sensitive to a
spurious feature will be less robust against perturbations with this feature
during inference. Conversely, data augmentation with this feature improves
robustness to similar perturbations. We find statistically significant inverse
correlations between sensitivity and robustness, providing empirical support
for our hypothesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence. (arXiv:2110.07160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07160">
<div class="article-summary-box-inner">
<span><p>This paper proposes a transformer over transformer framework, called
Transformer$^2$, to perform neural text segmentation. It consists of two
components: bottom-level sentence encoders using pre-trained transformers, and
an upper-level transformer-based segmentation model based on the sentence
embeddings. The bottom-level component transfers the pre-trained knowledge
learned from large external corpora under both single and pair-wise supervised
NLP tasks to model the sentence embeddings for the documents. Given the
sentence embeddings, the upper-level transformer is trained to recover the
segmentation boundaries as well as the topic labels of each sentence. Equipped
with a multi-task loss and the pre-trained knowledge, Transformer$^2$ can
better capture the semantic coherence within the same segments. Our experiments
show that (1) Transformer$^2$ manages to surpass state-of-the-art text
segmentation models in terms of a commonly-used semantic coherence measure; (2)
in most cases, both single and pair-wise pre-trained knowledge contribute to
the model performance; (3) bottom-level sentence encoders pre-trained on
specific languages yield better performance than those pre-trained on specific
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Attention-Aware Hierarchical Topic Model. (arXiv:2110.07161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07161">
<div class="article-summary-box-inner">
<span><p>Neural topic models (NTMs) apply deep neural networks to topic modelling.
Despite their success, NTMs generally ignore two important aspects: (1) only
document-level word count information is utilized for the training, while more
fine-grained sentence-level information is ignored, and (2) external semantic
knowledge regarding documents, sentences and words are not exploited for the
training. To address these issues, we propose a variational autoencoder (VAE)
NTM model that jointly reconstructs the sentence and document word counts using
combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic
embeddings. The pre-trained embeddings are first transformed into a common
latent topical space to align their semantics with the BoW embeddings. Our
model also features hierarchical KL divergence to leverage embeddings of each
document to regularize those of their sentences, thereby paying more attention
to semantically relevant sentences. Both quantitative and qualitative
experiments have shown the efficacy of our model in 1) lowering the
reconstruction errors at both the sentence and document levels, and 2)
discovering more coherent topics from real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07165">
<div class="article-summary-box-inner">
<span><p>Analysis of vision-and-language models has revealed their brittleness under
linguistic phenomena such as paraphrasing, negation, textual entailment, and
word substitutions with synonyms or antonyms. While data augmentation
techniques have been designed to mitigate against these failure modes, methods
that can integrate this knowledge into the training pipeline remain
under-explored. In this paper, we present \textbf{SDRO}, a model-agnostic
method that utilizes a set linguistic transformations in a distributed robust
optimization setting, along with an ensembling technique to leverage these
transformations during inference. Experiments on benchmark datasets with images
(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as
robustness to adversarial attacks. Experiments on binary VQA explore the
generalizability of this method to other V\&amp;L tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization. (arXiv:2110.07166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07166">
<div class="article-summary-box-inner">
<span><p>Neural abstractive summarization models are susceptible to generating
factually inconsistent content, a phenomenon known as hallucination. This
limits the usability and adoption of these systems in real-world applications.
To reduce the presence of hallucination, we propose the Mixture of Factual
Experts (MoFE) model, which combines multiple summarization experts that each
target a specific type of error. We train our experts using reinforcement
learning (RL) to minimize the error defined by two factual consistency metrics:
entity overlap and dependency arc entailment. We construct MoFE by combining
the experts using two ensembling strategies (weights and logits) and evaluate
them on two summarization datasets (XSUM and CNN/DM). Our experiments on BART
models show that the MoFE improves performance according to both entity overlap
and dependency arc entailment, without a significant performance drop on
standard ROUGE metrics. The performance improvement also transfers to unseen
factual consistency metrics, such as question answer-based factuality
evaluation metric and BERTScore precision with respect to the source document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-gloss Augmentation for Improving Word Sense Disambiguation. (arXiv:2110.07174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07174">
<div class="article-summary-box-inner">
<span><p>The goal of Word Sense Disambiguation (WSD) is to identify the sense of a
polysemous word in a specific context. Deep-learning techniques using BERT have
achieved very promising results in the field and different methods have been
proposed to integrate structured knowledge to enhance performance. At the same
time, an increasing number of data augmentation techniques have been proven to
be useful for NLP tasks. Building upon previous works leveraging BERT and
WordNet knowledge, we explore different data augmentation techniques on
context-gloss pairs to improve the performance of WSD. In our experiment, we
show that both sentence-level and word-level augmentation methods are effective
strategies for WSD. Also, we find out that performance can be improved by
adding hypernyms' glosses obtained from a lexical knowledge base. We compare
and analyze different context-gloss augmentation techniques, and the results
show that applying back translation on gloss performs the best.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. (arXiv:2110.07178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07178">
<div class="article-summary-box-inner">
<span><p>The common practice for training commonsense models has gone
from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in
order to train commonsense models. In this work, we investigate an alternative,
from-machine-to-corpus-to-machine: general language models author these
commonsense knowledge graphs to train commonsense models. Our study leads to a
new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge
Distillation (Hinton et al., 2015), our approach uses larger models to teach
smaller models. A key difference is that we distill knowledge symbolically-as
text-in addition to the neural model. We also distill only one aspect-the
commonsense of a general language model teacher, allowing the student to be a
different type, a commonsense model. Altogether, we show that careful prompt
engineering and a separately trained critic model allow us to selectively
distill high-quality causal commonsense from GPT-3, a general language model.
Empirical results demonstrate that, for the first time, a human-authored
commonsense knowledge graph is surpassed by our automatically distilled variant
in all three criteria: quantity, quality, and diversity. In addition, it
results in a neural commonsense model that surpasses the teacher model's
commonsense capabilities despite its 100x smaller size. We apply this to the
ATOMIC resource, and share our new symbolic knowledge graph and commonsense
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting IPA-based Cross-lingual Text-to-speech. (arXiv:2110.07187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07187">
<div class="article-summary-box-inner">
<span><p>International Phonetic Alphabet (IPA) has been widely used in cross-lingual
text-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,
IPA itself has been understudied in cross-lingual TTS. In this paper, we report
some empirical findings of building a cross-lingual TTS model using IPA as
inputs. Experiments show that the way to process the IPA and suprasegmental
sequence has a negligible impact on the CL VC performance. Furthermore, we find
that using a dataset including one speaker per language to build an IPA-based
TTS system would fail CL VC since the language-unique IPA and tone/stress
symbols could leak the speaker information. In addition, we experiment with
different combinations of speakers in the training dataset to further
investigate the effect of the number of speakers on the CL VC performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling. (arXiv:2110.07198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07198">
<div class="article-summary-box-inner">
<span><p>Although large-scale pre-trained neural models have shown impressive
performances in a variety of tasks, their ability to generate coherent text
that appropriately models discourse phenomena is harder to evaluate and less
understood. Given the claims of improved text generation quality across various
systems, we consider the coherence evaluation of machine generated text to be
one of the principal applications of coherence models that needs to be
investigated. We explore training data and self-supervision objectives that
result in a model that generalizes well across tasks and can be used
off-the-shelf to perform such evaluations. Prior work in neural coherence
modeling has primarily focused on devising new architectures, and trained the
model to distinguish coherent and incoherent text through pairwise
self-supervision on the permuted documents task. We instead use a basic model
architecture and show significant improvements over state of the art within the
same training regime. We then design a harder self-supervision objective by
increasing the ratio of negative samples within a contrastive learning setup,
and enhance the model further through automatic hard negative mining coupled
with a large global negative queue encoded by a momentum encoder. We show
empirically that increasing the density of negative samples improves the basic
model, and using a global negative queue further improves and stabilizes the
model while training with hard negative samples. We evaluate the coherence
model on task-independent test sets that resemble real-world use cases and show
significant improvements in coherence evaluations of downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechT5: Unified-Modal Encoder-Decoder Pre-training for Spoken Language Processing. (arXiv:2110.07205v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07205">
<div class="article-summary-box-inner">
<span><p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in
pre-training natural language processing models, we propose a unified-modal
SpeechT5 framework that explores the encoder-decoder pre-training for
self-supervised speech/text representation learning. The SpeechT5 framework
consists of a shared encoder-decoder network and six modal-specific
(speech/text) pre/post-nets. After preprocessing the speech/text input through
the pre-nets, the shared encoder-decoder network models the sequence to
sequence transformation, and then the post-nets generate the output in the
speech/text modality based on the decoder output. Particularly, SpeechT5 can
pre-train on a large scale of unlabeled speech and text data to improve the
capability of the speech and textual modeling. To align the textual and speech
information into a unified semantic space, we propose a cross-modal vector
quantization method with random mixing-up to bridge speech and text. Extensive
evaluations on a wide variety of spoken language processing tasks, including
voice conversion, automatic speech recognition, text to speech, and speaker
identification, show the superiority of the proposed SpeechT5 framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Attention Neural Network for Pun Location and Using Pun-Gloss Pairs for Interpretation. (arXiv:2110.07209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07209">
<div class="article-summary-box-inner">
<span><p>Pun location is to identify the punning word (usually a word or a phrase that
makes the text ambiguous) in a given short text, and pun interpretation is to
find out two different meanings of the punning word. Most previous studies
adopt limited word senses obtained by WSD(Word Sense Disambiguation) technique
or pronunciation information in isolation to address pun location. For the task
of pun interpretation, related work pays attention to various WSD algorithms.
In this paper, a model called DANN (Dual-Attentive Neural Network) is proposed
for pun location, effectively integrates word senses and pronunciation with
context information to address two kinds of pun at the same time. Furthermore,
we treat pun interpretation as a classification task and construct pungloss
pairs as processing data to solve this task. Experiments on the two benchmark
datasets show that our proposed methods achieve new state-of-the-art results.
Our source code is available in the public code repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data. (arXiv:2110.07210v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07210">
<div class="article-summary-box-inner">
<span><p>Recently, sequence-to-sequence (seq-to-seq) models have been successfully
applied in text-to-speech (TTS) to synthesize speech for single-language text.
To synthesize speech for multiple languages usually requires multi-lingual
speech from the target speaker. However, it is both laborious and expensive to
collect high-quality multi-lingual TTS data for the target speakers. In this
paper, we proposed to use low-quality code-switched found data from the
non-target speakers to achieve cross-lingual voice cloning for the target
speakers. Experiments show that our proposed method can generate high-quality
code-switched speech in the target voices in terms of both naturalness and
speaker consistency. More importantly, we find that our method can achieve a
comparable result to the state-of-the-art (SOTA) performance in cross-lingual
voice cloning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Transformers Perform Below Chance on Recursive Nested Constructions, Unlike Humans. (arXiv:2110.07240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07240">
<div class="article-summary-box-inner">
<span><p>Recursive processing is considered a hallmark of human linguistic abilities.
A recent study evaluated recursive processing in recurrent neural language
models (RNN-LMs) and showed that such models perform below chance level on
embedded dependencies within nested constructions -- a prototypical example of
recursion in natural language. Here, we study if state-of-the-art Transformer
LMs do any better. We test four different Transformer LMs on two different
types of nested constructions, which differ in whether the embedded (inner)
dependency is short or long range. We find that Transformers achieve
near-perfect performance on short-range embedded dependencies, significantly
better than previous results reported for RNN-LMs and humans. However, on
long-range embedded dependencies, Transformers' performance sharply drops below
chance level. Remarkably, the addition of only three words to the embedded
dependency caused Transformers to fall from near-perfect to below-chance
performance. Taken together, our results reveal Transformers' shortcoming when
it comes to recursive, structure-based, processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Chinese Biomedical Language Models via Multi-Level Text Discrimination. (arXiv:2110.07244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07244">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized
the field of NLP, not only in the general domain but also in the biomedical
domain. Most prior efforts in building biomedical PLMs have resorted simply to
domain adaptation and focused mainly on English. In this work we introduce
eHealth, a biomedical PLM in Chinese built with a new pre-training framework.
This new framework trains eHealth as a discriminator through both token-level
and sequence-level discrimination. The former is to detect input tokens
corrupted by a generator and select their original signals from plausible
candidates, while the latter is to further distinguish corruptions of a same
original sequence from those of the others. As such, eHealth can learn language
semantics at both the token and sequence levels. Extensive experiments on 11
Chinese biomedical language understanding tasks of various forms verify the
effectiveness and superiority of our approach. The pre-trained model is
available to the public at
\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and the
code will also be released later.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings. (arXiv:2110.07274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07274">
<div class="article-summary-box-inner">
<span><p>Many mispronunciation detection and diagnosis (MD&amp;D) research approaches try
to exploit both the acoustic and linguistic features as input. Yet the
improvement of the performance is limited, partially due to the shortage of
large amount annotated training data at the phoneme level. Phonetic embeddings,
extracted from ASR models trained with huge amount of word level annotations,
can serve as a good representation of the content of input speech, in a
noise-robust and speaker-independent manner. These embeddings, when used as
implicit phonetic supplementary information, can alleviate the data shortage of
explicit phoneme annotations. We propose to utilize Acoustic, Phonetic and
Linguistic (APL) embedding features jointly for building a more powerful MD\&amp;D
system. Experimental results obtained on the L2-ARCTIC database show the
proposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the
detection accuracy, diagnosis error rate and the F-measure, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts. (arXiv:2110.07280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07280">
<div class="article-summary-box-inner">
<span><p>Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of
the factual information extracted from Large Language Models (LLMs) depends on
the prompts used to query them. This inconsistency is problematic because
different users will query LLMs for the same information using different
wording, but should receive the same, accurate responses regardless. In this
work we aim to address this shortcoming by introducing P-Adapters: lightweight
models that sit between the embedding layer and first attention layer of LLMs.
They take LLM embeddings as input and output continuous prompts that are used
to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models
that learn a set of continuous prompts ("experts") and select one to query the
LLM. They require a separate classifier trained on human-annotated data to map
natural language prompts to the continuous ones. P-Adapters perform comparably
to the more complex MoE models in extracting factual information from BERT and
RoBERTa while eliminating the need for additional annotations. P-Adapters show
between 12-26% absolute improvement in precision and 36-50% absolute
improvement in consistency over a baseline of only using natural language
queries. Finally, we investigate what makes a P-adapter successful and conclude
that access to the LLM's embeddings of the original natural language prompt,
particularly the subject of the entity pair being asked about, is a significant
factor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07298">
<div class="article-summary-box-inner">
<span><p>Existing approaches to lifelong language learning rely on plenty of labeled
data for learning a new task, which is hard to obtain in most real scenarios.
Considering that humans can continually learn new tasks from a handful of
examples, we expect the models also to be able to generalize well on new
few-shot tasks without forgetting the previous ones. In this work, we define
this more challenging yet practical problem as Lifelong Few-shot Language
Learning (LFLL) and propose a unified framework for it based on prompt tuning
of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot
learning ability, and simultaneously trains the model as a task solver and a
data generator. Before learning a new domain of the same task type, LFPT5
generates pseudo (labeled) samples of previously learned domains, and later
gets trained on those samples to alleviate forgetting of previous knowledge as
it learns the new domain. In addition, a KL divergence loss is minimized to
achieve label consistency between the previous and the current model. While
adapting to a new task type, LFPT5 includes and tunes additional prompt
embeddings for the new task. With extensive experiments, we demonstrate that
LFPT5 can be applied to various different types of tasks and significantly
outperform previous methods in different LFLL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-Sentiment-Multiple-Opinion Triplet Extraction. (arXiv:2110.07303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07303">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term
(aspect), sentiment and opinion term (opinion) triplets from sentences and can
tell a complete story, i.e., the discussed aspect, the sentiment toward the
aspect, and the cause of the sentiment. ASTE is a charming task, however, one
triplet extracted by ASTE only includes one opinion of the aspect, but an
aspect in a sentence may have multiple corresponding opinions and one opinion
only provides part of the reason why the aspect has this sentiment, as a
consequence, some triplets extracted by ASTE are hard to understand, and
provide erroneous information for downstream tasks. In this paper, we introduce
a new task, named Aspect Sentiment Multiple Opinions Triplet Extraction
(ASMOTE). ASMOTE aims to extract aspect, sentiment and multiple opinions
triplets. Specifically, one triplet extracted by ASMOTE contains all opinions
about the aspect and can tell the exact reason that the aspect has the
sentiment. We propose an Aspect-Guided Framework (AGF) to address this task.
AGF first extracts aspects, then predicts their opinions and sentiments.
Moreover, with the help of the proposed Sequence Labeling Attention(SLA), AGF
improves the performance of the sentiment classification using the extracted
opinions. Experimental results on multiple datasets demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Investigation of Multi-bridge Multilingual NMT models. (arXiv:2110.07304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07304">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an extensive investigation of multi-bridge,
many-to-many multilingual NMT models (MB-M2M) ie., models trained on
non-English language pairs in addition to English-centric language pairs. In
addition to validating previous work which shows that MB-M2M models can
overcome zeroshot translation problems, our analysis reveals the following
results about multibridge models: (1) it is possible to extract a reasonable
amount of parallel corpora between non-English languages for low-resource
languages (2) with limited non-English centric data, MB-M2M models are
competitive with or outperform pivot models, (3) MB-M2M models can outperform
English-Any models and perform at par with Any-English models, so a single
multilingual NMT system can serve all translation directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Aspect Category Sentiment Analysis as a Text Generation Task. (arXiv:2110.07310v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07310">
<div class="article-summary-box-inner">
<span><p>Aspect category sentiment analysis has attracted increasing research
attention. The dominant methods make use of pre-trained language models by
learning effective aspect category-specific representations, and adding
specific output layers to its pre-trained representation. We consider a more
direct way of making use of pre-trained language models, by casting the ACSA
tasks into natural language generation tasks, using natural language sentences
to represent the output. Our method allows more direct use of pre-trained
knowledge in seq2seq language models by directly following the task setting
during pre-training. Experiments on several benchmarks show that our method
gives the best reported results, having large advantages in few-shot and
zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WMDecompose: A Framework for Leveraging the Interpretable Properties of Word Mover's Distance in Sociocultural Analysis. (arXiv:2110.07330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07330">
<div class="article-summary-box-inner">
<span><p>Despite the increasing popularity of NLP in the humanities and social
sciences, advances in model performance and complexity have been accompanied by
concerns about interpretability and explanatory power for sociocultural
analysis. One popular model that balances complexity and legibility is Word
Mover's Distance (WMD). Ostensibly adapted for its interpretability, WMD has
nonetheless been used and further developed in ways which frequently discard
its most interpretable aspect: namely, the word-level distances required for
translating a set of words into another set of words. To address this apparent
gap, we introduce WMDecompose: a model and Python library that 1) decomposes
document-level distances into their constituent word-level distances, and 2)
subsequently clusters words to induce thematic elements, such that useful
lexical information is retained and summarized for analysis. To illustrate its
potential in a social scientific context, we apply it to a longitudinal social
media corpus to explore the interrelationship between conspiracy theories and
conservative American discourses. Finally, because of the full WMD model's high
time-complexity, we additionally suggest a method of sampling document pairs
from large datasets in a reproducible way, with tight bounds that prevent
extrapolation of unreliable results due to poor sampling practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug-Tagger: A Pluggable Sequence Labeling Framework Using Language Models. (arXiv:2110.07331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07331">
<div class="article-summary-box-inner">
<span><p>Plug-and-play functionality allows deep learning models to adapt well to
different tasks without requiring any parameters modified. Recently,
prefix-tuning was shown to be a plug-and-play method on various text generation
tasks by simply inserting corresponding continuous vectors into the inputs.
However, sequence labeling tasks invalidate existing plug-and-play methods
since different label sets demand changes to the architecture of the model
classifier. In this work, we propose the use of label word prediction instead
of classification to totally reuse the architecture of pre-trained models for
sequence labeling tasks. Specifically, for each task, a label word set is first
constructed by selecting a high-frequency word for each class respectively, and
then, task-specific vectors are inserted into the inputs and optimized to
manipulate the model predictions towards the corresponding label words. As a
result, by simply switching the plugin vectors on the input, a frozen
pre-trained language model is allowed to perform different tasks. Experimental
results on three sequence labeling tasks show that the performance of the
proposed method can achieve comparable performance with standard fine-tuning
with only 0.1\% task-specific parameters. In addition, our method is up to 70
times faster than non-plug-and-play methods while switching different tasks
under the resource-constrained scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Legal Question Answering Systems. (arXiv:2110.07333v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07333">
<div class="article-summary-box-inner">
<span><p>Many legal professionals think that the explosion of information about local,
regional, national, and international legislation makes their practice more
costly, time-consuming, and even error-prone. The two main reasons for this are
that most legislation is usually unstructured, and the tremendous amount and
pace with which laws are released causes information overload in their daily
tasks. In the case of the legal domain, the research community agrees that a
system allowing to generate automatic responses to legal questions could
substantially impact many practical implications in daily activities. The
degree of usefulness is such that even a semi-automatic solution could
significantly help to reduce the workload to be faced. This is mainly because a
Question Answering system could be able to automatically process a massive
amount of legal resources to answer a question or doubt in seconds, which means
that it could save resources in the form of effort, money, and time to many
professionals in the legal sector. In this work, we quantitatively and
qualitatively survey the solutions that currently exist to meet this challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07342">
<div class="article-summary-box-inner">
<span><p>Recent methods for embodied instruction following are typically trained
end-to-end using imitation learning. This requires the use of expert
trajectories and low-level language instructions. Such approaches assume
learned hidden states will simultaneously integrate semantics from the language
and vision to perform state tracking, spatial memory, exploration, and
long-term planning. In contrast, we propose a modular method with structured
representations that (1) builds a semantic map of the scene, and (2) performs
exploration with a semantic search policy, to achieve the natural language
goal. Our modular method achieves SOTA performance (24.46%) with a substantial
(8.17 % absolute) gap from previous work while using less data by eschewing
both expert trajectories and low-level instructions. Leveraging low-level
language, however, can further increase our performance (26.49%). Our findings
suggest that an explicit spatial memory and a semantic search policy can
provide a stronger and more general representation for state-tracking and
guidance, even in the absence of expert trajectories or low-level instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Music Playlist Title Generation: A Machine-Translation Approach. (arXiv:2110.07354v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07354">
<div class="article-summary-box-inner">
<span><p>We propose a machine-translation approach to automatically generate a
playlist title from a set of music tracks. We take a sequence of track IDs as
input and a sequence of words in a playlist title as output, adapting the
sequence-to-sequence framework based on Recurrent Neural Network (RNN) and
Transformer to the music data. Considering the orderless nature of music tracks
in a playlist, we propose two techniques that remove the order of the input
sequence. One is data augmentation by shuffling and the other is deleting the
positional encoding. We also reorganize the existing music playlist datasets to
generate phrase-level playlist titles. The result shows that the Transformer
models generally outperform the RNN model. Also, removing the order of input
sequence improves the performance further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization. (arXiv:2110.07356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07356">
<div class="article-summary-box-inner">
<span><p>In medical dialogue summarization, summaries must be coherent and must
capture all the medically relevant information in the dialogue. However,
learning effective models for summarization require large amounts of labeled
data which is especially hard to obtain. We present an algorithm to create
synthetic training data with an explicit focus on capturing medically relevant
information. We utilize GPT-3 as the backbone of our algorithm and scale 210
human labeled examples to yield results comparable to using 6400 human labeled
examples (~30x) leveraging low-shot learning and an ensemble method. In
detailed experiments, we show that this approach produces high quality training
data that can further be combined with human labeled data to get summaries that
are strongly preferable to those produced by models trained on human data alone
both in terms of medical accuracy and coherency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Text Mining of COVID-19 Records. (arXiv:2110.07357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07357">
<div class="article-summary-box-inner">
<span><p>Since the beginning of coronavirus, the disease has spread worldwide and
drastically changed many aspects of the human's lifestyle. Twitter as a
powerful tool can help researchers measure public health in response to
COVID-19. According to the high volume of data production on social networks,
automated text mining approaches can help search, read and summarize helpful
information. This paper preprocessed the existing medical dataset regarding
COVID-19 named CORD-19 and annotated the dataset for supervised classification
tasks. At this time of the COVID-19 pandemic, we made a preprocessed dataset
for the research community. This may contribute towards finding new solutions
for some social interventions that COVID-19 has made. The preprocessed version
of the mentioned dataset is publicly available through Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Based Semantic Parsing. (arXiv:2110.07358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07358">
<div class="article-summary-box-inner">
<span><p>We present a memory-based model for context-dependent semantic parsing.
Previous approaches focus on enabling the decoder to copy or modify the parse
from the previous utterance, assuming there is a dependency between the current
and previous parses. In this work, we propose to represent contextual
information using an external memory. We learn a context memory controller that
manages the memory by maintaining the cumulative meaning of sequential user
utterances. We evaluate our approach on three semantic parsing benchmarks.
Experimental results show that our model can better process context-dependent
information and demonstrates improved performance without using task-specific
decoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. (arXiv:2110.07367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07367">
<div class="article-summary-box-inner">
<span><p>In various natural language processing tasks, passage retrieval and passage
re-ranking are two key procedures in finding and ranking relevant information.
Since both the two procedures contribute to the final performance, it is
important to jointly optimize them in order to achieve mutual improvement. In
this paper, we propose a novel joint training approach for dense passage
retrieval and passage re-ranking. A major contribution is that we introduce the
dynamic listwise distillation, where we design a unified listwise training
approach for both the retriever and the re-ranker. During the dynamic
distillation, the retriever and the re-ranker can be adaptively improved
according to each other's relevance information. We also propose a hybrid data
augmentation strategy to construct diverse training instances for listwise
training approach. Extensive experiments show the effectiveness of our approach
on both MSMARCO and Natural Questions datasets. Our code is available at
https://github.com/PaddlePaddle/RocketQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Semantic Knowledge Into Language Encoders. (arXiv:2110.07382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07382">
<div class="article-summary-box-inner">
<span><p>We introduce semantic form mid-tuning, an approach for transferring semantic
knowledge from semantic meaning representations into transformer-based language
encoders. In mid-tuning, we learn to align the text of general sentences -- not
tied to any particular inference task -- and structured semantic
representations of those sentences. Our approach does not require gold
annotated semantic representations. Instead, it makes use of automatically
generated semantic representations, such as from off-the-shelf PropBank and
FrameNet semantic parsers. We show that this alignment can be learned
implicitly via classification or directly via triplet loss. Our method yields
language encoders that demonstrate improved predictive performance across
inference, reading comprehension, textual similarity, and other semantic tasks
drawn from the GLUE, SuperGLUE, and SentEval benchmarks. We evaluate our
approach on three popular baseline models, where our experimental results and
analysis concludes that current pre-trained language models can further benefit
from structured semantic frames with the proposed mid-tuning method, as they
inject additional task-agnostic knowledge to the encoder, improving the
generated embeddings as well as the linguistic properties of the given model,
as evident from improvements on a popular sentence embedding toolkit and a
variety of probing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Neglected Sibling: Isotropic Gaussian Posterior for VAE. (arXiv:2110.07383v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07383">
<div class="article-summary-box-inner">
<span><p>Deep generative models have been widely used in several areas of NLP, and
various techniques have been proposed to augment them or address their training
challenges. In this paper, we propose a simple modification to Variational
Autoencoders (VAEs) by using an Isotropic Gaussian Posterior (IGP) that allows
for better utilisation of their latent representation space. This model avoids
the sub-optimal behavior of VAEs related to inactive dimensions in the
representation space. We provide both theoretical analysis, and empirical
evidence on various datasets and tasks that show IGP leads to consistent
improvement on several quantitative and qualitative grounds, from downstream
task performance and sample efficiency to robustness. Additionally, we give
insights about the representational properties encouraged by IGP and also show
that its gain generalises to image domain as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages. (arXiv:2110.07385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07385">
<div class="article-summary-box-inner">
<span><p>Style transfer is the task of rewriting an input sentence into a target style
while approximately preserving its content. While most prior literature assumes
access to large style-labelled corpora, recent work (Riley et al. 2021) has
attempted "few-shot" style transfer using only 3-10 sentences at inference for
extracting the target style. In this work we consider one such low resource
setting where no datasets are available: style transfer for Indian languages.
We find that existing few-shot methods perform this task poorly, with a strong
tendency to copy inputs verbatim. We push the state-of-the-art for few-shot
style transfer with a new method modeling the stylistic difference between
paraphrases. When compared to prior work using automatic and human evaluations,
our model achieves 2-3x better performance and output diversity in formality
transfer and code-mixing addition across five Indian languages. Moreover, our
method is better able to control the amount of style transfer using an input
scalar knob. We report promising qualitative results for several attribute
transfer directions, including sentiment transfer, text simplification, gender
neutralization and text anonymization, all without retraining the model.
Finally we found model evaluation to be difficult due to the lack of evaluation
datasets and metrics for Indian languages. To facilitate further research in
formality transfer for Indic languages, we crowdsource annotations for 4000
sentence pairs in four languages, and use this dataset to design our automatic
evaluation suite.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning. (arXiv:2110.07410v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07410">
<div class="article-summary-box-inner">
<span><p>Automated audio captioning (AAC) is the task of automatically generating
textual descriptions for general audio signals. A captioning system has to
identify various information from the input signal and express it with natural
language. Existing works mainly focus on investigating new methods and try to
improve their performance measured on existing datasets. Having attracted
attention only recently, very few works on AAC study the performance of
existing pre-trained audio and natural language processing resources. In this
paper, we evaluate the performance of off-the-shelf models with a
Transformer-based captioning approach. We utilize the freely available Clotho
dataset to compare four different pre-trained machine listening models, four
word embedding models, and their combinations in many different settings. Our
evaluation suggests that YAMNet combined with BERT embeddings produces the best
captions. Moreover, in general, fine-tuning pre-trained word embeddings can
lead to better performance. Finally, we show that sequences of audio embeddings
can be processed using a Transformer encoder to produce higher-quality
captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple, Strong and Robust Baseline for Distantly Supervised Relation Extraction. (arXiv:2110.07415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07415">
<div class="article-summary-box-inner">
<span><p>Distantly supervised relation extraction (DS-RE) is generally framed as a
multi-instance multi-label (MI-ML) task, where the optimal aggregation of
information from multiple instances is of key importance. Intra-bag attention
(Lin et al., 2016) is an example of a popularly used aggregation scheme for
this framework. Apart from this scheme, however, there is not much to choose
from in the DS-RE literature as most of the advances in this field are focused
on improving the instance-encoding step rather than the instance-aggregation
step. With recent works leveraging large pre-trained language models as
encoders, the increased capacity of models might allow for more flexibility in
the instance-aggregation step. In this work, we explore this hypothesis and
come up with a novel aggregation scheme which we call Passage-Att. Under this
aggregation scheme, we combine all instances mentioning an entity pair into a
"passage of instances", which is summarized independently for each relation
class. These summaries are used to predict the validity of a potential triple.
We show that our Passage-Att with BERT as passage encoder achieves
state-of-the-art performance in three different settings (monolingual DS,
monolingual DS with manually-annotated test set, multilingual DS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal Frames. (arXiv:2110.07420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07420">
<div class="article-summary-box-inner">
<span><p>Social concepts referring to non-physical objects--such as revolution,
violence, or friendship--are powerful tools to describe, index, and query the
content of visual data, including ever-growing collections of art images from
the Cultural Heritage (CH) field. While much progress has been made towards
complete image understanding in computer vision, automatic detection of social
concepts evoked by images is still a challenge. This is partly due to the
well-known semantic gap problem, worsened for social concepts given their lack
of unique physical features, and reliance on more unspecific features than
concrete concepts. In this paper, we propose the translation of recent
cognitive theories about social concept representation into a software approach
to represent them as multimodal frames, by integrating multisensory data. Our
method focuses on the extraction, analysis, and integration of multimodal
features from visual art material tagged with the concepts of interest. We
define a conceptual model and present a novel ontology for formally
representing social concepts as multimodal frames. Taking the Tate Gallery's
collection as an empirical basis, we experiment our method on a corpus of art
images to provide a proof of concept of its potential. We discuss further
directions of research, and provide all software, data sources, and results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Model Robustness to User-generated Noisy Texts. (arXiv:2110.07428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07428">
<div class="article-summary-box-inner">
<span><p>Sensitivity of deep-neural models to input noise is known to be a challenging
problem. In NLP, model performance often deteriorates with naturally occurring
noise, such as spelling errors. To mitigate this issue, models may leverage
artificially noised data. However, the amount and type of generated noise has
so far been determined arbitrarily. We therefore propose to model the errors
statistically from grammatical-error-correction corpora. We present a thorough
evaluation of several state-of-the-art NLP systems' robustness in multiple
languages, with tasks including morpho-syntactic analysis, named entity
recognition, neural machine translation, a subset of the GLUE benchmark and
reading comprehension. We also compare two approaches to address the
performance drop: a) training the NLP models with noised data generated by our
framework; and b) reducing the input noise with external system for natural
language correction. The code is released at https://github.com/ufal/kazitext.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards More Effective and Economic Sparsely-Activated Model. (arXiv:2110.07431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07431">
<div class="article-summary-box-inner">
<span><p>The sparsely-activated models have achieved great success in natural language
processing through large-scale parameters and relatively low computational
cost, and gradually become a feasible technique for training and implementing
extremely large models. Due to the limit of communication cost, activating
multiple experts is hardly affordable during training and inference. Therefore,
previous work usually activate just one expert at a time to alleviate
additional communication cost. Such routing mechanism limits the upper bound of
model performance. In this paper, we first investigate a phenomenon that
increasing the number of activated experts can boost the model performance with
higher sparse ratio. To increase the number of activated experts without an
increase in computational cost, we propose SAM (Switch and Mixture) routing, an
efficient hierarchical routing mechanism that activates multiple experts in a
same device (GPU). Our methods shed light on the training of extremely large
sparse models and experiments prove that our models can achieve significant
performance gain with great efficiency improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing Language Technologies for Social Good: The Road not Taken. (arXiv:2110.07444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07444">
<div class="article-summary-box-inner">
<span><p>Development of speech and language technology for social good (LT4SG),
especially those targeted at the welfare of marginalized communities and
speakers of low-resource and under-served languages, has been a prominent theme
of research within NLP, Speech, and the AI communities. Researchers have mostly
relied on their individual expertise, experiences or ad hoc surveys for
prioritization of language technologies that provide social good to the
end-users. This has been criticized by several scholars who argue that work on
LT4SG must include the target linguistic communities during the design and
development process. However, none of the LT4SG work and their critiques
suggest principled techniques for prioritization of the technologies and
methods for inclusion of the end-user during the development cycle. Drawing
inspiration from the fields of Economics, Ethics, Psychology, and Participatory
Design, here we chart out a set of methodologies for prioritizing LT4SG that
are aligned with the end-user preferences. We then analyze several LT4SG
efforts in light of the proposed methodologies and bring out their hidden
assumptions and potential pitfalls. While the current study is limited to
language technologies, we believe that the principles and prioritization
techniques highlighted here are applicable more broadly to AI for Social Good.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07474">
<div class="article-summary-box-inner">
<span><p>When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited.A typical example is when
using CNN/Daily Mail dataset for controllable text summarization, there is no
guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and control variables to guide
the generation, which can only be built with deep understanding of the domain
knowledge. Motivated by this vi-sion, our paper introduces a new text
generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews
and all its 45k meta-review sentences are manually annotated as one of the
carefully defined 9 categories, including abstract, strength, decision, etc. We
present experimental results on start-of-the-art summarization models, and
propose methods for controlled generation on both extractive and abstractive
models using our annotated data. By exploring various settings and analaysing
the model behavior with respect to the control inputs, we demonstrate the
challenges and values of our dataset. MReD allows us to have a better
understanding of the meta-review corpora and enlarge the research room for
controllable text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding. (arXiv:2110.07476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07476">
<div class="article-summary-box-inner">
<span><p>Event extraction is typically modeled as a multi-class classification problem
where both event types and argument roles are treated as atomic symbols. These
approaches are usually limited to a set of pre-defined types. We propose a
novel event extraction framework that takes event types and argument roles as
natural language queries to extract candidate triggers and arguments from the
input text. With the rich semantics in the queries, our framework benefits from
the attention mechanisms to better capture the semantic correlation between the
event types or argument roles and the input text. Furthermore, the
query-and-extract formulation allows our approach to leverage all available
event annotations from various ontologies as a unified model. Experiments on
two public benchmarks, ACE and ERE, demonstrate that our approach achieves
state-of-the-art performance on each dataset and significantly outperforms
existing methods on zero-shot event extraction. We will make all the programs
publicly available once the paper is accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning Large-Scale Pre-trained Language Models for Conversational Recommendation with Knowledge Graph. (arXiv:2110.07477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07477">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a pre-trained language model (PLM) based framework
called RID for conversational recommender system (CRS). RID finetunes the
large-scale PLMs such as DialoGPT, together with a pre-trained Relational Graph
Convolutional Network (RGCN) to encode the node representations of an
item-oriented knowledge graph. The former aims to generate fluent and diverse
dialogue responses based on the strong language generation ability of PLMs,
while the latter is to facilitate the item recommendation by learning better
node embeddings on the structural knowledge base. To unify two modules of
dialogue generation and item recommendation into a PLMs-based framework, we
expand the generation vocabulary of PLMs to include an extra item vocabulary,
and introduces a vocabulary pointer to control when to recommend target items
in the generation process. Extensive experiments on the benchmark dataset
ReDial show RID significantly outperforms the state-of-the-art methods on both
evaluations of dialogue and recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07480">
<div class="article-summary-box-inner">
<span><p>Nested entities are observed in many domains due to their compositionality,
which cannot be easily recognized by the widely-used sequence labeling
framework. A natural solution is to treat the task as a span classification
problem. To increase performance on span representation and classification, it
is crucial to effectively integrate all useful information of different
formats, which we refer to heterogeneous factors including tokens, labels,
boundaries, and related spans. To fuse these heterogeneous factors, we propose
a novel triaffine mechanism including triaffine attention and scoring, which
interacts with multiple factors in both the stages of representation and
classification. Experiments results show that our proposed method achieves the
state-of-the-art F1 scores on four nested NER datasets: ACE2004, ACE2005,
GENIA, and KBP2017.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Pitfalls of Analyzing Individual Neurons in Language Models. (arXiv:2110.07483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07483">
<div class="article-summary-box-inner">
<span><p>While many studies have shown that linguistic information is encoded in
hidden word representations, few have studied individual neurons, to show how
and in which neurons it is encoded. Among these, the common approach is to use
an external probe to rank neurons according to their relevance to some
linguistic attribute, and to evaluate the obtained ranking using the same probe
that produced it. We show two pitfalls in this methodology: 1. It confounds
distinct factors: probe quality and ranking quality. We separate them and draw
conclusions on each. 2. It focuses on encoded information, rather than
information that is used by the model. We show that these are not the same. We
compare two recent ranking methods and a simple one we introduce, and evaluate
them with regard to both of these aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Keyword Spotting using Xception-1d. (arXiv:2110.07498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07498">
<div class="article-summary-box-inner">
<span><p>The field of conversational agents is growing fast and there is an increasing
need for algorithms that enhance natural interaction. In this work we show how
we achieved state of the art results in the Keyword Spotting field by adapting
and tweaking the Xception algorithm, which achieved outstanding results in
several computer vision tasks. We obtained about 96\% accuracy when classifying
audio clips belonging to 35 different categories, beating human annotation at
the most complex tasks proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision. (arXiv:2110.07515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07515">
<div class="article-summary-box-inner">
<span><p>How do we perform efficient inference while retaining high translation
quality? Existing neural machine translation models, such as Transformer,
achieve high performance, but they decode words one by one, which is
inefficient. Recent non-autoregressive translation models speed up the
inference, but their quality is still inferior. In this work, we propose DSLP,
a highly efficient and high-performance model for machine translation. The key
insight is to train a non-autoregressive Transformer with Deep Supervision and
feed additional Layer-wise Predictions. We conducted extensive experiments on
four translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO).
Results show that our approach consistently improves the BLEU scores compared
with respective base models. Specifically, our best variant outperforms the
autoregressive model on three translation tasks, while being 14.8 times more
efficient in inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures. (arXiv:2110.07518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07518">
<div class="article-summary-box-inner">
<span><p>Current open-domain conversational models can easily be made to talk in
inadequate ways. Online learning from conversational feedback given by the
conversation partner is a promising avenue for a model to improve and adapt, so
as to generate fewer of these safety failures. However, current
state-of-the-art models tend to react to feedback with defensive or oblivious
responses. This makes for an unpleasant experience and may discourage
conversation partners from giving feedback in the future. This work proposes
SaFeRDialogues, a task and dataset of graceful responses to conversational
feedback about safety failures. We collect a dataset of 10k dialogues
demonstrating safety failures, feedback signaling them, and a response
acknowledging the feedback. We show how fine-tuning on this dataset results in
conversations that human raters deem considerably more likely to lead to a
civil conversation, without sacrificing engagingness or general conversational
ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Opinion Summarization via Collaborative Decoding. (arXiv:2110.07520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07520">
<div class="article-summary-box-inner">
<span><p>Opinion summarization focuses on generating summaries that reflect popular
opinions of multiple reviews for a single entity (e.g., a hotel or a product.)
While generated summaries offer general and concise information about a
particular entity, the information may be insufficient to help the user compare
multiple entities. Thus, the user may still struggle with the question "Which
one should I pick?" In this paper, we propose a {\em comparative opinion
summarization} task, which is to generate two contrastive summaries and one
common summary from two given sets of reviews from different entities. We
develop a comparative summarization framework CoCoSum, which consists of two
few-shot summarization models that are jointly used to generate contrastive and
common summaries. Experimental results on a newly created benchmark CoCoTrip
show that CoCoSum can produce high-quality contrastive and common summaries
than state-of-the-art opinion summarization models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Decoupling for Open-Domain Passage Retrieval. (arXiv:2110.07524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07524">
<div class="article-summary-box-inner">
<span><p>Training dense passage representations via contrastive learning (CL) has been
shown effective for Open-Domain Passage Retrieval (ODPR). Recent studies mainly
focus on optimizing this CL framework by improving the sampling strategy or
extra pretraining. Different from previous studies, this work devotes itself to
investigating the influence of conflicts in the widely used CL strategy in
ODPR, motivated by our observation that a passage can be organized by multiple
semantically different sentences, thus modeling such a passage as a unified
dense vector is not optimal. We call such conflicts Contrastive Conflicts. In
this work, we propose to solve it with a representation decoupling method, by
decoupling the passage representations into contextual sentence-level ones, and
design specific CL strategies to mediate these conflicts. Experiments on widely
used datasets including Natural Questions, Trivia QA, and SQuAD verify the
effectiveness of our method, especially on the dataset where the conflicting
problem is severe. Our method also presents good transferability across the
datasets, which further supports our idea of mediating Contrastive Conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Irrationality of Neural Rationale Models. (arXiv:2110.07550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07550">
<div class="article-summary-box-inner">
<span><p>Neural rationale models are popular for interpretable predictions of NLP
tasks. In these, a selector extracts segments of the input text, called
rationales, and passes these segments to a classifier for prediction. Since the
rationale is the only information accessible to the classifier, it is plausibly
defined as the explanation. Is such a characterization unconditionally correct?
In this paper, we argue to the contrary, with both philosophical perspectives
and empirical evidence suggesting that rationale models are, perhaps, less
rational and interpretable than expected. We call for more rigorous and
comprehensive evaluations of these models to ensure desired properties of
interpretability are indeed achieved. The code can be found at
https://github.com/yimingz89/Neural-Rationale-Analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BI-RADS BERT & Using Section Tokenization to Understand Radiology Reports. (arXiv:2110.07552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07552">
<div class="article-summary-box-inner">
<span><p>Radiology reports are the main form of communication between radiologists and
other clinicians, and contain important information for patient care. However
in order to use this information for research it is necessary to convert the
raw text into structured data suitable for analysis. Domain specific contextual
word embeddings have been shown to achieve impressive accuracy at such natural
language processing tasks in medicine. In this work we pre-trained a contextual
embedding BERT model using breast radiology reports and developed a classifier
that incorporated the embedding with auxiliary global textual features in order
to perform a section tokenization task. This model achieved a 98% accuracy at
segregating free text reports into sections of information outlined in the
Breast Imaging Reporting and Data System (BI-RADS) lexicon, a significant
improvement over the Classic BERT model without auxiliary information. We then
evaluated whether using section tokenization improved the downstream extraction
of the following fields: modality/procedure, previous cancer, menopausal
status, purpose of exam, breast density and background parenchymal enhancement.
Using the BERT model pre-trained on breast radiology reports combined with
section tokenization resulted in an overall accuracy of 95.9% in field
extraction. This is a 17% improvement compared to an overall accuracy of 78.9%
for field extraction for models without section tokenization and with Classic
BERT embeddings. Our work shows the strength of using BERT in radiology report
analysis and the advantages of section tokenization in identifying key features
of patient factors recorded in breast radiology reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composable Sparse Fine-Tuning for Cross-Lingual Transfer. (arXiv:2110.07560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07560">
<div class="article-summary-box-inner">
<span><p>Fine-tuning all parameters of a pre-trained model has become the mainstream
approach for transfer learning. To increase its efficiency and prevent
catastrophic forgetting and interference, techniques like adapters and sparse
fine-tuning have been developed. Adapters are modular, as they can be combined
to adapt a model towards different facets of knowledge (e.g., dedicated
language and/or task adapters). Sparse fine-tuning is expressive, as it
controls the behavior of all model components. In this work, we introduce a new
fine-tuning method with both these desirable properties. In particular, we
learn sparse, real-valued masks based on a simple variant of the Lottery Ticket
Hypothesis. Task-specific masks are obtained from annotated data in a source
language, and language-specific masks from masked language modeling in a target
language. Both these masks can then be composed with the pre-trained model.
Unlike adapter-based fine-tuning, this method neither increases the number of
parameters at inference time nor alters the original model architecture. Most
importantly, it outperforms adapters in zero-shot cross-lingual transfer by a
large margin in a series of multilingual benchmarks, including Universal
Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we
additionally find that sparsity is crucial to prevent both 1) interference
between the fine-tunings to be composed and 2) overfitting. We release the code
and models at https://github.com/cambridgeltl/composable-sft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07566">
<div class="article-summary-box-inner">
<span><p>In attempts to develop sample-efficient algorithms, researcher have explored
myriad mechanisms for collecting and exploiting feature feedback, auxiliary
annotations provided for training (but not test) instances that highlight
salient evidence. Examples include bounding boxes around objects and salient
spans in text. Despite its intuitive appeal, feature feedback has not delivered
significant gains in practical problems as assessed on iid holdout sets.
However, recent works on counterfactually augmented data suggest an alternative
benefit of supplemental annotations: lessening sensitivity to spurious patterns
and consequently delivering gains in out-of-domain evaluations. Inspired by
these findings, we hypothesize that while the numerous existing methods for
incorporating feature feedback have delivered negligible in-sample gains, they
may nevertheless generalize better out-of-domain. In experiments addressing
sentiment analysis, we show that feature feedback methods perform significantly
better on various natural out-of-domain datasets even absent differences on
in-domain evaluation. By contrast, on natural language inference tasks,
performance remains comparable. Finally, we compare those tasks where feature
feedback does (and does not) help.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing. (arXiv:2110.07572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07572">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is the task of producing a structured meaning representation
for natural language utterances or questions. Recent research has pointed out
that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle
to generalize systematically, i.e. to handle examples that require recombining
known knowledge in novel settings. In this work, we show that better systematic
generalization can be achieved by producing the meaning representation (MR)
directly as a graph and not as a sequence. To this end we propose LAGr, the
Labeling Aligned Graphs algorithm that produces semantic parses by predicting
node and edge labels for a complete multi-layer input-aligned graph. The
strongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas
weakly-supervised LAGr infers alignments for originally unaligned target graphs
using an approximate MAP inference procedure. On the COGS and CFQ compositional
generalization benchmarks the strongly- and weakly- supervised LAGr algorithms
achieve significant improvements upon the baseline seq2seq parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delphi: Towards Machine Ethics and Norms. (arXiv:2110.07574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07574">
<div class="article-summary-box-inner">
<span><p>What would it take to teach a machine to behave ethically? While broad
ethical rules may seem straightforward to state ("thou shalt not kill"),
applying such rules to real-world situations is far more complex. For example,
while "helping a friend" is generally a good thing to do, "helping a friend
spread fake news" is not. We identify four underlying challenges towards
machine ethics and norms: (1) an understanding of moral precepts and social
norms; (2) the ability to perceive real-world situations visually or by reading
natural language descriptions; (3) commonsense reasoning to anticipate the
outcome of alternative actions in different contexts; (4) most importantly, the
ability to make ethical judgments given the interplay between competing values
and their grounding in different contexts (e.g., the right to freedom of
expression vs. preventing the spread of fake news).
</p>
<p>Our paper begins to address these questions within the deep learning
paradigm. Our prototype model, Delphi, demonstrates strong promise of
language-based commonsense moral reasoning, with up to 92.1% accuracy vetted by
humans. This is in stark contrast to the zero-shot performance of GPT-3 of
52.3%, which suggests that massive scale alone does not endow pre-trained
neural language models with human values. Thus, we present Commonsense Norm
Bank, a moral textbook customized for machines, which compiles 1.7M examples of
people's ethical judgments on a broad spectrum of everyday situations. In
addition to the new resources and baseline performances for future research,
our study provides new insights that lead to several important open research
questions: differentiating between universal human values and personal values,
modeling different moral frameworks, and explainable, consistent approaches to
machine ethics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset. (arXiv:2110.07575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07575">
<div class="article-summary-box-inner">
<span><p>Visually-grounded spoken language datasets can enable models to learn
cross-modal correspondences with very weak supervision. However, modern
audio-visual datasets contain biases that undermine the real-world performance
of models trained on that data. We introduce Spoken ObjectNet, which is
designed to remove some of these biases and provide a way to better evaluate
how effectively models will perform in real-world scenarios. This dataset
expands upon ObjectNet, which is a bias-controlled image dataset that features
similar image classes to those present in ImageNet. We detail our data
collection pipeline, which features several methods to improve caption quality,
including automated language model checks. Lastly, we show baseline results on
image retrieval and audio retrieval tasks. These results show that models
trained on other datasets and then evaluated on Spoken ObjectNet tend to
perform poorly due to biases in other datasets that the models have learned. We
also show evidence that the performance decrease is due to the dataset
controls, and not the transfer setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. (arXiv:2110.07577v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07577">
<div class="article-summary-box-inner">
<span><p>Conventional fine-tuning of pre-trained language models tunes all model
parameters and stores a full model copy for each downstream task, which has
become increasingly infeasible as the model size grows larger. Recent
parameter-efficient language model tuning (PELT) methods manage to match the
performance of fine-tuning with much fewer trainable parameters and perform
especially well when the training data is limited. However, different PELT
methods may perform rather differently on the same task, making it nontrivial
to select the most appropriate method for a specific task, especially
considering the fast-growing number of new PELT methods and downstream tasks.
In light of model diversity and the difficulty of model selection, we propose a
unified framework, UniPELT, which incorporates different PELT methods as
submodules and learns to activate the ones that best suit the current data or
task setup. Remarkably, on the GLUE benchmark, UniPELT consistently achieves
1~3pt gains compared to the best individual PELT method that it incorporates
and even outperforms fine-tuning under different setups. Moreover, UniPELT
often surpasses the upper bound when taking the best performance of all its
submodules used individually on each task, indicating that a mixture of
multiple PELT methods may be inherently more effective than single methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations. (arXiv:2110.07581v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07581">
<div class="article-summary-box-inner">
<span><p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts
in the embedding space and then matching them by nearest neighbor search. This
requires strong locality properties from the representation space, i.e, the
close allocations of each small group of relevant texts, which are hard to
generalize to domains without sufficient training data. In this paper, we aim
to improve the generalization ability of DR models from source training domains
with rich supervision signals to target domains without any relevant labels, in
the zero-shot setting. To achieve that, we propose Momentum adversarial Domain
Invariant Representation learning (MoDIR), which introduces a momentum method
in the DR training process to train a domain classifier distinguishing source
versus target, and then adversarially updates the DR encoder to learn domain
invariant representations. Our experiments show that MoDIR robustly outperforms
its baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot
setup, with more than 10% relative gains on datasets with enough sensitivity
for DR models' evaluation. Source code of this paper will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Explanations Be Useful for Calibrating Black Box Models?. (arXiv:2110.07586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07586">
<div class="article-summary-box-inner">
<span><p>One often wants to take an existing, trained NLP model and use it on data
from a new domain. While fine-tuning or few-shot learning can be used to adapt
the base model, there is no one simple recipe to getting these working;
moreover, one may not have access to the original model weights if it is
deployed as a black box. To this end, we study how to improve a black box
model's performance on a new domain given examples from the new domain by
leveraging explanations of the model's behavior. Our approach first extracts a
set of features combining human intuition about the task with model
attributions generated by black box interpretation techniques, and then uses a
simple model to calibrate or rerank the model's predictions based on the
features. We experiment with our method on two tasks, extractive question
answering and natural language inference, covering adaptation from several
pairs of domains. The experimental results across all the domain pairs show
that explanations are useful for calibrating these models. We show that the
calibration features transfer to some extent between tasks and shed light on
how to effectively use them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Toxicity Analysis: A New Spoken Language Processing Task. (arXiv:2110.07592v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07592">
<div class="article-summary-box-inner">
<span><p>Toxic speech, also known as hate speech, is regarded as one of the crucial
issues plaguing online social media today. Most recent work on toxic speech
detection is constrained to the modality of text with no existing work on
toxicity detection from spoken utterances. In this paper, we propose a new
Spoken Language Processing task of detecting toxicity from spoken speech. We
introduce DeToxy, the first publicly available toxicity annotated dataset for
English speech, sourced from various openly available speech databases,
consisting of over 2 million utterances. Finally, we also provide analysis on
how a spoken speech corpus annotated for toxicity can help facilitate the
development of E2E models which better capture various prosodic cues in speech,
thereby boosting toxicity classification on spoken utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compressibility of Distributed Document Representations. (arXiv:2110.07595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07595">
<div class="article-summary-box-inner">
<span><p>Contemporary natural language processing (NLP) revolves around learning from
latent document representations, generated either implicitly by neural language
models or explicitly by methods such as doc2vec or similar. One of the key
properties of the obtained representations is their dimension. Whilst the
commonly adopted dimensions of 256 and 768 offer sufficient performance on many
tasks, it is many times unclear whether the default dimension is the most
suitable choice for the subsequent downstream learning tasks. Furthermore,
representation dimensions are seldom subject to hyperparameter tuning due to
computational constraints. The purpose of this paper is to demonstrate that a
surprisingly simple and efficient recursive compression procedure can be
sufficient to both significantly compress the initial representation, but also
potentially improve its performance when considering the task of text
classification. Having smaller and less noisy representations is the desired
property during deployment, as orders of magnitude smaller models can
significantly reduce the computational overload and with it the deployment
costs. We propose CoRe, a straightforward, representation learner-agnostic
framework suitable for representation compression. The CoRe's performance is
showcased and studied on a collection of 17 real-life corpora from biomedical,
news, social media, and literary domains. We explored CoRe's behavior when
considering contextual and non-contextual document representations, different
compression levels, and 9 different compression algorithms. Current results
based on more than 100,000 compression experiments indicate that recursive
Singular Value Decomposition offers a very good trade-off between the
compression efficiency and performance, making CoRe useful in many existing,
representation-dependent NLP pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-guided Counterfactual Generation for QA. (arXiv:2110.07596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07596">
<div class="article-summary-box-inner">
<span><p>Deep NLP models have been shown to learn spurious correlations, leaving them
brittle to input perturbations. Recent work has shown that counterfactual or
contrastive data -- i.e. minimally perturbed inputs -- can reveal these
weaknesses, and that data augmentation using counterfactuals can help
ameliorate them. Proposed techniques for generating counterfactuals rely on
human annotations, perturbations based on simple heuristics, and meaning
representation frameworks. We focus on the task of creating counterfactuals for
question answering, which presents unique challenges related to world
knowledge, semantic diversity, and answerability. To address these challenges,
we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual
evaluation and training data with minimal human supervision. Using an
open-domain QA framework and question generation model trained on original task
data, we create counterfactuals that are fluent, semantically diverse, and
automatically labeled. Data augmentation with RGF counterfactuals improves
performance on out-of-domain and challenging evaluation sets over and above
existing methods, in both the reading comprehension and open-domain QA
settings. Moreover, we find that RGF data leads to significant improvements in
a model's robustness to local perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07602">
<div class="article-summary-box-inner">
<span><p>Prompt tuning, which only tunes continuous prompts with a frozen language
model, substantially reduces per-task storage and memory usage at training.
However, in the context of NLU, prior work and our results reveal that existing
methods of prompt tuning do not perform well for normal-sized pre-trained
models and for hard sequence tasks, indicating lack of universality. We present
a novel empirical finding that properly-optimized prompt tuning can be
universally effective across a wide range of model scales and NLU tasks, where
it matches the performance of fine-tuning while having only 0.1\%-3\% tuned
parameters. Our method P-Tuning v2 is not a new method but a version of
prefix-tuning \cite{li2021prefix} optimized and adapted for NLU. Given the
universality and simplicity of P-Tuning v2, we believe it can serve as an
alternative for fine-tuning and a strong baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sub-word Level Lip Reading With Visual Attention. (arXiv:2110.07603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07603">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to learn strong lip reading models that can
recognise speech in silent videos. Most prior works deal with the open-set
visual speech recognition problem by adapting existing automatic speech
recognition techniques on top of trivially pooled visual features. Instead, in
this paper we focus on the unique challenges encountered in lip reading and
propose tailored solutions. To that end we make the following contributions:
(1) we propose an attention-based pooling mechanism to aggregate visual speech
representations; (2) we use sub-word units for lip reading for the first time
and show that this allows us to better model the ambiguities of the task; (3)
we propose a training pipeline that balances the lip reading performance with
other key factors such as data and compute efficiency. Following the above, we
obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks
when training on public datasets, and even surpass models trained on
large-scale industrial datasets by using an order of magnitude less data. Our
best model achieves 22.6% word error rate on the LRS2 dataset, a performance
unprecedented for lip reading models, significantly reducing the performance
gap between lip reading and automatic speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Political Text Scaling Meets Computational Semantics. (arXiv:1904.06217v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.06217">
<div class="article-summary-box-inner">
<span><p>During the last fifteen years, automatic text scaling has become one of the
key tools of the Text as Data community in political science. Prominent text
scaling algorithms, however, rely on the assumption that latent positions can
be captured just by leveraging the information about word frequencies in
documents under study. We challenge this traditional view and present a new,
semantically aware text scaling algorithm, SemScale, which combines recent
developments in the area of computational linguistics with unsupervised
graph-based clustering. We conduct an extensive quantitative analysis over a
collection of speeches from the European Parliament in five different languages
and from two different legislative terms, and show that a scaling approach
relying on semantic document representations is often better at capturing known
underlying political dimensions than the established frequency-based (i.e.,
symbolic) scaling method. We further validate our findings through a series of
experiments focused on text preprocessing and feature selection, document
representation, scaling of party manifestos, and a supervised extension of our
algorithm. To catalyze further research on this new branch of text scaling
methods, we release a Python implementation of SemScale with all included data
sets and evaluation procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAS: An Answer Selection Method Using BERT Language Model. (arXiv:1911.01528v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.01528">
<div class="article-summary-box-inner">
<span><p>In recent years, Question Answering systems have become more popular and
widely used by users. Despite the increasing popularity of these systems, the
their performance is not even sufficient for textual data and requires further
research. These systems consist of several parts that one of them is the Answer
Selection component. This component detects the most relevant answer from a
list of candidate answers. The methods presented in previous researches have
attempted to provide an independent model to undertake the answer-selection
task. An independent model cannot comprehend the syntactic and semantic
features of questions and answers with a small training dataset. To fill this
gap, language models can be employed in implementing the answer selection part.
This action enables the model to have a better understanding of the language in
order to understand questions and answers better than previous works. In this
research, we will present the "BAS" (BERT Answer Selection) that uses the BERT
language model to comprehend language. The empirical results of applying the
model on the TrecQA Raw, TrecQA Clean, and WikiQA datasets demonstrate that
using a robust language model such as BERT can enhance the performance. Using a
more robust classifier also enhances the effect of the language model on the
answer selection component. The results demonstrate that language comprehension
is an essential requirement in natural language processing tasks such as
answer-selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Deep Neural Networks. (arXiv:2010.01496v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01496">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are becoming more and more popular due to their
revolutionary success in diverse areas, such as computer vision, natural
language processing, and speech recognition. However, the decision-making
processes of these models are generally not interpretable to users. In various
domains, such as healthcare, finance, or law, it is critical to know the
reasons behind a decision made by an artificial intelligence system. Therefore,
several directions for explaining neural models have recently been explored. In
this thesis, I investigate two major directions for explaining deep neural
networks. The first direction consists of feature-based post-hoc explanatory
methods, that is, methods that aim to explain an already trained and fixed
model (post-hoc), and that provide explanations in terms of input features,
such as tokens for text and superpixels for images (feature-based). The second
direction consists of self-explanatory neural models that generate natural
language explanations, that is, models that have a built-in module that
generates explanations for the predictions of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering. (arXiv:2010.12643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12643">
<div class="article-summary-box-inner">
<span><p>Coupled with the availability of large scale datasets, deep learning
architectures have enabled rapid progress on the Question Answering task.
However, most of those datasets are in English, and the performances of
state-of-the-art multilingual models are significantly lower when evaluated on
non-English data. Due to high data collection costs, it is not realistic to
obtain annotated data for each language one desires to support.
</p>
<p>We propose a method to improve the Cross-lingual Question Answering
performance without requiring additional annotated data, leveraging Question
Generation models to produce synthetic samples in a cross-lingual fashion. We
show that the proposed method allows to significantly outperform the baselines
trained on English data only. We report a new state-of-the-art on four
multilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-the-Fly Attention Modulation for Neural Generation. (arXiv:2101.00371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00371">
<div class="article-summary-box-inner">
<span><p>Despite considerable advancements with deep neural language models (LMs),
neural text generation still suffers from degeneration: the generated text is
repetitive, generic, self-contradictory, and often lacks commonsense. Our
analyses on sentence-level attention patterns in LMs reveal that neural
degeneration may be associated with insufficient learning of task-specific
characteristics by the attention mechanism. This finding motivates on-the-fly
attention modulation -- a simple but effective method that enables the
injection of priors into attention computation during inference. Automatic and
human evaluation results on three text generation benchmarks demonstrate that
attention modulation helps LMs generate text with enhanced fluency, creativity,
and commonsense reasoning, in addition to significantly reduce sentence-level
repetition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Partisan Political Narrative Frameworks about COVID-19 on Twitter. (arXiv:2103.06960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06960">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic is a global crisis that has been testing every society
and exposing the critical role of local politics in crisis response. In the
United States, there has been a strong partisan divide between the Democratic
and Republican party's narratives about the pandemic which resulted in
polarization of individual behaviors and divergent policy adoption across
regions. As shown in this case, as well as in most major social issues,
strongly polarized narrative frameworks facilitate such narratives. To
understand polarization and other social chasms, it is critical to dissect
these diverging narratives. Here, taking the Democratic and Republican
political social media posts about the pandemic as a case study, we demonstrate
that a combination of computational methods can provide useful insights into
the different contexts, framing, and characters and relationships that
construct their narrative frameworks which individual posts source from.
Leveraging a dataset of tweets from elite politicians in the U.S., we found
that the Democrats' narrative tends to be more concerned with the pandemic as
well as financial and social support, while the Republicans discuss more about
other political entities such as China. We then perform an automatic framing
analysis to characterize the ways in which they frame their narratives, where
we found that the Democrats emphasize the government's role in responding to
the pandemic, and the Republicans emphasize the roles of individuals and
support for small businesses. Finally, we present a semantic role analysis that
uncovers the important characters and relationships in their narratives as well
as how they facilitate a membership categorization process. Our findings
concretely expose the gaps in the "elusive consensus" between the two parties.
Our methodologies may be applied to computationally study narratives in various
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08790">
<div class="article-summary-box-inner">
<span><p>Even to a simple and short news headline, readers react in a multitude of
ways: cognitively (e.g., inferring the writer's intent), emotionally (e.g.,
feeling distrust), and behaviorally (e.g., sharing the news with their
friends). Such reactions are instantaneous and yet complex, as they rely on
factors that go beyond interpreting the factual content the news headline.
Instead, understanding reactions require pragmatic understanding of the news
headline, including broader background knowledge about contentious news topics
as well as commonsense reasoning about people's intents and emotional
reactions. We propose Misinfo Reaction Frames, a pragmatic formalism for
modeling how readers might react to a news headline cognitively, emotionally,
and behaviorally. We also introduce a Misinfo Reaction Frames corpus, a dataset
of over 200k news headline annotated with crowdsourced reactions focusing on
global crises: the Covid-19 pandemic, climate change, and cancer. Empirical
results confirm that it is indeed possible to learn the prominent patterns of
readers' reactions to news headlines. We also find a potentially positive use
case of our model; When we present our model generated inferences to people, we
find that the machine inferences can increase readers' trust in real news while
decreasing their trust in misinformation. Our work demonstrates the feasibility
and the importance of pragmatic inferences of news to help enhance AI-guided
misinformation detection and mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation. (arXiv:2104.11710v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11710">
<div class="article-summary-box-inner">
<span><p>The audio segmentation mismatch between training data and those seen at
run-time is a major problem in direct speech translation. Indeed, while systems
are usually trained on manually segmented corpora, in real use cases they are
often presented with continuous audio requiring automatic (and sub-optimal)
segmentation. After comparing existing techniques (VAD-based, fixed-length and
hybrid segmentation methods), in this paper we propose enhanced hybrid
solutions to produce better results without sacrificing latency. Through
experiments on different domains and language pairs, we show that our methods
outperform all the other techniques, reducing by at least 30% the gap between
the traditional VAD-based approach and optimal manual segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Emotions in Hindi-English Code Mixed Text Data. (arXiv:2105.09226v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09226">
<div class="article-summary-box-inner">
<span><p>In recent times, we have seen an increased use of text chat for communication
on social networks and smartphones. This particularly involves the use of
Hindi-English code-mixed text which contains words which are not recognized in
English vocabulary. We have worked on detecting emotions in these mixed data
and classify the sentences in human emotions which are angry, fear, happy or
sad. We have used state of the art natural language processing models and
compared their performance on the dataset comprising sentences in this mixed
data. The dataset was collected and annotated from sources and then used to
train the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model. (arXiv:2105.11314v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11314">
<div class="article-summary-box-inner">
<span><p>We present RobeCzech, a monolingual RoBERTa language representation model
trained on Czech data. RoBERTa is a robustly optimized Transformer-based
pretraining approach. We show that RobeCzech considerably outperforms
equally-sized multilingual and Czech-trained contextualized language
representation models, surpasses current state of the art in all five evaluated
NLP tasks and reaches state-of-the-art results in four of them. The RobeCzech
model is released publicly at https://hdl.handle.net/11234/1-3691 and
https://huggingface.co/ufal/robeczech-base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07847">
<div class="article-summary-box-inner">
<span><p>While unbiased machine learning models are essential for many applications,
bias is a human-defined concept that can vary across tasks. Given only
input-label pairs, algorithms may lack sufficient information to distinguish
stable (causal) features from unstable (spurious) features. However, related
tasks often share similar biases -- an observation we may leverage to develop
stable classifiers in the transfer setting. In this work, we explicitly inform
the target classifier about unstable features in the source tasks.
Specifically, we derive a representation that encodes the unstable features by
contrasting different data environments in the source task. We achieve
robustness by clustering data of the target task according to this
representation and minimizing the worst-case risk across these clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConveRT for FAQ Answering. (arXiv:2108.00719v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00719">
<div class="article-summary-box-inner">
<span><p>Knowledgeable FAQ chatbots are a valuable resource to any organization. While
powerful and efficient retrieval-based models exist for English, it is rarely
the case for other languages for which the same amount of training data is not
available. In this paper, we propose a novel pre-training procedure to adapt
ConveRT, an English conversational retriever model, to other languages with
less training data available. We apply it for the first time to the task of
Dutch FAQ answering related to the COVID-19 vaccine. We show it performs better
than an open-source alternative in both a low-data regime and a high-data
regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13741">
<div class="article-summary-box-inner">
<span><p>Recent researches have demonstrated that BERT shows potential in a wide range
of natural language processing tasks. It is adopted as an encoder for many
state-of-the-art automatic summarizing systems, which achieve excellent
performance. However, so far, there is not much work done for Vietnamese. In
this paper, we showcase how BERT can be implemented for extractive text
summarization in Vietnamese on multi-document. We introduce a novel comparison
between different multilingual and monolingual BERT models. The experiment
results indicate that monolingual models produce promising results compared to
other multilingual models and previous text summarizing models for Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The VoicePrivacy 2020 Challenge: Results and findings. (arXiv:2109.00648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00648">
<div class="article-summary-box-inner">
<span><p>This paper presents the results and analyses stemming from the first
VoicePrivacy 2020 Challenge which focuses on developing anonymization solutions
for speech technology. We provide a systematic overview of the challenge design
with an analysis of submitted systems and evaluation results. In particular, we
describe the voice anonymization task and datasets used for system development
and evaluation. Also, we present different attack models and the associated
objective and subjective evaluation metrics. We introduce two anonymization
baselines and provide a summary description of the anonymization systems
developed by the challenge participants. We report objective and subjective
evaluation results for baseline and submitted systems. In addition, we present
experimental results for alternative privacy metrics and attack models
developed as a part of the post-evaluation analysis. Finally, we summarize our
insights and observations that will influence the design of the next
VoicePrivacy challenge edition and some directions for future voice
anonymization research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration. (arXiv:2109.06304v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06304">
<div class="article-summary-box-inner">
<span><p>Phrase representations derived from BERT often do not exhibit complex phrasal
compositionality, as the model relies instead on lexical similarity to
determine semantic relatedness. In this paper, we propose a contrastive
fine-tuning objective that enables BERT to produce more powerful phrase
embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal
paraphrases, which is automatically generated using a paraphrase generation
model, as well as a large-scale dataset of phrases in context mined from the
Books3 corpus. Phrase-BERT outperforms baselines across a variety of
phrase-level similarity tasks, while also demonstrating increased lexical
diversity between nearest neighbors in the vector space. Finally, as a case
study, we show that Phrase-BERT embeddings can be easily integrated with a
simple autoencoder to build a phrase-based neural topic model that interprets
topics as mixtures of words and phrases by performing a nearest neighbor search
in the embedding space. Crowdsourced evaluations demonstrate that this
phrase-based topic model produces more coherent and meaningful topics than
baseline word and phrase-level topic models, further validating the utility of
Phrase-BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10604">
<div class="article-summary-box-inner">
<span><p>While diverse question answering (QA) datasets have been proposed and
contributed significantly to the development of deep learning models for QA
tasks, the existing datasets fall short in two aspects. First, we lack QA
datasets covering complex questions that involve answers as well as the
reasoning processes to get the answers. As a result, the state-of-the-art QA
research on numerical reasoning still focuses on simple calculations and does
not provide the mathematical expressions or evidences justifying the answers.
Second, the QA community has contributed much effort to improving the
interpretability of QA models. However, these models fail to explicitly show
the reasoning process, such as the evidence order for reasoning and the
interactions between different pieces of evidence. To address the above
shortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset
with questions requiring numerical reasoning with compound mathematical
expressions. With NOAHQA, we develop an interpretable reasoning graph as well
as the appropriate evaluation metric to measure the answer quality. We evaluate
the state-of-the-art QA models trained using existing QA datasets on NOAHQA and
show that the best among them can only achieve 55.5 exact match scores, while
the human performance is 89.7. We also present a new QA model for generating a
reasoning graph where the reasoning graph metric still has a large gap compared
with that of humans, e.g., 28 scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking BERT: Understanding its Vulnerabilities for Biomedical Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11308">
<div class="article-summary-box-inner">
<span><p>Biomedical named entity recognition (NER) is a key task in the extraction of
information from biomedical literature and electronic health records. For this
task, both generic and biomedical BERT models are widely used. Robustness of
these models is vital for medical applications, such as automated medical
decision making. In this paper we investigate the vulnerability of BERT models
to variation in input data for NER through adversarial attack. Since
adversarial attack methods for NER are sparse, we propose two black-box methods
for NER based on existing methods for classification tasks. Experimental
results show that the original as well as the biomedical BERT models are highly
vulnerable to entity replacement: They can be fooled in 89.2 to 99.4% of the
cases to mislabel previously correct entities. BERT models are also vulnerable
to variation in the entity context with 20.2 to 45.0% of entities predicted
completely wrong and another 29.3 to 53.3% of entities predicted wrong
partially. Often a single change is sufficient to fool the model. BERT models
seem most vulnerable to changes in the local context of entities. Of the
biomedical BERT models, the vulnerability of BioBERT is comparable to the
original BERT model whereas SciBERT is even more vulnerable. Our results chart
the vulnerabilities of BERT models for biomedical NER and emphasize the
importance of further research into uncovering and reducing these weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11728">
<div class="article-summary-box-inner">
<span><p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively
used by states and language testing agencies alike to evaluate millions of
candidates for life-changing decisions ranging from college applications to
visa approvals. However, little research has been put to understand and
interpret the black-box nature of deep-learning based scoring algorithms.
Previous studies indicate that scoring models can be easily fooled. In this
paper, we explore the reason behind their surprising adversarial brittleness.
We utilize recent advances in interpretability to find the extent to which
features such as coherence, content, vocabulary, and relevance are important
for automated scoring mechanisms. We use this to investigate the
oversensitivity i.e., large change in output score with a little change in
input essay content) and overstability i.e., little change in output scores
with large changes in input essay content) of AES. Our results indicate that
autoscoring models, despite getting trained as "end-to-end" models with rich
contextual embeddings such as BERT, behave like bag-of-words models. A few
words determine the essay score without the requirement of any context making
the model largely overstable. This is in stark contrast to recent probing
studies on pre-trained representation learning models, which show that rich
linguistic features such as parts-of-speech and morphology are encoded by them.
Further, we also find that the models have learnt dataset biases, making them
oversensitive. To deal with these issues, we propose detection-based protection
models that can detect oversensitivity and overstability causing samples with
high accuracies. We find that our proposed models are able to detect unusual
attribution patterns and flag adversarial samples successfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15196">
<div class="article-summary-box-inner">
<span><p>We study multilingual AMR parsing from the perspective of knowledge
distillation, where the aim is to learn and improve a multilingual AMR parser
by using an existing English parser as its teacher. We constrain our
exploration in a strict multilingual setting: there is but one model to parse
all different languages including English. We identify that noisy input and
precise output are the key to successful distillation. Together with extensive
pre-training, we obtain an AMR parser whose performances surpass all previously
published results on four different foreign languages, including German,
Spanish, Italian, and Chinese, by large margins (up to 18.8 \textsc{Smatch}
points on Chinese and on average 11.3 \textsc{Smatch} points). Our parser also
achieves comparable performance on English to the latest state-of-the-art
English-only parser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00165">
<div class="article-summary-box-inner">
<span><p>Self- and semi-supervised learning methods have been actively investigated to
reduce labeled training data or enhance the model performance. However, the
approach mostly focus on in-domain performance for public datasets. In this
study, we utilize the combination of self- and semi-supervised learning methods
to solve unseen domain adaptation problem in a large-scale production setting
for online ASR model. This approach demonstrates that using the source domain
data with a small fraction of the target domain data (3%) can recover the
performance gap compared to a full data baseline: relative 13.5% WER
improvement for target domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00678">
<div class="article-summary-box-inner">
<span><p>To address the performance gap of English ASR models on L2 English speakers,
we evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;
Xu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,
2018) under different training settings. We compare \textbf{(a)} models trained
with a combination of diverse accents to ones trained with only specific
accents and \textbf{(b)} results from different single-accent models. Our
experiments demonstrate the promise of developing ASR models for non-native
English speakers, even with small amounts of L2 training data and even without
a language model. Our models also excel in the zero-shot setting where we train
on multiple L2 datasets and test on a blind L2 test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05896">
<div class="article-summary-box-inner">
<span><p>Trained on the large corpus, pre-trained language models (PLMs) can capture
different levels of concepts in context and hence generate universal language
representations. They can benefit multiple downstream natural language
processing (NLP) tasks. Although PTMs have been widely used in most NLP
applications, especially for high-resource languages such as English, it is
under-represented in Lao NLP research. Previous work on Lao has been hampered
by the lack of annotated datasets and the sparsity of language resources. In
this work, we construct a text classification dataset to alleviate the
resource-scare situation of the Lao language. We additionally present the first
transformer-based PTMs for Lao with four versions: BERT-small, BERT-base,
ELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:
part-of-speech tagging and text classification. Experiments demonstrate the
effectiveness of our Lao models. We will release our models and datasets to the
community, hoping to facilitate the future development of Lao NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Masking for Temporal Language Models. (arXiv:2110.06366v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06366">
<div class="article-summary-box-inner">
<span><p>Our world is constantly evolving, and so is the content on the web.
Consequently, our languages, often said to mirror the world, are dynamic in
nature. However, most current contextual language models are static and cannot
adapt to changes over time. In this work, we propose a temporal contextual
language model called TempoBERT, which uses time as an additional context of
texts. Our technique is based on modifying texts with temporal information and
performing time masking - specific masking for the supplementary time
information. We leverage our approach for the tasks of semantic change
detection and sentence time prediction, experimenting on diverse datasets in
terms of time, size, genre, and language. Our extensive evaluation shows that
both tasks benefit from exploiting time masking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese. (arXiv:2110.06696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06696">
<div class="article-summary-box-inner">
<span><p>Although pre-trained models (PLMs) have achieved remarkable improvements in a
wide range of NLP tasks, they are expensive in terms of time and resources.
This calls for the study of training more efficient models with less
computation but still ensures impressive performance. Instead of pursuing a
larger scale, we are committed to developing lightweight yet more powerful
models trained with equal or less computation and friendly to rapid deployment.
This technical report releases our pre-trained model called Mengzi, which
stands for a family of discriminative, generative, domain-specific, and
multimodal pre-trained model variants, capable of a wide range of language and
vision tasks. Compared with public Chinese PLMs, Mengzi is simple but more
powerful. Our lightweight model has achieved new state-of-the-art results on
the widely-used CLUE benchmark with our optimized pre-training and fine-tuning
techniques. Without modifying the model architecture, our model can be easily
employed as an alternative to existing PLMs. Our sources are available at
https://github.com/Langboat/Mengzi.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Considering user agreement in learning to predict the aesthetic quality. (arXiv:2110.06956v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06956">
<div class="article-summary-box-inner">
<span><p>How to robustly rank the aesthetic quality of given images has been a
long-standing ill-posed topic. Such challenge stems mainly from the diverse
subjective opinions of different observers about the varied types of content.
There is a growing interest in estimating the user agreement by considering the
standard deviation of the scores, instead of only predicting the mean aesthetic
opinion score. Nevertheless, when comparing a pair of contents, few studies
consider how confident are we regarding the difference in the aesthetic scores.
In this paper, we thus propose (1) a re-adapted multi-task attention network to
predict both the mean opinion score and the standard deviation in an end-to-end
manner; (2) a brand-new confidence interval ranking loss that encourages the
model to focus on image-pairs that are less certain about the difference of
their aesthetic scores. With such loss, the model is encouraged to learn the
uncertainty of the content that is relevant to the diversity of observers'
opinions, i.e., user disagreement. Extensive experiments have demonstrated that
the proposed multi-task aesthetic model achieves state-of-the-art performance
on two different types of aesthetic datasets, i.e., AVA and TMGA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Representational Continuity: Towards Unsupervised Continual Learning. (arXiv:2110.06976v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06976">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) aims to learn a sequence of tasks without forgetting
the previously acquired knowledge. However, recent advances in continual
learning are restricted to supervised continual learning (SCL) scenarios.
Consequently, they are not scalable to real-world applications where the data
distribution is often biased and unannotated. In this work, we focus on
unsupervised continual learning (UCL), where we learn the feature
representations on an unlabelled sequence of tasks and show that reliance on
annotated data is not necessary for continual learning. We conduct a systematic
study analyzing the learned feature representations and show that unsupervised
visual representations are surprisingly more robust to catastrophic forgetting,
consistently achieve better performance, and generalize better to
out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a
smoother loss landscape through qualitative analysis of the learned
representations and learns meaningful feature representations. Additionally, we
propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique
that leverages the interpolation between the current task and previous tasks'
instances to alleviate catastrophic forgetting for unsupervised
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v1 [cs.IT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06986">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new deep unfolding neural network based on the
ADMM algorithm for analysis Compressed Sensing. The proposed network jointly
learns a redundant analysis operator for sparsification and reconstructs the
signal of interest. We compare our proposed network with a state-of-the-art
unfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we
consider not only image, but also speech datasets as test examples.
Computational experiments demonstrate that our proposed network outperforms the
state-of-the-art deep unfolding networks, consistently for both real-world
image and speech datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers. (arXiv:2110.06990v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06990">
<div class="article-summary-box-inner">
<span><p>Empirical science of neural scaling laws is a rapidly growing area of
significant importance to the future of machine learning, particularly in the
light of recent breakthroughs achieved by large-scale pre-trained models such
as GPT-3, CLIP and DALL-e. Accurately predicting the neural network performance
with increasing resources such as data, compute and model size provides a more
comprehensive evaluation of different approaches across multiple scales, as
opposed to traditional point-wise comparisons of fixed-size models on
fixed-size benchmarks, and, most importantly, allows for focus on the
best-scaling, and thus most promising in the future, approaches. In this work,
we consider a challenging problem of few-shot learning in image classification,
especially when the target data distribution in the few-shot phase is different
from the source, training, data distribution, in a sense that it includes new
image classes not encountered during training. Our current main goal is to
investigate how the amount of pre-training data affects the few-shot
generalization performance of standard image classifiers. Our key observations
are that (1) such performance improvements are well-approximated by power laws
(linear log-log plots) as the training set size increases, (2) this applies to
both cases of target data coming from either the same or from a different
domain (i.e., new classes) as the training data, and (3) few-shot performance
on new classes converges at a faster rate than the standard classification
performance on previously seen classes. Our findings shed new light on the
relationship between scale and generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge. (arXiv:2110.07020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07020">
<div class="article-summary-box-inner">
<span><p>Kinship verification is the task of determining whether a parent-child,
sibling, or grandparent-grandchild relationship exists between two people and
is important in social media applications, forensic investigations, finding
missing children, and reuniting families. We demonstrate high quality kinship
verification by participating in the FG 2021 Recognizing Families in the Wild
challenge which provides the largest publicly available dataset in the field.
Our approach is among the top 3 winning entries in the competition. We ensemble
models written by both human experts and OpenAI Codex. We make our models and
code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Incubation -- Synthesizing Missing Data for Handwriting Recognition. (arXiv:2110.07040v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07040">
<div class="article-summary-box-inner">
<span><p>In this paper, we demonstrate how a generative model can be used to build a
better recognizer through the control of content and style. We are building an
online handwriting recognizer from a modest amount of training samples. By
training our controllable handwriting synthesizer on the same data, we can
synthesize handwriting with previously underrepresented content (e.g., URLs and
email addresses) and style (e.g., cursive and slanted). Moreover, we propose a
framework to analyze a recognizer that is trained with a mixture of real and
synthetic training data. We use the framework to optimize data synthesis and
demonstrate significant improvement on handwriting recognition over a model
trained on real data only. Overall, we achieve a 66% reduction in Character
Error Rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-throughput Phenotyping of Nematode Cysts. (arXiv:2110.07057v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07057">
<div class="article-summary-box-inner">
<span><p>The beet cyst nematode (BCN) Heterodera schachtii is a plant pest responsible
for crop loss on a global scale. Here, we introduce a high-throughput system
based on computer vision that allows quantifying BCN infestation and
characterizing nematode cysts through phenotyping. After recording microscopic
images of soil extracts in a standardized setting, an instance segmentation
algorithm serves to detect nematode cysts in these samples. Going beyond fast
and precise cyst counting, the image-based approach enables quantification of
cyst density and phenotyping of morphological features of cysts under different
conditions, providing the basis for high-throughput applications in agriculture
and plant breeding research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ego4D: Around the World in 3,000 Hours of Egocentric Video. (arXiv:2110.07058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07058">
<div class="article-summary-box-inner">
<span><p>We introduce Ego4D, a massive-scale egocentric video dataset and benchmark
suite. It offers 3,025 hours of daily-life activity video spanning hundreds of
scenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique
camera wearers from 74 worldwide locations and 9 different countries. The
approach to collection is designed to uphold rigorous privacy and ethics
standards with consenting participants and robust de-identification procedures
where relevant. Ego4D dramatically expands the volume of diverse egocentric
video footage publicly available to the research community. Portions of the
video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo,
and/or synchronized videos from multiple egocentric cameras at the same event.
Furthermore, we present a host of new benchmark challenges centered around
understanding the first-person visual experience in the past (querying an
episodic memory), present (analyzing hand-object manipulation, audio-visual
conversation, and social interactions), and future (forecasting activities). By
publicly sharing this massive annotated dataset and benchmark suite, we aim to
push the frontier of first-person perception. Project page:
https://ego4d-data.org/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subspace Regularizers for Few-Shot Class Incremental Learning. (arXiv:2110.07059v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07059">
<div class="article-summary-box-inner">
<span><p>Few-shot class incremental learning -- the problem of updating a trained
classifier to discriminate among an expanded set of classes with limited
labeled data -- is a key challenge for machine learning systems deployed in
non-stationary environments. Existing approaches to the problem rely on complex
model architectures and training procedures that are difficult to tune and
re-use. In this paper, we present an extremely simple approach that enables the
use of ordinary logistic regression classifiers for few-shot incremental
learning. The key to this approach is a new family of subspace regularization
schemes that encourage weight vectors for new classes to lie close to the
subspace spanned by the weights of existing classes. When combined with
pretrained convolutional feature extractors, logistic regression models trained
with subspace regularization outperform specialized, state-of-the-art
approaches to few-shot incremental image classification by up to 22% on the
miniImageNet dataset. Because of its simplicity, subspace regularization can be
straightforwardly extended to incorporate additional background information
about the new classes (including class names and descriptions specified in
natural language); these further improve accuracy by up to 2%. Our results show
that simple geometric regularization of class representations offers an
effective tool for continual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Hand Detection in Collaborative Learning Environments. (arXiv:2110.07070v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07070">
<div class="article-summary-box-inner">
<span><p>Long-term object detection requires the integration of frame-based results
over several seconds. For non-deformable objects, long-term detection is often
addressed using object detection followed by video tracking. Unfortunately,
tracking is inapplicable to objects that undergo dramatic changes in appearance
from frame to frame. As a related example, we study hand detection over long
video recordings in collaborative learning environments. More specifically, we
develop long-term hand detection methods that can deal with partial occlusions
and dramatic changes in appearance.
</p>
<p>Our approach integrates object-detection, followed by time projections,
clustering, and small region removal to provide effective hand detection over
long videos. The hand detector achieved average precision (AP) of 72% at 0.5
intersection over union (IoU). The detection results were improved to 81% by
using our optimized approach for data augmentation. The method runs at 4.7x the
real-time with AP of 81% at 0.5 intersection over the union. Our method reduced
the number of false-positive hand detections by 80% by improving IoU ratios
from 0.2 to 0.5. The overall hand detection system runs at 4x real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Spatiotemporal Augmentations on Self-Supervised Audiovisual Representation Learning. (arXiv:2110.07082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07082">
<div class="article-summary-box-inner">
<span><p>Contrastive learning of auditory and visual perception has been extremely
successful when investigated individually. However, there are still major
questions on how we could integrate principles learned from both domains to
attain effective audiovisual representations. In this paper, we present a
contrastive framework to learn audiovisual representations from unlabeled
videos. The type and strength of augmentations utilized during self-supervised
pre-training play a crucial role for contrastive frameworks to work
sufficiently. Hence, we extensively investigate composition of temporal
augmentations suitable for learning audiovisual representations; we find lossy
spatio-temporal transformations that do not corrupt the temporal coherency of
videos are the most effective. Furthermore, we show that the effectiveness of
these transformations scales with higher temporal resolution and stronger
transformation intensity. Compared to self-supervised models pre-trained on
only sampling-based temporal augmentation, self-supervised models pre-trained
with our temporal augmentations lead to approximately 6.5% gain on linear
classifier performance on AVE dataset. Lastly, we show that despite their
simplicity, our proposed transformations work well across self-supervised
learning frameworks (SimSiam, MoCoV3, etc), and benchmark audiovisual dataset
(AVE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Study on Torchvision Pre-trained Models for Fine-grained Inter-species Classification. (arXiv:2110.07097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07097">
<div class="article-summary-box-inner">
<span><p>This study aims to explore different pre-trained models offered in the
Torchvision package which is available in the PyTorch library. And investigate
their effectiveness on fine-grained images classification. Transfer Learning is
an effective method of achieving extremely good performance with insufficient
training data. In many real-world situations, people cannot collect sufficient
data required to train a deep neural network model efficiently. Transfer
Learning models are pre-trained on a large data set, and can bring a good
performance on smaller datasets with significantly lower training time.
Torchvision package offers us many models to apply the Transfer Learning on
smaller datasets. Therefore, researchers may need a guideline for the selection
of a good model. We investigate Torchvision pre-trained models on four
different data sets: 10 Monkey Species, 225 Bird Species, Fruits 360, and
Oxford 102 Flowers. These data sets have images of different resolutions, class
numbers, and different achievable accuracies. We also apply their usual
fully-connected layer and the Spinal fully-connected layer to investigate the
effectiveness of SpinalNet. The Spinal fully-connected layer brings better
performance in most situations. We apply the same augmentation for different
models for the same data set for a fair comparison. This paper may help future
Computer Vision researchers in choosing a proper Transfer Learning model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based cattle identification and action recognition. (arXiv:2110.07103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07103">
<div class="article-summary-box-inner">
<span><p>We demonstrate a working prototype for the monitoring of cow welfare by
automatically analysing the animal behaviours. Deep learning models have been
developed and tested with videos acquired in a farm, and a precision of 81.2\%
has been achieved for cow identification. An accuracy of 84.4\% has been
achieved for the detection of drinking events, and 94.4\% for the detection of
grazing events. Experimental results show that the proposed deep learning
method can be used to identify the behaviours of individual animals to enable
automated farm provenance. Our raw and ground-truth dataset will be released as
the first public video dataset for cow identification and action recognition.
Recommendations for further development are also provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Semantic Segmentation by Pixel-to-Prototype Contrast. (arXiv:2110.07110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07110">
<div class="article-summary-box-inner">
<span><p>Though image-level weakly supervised semantic segmentation (WSSS) has
achieved great progress with Class Activation Map (CAM) as the cornerstone, the
large supervision gap between classification and segmentation still hampers the
model to generate more complete and precise pseudo masks for segmentation.
</p>
<p>In this study, we explore two implicit but intuitive constraints, i.e.,
cross-view feature semantic consistency and intra(inter)-class
compactness(dispersion), to narrow the supervision gap.
</p>
<p>To this end, we propose two novel pixel-to-prototype contrast regularization
terms that are conducted cross different views and within per single view of an
image, respectively. Besides, we adopt two sample mining strategies, named
semi-hard prototype mining and hard pixel sampling, to better leverage hard
examples while reducing incorrect contrasts caused due to the absence of
precise pixel-wise labels.
</p>
<p>Our method can be seamlessly incorporated into existing WSSS models without
any changes to the base network and does not incur any extra inference burden.
Experiments on standard benchmark show that our method consistently improves
two strong baselines by large margins, demonstrating the effectiveness of our
method. Specifically, built on top of SEAM, we improve the initial seed mIoU on
PASCAL VOC 2012 from 55.4% to 61.5%. Moreover, armed with our method, we
increase the segmentation mIoU of EPS from 70.8% to 73.6%, achieving new
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nuisance-Label Supervision: Robustness Improvement by Free Labels. (arXiv:2110.07118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07118">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a Nuisance-label Supervision (NLS) module, which
can make models more robust to nuisance factor variations. Nuisance factors are
those irrelevant to a task, and an ideal model should be invariant to them. For
example, an activity recognition model should perform consistently regardless
of the change of clothes and background. But our experiments show existing
models are far from this capability. So we explicitly supervise a model with
nuisance labels to make extracted features less dependent on nuisance factors.
Although the values of nuisance factors are rarely annotated, we demonstrate
that besides existing annotations, nuisance labels can be acquired freely from
data augmentation and synthetic data. Experiments show consistent improvement
in robustness towards image corruption and appearance change in action
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brittle interpretations: The Vulnerability of TCAV and Other Concept-based Explainability Tools to Adversarial Attack. (arXiv:2110.07120v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07120">
<div class="article-summary-box-inner">
<span><p>Methods for model explainability have become increasingly critical for
testing the fairness and soundness of deep learning. A number of explainability
techniques have been developed which use a set of examples to represent a
human-interpretable concept in a model's activations. In this work we show that
these explainability methods can suffer the same vulnerability to adversarial
attacks as the models they are meant to analyze. We demonstrate this phenomenon
on two well-known concept-based approaches to the explainability of deep
learning models: TCAV and faceted feature visualization. We show that by
carefully perturbing the examples of the concept that is being investigated, we
can radically change the output of the interpretability method, e.g. showing
that stripes are not an important factor in identifying images of a zebra. Our
work highlights the fact that in safety-critical applications, there is need
for security around not only the machine learning pipeline but also the model
interpretation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region Semantically Aligned Network for Zero-Shot Learning. (arXiv:2110.07130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07130">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims to recognize unseen classes based on the
knowledge of seen classes. Previous methods focused on learning direct
embeddings from global features to the semantic space in hope of knowledge
transfer from seen classes to unseen classes. However, an unseen class shares
local visual features with a set of seen classes and leveraging global visual
features makes the knowledge transfer ineffective. To tackle this problem, we
propose a Region Semantically Aligned Network (RSAN), which maps local features
of unseen classes to their semantic attributes. Instead of using global
features which are obtained by an average pooling layer after an image encoder,
we directly utilize the output of the image encoder which maintains local
information of the image. Concretely, we obtain each attribute from a specific
region of the output and exploit these attributes for recognition. As a result,
the knowledge of seen classes can be successfully transferred to unseen classes
in a region-bases manner. In addition, we regularize the image encoder through
attribute regression with a semantic knowledge to extract robust and
attribute-related visual features. Experiments on several standard ZSL datasets
reveal the benefit of the proposed RSAN method, outperforming state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A CLIP-Enhanced Method for Video-Language Understanding. (arXiv:2110.07137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07137">
<div class="article-summary-box-inner">
<span><p>This technical report summarizes our method for the Video-And-Language
Understanding Evaluation (VALUE) challenge
(https://value-benchmark.github.io/challenge\_2021.html). We propose a
CLIP-Enhanced method to incorporate the image-text pretrained knowledge into
downstream video-text tasks. Combined with several other improved designs, our
method outperforms the state-of-the-art by $2.4\%$ ($57.58$ to $60.00$)
Meta-Ave score on VALUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Data-Driven Nuclei Segmentation For Histology Images. (arXiv:2110.07147v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07147">
<div class="article-summary-box-inner">
<span><p>An unsupervised data-driven nuclei segmentation method for histology images,
called CBM, is proposed in this work. CBM consists of three modules applied in
a block-wise manner: 1) data-driven color transform for energy compaction and
dimension reduction, 2) data-driven binarization, and 3) incorporation of
geometric priors with morphological processing. CBM comes from the first letter
of the three modules - "Color transform", "Binarization" and "Morphological
processing". Experiments on the MoNuSeg dataset validate the effectiveness of
the proposed CBM method. CBM outperforms all other unsupervised methods and
offers a competitive standing among supervised models based on the Aggregated
Jaccard Index (AJI) metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSSM: A Blueprint for Image-to-Shape Deep Learning Models. (arXiv:2110.07152v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07152">
<div class="article-summary-box-inner">
<span><p>Statistical shape modeling (SSM) characterizes anatomical variations in a
population of shapes generated from medical images. SSM requires consistent
shape representation across samples in shape cohort. Establishing this
representation entails a processing pipeline that includes anatomy
segmentation, re-sampling, registration, and non-linear optimization. These
shape representations are then used to extract low-dimensional shape
descriptors that facilitate subsequent analyses in different applications.
However, the current process of obtaining these shape descriptors from imaging
data relies on human and computational resources, requiring domain expertise
for segmenting anatomies of interest. Moreover, this same taxing pipeline needs
to be repeated to infer shape descriptors for new image data using a
pre-trained/existing shape model. Here, we propose DeepSSM, a deep
learning-based framework for learning the functional mapping from images to
low-dimensional shape descriptors and their associated shape representations,
thereby inferring statistical representation of anatomy directly from 3D
images. Once trained using an existing shape model, DeepSSM circumvents the
heavy and manual pre-processing and segmentation and significantly improves the
computational time, making it a viable solution for fully end-to-end SSM
applications. In addition, we introduce a model-based data-augmentation
strategy to address data scarcity. Finally, this paper presents and analyzes
two different architectural variants of DeepSSM with different loss functions
using three medical datasets and their downstream clinical application.
Experiments showcase that DeepSSM performs comparably or better to the
state-of-the-art SSM both quantitatively and on application-driven downstream
tasks. Therefore, DeepSSM aims to provide a comprehensive blueprint for deep
learning-based image-to-shape models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07165">
<div class="article-summary-box-inner">
<span><p>Analysis of vision-and-language models has revealed their brittleness under
linguistic phenomena such as paraphrasing, negation, textual entailment, and
word substitutions with synonyms or antonyms. While data augmentation
techniques have been designed to mitigate against these failure modes, methods
that can integrate this knowledge into the training pipeline remain
under-explored. In this paper, we present \textbf{SDRO}, a model-agnostic
method that utilizes a set linguistic transformations in a distributed robust
optimization setting, along with an ensembling technique to leverage these
transformations during inference. Experiments on benchmark datasets with images
(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as
robustness to adversarial attacks. Experiments on binary VQA explore the
generalizability of this method to other V\&amp;L tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGoLAM: Simultaneous Goal Localization and Mapping for Multi-Object Goal Navigation. (arXiv:2110.07171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07171">
<div class="article-summary-box-inner">
<span><p>We present SGoLAM, short for simultaneous goal localization and mapping,
which is a simple and efficient algorithm for Multi-Object Goal navigation.
Given an agent equipped with an RGB-D camera and a GPS/Compass sensor, our
objective is to have the agent navigate to a sequence of target objects in
realistic 3D environments. Our pipeline fully leverages the strength of
classical approaches for visual navigation, by decomposing the problem into two
key components: mapping and goal localization. The mapping module converts the
depth observations into an occupancy map, and the goal localization module
marks the locations of goal objects. The agent's policy is determined using the
information provided by the two modules: if a current goal is found, plan
towards the goal and otherwise, perform exploration. As our approach does not
require any training of neural networks, it could be used in an off-the-shelf
manner, and amenable for fast generalization in new, unseen environments.
Nonetheless, our approach performs on par with the state-of-the-art
learning-based approaches. SGoLAM is ranked 2nd in the CVPR 2021 MultiON
(Multi-Object Goal Navigation) challenge. We have made our code publicly
available at \emph{https://github.com/eunsunlee/SGoLAM}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial examples by perturbing high-level features in intermediate decoder layers. (arXiv:2110.07182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07182">
<div class="article-summary-box-inner">
<span><p>We propose a novel method for creating adversarial examples. Instead of
perturbing pixels, we use an encoder-decoder representation of the input image
and perturb intermediate layers in the decoder. This changes the high-level
features provided by the generative model. Therefore, our perturbation
possesses semantic meaning, such as a longer beak or green tints. We formulate
this task as an optimization problem by minimizing the Wasserstein distance
between the adversarial and initial images under a misclassification
constraint. We employ the projected gradient method with a simple inexact
projection. Due to the projection, all iterations are feasible, and our method
always generates adversarial images. We perform numerical experiments on the
MNIST and ImageNet datasets in both targeted and untargeted settings. We
demonstrate that our adversarial images are much less vulnerable to
steganographic defence techniques than pixel-based attacks. Moreover, we show
that our method modifies key features such as edges and that defence techniques
based on adversarial training are vulnerable to our attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Domain Adaptation for Visual Navigation with Global Map Consistency. (arXiv:2110.07184v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07184">
<div class="article-summary-box-inner">
<span><p>We propose a light-weight, self-supervised adaptation for a visual navigation
agent to generalize to unseen environment. Given an embodied agent trained in a
noiseless environment, our objective is to transfer the agent to a noisy
environment where actuation and odometry sensor noise is present. Our method
encourages the agent to maximize the consistency between the global maps
generated at different time steps in a round-trip trajectory. The proposed task
is completely self-supervised, not requiring any supervision from ground-truth
pose data or explicit noise model. In addition, optimization of the task
objective is extremely light-weight, as training terminates within a few
minutes on a commodity GPU. Our experiments show that the proposed task helps
the agent to successfully transfer to new, noisy environments. The transferred
agent exhibits improved localization and mapping accuracy, further leading to
enhanced performance in downstream visual navigation tasks. Moreover, we
demonstrate test-time adaptation with our self-supervised task to show its
potential applicability in real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Multi-task Learning for Semantics and Depth. (arXiv:2110.07197v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07197">
<div class="article-summary-box-inner">
<span><p>Multi-Task Learning (MTL) aims to enhance the model generalization by sharing
representations between related tasks for better performance. Typical MTL
methods are jointly trained with the complete multitude of ground-truths for
all tasks simultaneously. However, one single dataset may not contain the
annotations for each task of interest. To address this issue, we propose the
Semi-supervised Multi-Task Learning (SemiMTL) method to leverage the available
supervisory signals from different datasets, particularly for semantic
segmentation and depth estimation tasks. To this end, we design an adversarial
learning scheme in our semi-supervised training by leveraging unlabeled data to
optimize all the task branches simultaneously and accomplish all tasks across
datasets with partial annotations. We further present a domain-aware
discriminator structure with various alignment formulations to mitigate the
domain discrepancy issue among datasets. Finally, we demonstrate the
effectiveness of the proposed method to learn across different datasets on
challenging street view and remote sensing benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse to Fine: Video Retrieval before Moment Localization. (arXiv:2110.07201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07201">
<div class="article-summary-box-inner">
<span><p>The current state-of-the-art methods for video corpus moment retrieval (VCMR)
often use similarity-based feature alignment approach for the sake of
convenience and speed. However, late fusion methods like cosine similarity
alignment are unable to make full use of the information from both query texts
and videos. In this paper, we combine feature alignment with feature fusion to
promote the performance on VCMR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unrolled Variational Bayesian Algorithm for Image Blind Deconvolution. (arXiv:2110.07202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07202">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a variational Bayesian algorithm (VBA) for image
blind deconvolution. Our generic framework incorporates smoothness priors on
the unknown blur/image and possible affine constraints (e.g., sum to one) on
the blur kernel. One of our main contributions is the integration of VBA within
a neural network paradigm, following an unrolling methodology. The proposed
architecture is trained in a supervised fashion, which allows us to optimally
set two key hyperparameters of the VBA model and lead to further improvements
in terms of resulting visual quality. Various experiments involving
grayscale/color images and diverse kernel shapes, are performed. The numerical
examples illustrate the high performance of our approach when compared to
state-of-the-art techniques based on optimization, Bayesian estimation, or deep
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Driven Deep Image Enhancement Network for Autonomous Driving in Bad Weather. (arXiv:2110.07206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07206">
<div class="article-summary-box-inner">
<span><p>Visual perception in autonomous driving is a crucial part of a vehicle to
navigate safely and sustainably in different traffic conditions. However, in
bad weather such as heavy rain and haze, the performance of visual perception
is greatly affected by several degrading effects. Recently, deep learning-based
perception methods have addressed multiple degrading effects to reflect
real-world bad weather cases but have shown limited success due to 1) high
computational costs for deployment on mobile devices and 2) poor relevance
between image enhancement and visual perception in terms of the model ability.
To solve these issues, we propose a task-driven image enhancement network
connected to the high-level vision task, which takes in an image corrupted by
bad weather as input. Specifically, we introduce a novel low memory network to
reduce most of the layer connections of dense blocks for less memory and
computational cost while maintaining high performance. We also introduce a new
task-driven training strategy to robustly guide the high-level task model
suitable for both high-quality restoration of images and highly accurate
perception. Experiment results demonstrate that the proposed method improves
the performance among lane and 2D object detection, and depth estimation
largely under adverse weather in terms of both low memory and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media. (arXiv:2110.07235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07235">
<div class="article-summary-box-inner">
<span><p>We introduce HUMAN4D, a large and multimodal 4D dataset that contains a
variety of human activities simultaneously captured by a professional
marker-based MoCap, a volumetric capture and an audio recording system. By
capturing 2 female and $2$ male professional actors performing various
full-body movements and expressions, HUMAN4D provides a diverse set of motions
and poses encountered as part of single- and multi-person daily, physical and
social activities (jumping, dancing, etc.), along with multi-RGBD (mRGBD),
volumetric and audio data. Despite the existence of multi-view color datasets
captured with the use of hardware (HW) synchronization, to the best of our
knowledge, HUMAN4D is the first and only public resource that provides
volumetric depth maps with high synchronization precision due to the use of
intra- and inter-sensor HW-SYNC. Moreover, a spatio-temporally aligned scanned
and rigged 3D character complements HUMAN4D to enable joint research on
time-varying and high-quality dynamic meshes. We provide evaluation baselines
by benchmarking HUMAN4D with state-of-the-art human pose estimation and 3D
compression methods. For the former, we apply 2D and 3D pose estimation
algorithms both on single- and multi-view data cues. For the latter, we
benchmark open-source 3D codecs on volumetric data respecting online volumetric
video encoding and steady bit-rates. Furthermore, qualitative and quantitative
visual comparison between mesh-based volumetric data reconstructed in different
qualities showcases the available options with respect to 4D representations.
HUMAN4D is introduced to the computer vision and graphics research communities
to enable joint research on spatio-temporally aligned pose, volumetric, mRGBD
and audio data cues. The dataset and its code are available
https://tofis.github.io/myurls/human4d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Point Cloud Filtering: A Non-Local Position Based Approach. (arXiv:2110.07253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07253">
<div class="article-summary-box-inner">
<span><p>Existing position based point cloud filtering methods can hardly preserve
sharp geometric features. In this paper, we rethink point cloud filtering from
a non-learning non-local non-normal perspective, and propose a novel position
based approach for feature-preserving point cloud filtering. Unlike normal
based techniques, our method does not require the normal information. The core
idea is to first design a similarity metric to search the non-local similar
patches of a queried local patch. We then map the non-local similar patches
into a canonical space and aggregate the non-local information. The aggregated
outcome (i.e. coordinate) will be inversely mapped into the original space. Our
method is simple yet effective. Extensive experiments validate our method, and
show that it generally outperforms position based methods (deep learning and
non-learning), and generates better or comparable outcomes to normal based
techniques (deep learning and non-learning).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepMoCap: Deep Optical Motion Capture Using Multiple Depth Sensors and Retro-Reflectors. (arXiv:2110.07283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07283">
<div class="article-summary-box-inner">
<span><p>In this paper, a marker-based, single-person optical motion capture method
(DeepMoCap) is proposed using multiple spatio-temporally aligned infrared-depth
sensors and retro-reflective straps and patches (reflectors). DeepMoCap
explores motion capture by automatically localizing and labeling reflectors on
depth images and, subsequently, on 3D space. Introducing a non-parametric
representation to encode the temporal correlation among pairs of colorized
depthmaps and 3D optical flow frames, a multi-stage Fully Convolutional Network
(FCN) architecture is proposed to jointly learn reflector locations and their
temporal dependency among sequential frames. The extracted reflector 2D
locations are spatially mapped in 3D space, resulting in robust 3D optical data
extraction. The subject's motion is efficiently captured by applying a
template-based fitting technique on the extracted optical data. Two datasets
have been created and made publicly available for evaluation purposes; one
comprising multi-view depth and 3D optical flow annotated images (DMC2.5D), and
a second, consisting of spatio-temporally aligned multi-view depth images along
with skeleton, inertial and ground truth MoCap data (DMC3D). The FCN model
outperforms its competitors on the DMC2.5D dataset using 2D Percentage of
Correct Keypoints (PCK) metric, while the motion capture outcome is evaluated
against RGB-D and inertial data fusion approaches on DMC3D, outperforming the
next best method by 4.5% in total 3D PCK accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">View Vertically: A Hierarchical Network for Trajectory Prediction via Fourier Spectrums. (arXiv:2110.07288v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07288">
<div class="article-summary-box-inner">
<span><p>Learning to understand and predict future motions or behaviors for agents
like humans and robots are critical to various autonomous platforms, such as
behavior analysis, robot navigation, and self-driving cars. Intrinsic factors
such as agents' diversified personalities and decision-making styles bring rich
and diverse changes and multi-modal characteristics to their future plannings.
Besides, the extrinsic interactive factors have also brought rich and varied
changes to their trajectories. Previous methods mostly treat trajectories as
time sequences, and reach great prediction performance. In this work, we try to
focus on agents' trajectories in another view, i.e., the Fourier spectrums, to
explore their future behavior rules in a novel hierarchical way. We propose the
Transformer-based V model, which concatenates two continuous keypoints
estimation and spectrum interpolation sub-networks, to model and predict
agents' trajectories with spectrums in the keypoints and interactions levels
respectively. Experimental results show that V outperforms most of current
state-of-the-art methods on ETH-UCY and SDD trajectories dataset for about 15\%
quantitative improvements, and performs better qualitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClonalNet: Classifying Better by Focusing on Confusing Categories. (arXiv:2110.07307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07307">
<div class="article-summary-box-inner">
<span><p>Existing neural classification networks predominately adopt one-hot encoding
due to its simplicity in representing categorical data. However, the one-hot
representation neglects inter-category correlations, which may result in poor
generalization. Herein, we observe that a pre-trained baseline network has paid
attention to the target image region even though it incorrectly predicts the
image, revealing which categories confuse the baseline. This observation
motivates us to consider inter-category correlations. Therefore, we propose a
clonal network, named ClonalNet, which learns to discriminate between confusing
categories derived from the pre-trained baseline. The ClonalNet architecture
can be identical or smaller than the baseline architecture. When identical,
ClonalNet is a clonal version of the baseline but does not share weights. When
smaller, the training process of ClonalNet resembles that of the standard
knowledge distillation. The difference from knowledge distillation is that we
design a focusing-picking loss to optimize ClonalNet. This novel loss enforces
ClonalNet to concentrate on confusing categories and make more confident
predictions on ground-truth labels with the baseline reference. Experiments
show that ClonalNet significantly outperforms baseline networks and knowledge
distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling dynamic target deformation in camera calibration. (arXiv:2110.07322v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07322">
<div class="article-summary-box-inner">
<span><p>Most approaches to camera calibration rely on calibration targets of
well-known geometry. During data acquisition, calibration target and camera
system are typically moved w.r.t. each other, to allow image coverage and
perspective versatility. We show that moving the target can lead to small
temporary deformations of the target, which can introduce significant errors
into the calibration result. While static inaccuracies of calibration targets
have been addressed in previous works, to our knowledge, none of the existing
approaches can capture time-varying, dynamic deformations. To achieve
high-accuracy calibrations despite moving the target, we propose a way to
explicitly model dynamic target deformations in camera calibration. This is
achieved by using a low-dimensional deformation model with only few parameters
per image, which can be optimized jointly with target poses and intrinsics. We
demonstrate the effectiveness of modeling dynamic deformations using different
calibration targets and show its significance in a structure-from-motion
application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-center, multi-vendor automated segmentation of left ventricular anatomy in contrast-enhanced MRI. (arXiv:2110.07360v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07360">
<div class="article-summary-box-inner">
<span><p>Accurate delineation of the left ventricular boundaries in late
gadolinium-enhanced magnetic resonance imaging (LGE-MRI) is an essential step
for scar tissue quantification and patient-specific assessment of myocardial
infarction. Many deep-learning techniques have been proposed to perform
automatic segmentations of the left ventricle (LV) in LGE-MRI showing
segmentations as accurate as those obtained by expert cardiologists. Thus far,
the existing models have been overwhelmingly developed and evaluated with
LGE-MRI datasets from single clinical centers. However, in practice, LGE-MRI
images vary significantly between clinical centers within and across countries,
in particular due to differences in the MRI scanners, imaging conditions,
contrast injection protocols and local clinical practise. This work
investigates for the first time multi-center and multi-vendor LV segmentation
in LGE-MRI, by proposing, implementing and evaluating in detail several
strategies to enhance model generalizability across clinical cites. These
include data augmentation to artificially augment the image variability in the
training sample, image harmonization to align the distributions of LGE-MRI
images across centers, and transfer learning to adjust existing single-center
models to unseen images from new clinical sites. The results obtained based on
a new multi-center LGE-MRI dataset acquired in four clinical centers in Spain,
France and China, show that the combination of data augmentation and transfer
learning can lead to single-center models that generalize well to new clinical
centers not included in the original training. The proposed framework shows the
potential for developing clinical tools for automated LV segmentation in
LGE-MRI that can be deployed in multiple clinical centers across distinct
geographical locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Style Transfer via Variational AutoEncoder. (arXiv:2110.07375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07375">
<div class="article-summary-box-inner">
<span><p>Modern works on style transfer focus on transferring style from a single
image. Recently, some approaches study multiple style transfer; these, however,
are either too slow or fail to mix multiple styles. We propose ST-VAE, a
Variational AutoEncoder for latent space-based style transfer. It performs
multiple style transfer by projecting nonlinear styles to a linear latent
space, enabling to merge styles via linear interpolation before transferring
the new style to the content image. To evaluate ST-VAE, we experiment on COCO
for single and multiple style transfer. We also present a case study revealing
that ST-VAE outperforms other methods while being faster, flexible, and setting
a new path for multiple style transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation on Semantic Segmentation with Separate Affine Transformation in Batch Normalization. (arXiv:2110.07376v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07376">
<div class="article-summary-box-inner">
<span><p>In recent years, unsupervised domain adaptation (UDA) for semantic
segmentation has brought many researchers'attention. Many of them take an
approach to design a complex system so as to better align the gap between
source and target domain. Instead, we focus on the very basic structure of the
deep neural network, Batch Normalization, and propose to replace the Sharing
Affine Transformation with our proposed Separate Affine Transformation (SEAT).
The proposed SEAT is simple, easily implemented and easy to integrate into
existing adversarial learning based UDA methods. Also, to further improve the
adaptation quality, we introduce multi level adaptation by adding the
lower-level features to the higher-level ones before feeding them to the
discriminator, without adding extra discriminator like others. Experiments show
that the proposed methods is less complex without losing performance accuracy
when compared with other UDA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Safer Transportation: a self-supervised learning approach for traffic video deraining. (arXiv:2110.07379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07379">
<div class="article-summary-box-inner">
<span><p>Video monitoring of traffic is useful for traffic management and control,
traffic counting, and traffic law enforcement. However, traffic monitoring
during inclement weather such as rain is a challenging task because video
quality is corrupted by streaks of falling rain on the video image, and this
hinders reliable characterization not only of the road environment but also of
road-user behavior during such adverse weather events. This study proposes a
two-stage self-supervised learning method to remove rain streaks in traffic
videos. The first and second stages address intra- and inter-frame noise,
respectively. The results indicated that the model exhibits satisfactory
performance in terms of the image visual quality and the Peak Signal-Noise
Ratio value.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reason induced visual attention for explainable autonomous driving. (arXiv:2110.07380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07380">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) based computer vision (CV) models are generally considered
as black boxes due to poor interpretability. This limitation impedes efficient
diagnoses or predictions of system failure, thereby precluding the widespread
deployment of DLCV models in safety-critical tasks such as autonomous driving.
This study is motivated by the need to enhance the interpretability of DL model
in autonomous driving and therefore proposes an explainable DL-based framework
that generates textual descriptions of the driving environment and makes
appropriate decisions based on the generated descriptions. The proposed
framework imitates the learning process of human drivers by jointly modeling
the visual input (images) and natural language, while using the language to
induce the visual attention in the image. The results indicate strong
explainability of autonomous driving decisions obtained by focusing on relevant
features from visual inputs. Furthermore, the output attention maps enhance the
interpretability of the model not only by providing meaningful explanation to
the model behavior but also by identifying the weakness of and potential
improvement directions for the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning by Estimating Twin Class Distributions. (arXiv:2110.07402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07402">
<div class="article-summary-box-inner">
<span><p>We present TWIST, a novel self-supervised representation learning method by
classifying large-scale unlabeled datasets in an end-to-end way. We employ a
siamese network terminated by a softmax operation to produce twin class
distributions of two augmented images. Without supervision, we enforce the
class distributions of different augmentations to be consistent. In the
meantime, we regularize the class distributions to make them sharp and diverse.
Specifically, we minimize the entropy of the distribution for each sample to
make the class prediction for each sample assertive and maximize the entropy of
the mean distribution to make the predictions of different samples diverse. In
this way, TWIST can naturally avoid the trivial solutions without specific
designs such as asymmetric network, stop-gradient operation, or momentum
encoder. Different from the clustering-based methods which alternate between
clustering and learning, our method is a single learning process guided by a
unified loss function. As a result, TWIST outperforms state-of-the-art methods
on a wide range of tasks, including unsupervised classification, linear
classification, semi-supervised learning, transfer learning, and some dense
prediction tasks such as detection and segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB-D Image Inpainting Using Generative Adversarial Network with a Late Fusion Approach. (arXiv:2110.07413v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07413">
<div class="article-summary-box-inner">
<span><p>Diminished reality is a technology that aims to remove objects from video
images and fills in the missing region with plausible pixels. Most conventional
methods utilize the different cameras that capture the same scene from
different viewpoints to allow regions to be removed and restored. In this
paper, we propose an RGB-D image inpainting method using generative adversarial
network, which does not require multiple cameras. Recently, an RGB image
inpainting method has achieved outstanding results by employing a generative
adversarial network. However, RGB inpainting methods aim to restore only the
texture of the missing region and, therefore, does not recover geometric
information (i.e, 3D structure of the scene). We expand conventional image
inpainting method to RGB-D image inpainting to jointly restore the texture and
geometry of missing regions from a pair of RGB and depth images. Inspired by
other tasks that use RGB and depth images (e.g., semantic segmentation and
object detection), we propose late fusion approach that exploits the advantage
of RGB and depth information each other. The experimental results verify the
effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal Frames. (arXiv:2110.07420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07420">
<div class="article-summary-box-inner">
<span><p>Social concepts referring to non-physical objects--such as revolution,
violence, or friendship--are powerful tools to describe, index, and query the
content of visual data, including ever-growing collections of art images from
the Cultural Heritage (CH) field. While much progress has been made towards
complete image understanding in computer vision, automatic detection of social
concepts evoked by images is still a challenge. This is partly due to the
well-known semantic gap problem, worsened for social concepts given their lack
of unique physical features, and reliance on more unspecific features than
concrete concepts. In this paper, we propose the translation of recent
cognitive theories about social concept representation into a software approach
to represent them as multimodal frames, by integrating multisensory data. Our
method focuses on the extraction, analysis, and integration of multimodal
features from visual art material tagged with the concepts of interest. We
define a conceptual model and present a novel ontology for formally
representing social concepts as multimodal frames. Taking the Tate Gallery's
collection as an empirical basis, we experiment our method on a corpus of art
images to provide a proof of concept of its potential. We discuss further
directions of research, and provide all software, data sources, and results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Possibilistic Fuzzy Local Information C-Means with Automated Feature Selection for Seafloor Segmentation. (arXiv:2110.07433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07433">
<div class="article-summary-box-inner">
<span><p>The Possibilistic Fuzzy Local Information C-Means (PFLICM) method is
presented as a technique to segment side-look synthetic aperture sonar (SAS)
imagery into distinct regions of the sea-floor. In this work, we investigate
and present the results of an automated feature selection approach for SAS
image segmentation. The chosen features and resulting segmentation from the
image will be assessed based on a select quantitative clustering validity
criterion and the subset of the features that reach a desired threshold will be
used for the segmentation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse Problems Leveraging Pre-trained Contrastive Representations. (arXiv:2110.07439v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07439">
<div class="article-summary-box-inner">
<span><p>We study a new family of inverse problems for recovering representations of
corrupted data. We assume access to a pre-trained representation learning
network R(x) that operates on clean images, like CLIP. The problem is to
recover the representation of an image R(x), if we are only given a corrupted
version A(x), for some known forward operator A. We propose a supervised
inversion method that uses a contrastive objective to obtain excellent
representations for highly corrupted images. Using a linear probe on our robust
representations, we achieve a higher accuracy than end-to-end supervised
baselines when classifying images with various types of distortions, including
blurring, additive noise, and random pixel masking. We evaluate on a subset of
ImageNet and observe that our method is robust to varying levels of distortion.
Our method outperforms end-to-end baselines even with a fraction of the labeled
data in a wide range of forward operators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?. (arXiv:2110.07472v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07472">
<div class="article-summary-box-inner">
<span><p>Equivariance has emerged as a desirable property of representations of
objects subject to identity-preserving transformations that constitute a group,
such as translations and rotations. However, the expressivity of a
representation constrained by group equivariance is still not fully understood.
We address this gap by providing a generalization of Cover's Function Counting
Theorem that quantifies the number of linearly separable and group-invariant
binary dichotomies that can be assigned to equivariant representations of
objects. We find that the fraction of separable dichotomies is determined by
the dimension of the space that is fixed by the group action. We show how this
relation extends to operations such as convolutions, element-wise
nonlinearities, and global and local pooling. While other operations do not
change the fraction of separable dichotomies, local pooling decreases the
fraction, despite being a highly nonlinear operation. Finally, we test our
theory on intermediate representations of randomly initialized and fully
trained convolutional neural networks and find perfect agreement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Baseline for Single Human Motion Forecasting. (arXiv:2110.07495v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07495">
<div class="article-summary-box-inner">
<span><p>Global human motion forecasting is important in many fields, which is the
combination of global human trajectory prediction and local human pose
prediction. Visual and social information are often used to boost model
performance, however, they may consume too much computational resource. In this
paper, we establish a simple but effective baseline for single human motion
forecasting without visual and social information, equipped with useful
training tricks. Our method "futuremotion_ICCV21" outperforms existing methods
by a large margin on SoMoF benchmark. We hope our work provide new ideas for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors. (arXiv:2110.07509v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07509">
<div class="article-summary-box-inner">
<span><p>Sensor drift is a long-existing unpredictable problem that deteriorates the
performance of gaseous substance recognition, calling for an antidrift domain
adaptation algorithm. However, the prerequisite for traditional methods to
achieve fine results is to have data from both nondrift distributions (source
domain) and drift distributions (target domain) for domain alignment, which is
usually unrealistic and unachievable in real-life scenarios. To compensate for
this, in this paper, deep learning based on a target-domain-free domain
adaptation convolutional neural network (TDACNN) is proposed. The main concept
is that CNNs extract not only the domain-specific features of samples but also
the domain-invariant features underlying both the source and target domains.
Making full use of these various levels of embedding features can lead to
comprehensive utilization of different levels of characteristics, thus
achieving drift compensation by the extracted intermediate features between two
domains. In the TDACNN, a flexible multibranch backbone with a multiclassifier
structure is proposed under the guidance of bionics, which utilizes multiple
embedding features comprehensively without involving target domain data during
training. A classifier ensemble method based on maximum mean discrepancy (MMD)
is proposed to evaluate all the classifiers jointly based on the credibility of
the pseudolabel. To optimize network training, an additive angular margin
softmax loss with parameter dynamic adjustment is utilized. Experiments on two
drift datasets under different settings demonstrate the superiority of TDACNN
compared with several state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Proposal Extension with Sequential Network for Weakly Supervised Object Detection. (arXiv:2110.07511v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07511">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object detection (WSOD) has attracted more and more
attention since it only uses image-level labels and can save huge annotation
costs. Most of the WSOD methods use Multiple Instance Learning (MIL) as their
basic framework, which regard it as an instance classification problem.
However, these methods based on MIL tends to converge only on the most
discriminate regions of different instances, rather than their corresponding
complete regions, that is, insufficient integrity. Inspired by the habit of
observing things by the human, we propose a new method by comparing the initial
proposals and the extension ones to optimize those initial proposals.
Specifically, we propose one new strategy for WSOD by involving contrastive
proposal extension (CPE), which consists of multiple directional contrastive
proposal extensions (D-CPE), and each D-CPE contains encoders based on LSTM
network and corresponding decoders. %\textcolor{red}{with temporal network}.
</p>
<p>Firstly, the boundary of initial proposals in MIL is extended to different
positions according to well-designed sequential order. Then, CPE compares the
extended proposal and the initial proposal by extracting the feature semantics
of them using the encoders, and calculates the integrity of the initial
proposal to optimize the score of the initial proposal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset. (arXiv:2110.07575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07575">
<div class="article-summary-box-inner">
<span><p>Visually-grounded spoken language datasets can enable models to learn
cross-modal correspondences with very weak supervision. However, modern
audio-visual datasets contain biases that undermine the real-world performance
of models trained on that data. We introduce Spoken ObjectNet, which is
designed to remove some of these biases and provide a way to better evaluate
how effectively models will perform in real-world scenarios. This dataset
expands upon ObjectNet, which is a bias-controlled image dataset that features
similar image classes to those present in ImageNet. We detail our data
collection pipeline, which features several methods to improve caption quality,
including automated language model checks. Lastly, we show baseline results on
image retrieval and audio retrieval tasks. These results show that models
trained on other datasets and then evaluated on Spoken ObjectNet tend to
perform poorly due to biases in other datasets that the models have learned. We
also show evidence that the performance decrease is due to the dataset
controls, and not the transfer setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Temporal 3D Human Pose Estimation with Pseudo-Labels. (arXiv:2110.07578v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07578">
<div class="article-summary-box-inner">
<span><p>We present a simple, yet effective, approach for self-supervised 3D human
pose estimation. Unlike the prior work, we explore the temporal information
next to the multi-view self-supervision. During training, we rely on
triangulating 2D body pose estimates of a multiple-view camera system. A
temporal convolutional neural network is trained with the generated 3D
ground-truth and the geometric multi-view consistency loss, imposing
geometrical constraints on the predicted 3D body skeleton. During inference,
our model receives a sequence of 2D body pose estimates from a single-view to
predict the 3D body pose for each of them. An extensive evaluation shows that
our method achieves state-of-the-art performance in the Human3.6M and
MPI-INF-3DHP benchmarks. Our code and models are publicly available at
\url{https://github.com/vru2020/TM_HPE/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Playing for 3D Human Recovery. (arXiv:2110.07588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07588">
<div class="article-summary-box-inner">
<span><p>Image- and video-based 3D human recovery (i.e. pose and shape estimation)
have achieved substantial progress. However, due to the prohibitive cost of
motion capture, existing datasets are often limited in scale and diversity,
which hinders the further development of more powerful models. In this work, we
obtain massive human sequences as well as their 3D ground truths by playing
video games. Specifically, we contribute, GTA-Human, a mega-scale and
highly-diverse 3D human dataset generated with the GTA-V game engine. With a
rich set of subjects, actions, and scenarios, GTA-Human serves as both an
effective training source. Notably, the "unreasonable effectiveness of data"
phenomenon is validated in 3D human recovery using our game-playing data. A
simple frame-based baseline trained on GTA-Human already outperforms more
sophisticated methods by a large margin; for video-based methods, GTA-Human
demonstrates superiority over even the in-domain training set. We extend our
study to larger models to observe the same consistent improvements, and the
study on supervision signals suggests the rich collection of SMPL annotations
is key. Furthermore, equipped with the diverse annotations in GTA-Human, we
systematically investigate the performance of various methods under a wide
spectrum of real-world variations, e.g. camera angles, poses, and occlusions.
We hope our work could pave way for scaling up 3D human recovery to the real
world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sub-word Level Lip Reading With Visual Attention. (arXiv:2110.07603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07603">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to learn strong lip reading models that can
recognise speech in silent videos. Most prior works deal with the open-set
visual speech recognition problem by adapting existing automatic speech
recognition techniques on top of trivially pooled visual features. Instead, in
this paper we focus on the unique challenges encountered in lip reading and
propose tailored solutions. To that end we make the following contributions:
(1) we propose an attention-based pooling mechanism to aggregate visual speech
representations; (2) we use sub-word units for lip reading for the first time
and show that this allows us to better model the ambiguities of the task; (3)
we propose a training pipeline that balances the lip reading performance with
other key factors such as data and compute efficiency. Following the above, we
obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks
when training on public datasets, and even surpass models trained on
large-scale industrial datasets by using an order of magnitude less data. Our
best model achieves 22.6% word error rate on the LRS2 dataset, a performance
unprecedented for lip reading models, significantly reducing the performance
gap between lip reading and automatic speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild. (arXiv:2110.07604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07604">
<div class="article-summary-box-inner">
<span><p>Recent history has seen a tremendous growth of work exploring implicit
representations of geometry and radiance, popularized through Neural Radiance
Fields (NeRF). Such works are fundamentally based on a (implicit) {\em
volumetric} representation of occupancy, allowing them to model diverse scene
structure including translucent objects and atmospheric obscurants. But because
the vast majority of real-world scenes are composed of well-defined surfaces,
we introduce a {\em surface} analog of such implicit models called Neural
Reflectance Surfaces (NeRS). NeRS learns a neural shape representation of a
closed surface that is diffeomorphic to a sphere, guaranteeing water-tight
reconstructions. Even more importantly, surface parameterizations allow NeRS to
learn (neural) bidirectional surface reflectance functions (BRDFs) that
factorize view-dependent appearance into environmental illumination, diffuse
color (albedo), and specular "shininess." Finally, rather than illustrating our
results on synthetic scenes or controlled in-the-lab capture, we assemble a
novel dataset of multi-view images from online marketplaces for selling goods.
Such "in-the-wild" multi-view image sets pose a number of challenges, including
a small number of views with unknown/rough camera estimates. We demonstrate
that surface-based neural reconstructions enable learning from such data,
outperforming volumetric neural rendering-based reconstructions. We hope that
NeRS serves as a first step toward building scalable, high-quality libraries of
real-world shape, materials, and illumination. The project page with code and
video visualizations can be found at
https://jasonyzhang.com/ners}{jasonyzhang.com/ners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plan-Recognition-Driven Attention Modeling for Visual Recognition. (arXiv:1812.00301v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.00301">
<div class="article-summary-box-inner">
<span><p>Human visual recognition of activities or external agents involves an
interplay between high-level plan recognition and low-level perception. Given
that, a natural question to ask is: can low-level perception be improved by
high-level plan recognition? We formulate the problem of leveraging recognized
plans to generate better top-down attention maps
\cite{gazzaniga2009,baluch2011} to improve the perception performance. We call
these top-down attention maps specifically as plan-recognition-driven attention
maps. To address this problem, we introduce the Pixel Dynamics Network. Pixel
Dynamics Network serves as an observation model, which predicts next states of
object points at each pixel location given observation of pixels and
pixel-level action feature. This is like internally learning a pixel-level
dynamics model. Pixel Dynamics Network is a kind of Convolutional Neural
Network (ConvNet), with specially-designed architecture. Therefore, Pixel
Dynamics Network could take the advantage of parallel computation of ConvNets,
while learning the pixel-level dynamics model. We further prove the equivalence
between Pixel Dynamics Network as an observation model, and the belief update
in partially observable Markov decision process (POMDP) framework. We evaluate
our Pixel Dynamics Network in event recognition tasks. We build an event
recognition system, ER-PRN, which takes Pixel Dynamics Network as a subroutine,
to recognize events based on observations augmented by plan-recognition-driven
attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drone-based RGB-Infrared Cross-Modality Vehicle Detection via Uncertainty-Aware Learning. (arXiv:2003.02437v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.02437">
<div class="article-summary-box-inner">
<span><p>Drone-based vehicle detection aims at finding the vehicle locations and
categories in an aerial image. It empowers smart city traffic management and
disaster rescue. Researchers have made mount of efforts in this area and
achieved considerable progress. Nevertheless, it is still a challenge when the
objects are hard to distinguish, especially in low light conditions. To tackle
this problem, we construct a large-scale drone-based RGB-Infrared vehicle
detection dataset, termed DroneVehicle. Our DroneVehicle collects 28, 439
RGB-Infrared image pairs, covering urban roads, residential areas, parking
lots, and other scenarios from day to night. Due to the great gap between RGB
and infrared images, cross-modal images provide both effective information and
redundant information. To address this dilemma, we further propose an
uncertainty-aware cross-modality vehicle detection (UA-CMDet) framework to
extract complementary information from cross-modal images, which can
significantly improve the detection performance in low light conditions. An
uncertainty-aware module (UAM) is designed to quantify the uncertainty weights
of each modality, which is calculated by the cross-modal Intersection over
Union (IoU) and the RGB illumination value. Furthermore, we design an
illumination-aware cross-modal non-maximum suppression algorithm to better
integrate the modal-specific information in the inference phase. Extensive
experiments on the DroneVehicle dataset demonstrate the flexibility and
effectiveness of the proposed method for crossmodality vehicle detection. The
dataset can be download from https://github.com/VisDrone/DroneVehicle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Spatial-Temporal Attentive Network with Spatial Continuity for Trajectory Prediction. (arXiv:2003.06107v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06107">
<div class="article-summary-box-inner">
<span><p>It remains challenging to automatically predict the multi-agent trajectory
due to multiple interactions including agent to agent interaction and scene to
agent interaction. Although recent methods have achieved promising performance,
most of them just consider spatial influence of the interactions and ignore the
fact that temporal influence always accompanies spatial influence. Moreover,
those methods based on scene information always require extra segmented scene
images to generate multiple socially acceptable trajectories. To solve these
limitations, we propose a novel model named spatial-temporal attentive network
with spatial continuity (STAN-SC). First, spatial-temporal attention mechanism
is presented to explore the most useful and important information. Second, we
conduct a joint feature sequence based on the sequence and instant state
information to make the generative trajectories keep spatial continuity.
Experiments are performed on the two widely used ETH-UCY datasets and
demonstrate that the proposed model achieves state-of-the-art prediction
accuracy and handles more complex scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness of Deep Sensor Fusion Models. (arXiv:2006.13192v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.13192">
<div class="article-summary-box-inner">
<span><p>We experimentally study the robustness of deep camera-LiDAR fusion
architectures for 2D object detection in autonomous driving. First, we find
that the fusion model is usually both more accurate, and more robust against
single-source attacks than single-sensor deep neural networks. Furthermore, we
show that without adversarial training, early fusion is more robust than late
fusion, whereas the two perform similarly after adversarial training. However,
we note that single-channel adversarial training of deep fusion is often
detrimental even to robustness. Moreover, we observe cross-channel
externalities, where single-channel adversarial training reduces robustness to
attacks on the other channel. Additionally, we observe that the choice of
adversarial model in adversarial training is critical: using attacks restricted
to cars' bounding boxes is more effective in adversarial training and exhibits
less significant cross-channel externalities. Finally, we find that
joint-channel adversarial training helps mitigate many of the issues above, but
does not significantly boost adversarial robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Angular Attention Network for Light Field Reconstruction. (arXiv:2007.02252v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02252">
<div class="article-summary-box-inner">
<span><p>Typical learning-based light field reconstruction methods demand in
constructing a large receptive field by deepening the network to capture
correspondences between input views. In this paper, we propose a
spatial-angular attention network to perceive correspondences in the light
field non-locally, and reconstruction high angular resolution light field in an
end-to-end manner. Motivated by the non-local attention mechanism, a
spatial-angular attention module specifically for the high-dimensional light
field data is introduced to compute the responses from all the positions in the
epipolar plane for each pixel in the light field, and generate an attention map
that captures correspondences along the angular dimension. We then propose a
multi-scale reconstruction structure to efficiently implement the non-local
attention in the low spatial scale, while also preserving the high frequency
components in the high spatial scales. Extensive experiments demonstrate the
superior performance of the proposed spatial-angular attention network for
reconstructing sparsely-sampled light fields with non-Lambertian effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unseen Object Instance Segmentation for Robotic Environments. (arXiv:2007.08073v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08073">
<div class="article-summary-box-inner">
<span><p>In order to function in unstructured environments, robots need the ability to
recognize unseen objects. We take a step in this direction by tackling the
problem of segmenting unseen object instances in tabletop environments.
However, the type of large-scale real-world dataset required for this task
typically does not exist for most robotic settings, which motivates the use of
synthetic data. Our proposed method, UOIS-Net, separately leverages synthetic
RGB and synthetic depth for unseen object instance segmentation. UOIS-Net is
comprised of two stages: first, it operates only on depth to produce object
instance center votes in 2D or 3D and assembles them into rough initial masks.
Secondly, these initial masks are refined using RGB. Surprisingly, our
framework is able to learn from synthetic RGB-D data where the RGB is
non-photorealistic. To train our method, we introduce a large-scale synthetic
dataset of random objects on tabletops. We show that our method can produce
sharp and accurate segmentation masks, outperforming state-of-the-art methods
on unseen object instance segmentation. We also show that our method can
segment unseen objects for robot grasping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LT4REC:A Lottery Ticket Hypothesis Based Multi-task Practice for Video Recommendation System. (arXiv:2008.09872v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09872">
<div class="article-summary-box-inner">
<span><p>Click-through rate prediction (CTR) and post-click conversion rate prediction
(CVR) play key roles across all industrial ranking systems, such as
recommendation systems, online advertising, and search engines. Different from
the extensive research on CTR, there is much less research on CVR estimation,
whose main challenge is extreme data sparsity with one or two orders of
magnitude reduction in the number of samples than CTR. People try to solve this
problem with the paradigm of multi-task learning with the sufficient samples of
CTR, but the typical hard sharing method can't effectively solve this problem,
because it is difficult to analyze which parts of network components can be
shared and which parts are in conflict, i.e., there is a large inaccuracy with
artificially designed neurons sharing. In this paper, we model CVR in a
brand-new method by adopting the lottery-ticket-hypothesis-based sparse sharing
multi-task learning, which can automatically and flexibly learn which neuron
weights to be shared without artificial experience. Experiments on the dataset
gathered from traffic logs of Tencent video's recommendation system demonstrate
that sparse sharing in the CVR model significantly outperforms competitive
methods. Due to the nature of weight sparsity in sparse sharing, it can also
significantly reduce computational complexity and memory usage which are very
important in the industrial recommendation system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-invariant Similarity Activation Map Contrastive Learning for Retrieval-based Long-term Visual Localization. (arXiv:2009.07719v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.07719">
<div class="article-summary-box-inner">
<span><p>Visual localization is a crucial component in the application of mobile robot
and autonomous driving. Image retrieval is an efficient and effective technique
in image-based localization methods. Due to the drastic variability of
environmental conditions, e.g. illumination, seasonal and weather changes,
retrieval-based visual localization is severely affected and becomes a
challenging problem. In this work, a general architecture is first formulated
probabilistically to extract domain invariant feature through multi-domain
image translation. And then a novel gradient-weighted similarity activation
mapping loss (Grad-SAM) is incorporated for finer localization with high
accuracy. We also propose a new adaptive triplet loss to boost the contrastive
learning of the embedding in a self-supervised manner. The final coarse-to-fine
image retrieval pipeline is implemented as the sequential combination of models
without and with Grad-SAM loss. Extensive experiments have been conducted to
validate the effectiveness of the proposed approach on the CMUSeasons dataset.
The strong generalization ability of our approach is verified on RobotCar
dataset using models pre-trained on urban part of CMU-Seasons dataset. Our
performance is on par with or even outperforms the state-of-the-art image-based
localization baselines in medium or high precision, especially under the
challenging environments with illumination variance, vegetation and night-time
images. The code and pretrained models are available on
https://github.com/HanjiangHu/DISAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Point Cloud Pre-Training via Occlusion Completion. (arXiv:2010.01089v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01089">
<div class="article-summary-box-inner">
<span><p>We describe a simple pre-training approach for point clouds. It works in
three steps: 1. Mask all points occluded in a camera view; 2. Learn an
encoder-decoder model to reconstruct the occluded points; 3. Use the encoder
weights as initialisation for downstream point cloud tasks. We find that even
when we construct a single pre-training dataset (from ModelNet40), this
pre-training method improves accuracy across different datasets and encoders,
on a wide range of downstream tasks. Specifically, we show that our method
outperforms previous pre-training methods in object classification, and both
part-based and semantic segmentation tasks. We study the pre-trained features
and find that they lead to wide downstream minima, have high transformation
invariance, and have activations that are highly correlated with part labels.
Code and data are available at: https://github.com/hansen7/OcCo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Transformer. (arXiv:2011.00931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00931">
<div class="article-summary-box-inner">
<span><p>In this work, we present Point Transformer, a deep neural network that
operates directly on unordered and unstructured point sets. We design Point
Transformer to extract local and global features and relate both
representations by introducing the local-global attention mechanism, which aims
to capture spatial point relations and shape information. For that purpose, we
propose SortNet, as part of the Point Transformer, which induces input
permutation invariance by selecting points based on a learned score. The output
of Point Transformer is a sorted and permutation invariant feature list that
can directly be incorporated into common computer vision applications. We
evaluate our approach on standard classification and part segmentation
benchmarks to demonstrate competitive results compared to the prior work. Code
is publicly available at: https://github.com/engelnico/point-transformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Physics-aware Inference of Cloth Deformation for Monocular Human Performance Capture. (arXiv:2011.12866v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12866">
<div class="article-summary-box-inner">
<span><p>Recent monocular human performance capture approaches have shown compelling
dense tracking results of the full body from a single RGB camera. However,
existing methods either do not estimate clothing at all or model cloth
deformation with simple geometric priors instead of taking into account the
underlying physical principles. This leads to noticeable artifacts in their
reconstructions, e.g. baked-in wrinkles, implausible deformations that
seemingly defy gravity, and intersections between cloth and body. To address
these problems, we propose a person-specific, learning-based method that
integrates a simulation layer into the training process to provide for the
first time physics supervision in the context of weakly supervised deep
monocular human performance capture. We show how integrating physics into the
training process improves the learned cloth deformations, allows modeling
clothing as a separate piece of geometry, and largely reduces cloth-body
intersections. Relying only on weak 2D multi-view supervision during training,
our approach leads to a significant improvement over current state-of-the-art
methods and is thus a clear step towards realistic monocular capture of the
entire deforming surface of a clothed human.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-mining: Self-Supervised Learning for Sparsely Annotated Object Detection. (arXiv:2012.01950v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01950">
<div class="article-summary-box-inner">
<span><p>Object detectors usually achieve promising results with the supervision of
complete instance annotations. However, their performance is far from
satisfactory with sparse instance annotations. Most existing methods for
sparsely annotated object detection either re-weight the loss of hard negative
samples or convert the unlabeled instances into ignored regions to reduce the
interference of false negatives. We argue that these strategies are
insufficient since they can at most alleviate the negative effect caused by
missing annotations. In this paper, we propose a simple but effective
mechanism, called Co-mining, for sparsely annotated object detection. In our
Co-mining, two branches of a Siamese network predict the pseudo-label sets for
each other. To enhance multi-view learning and better mine unlabeled instances,
the original image and corresponding augmented image are used as the inputs of
two branches of the Siamese network, respectively. Co-mining can serve as a
general training mechanism applied to most of modern object detectors.
Experiments are performed on MS COCO dataset with three different sparsely
annotated settings using two typical frameworks: anchor-based detector
RetinaNet and anchor-free detector FCOS. Experimental results show that our
Co-mining with RetinaNet achieves 1.4%~2.1% improvements compared with
different baselines and surpasses existing methods under the same sparsely
annotated setting. Code is available at
https://github.com/megvii-research/Co-mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unleashing the Power of Contrastive Self-Supervised Visual Models via Contrast-Regularized Fine-Tuning. (arXiv:2102.06605v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06605">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning (CSL) has attracted increasing attention
for model pre-training via unlabeled data. The resulted CSL models provide
instance-discriminative visual features that are uniformly scattered in the
feature space. During deployment, the common practice is to directly fine-tune
CSL models with cross-entropy, which however may not be the best strategy in
practice. Although cross-entropy tends to separate inter-class features, the
resulting models still have limited capability for reducing intra-class feature
scattering that exists in CSL models. In this paper, we investigate whether
applying contrastive learning to fine-tuning would bring further benefits, and
analytically find that optimizing the contrastive loss benefits both
discriminative representation learning and model optimization during
fine-tuning. Inspired by these findings, we propose Contrast-regularized tuning
(Core-tuning), a new approach for fine-tuning CSL models. Instead of simply
adding the contrastive loss to the objective of fine-tuning, Core-tuning
further applies a novel hard pair mining strategy for more effective
contrastive fine-tuning, as well as smoothing the decision boundary to better
exploit the learned discriminative feature space. Extensive experiments on
image classification and semantic segmentation verify the effectiveness of
Core-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Reconstruction and Disparity from Spatio-Spectrally Coded Light Fields via Multi-Task Deep Learning. (arXiv:2103.10179v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10179">
<div class="article-summary-box-inner">
<span><p>We present a novel method to reconstruct a spectral central view and its
aligned disparity map from spatio-spectrally coded light fields. Since we do
not reconstruct an intermediate full light field from the coded measurement, we
refer to this as principal reconstruction. The coded light fields correspond to
those captured by a light field camera in the unfocused design with a
spectrally coded microlens array. In this application, the spectrally coded
light field camera can be interpreted as a single-shot spectral depth camera.
</p>
<p>We investigate several multi-task deep learning methods and propose a new
auxiliary loss-based training strategy to enhance the reconstruction
performance. The results are evaluated using a synthetic as well as a new
real-world spectral light field dataset that we captured using a custom-built
camera. The results are compared to state-of-the art compressed sensing
reconstruction and disparity estimation.
</p>
<p>We achieve a high reconstruction quality for both synthetic and real-world
coded light fields. The disparity estimation quality is on par with or even
outperforms state-of-the-art disparity estimation from uncoded RGB light
fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Point Cloud Registration with Multi-Scale Architecture and Unsupervised Transfer Learning. (arXiv:2103.14533v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14533">
<div class="article-summary-box-inner">
<span><p>We propose a method for generalizing deep learning for 3D point cloud
registration on new, totally different datasets. It is based on two components,
MS-SVConv and UDGE. Using Multi-Scale Sparse Voxel Convolution, MS-SVConv is a
fast deep neural network that outputs the descriptors from point clouds for 3D
registration between two scenes. UDGE is an algorithm for transferring deep
networks on unknown datasets in a unsupervised way. The interest of the
proposed method appears while using the two components, MS-SVConv and UDGE,
together as a whole, which leads to state-of-the-art results on real world
registration datasets such as 3DMatch, ETH and TUM. The code is publicly
available at https://github.com/humanpose1/MS-SVConv .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective. (arXiv:2103.17263v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17263">
<div class="article-summary-box-inner">
<span><p>Learning a good representation for space-time correspondence is the key for
various computer vision tasks, including tracking object bounding boxes and
performing video object pixel segmentation. To learn generalizable
representation for correspondence in large-scale, a variety of self-supervised
pretext tasks are proposed to explicitly perform object-level or patch-level
similarity learning. Instead of following the previous literature, we propose
to learn correspondence using Video Frame-level Similarity (VFS) learning, i.e,
simply learning from comparing video frames. Our work is inspired by the recent
success in image-level contrastive learning and similarity learning for visual
recognition. Our hypothesis is that if the representation is good for
recognition, it requires the convolutional features to find correspondence
between similar objects or parts. Our experiments show surprising results that
VFS surpasses state-of-the-art self-supervised approaches for both OTB visual
object tracking and DAVIS video object segmentation. We perform detailed
analysis on what matters in VFS and reveals new properties on image and frame
level similarity learning. Project page with code is available at
https://jerryxu.net/VFS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct-PoseNet: Absolute Pose Regression with Photometric Consistency. (arXiv:2104.04073v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04073">
<div class="article-summary-box-inner">
<span><p>We present a relocalization pipeline, which combines an absolute pose
regression (APR) network with a novel view synthesis based direct matching
module, offering superior accuracy while maintaining low inference time. Our
contribution is twofold: i) we design a direct matching module that supplies a
photometric supervision signal to refine the pose regression network via
differentiable rendering; ii) we modify the rotation representation from the
classical quaternion to SO(3) in pose regression, removing the need for
balancing rotation and translation loss terms. As a result, our network
Direct-PoseNet achieves state-of-the-art performance among all other
single-image APR methods on the 7-Scenes benchmark and the LLFF dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Deep Networks for 3D Mitochondria Instance Segmentation. (arXiv:2104.07961v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07961">
<div class="article-summary-box-inner">
<span><p>Mitochondria instance segmentation from electron microscopy (EM) images has
seen notable progress since the introduction of deep learning methods. In this
paper, we propose two advanced deep networks, named Res-UNet-R and Res-UNet-H,
for 3D mitochondria instance segmentation from Rat and Human samples.
Specifically, we design a simple yet effective anisotropic convolution block
and deploy a multi-scale training strategy, which together boost the
segmentation performance. Moreover, we enhance the generalizability of the
trained models on the test set by adding a denoising operation as
pre-processing. In the Large-scale 3D Mitochondria Instance Segmentation
Challenge at ISBI 2021, our method ranks the 1st place. Code is available at
https://github.com/Limingxing00/MitoEM2021-Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft Expectation and Deep Maximization for Image Feature Detection. (arXiv:2104.10291v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10291">
<div class="article-summary-box-inner">
<span><p>Central to the application of many multi-view geometry algorithms is the
extraction of matching points between multiple viewpoints, enabling classical
tasks such as camera pose estimation and 3D reconstruction. Many approaches
that characterize these points have been proposed based on hand-tuned
appearance models or data-driven learning methods. We propose Soft Expectation
and Deep Maximization (SEDM), an iterative unsupervised learning process that
directly optimizes the repeatability of the features by posing the problem in a
similar way to expectation maximization (EM). We found convergence to be
reliable and the new model to be more lighting invariant and better at localize
the underlying 3D points in a scene, improving SfM quality when compared to
other state of the art deep learning detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantization of Deep Neural Networks for Accurate Edge Computing. (arXiv:2104.12046v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12046">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have demonstrated their great potential in recent
years, exceeding the per-formance of human experts in a wide range of
applications. Due to their large sizes, however, compressiontechniques such as
weight quantization and pruning are usually applied before they can be
accommodated onthe edge. It is generally believed that quantization leads to
performance degradation, and plenty of existingworks have explored quantization
strategies aiming at minimum accuracy loss. In this paper, we argue
thatquantization, which essentially imposes regularization on weight
representations, can sometimes help toimprove accuracy. We conduct
comprehensive experiments on three widely used applications: fully con-nected
network (FCN) for biomedical image segmentation, convolutional neural network
(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)
for automatic speech recognition, and experi-mental results show that
quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three
applicationsrespectively with 3.5x-6.4x memory reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13677">
<div class="article-summary-box-inner">
<span><p>This paper presents an efficient multi-scale vision Transformer, called ResT,
that capably served as a general-purpose backbone for image recognition. Unlike
existing Transformer methods, which employ standard Transformer blocks to
tackle raw images with a fixed resolution, our ResT have several advantages:
(1) A memory-efficient multi-head self-attention is built, which compresses the
memory by a simple depth-wise convolution, and projects the interaction across
the attention-heads dimension while keeping the diversity ability of
multi-heads; (2) Position encoding is constructed as spatial attention, which
is more flexible and can tackle with input images of arbitrary size without
interpolation or fine-tune; (3) Instead of the straightforward tokenization at
the beginning of each stage, we design the patch embedding as a stack of
overlapping convolution operation with stride on the 2D-reshaped token map. We
comprehensively validate ResT on image classification and downstream tasks.
Experimental results show that the proposed ResT can outperform the recently
state-of-the-art backbones by a large margin, demonstrating the potential of
ResT as strong backbones. The code and models will be made publicly available
at https://github.com/wofmanaf/ResT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Seamless and High-Performance Out-of-Distribution Detection Approach Simply Replacing the SoftMax Loss. (arXiv:2105.14399v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14399">
<div class="article-summary-box-inner">
<span><p>Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (e.g., classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all of the previously
mentioned drawbacks). The entropic out-of-distribution detection solution uses
the IsoMax loss for training and the entropic score for out-of-distribution
detection. The IsoMax loss works as a SoftMax loss drop-in replacement because
swapping the SoftMax loss with the IsoMax loss requires no changes in the
model's architecture or training procedures/hyperparameters. In this paper, we
perform what we call an isometrization of the distances used in the IsoMax
loss. Additionally, we propose replacing the entropic score with the minimum
distance score. Experiments showed that these simple modifications increase
out-of-distribution detection performance while keeping the solution seamless.
Besides being competitive with or outperforming all major current approaches,
the proposed solution avoids all their current limitations in addition to being
much easier to use because only a simple loss replacement for training the
neural network is required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural Network Robustness via Persistency of Excitation. (arXiv:2106.02078v4 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02078">
<div class="article-summary-box-inner">
<span><p>Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a neural network via gradient descent is a
parameter estimation problem. In adaptive control, maintaining persistency of
excitation (PoE) is integral to ensuring convergence of parameter estimates in
dynamical systems to their true values. We show that parameter estimation with
gradient descent can be modeled as a sampling of an adaptive linear
time-varying continuous system. Leveraging this model, and with inspiration
from Model-Reference Adaptive Control (MRAC), we prove a sufficient condition
to constrain gradient descent updates to reference persistently excited
trajectories converging to the true parameters. The sufficient condition is
achieved when the learning rate is less than the inverse of the Lipschitz
constant of the gradient of loss function. We provide an efficient technique
for estimating the corresponding Lipschitz constant in practice using extreme
value theory. Our experimental results in both standard and adversarial
training illustrate that networks trained with the PoE-motivated learning rate
schedule have similar clean accuracy but are significantly more robust to
adversarial attacks than models trained using current state-of-the-art
heuristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07847">
<div class="article-summary-box-inner">
<span><p>While unbiased machine learning models are essential for many applications,
bias is a human-defined concept that can vary across tasks. Given only
input-label pairs, algorithms may lack sufficient information to distinguish
stable (causal) features from unstable (spurious) features. However, related
tasks often share similar biases -- an observation we may leverage to develop
stable classifiers in the transfer setting. In this work, we explicitly inform
the target classifier about unstable features in the source tasks.
Specifically, we derive a representation that encodes the unstable features by
contrasting different data environments in the source task. We achieve
robustness by clustering data of the target task according to this
representation and minimizing the worst-case risk across these clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving On-Screen Sound Separation for Open-Domain Videos with Audio-Visual Self-Attention. (arXiv:2106.09669v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09669">
<div class="article-summary-box-inner">
<span><p>We introduce a state-of-the-art audio-visual on-screen sound separation
system which is capable of learning to separate sounds and associate them with
on-screen objects by looking at in-the-wild videos. We identify limitations of
previous work on audio-visual on-screen sound separation, including the
simplicity and coarse resolution of spatio-temporal attention, and poor
convergence of the audio separation model. Our proposed model addresses these
issues using cross-modal and self-attention modules that capture audio-visual
dependencies at a finer resolution over time, and by unsupervised pre-training
of audio separation model. These improvements allow the model to generalize to
a much wider set of unseen videos. We also show a robust way to further improve
the generalization capability of our models by calibrating the probabilities of
our audio-visual on-screen classifier, using only a small amount of in-domain
videos labeled for their on-screen presence. For evaluation and semi-supervised
training, we collected human annotations of on-screen audio from a large
database of in-the-wild videos (YFCC100m). Our results show marked improvements
in on-screen separation performance, in more general conditions than previous
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training also Transfers Non-Robustness. (arXiv:2106.10989v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10989">
<div class="article-summary-box-inner">
<span><p>Pre-training has enabled state-of-the-art results on many tasks. In spite of
its recognized contribution to generalization, we observed in this study that
pre-training also transfers adversarial non-robustness from pre-trained model
into fine-tuned model in the downstream tasks. Using image classification as an
example, we first conducted experiments on various datasets and network
backbones to uncover the adversarial non-robustness in fine-tuned model.
Further analysis was conducted on examining the learned knowledge of fine-tuned
model and standard model, and revealed that the reason leading to the
non-robustness is the non-robust features transferred from pre-trained model.
Finally, we analyzed the preference for feature learning of the pre-trained
model, explored the factors influencing robustness, and introduced a simple
robust pre-traning solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity. (arXiv:2106.14568v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14568">
<div class="article-summary-box-inner">
<span><p>Recent works on sparse neural networks have demonstrated the possibility to
train a sparse subnetwork independently from scratch, to match the performance
of its corresponding dense network. However, identifying such sparse
subnetworks (winning tickets) either involves a costly iterative
train-prune-retrain process (e.g., Lottery Ticket Hypothesis) or an
over-extended training time (e.g., Dynamic Sparse Training). In this work, we
draw a unique connection between sparse neural network training and the deep
ensembling technique, yielding a novel ensemble learning framework called
FreeTickets. Instead of starting from a dense network, FreeTickets randomly
initializes a sparse subnetwork and then trains the subnetwork while
dynamically adjusting its sparse mask, resulting in many diverse sparse
subnetworks throughout the training process. FreeTickets is defined as the
ensemble of these sparse subnetworks freely obtained during this one-pass,
sparse-to-sparse training, which uses only a fraction of the computational
resources required by the vanilla dense training. Moreover, despite being an
ensemble of models, FreeTickets has even fewer parameters and training FLOPs
compared to a single dense model: this seemingly counter-intuitive outcome is
due to the high sparsity of each subnetwork. FreeTickets is observed to
demonstrate a significant all-round improvement compared to standard dense
baselines, in prediction accuracy, uncertainty estimation, robustness, and
efficiency. FreeTickets easily outperforms the naive deep ensemble with
ResNet50 on ImageNet using only a quarter of the training FLOPs required by the
latter. Our results provide insights into the strength of sparse neural
networks and suggest that the benefits of sparsity go way beyond the usually
expected inference efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Attention Mechanism in 3D Point Cloud Object Detection. (arXiv:2108.00620v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00620">
<div class="article-summary-box-inner">
<span><p>Object detection in three-dimensional (3D) space attracts much interest from
academia and industry since it is an essential task in AI-driven applications
such as robotics, autonomous driving, and augmented reality. As the basic
format of 3D data, the point cloud can provide detailed geometric information
about the objects in the original 3D space. However, due to 3D data's sparsity
and unorderedness, specially designed networks and modules are needed to
process this type of data. Attention mechanism has achieved impressive
performance in diverse computer vision tasks; however, it is unclear how
attention modules would affect the performance of 3D point cloud object
detection and what sort of attention modules could fit with the inherent
properties of 3D data. This work investigates the role of the attention
mechanism in 3D point cloud object detection and provides insights into the
potential of different attention modules. To achieve that, we comprehensively
investigate classical 2D attentions, novel 3D attentions, including the latest
point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the
detailed experiments and analysis, we conclude the effects of different
attention modules. This paper is expected to serve as a reference source for
benefiting attention-embedded 3D point cloud object detection. The code and
trained models are available at:
https://github.com/ShiQiu0419/attentions_in_3D_detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ME-PCN: Point Completion Conditioned on Mask Emptiness. (arXiv:2108.08187v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08187">
<div class="article-summary-box-inner">
<span><p>Point completion refers to completing the missing geometries of an object
from incomplete observations. Main-stream methods predict the missing shapes by
decoding a global feature learned from the input point cloud, which often leads
to deficient results in preserving topology consistency and surface details. In
this work, we present ME-PCN, a point completion network that leverages
`emptiness' in 3D shape space. Given a single depth scan, previous methods
often encode the occupied partial shapes while ignoring the empty regions (e.g.
holes) in depth maps. In contrast, we argue that these `emptiness' clues
indicate shape boundaries that can be used to improve topology representation
and detail granularity on surfaces. Specifically, our ME-PCN encodes both the
occupied point cloud and the neighboring `empty points'. It estimates
coarse-grained but complete and reasonable surface points in the first stage,
followed by a refinement stage to produce fine-grained surface details.
Comprehensive experiments verify that our ME-PCN presents better qualitative
and quantitative performance against the state-of-the-art. Besides, we further
prove that our `emptiness' design is lightweight and easy to embed in existing
methods, which shows consistent effectiveness in improving the CD and EMD
scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Bangla License Plate Recognition System for Low Resource Video-based Applications. (arXiv:2108.08339v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08339">
<div class="article-summary-box-inner">
<span><p>Automatic License Plate Recognition systems aim to provide a solution for
detecting, localizing, and recognizing license plate characters from vehicles
appearing in video frames. However, deploying such systems in the real world
requires real-time performance in low-resource environments. In our paper, we
propose a two-stage detection pipeline paired with Vision API that provides
real-time inference speed along with consistently accurate detection and
recognition performance. We used a haar-cascade classifier as a filter on top
of our backbone MobileNet SSDv2 detection model. This reduces inference time by
only focusing on high confidence detections and using them for recognition. We
also impose a temporal frame separation strategy to distinguish between
multiple vehicle license plates in the same clip. Furthermore, there are no
publicly available Bangla license plate datasets, for which we created an image
dataset and a video dataset containing license plates in the wild. We trained
our models on the image dataset and achieved an AP(0.5) score of 86% and tested
our pipeline on the video dataset and observed reasonable detection and
recognition performance (82.7% detection rate, and 60.8% OCR F1 score) with
real-time processing speed (27.2 frames per second).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Oriented Object Detection in Aerial Images Based on Area Ratio of Parallelogram. (arXiv:2109.10187v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10187">
<div class="article-summary-box-inner">
<span><p>Rotated object detection is a challenging task in aerial images since the
objects in aerial images are displayed in arbitrary directions and are
frequently densely packed. Although considerable progress has been made, there
are still challenges that existing regression-based rotation detectors suffer
from the representation ambiguity. In this paper, we propose a simple,
practical framework to optimize the bounding box regression for rotating
objects. Rather than directly regressing the five parameters (coordinates of
the central point, width, height, and rotation angle) or the four vertices, we
employ the area ratio of the parallelogram (ARP) to describe a multi-oriented
object accurately. Specifically, ARP regresses coordinates of the center point,
height, and width of the oriented object's minimum circumscribed rectangle and
three area ratios. It may facilitate learning offset and avoid the issue of
angular periodicity or label points sequence for oriented objects. To further
remedy the confusion issue of nearly horizontal objects, the area ratio between
the object and its minimal circumscribed rectangle has been used to guide the
selection of horizontal or oriented detection for each object. The rotation
efficient IOU loss (R-EIOU) connects the flat bounding box with the three area
ratios and improves the accuracy of the rotating bounding. Experimental results
on remote sensing datasets, including HRSC2016, DOTA, and UCAS-AOD, show that
our method achieves superior detection performance than many state-of-the-art
approaches. The code and model will be coming with the paper published.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01052">
<div class="article-summary-box-inner">
<span><p>We introduce Learn then Test, a framework for calibrating machine learning
models so that their predictions satisfy explicit, finite-sample statistical
guarantees regardless of the underlying model and (unknown) data-generating
distribution. The framework addresses, among other examples, false discovery
rate control in multi-label classification, intersection-over-union control in
instance segmentation, and the simultaneous control of the type-1 error of
outlier detection and confidence set coverage in classification or regression.
To accomplish this, we solve a key technical challenge: the control of
arbitrary risks that are not necessarily monotonic. Our main insight is to
reframe the risk-control problem as multiple hypothesis testing, enabling
techniques and mathematical arguments different from those in the previous
literature. We use our framework to provide new calibration methods for several
core machine learning tasks with detailed worked examples in computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MovingFashion: a Benchmark for the Video-to-Shop Challenge. (arXiv:2110.02627v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02627">
<div class="article-summary-box-inner">
<span><p>Retrieving clothes which are worn in social media videos (Instagram, TikTok)
is the latest frontier of e-fashion, referred to as "video-to-shop" in the
computer vision literature. In this paper we present MovingFashion, the first
publicly available dataset to cope with this challenge. MovingFashion is
composed of 14855 social videos, each one of them associated to e-commerce
"shop" images where the corresponding clothing items are clearly portrayed. In
addition, we present a network for retrieving the shop images in this scenario,
dubbed SEAM Match-RCNN. The model is trained by image-to-video domain
adaptation, allowing to use video sequences where only their association with a
shop image is given, eliminating the need of millions of annotated bounding
boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted
sum of few frames (10) of a social video is enough to individuate the correct
product within the first 5 retrieved items in a 14K+ shop element gallery with
an accuracy of 80%. This provides the best performance on MovingFashion,
comparing exhaustively against the related state-of-the-art approaches and
alternative baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Unlearning of Backdoors via Implicit Hypergradient. (arXiv:2110.03735v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03735">
<div class="article-summary-box-inner">
<span><p>We propose a minimax formulation for removing backdoors from a given poisoned
model based on a small set of clean data. This formulation encompasses much of
prior work on backdoor removal. We propose the Implicit Bacdoor Adversarial
Unlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which
breaks down the minimax into separate inner and outer problems, our algorithm
utilizes the implicit hypergradient to account for the interdependence between
inner and outer optimization. We theoretically analyze its convergence and the
generalizability of the robustness gained by solving minimax on clean data to
unseen test data. In our evaluation, we compare I-BAU with six state-of-art
backdoor defenses on seven backdoor attacks over two datasets and various
attack settings, including the common setting where the attacker targets one
class as well as important but underexplored settings where multiple classes
are targeted. I-BAU's performance is comparable to and most often significantly
better than the best baseline. Particularly, its performance is more robust to
the variation on triggers, attack settings, poison ratio, and clean data size.
Moreover, I-BAU requires less computation to take effect; particularly, it is
more than $13\times$ faster than the most efficient baseline in the
single-target attack setting. Furthermore, it can remain effective in the
extreme case where the defender can only access 100 clean samples -- a setting
where all the baselines fail to produce acceptable results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03825">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are known to be vulnerable to adversarial
attacks. A range of defense methods have been proposed to train adversarially
robust DNNs, among which adversarial training has demonstrated promising
results. However, despite preliminary understandings developed for adversarial
training, it is still not clear, from the architectural perspective, what
configurations can lead to more robust DNNs. In this paper, we address this gap
via a comprehensive investigation on the impact of network width and depth on
the robustness of adversarially trained DNNs. Specifically, we make the
following key observations: 1) more parameters (higher model capacity) does not
necessarily help adversarial robustness; 2) reducing capacity at the last stage
(the last group of blocks) of the network can actually improve adversarial
robustness; and 3) under the same parameter budget, there exists an optimal
architectural configuration for adversarial robustness. We also provide a
theoretical analysis explaning why such network configuration can help
robustness. These architectural insights can help design adversarially robust
DNNs. Code is available at \url{https://github.com/HanxunH/RobustWRN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04079">
<div class="article-summary-box-inner">
<span><p>Reliable and accurate lane detection is of vital importance for the safe
performance of Lane Keeping Assistance and Lane Departure Warning systems.
However, under certain challenging peculiar circumstances, it is difficult to
get satisfactory performance in accurately detecting the lanes from one single
image which is often the case in current literature. Since lane markings are
continuous lines, the lanes that are difficult to be accurately detected in the
single current image can potentially be better deduced if information from
previous frames is incorporated. This study proposes a novel hybrid
spatial-temporal sequence-to-one deep learning architecture making full use of
the spatial-temporal information in multiple continuous image frames to detect
lane markings in the very last current frame. Specifically, the hybrid model
integrates the single image feature extraction module with the spatial
convolutional neural network (SCNN) embedded for excavating spatial features
and relationships in one single image, the spatial-temporal feature integration
module with spatial-temporal recurrent neural network (ST-RNN), which can
capture the spatial-temporal correlations and time dependencies among image
sequences, and the encoder-decoder structure, which makes this image
segmentation problem work in an end-to-end supervised learning format.
Extensive experiments reveal that the proposed model can effectively handle
challenging driving scenes and outperforms available state-of-the-art methods
with a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Prototype Classifier for Few-shot Image Classification. (arXiv:2110.05076v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05076">
<div class="article-summary-box-inner">
<span><p>The prototypical network is a prototype classifier based on meta-learning and
is widely used for few-shot learning because it classifies unseen examples by
constructing class-specific prototypes without adjusting hyper-parameters
during meta-testing. Interestingly, recent research has attracted a lot of
attention, showing that a linear classifier with fine-tuning, which does not
use a meta-learning algorithm, performs comparably with the prototypical
network. However, fine-tuning requires additional hyper-parameters when
adapting a model to a new environment. In addition, although the purpose of
few-shot learning is to enable the model to quickly adapt to a new environment,
fine-tuning needs to be applied every time a new class appears, making fast
adaptation difficult. In this paper, we analyze how a prototype classifier
works equally well without fine-tuning and meta-learning. We experimentally
found that directly using the feature vector extracted using standard
pre-trained models to construct a prototype classifier in meta-testing does not
perform as well as the prototypical network and linear classifiers with
fine-tuning and feature vectors of pre-trained models. Thus, we derive a novel
generalization bound for the prototypical network and show that focusing on the
variance of the norm of a feature vector can improve performance. We
experimentally investigated several normalization methods for minimizing the
variance of the norm and found that the same performance can be obtained by
using the L2 normalization and embedding space transformation without
fine-tuning or meta-learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP4Caption ++: Multi-CLIP for Video Caption. (arXiv:2110.05204v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05204">
<div class="article-summary-box-inner">
<span><p>This report describes our solution to the VALUE Challenge 2021 in the
captioning task. Our solution, named CLIP4Caption++, is built on
X-Linear/X-Transformer, which is an advanced model with encoder-decoder
architecture. We make the following improvements on the proposed
CLIP4Caption++: We employ an advanced encoder-decoder model architecture
X-Transformer as our main framework and make the following improvements: 1) we
utilize three strong pre-trained CLIP models to extract the text-related
appearance visual features. 2) we adopt the TSN sampling strategy for data
enhancement. 3) we involve the video subtitle information to provide richer
semantic information. 3) we introduce the subtitle information, which fuses
with the visual features as guidance. 4) we design word-level and
sentence-level ensemble strategies. Our proposed method achieves 86.5, 148.4,
64.5 CIDEr scores on VATEX, YC2C, and TVC datasets, respectively, which shows
the superior performance of our proposed CLIP4Caption++ on all three datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ByteTrack: Multi-Object Tracking by Associating Every Detection Box. (arXiv:2110.06864v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06864">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) aims at estimating bounding boxes and identities
of objects in videos. Most methods obtain identities by associating detection
boxes whose scores are higher than a threshold. The objects with low detection
scores, e.g. occluded objects, are simply thrown away, which brings
non-negligible true object missing and fragmented trajectories. To solve this
problem, we present a simple, effective and generic association method, called
BYTE, tracking BY associaTing Every detection box instead of only the high
score ones. For the low score detection boxes, we utilize their similarities
with tracklets to recover true objects and filter out the background
detections. We apply BYTE to 9 different state-of-the-art trackers and achieve
consistent improvement on IDF1 score ranging from 1 to 10 points. To put
forwards the state-of-the-art performance of MOT, we design a simple and strong
tracker, named ByteTrack. For the first time, we achieve 80.3 MOTA, 77.3 IDF1
and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single
V100 GPU. The source code, pre-trained models with deploy versions and
tutorials of applying to other trackers are released at
https://github.com/ifzhang/ByteTrack.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-16 23:02:36.681170905 UTC">2021-10-16 23:02:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>