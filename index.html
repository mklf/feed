<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-20T01:30:00Z">07-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Using attention methods to predict judicial outcomes. (arXiv:2207.08823v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08823">
<div class="article-summary-box-inner">
<span><p>Legal Judgment Prediction is one of the most acclaimed fields for the
combined area of NLP, AI, and Law. By legal prediction we mean an intelligent
systems capable to predict specific judicial characteristics, such as judicial
outcome, a judicial class, predict an specific case. In this research, we have
used AI classifiers to predict judicial outcomes in the Brazilian legal system.
For this purpose, we developed a text crawler to extract data from the official
Brazilian electronic legal systems. These texts formed a dataset of
second-degree murder and active corruption cases. We applied different
classifiers, such as Support Vector Machines and Neural Networks, to predict
judicial outcomes by analyzing textual features from the dataset. Our research
showed that Regression Trees, Gated Recurring Units and Hierarchical Attention
Networks presented higher metrics for different subsets. As a final goal, we
explored the weights of one of the algorithms, the Hierarchical Attention
Networks, to find a sample of the most important words used to absolve or
convict defendants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Sequence Models for Text Classification Tasks. (arXiv:2207.08880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08880">
<div class="article-summary-box-inner">
<span><p>The exponential growth of data generated on the Internet in the current
information age is a driving force for the digital economy. Extraction of
information is the major value in an accumulated big data. Big data dependency
on statistical analysis and hand-engineered rules machine learning algorithms
are overwhelmed with vast complexities inherent in human languages. Natural
Language Processing (NLP) is equipping machines to understand these human
diverse and complicated languages. Text Classification is an NLP task which
automatically identifies patterns based on predefined or undefined labeled
sets. Common text classification application includes information retrieval,
modeling news topic, theme extraction, sentiment analysis, and spam detection.
In texts, some sequences of words depend on the previous or next word sequences
to make full meaning; this is a challenging dependency task that requires the
machine to be able to store some previous important information to impact
future meaning. Sequence models such as RNN, GRU, and LSTM is a breakthrough
for tasks with long-range dependencies. As such, we applied these models to
Binary and Multi-class classification. Results generated were excellent with
most of the models performing within the range of 80% and 94%. However, this
result is not exhaustive as we believe there is room for improvement if
machines are to compete with humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRCLens: an MRC Dataset Bias Detection Toolkit. (arXiv:2207.08943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08943">
<div class="article-summary-box-inner">
<span><p>Many recent neural models have shown remarkable empirical results in Machine
Reading Comprehension, but evidence suggests sometimes the models take
advantage of dataset biases to predict and fail to generalize on out-of-sample
data. While many other approaches have been proposed to address this issue from
the computation perspective such as new architectures or training procedures,
we believe a method that allows researchers to discover biases, and adjust the
data or the models in an earlier stage will be beneficial. Thus, we introduce
MRCLens, a toolkit that detects whether biases exist before users train the
full model. For the convenience of introducing the toolkit, we also provide a
categorization of common biases in MRC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selection Bias Induced Spurious Correlations in Large Language Models. (arXiv:2207.08982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08982">
<div class="article-summary-box-inner">
<span><p>In this work we show how large language models (LLMs) can learn statistical
dependencies between otherwise unconditionally independent variables due to
dataset selection bias. To demonstrate the effect, we developed a masked gender
task that can be applied to BERT-family models to reveal spurious correlations
between predicted gender pronouns and a variety of seemingly gender-neutral
variables like date and location, on pre-trained (unmodified) BERT and RoBERTa
large models. Finally, we provide an online demo, inviting readers to
experiment further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices. (arXiv:2207.08988v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08988">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) is a technique to train models using data distributed
across devices. Differential Privacy (DP) provides a formal privacy guarantee
for sensitive data. Our goal is to train a large neural network language model
(NNLM) on compute-constrained devices while preserving privacy using FL and DP.
However, the DP-noise introduced to the model increases as the model size
grows, which often prevents convergence. We propose Partial Embedding Updates
(PEU), a novel technique to decrease noise by decreasing payload size.
Furthermore, we adopt Low Rank Adaptation (LoRA) and Noise Contrastive
Estimation (NCE) to reduce the memory demands of large models on
compute-constrained devices. This combination of techniques makes it possible
to train large-vocabulary language models while preserving accuracy and
privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search. (arXiv:2207.09068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09068">
<div class="article-summary-box-inner">
<span><p>Since BERT (Devlin et al., 2018), learning contextualized word embeddings has
been a de-facto standard in NLP. However, the progress of learning
contextualized phrase embeddings is hindered by the lack of a human-annotated,
phrase-in-context benchmark. To fill this gap, we propose PiC - a dataset of
~28K of noun phrases accompanied by their contextual Wikipedia pages and a
suite of three tasks of increasing difficulty for evaluating the quality of
phrase embeddings. We find that training on our dataset improves ranking
models' accuracy and remarkably pushes Question Answering (QA) models to
near-human accuracy which is 95% Exact Match (EM) on semantic search given a
query phrase and a passage. Interestingly, we find evidence that such
impressive performance is because the QA models learn to better capture the
common meaning of a phrase regardless of its actual context. That is, on our
Phrase Sense Disambiguation (PSD) task, SotA model accuracy drops substantially
(60% EM), failing to differentiate between two different senses of the same
phrase under two different contexts. Further results on our 3-task PiC
benchmark reveal that learning contextualized phrase embeddings remains an
interesting, open challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Transformer Encoders: a Word-Level Task-Agnostic Evaluation. (arXiv:2207.09076v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09076">
<div class="article-summary-box-inner">
<span><p>Some Transformer-based models can perform cross-lingual transfer learning:
those models can be trained on a specific task in one language and give
relatively good results on the same task in another language, despite having
been pre-trained on monolingual tasks only. But, there is no consensus yet on
whether those transformer-based models learn universal patterns across
languages. We propose a word-level task-agnostic method to evaluate the
alignment of contextualized representations built by such models. We show that
our method provides more accurate translated word pairs than previous methods
to evaluate word-level alignment. And our results show that some inner layers
of multilingual Transformer-based models outperform other explicitly aligned
representations, and even more so according to a stricter definition of
multilingual alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ILASR: Privacy-Preserving Incremental Learning for AutomaticSpeech Recognition at Production Scale. (arXiv:2207.09078v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09078">
<div class="article-summary-box-inner">
<span><p>Incremental learning is one paradigm to enable model building and updating at
scale with streaming data. For end-to-end automatic speech recognition (ASR)
tasks, the absence of human annotated labels along with the need for privacy
preserving policies for model building makes it a daunting challenge. Motivated
by these challenges, in this paper we use a cloud based framework for
production systems to demonstrate insights from privacy preserving incremental
learning for automatic speech recognition (ILASR). By privacy preserving, we
mean, usage of ephemeral data which are not human annotated. This system is a
step forward for production levelASR models for incremental/continual learning
that offers near real-time test-bed for experimentation in the cloud for
end-to-end ASR, while adhering to privacy-preserving policies. We show that the
proposed system can improve the production models significantly(3%) over a new
time period of six months even in the absence of human annotated labels with
varying levels of weak supervision and large batch sizes in incremental
learning. This improvement is 20% over test sets with new words and phrases in
the new time period. We demonstrate the effectiveness of model building in a
privacy-preserving incremental fashion for ASR while further exploring the
utility of having an effective teacher model and use of large batch sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks. (arXiv:2207.09083v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09083">
<div class="article-summary-box-inner">
<span><p>Domestic service robots that support daily tasks are a promising solution for
elderly or disabled people. It is crucial for domestic service robots to
explain the collision risk before they perform actions. In this paper, our aim
is to generate a caption about a future event. We propose the Relational Future
Captioning Model (RFCM), a crossmodal language generation model for the future
captioning task. The RFCM has the Relational Self-Attention Encoder to extract
the relationships between events more effectively than the conventional
self-attention in transformers. We conducted comparison experiments, and the
results show the RFCM outperforms a baseline method on two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can You Fool AI by Doing a 180? $\unicode{x2013}$ A Case Study on Authorship Analysis of Texts by Arata Osada. (arXiv:2207.09085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09085">
<div class="article-summary-box-inner">
<span><p>This paper is our attempt at answering a twofold question covering the areas
of ethics and authorship analysis. Firstly, since the methods used for
performing authorship analysis imply that an author can be recognized by the
content he or she creates, we were interested in finding out whether it would
be possible for an author identification system to correctly attribute works to
authors if in the course of years they have undergone a major psychological
transition. Secondly, and from the point of view of the evolution of an
author's ethical values, we checked what it would mean if the authorship
attribution system encounters difficulties in detecting single authorship. We
set out to answer those questions through performing a binary authorship
analysis task using a text classifier based on a pre-trained transformer model
and a baseline method relying on conventional similarity metrics. For the test
set, we chose works of Arata Osada, a Japanese educator and specialist in the
history of education, with half of them being books written before the World
War II and another half in the 1950s, in between which he underwent a
transformation in terms of political opinions. As a result, we were able to
confirm that in the case of texts authored by Arata Osada in a time span of
more than 10 years, while the classification accuracy drops by a large margin
and is substantially lower than for texts by other non-fiction writers,
confidence scores of the predictions remain at a similar level as in the case
of a shorter time span, indicating that the classifier was in many instances
tricked into deciding that texts written over a time span of multiple years
were actually written by two different people, which in turn leads us to
believe that such a change can affect authorship analysis, and that historical
events have great impact on a person's ethical outlook as expressed in their
writings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoEC: Mixture of Expert Clusters. (arXiv:2207.09094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09094">
<div class="article-summary-box-inner">
<span><p>Sparsely Mixture of Experts (MoE) has received great interest due to its
promising scaling capability with affordable computational overhead. MoE
converts dense layers into sparse experts, and utilizes a gated routing network
to make experts conditionally activated. However, as the number of experts
grows, MoE with outrageous parameters suffers from overfitting and sparse data
allocation. Such problems are especially severe on tasks with limited data,
thus hindering the progress for MoE models to improve performance by scaling
up. In this work, we propose Mixture of Expert Clusters - a general approach to
enable expert layers to learn more diverse and appropriate knowledge by
imposing variance-based constraints on the routing stage. We further propose a
cluster-level expert dropout strategy specifically designed for the expert
cluster structure. Our experiments reveal that MoEC could improve performance
on machine translation and natural language understanding tasks, and raise the
performance upper bound for scaling up experts under limited data. We also
verify that MoEC plays a positive role in mitigating overfitting and sparse
data allocation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Bagging Methods for Language Models. (arXiv:2207.09099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09099">
<div class="article-summary-box-inner">
<span><p>Modern language models leverage increasingly large numbers of parameters to
achieve performance on natural language understanding tasks. Ensembling these
models in specific configurations for downstream tasks show even further
performance improvements. In this paper, we perform an analysis of bagging
language models and compare single language models to bagged ensembles that are
roughly equivalent in terms of final model size. We explore an array of model
bagging configurations for natural language understanding tasks with final
ensemble sizes ranging from 300M parameters to 1.5B parameters and determine
that our ensembling methods are at best roughly equivalent to single LM
baselines. We note other positive effects of bagging and pruning in specific
scenarios according to findings in our experiments such as variance reduction
and minor performance improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Usability of Transformers-based models for a French Question-Answering task. (arXiv:2207.09150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09150">
<div class="article-summary-box-inner">
<span><p>For many tasks, state-of-the-art results have been achieved with
Transformer-based architectures, resulting in a paradigmatic shift in practices
from the use of task-specific architectures to the fine-tuning of pre-trained
language models. The ongoing trend consists in training models with an
ever-increasing amount of data and parameters, which requires considerable
resources. It leads to a strong search to improve resource efficiency based on
algorithmic and hardware improvements evaluated only for English. This raises
questions about their usability when applied to small-scale learning problems,
for which a limited amount of training data is available, especially for
under-resourced languages tasks. The lack of appropriately sized corpora is a
hindrance to applying data-driven and transfer learning-based approaches with
strong instability cases. In this paper, we establish a state-of-the-art of the
efforts dedicated to the usability of Transformer-based models and propose to
evaluate these improvements on the question-answering performances of French
language which have few resources. We address the instability relating to data
scarcity by investigating various training strategies with data augmentation,
hyperparameters optimization and cross-lingual transfer. We also introduce a
new compact model for French FrALBERT which proves to be competitive in
low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Transformers-based models on French Spoken Language Understanding tasks. (arXiv:2207.09152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09152">
<div class="article-summary-box-inner">
<span><p>In the last five years, the rise of the self-attentional Transformer-based
architectures led to state-of-the-art performances over many natural language
tasks. Although these approaches are increasingly popular, they require large
amounts of data and computational resources. There is still a substantial need
for benchmarking methodologies ever upwards on under-resourced languages in
data-scarce application conditions. Most pre-trained language models were
massively studied using the English language and only a few of them were
evaluated on French. In this paper, we propose a unified benchmark, focused on
evaluating models quality and their ecological impact on two well-known French
spoken language understanding tasks. Especially we benchmark thirteen
well-established Transformer-based models on the two available spoken language
understanding tasks for French: MEDIA and ATIS-FR. Within this framework, we
show that compact models can reach comparable results to bigger ones while
their ecological impact is considerably lower. However, this assumption is
nuanced and depends on the considered compression method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the cross-lingual transferability of multilingual prototypical models across NLU tasks. (arXiv:2207.09157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09157">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning-based approaches have been applied to task-oriented
dialog and have proven to be effective for limited domain and language
applications when a sufficient number of training examples are available. In
practice, these approaches suffer from the drawbacks of domain-driven design
and under-resourced languages. Domain and language models are supposed to grow
and change as the problem space evolves. On one hand, research on transfer
learning has demonstrated the cross-lingual ability of multilingual
Transformers-based models to learn semantically rich representations. On the
other, in addition to the above approaches, meta-learning have enabled the
development of task and language learning algorithms capable of far
generalization. Through this context, this article proposes to investigate the
cross-lingual transferability of using synergistically few-shot learning with
prototypical neural networks and multilingual Transformers-based models.
Experiments in natural language understanding tasks on MultiATIS++ corpus shows
that our approach substantially improves the observed transfer learning
performances between the low and the high resource languages. More generally
our approach confirms that the meaningful latent space learned in a given
language can be can be generalized to unseen and under-resourced ones using
meta-learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Urdu Speech and Text Based Sentiment Analyzer. (arXiv:2207.09163v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09163">
<div class="article-summary-box-inner">
<span><p>Discovering what other people think has always been a key aspect of our
information-gathering strategy. People can now actively utilize information
technology to seek out and comprehend the ideas of others, thanks to the
increased availability and popularity of opinion-rich resources such as online
review sites and personal blogs. Because of its crucial function in
understanding people's opinions, sentiment analysis (SA) is a crucial task.
Existing research, on the other hand, is primarily focused on the English
language, with just a small amount of study devoted to low-resource languages.
For sentiment analysis, this work presented a new multi-class Urdu dataset
based on user evaluations. The tweeter website was used to get Urdu dataset.
Our proposed dataset includes 10,000 reviews that have been carefully
classified into two categories by human experts: positive, negative. The
primary purpose of this research is to construct a manually annotated dataset
for Urdu sentiment analysis and to establish the baseline result. Five
different lexicon- and rule-based algorithms including Naivebayes, Stanza,
Textblob, Vader, and Flair are employed and the experimental results show that
Flair with an accuracy of 70% outperforms other tested algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Similarity is More Valuable than Character Similarity: Curriculum Learning for Chinese Spell Checking. (arXiv:2207.09217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09217">
<div class="article-summary-box-inner">
<span><p>Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling
errors. In recent years, related researches focus on introducing the character
similarity from confusion set to enhance the CSC models, ignoring the context
of characters that contain richer information. To make better use of contextual
similarity, we propose a simple yet effective curriculum learning framework for
the CSC task. With the help of our designed model-agnostic framework, existing
CSC models will be trained from easy to difficult as humans learn Chinese
characters and achieve further performance improvements. Extensive experiments
and detailed analyses on widely used SIGHAN datasets show that our method
outperforms previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal Algorithms for Transformers. (arXiv:2207.09238v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09238">
<div class="article-summary-box-inner">
<span><p>This document aims to be a self-contained, mathematically precise overview of
transformer architectures and algorithms (*not* results). It covers what
transformers are, how they are trained, what they are used for, their key
architectural components, and a preview of the most prominent models. The
reader is assumed to be familiar with basic ML terminology and simpler neural
network architectures such as MLPs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. (arXiv:2110.08534v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08534">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PTLMs) are typically learned over a large, static
corpus and further fine-tuned for various downstream tasks. However, when
deployed in the real world, a PTLM-based model must deal with data
distributions that deviate from what the PTLM was initially trained on. In this
paper, we study a lifelong language model pretraining challenge where a PTLM is
continually updated so as to adapt to emerging data. Over a domain-incremental
research paper stream and a chronologically-ordered tweet stream, we
incrementally pretrain a PTLM with different continual learning algorithms, and
keep track of the downstream task performance (after fine-tuning). We evaluate
PTLM's ability to adapt to new corpora while retaining learned knowledge in
earlier corpora. Our experiments show distillation-based approaches to be most
effective in retaining downstream performance in earlier domains. The
algorithms also improve knowledge transfer, allowing models to achieve better
downstream performance over the latest data, and improve temporal
generalization when distribution gaps exist between training and evaluation
because of time. We believe our problem formulation, methods, and analysis will
inspire future studies towards continual pretraining of language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Centric Unsupervised Image Captioning. (arXiv:2112.00969v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00969">
<div class="article-summary-box-inner">
<span><p>Image captioning is a longstanding problem in the field of computer vision
and natural language processing. To date, researchers have produced impressive
state-of-the-art performance in the age of deep learning. Most of these
state-of-the-art, however, requires large volume of annotated image-caption
pairs in order to train their models. When given an image dataset of interests,
practitioner needs to annotate the caption for each image in the training set
and this process needs to happen for each newly collected image dataset. In
this paper, we explore the task of unsupervised image captioning which utilizes
unpaired images and texts to train the model so that the texts can come from
different sources than the images. A main school of research on this topic that
has been shown to be effective is to construct pairs from the images and texts
in the training set according to their overlap of objects. Unlike in the
supervised setting, these constructed pairings are however not guaranteed to
have fully overlapping set of objects. Our work in this paper overcomes this by
harvesting objects corresponding to a given sentence from the training set,
even if they don't belong to the same image. When used as input to a
transformer, such mixture of objects enables larger if not full object
coverage, and when supervised by the corresponding sentence, produced results
that outperform current state of the art unsupervised methods by a significant
margin. Building upon this finding, we further show that (1) additional
information on relationship between objects and attributes of objects also
helps in boosting performance; and (2) our method also extends well to
non-English image captioning, which usually suffers from a scarcer level of
annotations. Our findings are supported by strong empirical results. Our code
is available at https://github.com/zihangm/obj-centric-unsup-caption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12886">
<div class="article-summary-box-inner">
<span><p>Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about children's growth and development. The COVID-19
pandemic has highlighted the necessity of online assessment for preschool
children. One of the areas that should be tested is their ability to speak.
Employing Automatic Speech Recognition(ASR) system is useless, since they of
pre-trained on voices, that are different from children's voices in terms of
frequency and amplitude. We constructed an ASR for our cognitive test system to
solve this issue using the Wav2Vec 2.0 model with a new pre-training objective
called Random Frequency Pitch(RFP). In addition, we used our new dataset to
fine-tune our model for Meaningless Words(MW) and Rapid Automatic Naming(RAN)
tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on the Persian
section of the CommonVoice dataset. Furthermore, our novel methodology produces
positive outcomes in zero- and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations. (arXiv:2204.06508v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06508">
<div class="article-summary-box-inner">
<span><p>Despite recent improvements in abstractive summarization, most current
approaches generate summaries that are not factually consistent with the source
document, severely restricting their trust and usage in real-world
applications. Recent works have shown promising improvements in factuality
error identification using text or dependency arc entailments; however, they do
not consider the entire semantic graph simultaneously. To this end, we propose
FactGraph, a method that decomposes the document and the summary into
structured meaning representations (MR), which are more suitable for factuality
evaluation. MRs describe core semantic concepts and their relations,
aggregating the main content in both document and summary in a canonical form,
and reducing data sparsity. FactGraph encodes such graphs using a graph encoder
augmented with structure-aware adapters to capture interactions among the
concepts based on the graph connectivity, along with text representations using
an adapter-based text encoder. Experiments on different benchmarks for
evaluating factuality show that FactGraph outperforms previous approaches by up
to 15%. Furthermore, FactGraph improves performance on identifying content
verifiability errors and better captures subsentence-level factual
inconsistencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08387">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training techniques have achieved remarkable progress in
Document AI. Most multimodal pre-trained models use a masked language modeling
objective to learn bidirectional representations on the text modality, but they
differ in pre-training objectives for the image modality. This discrepancy adds
difficulty to multimodal representation learning. In this paper, we propose
\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with
unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a
word-patch alignment objective to learn cross-modal alignment by predicting
whether the corresponding image patch of a text word is masked. The simple
unified architecture and training objectives make LayoutLMv3 a general-purpose
pre-trained model for both text-centric and image-centric Document AI tasks.
Experimental results show that LayoutLMv3 achieves state-of-the-art performance
not only in text-centric tasks, including form understanding, receipt
understanding, and document visual question answering, but also in
image-centric tasks such as document image classification and document layout
analysis. The code and models are publicly available at
\url{https://aka.ms/layoutlmv3}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Few-Shot Named Entity Linking by Meta-Learning. (arXiv:2207.05280v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05280">
<div class="article-summary-box-inner">
<span><p>Entity linking aims to link ambiguous mentions to their corresponding
entities in a knowledge base, which is significant and fundamental for various
downstream applications, e.g., knowledge base completion, question answering,
and information extraction. While great efforts have been devoted to this task,
most of these studies follow the assumption that large-scale labeled data is
available. However, when the labeled data is insufficient for specific domains
due to labor-intensive annotation work, the performance of existing algorithms
will suffer an intolerable decline. In this paper, we endeavor to solve the
problem of few-shot entity linking, which only requires a minimal amount of
in-domain labeled data and is more practical in real situations. Specifically,
we firstly propose a novel weak supervision strategy to generate non-trivial
synthetic entity-mention pairs based on mention rewriting. Since the quality of
the synthetic data has a critical impact on effective model training, we
further design a meta-learning mechanism to assign different weights to each
synthetic entity-mention pair automatically. Through this way, we can
profoundly exploit rich and precious semantic information to derive a
well-trained entity linking model under the few-shot setting. The experiments
on real-world datasets show that the proposed method can extensively improve
the state-of-the-art few-shot entity linking model and achieve impressive
performance when only a small amount of labeled data is available. Moreover, we
also demonstrate the outstanding ability of the model's transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Synergistic Compilation Workflow for Tackling Crosstalk in Quantum Machines. (arXiv:2207.05751v2 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05751">
<div class="article-summary-box-inner">
<span><p>Near-term quantum systems tend to be noisy. Crosstalk noise has been
recognized as one of several major types of noises in superconducting Noisy
Intermediate-Scale Quantum (NISQ) devices. Crosstalk arises from the concurrent
execution of two-qubit gates on nearby qubits, such as \texttt{CX}. It might
significantly raise the error rate of gates in comparison to running them
individually. Crosstalk can be mitigated through scheduling or hardware machine
tuning. Prior scientific studies, however, manage crosstalk at a really late
phase in the compilation process, usually after hardware mapping is done. It
may miss great opportunities of optimizing algorithm logic, routing, and
crosstalk at the same time. In this paper, we push the envelope by considering
all these factors simultaneously at the very early compilation stage. We
propose a crosstalk-aware quantum program compilation framework called CQC that
can enhance crosstalk mitigation while achieving satisfactory circuit depth.
Moreover, we identify opportunities for translation from intermediate
representation to the circuit for application-specific crosstalk mitigation,
for instance, the \texttt{CX} ladder construction in variational quantum
eigensolvers (VQE). Evaluations through simulation and on real IBM-Q devices
show that our framework can significantly reduce the error rate by up to
6$\times$, with only $\sim$60\% circuit depth compared to state-of-the-art gate
scheduling approaches. In particular, for VQE, we demonstrate 49\% circuit
depth reduction with 9.6\% fidelity improvement over prior art on the H4
molecule using IBMQ Guadalupe. Our CQC framework will be released on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Context Pattern Generation for Entity Set Expansion. (arXiv:2207.08087v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08087">
<div class="article-summary-box-inner">
<span><p>Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various NLP and IR
downstream applications have benefited from ESE due to its ability to discover
knowledge. Although existing bootstrapping methods have achieved great
progress, most of them still rely on manually pre-defined context patterns. A
non-negligible shortcoming of the pre-defined context patterns is that they
cannot be flexibly generalized to all kinds of semantic classes, and we call
this phenomenon as "semantic sensitivity". To address this problem, we devise a
context pattern generation module that utilizes autoregressive language models
(e.g., GPT-2) to automatically generate high-quality context patterns for
entities. In addition, we propose the GAPA, a novel ESE framework that
leverages the aforementioned GenerAted PAtterns to expand target entities.
Extensive experiments and detailed analyses on three widely used datasets
demonstrate the effectiveness of our method. All the codes of our experiments
will be available for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-Local Stepwise Generative Network for Ultra High-Resolution Image Restoration. (arXiv:2207.08808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08808">
<div class="article-summary-box-inner">
<span><p>While the research on image background restoration from regular size of
degraded images has achieved remarkable progress, restoring ultra
high-resolution (e.g., 4K) images remains an extremely challenging task due to
the explosion of computational complexity and memory usage, as well as the
deficiency of annotated data. In this paper we present a novel model for ultra
high-resolution image restoration, referred to as the Global-Local Stepwise
Generative Network (GLSGN), which employs a stepwise restoring strategy
involving four restoring pathways: three local pathways and one global pathway.
The local pathways focus on conducting image restoration in a fine-grained
manner over local but high-resolution image patches, while the global pathway
performs image restoration coarsely on the scale-down but intact image to
provide cues for the local pathways in a global view including semantics and
noise patterns. To smooth the mutual collaboration between these four pathways,
our GLSGN is designed to ensure the inter-pathway consistency in four aspects
in terms of low-level content, perceptual attention, restoring intensity and
high-level semantics, respectively. As another major contribution of this work,
we also introduce the first ultra high-resolution dataset to date for both
reflection removal and rain streak removal, comprising 4,670 real-world and
synthetic images. Extensive experiments across three typical tasks for image
background restoration, including image reflection removal, image rain streak
removal and image dehazing, show that our GLSGN consistently outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio Input Generates Continuous Frames to Synthesize Facial Video Using Generative Adiversarial Networks. (arXiv:2207.08813v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08813">
<div class="article-summary-box-inner">
<span><p>This paper presents a simple method for speech videos generation based on
audio: given a piece of audio, we can generate a video of the target face
speaking this audio. We propose Generative Adversarial Networks (GAN) with cut
speech audio input as condition and use Convolutional Gate Recurrent Unit (GRU)
in generator and discriminator. Our model is trained by exploiting the short
audio and the frames in this duration. For training, we cut the audio and
extract the face in the corresponding frames. We designed a simple encoder and
compare the generated frames using GAN with and without GRU. We use GRU for
temporally coherent frames and the results show that short audio can produce
relatively realistic output results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prior-Guided Adversarial Initialization for Fast Adversarial Training. (arXiv:2207.08859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08859">
<div class="article-summary-box-inner">
<span><p>Fast adversarial training (FAT) effectively improves the efficiency of
standard adversarial training (SAT). However, initial FAT encounters
catastrophic overfitting, i.e.,the robust accuracy against adversarial attacks
suddenly and dramatically decreases. Though several FAT variants spare no
effort to prevent overfitting, they sacrifice much calculation cost. In this
paper, we explore the difference between the training processes of SAT and FAT
and observe that the attack success rate of adversarial examples (AEs) of FAT
gets worse gradually in the late training stage, resulting in overfitting. The
AEs are generated by the fast gradient sign method (FGSM) with a zero or random
initialization. Based on the observation, we propose a prior-guided FGSM
initialization method to avoid overfitting after investigating several
initialization strategies, improving the quality of the AEs during the whole
training process. The initialization is formed by leveraging historically
generated AEs without additional calculation cost. We further provide a
theoretical analysis for the proposed initialization method. We also propose a
simple yet effective regularizer based on the prior-guided initialization,i.e.,
the currently generated perturbation should not deviate too much from the
prior-guided initialization. The regularizer adopts both historical and current
adversarial perturbations to guide the model learning. Evaluations on four
datasets demonstrate that the proposed method can prevent catastrophic
overfitting and outperform state-of-the-art FAT methods. The code is released
at https://github.com/jiaxiaojunQAQ/FGSM-PGI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Romanus: Robust Task Offloading in Modular Multi-Sensor Autonomous Driving Systems. (arXiv:2207.08865v1 [cs.DC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08865">
<div class="article-summary-box-inner">
<span><p>Due to the high performance and safety requirements of self-driving
applications, the complexity of modern autonomous driving systems (ADS) has
been growing, instigating the need for more sophisticated hardware which could
add to the energy footprint of the ADS platform. Addressing this, edge
computing is poised to encompass self-driving applications, enabling the
compute-intensive autonomy-related tasks to be offloaded for processing at
compute-capable edge servers. Nonetheless, the intricate hardware architecture
of ADS platforms, in addition to the stringent robustness demands, set forth
complications for task offloading which are unique to autonomous driving.
Hence, we present $ROMANUS$, a methodology for robust and efficient task
offloading for modular ADS platforms with multi-sensor processing pipelines.
Our methodology entails two phases: (i) the introduction of efficient
offloading points along the execution path of the involved deep learning
models, and (ii) the implementation of a runtime solution based on Deep
Reinforcement Learning to adapt the operating mode according to variations in
the perceived road scene complexity, network connectivity, and server load.
Experiments on the object detection use case demonstrated that our approach is
14.99% more energy-efficient than pure local execution while achieving a 77.06%
reduction in risky behavior from a robust-agnostic offloading baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLAIR: Federated Learning Annotated Image Repository. (arXiv:2207.08869v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08869">
<div class="article-summary-box-inner">
<span><p>Cross-device federated learning is an emerging machine learning (ML) paradigm
where a large population of devices collectively train an ML model while the
data remains on the devices. This research field has a unique set of practical
challenges, and to systematically make advances, new datasets curated to be
compatible with this paradigm are needed. Existing federated learning
benchmarks in the image domain do not accurately capture the scale and
heterogeneity of many real-world use cases. We introduce FLAIR, a challenging
large-scale annotated image dataset for multi-label classification suitable for
federated learning. FLAIR has 429,078 images from 51,414 Flickr users and
captures many of the intricacies typically encountered in federated learning,
such as heterogeneous user data and a long-tailed label distribution. We
implement multiple baselines in different learning setups for different tasks
on this dataset. We believe FLAIR can serve as a challenging benchmark for
advancing the state-of-the art in federated learning. Dataset access and the
code for the benchmark are available at
\url{https://github.com/apple/ml-flair}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prior Knowledge Guided Unsupervised Domain Adaptation. (arXiv:2207.08877v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08877">
<div class="article-summary-box-inner">
<span><p>The waive of labels in the target domain makes Unsupervised Domain Adaptation
(UDA) an attractive technique in many real-world applications, though it also
brings great challenges as model adaptation becomes harder without labeled
target data. In this paper, we address this issue by seeking compensation from
target domain prior knowledge, which is often (partially) available in
practice, e.g., from human expertise. This leads to a novel yet practical
setting where in addition to the training data, some prior knowledge about the
target class distribution are available. We term the setting as
Knowledge-guided Unsupervised Domain Adaptation (KUDA). In particular, we
consider two specific types of prior knowledge about the class distribution in
the target domain: Unary Bound that describes the lower and upper bounds of
individual class probabilities, and Binary Relationship that describes the
relations between two class probabilities. We propose a general rectification
module that uses such prior knowledge to refine model generated pseudo labels.
The module is formulated as a Zero-One Programming problem derived from the
prior knowledge and a smooth regularizer. It can be easily plugged into
self-training based UDA methods, and we combine it with two state-of-the-art
methods, SHOT and DINE. Empirical results on four benchmarks confirm that the
rectification module clearly improves the quality of pseudo labels, which in
turn benefits the self-training stage. With the guidance from prior knowledge,
the performances of both methods are substantially boosted. We expect our work
to inspire further investigations in integrating prior knowledge in UDA. Code
is available at https://github.com/tsun/KUDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A hierarchical semantic segmentation framework for computer vision-based bridge damage detection. (arXiv:2207.08878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08878">
<div class="article-summary-box-inner">
<span><p>Computer vision-based damage detection using remote cameras and unmanned
aerial vehicles (UAVs) enables efficient and low-cost bridge health monitoring
that reduces labor costs and the needs for sensor installation and maintenance.
By leveraging recent semantic image segmentation approaches, we are able to
find regions of critical structural components and recognize damage at the
pixel level using images as the only input. However, existing methods perform
poorly when detecting small damages (e.g., cracks and exposed rebars) and thin
objects with limited image samples, especially when the components of interest
are highly imbalanced. To this end, this paper introduces a semantic
segmentation framework that imposes the hierarchical semantic relationship
between component category and damage types. For example, certain concrete
cracks only present on bridge columns and therefore the non-column region will
be masked out when detecting such damages. In this way, the damage detection
model could focus on learning features from possible damaged regions only and
avoid the effects of other irrelevant regions. We also utilize multi-scale
augmentation that provides views with different scales that preserves
contextual information of each image without losing the ability of handling
small and thin objects. Furthermore, the proposed framework employs important
sampling that repeatedly samples images containing rare components (e.g.,
railway sleeper and exposed rebars) to provide more data samples, which
addresses the imbalanced data challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuForm: Adaptive Overfitting for Neural Shape Editing. (arXiv:2207.08890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08890">
<div class="article-summary-box-inner">
<span><p>Neural representations are popular for representing shapes, as they can be
learned form sensor data and used for data cleanup, model completion, shape
editing, and shape synthesis. Current neural representations can be categorized
as either overfitting to a single object instance, or representing a collection
of objects. However, neither allows accurate editing of neural scene
representations: on the one hand, methods that overfit objects achieve highly
accurate reconstructions, but do not generalize to unseen object configurations
and thus cannot support editing; on the other hand, methods that represent a
family of objects with variations do generalize but produce only approximate
reconstructions. We propose NEUFORM to combine the advantages of both
overfitted and generalizable representations by adaptively using the one most
appropriate for each shape region: the overfitted representation where reliable
data is available, and the generalizable representation everywhere else. We
achieve this with a carefully designed architecture and an approach that blends
the network weights of the two representations, avoiding seams and other
artifacts. We demonstrate edits that successfully reconfigure parts of
human-designed shapes, such as chairs, tables, and lamps, while preserving
semantic integrity and the accuracy of an overfitted shape representation. We
compare with two state-of-the-art competitors and demonstrate clear
improvements in terms of plausibility and fidelity of the resultant edits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional DETR V2: Efficient Detection Transformer with Box Queries. (arXiv:2207.08914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08914">
<div class="article-summary-box-inner">
<span><p>In this paper, we are interested in Detection Transformer (DETR), an
end-to-end object detection approach based on a transformer encoder-decoder
architecture without hand-crafted postprocessing, such as NMS. Inspired by
Conditional DETR, an improved DETR with fast training convergence, that
presented box queries (originally called spatial queries) for internal decoder
layers, we reformulate the object query into the format of the box query that
is a composition of the embeddings of the reference point and the
transformation of the box with respect to the reference point. This
reformulation indicates the connection between the object query in DETR and the
anchor box that is widely studied in Faster R-CNN. Furthermore, we learn the
box queries from the image content, further improving the detection quality of
Conditional DETR still with fast training convergence. In addition, we adopt
the idea of axial self-attention to save the memory cost and accelerate the
encoder. The resulting detector, called Conditional DETR V2, achieves better
results than Conditional DETR, saves the memory cost and runs more efficiently.
For example, for the DC$5$-ResNet-$50$ backbone, our approach achieves $44.8$
AP with $16.4$ FPS on the COCO $val$ set and compared to Conditional DETR, it
runs $1.6\times$ faster, saves $74$\% of the overall memory cost, and improves
$1.0$ AP score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recognizing Hand Use and Hand Role at Home After Stroke from Egocentric Video. (arXiv:2207.08920v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08920">
<div class="article-summary-box-inner">
<span><p>Introduction: Hand function is a central determinant of independence after
stroke. Measuring hand use in the home environment is necessary to evaluate the
impact of new interventions, and calls for novel wearable technologies.
Egocentric video can capture hand-object interactions in context, as well as
show how more-affected hands are used during bilateral tasks (for stabilization
or manipulation). Automated methods are required to extract this information.
Objective: To use artificial intelligence-based computer vision to classify
hand use and hand role from egocentric videos recorded at home after stroke.
Methods: Twenty-one stroke survivors participated in the study. A random forest
classifier, a SlowFast neural network, and the Hand Object Detector neural
network were applied to identify hand use and hand role at home.
Leave-One-Subject-Out-Cross-Validation (LOSOCV) was used to evaluate the
performance of the three models. Between-group differences of the models were
calculated based on the Mathews correlation coefficient (MCC). Results: For
hand use detection, the Hand Object Detector had significantly higher
performance than the other models. The macro average MCCs using this model in
the LOSOCV were 0.50 +- 0.23 for the more-affected hands and 0.58 +- 0.18 for
the less-affected hands. Hand role classification had macro average MCCs in the
LOSOCV that were close to zero for all models. Conclusion: Using egocentric
video to capture the hand use of stroke survivors at home is feasible. Pose
estimation to track finger movements may be beneficial to classifying hand
roles in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I2I: Image to Icosahedral Projection for $\mathrm{SO}(3)$ Object Reasoning from Single-View Images. (arXiv:2207.08925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08925">
<div class="article-summary-box-inner">
<span><p>Reasoning about 3D objects based on 2D images is challenging due to large
variations in appearance caused by viewing the object from different
orientations. Ideally, our model would be invariant or equivariant to changes
in object pose. Unfortunately, this is typically not possible with 2D image
input because we do not have an a priori model of how the image would change
under out-of-plane object rotations. The only $\mathrm{SO}(3)$-equivariant
models that currently exist require point cloud input rather than 2D images. In
this paper, we propose a novel model architecture based on icosahedral group
convolution that reasons in $\mathrm{SO(3)}$ by projecting the input image onto
an icosahedron. As a result of this projection, the model is approximately
equivariant to rotation in $\mathrm{SO}(3)$. We apply this model to an object
pose estimation task and find that it outperforms reasonable baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Easy Batch Normalization. (arXiv:2207.08940v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08940">
<div class="article-summary-box-inner">
<span><p>It was shown that adversarial examples improve object recognition. But what
about their opposite side, easy examples? Easy examples are samples that the
machine learning model classifies correctly with high confidence. In our paper,
we are making the first step toward exploring the potential benefits of using
easy examples in the training procedure of neural networks. We propose to use
an auxiliary batch normalization for easy examples for the standard and robust
accuracy improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustar: Interactive Toolbox Supporting Precise Data Annotation for Robust Vision Learning. (arXiv:2207.08944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08944">
<div class="article-summary-box-inner">
<span><p>We introduce the initial release of our software Robustar, which aims to
improve the robustness of vision classification machine learning models through
a data-driven perspective. Building upon the recent understanding that the lack
of machine learning model's robustness is the tendency of the model's learning
of spurious features, we aim to solve this problem from its root at the data
perspective by removing the spurious features from the data before training. In
particular, we introduce a software that helps the users to better prepare the
data for training image classification models by allowing the users to annotate
the spurious features at the pixel level of images. To facilitate this process,
our software also leverages recent advances to help identify potential images
and pixels worthy of attention and to continue the training with newly
annotated data. Our software is hosted at the GitHub Repository
https://github.com/HaohanWang/Robustar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-step domain adaptation by adversarial attack to $\mathcal{H} \Delta \mathcal{H}$-divergence. (arXiv:2207.08948v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08948">
<div class="article-summary-box-inner">
<span><p>Adversarial examples are transferable between different models. In our paper,
we propose to use this property for multi-step domain adaptation. In
unsupervised domain adaptation settings, we demonstrate that replacing the
source domain with adversarial examples to $\mathcal{H} \Delta
\mathcal{H}$-divergence can improve source classifier accuracy on the target
domain. Our method can be connected to most domain adaptation techniques. We
conducted a range of experiments and achieved improvement in accuracy on Digits
and Office-Home datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoIndoor++:Towards Better Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments. (arXiv:2207.08951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08951">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth estimation has seen significant progress in
recent years, especially in outdoor environments. However, depth prediction
results are not satisfying in indoor scenes where most of the existing data are
captured with hand-held devices. As compared to outdoor environments,
estimating depth of monocular videos for indoor environments, using
self-supervised methods, results in two additional challenges: (i) the depth
range of indoor video sequences varies a lot across different frames, making it
difficult for the depth network to induce consistent depth cues for training;
(ii) the indoor sequences recorded with handheld devices often contain much
more rotational motions, which cause difficulties for the pose network to
predict accurate relative camera poses. In this work, we propose a novel
framework-MonoIndoor++ by giving special considerations to those challenges and
consolidating a set of good practices for improving the performance of
self-supervised monocular depth estimation for indoor environments. First, a
depth factorization module with transformer-based scale regression network is
proposed to estimate a global depth scale factor explicitly, and the predicted
scale factor can indicate the maximum depth values. Second, rather than using a
single-stage pose estimation strategy as in previous methods, we propose to
utilize a residual pose estimation module to estimate relative camera poses
across consecutive frames iteratively. Third, to incorporate extensive
coordinates guidance for our residual pose estimation module, we propose to
perform coordinate convolutional encoding directly over the inputs to pose
networks. The proposed method is validated on a variety of benchmark indoor
datasets, i.e., EuRoC MAV, NYUv2, ScanNet and 7-Scenes, demonstrating the
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Unlabeled Data with Vision and Language Models for Object Detection. (arXiv:2207.08954v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08954">
<div class="article-summary-box-inner">
<span><p>Building robust and generic object detection frameworks requires scaling to
larger label spaces and bigger training datasets. However, it is prohibitively
costly to acquire annotations for thousands of categories at a large scale. We
propose a novel method that leverages the rich semantics available in recent
vision and language models to localize and classify objects in unlabeled
images, effectively generating pseudo labels for object detection. Starting
with a generic and class-agnostic region proposal mechanism, we use vision and
language models to categorize each region of an image into any object category
that is required for downstream tasks. We demonstrate the value of the
generated pseudo labels in two specific tasks, open-vocabulary detection, where
a model needs to generalize to unseen object categories, and semi-supervised
object detection, where additional unlabeled images can be used to improve the
model. Our empirical evaluation shows the effectiveness of the pseudo labels in
both tasks, where we outperform competitive baselines and achieve a novel
state-of-the-art for open-vocabulary object detection. Our code is available at
https://github.com/xiaofeng94/VL-PLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Space-time Video Super-resolution via Spatial-temporal Feature Interaction. (arXiv:2207.08960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08960">
<div class="article-summary-box-inner">
<span><p>The target of space-time video super-resolution (STVSR) is to increase both
the frame rate (also referred to as the temporal resolution) and the spatial
resolution of a given video. Recent approaches solve STVSR with end-to-end deep
neural networks. A popular solution is to first increase the frame rate of the
video; then perform feature refinement among different frame features; and last
increase the spatial resolutions of these features. The temporal correlation
among features of different frames is carefully exploited in this process. The
spatial correlation among features of different (spatial) resolutions, despite
being also very important, is however not emphasized. In this paper, we propose
a spatial-temporal feature interaction network to enhance STVSR by exploiting
both spatial and temporal correlations among features of different frames and
spatial resolutions. Specifically, the spatial-temporal frame interpolation
module is introduced to interpolate low- and high-resolution intermediate frame
features simultaneously and interactively. The spatial-temporal local and
global refinement modules are respectively deployed afterwards to exploit the
spatial-temporal correlation among different features for their refinement.
Finally, a novel motion consistency loss is employed to enhance the motion
continuity among reconstructed frames. We conduct experiments on three standard
benchmarks, Vid4, Vimeo-90K and Adobe240, and the results demonstrate that our
method improves the state of the art methods by a considerable margin. Our
codes will be available at
https://github.com/yuezijie/STINet-Space-time-Video-Super-resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superficial White Matter Analysis: An Efficient Point-cloud-based Deep Learning Framework with Supervised Contrastive Learning for Consistent Tractography Parcellation across Populations and dMRI Acquisitions. (arXiv:2207.08975v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08975">
<div class="article-summary-box-inner">
<span><p>Diffusion MRI tractography is an advanced imaging technique that enables in
vivo mapping of the brain's white matter connections. White matter parcellation
classifies tractography streamlines into clusters or anatomically meaningful
tracts. It enables quantification and visualization of whole-brain
tractography. Currently, most parcellation methods focus on the deep white
matter (DWM), whereas fewer methods address the superficial white matter (SWM)
due to its complexity. We propose a novel two-stage deep-learning-based
framework, Superficial White Matter Analysis (SupWMA), that performs an
efficient and consistent parcellation of 198 SWM clusters from whole-brain
tractography. A point-cloud-based network is adapted to our SWM parcellation
task, and supervised contrastive learning enables more discriminative
representations between plausible streamlines and outliers for SWM. We train
our model on a large-scale tractography dataset including streamline samples
from labeled SWM clusters and anatomically implausible streamline samples, and
we perform testing on six independently acquired datasets of different ages and
health conditions (including neonates and patients with space-occupying brain
tumors). Compared to several state-of-the-art methods, SupWMA obtains highly
consistent and accurate SWM parcellation results on all datasets, showing good
generalization across the lifespan in health and disease. In addition, the
computational speed of SupWMA is much faster than other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelectionConv: Convolutional Neural Networks for Non-rectilinear Image Data. (arXiv:2207.08979v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08979">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have revolutionized vision applications. There
are image domains and representations, however, that cannot be handled by
standard CNNs (e.g., spherical images, superpixels). Such data are usually
processed using networks and algorithms specialized for each type. In this
work, we show that it may not always be necessary to use specialized neural
networks to operate on such spaces. Instead, we introduce a new structured
graph convolution operator that can copy 2D convolution weights, transferring
the capabilities of already trained traditional CNNs to our new graph network.
This network can then operate on any data that can be represented as a
positional graph. By converting non-rectilinear data to a graph, we can apply
these convolutions on these irregular image domains without requiring training
on large domain-specific datasets. Results of transferring pre-trained image
networks for segmentation, stylization, and depth prediction are demonstrated
for a variety of such data forms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeformIrisNet: An Identity-Preserving Model of Iris Texture Deformation. (arXiv:2207.08980v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08980">
<div class="article-summary-box-inner">
<span><p>Nonlinear iris texture deformations due to pupil size variations are one of
the main factors responsible for within-class variance of genuine comparison
scores in iris recognition. In dominant approaches to iris recognition, the
size of a ring-shaped iris region is linearly scaled to a canonical rectangle,
used further in encoding and matching. However, the biological complexity of
iris sphincter and dilator muscles causes the movements of iris features to be
nonlinear in a function of pupil size, and not solely organized along radial
paths. Alternatively to the existing theoretical models based on biomechanics
of iris musculature, in this paper we propose a novel deep autoencoder-based
model that can effectively learn complex movements of iris texture features
directly from the data. The proposed model takes two inputs, (a) an
ISO-compliant near-infrared iris image with initial pupil size, and (b) the
binary mask defining the target shape of the iris. The model makes all the
necessary nonlinear deformations to the iris texture to match the shape of iris
in image (a) with the shape provided by the target mask (b). The
identity-preservation component of the loss function helps the model in finding
deformations that preserve identity and not only visual realism of generated
samples. We also demonstrate two immediate applications of this model: better
compensation for iris texture deformations in iris recognition algorithms,
compared to linear models, and creation of generative algorithm that can aid
human forensic examiners, who may need to compare iris images with large
difference in pupil dilation. We offer the source codes and model weights
available along with this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capabilities, Limitations and Challenges of Style Transfer with CycleGANs: A Study on Automatic Ring Design Generation. (arXiv:2207.08989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08989">
<div class="article-summary-box-inner">
<span><p>Rendering programs have changed the design process completely as they permit
to see how the products will look before they are fabricated. However, the
rendering process is complicated and takes a significant amount of time, not
only in the rendering itself but in the setting of the scene as well.
Materials, lights and cameras need to be set in order to get the best quality
results. Nevertheless, the optimal output may not be obtained in the first
render. This all makes the rendering process a tedious process. Since
Goodfellow et al. introduced Generative Adversarial Networks (GANs) in 2014
[1], they have been used to generate computer-assigned synthetic data, from
non-existing human faces to medical data analysis or image style transfer. GANs
have been used to transfer image textures from one domain to another. However,
paired data from both domains was needed. When Zhu et al. introduced the
CycleGAN model, the elimination of this expensive constraint permitted
transforming one image from one domain into another, without the need for
paired data. This work validates the applicability of CycleGANs on style
transfer from an initial sketch to a final render in 2D that represents a 3D
design, a step that is paramount in every product design process. We inquiry
the possibilities of including CycleGANs as part of the design pipeline, more
precisely, applied to the rendering of ring designs. Our contribution entails a
crucial part of the process as it allows the customer to see the final product
before buying. This work sets a basis for future research, showing the
possibilities of GANs in design and establishing a starting point for novel
applications to approach crafts design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure from Action: Learning Interactions for Articulated Object 3D Structure Discovery. (arXiv:2207.08997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08997">
<div class="article-summary-box-inner">
<span><p>Articulated objects are abundant in daily life. Discovering their parts,
joints, and kinematics is crucial for robots to interact with these objects. We
introduce Structure from Action (SfA), a framework that discovers the 3D part
geometry and joint parameters of unseen articulated objects via a sequence of
inferred interactions. Our key insight is that 3D interaction and perception
should be considered in conjunction to construct 3D articulated CAD models,
especially in the case of categories not seen during training. By selecting
informative interactions, SfA discovers parts and reveals initially occluded
surfaces, like the inside of a closed drawer. By aggregating visual
observations in 3D, SfA accurately segments multiple parts, reconstructs part
geometry, and infers all joint parameters in a canonical coordinate frame. Our
experiments demonstrate that a single SfA model trained in simulation can
generalize to many unseen object categories with unknown kinematic structures
and to real-world objects. Code and data will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering novel systemic biomarkers in photos of the external eye. (arXiv:2207.08998v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08998">
<div class="article-summary-box-inner">
<span><p>External eye photos were recently shown to reveal signs of diabetic retinal
disease and elevated HbA1c. In this paper, we evaluate if external eye photos
contain information about additional systemic medical conditions. We developed
a deep learning system (DLS) that takes external eye photos as input and
predicts multiple systemic parameters, such as those related to the liver
(albumin, AST); kidney (eGFR estimated using the race-free 2021 CKD-EPI
creatinine equation, the urine ACR); bone &amp; mineral (calcium); thyroid (TSH);
and blood count (Hgb, WBC, platelets). Development leveraged 151,237 images
from 49,015 patients with diabetes undergoing diabetic eye screening in 11
sites across Los Angeles county, CA. Evaluation focused on 9 pre-specified
systemic parameters and leveraged 3 validation sets (A, B, C) spanning 28,869
patients with and without diabetes undergoing eye screening in 3 independent
sites in Los Angeles County, CA, and the greater Atlanta area, GA. We compared
against baseline models incorporating available clinicodemographic variables
(e.g. age, sex, race/ethnicity, years with diabetes). Relative to the baseline,
the DLS achieved statistically significant superior performance at detecting
AST&gt;36, calcium&lt;8.6, eGFR&lt;60, Hgb&lt;11, platelets&lt;150, ACR&gt;=300, and WBC&lt;4 on
validation set A (a patient population similar to the development sets), where
the AUC of DLS exceeded that of the baseline by 5.2-19.4%. On validation sets B
and C, with substantial patient population differences compared to the
development sets, the DLS outperformed the baseline for ACR&gt;=300 and Hgb&lt;11 by
7.3-13.2%. Our findings provide further evidence that external eye photos
contain important biomarkers of systemic health spanning multiple organ
systems. Further work is needed to investigate whether and how these biomarkers
can be translated into clinical impact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SS-MFAR : Semi-supervised Multi-task Facial Affect Recognition. (arXiv:2207.09012v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09012">
<div class="article-summary-box-inner">
<span><p>Automatic affect recognition has applications in many areas such as
education, gaming, software development, automotives, medical care, etc. but it
is non trivial task to achieve appreciable performance on in-the-wild data
sets. In-the-wild data sets though represent real-world scenarios better than
synthetic data sets, the former ones suffer from the problem of incomplete
labels. Inspired by semi-supervised learning, in this paper, we introduce our
submission to the Multi-Task-Learning Challenge at the 4th Affective Behavior
Analysis in-the-wild (ABAW) 2022 Competition. The three tasks that are
considered in this challenge are valence-arousal(VA) estimation, classification
of expressions into 6 basic (anger, disgust, fear, happiness, sadness,
surprise), neutral, and the 'other' category and 12 action units(AU) numbered
AU-\{1,2,4,6,7,10,12,15,23,24,25,26\}. Our method Semi-supervised Multi-task
Facial Affect Recognition titled \textbf{SS-MFAR} uses a deep residual network
with task specific classifiers for each of the tasks along with adaptive
thresholds for each expression class and semi-supervised learning for the
incomplete labels. Source code is available at
https://github.com/1980x/ABAW2022DMACS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-aware Editable Morphable Model for 3D Facial Detail Animation and Manipulation. (arXiv:2207.09019v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09019">
<div class="article-summary-box-inner">
<span><p>Morphable models are essential for the statistical modeling of 3D faces.
Previous works on morphable models mostly focus on large-scale facial geometry
but ignore facial details. This paper augments morphable models in representing
facial details by learning a Structure-aware Editable Morphable Model (SEMM).
SEMM introduces a detail structure representation based on the distance field
of wrinkle lines, jointly modeled with detail displacements to establish better
correspondences and enable intuitive manipulation of wrinkle structure.
Besides, SEMM introduces two transformation modules to translate expression
blendshape weights and age values into changes in latent space, allowing
effective semantic detail editing while maintaining identity. Extensive
experiments demonstrate that the proposed model compactly represents facial
details, outperforms previous methods in expression animation qualitatively and
quantitatively, and achieves effective age editing and wrinkle line editing of
facial details. Code and model are available at
https://github.com/gerwang/facial-detail-manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indoor Localization for Personalized Ambient Assisted Living of Multiple Users in Multi-Floor Smart Environments. (arXiv:2207.09025v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09025">
<div class="article-summary-box-inner">
<span><p>This paper presents a multifunctional interdisciplinary framework that makes
four scientific contributions towards the development of personalized ambient
assisted living, with a specific focus to address the different and dynamic
needs of the diverse aging population in the future of smart living
environments. First, it presents a probabilistic reasoning-based mathematical
approach to model all possible forms of user interactions for any activity
arising from the user diversity of multiple users in such environments. Second,
it presents a system that uses this approach with a machine learning method to
model individual user profiles and user-specific user interactions for
detecting the dynamic indoor location of each specific user. Third, to address
the need to develop highly accurate indoor localization systems for increased
trust, reliance, and seamless user acceptance, the framework introduces a novel
methodology where two boosting approaches Gradient Boosting and the AdaBoost
algorithm are integrated and used on a decision tree-based learning model to
perform indoor localization. Fourth, the framework introduces two novel
functionalities to provide semantic context to indoor localization in terms of
detecting each user's floor-specific location as well as tracking whether a
specific user was located inside or outside a given spatial region in a
multi-floor-based indoor setting. These novel functionalities of the proposed
framework were tested on a dataset of localization-related Big Data collected
from 18 different users who navigated in 3 buildings consisting of 5 floors and
254 indoor spatial regions. The results show that this approach of indoor
localization for personalized AAL that models each specific user always
achieves higher accuracy as compared to the traditional approach of modeling an
average user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ML-BPM: Multi-teacher Learning with Bidirectional Photometric Mixing for Open Compound Domain Adaptation in Semantic Segmentation. (arXiv:2207.09045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09045">
<div class="article-summary-box-inner">
<span><p>Open compound domain adaptation (OCDA) considers the target domain as the
compound of multiple unknown homogeneous subdomains. The goal of OCDA is to
minimize the domain gap between the labeled source domain and the unlabeled
compound target domain, which benefits the model generalization to the unseen
domains. Current OCDA for semantic segmentation methods adopt manual domain
separation and employ a single model to simultaneously adapt to all the target
subdomains. However, adapting to a target subdomain might hinder the model from
adapting to other dissimilar target subdomains, which leads to limited
performance. In this work, we introduce a multi-teacher framework with
bidirectional photometric mixing to separately adapt to every target subdomain.
First, we present an automatic domain separation to find the optimal number of
subdomains. On this basis, we propose a multi-teacher framework in which each
teacher model uses bidirectional photometric mixing to adapt to one target
subdomain. Furthermore, we conduct an adaptive distillation to learn a student
model and apply consistency regularization to improve the student
generalization. Experimental results on benchmark datasets show the efficacy of
the proposed approach for both the compound domain and the open domains against
existing state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Prototype Mask for Occluded Person Re-Identification. (arXiv:2207.09046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09046">
<div class="article-summary-box-inner">
<span><p>Although person re-identification has achieved an impressive improvement in
recent years, the common occlusion case caused by different obstacles is still
an unsettled issue in real application scenarios. Existing methods mainly
address this issue by employing body clues provided by an extra network to
distinguish the visible part. Nevertheless, the inevitable domain gap between
the assistant model and the ReID datasets has highly increased the difficulty
to obtain an effective and efficient model. To escape from the extra
pre-trained networks and achieve an automatic alignment in an end-to-end
trainable network, we propose a novel Dynamic Prototype Mask (DPM) based on two
self-evident prior knowledge. Specifically, we first devise a Hierarchical Mask
Generator which utilizes the hierarchical semantic to select the visible
pattern space between the high-quality holistic prototype and the feature
representation of the occluded input image. Under this condition, the occluded
representation could be well aligned in a selected subspace spontaneously.
Then, to enrich the feature representation of the high-quality holistic
prototype and provide a more complete feature space, we introduce a Head Enrich
Module to encourage different heads to aggregate different patterns
representation in the whole image. Extensive experimental evaluations conducted
on occluded and holistic person re-identification benchmarks demonstrate the
superior performance of the DPM over the state-of-the-art methods. The code is
released at https://github.com/stone96123/DPM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation. (arXiv:2207.09048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09048">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI) aims to synthesize an intermediate frame
between two consecutive frames. State-of-the-art approaches usually adopt a
two-step solution, which includes 1) generating locally-warped pixels by
flow-based motion estimations, 2) blending the warped pixels to form a full
frame through deep neural synthesis networks. However, due to the inconsistent
warping from the two consecutive frames, the warped features for new frames are
usually not aligned, which leads to distorted and blurred frames, especially
when large and complex motions occur. To solve this issue, in this paper we
propose a novel Trajectory-aware Transformer for Video Frame Interpolation
(TTVFI). In particular, we formulate the warped features with inconsistent
motions as query tokens, and formulate relevant regions in a motion trajectory
from two original consecutive frames into keys and values. Self-attention is
learned on relevant tokens along the trajectory to blend the pristine features
into intermediate frames through end-to-end training. Experimental results
demonstrate that our method outperforms other state-of-the-art methods in four
widely-used VFI benchmarks. Both code and pre-trained models will be released
soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepBNN: towards a precise Binary Neural Network with Enhanced Feature Map via Repeating. (arXiv:2207.09049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09049">
<div class="article-summary-box-inner">
<span><p>Binary neural network (BNN) is an extreme quantization version of
convolutional neural networks (CNNs) with all features and weights mapped to
just 1-bit. Although BNN saves a lot of memory and computation demand to make
CNN applicable on edge or mobile devices, BNN suffers the drop of network
performance due to the reduced representation capability after binarization. In
this paper, we propose a new replaceable and easy-to-use convolution module
RepConv, which enhances feature maps through replicating input or output along
channel dimension by $\beta$ times without extra cost on the number of
parameters and convolutional computation. We also define a set of RepTran rules
to use RepConv throughout BNN modules like binary convolution, fully connected
layer and batch normalization. Experiments demonstrate that after the RepTran
transformation, a set of highly cited BNNs have achieved universally better
performance than the original BNN versions. For example, the Top-1 accuracy of
Rep-ReCU-ResNet-20, i.e., a RepBconv enhanced ReCU-ResNet-20, reaches 88.97% on
CIFAR-10, which is 1.47% higher than that of the original network. And
Rep-AdamBNN-ReActNet-A achieves 71.342% Top-1 accuracy on ImageNet, a fresh
state-of-the-art result of BNNs. Code and models are available
at:https://github.com/imfinethanks/Rep_AdamBNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balanced Contrastive Learning for Long-Tailed Visual Recognition. (arXiv:2207.09052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09052">
<div class="article-summary-box-inner">
<span><p>Real-world data typically follow a long-tailed distribution, where a few
majority categories occupy most of the data while most minority categories
contain a limited number of samples. Classification models minimizing
cross-entropy struggle to represent and classify the tail classes. Although the
problem of learning unbiased classifiers has been well studied, methods for
representing imbalanced data are under-explored. In this paper, we focus on
representation learning for imbalanced data. Recently, supervised contrastive
learning has shown promising performance on balanced data recently. However,
through our theoretical analysis, we find that for long-tailed data, it fails
to form a regular simplex which is an ideal geometric configuration for
representation learning. To correct the optimization behavior of SCL and
further improve the performance of long-tailed visual recognition, we propose a
novel loss for balanced contrastive learning (BCL). Compared with SCL, we have
two improvements in BCL: class-averaging, which balances the gradient
contribution of negative classes; class-complement, which allows all classes to
appear in every mini-batch. The proposed balanced contrastive learning (BCL)
method satisfies the condition of forming a regular simplex and assists the
optimization of cross-entropy. Equipped with BCL, the proposed two-branch
framework can obtain a stronger feature representation and achieve competitive
performance on long-tailed benchmark datasets such as CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist2018. Our code is available at
\href{https://github.com/FlamieZhu/BCL}{this URL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Box-supervised Instance Segmentation with Level Set Evolution. (arXiv:2207.09055v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09055">
<div class="article-summary-box-inner">
<span><p>In contrast to the fully supervised methods using pixel-wise mask labels,
box-supervised instance segmentation takes advantage of the simple box
annotations, which has recently attracted a lot of research attentions. In this
paper, we propose a novel single-shot box-supervised instance segmentation
approach, which integrates the classical level set model with deep neural
network delicately. Specifically, our proposed method iteratively learns a
series of level sets through a continuous Chan-Vese energy-based function in an
end-to-end fashion. A simple mask supervised SOLOv2 model is adapted to predict
the instance-aware mask map as the level set for each instance. Both the input
image and its deep features are employed as the input data to evolve the level
set curves, where a box projection function is employed to obtain the initial
boundary. By minimizing the fully differentiable energy function, the level set
for each instance is iteratively optimized within its corresponding bounding
box annotation. The experimental results on four challenging benchmarks
demonstrate the leading performance of our proposed approach to robust instance
segmentation in various scenarios. The code is available at:
https://github.com/LiWentomng/boxlevelset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Open-set Recognition Using Background as Unknowns. (arXiv:2207.09059v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09059">
<div class="article-summary-box-inner">
<span><p>Few-shot open-set recognition aims to classify both seen and novel images
given only limited training data of seen classes. The challenge of this task is
that the model is required not only to learn a discriminative classifier to
classify the pre-defined classes with few training data but also to reject
inputs from unseen classes that never appear at training time. In this paper,
we propose to solve the problem from two novel aspects. First, instead of
learning the decision boundaries between seen classes, as is done in standard
close-set classification, we reserve space for unseen classes, such that images
located in these areas are recognized as the unseen classes. Second, to
effectively learn such decision boundaries, we propose to utilize the
background features from seen classes. As these background regions do not
significantly contribute to the decision of close-set classification, it is
natural to use them as the pseudo unseen classes for classifier learning. Our
extensive experiments show that our proposed method not only outperforms
multiple baselines but also sets new state-of-the-art results on three popular
benchmarks, namely tieredImageNet, miniImageNet, and Caltech-USCD
Birds-200-2011 (CUB).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moment Centralization based Gradient Descent Optimizers for Convolutional Neural Networks. (arXiv:2207.09066v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09066">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have shown very appealing performance
for many computer vision applications. The training of CNNs is generally
performed using stochastic gradient descent (SGD) based optimization
techniques. The adaptive momentum-based SGD optimizers are the recent trends.
However, the existing optimizers are not able to maintain a zero mean in the
first-order moment and struggle with optimization. In this paper, we propose a
moment centralization-based SGD optimizer for CNNs. Specifically, we impose the
zero mean constraints on the first-order moment explicitly. The proposed moment
centralization is generic in nature and can be integrated with any of the
existing adaptive momentum-based optimizers. The proposed idea is tested with
three state-of-the-art optimization techniques, including Adam, Radam, and
Adabelief on benchmark CIFAR10, CIFAR100, and TinyImageNet datasets for image
classification. The performance of the existing optimizers is generally
improved when integrated with the proposed moment centralization. Further, The
results of the proposed moment centralization are also better than the existing
gradient centralization. The analytical analysis using the toy example shows
that the proposed method leads to a shorter and smoother optimization
trajectory. The source code is made publicly available at
\url{https://github.com/sumanthsadhu/MC-optimizer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Is MattEr: Temporal Self-supervision for Video Transformers. (arXiv:2207.09067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09067">
<div class="article-summary-box-inner">
<span><p>Understanding temporal dynamics of video is an essential aspect of learning
better video representations. Recently, transformer-based architectural designs
have been extensively explored for video tasks due to their capability to
capture long-term dependency of input sequences. However, we found that these
Video Transformers are still biased to learn spatial dynamics rather than
temporal ones, and debiasing the spurious correlation is critical for their
performance. Based on the observations, we design simple yet effective
self-supervised tasks for video models to learn temporal dynamics better.
Specifically, for debiasing the spatial bias, our method learns the temporal
order of video frames as extra self-supervision and enforces the randomly
shuffled frames to have low-confidence outputs. Also, our method learns the
temporal flow direction of video tokens among consecutive frames for enhancing
the correlation toward temporal dynamics. Under various video action
recognition tasks, we demonstrate the effectiveness of our method and its
compatibility with state-of-the-art Video Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Unaware Knowledge Distillation for Image Retrieval. (arXiv:2207.09070v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09070">
<div class="article-summary-box-inner">
<span><p>Existing data-dependent hashing methods use large backbone networks with
millions of parameters and are computationally complex. Existing knowledge
distillation methods use logits and other features of the deep (teacher) model
and as knowledge for the compact (student) model, which requires the teacher's
network to be fine-tuned on the context in parallel with the student model on
the context. Training teacher on the target context requires more time and
computational resources. In this paper, we propose context unaware knowledge
distillation that uses the knowledge of the teacher model without fine-tuning
it on the target context. We also propose a new efficient student model
architecture for knowledge distillation. The proposed approach follows a
two-step process. The first step involves pre-training the student model with
the help of context unaware knowledge distillation from the teacher model. The
second step involves fine-tuning the student model on the context of image
retrieval. In order to show the efficacy of the proposed approach, we compare
the retrieval results, no. of parameters and no. of operations of the student
models with the teacher models under different retrieval frameworks, including
deep cauchy hashing (DCH) and central similarity quantization (CSQ). The
experimental results confirm that the proposed approach provides a promising
trade-off between the retrieval results and efficiency. The code used in this
paper is released publicly at \url{https://github.com/satoru2001/CUKDFIR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Task Learning with Incremental Rank Updates. (arXiv:2207.09074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09074">
<div class="article-summary-box-inner">
<span><p>Incremental Task learning (ITL) is a category of continual learning that
seeks to train a single network for multiple tasks (one after another), where
training data for each task is only available during the training of that task.
Neural networks tend to forget older tasks when they are trained for the newer
tasks; this property is often known as catastrophic forgetting. To address this
issue, ITL methods use episodic memory, parameter regularization, masking and
pruning, or extensible network structures. In this paper, we propose a new
incremental task learning framework based on low-rank factorization. In
particular, we represent the network weights for each layer as a linear
combination of several rank-1 matrices. To update the network for a new task,
we learn a rank-1 (or low-rank) matrix and add that to the weights of every
layer. We also introduce an additional selector vector that assigns different
weights to the low-rank matrices learned for the previous tasks. We show that
our approach performs better than the current state-of-the-art methods in terms
of accuracy and forgetting. Our method also offers better memory efficiency
compared to episodic memory- and mask-based approaches. Our code will be
available at https://github.com/CSIPlab/task-increment-rank-update.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Future Captioning Model for Explaining Likely Collisions in Daily Tasks. (arXiv:2207.09083v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09083">
<div class="article-summary-box-inner">
<span><p>Domestic service robots that support daily tasks are a promising solution for
elderly or disabled people. It is crucial for domestic service robots to
explain the collision risk before they perform actions. In this paper, our aim
is to generate a caption about a future event. We propose the Relational Future
Captioning Model (RFCM), a crossmodal language generation model for the future
captioning task. The RFCM has the Relational Self-Attention Encoder to extract
the relationships between events more effectively than the conventional
self-attention in transformers. We conducted comparison experiments, and the
results show the RFCM outperforms a baseline method on two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Adaptive Transformations for Weakly Supervised Point Cloud Segmentation. (arXiv:2207.09084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09084">
<div class="article-summary-box-inner">
<span><p>Weakly supervised point cloud segmentation, i.e. semantically segmenting a
point cloud with only a few labeled points in the whole 3D scene, is highly
desirable due to the heavy burden of collecting abundant dense annotations for
the model training. However, existing methods remain challenging to accurately
segment 3D point clouds since limited annotated data may lead to insufficient
guidance for label propagation to unlabeled data. Considering the
smoothness-based methods have achieved promising progress, in this paper, we
advocate applying the consistency constraint under various perturbations to
effectively regularize unlabeled 3D points. Specifically, we propose a novel
DAT (\textbf{D}ual \textbf{A}daptive \textbf{T}ransformations) model for weakly
supervised point cloud segmentation, where the dual adaptive transformations
are performed via an adversarial strategy at both point-level and region-level,
aiming at enforcing the local and structural smoothness constraints on 3D point
clouds. We evaluate our proposed DAT model with two popular backbones on the
large-scale S3DIS and ScanNet-V2 datasets. Extensive experiments demonstrate
that our model can effectively leverage the unlabeled 3D points and achieve
significant performance gains on both datasets, setting new state-of-the-art
performance for weakly supervised point cloud segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHR-Net: Multiple-Hypothesis Reconstruction of Non-Rigid Shapes from 2D Views. (arXiv:2207.09086v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09086">
<div class="article-summary-box-inner">
<span><p>We propose MHR-Net, a novel method for recovering Non-Rigid Shapes from
Motion (NRSfM). MHR-Net aims to find a set of reasonable reconstructions for a
2D view, and it also selects the most likely reconstruction from the set. To
deal with the challenging unsupervised generation of non-rigid shapes, we
develop a new Deterministic Basis and Stochastic Deformation scheme in MHR-Net.
The non-rigid shape is first expressed as the sum of a coarse shape basis and a
flexible shape deformation, then multiple hypotheses are generated with
uncertainty modeling of the deformation part. MHR-Net is optimized with
reprojection loss on the basis and the best hypothesis. Furthermore, we design
a new Procrustean Residual Loss, which reduces the rigid rotations between
similar shapes and further improves the performance. Experiments show that
MHR-Net achieves state-of-the-art reconstruction accuracy on Human3.6M, SURREAL
and 300-VW datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MONet: Multi-scale Overlap Network for Duplication Detection in Biomedical Images. (arXiv:2207.09107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09107">
<div class="article-summary-box-inner">
<span><p>Manipulation of biomedical images to misrepresent experimental results has
plagued the biomedical community for a while. Recent interest in the problem
led to the curation of a dataset and associated tasks to promote the
development of biomedical forensic methods. Of these, the largest manipulation
detection task focuses on the detection of duplicated regions between images.
Traditional computer-vision based forensic models trained on natural images are
not designed to overcome the challenges presented by biomedical images. We
propose a multi-scale overlap detection model to detect duplicated image
regions. Our model is structured to find duplication hierarchically, so as to
reduce the number of patch operations. It achieves state-of-the-art performance
overall and on multiple biomedical image categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">eCDT: Event Clustering for Simultaneous Feature Detection and Tracking-. (arXiv:2207.09108v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09108">
<div class="article-summary-box-inner">
<span><p>Contrary to other standard cameras, event cameras interpret the world in an
entirely different manner; as a collection of asynchronous events. Despite
event camera's unique data output, many event feature detection and tracking
algorithms have shown significant progress by making detours to frame-based
data representations. This paper questions the need to do so and proposes a
novel event data-friendly method that achieve simultaneous feature detection
and tracking, called event Clustering-based Detection and Tracking (eCDT). Our
method employs a novel clustering method, named as k-NN Classifier-based
Spatial Clustering and Applications with Noise (KCSCAN), to cluster adjacent
polarity events to retrieve event trajectories.With the aid of a Head and Tail
Descriptor Matching process, event clusters that reappear in a different
polarity are continually tracked, elongating the feature tracks. Thanks to our
clustering approach in spatio-temporal space, our method automatically solves
feature detection and feature tracking simultaneously. Also, eCDT can extract
feature tracks at any frequency with an adjustable time window, which does not
corrupt the high temporal resolution of the original event data. Our method
achieves 30% better feature tracking ages compared with the state-of-the-art
approach while also having a low error approximately equal to it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expert-LaSTS: Expert-Knowledge Guided Latent Space for Traffic Scenarios. (arXiv:2207.09120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09120">
<div class="article-summary-box-inner">
<span><p>Clustering traffic scenarios and detecting novel scenario types are required
for scenario-based testing of autonomous vehicles. These tasks benefit from
either good similarity measures or good representations for the traffic
scenarios. In this work, an expert-knowledge aided representation learning for
traffic scenarios is presented. The latent space so formed is used for
successful clustering and novel scenario type detection. Expert-knowledge is
used to define objectives that the latent representations of traffic scenarios
shall fulfill. It is presented, how the network architecture and loss is
designed from these objectives, thereby incorporating expert-knowledge. An
automatic mining strategy for traffic scenarios is presented, such that no
manual labeling is required. Results show the performance advantage compared to
baseline methods. Additionally, extensive analysis of the latent space is
performed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shrinking the Semantic Gap: Spatial Pooling of Local Moment Invariants for Copy-Move Forgery Detection. (arXiv:2207.09135v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09135">
<div class="article-summary-box-inner">
<span><p>Copy-move forgery is a manipulation of copying and pasting specific patches
from and to an image, with potentially illegal or unethical uses. Recent
advances in the forensic methods for copy-move forgery have shown increasing
success in detection accuracy and robustness. However, for images with high
self-similarity or strong signal corruption, the existing algorithms often
exhibit inefficient processes and unreliable results. This is mainly due to the
inherent semantic gap between low-level visual representation and high-level
semantic concept. In this paper, we present a very first study of trying to
mitigate the semantic gap problem in copy-move forgery detection, with spatial
pooling of local moment invariants for midlevel image representation. Our
detection method expands the traditional works on two aspects: 1) we introduce
the bag-of-visual-words model into this field for the first time, may meaning a
new perspective of forensic study; 2) we propose a word-to-phrase feature
description and matching pipeline, covering the spatial structure and visual
saliency information of digital images. Extensive experimental results show the
superior performance of our framework over state-of-the-art algorithms in
overcoming the related problems caused by the semantic gap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParticleSfM: Exploiting Dense Point Trajectories for Localizing Moving Cameras in the Wild. (arXiv:2207.09137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09137">
<div class="article-summary-box-inner">
<span><p>Estimating the pose of a moving camera from monocular video is a challenging
problem, especially due to the presence of moving objects in dynamic
environments, where the performance of existing camera pose estimation methods
are susceptible to pixels that are not geometrically consistent. To tackle this
challenge, we present a robust dense indirect structure-from-motion method for
videos that is based on dense correspondence initialized from pairwise optical
flow. Our key idea is to optimize long-range video correspondence as dense
point trajectories and use it to learn robust estimation of motion
segmentation. A novel neural network architecture is proposed for processing
irregular point trajectory data. Camera poses are then estimated and optimized
with global bundle adjustment over the portion of long-range point trajectories
that are classified as static. Experiments on MPI Sintel dataset show that our
system produces significantly more accurate camera trajectories compared to
existing state-of-the-art methods. In addition, our method is able to retain
reasonable accuracy of camera poses on fully static scenes, which consistently
outperforms strong state-of-the-art dense correspondence based methods with
end-to-end deep learning, demonstrating the potential of dense indirect methods
based on optical flow and point trajectories. As the point trajectory
representation is general, we further present results and comparisons on
in-the-wild monocular videos with complex motion of dynamic objects. Code is
available at https://github.com/bytedance/particle-sfm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Matters for 3D Scene Flow Network. (arXiv:2207.09143v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09143">
<div class="article-summary-box-inner">
<span><p>3D scene flow estimation from point clouds is a low-level 3D motion
perception task in computer vision. Flow embedding is a commonly used technique
in scene flow estimation, and it encodes the point motion between two
consecutive frames. Thus, it is critical for the flow embeddings to capture the
correct overall direction of the motion. However, previous works only search
locally to determine a soft correspondence, ignoring the distant points that
turn out to be the actual matching ones. In addition, the estimated
correspondence is usually from the forward direction of the adjacent point
clouds, and may not be consistent with the estimated correspondence acquired
from the backward direction. To tackle these problems, we propose a novel
all-to-all flow embedding layer with backward reliability validation during the
initial scene flow estimation. Besides, we investigate and compare several
design choices in key components of the 3D scene flow network, including the
point similarity calculation, input elements of predictor, and predictor &amp;
refinement level design. After carefully choosing the most effective designs,
we are able to present a model that achieves the state-of-the-art performance
on FlyingThings3D and KITTI Scene Flow datasets. Our proposed model surpasses
all existing methods by at least 38.2% on FlyingThings3D dataset and 24.7% on
KITTI Scene Flow dataset for EPE3D metric. We release our codes at
https://github.com/IRMVLab/3DFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Mutual Modulation for Self-Supervised Cross-Modal Super-Resolution. (arXiv:2207.09156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09156">
<div class="article-summary-box-inner">
<span><p>Self-supervised cross-modal super-resolution (SR) can overcome the difficulty
of acquiring paired training data, but is challenging because only
low-resolution (LR) source and high-resolution (HR) guide images from different
modalities are available. Existing methods utilize pseudo or weak supervision
in LR space and thus deliver results that are blurry or not faithful to the
source modality. To address this issue, we present a mutual modulation SR
(MMSR) model, which tackles the task by a mutual modulation strategy, including
a source-to-guide modulation and a guide-to-source modulation. In these
modulations, we develop cross-domain adaptive filters to fully exploit
cross-modal spatial dependency and help induce the source to emulate the
resolution of the guide and induce the guide to mimic the modality
characteristics of the source. Moreover, we adopt a cycle consistency
constraint to train MMSR in a fully self-supervised manner. Experiments on
various tasks demonstrate the state-of-the-art performance of our MMSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedX: Unsupervised Federated Learning with Cross Knowledge Distillation. (arXiv:2207.09158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09158">
<div class="article-summary-box-inner">
<span><p>This paper presents FedX, an unsupervised federated learning framework. Our
model learns unbiased representation from decentralized and heterogeneous local
data. It employs a two-sided knowledge distillation with contrastive learning
as a core component, allowing the federated system to function without
requiring clients to share any data features. Furthermore, its adaptable
architecture can be used as an add-on module for existing unsupervised
algorithms in federated settings. Experiments show that our model improves
performance significantly (1.58--5.52pp) on five unsupervised algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Stage Virtual Try-on via Deformable Attention Flows. (arXiv:2207.09161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09161">
<div class="article-summary-box-inner">
<span><p>Virtual try-on aims to generate a photo-realistic fitting result given an
in-shop garment and a reference person image. Existing methods usually build up
multi-stage frameworks to deal with clothes warping and body blending
respectively, or rely heavily on intermediate parser-based labels which may be
noisy or even inaccurate. To solve the above challenges, we propose a
single-stage try-on framework by developing a novel Deformable Attention Flow
(DAFlow), which applies the deformable attention scheme to multi-flow
estimation. With pose keypoints as the guidance only, the self- and
cross-deformable attention flows are estimated for the reference person and the
garment images, respectively. By sampling multiple flow fields, the
feature-level and pixel-level information from different semantic areas are
simultaneously extracted and merged through the attention mechanism. It enables
clothes warping and body synthesizing at the same time which leads to
photo-realistic results in an end-to-end manner. Extensive experiments on two
try-on datasets demonstrate that our proposed method achieves state-of-the-art
performance both qualitatively and quantitatively. Furthermore, additional
experiments on the other two image editing tasks illustrate the versatility of
our method for multi-view synthesis and image animation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global and Local Features through Gaussian Mixture Models on Image Semantic Segmentation. (arXiv:2207.09162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09162">
<div class="article-summary-box-inner">
<span><p>The semantic segmentation task aims at dense classification at the pixel-wise
level. Deep models exhibited progress in tackling this task. However, one
remaining problem with these approaches is the loss of spatial precision, often
produced at the segmented objects' boundaries. Our proposed model addresses
this problem by providing an internal structure for the feature representations
while extracting a global representation that supports the former. To fit the
internal structure, during training, we predict a Gaussian Mixture Model from
the data, which, merged with the skip connections and the decoding stage, helps
avoid wrong inductive biases. Furthermore, our results show that we can improve
semantic segmentation by providing both learning representations (global and
local) with a clustering behavior and combining them. Finally, we present
results demonstrating our advances in Cityscapes and Synthia datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Stage Framework for the 2022 Multi-Structure Segmentation for Renal Cancer Treatment. (arXiv:2207.09165v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09165">
<div class="article-summary-box-inner">
<span><p>Three-dimensional (3D) kidney parsing on computed tomography angiography
(CTA) images is of great clinical significance. Automatic segmentation of
kidney, renal tumor, renal vein and renal artery benefits a lot on
surgery-based renal cancer treatment. In this paper, we propose a new
nnhra-unet network, and use a multi-stage framework which is based on it to
segment the multi-structure of kidney and participate in the KiPA2022
challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervision Can Be a Good Few-Shot Learner. (arXiv:2207.09176v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09176">
<div class="article-summary-box-inner">
<span><p>Existing few-shot learning (FSL) methods rely on training with a large
labeled dataset, which prevents them from leveraging abundant unlabeled data.
From an information-theoretic perspective, we propose an effective unsupervised
FSL method, learning representations with self-supervision. Following the
InfoMax principle, our method learns comprehensive representations by capturing
the intrinsic structure of the data. Specifically, we maximize the mutual
information (MI) of instances and their representations with a low-bias MI
estimator to perform self-supervised pre-training. Rather than supervised
pre-training focusing on the discriminable features of the seen classes, our
self-supervised model has less bias toward the seen classes, resulting in
better generalization for unseen classes. We explain that supervised
pre-training and self-supervised pre-training are actually maximizing different
MI objectives. Extensive experiments are further conducted to analyze their FSL
performance with various training settings. Surprisingly, the results show that
self-supervised pre-training can outperform supervised pre-training under the
appropriate conditions. Compared with state-of-the-art FSL methods, our
approach achieves comparable performance on widely used FSL benchmarks without
any labels of the base classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NDF: Neural Deformable Fields for Dynamic Human Modelling. (arXiv:2207.09193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09193">
<div class="article-summary-box-inner">
<span><p>We propose Neural Deformable Fields (NDF), a new representation for dynamic
human digitization from a multi-view video. Recent works proposed to represent
a dynamic human body with shared canonical neural radiance fields which links
to the observation space with deformation fields estimations. However, the
learned canonical representation is static and the current design of the
deformation fields is not able to represent large movements or detailed
geometry changes. In this paper, we propose to learn a neural deformable field
wrapped around a fitted parametric body model to represent the dynamic human.
The NDF is spatially aligned by the underlying reference surface. A neural
network is then learned to map pose to the dynamics of NDF. The proposed NDF
representation can synthesize the digitized performer with novel views and
novel poses with a detailed and reasonable dynamic appearance. Experiments show
that our method significantly outperforms recent human synthesis methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Disentangled Content Information for Face Forgery Detection. (arXiv:2207.09202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09202">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network based face forgery detection methods have
achieved remarkable results during training, but struggled to maintain
comparable performance during testing. We observe that the detector is prone to
focus more on content information than artifact traces, suggesting that the
detector is sensitive to the intrinsic bias of the dataset, which leads to
severe overfitting. Motivated by this key observation, we design an easily
embeddable disentanglement framework for content information removal, and
further propose a Content Consistency Constraint (C2C) and a Global
Representation Contrastive Constraint (GRCC) to enhance the independence of
disentangled features. Furthermore, we cleverly construct two unbalanced
datasets to investigate the impact of the content bias. Extensive
visualizations and experiments demonstrate that our framework can not only
ignore the interference of content information, but also guide the detector to
mine suspicious artifact traces and achieve competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoloGAN: Adversarial Domain Adaptation for Synthetic Depth Data. (arXiv:2207.09204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09204">
<div class="article-summary-box-inner">
<span><p>We present VoloGAN, an adversarial domain adaptation network that translates
synthetic RGB-D images of a high-quality 3D model of a person, into RGB-D
images that could be generated with a consumer depth sensor. This system is
especially useful to generate high amount training data for single-view 3D
reconstruction algorithms replicating the real-world capture conditions, being
able to imitate the style of different sensor types, for the same high-end 3D
model database. The network uses a CycleGAN framework with a U-Net architecture
for the generator and a discriminator inspired by SIV-GAN. We use different
optimizers and learning rate schedules to train the generator and the
discriminator. We further construct a loss function that considers image
channels individually and, among other metrics, evaluates the structural
similarity. We demonstrate that CycleGANs can be used to apply adversarial
domain adaptation of synthetic 3D data to train a volumetric video generator
model having only few training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KinD-LCE Curve Estimation And Retinex Fusion On Low-Light Image. (arXiv:2207.09210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09210">
<div class="article-summary-box-inner">
<span><p>The problems of low light image noise and chromatic aberration is a
challenging problem for tasks such as object detection, semantic segmentation,
instance segmentation, etc. In this paper, we propose the algorithm for low
illumination enhancement. KinD-LCE uses the light curve estimation module in
the network structure to enhance the illumination map in the Retinex decomposed
image, which improves the image brightness; we proposed the illumination map
and reflection map fusion module to restore the restored image details and
reduce the detail loss. Finally, we included a total variation loss function to
eliminate noise. Our method uses the GladNet dataset as the training set, and
the LOL dataset as the test set and is validated using ExDark as the dataset
for downstream tasks. Extensive Experiments on the benchmarks demonstrate the
advantages of our method and are close to the state-of-the-art results, which
achieve a PSNR of 19.7216 and SSIM of 0.8213 in terms of metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Super-Resolution with Deep Dictionary. (arXiv:2207.09228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09228">
<div class="article-summary-box-inner">
<span><p>Since the first success of Dong et al., the deep-learning-based approach has
become dominant in the field of single-image super-resolution. This replaces
all the handcrafted image processing steps of traditional sparse-coding-based
methods with a deep neural network. In contrast to sparse-coding-based methods,
which explicitly create high/low-resolution dictionaries, the dictionaries in
deep-learning-based methods are implicitly acquired as a nonlinear combination
of multiple convolutions. One disadvantage of deep-learning-based methods is
that their performance is degraded for images created differently from the
training dataset (out-of-domain images). We propose an end-to-end
super-resolution network with a deep dictionary (SRDD), where a high-resolution
dictionary is explicitly learned without sacrificing the advantages of deep
learning. Extensive experiments show that explicit learning of high-resolution
dictionary makes the network more robust for out-of-domain test images while
maintaining the performance of the in-domain test images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IDET: Iterative Difference-Enhanced Transformers for High-Quality Change Detection. (arXiv:2207.09240v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09240">
<div class="article-summary-box-inner">
<span><p>Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role for diverse real-world
applications. Nevertheless, most of existing works focus on designing advanced
network architectures to map the feature difference to the final change map
while ignoring the influence of the quality of the feature difference. In this
paper, we study the CD from a new perspective, i.e., how to optimize the
feature difference to highlight changes and suppress unchanged regions, and
propose a novel module denoted as iterative difference-enhanced transformers
(IDET). IDET contains three transformers: two transformers for extracting the
long-range information of the two images and one transformer for enhancing the
feature difference. In contrast to the previous transformers, the third
transformer takes the outputs of the first two transformers to guide the
enhancement of the feature difference iteratively. To achieve more effective
refinement, we further propose the multi-scale IDET-based change detection that
uses multi-scale representations of the images for multiple feature difference
refinements and proposes a coarse-to-fine fusion strategy to combine all
refinements. Our final CD method outperforms seven state-of-the-art methods on
six large-scale datasets under diverse application scenarios, which
demonstrates the importance of feature difference enhancements and the
effectiveness of IDET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Stop Learning: Towards Continual Learning for the CLIP Model. (arXiv:2207.09248v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09248">
<div class="article-summary-box-inner">
<span><p>The Contrastive Language-Image Pre-training (CLIP) Model is a recently
proposed large-scale pre-train model which attracts increasing attention in the
computer vision community. Benefiting from its gigantic image-text training
set, the CLIP model has learned outstanding capabilities in zero-shot learning
and image-text matching. To boost the recognition performance of CLIP on some
target visual concepts, it is often desirable to further update the CLIP model
by fine-tuning some classes-of-interest on extra training data. This operation,
however, raises an important concern: will the update hurt the zero-shot
learning or image-text matching capability of the CLIP, i.e., the catastrophic
forgetting issue? If yes, could existing continual learning algorithms be
adapted to alleviate the risk of catastrophic forgetting? To answer these
questions, this work conducts a systemic study on the continual learning issue
of the CLIP model. We construct evaluation protocols to measure the impact of
fine-tuning updates and explore different ways to upgrade existing continual
learning methods to mitigate the forgetting issue of the CLIP model. Our study
reveals the particular challenges of CLIP continual learning problem and lays a
foundation for further researches. Moreover, we propose a new algorithm, dubbed
Learning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact
effectiveness for alleviating the forgetting issue of the CLIP model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action Quality Assessment with Temporal Parsing Transformer. (arXiv:2207.09270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09270">
<div class="article-summary-box-inner">
<span><p>Action Quality Assessment(AQA) is important for action understanding and
resolving the task poses unique challenges due to subtle visual differences.
Existing state-of-the-art methods typically rely on the holistic video
representations for score regression or ranking, which limits the
generalization to capture fine-grained intra-class variation. To overcome the
above limitation, we propose a temporal parsing transformer to decompose the
holistic feature into temporal part-level representations. Specifically, we
utilize a set of learnable queries to represent the atomic temporal patterns
for a specific action. Our decoding process converts the frame representations
to a fixed number of temporally ordered part representations. To obtain the
quality score, we adopt the state-of-the-art contrastive regression based on
the part representations. Since existing AQA datasets do not provide temporal
part-level labels or partitions, we propose two novel loss functions on the
cross attention responses of the decoder: a ranking loss to ensure the
learnable queries to satisfy the temporal order in cross attention and a
sparsity loss to encourage the part representations to be more discriminative.
Extensive experiments show that our proposed method outperforms prior work on
three public AQA benchmarks by a considerable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Inter-Sample Affinity for Knowability-Aware Universal Domain Adaptation. (arXiv:2207.09280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09280">
<div class="article-summary-box-inner">
<span><p>Universal domain adaptation (UDA) aims to transfer the knowledge of common
classes from source domain to target domain without any prior knowledge on the
label set, which requires to distinguish the unknown samples from the known
ones in the target domain. Recent methods preferred to increase the
inter-sample affinity within a known class, while they ignored the inter-sample
affinity between the unknown samples and the known ones. This paper reveals
that exploiting such inter-sample affinity can significantly improve the
performance of UDA and proposes a knowability-aware UDA framework based on it.
First, we estimate the knowability of each target sample by searching its
neighboring samples in the source domain. Then, we propose an auto-thresholding
scheme applied to the estimated knowability to determine whether a target
sample is unknown or known. Next, in addition to increasing the inter-sample
affinity within each known class like previous methods, we design new losses
based on the estimated knowability to reduce the inter-sample affinity between
the unknown target samples and the known ones. Finally, experiments on four
public datasets demonstrate that our method significantly outperforms existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Room Layout Estimation from a Cubemap of Panorama Image via Deep Manhattan Hough Transform. (arXiv:2207.09291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09291">
<div class="article-summary-box-inner">
<span><p>Significant geometric structures can be compactly described by global
wireframes in the estimation of 3D room layout from a single panoramic image.
Based on this observation, we present an alternative approach to estimate the
walls in 3D space by modeling long-range geometric patterns in a learnable
Hough Transform block. We transform the image feature from a cubemap tile to
the Hough space of a Manhattan world and directly map the feature to the
geometric output. The convolutional layers not only learn the local
gradient-like line features, but also utilize the global information to
successfully predict occluded walls with a simple network structure. Unlike
most previous work, the predictions are performed individually on each cubemap
tile, and then assembled to get the layout estimation. Experimental results
show that we achieve comparable results with recent state-of-the-art in
prediction accuracy and performance. Code is available at
https://github.com/Starrah/DMH-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Caltech Fish Counting Dataset: A Benchmark for Multiple-Object Tracking and Counting. (arXiv:2207.09295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09295">
<div class="article-summary-box-inner">
<span><p>We present the Caltech Fish Counting Dataset (CFC), a large-scale dataset for
detecting, tracking, and counting fish in sonar videos. We identify sonar
videos as a rich source of data for advancing low signal-to-noise computer
vision applications and tackling domain generalization in multiple-object
tracking (MOT) and counting. In comparison to existing MOT and counting
datasets, which are largely restricted to videos of people and vehicles in
cities, CFC is sourced from a natural-world domain where targets are not easily
resolvable and appearance features cannot be easily leveraged for target
re-identification. With over half a million annotations in over 1,500 videos
sourced from seven different sonar cameras, CFC allows researchers to train MOT
and counting algorithms and evaluate generalization performance at unseen test
locations. We perform extensive baseline experiments and identify key
challenges and opportunities for advancing the state of the art in
generalization in MOT and counting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Semantic Statistics Matching (D2SM) Denoising Network. (arXiv:2207.09302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09302">
<div class="article-summary-box-inner">
<span><p>The ultimate aim of image restoration like denoising is to find an exact
correlation between the noisy and clear image domains. But the optimization of
end-to-end denoising learning like pixel-wise losses is performed in a
sample-to-sample manner, which ignores the intrinsic correlation of images,
especially semantics. In this paper, we introduce the Deep Semantic Statistics
Matching (D2SM) Denoising Network. It exploits semantic features of pretrained
classification networks, then it implicitly matches the probabilistic
distribution of clear images at the semantic feature space. By learning to
preserve the semantic distribution of denoised images, we empirically find our
method significantly improves the denoising capabilities of networks, and the
denoised results can be better understood by high-level vision tasks.
Comprehensive experiments conducted on the noisy Cityscapes dataset demonstrate
the superiority of our method on both the denoising performance and semantic
segmentation accuracy. Moreover, the performance improvement observed on our
extended tasks including super-resolution and dehazing experiments shows its
potentiality as a new general plug-and-play component.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DH-AUG: DH Forward Kinematics Model Driven Augmentation for 3D Human Pose Estimation. (arXiv:2207.09303v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09303">
<div class="article-summary-box-inner">
<span><p>Due to the lack of diversity of datasets, the generalization ability of the
pose estimator is poor. To solve this problem, we propose a pose augmentation
solution via DH forward kinematics model, which we call DH-AUG. We observe that
the previous work is all based on single-frame pose augmentation, if it is
directly applied to video pose estimator, there will be several previously
ignored problems: (i) angle ambiguity in bone rotation (multiple solutions);
(ii) the generated skeleton video lacks movement continuity. To solve these
problems, we propose a special generator based on DH forward kinematics model,
which is called DH-generator. Extensive experiments demonstrate that DH-AUG can
greatly increase the generalization ability of the video pose estimator. In
addition, when applied to a single-frame 3D pose estimator, our method
outperforms the previous best pose augmentation method. The source code has
been released at
https://github.com/hlz0606/DH-AUG-DH-Forward-Kinematics-Model-Driven-Augmentation-for-3D-Human-Pose-Estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Trustworthy Healthcare AI: Attention-Based Feature Learning for COVID-19 Screening With Chest Radiography. (arXiv:2207.09312v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09312">
<div class="article-summary-box-inner">
<span><p>Building AI models with trustworthiness is important especially in regulated
areas such as healthcare. In tackling COVID-19, previous work uses
convolutional neural networks as the backbone architecture, which has shown to
be prone to over-caution and overconfidence in making decisions, rendering them
less trustworthy -- a crucial flaw in the context of medical imaging. In this
study, we propose a feature learning approach using Vision Transformers, which
use an attention-based mechanism, and examine the representation learning
capability of Transformers as a new backbone architecture for medical imaging.
Through the task of classifying COVID-19 chest radiographs, we investigate into
whether generalization capabilities benefit solely from Vision Transformers'
architectural advances. Quantitative and qualitative evaluations are conducted
on the trustworthiness of the models, through the use of "trust score"
computation and a visual explainability technique. We conclude that the
attention-based feature learning approach is promising in building trustworthy
deep learning models for healthcare.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-aware Scalable Deep Compressed Sensing. (arXiv:2207.09313v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09313">
<div class="article-summary-box-inner">
<span><p>To more efficiently address image compressed sensing (CS) problems, we
present a novel content-aware scalable network dubbed CASNet which collectively
achieves adaptive sampling rate allocation, fine granular scalability and
high-quality reconstruction. We first adopt a data-driven saliency detector to
evaluate the importances of different image regions and propose a
saliency-based block ratio aggregation (BRA) strategy for sampling rate
allocation. A unified learnable generating matrix is then developed to produce
sampling matrix of any CS ratio with an ordered structure. Being equipped with
the optimization-inspired recovery subnet guided by saliency information and a
multi-block training scheme preventing blocking artifacts, CASNet jointly
reconstructs the image blocks sampled at various sampling rates with one single
model. To accelerate training convergence and improve network robustness, we
propose an SVD-based initialization scheme and a random transformation
enhancement (RTE) strategy, which are extensible without introducing extra
parameters. All the CASNet components can be combined and learned end-to-end.
We further provide a four-stage implementation for evaluation and practical
deployments. Experiments demonstrate that CASNet outperforms other CS networks
by a large margin, validating the collaboration and mutual supports among its
components and strategies. Codes are available at
https://github.com/Guaishou74851/CASNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Interactive Object Segmentation Through a Singulation-and-Grasping Approach. (arXiv:2207.09314v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09314">
<div class="article-summary-box-inner">
<span><p>Instance segmentation with unseen objects is a challenging problem in
unstructured environments. To solve this problem, we propose a robot learning
approach to actively interact with novel objects and collect each object's
training label for further fine-tuning to improve the segmentation model
performance, while avoiding the time-consuming process of manually labeling a
dataset. The Singulation-and-Grasping (SaG) policy is trained through
end-to-end reinforcement learning. Given a cluttered pile of objects, our
approach chooses pushing and grasping motions to break the clutter and conducts
object-agnostic grasping for which the SaG policy takes as input the visual
observations and imperfect segmentation. We decompose the problem into three
subtasks: (1) the object singulation subtask aims to separate the objects from
each other, which creates more space that alleviates the difficulty of (2) the
collision-free grasping subtask; (3) the mask generation subtask to obtain the
self-labeled ground truth masks by using an optical flow-based binary
classifier and motion cue post-processing for transfer learning. Our system
achieves 70% singulation success rate in simulated cluttered scenes. The
interactive segmentation of our system achieves 87.8%, 73.9%, and 69.3% average
precision for toy blocks, YCB objects in simulation and real-world novel
objects, respectively, which outperforms several baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking IoU-based Optimization for Single-stage 3D Object Detection. (arXiv:2207.09332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09332">
<div class="article-summary-box-inner">
<span><p>Since Intersection-over-Union (IoU) based optimization maintains the
consistency of the final IoU prediction metric and losses, it has been widely
used in both regression and classification branches of single-stage 2D object
detectors. Recently, several 3D object detection methods adopt IoU-based
optimization and directly replace the 2D IoU with 3D IoU. However, such a
direct computation in 3D is very costly due to the complex implementation and
inefficient backward operations. Moreover, 3D IoU-based optimization is
sub-optimal as it is sensitive to rotation and thus can cause training
instability and detection performance deterioration. In this paper, we propose
a novel Rotation-Decoupled IoU (RDIoU) method that can mitigate the
rotation-sensitivity issue, and produce more efficient optimization objectives
compared with 3D IoU during the training stage. Specifically, our RDIoU
simplifies the complex interactions of regression parameters by decoupling the
rotation variable as an independent term, yet preserving the geometry of 3D
IoU. By incorporating RDIoU into both the regression and classification
branches, the network is encouraged to learn more precise bounding boxes and
concurrently overcome the misalignment issue between classification and
regression. Extensive experiments on the benchmark KITTI and Waymo Open Dataset
validate that our RDIoU method can bring substantial improvement for the
single-stage 3D object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty in Contrastive Learning: On the Predictability of Downstream Performance. (arXiv:2207.09336v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09336">
<div class="article-summary-box-inner">
<span><p>The superior performance of some of today's state-of-the-art deep learning
models is to some extent owed to extensive (self-)supervised contrastive
pretraining on large-scale datasets. In contrastive learning, the network is
presented with pairs of positive (similar) and negative (dissimilar) datapoints
and is trained to find an embedding vector for each datapoint, i.e., a
representation, which can be further fine-tuned for various downstream tasks.
In order to safely deploy these models in critical decision-making systems, it
is crucial to equip them with a measure of their uncertainty or reliability.
However, due to the pairwise nature of training a contrastive model, and the
lack of absolute labels on the output (an abstract embedding vector), adapting
conventional uncertainty estimation techniques to such models is non-trivial.
In this work, we study whether the uncertainty of such a representation can be
quantified for a single datapoint in a meaningful way. In other words, we
explore if the downstream performance on a given datapoint is predictable,
directly from its pre-trained embedding. We show that this goal can be achieved
by directly estimating the distribution of the training data in the embedding
space and accounting for the local consistency of the representations. Our
experiments show that this notion of uncertainty for an embedding vector often
strongly correlates with its downstream accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Representation Learning with Transformer: A Sequence-to-Sequence Perspective. (arXiv:2207.09339v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09339">
<div class="article-summary-box-inner">
<span><p>Visual representation learning is the key of solving various vision problems.
Relying on the seminal grid structure priors, convolutional neural networks
(CNNs) have been the de facto standard architectures of most deep vision
models. For instance, classical semantic segmentation methods often adopt a
fully-convolutional network (FCN) with an encoder-decoder architecture. The
encoder progressively reduces the spatial resolution and learns more abstract
visual concepts with larger receptive fields. Since context modeling is
critical for segmentation, the latest efforts have been focused on increasing
the receptive field, through either dilated (i.e., atrous) convolutions or
inserting attention modules. However, the FCN-based architecture remains
unchanged. In this paper, we aim to provide an alternative perspective by
treating visual representation learning generally as a sequence-to-sequence
prediction task. Specifically, we deploy a pure Transformer to encode an image
as a sequence of patches, without local convolution and resolution reduction.
With the global context modeled in every layer of the Transformer, stronger
visual representation can be learned for better tackling vision tasks. In
particular, our segmentation model, termed as SEgmentation TRansformer (SETR),
excels on ADE20K (50.28% mIoU, the first position in the test leaderboard on
the day of submission), Pascal Context (55.83% mIoU) and reaches competitive
results on Cityscapes. Further, we formulate a family of Hierarchical
Local-Global (HLG) Transformers characterized by local attention within windows
and global-attention across windows in a hierarchical and pyramidal
architecture. Extensive experiments show that our method achieves appealing
performance on a variety of visual recognition tasks (e.g., image
classification, object detection and instance segmentation and semantic
segmentation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision to the Rescue: Infant Postural Symmetry Estimation from Incongruent Annotations. (arXiv:2207.09352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09352">
<div class="article-summary-box-inner">
<span><p>Bilateral postural symmetry plays a key role as a potential risk marker for
autism spectrum disorder (ASD) and as a symptom of congenital muscular
torticollis (CMT) in infants, but current methods of assessing symmetry require
laborious clinical expert assessments. In this paper, we develop a computer
vision based infant symmetry assessment system, leveraging 3D human pose
estimation for infants. Evaluation and calibration of our system against ground
truth assessments is complicated by our findings from a survey of human ratings
of angle and symmetry, that such ratings exhibit low inter-rater reliability.
To rectify this, we develop a Bayesian estimator of the ground truth derived
from a probabilistic graphical model of fallible human raters. We show that the
3D infant pose estimation model can achieve 68% area under the receiver
operating characteristic curve performance in predicting the Bayesian aggregate
labels, compared to only 61% from a 2D infant pose estimation model and 60%
from a 3D adult pose estimation model, highlighting the importance of 3D poses
and infant domain knowledge in assessing infant body symmetry. Our survey
analysis also suggests that human ratings are susceptible to higher levels of
bias and inconsistency, and hence our final 3D pose-based symmetry assessment
system is calibrated but not directly supervised by Bayesian aggregate human
ratings, yielding higher levels of consistency and lower levels of inter-limb
assessment bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and Editability. (arXiv:2207.09367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09367">
<div class="article-summary-box-inner">
<span><p>GAN inversion aims to invert an input image into the latent space of a
pre-trained GAN. Despite the recent advances in GAN inversion, there remain
challenges to mitigate the tradeoff between distortion and editability, i.e.
reconstructing the input image accurately and editing the inverted image with a
small visual quality drop. The recently proposed pivotal tuning model makes
significant progress towards reconstruction and editability, by using a
two-step approach that first inverts the input image into a latent code, called
pivot code, and then alters the generator so that the input image can be
accurately mapped into the pivot code. Here, we show that both reconstruction
and editability can be improved by a proper design of the pivot code. We
present a simple yet effective method, named cycle encoding, for a high-quality
pivot code. The key idea of our method is to progressively train an encoder in
varying spaces according to a cycle scheme: W-&gt;W+-&gt;W. This training methodology
preserves the properties of both W and W+ spaces, i.e. high editability of W
and low distortion of W+. To further decrease the distortion, we also propose
to refine the pivot code with an optimization-based method, where a
regularization term is introduced to reduce the degradation in editability.
Qualitative and quantitative comparisons to several state-of-the-art methods
demonstrate the superiority of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Recognition based on Multi-Task Learning Framework in the ABAW4 Challenge. (arXiv:2207.09373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09373">
<div class="article-summary-box-inner">
<span><p>This paper presents our submission to the Multi-Task Learning (MTL) Challenge
of the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. Based on
visual feature representations, we utilize three types of temporal encoder to
capture the temporal context information in the video, including the
transformer based encoder, LSTM based encoder and GRU based encoder. With the
temporal context-aware representations, we employ multi-task framework to
predict the valence, arousal, expression and AU values of the images. In
addition, smoothing processing is applied to refine the initial valence and
arousal predictions, and a model ensemble strategy is used to combine multiple
results from different model setups. Our system achieves the performance of
$1.742$ on MTL Challenge validation dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Synthesis with Disentangled Attributes for Chest X-Ray Nodule Augmentation and Detection. (arXiv:2207.09389v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09389">
<div class="article-summary-box-inner">
<span><p>Lung nodule detection in chest X-ray (CXR) images is common to early
screening of lung cancers. Deep-learning-based Computer-Assisted Diagnosis
(CAD) systems can support radiologists for nodule screening in CXR. However, it
requires large-scale and diverse medical data with high-quality annotations to
train such robust and accurate CADs. To alleviate the limited availability of
such datasets, lung nodule synthesis methods are proposed for the sake of data
augmentation. Nevertheless, previous methods lack the ability to generate
nodules that are realistic with the size attribute desired by the detector. To
address this issue, we introduce a novel lung nodule synthesis framework in
this paper, which decomposes nodule attributes into three main aspects
including shape, size, and texture, respectively. A GAN-based Shape Generator
firstly models nodule shapes by generating diverse shape masks. The following
Size Modulation then enables quantitative control on the diameters of the
generated nodule shapes in pixel-level granularity. A coarse-to-fine gated
convolutional Texture Generator finally synthesizes visually plausible nodule
textures conditioned on the modulated shape masks. Moreover, we propose to
synthesize nodule CXR images by controlling the disentangled nodule attributes
for data augmentation, in order to better compensate for the nodules that are
easily missed in the detection task. Our experiments demonstrate the enhanced
image quality, diversity, and controllability of the proposed lung nodule
synthesis framework. We also validate the effectiveness of our data
augmentation on greatly improving nodule detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCLane: Relay Chain Prediction for Lane Detection. (arXiv:2207.09399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09399">
<div class="article-summary-box-inner">
<span><p>Lane detection is an important component of many real-world autonomous
systems. Despite a wide variety of lane detection approaches have been
proposed, reporting steady benchmark improvements over time, lane detection
remains a largely unsolved problem. This is because most of the existing lane
detection methods either treat the lane detection as a dense prediction or a
detection task, few of them consider the unique topologies (Y-shape,
Fork-shape, nearly horizontal lane) of the lane markers, which leads to
sub-optimal solution. In this paper, we present a new method for lane detection
based on relay chain prediction. Specifically, our model predicts a
segmentation map to classify the foreground and background region. For each
pixel point in the foreground region, we go through the forward branch and
backward branch to recover the whole lane. Each branch decodes a transfer map
and a distance map to produce the direction moving to the next point, and how
many steps to progressively predict a relay station (next point). As such, our
model is able to capture the keypoints along the lanes. Despite its simplicity,
our strategy allows us to establish new state-of-the-art on four major
benchmarks including TuSimple, CULane, CurveLanes and LLAMAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Det6D: A Ground-Aware Full-Pose 3D Object Detector for Improving Terrain Robustness. (arXiv:2207.09412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09412">
<div class="article-summary-box-inner">
<span><p>Accurate 3D object detection with LiDAR is critical for autonomous driving.
Existing research is all based on the flat-world assumption. However, the
actual road can be complex with steep sections, which breaks the premise.
Current methods suffer from performance degradation in this case due to
difficulty correctly detecting objects on sloped terrain. In this work, we
propose Det6D, the first full-degree-of-freedom 3D object detector without
spatial and postural limitations, to improve terrain robustness. We choose the
point-based framework by founding their capability of detecting objects in the
entire spatial range. To predict full-degree poses, including pitch and roll,
we design a ground-aware orientation branch that leverages the local ground
constraints. Given the difficulty of long-tail non-flat scene data collection
and 6D pose annotation, we present Slope-Aug, a data augmentation method for
synthesizing non-flat terrain from existing datasets recorded in flat scenes.
Experiments on various datasets demonstrate the effectiveness and robustness of
our method in different terrains. We further conducted an extended experiment
to explore how the network predicts the two extra poses. The proposed modules
are plug-and-play for existing point-based frameworks. The code is available at
https://github.com/HITSZ-NRSL/De6D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SphereFed: Hyperspherical Federated Learning. (arXiv:2207.09413v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09413">
<div class="article-summary-box-inner">
<span><p>Federated Learning aims at training a global model from multiple
decentralized devices (i.e. clients) without exchanging their private local
data. A key challenge is the handling of non-i.i.d. (independent identically
distributed) data across multiple clients that may induce disparities of their
local features. We introduce the Hyperspherical Federated Learning (SphereFed)
framework to address the non-i.i.d. issue by constraining learned
representations of data points to be on a unit hypersphere shared by clients.
Specifically, all clients learn their local representations by minimizing the
loss with respect to a fixed classifier whose weights span the unit
hypersphere. After federated training in improving the global model, this
classifier is further calibrated with a closed-form solution by minimizing a
mean squared loss. We show that the calibration solution can be computed
efficiently and distributedly without direct access of local data. Extensive
experiments indicate that our SphereFed approach is able to improve the
accuracy of multiple existing federated learning algorithms by a considerable
margin (up to 6% on challenging datasets) with enhanced computation and
communication efficiency across datasets and model architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Features Informed Multi-person Human-object Interaction Recognition in Videos. (arXiv:2207.09425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09425">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) recognition in videos is important for
analyzing human activity. Most existing work focusing on visual features
usually suffer from occlusion in the real-world scenarios. Such a problem will
be further complicated when multiple people and objects are involved in HOIs.
Consider that geometric features such as human pose and object position provide
meaningful information to understand HOIs, we argue to combine the benefits of
both visual and geometric features in HOI recognition, and propose a novel
Two-level Geometric feature-informed Graph Convolutional Network (2G-GCN). The
geometric-level graph models the interdependency between geometric features of
humans and objects, while the fusion-level graph further fuses them with visual
features of humans and objects. To demonstrate the novelty and effectiveness of
our method in challenging scenarios, we propose a new multi-person HOI dataset
(MPHOI-72). Extensive experiments on MPHOI-72 (multi-person HOI), CAD-120
(single-human HOI) and Bimanual Actions (two-hand HOI) datasets demonstrate our
superior performance compared to state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theseus: A Library for Differentiable Nonlinear Optimization. (arXiv:2207.09442v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09442">
<div class="article-summary-box-inner">
<span><p>We present Theseus, an efficient application-agnostic open source library for
differentiable nonlinear least squares (DNLS) optimization built on PyTorch,
providing a common framework for end-to-end structured learning in robotics and
vision. Existing DNLS implementations are application specific and do not
always incorporate many ingredients important for efficiency. Theseus is
application-agnostic, as we illustrate with several example applications that
are built using the same underlying differentiable components, such as
second-order optimizers, standard costs functions, and Lie groups. For
efficiency, Theseus incorporates support for sparse solvers, automatic
vectorization, batching, GPU acceleration, and gradient computation with
implicit differentiation and direct loss minimization. We do extensive
performance evaluation in a set of applications, demonstrating significant
efficiency gains and better scalability when these features are incorporated.
Project page: https://sites.google.com/view/theseus-ai
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoserNet: Refining Relative Camera Poses Exploiting Object Detections. (arXiv:2207.09445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09445">
<div class="article-summary-box-inner">
<span><p>The estimation of the camera poses associated with a set of images commonly
relies on feature matches between the images. In contrast, we are the first to
address this challenge by using objectness regions to guide the pose estimation
problem rather than explicit semantic object detections. We propose Pose
Refiner Network (PoserNet) a light-weight Graph Neural Network to refine the
approximate pair-wise relative camera poses. PoserNet exploits associations
between the objectness regions - concisely expressed as bounding boxes - across
multiple views to globally refine sparsely connected view graphs. We evaluate
on the 7-Scenes dataset across varied sizes of graphs and show how this process
can be beneficial to optimisation-based Motion Averaging algorithms improving
the median error on the rotation by 62 degrees with respect to the initial
estimates obtained based on bounding boxes. Code and data are available at
https://github.com/IIT-PAVIS/PoserNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model. (arXiv:2207.09446v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09446">
<div class="article-summary-box-inner">
<span><p>We present ShapeCrafter, a neural network for recursive text-conditioned 3D
shape generation. Existing methods to generate text-conditioned 3D shapes
consume an entire text prompt to generate a 3D shape in a single step. However,
humans tend to describe shapes recursively-we may start with an initial
description and progressively add details based on intermediate results. To
capture this recursive process, we introduce a method to generate a 3D shape
distribution, conditioned on an initial phrase, that gradually evolves as more
phrases are added. Since existing datasets are insufficient for training this
approach, we present Text2Shape++, a large dataset of 369K shape-text pairs
that supports recursive shape generation. To capture local details that are
often used to refine shape descriptions, we build on top of vector-quantized
deep implicit functions that generate a distribution of high-quality shapes.
Results show that our method can generate shapes consistent with text
descriptions, and shapes evolve gradually as more phrases are added. Our method
supports shape editing, extrapolation, and can enable new applications in
human-machine collaboration for creative design.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-to-Robot Imitation in the Wild. (arXiv:2207.09450v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.09450">
<div class="article-summary-box-inner">
<span><p>We approach the problem of learning by watching humans in the wild. While
traditional approaches in Imitation and Reinforcement Learning are promising
for learning in the real world, they are either sample inefficient or are
constrained to lab settings. Meanwhile, there has been a lot of success in
processing passive, unstructured human data. We propose tackling this problem
via an efficient one-shot robot learning algorithm, centered around learning
from a third-person perspective. We call our method WHIRL: In-the-Wild Human
Imitating Robot Learning. WHIRL extracts a prior over the intent of the human
demonstrator, using it to initialize our agent's policy. We introduce an
efficient real-world policy learning scheme that improves using interactions.
Our key contributions are a simple sampling-based policy optimization approach,
a novel objective function for aligning human and robot videos as well as an
exploration method to boost sample efficiency. We show one-shot generalization
and success in real-world settings, including 20 different manipulation tasks
in the wild. Videos and talk at https://human2robot.github.io
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Perspective on Stabilizing GANs training: Direct Adversarial Training. (arXiv:2008.09041v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09041">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) are the most popular image generation
models that have achieved remarkable progress on various computer vision tasks.
However, training instability is still one of the open problems for all
GAN-based algorithms. Quite a number of methods have been proposed to stabilize
the training of GANs, the focuses of which were respectively put on the loss
functions, regularization and normalization technologies, training algorithms,
and model architectures. Different from the above methods, in this paper, a new
perspective on stabilizing GANs training is presented. It is found that
sometimes the images produced by the generator act like adversarial examples of
the discriminator during the training process, which may be part of the reason
causing the unstable training of GANs. With this finding, we propose the Direct
Adversarial Training (DAT) method to stabilize the training process of GANs.
Furthermore, we prove that the DAT method is able to minimize the Lipschitz
constant of the discriminator adaptively. The advanced performance of DAT is
verified on multiple loss functions, network architectures, hyper-parameters,
and datasets. Specifically, DAT achieves significant improvements of 11.5% FID
on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10
unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom
unconditional generation based on SSGAN. Code will be available at
https://github.com/iceli1007/DAT-GAN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Learning the Right Attention Point for Feature Enhancement. (arXiv:2012.06257v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06257">
<div class="article-summary-box-inner">
<span><p>We present a novel attention-based mechanism to learn enhanced point features
for point cloud processing tasks, e.g., classification and segmentation. Unlike
prior works, which were trained to optimize the weights of a pre-selected set
of attention points, our approach learns to locate the best attention points to
maximize the performance of a specific task, e.g., point cloud classification.
Importantly, we advocate the use of single attention point to facilitate
semantic understanding in point feature learning. Specifically, we formulate a
new and simple convolution, which combines convolutional features from an input
point and its corresponding learned attention point, or LAP, for short. Our
attention mechanism can be easily incorporated into state-of-the-art point
cloud classification and segmentation networks. Extensive experiments on common
benchmarks such as ModelNet40, ShapeNetPart, and S3DIS all demonstrate that our
LAP-enabled networks consistently outperform the respective original networks,
as well as other competitive alternatives, which employ multiple attention
points, either pre-selected or learned under our LAP framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainability of deep vision-based autonomous driving systems: Review and challenges. (arXiv:2101.05307v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05307">
<div class="article-summary-box-inner">
<span><p>This survey reviews explainability methods for vision-based self-driving
systems trained with behavior cloning. The concept of explainability has
several facets and the need for explainability is strong in driving, a
safety-critical application. Gathering contributions from several research
fields, namely computer vision, deep learning, autonomous driving, explainable
AI (X-AI), this survey tackles several points. First, it discusses definitions,
context, and motivation for gaining more interpretability and explainability
from self-driving systems, as well as the challenges that are specific to this
application. Second, methods providing explanations to a black-box self-driving
system in a post-hoc fashion are comprehensively organized and detailed. Third,
approaches from the literature that aim at building more interpretable
self-driving systems by design are presented and discussed in detail. Finally,
remaining open-challenges and potential future research directions are
identified and examined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOTR: End-to-End Multiple-Object Tracking with Transformer. (arXiv:2105.03247v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03247">
<div class="article-summary-box-inner">
<span><p>Temporal modeling of objects is a key challenge in multiple object tracking
(MOT). Existing methods track by associating detections through motion-based
and appearance-based similarity heuristics. The post-processing nature of
association prevents end-to-end exploitation of temporal variations in video
sequence. In this paper, we propose MOTR, which extends DETR and introduces
track query to model the tracked instances in the entire video. Track query is
transferred and updated frame-by-frame to perform iterative prediction over
time. We propose tracklet-aware label assignment to train track queries and
newborn object queries. We further propose temporal aggregation network and
collective average loss to enhance temporal relation modeling. Experimental
results on DanceTrack show that MOTR significantly outperforms state-of-the-art
method, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our
concurrent works, TrackFormer and TransTrack, on association performance. MOTR
can serve as a stronger baseline for future research on temporal modeling and
Transformer-based trackers. Code is available at
https://github.com/megvii-research/MOTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Deep Classifiers Agree: Analyzing Correlations between Learning Order and Image Statistics. (arXiv:2105.08997v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08997">
<div class="article-summary-box-inner">
<span><p>Although a plethora of architectural variants for deep classification has
been introduced over time, recent works have found empirical evidence towards
similarities in their training process. It has been hypothesized that neural
networks converge not only to similar representations, but also exhibit a
notion of empirical agreement on which data instances are learned first.
Following in the latter works$'$ footsteps, we define a metric to quantify the
relationship between such classification agreement over time, and posit that
the agreement phenomenon can be mapped to core statistics of the investigated
dataset. We empirically corroborate this hypothesis across the CIFAR10, Pascal,
ImageNet and KTH-TIPS2 datasets. Our findings indicate that agreement seems to
be independent of specific architectures, training hyper-parameters or labels,
albeit follows an ordering according to image statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaboration of Experts: Achieving 80% Top-1 Accuracy on ImageNet with 100M FLOPs. (arXiv:2107.03815v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03815">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a Collaboration of Experts (CoE) framework to pool
together the expertise of multiple networks towards a common aim. Each expert
is an individual network with expertise on a unique portion of the dataset,
which enhances the collective capacity. Given a sample, an expert is selected
by the delegator, which simultaneously outputs a rough prediction to support
early termination. To fulfill this framework, we propose three modules to impel
each model to play its role, namely weight generation module (WGM), label
generation module (LGM) and variance calculation module (VCM). Our method
achieves the state-of-the-art performance on ImageNet, 80.7% top-1 accuracy
with 194M FLOPs. Combined with PWLU activation function and CondConv, CoE
further achieves the accuracy of 80.0% with only 100M FLOPs for the first time.
More importantly, our method is hardware friendly and achieves a 3-6x speedup
compared with some existing conditional computation approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset. (arXiv:2107.04286v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04286">
<div class="article-summary-box-inner">
<span><p>We present UrbanScene3D, a large-scale data platform for research of urban
scene perception and reconstruction. UrbanScene3D contains over 128k
high-resolution images covering 16 scenes including large-scale real urban
regions and synthetic cities with 136 km^2 area in total. The dataset also
contains high-precision LiDAR scans and hundreds of image sets with different
observation patterns, which provide a comprehensive benchmark to design and
evaluate aerial path planning and 3D reconstruction algorithms. In addition,
the dataset, which is built on Unreal Engine and Airsim simulator together with
the manually annotated unique instance label for each building in the dataset,
enables the generation of all kinds of data, e.g., 2D depth maps, 2D/3D
bounding boxes, and 3D point cloud/mesh segmentations, etc. The simulator with
physical engine and lighting system not only produce variety of data but also
enable users to simulate cars or drones in the proposed urban environment for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust outlier detection by de-biasing VAE likelihoods. (arXiv:2108.08760v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08760">
<div class="article-summary-box-inner">
<span><p>Deep networks often make confident, yet, incorrect, predictions when tested
with outlier data that is far removed from their training distributions.
Likelihoods computed by deep generative models (DGMs) are a candidate metric
for outlier detection with unlabeled data. Yet, previous studies have shown
that DGM likelihoods are unreliable and can be easily biased by simple
transformations to input data. Here, we examine outlier detection with
variational autoencoders (VAEs), among the simplest of DGMs. We propose novel
analytical and algorithmic approaches to ameliorate key biases with VAE
likelihoods. Our bias corrections are sample-specific, computationally
inexpensive, and readily computed for various decoder visible distributions.
Next, we show that a well-known image pre-processing technique -- contrast
stretching -- extends the effectiveness of bias correction to further improve
outlier detection. Our approach achieves state-of-the-art accuracies with nine
grayscale and natural image datasets, and demonstrates significant advantages
-- both with speed and performance -- over four recent, competing approaches.
In summary, lightweight remedies suffice to achieve robust outlier detection
with VAEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaitPrivacyON: Privacy-Preserving Mobile Gait Biometrics using Unsupervised Learning. (arXiv:2110.03967v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03967">
<div class="article-summary-box-inner">
<span><p>Numerous studies in the literature have already shown the potential of
biometrics on mobile devices for authentication purposes. However, it has been
shown that, the learning processes associated to biometric systems might expose
sensitive personal information about the subjects. This study proposes
GaitPrivacyON, a novel mobile gait biometrics verification approach that
provides accurate authentication results while preserving the sensitive
information of the subject. It comprises two modules: i) a convolutional
Autoencoder that transforms attributes of the biometric raw data, such as the
gender or the activity being performed, into a new privacy-preserving
representation; and ii) a mobile gait verification system based on the
combination of Convolutional Neural Networks (CNNs) and Recurrent Neural
Networks (RNNs) with a Siamese architecture. The main advantage of
GaitPrivacyON is that the first module (convolutional Autoencoder) is trained
in an unsupervised way, without specifying the sensitive attributes of the
subject to protect. The experimental results achieved using two popular
databases (MotionSense and MobiAct) suggest the potential of GaitPrivacyON to
significantly improve the privacy of the subject while keeping user
authentication results higher than 99% Area Under the Curve (AUC). To the best
of our knowledge, this is the first mobile gait verification approach that
considers privacy-preserving methods trained in an unsupervised way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization. (arXiv:2110.04538v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04538">
<div class="article-summary-box-inner">
<span><p>The essence of unsupervised anomaly detection is to learn the compact
distribution of normal samples and detect outliers as anomalies in testing.
Meanwhile, the anomalies in real-world are usually subtle and fine-grained in a
high-resolution image especially for industrial applications. Towards this end,
we propose a novel framework for unsupervised anomaly detection and
localization. Our method aims at learning dense and compact distribution from
normal images with a coarse-to-fine alignment process. The coarse alignment
stage standardizes the pixel-wise position of objects in both image and feature
levels. The fine alignment stage then densely maximizes the similarity of
features among all corresponding locations in a batch. To facilitate the
learning with only normal images, we propose a new pretext task called
non-contrastive learning for the fine alignment stage. Non-contrastive learning
extracts robust and discriminating normal image representations without making
assumptions on abnormal samples, and it thus empowers our model to generalize
to various anomalous scenarios. Extensive experiments on two typical industrial
datasets of MVTec AD and BenTech AD demonstrate that our framework is effective
in detecting various real-world defects and achieves a new state-of-the-art in
industrial unsupervised anomaly detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content Preserving Image Translation with Texture Co-occurrence and Spatial Self-Similarity for Texture Debiasing and Domain Adaptation. (arXiv:2110.07920v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07920">
<div class="article-summary-box-inner">
<span><p>Models trained on datasets with texture bias usually perform poorly on
out-of-distribution samples since biased representations are embedded into the
model. Recently, various image translation and debiasing methods have attempted
to disentangle texture biased representations for downstream tasks, but
accurately discarding biased features without altering other relevant
information is still challenging. In this paper, we propose a novel framework
that leverages image translation to generate additional training images using
the content of a source image and the texture of a target image with a
different bias property to explicitly mitigate texture bias when training a
model on a target task. Our model ensures texture similarity between the target
and generated images via a texture co-occurrence loss while preserving content
details from source images with a spatial self-similarity loss. Both the
generated and original training images are combined to train improved
classification or segmentation models robust to inconsistent texture bias.
Evaluation on five classification- and two segmentation-datasets with known
texture biases demonstrates the utility of our method, and reports significant
improvements over recent state-of-the-art methods in all cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Keypoint Representations: Modeling Keypoints and Poses as Objects for Multi-Person Human Pose Estimation. (arXiv:2111.08557v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08557">
<div class="article-summary-box-inner">
<span><p>In keypoint estimation tasks such as human pose estimation, heatmap-based
regression is the dominant approach despite possessing notable drawbacks:
heatmaps intrinsically suffer from quantization error and require excessive
computation to generate and post-process. Motivated to find a more efficient
solution, we propose to model individual keypoints and sets of spatially
related keypoints (i.e., poses) as objects within a dense single-stage
anchor-based detection framework. Hence, we call our method KAPAO (pronounced
"Ka-Pow"), for Keypoints And Poses As Objects. KAPAO is applied to the problem
of single-stage multi-person human pose estimation by simultaneously detecting
human pose and keypoint objects and fusing the detections to exploit the
strengths of both object representations. In experiments, we observe that KAPAO
is faster and more accurate than previous methods, which suffer greatly from
heatmap post-processing. The accuracy-speed trade-off is especially favourable
in the practical setting when not using test-time augmentation. Source code:
https://github.com/wmcnally/kapao.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STEEX: Steering Counterfactual Explanations with Semantics. (arXiv:2111.09094v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09094">
<div class="article-summary-box-inner">
<span><p>As deep learning models are increasingly used in safety-critical
applications, explainability and trustworthiness become major concerns. For
simple images, such as low-resolution face portraits, synthesizing visual
counterfactual explanations has recently been proposed as a way to uncover the
decision mechanisms of a trained classification model. In this work, we address
the problem of producing counterfactual explanations for high-quality images
and complex scenes. Leveraging recent semantic-to-image models, we propose a
new generative counterfactual explanation framework that produces plausible and
sparse modifications which preserve the overall scene structure. Furthermore,
we introduce the concept of "region-targeted counterfactual explanations", and
a corresponding framework, where users can guide the generation of
counterfactuals by specifying a set of semantic regions of the query image the
explanation must be about. Extensive experiments are conducted on challenging
datasets including high-quality portraits (CelebAMask-HQ) and driving scenes
(BDD100k). Code is available at https://github.com/valeoai/STEEX
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11430">
<div class="article-summary-box-inner">
<span><p>What constitutes an object? This has been a long-standing question in
computer vision. Towards this goal, numerous learning-free and learning-based
approaches have been developed to score objectness. However, they generally do
not scale well across new domains and novel objects. In this paper, we advocate
that existing methods lack a top-down supervision signal governed by
human-understandable semantics. For the first time in literature, we
demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned
image-text pairs can effectively bridge this gap. Our extensive experiments
across various domains and novel objects show the state-of-the-art performance
of MViTs to localize generic objects in images. Based on the observation that
existing MViTs do not include multi-scale feature processing and usually
require longer training schedules, we develop an efficient MViT architecture
using multi-scale deformable attention and late vision-language fusion. We show
the significance of MViT proposals in a diverse range of applications including
open-world object detection, salient and camouflage object detection,
supervised and self-supervised detection tasks. Further, MViTs can adaptively
generate proposals given a specific language query and thus offer enhanced
interactability. Code: \url{https://git.io/J1HPY}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Depth and Normal Estimation using Single Dual-Pixel Camera. (arXiv:2111.12928v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12928">
<div class="article-summary-box-inner">
<span><p>Many mobile manufacturers recently have adopted Dual-Pixel (DP) sensors in
their flagship models for faster auto-focus and aesthetic image captures.
Despite their advantages, research on their usage for 3D facial understanding
has been limited due to the lack of datasets and algorithmic designs that
exploit parallax in DP images. This is because the baseline of sub-aperture
images is extremely narrow and parallax exists in the defocus blur region. In
this paper, we introduce a DP-oriented Depth/Normal network that reconstructs
the 3D facial geometry. For this purpose, we collect a DP facial data with more
than 135K images for 101 persons captured with our multi-camera structured
light systems. It contains the corresponding ground-truth 3D models including
depth map and surface normal in metric scale. Our dataset allows the proposed
matching network to be generalized for 3D facial depth/normal estimation. The
proposed network consists of two novel modules: Adaptive Sampling Module and
Adaptive Normal Module, which are specialized in handling the defocus blur in
DP images. Finally, the proposed method achieves state-of-the-art performances
over recent DP-based depth/normal estimation methods. We also demonstrate the
applicability of the estimated depth/normal to face spoofing and relighting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-LTR: Learning Class-wise Visual-Linguistic Representation for Long-Tailed Visual Recognition. (arXiv:2111.13579v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13579">
<div class="article-summary-box-inner">
<span><p>Deep learning-based models encounter challenges when processing long-tailed
data in the real world. Existing solutions usually employ some balancing
strategies or transfer learning to deal with the class imbalance problem, based
on the image modality. In this work, we present a visual-linguistic long-tailed
recognition framework, termed VL-LTR, and conduct empirical studies on the
benefits of introducing text modality for long-tailed recognition (LTR).
Compared to existing approaches, the proposed VL-LTR has the following merits.
(1) Our method can not only learn visual representation from images but also
learn corresponding linguistic representation from noisy class-level text
descriptions collected from the Internet; (2) Our method can effectively use
the learned visual-linguistic representation to improve the visual recognition
performance, especially for classes with fewer image samples. We also conduct
extensive experiments and set the new state-of-the-art performance on
widely-used LTR benchmarks. Notably, our method achieves 77.2% overall accuracy
on ImageNet-LT, which significantly outperforms the previous best method by
over 17 points, and is close to the prevailing performance training on the full
ImageNet. Code is available at https://github.com/ChangyaoTian/VL-LTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\ell_\infty$-Robustness and Beyond: Unleashing Efficient Adversarial Training. (arXiv:2112.00378v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00378">
<div class="article-summary-box-inner">
<span><p>Neural networks are vulnerable to adversarial attacks: adding well-crafted,
imperceptible perturbations to their input can modify their output. Adversarial
training is one of the most effective approaches in training robust models
against such attacks. However, it is much slower than vanilla training of
neural networks since it needs to construct adversarial examples for the entire
training data at every iteration, hampering its effectiveness. Recently, Fast
Adversarial Training (FAT) was proposed that can obtain robust models
efficiently. However, the reasons behind its success are not fully understood,
and more importantly, it can only train robust models for $\ell_\infty$-bounded
attacks as it uses FGSM during training. In this paper, by leveraging the
theory of coreset selection, we show how selecting a small subset of training
data provides a general, more principled approach toward reducing the time
complexity of robust training. Unlike existing methods, our approach can be
adapted to a wide variety of training objectives, including TRADES,
$\ell_p$-PGD, and Perceptual Adversarial Training (PAT). Our experimental
results indicate that our approach speeds up adversarial training by 2-3 times
while experiencing a slight reduction in the clean and robust accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-Centric Unsupervised Image Captioning. (arXiv:2112.00969v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00969">
<div class="article-summary-box-inner">
<span><p>Image captioning is a longstanding problem in the field of computer vision
and natural language processing. To date, researchers have produced impressive
state-of-the-art performance in the age of deep learning. Most of these
state-of-the-art, however, requires large volume of annotated image-caption
pairs in order to train their models. When given an image dataset of interests,
practitioner needs to annotate the caption for each image in the training set
and this process needs to happen for each newly collected image dataset. In
this paper, we explore the task of unsupervised image captioning which utilizes
unpaired images and texts to train the model so that the texts can come from
different sources than the images. A main school of research on this topic that
has been shown to be effective is to construct pairs from the images and texts
in the training set according to their overlap of objects. Unlike in the
supervised setting, these constructed pairings are however not guaranteed to
have fully overlapping set of objects. Our work in this paper overcomes this by
harvesting objects corresponding to a given sentence from the training set,
even if they don't belong to the same image. When used as input to a
transformer, such mixture of objects enables larger if not full object
coverage, and when supervised by the corresponding sentence, produced results
that outperform current state of the art unsupervised methods by a significant
margin. Building upon this finding, we further show that (1) additional
information on relationship between objects and attributes of objects also
helps in boosting performance; and (2) our method also extends well to
non-English image captioning, which usually suffers from a scarcer level of
annotations. Our findings are supported by strong empirical results. Our code
is available at https://github.com/zihangm/obj-centric-unsup-caption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TISE: Bag of Metrics for Text-to-Image Synthesis Evaluation. (arXiv:2112.01398v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01398">
<div class="article-summary-box-inner">
<span><p>In this paper, we conduct a study on the state-of-the-art methods for
text-to-image synthesis and propose a framework to evaluate these methods. We
consider syntheses where an image contains a single or multiple objects. Our
study outlines several issues in the current evaluation pipeline: (i) for image
quality assessment, a commonly used metric, e.g., Inception Score (IS), is
often either miscalibrated for the single-object case or misused for the
multi-object case; (ii) for text relevance and object accuracy assessment,
there is an overfitting phenomenon in the existing R-precision (RP) and
Semantic Object Accuracy (SOA) metrics, respectively; (iii) for multi-object
case, many vital factors for evaluation, e.g., object fidelity, positional
alignment, counting alignment, are largely dismissed; (iv) the ranking of the
methods based on current metrics is highly inconsistent with real images. To
overcome these issues, we propose a combined set of existing and new metrics to
systematically evaluate the methods. For existing metrics, we offer an improved
version of IS named IS* by using temperature scaling to calibrate the
confidence of the classifier used by IS; we also propose a solution to mitigate
the overfitting issues of RP and SOA. For new metrics, we develop counting
alignment, positional alignment, object-centric IS, and object-centric FID
metrics for evaluating the multi-object case. We show that benchmarking with
our bag of metrics results in a highly consistent ranking among existing
methods that is well-aligned with human evaluation. As a by-product, we create
AttnGAN++, a simple but strong baseline for the benchmark by stabilizing the
training of AttnGAN using spectral normalization. We also release our toolbox,
so-called TISE, for advocating fair and consistent evaluation of text-to-image
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CA-SSL: Class-Agnostic Semi-Supervised Learning for Detection and Segmentation. (arXiv:2112.04966v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04966">
<div class="article-summary-box-inner">
<span><p>To improve instance-level detection/segmentation performance, existing
self-supervised and semi-supervised methods extract either task-unrelated or
task-specific training signals from unlabeled data. We show that these two
approaches, at the two extreme ends of the task-specificity spectrum, are
suboptimal for the task performance. Utilizing too little task-specific
training signals causes underfitting to the ground-truth labels of downstream
tasks, while the opposite causes overfitting to the ground-truth labels. To
this end, we propose a novel Class-Agnostic Semi-Supervised Learning (CA-SSL)
framework to achieve a more favorable task-specificity balance in extracting
training signals from unlabeled data. CA-SSL has three training stages that act
on either ground-truth labels (labeled data) or pseudo labels (unlabeled data).
This decoupling strategy avoids the complicated scheme in traditional SSL
methods that balances the contributions from both data types. Especially, we
introduce a warmup training stage to achieve a more optimal balance in task
specificity by ignoring class information in the pseudo labels, while
preserving localization training signals. As a result, our warmup model can
better avoid underfitting/overfitting when fine-tuned on the ground-truth
labels in detection and segmentation tasks. Using 3.6M unlabeled data, we
achieve a significant performance gain of 4.7% over ImageNet-pretrained
baseline on FCOS object detection. In addition, our warmup model demonstrates
excellent transferability to other detection and segmentation frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FEAR: Fast, Efficient, Accurate and Robust Visual Tracker. (arXiv:2112.07957v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07957">
<div class="article-summary-box-inner">
<span><p>We present FEAR, a family of fast, efficient, accurate, and robust Siamese
visual trackers. We present a novel and efficient way to benefit from
dual-template representation for object model adaption, which incorporates
temporal information with only a single learnable parameter. We further improve
the tracker architecture with a pixel-wise fusion block. By plugging-in
sophisticated backbones with the abovementioned modules, FEAR-M and FEAR-L
trackers surpass most Siamese trackers on several academic benchmarks in both
accuracy and efficiency. Employed with the lightweight backbone, the optimized
version FEAR-XS offers more than 10 times faster tracking than current Siamese
trackers while maintaining near state-of-the-art results. FEAR-XS tracker is
2.4x smaller and 4.3x faster than LightTrack with superior accuracy. In
addition, we expand the definition of the model efficiency by introducing FEAR
benchmark that assesses energy consumption and execution speed. We show that
energy consumption is a limiting factor for trackers on mobile devices. Source
code, pretrained models, and evaluation protocol are available at
https://github.com/PinataFarms/FEARTracker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Analysis of the Object Detection Pipeline on UAVs. (arXiv:2203.00306v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00306">
<div class="article-summary-box-inner">
<span><p>An object detection pipeline comprises a camera that captures the scene and
an object detector that processes these images. The quality of the images
directly affects the performance of the object detector. Many works nowadays
focus either on improving the image quality or improving the object detection
models independently, but neglect the importance of joint optimization of the
two subsystems. The goal of this paper is to tune the detection throughput and
accuracy of existing object detectors in the remote sensing scenario by
focusing on optimizing the input images tailored to the object detector. To
achieve this, we empirically analyze the influence of two selected camera
calibration parameters (camera distortion correction and gamma correction) and
five image parameters (quantization, compression, resolution, color model,
additional channels) for these applications. For our experiments, we utilize
three UAV data sets from different domains and a mixture of large and small
state-of-the-art object detector models to provide an extensive evaluation of
the influence of the pipeline parameters. Finally, we realize an object
detection pipeline prototype on an embedded platform for an UAV and give a best
practice recommendation for building object detection pipelines based on our
findings. We show that not all parameters have an equal impact on detection
accuracy and data throughput, and that by using a suitable compromise between
parameters we are able to achieve higher detection accuracy for lightweight
object detection models, while keeping the same data throughput.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Hierarchical Graph Representation for Large-Scale Zero-Shot Image Classification. (arXiv:2203.01386v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01386">
<div class="article-summary-box-inner">
<span><p>The main question we address in this paper is how to scale up visual
recognition of unseen classes, also known as zero-shot learning, to tens of
thousands of categories as in the ImageNet-21K benchmark. At this scale,
especially with many fine-grained categories included in ImageNet-21K, it is
critical to learn quality visual semantic representations that are
discriminative enough to recognize unseen classes and distinguish them from
seen ones. We propose a \emph{H}ierarchical \emph{G}raphical knowledge
\emph{R}epresentation framework for the confidence-based classification method,
dubbed as HGR-Net. Our experimental results demonstrate that HGR-Net can grasp
class inheritance relations by utilizing hierarchical conceptual knowledge. Our
method significantly outperformed all existing techniques, boosting the
performance by 7\% compared to the runner-up approach on the ImageNet-21K
benchmark. We show that HGR-Net is learning-efficient in few-shot scenarios. We
also analyzed our method on smaller datasets like ImageNet-21K-P, 2-hops and
3-hops, demonstrating its generalization ability. Our benchmark and code are
available at https://kaiyi.me/p/hgrnet.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiT: Self-supervised Pre-training for Document Image Transformer. (arXiv:2203.02378v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02378">
<div class="article-summary-box-inner">
<span><p>Image Transformer has recently achieved significant progress for natural
image understanding, either using supervised (ViT, DeiT, etc.) or
self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we
propose \textbf{DiT}, a self-supervised pre-trained \textbf{D}ocument
\textbf{I}mage \textbf{T}ransformer model using large-scale unlabeled text
images for Document AI tasks, which is essential since no supervised
counterparts ever exist due to the lack of human-labeled document images. We
leverage DiT as the backbone network in a variety of vision-based Document AI
tasks, including document image classification, document layout analysis, table
detection as well as text detection for OCR. Experiment results have
illustrated that the self-supervised pre-trained DiT model achieves new
state-of-the-art results on these downstream tasks, e.g. document image
classification (91.11 $\rightarrow$ 92.69), document layout analysis (91.0
$\rightarrow$ 94.9), table detection (94.23 $\rightarrow$ 96.55) and text
detection for OCR (93.07 $\rightarrow$ 94.29). The code and pre-trained models
are publicly available at \url{https://aka.ms/msdit}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CF-ViT: A General Coarse-to-Fine Method for Vision Transformer. (arXiv:2203.03821v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03821">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have made many breakthroughs in computer vision
tasks. However, considerable redundancy arises in the spatial dimension of an
input image, leading to massive computational costs. Therefore, We propose a
coarse-to-fine vision transformer (CF-ViT) to relieve computational burden
while retaining performance in this paper. Our proposed CF-ViT is motivated by
two important observations in modern ViT models: (1) The coarse-grained patch
splitting can locate informative regions of an input image. (2) Most images can
be well recognized by a ViT model in a small-length token sequence. Therefore,
our CF-ViT implements network inference in a two-stage manner. At coarse
inference stage, an input image is split into a small-length patch sequence for
a computationally economical classification. If not well recognized, the
informative patches are identified and further re-split in a fine-grained
granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For
example, without any compromise on performance, CF-ViT reduces 53% FLOPs of
LV-ViT, and also achieves 2.01x throughput.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quasi-Balanced Self-Training on Noise-Aware Synthesis of Object Point Clouds for Closing Domain Gap. (arXiv:2203.03833v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03833">
<div class="article-summary-box-inner">
<span><p>Semantic analyses of object point clouds are largely driven by releasing of
benchmarking datasets, including synthetic ones whose instances are sampled
from object CAD models. However, learning from synthetic data may not
generalize to practical scenarios, where point clouds are typically incomplete,
non-uniformly distributed, and noisy. Such a challenge of Simulation-to-Reality
(Sim2Real) domain gap could be mitigated via learning algorithms of domain
adaptation; however, we argue that generation of synthetic point clouds via
more physically realistic rendering is a powerful alternative, as systematic
non-uniform noise patterns can be captured. To this end, we propose an
integrated scheme consisting of physically realistic synthesis of object point
clouds via rendering stereo images via projection of speckle patterns onto CAD
models and a novel quasi-balanced self-training designed for more balanced data
distribution by sparsity-driven selection of pseudo labeled samples for long
tailed classes. Experiment results can verify the effectiveness of our method
as well as both of its modules for unsupervised domain adaptation on point
cloud classification, achieving the state-of-the-art performance. Source codes
and the SpeckleNet synthetic dataset are available at
https://github.com/Gorilla-Lab-SCUT/QS3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer. (arXiv:2203.04099v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04099">
<div class="article-summary-box-inner">
<span><p>This paper presents an audio-visual approach for voice separation which
produces state-of-the-art results at a low latency in two scenarios: speech and
singing voice. The model is based on a two-stage network. Motion cues are
obtained with a lightweight graph convolutional network that processes face
landmarks. Then, both audio and motion features are fed to an audio-visual
transformer which produces a fairly good estimation of the isolated target
source. In a second stage, the predominant voice is enhanced with an audio-only
network. We present different ablation studies and comparison to
state-of-the-art methods. Finally, we explore the transferability of models
trained for speech separation in the task of singing voice separation. The
demos, code, and weights are available in https://ipcv.github.io/VoViT/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PETR: Position Embedding Transformation for Multi-View 3D Object Detection. (arXiv:2203.05625v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05625">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research. Code is
available at \url{https://github.com/megvii-research/PETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving. (arXiv:2203.07724v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07724">
<div class="article-summary-box-inner">
<span><p>Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning more than 30 object categories. On CODA,
the performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REALY: Rethinking the Evaluation of 3D Face Reconstruction. (arXiv:2203.09729v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09729">
<div class="article-summary-box-inner">
<span><p>The evaluation of 3D face reconstruction results typically relies on a rigid
shape alignment between the estimated 3D model and the ground-truth scan. We
observe that aligning two shapes with different reference points can largely
affect the evaluation results. This poses difficulties for precisely diagnosing
and improving a 3D face reconstruction method. In this paper, we propose a
novel evaluation approach with a new benchmark REALY, consists of 100 globally
aligned face scans with accurate facial keypoints, high-quality region masks,
and topology-consistent meshes. Our approach performs region-wise shape
alignment and leads to more accurate, bidirectional correspondences during
computing the shape errors. The fine-grained, region-wise evaluation results
provide us detailed understandings about the performance of state-of-the-art 3D
face reconstruction methods. For example, our experiments on single-image based
reconstruction methods reveal that DECA performs the best on nose regions,
while GANFit performs better on cheek regions. Besides, a new and high-quality
3DMM basis, HIFI3D++, is further derived using the same procedure as we
construct REALY to align and retopologize several 3D face datasets. We will
release REALY, HIFI3D++, and our new evaluation pipeline at
https://realy3dface.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARM: Any-Time Super-Resolution Method. (arXiv:2203.10812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10812">
<div class="article-summary-box-inner">
<span><p>This paper proposes an Any-time super-Resolution Method (ARM) to tackle the
over-parameterized single image super-resolution (SISR) models. Our ARM is
motivated by three observations: (1) The performance of different image patches
varies with SISR networks of different sizes. (2) There is a tradeoff between
computation overhead and performance of the reconstructed image. (3) Given an
input image, its edge information can be an effective option to estimate its
PSNR. Subsequently, we train an ARM supernet containing SISR subnets of
different sizes to deal with image patches of various complexity. To that
effect, we construct an Edge-to-PSNR lookup table that maps the edge score of
an image patch to the PSNR performance for each subnet, together with a set of
computation costs for the subnets. In the inference, the image patches are
individually distributed to different subnets for a better
computation-performance tradeoff. Moreover, each SISR subnet shares weights of
the ARM supernet, thus no extra parameters are introduced. The setting of
multiple subnets can well adapt the computational cost of SISR model to the
dynamically available hardware resources, allowing the SISR task to be in
service at any time. Extensive experiments on resolution datasets of different
sizes with popular SISR networks as backbones verify the effectiveness and the
versatility of our ARM. The source code is available at
https://github.com/chenbong/ARM-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark. (arXiv:2203.11089v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11089">
<div class="article-summary-box-inner">
<span><p>Methods for 3D lane detection have been recently proposed to address the
issue of inaccurate lane layouts in many autonomous driving scenarios
(uphill/downhill, bump, etc.). Previous work struggled in complex cases due to
their simple designs of the spatial transformation between front view and
bird's eye view (BEV) and the lack of a realistic dataset. Towards these
issues, we present PersFormer: an end-to-end monocular 3D lane detector with a
novel Transformer-based spatial feature transformation module. Our model
generates BEV features by attending to related front-view local regions with
camera parameters as a reference. PersFormer adopts a unified 2D/3D anchor
design and an auxiliary task to detect 2D/3D lanes simultaneously, enhancing
the feature consistency and sharing the benefits of multi-task learning.
Moreover, we release one of the first large-scale real-world 3D lane datasets:
OpenLane, with high-quality annotation and scenario diversity. OpenLane
contains 200,000 frames, over 880,000 instance-level lanes, 14 lane categories,
along with scene tags and the closed-in-path object annotations to encourage
the development of lane detection and more industrial-related autonomous
driving methods. We show that PersFormer significantly outperforms competitive
baselines in the 3D lane detection task on our new OpenLane dataset as well as
Apollo 3D Lane Synthetic dataset, and is also on par with state-of-the-art
algorithms in the 2D task on OpenLane. The project page is available at
https://github.com/OpenPerceptionX/PersFormer_3DLane and OpenLane dataset is
provided at https://github.com/OpenPerceptionX/OpenLane.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-based Learning of Parameterized Thermodynamics from Real-time Thermography. (arXiv:2203.13148v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13148">
<div class="article-summary-box-inner">
<span><p>Progress in automatic control of thermal processes and real-time estimation
of heat penetration into live tissue has long been limited by the difficulty of
obtaining high-fidelity thermodynamic models. Traditionally, in complex
thermodynamic systems, it is often infeasible to estimate the thermophysical
parameters of spatiotemporally varying processes, forcing the adoption of
model-free control architectures. This comes at the cost of losing any
robustness guarantees, and implies a need for extensive real-life testing. In
recent years, however, infrared cameras and other thermographic equipment have
become readily applicable to these processes, allowing for a real-time,
non-invasive means of sensing the thermal state of a process. In this work, we
present a novel physics-based approach to learning a thermal process's dynamics
directly from such real-time thermographic data, while focusing attention on
regions with high thermal activity. We call this process, which applies to any
higher-dimensional scalar field, attention-based noise robust averaging (ANRA).
Given a partial-differential equation model structure, we show that our
approach is robust against noise, and can be used to initialize optimization
routines to further refine parameter estimates. We demonstrate our method on
several simulation examples, as well as by applying it to electrosurgical
thermal response data on in vivo porcine skin tissue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Primitive-based Shape Abstraction via Nonparametric Bayesian Inference. (arXiv:2203.14714v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14714">
<div class="article-summary-box-inner">
<span><p>3D shape abstraction has drawn great interest over the years. Apart from
low-level representations such as meshes and voxels, researchers also seek to
semantically abstract complex objects with basic geometric primitives. Recent
deep learning methods rely heavily on datasets, with limited generality to
unseen categories. Furthermore, abstracting an object accurately yet with a
small number of primitives still remains a challenge. In this paper, we propose
a novel non-parametric Bayesian statistical method to infer an abstraction,
consisting of an unknown number of geometric primitives, from a point cloud. We
model the generation of points as observations sampled from an infinite mixture
of Gaussian Superquadric Taper Models (GSTM). Our approach formulates the
abstraction as a clustering problem, in which: 1) each point is assigned to a
cluster via the Chinese Restaurant Process (CRP); 2) a primitive representation
is optimized for each cluster, and 3) a merging post-process is incorporated to
provide a concise representation. We conduct extensive experiments on two
datasets. The results indicate that our method outperforms the state-of-the-art
in terms of accuracy and is generalizable to various types of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantized GAN for Complex Music Generation from Dance Videos. (arXiv:2204.00604v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00604">
<div class="article-summary-box-inner">
<span><p>We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal
framework that generates complex musical samples conditioned on dance videos.
Our proposed framework takes dance video frames and human body motions as
input, and learns to generate music samples that plausibly accompany the
corresponding input. Unlike most existing conditional music generation works
that generate specific types of mono-instrumental sounds using symbolic audio
representations (e.g., MIDI), and that usually rely on pre-defined musical
synthesizers, in this work we generate dance music in complex styles (e.g.,
pop, breaking, etc.) by employing a Vector Quantized (VQ) audio representation,
and leverage both its generality and high abstraction capacity of its symbolic
and continuous counterparts. By performing an extensive set of experiments on
multiple datasets, and following a comprehensive evaluation protocol, we assess
the generative qualities of our proposal against alternatives. The attained
quantitative results, which measure the music consistency, beats
correspondence, and music diversity, demonstrate the effectiveness of our
proposed method. Last but not least, we curate a challenging dance-music
dataset of in-the-wild TikTok videos, which we use to further demonstrate the
efficacy of our approach in real-world applications -- and which we hope to
serve as a starting point for relevant future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation. (arXiv:2204.02548v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02548">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the task of synthetic-to-real domain generalized
semantic segmentation, which aims to learn a model that is robust to unseen
real-world scenes using only synthetic data. The large domain shift between
synthetic and real-world data, including the limited source environmental
variations and the large distribution gap between synthetic and real-world
data, significantly hinders the model performance on unseen real-world scenes.
In this work, we propose the Style-HAllucinated Dual consistEncy learning
(SHADE) framework to handle such domain shift. Specifically, SHADE is
constructed based on two consistency constraints, Style Consistency (SC) and
Retrospection Consistency (RC). SC enriches the source situations and
encourages the model to learn consistent representation across
style-diversified samples. RC leverages real-world knowledge to prevent the
model from overfitting to synthetic data and thus largely keeps the
representation consistent between the synthetic and real-world models.
Furthermore, we present a novel style hallucination module (SHM) to generate
style-diversified samples that are essential to consistency learning. SHM
selects basis styles from the source distribution, enabling the model to
dynamically generate diverse and realistic samples during training. Experiments
show that our SHADE yields significant improvement and outperforms
state-of-the-art methods by 5.05% and 8.35% on the average mIoU of three
real-world datasets on single- and multi-source settings, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unidirectional Video Denoising by Mimicking Backward Recurrent Modules with Look-ahead Forward Ones. (arXiv:2204.05532v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05532">
<div class="article-summary-box-inner">
<span><p>While significant progress has been made in deep video denoising, it remains
very challenging for exploiting historical and future frames. Bidirectional
recurrent networks (BiRNN) have exhibited appealing performance in several
video restoration tasks. However, BiRNN is intrinsically offline because it
uses backward recurrent modules to propagate from the last to current frames,
which causes high latency and large memory consumption. To address the offline
issue of BiRNN, we present a novel recurrent network consisting of forward and
look-ahead recurrent modules for unidirectional video denoising. Particularly,
look-ahead module is an elaborate forward module for leveraging information
from near-future frames. When denoising the current frame, the hidden features
by forward and look-ahead recurrent modules are combined, thereby making it
feasible to exploit both historical and near-future frames. Due to the scene
motion between non-neighboring frames, border pixels missing may occur when
warping look-ahead feature from near-future frame to current frame, which can
be largely alleviated by incorporating forward warping and proposed border
enlargement. Experiments show that our method achieves state-of-the-art
performance with constant latency and memory consumption. Code is avaliable at
https://github.com/nagejacob/FloRNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08387">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training techniques have achieved remarkable progress in
Document AI. Most multimodal pre-trained models use a masked language modeling
objective to learn bidirectional representations on the text modality, but they
differ in pre-training objectives for the image modality. This discrepancy adds
difficulty to multimodal representation learning. In this paper, we propose
\textbf{LayoutLMv3} to pre-train multimodal Transformers for Document AI with
unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a
word-patch alignment objective to learn cross-modal alignment by predicting
whether the corresponding image patch of a text word is masked. The simple
unified architecture and training objectives make LayoutLMv3 a general-purpose
pre-trained model for both text-centric and image-centric Document AI tasks.
Experimental results show that LayoutLMv3 achieves state-of-the-art performance
not only in text-centric tasks, including form understanding, receipt
understanding, and document visual question answering, but also in
image-centric tasks such as document image classification and document layout
analysis. The code and models are publicly available at
\url{https://aka.ms/layoutlmv3}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIMO: Gaze-Informed Human Motion Prediction in Context. (arXiv:2204.09443v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09443">
<div class="article-summary-box-inner">
<span><p>Predicting human motion is critical for assistive robots and AR/VR
applications, where the interaction with humans needs to be safe and
comfortable. Meanwhile, an accurate prediction depends on understanding both
the scene context and human intentions. Even though many works study
scene-aware human motion prediction, the latter is largely underexplored due to
the lack of ego-centric views that disclose human intent and the limited
diversity in motion and scenes. To reduce the gap, we propose a large-scale
human motion dataset that delivers high-quality body pose sequences, scene
scans, as well as ego-centric views with the eye gaze that serves as a
surrogate for inferring human intent. By employing inertial sensors for motion
capture, our data collection is not tied to specific scenes, which further
boosts the motion dynamics observed from our subjects. We perform an extensive
study of the benefits of leveraging the eye gaze for ego-centric human motion
prediction with various state-of-the-art architectures. Moreover, to realize
the full potential of the gaze, we propose a novel network architecture that
enables bidirectional communication between the gaze and motion branches. Our
network achieves the top performance in human motion prediction on the proposed
dataset, thanks to the intent information from eye gaze and the denoised gaze
feature modulated by the motion. Code and data can be found at
https://github.com/y-zheng18/GIMO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR). (arXiv:2204.09815v2 [math.NA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09815">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the restoration and reconstruction of piecewise
constant objects in two and three dimensions using PaLEnTIR, a significantly
enhanced Parametric level set (PaLS) model relative to the current
state-of-the-art. The primary contribution of this paper is a new PaLS
formulation which requires only a single level set function to recover a scene
with piecewise constant objects possessing multiple unknown contrasts. Our
model offers distinct advantages over current approaches to the multi-contrast,
multi-object problem, all of which require multiple level sets and explicit
estimation of the contrast magnitudes. Given upper and lower bounds on the
contrast, our approach is able to recover objects with any distribution of
contrasts and eliminates the need to know either the number of contrasts in a
given scene or their values. We provide an iterative process for finding these
space-varying contrast limits. Relative to most PaLS methods which employ
radial basis functions (RBFs), our model makes use of non-isotropic basis
functions, thereby expanding the class of shapes that a PaLS model of a given
complexity can approximate. Finally, PaLEnTIR improves the conditioning of the
Jacobian matrix required as part of the parameter identification process and
consequently accelerates the optimization methods by controlling the magnitude
of the PaLS expansion coefficients, fixing the centers of the basis functions,
and the uniqueness of parametric to image mappings provided by the new
parameterization. We demonstrate the performance of the new approach using both
2D and 3D variants of X-ray computed tomography, diffuse optical tomography
(DOT), denoising, deconvolution problems. Application to experimental sparse CT
data and simulated data with different types of noise are performed to further
validate the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMRotate: A Rotated Object Detection Benchmark using PyTorch. (arXiv:2204.13317v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13317">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Color Transfer and Style Transfer for Images and Videos. (arXiv:2204.13339v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13339">
<div class="article-summary-box-inner">
<span><p>Image or video appearance features (e.g., color, texture, tone, illumination,
and so on) reflect one's visual perception and direct impression of an image or
video. Given a source image (video) and a target image (video), the image
(video) color transfer technique aims to process the color of the source image
or video (note that the source image or video is also referred to the reference
image or video in some literature) to make it look like that of the target
image or video, i.e., transferring the appearance of the target image or video
to that of the source image or video, which can thereby change one's perception
of the source image or video. As an extension of color transfer, style transfer
refers to rendering the content of a target image or video in the style of an
artist with either a style sample or a set of images through a style transfer
model. As an emerging field, the study of style transfer has attracted the
attention of a large number of researchers. After decades of development, it
has become a highly interdisciplinary research with a variety of artistic
expression styles can be achieved. This paper provides an overview of color
transfer and style transfer methods over the past years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Label Correction is a Good Booster When Learning with Extremely Noisy Labels. (arXiv:2205.00186v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00186">
<div class="article-summary-box-inner">
<span><p>Learning with noisy labels has aroused much research interest since data
annotations, especially for large-scale datasets, may be inevitably imperfect.
Recent approaches resort to a semi-supervised learning problem by dividing
training samples into clean and noisy sets. This paradigm, however, is prone to
significant degeneration under heavy label noise, as the number of clean
samples is too small for conventional methods to behave well. In this paper, we
introduce a novel framework, termed as LC-Booster, to explicitly tackle
learning under extreme noise. The core idea of LC-Booster is to incorporate
label correction into the sample selection, so that more purified samples,
through the reliable label correction, can be utilized for training, thereby
alleviating the confirmation bias. Experiments show that LC-Booster advances
state-of-the-art results on several noisy-label benchmarks, including CIFAR-10,
CIFAR-100, Clothing1M and WebVision. Remarkably, under the extreme 90\% noise
ratio, LC-Booster achieves 92.9\% and 48.4\% accuracy on CIFAR-10 and
CIFAR-100, surpassing state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Object Detection with a Self-supervised Lidar Scene Flow Backbone. (arXiv:2205.00705v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00705">
<div class="article-summary-box-inner">
<span><p>State-of-the-art lidar-based 3D object detection methods rely on supervised
learning and large labeled datasets. However, annotating lidar data is
resource-consuming, and depending only on supervised learning limits the
applicability of trained models. Self-supervised training strategies can
alleviate these issues by learning a general point cloud backbone model for
downstream 3D vision tasks. Against this backdrop, we show the relationship
between self-supervised multi-frame flow representations and single-frame 3D
detection hypotheses. Our main contribution leverages learned flow and motion
representations and combines a self-supervised backbone with a supervised 3D
detection head. First, a self-supervised scene flow estimation model is trained
with cycle consistency. Then, the point cloud encoder of this model is used as
the backbone of a single-frame 3D object detection head model. This second 3D
object detection model learns to utilize motion representations to distinguish
dynamic objects exhibiting different movement patterns. Experiments on KITTI
and nuScenes benchmarks show that the proposed self-supervised pre-training
increases 3D detection performance significantly.
https://github.com/emecercelik/ssl-3d-detection.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Balancing for Domain Generalization. (arXiv:2206.05263v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05263">
<div class="article-summary-box-inner">
<span><p>While machine learning models rapidly advance the state-of-the-art on various
real-world tasks, out-of-domain (OOD) generalization remains a challenging
problem given the vulnerability of these models to spurious correlations. We
propose a causally-motivated balanced mini-batch sampling strategy to transform
the observed train distribution to a balanced distribution that is free of
spurious correlations. We argue that the Bayes optimal classifier trained on
such balanced distribution is minimax optimal across a diverse enough
environment space. We also provide an identifiability guarantee of the latent
variable model of the proposed underlying data generation process with
invariant causal mechanisms, by utilizing enough number of train environments.
Experiments are conducted on three domain generalization datasets,
demonstrating empirically that our balanced mini-batch sampling strategy
improves the performance of four different established domain generalization
model baselines compared to the random mini-batch sampling strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07990">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised learning (SSL) methods have shown impressive results
in learning visual representations from unlabeled images. This paper aims to
improve their performance further by utilizing the architectural advantages of
the underlying neural network, as the current state-of-the-art visual pretext
tasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.
In particular, we focus on Vision Transformers (ViTs), which have gained much
attention recently as a better architectural choice, often outperforming
convolutional networks for various visual tasks. The unique characteristic of
ViT is that it takes a sequence of disjoint patches from an image and processes
patch-level representations internally. Inspired by this, we design a simple
yet effective visual pretext task, coined SelfPatch, for learning better
patch-level representations. To be specific, we enforce invariance against each
patch and its neighbors, i.e., each patch treats similar neighboring patches as
positive samples. Consequently, training ViTs with SelfPatch learns more
semantically meaningful relations among patches (without using human-annotated
labels), which can be beneficial, in particular, to downstream tasks of a dense
prediction type. Despite its simplicity, we demonstrate that it can
significantly improve the performance of existing SSL methods for various
visual tasks, including object detection and semantic segmentation.
Specifically, SelfPatch significantly improves the recent self-supervised ViT,
DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance
segmentation, and +2.9 mIoU on ADE20K semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology. (arXiv:2207.00095v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00095">
<div class="article-summary-box-inner">
<span><p>Current approaches for classification of whole slide images (WSI) in digital
pathology predominantly utilize a two-stage learning pipeline. The first stage
identifies areas of interest (e.g. tumor tissue), while the second stage
processes cropped tiles from these areas in a supervised fashion. During
inference, a large number of tiles are combined into a unified prediction for
the entire slide. A major drawback of such approaches is the requirement for
task-specific auxiliary labels which are not acquired in clinical routine. We
propose a novel learning pipeline for WSI classification that is trainable
end-to-end and does not require any auxiliary annotations. We apply our
approach to predict molecular alterations for a number of different use-cases,
including detection of microsatellite instability in colorectal tumors and
prediction of specific mutations for colon, lung, and breast cancer cases from
The Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to
be competitive with state of the art two-stage pipelines. We believe our
approach can facilitate future research in digital pathology and contribute to
solve a large range of problems around the prediction of cancer phenotypes,
hopefully enabling personalized therapies for more patients in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaTeRF: Label and Text Driven Object Radiance Fields. (arXiv:2207.01583v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01583">
<div class="article-summary-box-inner">
<span><p>Obtaining 3D object representations is important for creating photo-realistic
simulations and for collecting AR and VR assets. Neural fields have shown their
effectiveness in learning a continuous volumetric representation of a scene
from 2D images, but acquiring object representations from these models with
weak supervision remains an open challenge. In this paper we introduce LaTeRF,
a method for extracting an object of interest from a scene given 2D images of
the entire scene, known camera poses, a natural language description of the
object, and a set of point-labels of object and non-object points in the input
images. To faithfully extract the object from the scene, LaTeRF extends the
NeRF formulation with an additional `objectness' probability at each 3D point.
Additionally, we leverage the rich latent space of a pre-trained CLIP model
combined with our differentiable object renderer, to inpaint the occluded parts
of the object. We demonstrate high-fidelity object extraction on both synthetic
and real-world datasets and justify our design choices through an extensive
ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling. (arXiv:2207.02196v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02196">
<div class="article-summary-box-inner">
<span><p>Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. However, a fundamental limitation is that their
inference is very slow due to a need for many (e.g., 2000) iterations of
sequential computations. An intuitive acceleration method is to reduce the
sampling iterations which however causes severe performance degradation. We
investigate this problem by viewing the diffusion sampling process as a
Metropolis adjusted Langevin algorithm, which helps reveal the underlying cause
to be ill-conditioned curvature. Under this insight, we propose a
model-agnostic preconditioned diffusion sampling (PDS) method that leverages
matrix preconditioning to alleviate the aforementioned problem. Crucially, PDS
is proven theoretically to converge to the original target distribution of a
SGM, no need for retraining. Extensive experiments on three image datasets with
a variety of resolutions and diversity validate that PDS consistently
accelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In
particular, PDS can accelerate by up to 29x on more challenging high resolution
(1024x1024) image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection. (arXiv:2207.02541v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02541">
<div class="article-summary-box-inner">
<span><p>To date, the most powerful semi-supervised object detectors (SS-OD) are based
on pseudo-boxes, which need a sequence of post-processing with fine-tuned
hyper-parameters. In this work, we propose replacing the sparse pseudo-boxes
with the dense prediction as a united and straightforward form of pseudo-label.
Compared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any
post-processing method, thus retaining richer information. We also introduce a
region selection technique to highlight the key information while suppressing
the noise carried by dense labels. We name our proposed SS-OD algorithm that
leverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows
superior performance under various settings compared with the pseudo-box-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer. (arXiv:2207.04808v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04808">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to devise a universally versatile style transfer method
capable of performing artistic, photo-realistic, and video style transfer
jointly, without seeing videos during training. Previous single-frame methods
assume a strong constraint on the whole image to maintain temporal consistency,
which could be violated in many cases. Instead, we make a mild and reasonable
assumption that global inconsistency is dominated by local inconsistencies and
devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local
patches. CCPL can preserve the coherence of the content source during style
transfer without degrading stylization. Moreover, it owns a neighbor-regulating
mechanism, resulting in a vast reduction of local distortions and considerable
visual quality improvement. Aside from its superior performance on versatile
style transfer, it can be easily extended to other tasks, such as
image-to-image translation. Besides, to better fuse content and style features,
we propose Simple Covariance Transformation (SCT) to effectively align
second-order statistics of the content feature with the style feature.
Experiments demonstrate the effectiveness of the resulting model for versatile
style transfer, when armed with CCPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compound Prototype Matching for Few-shot Action Recognition. (arXiv:2207.05515v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05515">
<div class="article-summary-box-inner">
<span><p>Few-shot action recognition aims to recognize novel action classes using only
a small number of labeled training samples. In this work, we propose a novel
approach that first summarizes each video into compound prototypes consisting
of a group of global prototypes and a group of focused prototypes, and then
compares video similarity based on the prototypes. Each global prototype is
encouraged to summarize a specific aspect from the entire video, for example,
the start/evolution of the action. Since no clear annotation is provided for
the global prototypes, we use a group of focused prototypes to focus on certain
timestamps in the video. We compare video similarity by matching the compound
prototypes between the support and query videos. The global prototypes are
directly matched to compare videos from the same perspective, for example, to
compare whether two actions start similarly. For the focused prototypes, since
actions have various temporal variations in the videos, we apply bipartite
matching to allow the comparison of actions with different temporal positions
and shifts. Experiments demonstrate that our proposed method achieves
state-of-the-art results on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure PLP-SLAM: Efficient Sparse Mapping and Localization using Point, Line and Plane for Monocular, RGB-D and Stereo Cameras. (arXiv:2207.06058v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06058">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates a visual SLAM system that utilizes point and line
cloud for robust camera localization, simultaneously, with an embedded
piece-wise planar reconstruction (PPR) module which in all provides a
structural map. To build a scale consistent map in parallel with tracking, such
as employing a single camera brings the challenge of reconstructing geometric
primitives with scale ambiguity, and further introduces the difficulty in graph
optimization of bundle adjustment (BA). We address these problems by proposing
several run-time optimizations on the reconstructed lines and planes. The
system is then extended with depth and stereo sensors based on the design of
the monocular framework. The results show that our proposed SLAM tightly
incorporates the semantic features to boost both frontend tracking as well as
backend optimization. We evaluate our system exhaustively on various datasets,
and open-source our code for the community
(https://github.com/PeterFWS/Structure-PLP-SLAM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07316">
<div class="article-summary-box-inner">
<span><p>Face recognition technology has been used in many fields due to its high
recognition accuracy, including the face unlocking of mobile devices, community
access control systems, and city surveillance. As the current high accuracy is
guaranteed by very deep network structures, facial images often need to be
transmitted to third-party servers with high computational power for inference.
However, facial images visually reveal the user's identity information. In this
process, both untrusted service providers and malicious users can significantly
increase the risk of a personal privacy breach. Current privacy-preserving
approaches to face recognition are often accompanied by many side effects, such
as a significant increase in inference time or a noticeable decrease in
recognition accuracy. This paper proposes a privacy-preserving face recognition
method using differential privacy in the frequency domain. Due to the
utilization of differential privacy, it offers a guarantee of privacy in
theory. Meanwhile, the loss of accuracy is very slight. This method first
converts the original image to the frequency domain and removes the direct
component termed DC. Then a privacy budget allocation method can be learned
based on the loss of the back-end face recognition network within the
differential privacy framework. Finally, it adds the corresponding noise to the
frequency domain features. Our method performs very well with several classical
face recognition test sets according to the extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection. (arXiv:2207.07783v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07783">
<div class="article-summary-box-inner">
<span><p>Active speaker detection (ASD) in videos with multiple speakers is a
challenging task as it requires learning effective audiovisual features and
spatial-temporal correlations over long temporal windows. In this paper, we
present SPELL, a novel spatial-temporal graph learning framework that can solve
complex tasks such as ASD. To this end, each person in a video frame is first
encoded in a unique node for that frame. Nodes corresponding to a single person
across frames are connected to encode their temporal dynamics. Nodes within a
frame are also connected to encode inter-person relationships. Thus, SPELL
reduces ASD to a node classification task. Importantly, SPELL is able to reason
over long temporal contexts for all nodes without relying on computationally
expensive fully connected graph neural networks. Through extensive experiments
on the AVA-ActiveSpeaker dataset, we demonstrate that learning graph-based
representations can significantly improve the active speaker detection
performance owing to its explicit spatial and temporal structure. SPELL
outperforms all previous state-of-the-art approaches while requiring
significantly lower memory and computational resources. Our code is publicly
available at https://github.com/SRA2/SPELL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCRN: Real-world Character Image Restoration Network via Skeleton Extraction. (arXiv:2207.07795v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07795">
<div class="article-summary-box-inner">
<span><p>Constructing high-quality character image datasets is challenging because
real-world images are often affected by image degradation. There are
limitations when applying current image restoration methods to such real-world
character images, since (i) the categories of noise in character images are
different from those in general images; (ii) real-world character images
usually contain more complex image degradation, e.g., mixed noise at different
noise levels. To address these problems, we propose a real-world character
restoration network (RCRN) to effectively restore degraded character images,
where character skeleton information and scale-ensemble feature extraction are
utilized to obtain better restoration performance. The proposed method consists
of a skeleton extractor (SENet) and a character image restorer (CiRNet). SENet
aims to preserve the structural consistency of the character and normalize
complex noise. Then, CiRNet reconstructs clean images from degraded character
images and their skeletons. Due to the lack of benchmarks for real-world
character image restoration, we constructed a dataset containing 1,606
character images with real-world degradation to evaluate the validity of the
proposed method. The experimental results demonstrate that RCRN outperforms
state-of-the-art methods quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising. (arXiv:2207.07798v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07798">
<div class="article-summary-box-inner">
<span><p>Degraded images commonly exist in the general sources of character images,
leading to unsatisfactory character recognition results. Existing methods have
dedicated efforts to restoring degraded character images. However, the
denoising results obtained by these methods do not appear to improve character
recognition performance. This is mainly because current methods only focus on
pixel-level information and ignore critical features of a character, such as
its glyph, resulting in character-glyph damage during the denoising process. In
this paper, we introduce a novel generic framework based on glyph fusion and
attention mechanisms, i.e., CharFormer, for precisely recovering character
images without changing their inherent glyphs. Unlike existing frameworks,
CharFormer introduces a parallel target task for capturing additional
information and injecting it into the image denoising backbone, which will
maintain the consistency of character glyphs during character image denoising.
Moreover, we utilize attention-based networks for global-local feature
interaction, which will help to deal with blind denoising and enhance denoising
performance. We compare CharFormer with state-of-the-art methods on multiple
datasets. The experimental results show the superiority of CharFormer
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement. (arXiv:2207.07828v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07828">
<div class="article-summary-box-inner">
<span><p>We propose an effective Structural Prior guided Generative Adversarial
Transformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly
contains a generator with two discriminators and a structural prior estimator
(SPE). The generator is based on a U-shaped Transformer which is used to
explore non-local information for better clear image restoration. The SPE is
used to explore useful structures from images to guide the generator for better
structural detail estimation. To generate more realistic images, we develop a
new structural prior guided adversarial learning method by building the skip
connections between the generator and discriminators so that the discriminators
can better discriminate between real and fake features. Finally, we propose a
parallel windows-based Swin Transformer block to aggregate different level
hierarchical features for high-quality image restoration. Experimental results
demonstrate that the proposed SPGAT performs favorably against recent
state-of-the-art methods on both synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches. (arXiv:2207.08220v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08220">
<div class="article-summary-box-inner">
<span><p>Contrastive-based self-supervised learning methods achieved great success in
recent years. However, self-supervision requires extremely long training epochs
(e.g., 800 epochs for MoCo v3) to achieve promising results, which is
unacceptable for the general academic community and hinders the development of
this topic. This work revisits the momentum-based contrastive learning
frameworks and identifies the inefficiency in which two augmented views
generate only one positive pair. We propose Fast-MoCo - a novel framework that
utilizes combinatorial patches to construct multiple positive pairs from two
augmented views, which provides abundant supervision signals that bring
significant acceleration with neglectable extra computational cost. Fast-MoCo
trained with 100 epochs achieves 73.5% linear evaluation accuracy, similar to
MoCo v3 (ResNet-50 backbone) trained with 800 epochs. Extra training (200
epochs) further improves the result to 75.1%, which is on par with
state-of-the-art methods. Experiments on several downstream tasks also confirm
the effectiveness of Fast-MoCo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Show Me What I Like: Detecting User-Specific Video Highlights Using Content-Based Multi-Head Attention. (arXiv:2207.08352v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08352">
<div class="article-summary-box-inner">
<span><p>We propose a method to detect individualized highlights for users on given
target videos based on their preferred highlight clips marked on previous
videos they have watched. Our method explicitly leverages the contents of both
the preferred clips and the target videos using pre-trained features for the
objects and the human activities. We design a multi-head attention mechanism to
adaptively weigh the preferred clips based on their object- and
human-activity-based contents, and fuse them using these weights into a single
feature representation for each user. We compute similarities between these
per-user feature representations and the per-frame features computed from the
desired target videos to estimate the user-specific highlight clips from the
target videos. We test our method on a large-scale highlight detection dataset
containing the annotated highlights of individual users. Compared to current
baselines, we observe an absolute improvement of 2-4% in the mean average
precision of the detected highlights. We also perform extensive ablation
experiments on the number of preferred highlight clips associated with each
user as well as on the object- and human-activity-based feature representations
to validate that our method is indeed both content-based and user-specific.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-world Semantic Segmentation via Contrasting and Clustering Vision-Language Embedding. (arXiv:2207.08455v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08455">
<div class="article-summary-box-inner">
<span><p>To bridge the gap between supervised semantic segmentation and real-world
applications that acquires one model to recognize arbitrary new concepts,
recent zero-shot segmentation attracts a lot of attention by exploring the
relationships between unseen and seen object categories, yet requiring large
amounts of densely-annotated data with diverse base classes. In this paper, we
propose a new open-world semantic segmentation pipeline that makes the first
attempt to learn to segment semantic objects of various open-world categories
without any efforts on dense annotations, by purely exploiting the
image-caption data that naturally exist on the Internet. Our method,
Vision-language-driven Semantic Segmentation (ViL-Seg), employs an image and a
text encoder to generate visual and text embeddings for the image-caption data,
with two core components that endow its segmentation ability: First, the image
encoder is jointly trained with a vision-based contrasting and a cross-modal
contrasting, which encourage the visual embeddings to preserve both
fine-grained semantics and high-level category information that are crucial for
the segmentation task. Furthermore, an online clustering head is devised over
the image encoder, which allows to dynamically segment the visual embeddings
into distinct semantic groups such that they can be classified by comparing
with various text embeddings to complete our segmentation pipeline. Experiments
show that without using any data with dense annotations, our method can
directly segment objects of arbitrary categories, outperforming zero-shot
segmentation methods that require data labeling on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmenting white matter hyperintensities on isotropic three-dimensional Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison of Deep learning tools on a Norwegian national imaging database. (arXiv:2207.08467v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08467">
<div class="article-summary-box-inner">
<span><p>Automated segmentation of white matter hyperintensities (WMHs) is an
essential step in neuroimaging analysis of Magnetic Resonance Imaging (MRI).
Fluid Attenuated Inversion Recovery (FLAIR-weighted) is an MRI contrast that is
particularly useful to visualize and quantify WMHs, a hallmark of cerebral
small vessel disease and Alzheimer's disease (AD). Clinical MRI protocols
migrate to a three-dimensional (3D) FLAIR-weighted acquisition to enable high
spatial resolution in all three voxel dimensions. The current study details the
deployment of deep learning tools to enable automated WMH segmentation and
characterization from 3D FLAIR-weighted images acquired as part of a national
AD imaging initiative.
</p>
<p>Among 642 participants (283 male, mean age: (65.18 +/- 9.33) years) from the
DDI study, two in-house networks were trained and validated across five
national collection sites. Three models were tested on a held-out subset of the
internal data from the 642 participants and an external dataset with 29 cases
from an international collaborator. These test sets were evaluated
independently. Five established WMH performance metrics were used for
comparison against ground truth human-in-the-loop segmentation.
</p>
<p>Results of the three networks tested, the 3D nnU-Net had the best performance
with an average dice similarity coefficient score of 0.78 +/- 0.10, performing
better than both the in-house developed 2.5D model and the SOTA Deep Bayesian
network.
</p>
<p>With the increasing use of 3D FLAIR-weighted images in MRI protocols, our
results suggest that WMH segmentation models can be trained on 3D data and
yield WMH segmentation performance that is comparable to or better than
state-of-the-art without the need for including T1-weighted image series.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Feature Alignment Network for Unsupervised Video Object Segmentation. (arXiv:2207.08485v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08485">
<div class="article-summary-box-inner">
<span><p>Optical flow is an easily conceived and precious cue for advancing
unsupervised video object segmentation (UVOS). Most of the previous methods
directly extract and fuse the motion and appearance features for segmenting
target objects in the UVOS setting. However, optical flow is intrinsically an
instantaneous velocity of all pixels among consecutive frames, thus making the
motion features not aligned well with the primary objects among the
corresponding frames. To solve the above challenge, we propose a concise,
practical, and efficient architecture for appearance and motion feature
alignment, dubbed hierarchical feature alignment network (HFAN). Specifically,
the key merits in HFAN are the sequential Feature AlignMent (FAM) module and
the Feature AdaptaTion (FAT) module, which are leveraged for processing the
appearance and motion features hierarchically. FAM is capable of aligning both
appearance and motion features with the primary object semantic
representations, respectively. Further, FAT is explicitly designed for the
adaptive fusion of appearance and motion features to achieve a desirable
trade-off between cross-modal features. Extensive experiments demonstrate the
effectiveness of the proposed HFAN, which reaches a new state-of-the-art
performance on DAVIS-16, achieving 88.7 $\mathcal{J}\&amp;\mathcal{F}$ Mean, i.e.,
a relative improvement of 3.5% over the best published result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Study of the performance and scalability of federated learning for medical imaging with intermittent clients. (arXiv:2207.08581v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08581">
<div class="article-summary-box-inner">
<span><p>Federated learning is a data decentralization privacy-preserving technique
used to perform machine or deep learning in a secure way. In this paper we
present theoretical aspects about federated learning, such as the presentation
of an aggregation operator, different types of federated learning, and issues
to be taken into account in relation to the distribution of data from the
clients, together with the exhaustive analysis of a use case where the number
of clients varies. Specifically, a use case of medical image analysis is
proposed, using chest X-ray images obtained from an open data repository. In
addition to the advantages related to privacy, improvements in predictions (in
terms of accuracy and area under the curve) and reduction of execution times
will be studied with respect to the classical case (the centralized approach).
Different clients will be simulated from the training data, selected in an
unbalanced manner, i.e., they do not all have the same number of data. The
results of considering three or ten clients are exposed and compared between
them and against the centralized case. Two approaches to follow will be
analyzed in the case of intermittent clients, as in a real scenario some
clients may leave the training, and some new ones may enter the training. The
evolution of the results for the test set in terms of accuracy, area under the
curve and execution time is shown as the number of clients into which the
original data is divided increases. Finally, improvements and future work in
the field are proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FakeCLR: Exploring Contrastive Learning for Solving Latent Discontinuity in Data-Efficient GANs. (arXiv:2207.08630v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08630">
<div class="article-summary-box-inner">
<span><p>Data-Efficient GANs (DE-GANs), which aim to learn generative models with a
limited amount of training data, encounter several challenges for generating
high-quality samples. Since data augmentation strategies have largely
alleviated the training instability, how to further improve the generative
performance of DE-GANs becomes a hotspot. Recently, contrastive learning has
shown the great potential of increasing the synthesis quality of DE-GANs, yet
related principles are not well explored. In this paper, we revisit and compare
different contrastive learning strategies in DE-GANs, and identify (i) the
current bottleneck of generative performance is the discontinuity of latent
space; (ii) compared to other contrastive learning strategies,
Instance-perturbation works towards latent space continuity, which brings the
major improvement to DE-GANs. Based on these observations, we propose FakeCLR,
which only applies contrastive learning on perturbed fake samples, and devises
three related training techniques: Noise-related Latent Augmentation,
Diversity-aware Queue, and Forgetting Factor of Queue. Our experimental results
manifest the new state of the arts on both few-shot generation and limited-data
generation. On multiple datasets, FakeCLR acquires more than 15% FID
improvement compared to existing DE-GANs. Code is available at
https://github.com/iceli1007/FakeCLR.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-20 23:09:03.551894050 UTC">2022-07-20 23:09:03 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>