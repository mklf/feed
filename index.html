<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-07T01:30:00Z">07-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia. (arXiv:2207.02253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02253">
<div class="article-summary-box-inner">
<span><p>While neural networks demonstrate a remarkable ability to model linguistic
content, capturing contextual information related to a speaker's conversational
role is an open area of research. In this work, we analyze the effect of
speaker role on language use through the game of Mafia, in which participants
are assigned either an honest or a deceptive role. In addition to building a
framework to collect a dataset of Mafia game records, we demonstrate that there
are differences in the language produced by players with different roles. We
confirm that classification models are able to rank deceptive players as more
suspicious than honest ones based only on their use of language. Furthermore,
we show that training models on two auxiliary tasks outperforms a standard
BERT-based text classification approach. We also present methods for using our
trained models to identify features that distinguish between player roles,
which could be used to assist players during the Mafia game.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control. (arXiv:2207.02263v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02263">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization systems leveraging pre-training language models
have achieved superior results on benchmark datasets. However, such models have
been shown to be more prone to hallucinate facts that are unfaithful to the
input context. In this paper, we propose a method to remedy entity-level
extrinsic hallucinations with Entity Coverage Control (ECC). We first compute
entity coverage precision and prepend the corresponding control code for each
training example, which implicitly guides the model to recognize faithfulness
contents in the training phase. We further extend our method via intermediate
fine-tuning on large but noisy data extracted from Wikipedia to unlock
zero-shot summarization. We show that the proposed method leads to more
faithful and salient abstractive summarization in supervised fine-tuning and
zero-shot settings according to our experimental results on three benchmark
datasets XSum, Pubmed, and SAMSum of very different domains and styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining on Interactions for Learning Grounded Affordance Representations. (arXiv:2207.02272v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02272">
<div class="article-summary-box-inner">
<span><p>Lexical semantics and cognitive science point to affordances (i.e. the
actions that objects support) as critical for understanding and representing
nouns and verbs. However, study of these semantic features has not yet been
integrated with the "foundation" models that currently dominate language
representation research. We hypothesize that predictive modeling of object
state over time will result in representations that encode object affordance
information "for free". We train a neural network to predict objects'
trajectories in a simulated interaction and show that our network's latent
representations differentiate between both observed and unobserved affordances.
We find that models trained using 3D simulations from our SPATIAL dataset
outperform conventional 2D computer vision models trained on a similar task,
and, on initial inspection, that differences between concepts correspond to
expected features (e.g., roll entails rotation). Our results suggest a way in
which modern deep learning approaches to grounded language learning can be
integrated with traditional formal semantic notions of lexical representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Cross-Linguistic Learning of Event Semantics. (arXiv:2207.02356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02356">
<div class="article-summary-box-inner">
<span><p>Typologically diverse languages offer systems of lexical and grammatical
aspect that allow speakers to focus on facets of event structure in ways that
comport with the specific communicative setting and discourse constraints they
face. In this paper, we look specifically at captions of images across Arabic,
Chinese, Farsi, German, Russian, and Turkish and describe a computational model
for predicting lexical aspects. Despite the heterogeneity of these languages,
and the salient invocation of distinctive linguistic resources across their
caption corpora, speakers of these languages show surprising similarities in
the ways they frame image content. We leverage this observation for zero-shot
cross-lingual learning and show that lexical aspects can be predicted for a
given language despite not having observed any annotated data for this language
at all.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compute Cost Amortized Transformer for Streaming ASR. (arXiv:2207.02393v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02393">
<div class="article-summary-box-inner">
<span><p>We present a streaming, Transformer-based end-to-end automatic speech
recognition (ASR) architecture which achieves efficient neural inference
through compute cost amortization. Our architecture creates sparse computation
pathways dynamically at inference time, resulting in selective use of compute
resources throughout decoding, enabling significant reductions in compute with
minimal impact on accuracy. The fully differentiable architecture is trained
end-to-end with an accompanying lightweight arbitrator mechanism operating at
the frame-level to make dynamic decisions on each input while a tunable loss
function is used to regularize the overall level of compute against predictive
performance. We report empirical results from experiments using the compute
amortized Transformer-Transducer (T-T) model conducted on LibriSpeech data. Our
best model can achieve a 60% compute cost reduction with only a 3% relative
word error rate (WER) increase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioTABQA: Instruction Learning for Biomedical Table Question Answering. (arXiv:2207.02419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02419">
<div class="article-summary-box-inner">
<span><p>Table Question Answering (TQA) is an important but under-explored task. Most
of the existing QA datasets are in unstructured text format and only few of
them use tables as the context. To the best of our knowledge, none of TQA
datasets exist in the biomedical domain where tables are frequently used to
present information. In this paper, we first curate a table question answering
dataset, BioTABQA, using 22 templates and the context from a biomedical
textbook on differential diagnosis. BioTABQA can not only be used to teach a
model how to answer questions from tables but also evaluate how a model
generalizes to unseen questions, an important scenario for biomedical
applications. To achieve the generalization evaluation, we divide the templates
into 17 training and 5 cross-task evaluations. Then, we develop two baselines
using single and multi-tasks learning on BioTABQA. Furthermore, we explore
instructional learning, a recent technique showing impressive generalizing
performance. Experimental results show that our instruction-tuned model
outperforms single and multi-task baselines on an average by ~23% and ~6%
across various evaluation settings, and more importantly, instruction-tuned
model outperforms baselines by ~5% on cross-tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa. (arXiv:2207.02424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02424">
<div class="article-summary-box-inner">
<span><p>Text sentiment analysis, also known as opinion mining, is research on the
calculation of people's views, evaluations, attitude and emotions expressed by
entities. Text sentiment analysis can be divided into text-level sentiment
analysis, sen-tence-level sentiment analysis and aspect-level sentiment
analysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the
field of sentiment analysis, which aims to predict the polarity of aspects. The
research of pre-training neural model has significantly improved the
performance of many natural language processing tasks. In recent years, pre
training model (PTM) has been applied in ABSA. Therefore, there has been a
question, which is whether PTMs contain sufficient syntactic information for
ABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced
BERT with disentangled attention) to solve Aspect-Based Sentiment Analysis
problem. DeBERTa is a kind of neural language model based on transformer, which
uses self-supervised learning to pre-train on a large number of original text
corpora. Based on the Local Context Focus (LCF) mechanism, by integrating
DeBERTa model, we purpose a multi-task learning model for aspect-based
sentiment analysis. The experiments result on the most commonly used the laptop
and restaurant datasets of SemEval-2014 and the ACL twitter dataset show that
LCF mechanism with DeBERTa has significant improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EEPT: Early Discovery of Emerging Entities in Twitter with Semantic Similarity. (arXiv:2207.02434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02434">
<div class="article-summary-box-inner">
<span><p>Some events which happen in the future could be important for companies,
governments, and even our personal life. Prediction of these events before
their establishment is helpful for efficient decision-making. We call such
events emerging entities. They have not taken place yet, and there is no
information about them in KB. However, some clues exist in different areas,
especially on social media. Thus, retrieving these type of entities are
possible. This paper proposes a method of early discovery of emerging entities.
We use semantic clustering of short messages. To evaluate the performance of
our proposal, we devise and utilize a performance evaluation metric. The
results show that our proposed method finds those emerging entities of which
Twitter trends are not always capable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brain-inspired probabilistic generative model for double articulation analysis of spoken language. (arXiv:2207.02457v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02457">
<div class="article-summary-box-inner">
<span><p>The human brain, among its several functions, analyzes the double
articulation structure in spoken language, i.e., double articulation analysis
(DAA). A hierarchical structure in which words are connected to form a sentence
and words are composed of phonemes or syllables is called a double articulation
structure. Where and how DAA is performed in the human brain has not been
established, although some insights have been obtained. In addition, existing
computational models based on a probabilistic generative model (PGM) do not
incorporate neuroscientific findings, and their consistency with the brain has
not been previously discussed. This study compared, mapped, and integrated
these existing computational models with neuroscientific findings to bridge
this gap, and the findings are relevant for future applications and further
research. This study proposes a PGM for a DAA hypothesis that can be realized
in the brain based on the outcomes of several neuroscientific surveys. The
study involved (i) investigation and organization of anatomical structures
related to spoken language processing, and (ii) design of a PGM that matches
the anatomy and functions of the region of interest. Therefore, this study
provides novel insights that will be foundational to further exploring DAA in
the brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning. (arXiv:2207.02463v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02463">
<div class="article-summary-box-inner">
<span><p>Language model debiasing has emerged as an important field of study in the
NLP community. Numerous debiasing techniques were proposed, but bias ablation
remains an unaddressed issue. We demonstrate a novel framework for inspecting
bias in pre-trained transformer-based language models via movement pruning.
Given a model and a debiasing objective, our framework finds a subset of the
model containing less bias than the original model. We implement our framework
by pruning the model while fine-tuning it on the debiasing objective. Optimized
are only the pruning scores - parameters coupled with the model's weights that
act as gates. We experiment with pruning attention heads, an important building
block of transformers: we prune square blocks, as well as establish a new way
of pruning the entire heads. Lastly, we demonstrate the usage of our framework
using gender bias, and based on our findings, we propose an improvement to an
existing debiasing method. Additionally, we re-discover a bias-performance
trade-off: the better the model performs, the more bias it contains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Generalization in Grounded Language Learning via Induced Model Sparsity. (arXiv:2207.02518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02518">
<div class="article-summary-box-inner">
<span><p>We provide a study of how induced model sparsity can help achieve
compositional generalization and better sample efficiency in grounded language
learning problems. We consider simple language-conditioned navigation problems
in a grid world environment with disentangled observations. We show that
standard neural architectures do not always yield compositional generalization.
To address this, we design an agent that contains a goal identification module
that encourages sparse correlations between words in the instruction and
attributes of objects, composing them together to find the goal. The output of
the goal identification module is the input to a value iteration network
planner. Our agent maintains a high level of performance on goals containing
novel combinations of properties even when learning from a handful of
demonstrations. We examine the internal representations of our agent and find
the correct correspondences between words in its dictionary and attributes in
the environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Complex NLP in Transformers for Text Ranking?. (arXiv:2207.02522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02522">
<div class="article-summary-box-inner">
<span><p>Even though term-based methods such as BM25 provide strong baselines in
ranking, under certain conditions they are dominated by large pre-trained
masked language models (MLMs) such as BERT. To date, the source of their
effectiveness remains unclear. Is it their ability to truly understand the
meaning through modeling syntactic aspects? We answer this by manipulating the
input order and position information in a way that destroys the natural
sequence order of query and passage and shows that the model still achieves
comparable performance. Overall, our results highlight that syntactic aspects
do not play a critical role in the effectiveness of re-ranking with BERT. We
point to other mechanisms such as query-passage cross-attention and richer
embeddings that capture word meanings based on aggregated context regardless of
the word order for being the main attributions for its superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Diversify for Product Question Generation. (arXiv:2207.02534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02534">
<div class="article-summary-box-inner">
<span><p>We address the product question generation task. For a given product
description, our goal is to generate questions that reflect potential user
information needs that are either missing or not well covered in the
description. Moreover, we wish to cover diverse user information needs that may
span a multitude of product types. To this end, we first show how the T5
pre-trained Transformer encoder-decoder model can be fine-tuned for the task.
Yet, while the T5 generated questions have a reasonable quality compared to the
state-of-the-art method for the task (KPCNet), many of such questions are still
too general, resulting in a sub-optimal global question diversity. As an
alternative, we propose a novel learning-to-diversify (LTD) fine-tuning
approach that allows to enrich the language learned by the underlying
Transformer model. Our empirical evaluation shows that, using our approach
significantly improves the global diversity of the underlying Transformer
model, while preserves, as much as possible, its generation relevance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowing Earlier what Right Means to You: A Comprehensive VQA Dataset for Grounding Relative Directions via Multi-Task Learning. (arXiv:2207.02624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02624">
<div class="article-summary-box-inner">
<span><p>Spatial reasoning poses a particular challenge for intelligent agents and is
at the same time a prerequisite for their successful interaction and
communication in the physical world. One such reasoning task is to describe the
position of a target object with respect to the intrinsic orientation of some
reference object via relative directions. In this paper, we introduce
GRiD-A-3D, a novel diagnostic visual question-answering (VQA) dataset based on
abstract objects. Our dataset allows for a fine-grained analysis of end-to-end
VQA models' capabilities to ground relative directions. At the same time, model
training requires considerably fewer computational resources compared with
existing datasets, yet yields a comparable or even higher performance. Along
with the new dataset, we provide a thorough evaluation based on two widely
known end-to-end VQA architectures trained on GRiD-A-3D. We demonstrate that
within a few epochs, the subtasks required to reason over relative directions,
such as recognizing and locating objects in a scene and estimating their
intrinsic orientations, are learned in the order in which relative directions
are intuitively processed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Challenge on Semi-Supervised and Reinforced Task-Oriented Dialog Systems. (arXiv:2207.02657v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02657">
<div class="article-summary-box-inner">
<span><p>A challenge on Semi-Supervised and Reinforced Task-Oriented Dialog Systems,
Co-located with EMNLP2022 SereTOD Workshop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kaggle Competition: Cantonese Audio-Visual Speech Recognition for In-car Commands. (arXiv:2207.02663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02663">
<div class="article-summary-box-inner">
<span><p>With the rise of deep learning and intelligent vehicles, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, in this research field, most
datasets are in major languages, such as English and Chinese. There is a huge
data scarcity issue for low-resource languages, hindering the development of
research and applications for broader communities. Therefore, it is crucial to
have more benchmarks to raise awareness and motivate the research in
low-resource languages. To mitigate this problem, we collect a new dataset,
namely Cantonese In-car Audio-Visual Speech Recognition (CI-AVSR), for in-car
speech recognition in the Cantonese language with video and audio data.
Together with it, we propose Cantonese Audio-Visual Speech Recognition for
In-car Commands as a new challenge for the community to tackle low-resource
speech recognition under in-car scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Value of Gazetteer in Chinese Named Entity Recognition. (arXiv:2207.02802v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02802">
<div class="article-summary-box-inner">
<span><p>Gazetteer is widely used in Chinese named entity recognition (NER) to enhance
span boundary detection and type classification. However, to further understand
the generalizability and effectiveness of gazetteers, the NLP community still
lacks a systematic analysis of the gazetteer-enhanced NER model. In this paper,
we first re-examine the effectiveness several common practices of the
gazetteer-enhanced NER models and carry out a series of detailed analysis to
evaluate the relationship between the model performance and the gazetteer
characteristics, which can guide us to build a more suitable gazetteer. The
findings of this paper are as follows: (1) the gazetteer has improved the most
situations where the dataset is difficult to learn well for the conventional
NER model. (2) the performance of model greatly benefits from the high-quality
pre-trained lexeme embeddings. (3) a good gazetteer should cover more entities
that can be matched in both the training set and testing set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strong Heuristics for Named Entity Linking. (arXiv:2207.02824v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02824">
<div class="article-summary-box-inner">
<span><p>Named entity linking (NEL) in news is a challenging endeavour due to the
frequency of unseen and emerging entities, which necessitates the use of
unsupervised or zero-shot methods. However, such methods tend to come with
caveats, such as no integration of suitable knowledge bases (like Wikidata) for
emerging entities, a lack of scalability, and poor interpretability. Here, we
consider person disambiguation in Quotebank, a massive corpus of
speaker-attributed quotations from the news, and investigate the suitability of
intuitive, lightweight, and scalable heuristics for NEL in web-scale corpora.
Our best performing heuristic disambiguates 94% and 63% of the mentions on
Quotebank and the AIDA-CoNLL benchmark, respectively. Additionally, the
proposed heuristics compare favourably to the state-of-the-art unsupervised and
zero-shot methods, Eigenthemes and mGENRE, respectively, thereby serving as
strong baselines for unsupervised and zero-shot entity linking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric. (arXiv:2110.05376v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05376">
<div class="article-summary-box-inner">
<span><p>Measuring automatic speech recognition (ASR) system quality is critical for
creating user-satisfying voice-driven applications. Word Error Rate (WER) has
been traditionally used to evaluate ASR system quality; however, it sometimes
correlates poorly with user perception/judgement of transcription quality. This
is because WER weighs every word equally and does not consider semantic
correctness which has a higher impact on user perception. In this work, we
propose evaluating ASR output hypotheses quality with SemDist that can measure
semantic correctness by using the distance between the semantic vectors of the
reference and hypothesis extracted from a pre-trained language model. Our
experimental results of 71K and 36K user annotated ASR output quality show that
SemDist achieves higher correlation with user perception than WER. We also show
that SemDist has higher correlation with downstream Natural Language
Understanding (NLU) tasks than WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. (arXiv:2202.04774v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04774">
<div class="article-summary-box-inner">
<span><p>Speech translation models are unable to directly process long audios, like
TED talks, which have to be split into shorter segments. Speech translation
datasets provide manual segmentations of the audios, which are not available in
real-world scenarios, and existing segmentation methods usually significantly
reduce translation quality at inference time. To bridge the gap between the
manual segmentation of training and the automatic one at inference, we propose
Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively
learn the optimal segmentation from any manually segmented speech corpus.
First, we train a classifier to identify the included frames in a segmentation,
using speech representations from a pre-trained wav2vec 2.0. The optimal
splitting points are then found by a probabilistic Divide-and-Conquer algorithm
that progressively splits at the frame of lowest probability until all segments
are below a pre-specified length. Experiments on MuST-C and mTEDx show that the
translation of the segments produced by our method approaches the quality of
the manual segmentation on 5 language pairs. Namely, SHAS retains 95-98% of the
manual segmentation's BLEU score, compared to the 87-93% of the best existing
methods. Our method is additionally generalizable to different domains and
achieves high zero-shot performance in unseen languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Hate Speech Detection with Cross-Domain Transfer. (arXiv:2203.01111v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01111">
<div class="article-summary-box-inner">
<span><p>The performance of hate speech detection models relies on the datasets on
which the models are trained. Existing datasets are mostly prepared with a
limited number of instances or hate domains that define hate topics. This
hinders large-scale analysis and transfer learning with respect to hate
domains. In this study, we construct large-scale tweet datasets for hate speech
detection in English and a low-resource language, Turkish, consisting of
human-labeled 100k tweets per each. Our datasets are designed to have equal
number of tweets distributed over five domains. The experimental results
supported by statistical tests show that Transformer-based language models
outperform conventional bag-of-words and neural models by at least 5% in
English and 10% in Turkish for large-scale hate speech detection. The
performance is also scalable to different training sizes, such that 98% of
performance in English, and 97% in Turkish, are recovered when 20% of training
instances are used. We further examine the generalization ability of
cross-domain transfer among hate domains. We show that 96% of the performance
of a target domain in average is recovered by other domains for English, and
92% for Turkish. Gender and religion are more successful to generalize to other
domains, while sports fail most.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17255">
<div class="article-summary-box-inner">
<span><p>This theoretical article examines how to construct human-like working memory
and thought processes within a computer. There should be two working memory
stores, one analogous to sustained firing in association cortex, and one
analogous to synaptic potentiation in the cerebral cortex. These stores must be
constantly updated with new representations that arise from either
environmental stimulation or internal processing. They should be updated
continuously, and in an iterative fashion, meaning that, in the next state,
some items in the set of coactive items should always be retained. Thus, the
set of concepts coactive in working memory will evolve gradually and
incrementally over time. This makes each state is a revised iteration of the
preceding state and causes successive states to overlap and blend with respect
to the set of representations they contain. As new representations are added
and old ones are subtracted, some remain active for several seconds over the
course of these changes. This persistent activity, similar to that used in
artificial recurrent neural networks, is used to spread activation energy
throughout the global workspace to search for the next associative update. The
result is a chain of associatively linked intermediate states that are capable
of advancing toward a solution or goal. Iterative updating is conceptualized
here as an information processing strategy, a computational and
neurophysiological determinant of the stream of thought, and an algorithm for
designing and programming artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology Reuse: the Real Test of Ontological Design. (arXiv:2205.02892v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02892">
<div class="article-summary-box-inner">
<span><p>Reusing ontologies in practice is still very challenging, especially when
multiple ontologies are (jointly) involved. Moreover, despite recent advances,
the realization of systematic ontology quality assurance remains a difficult
problem. In this work, the quality of thirty biomedical ontologies, and the
Computer Science Ontology are investigated, from the perspective of a practical
use case. Special scrutiny is given to cross-ontology references, which are
vital for combining ontologies. Diverse methods to detect potential issues are
proposed, including natural language processing and network analysis. Moreover,
several suggestions for improving ontologies and their quality assurance
processes are presented. It is argued that while the advancing automatic tools
for ontology quality assurance are crucial for ontology improvement, they will
not solve the problem entirely. It is ontology reuse that is the ultimate
method for continuously verifying and improving ontology quality, as well as
for guiding its future development. Specifically, multiple issues can be found
and fixed primarily through practical and diverse ontology reuse scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciTweets -- A Dataset and Annotation Framework for Detecting Scientific Online Discourse. (arXiv:2206.07360v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07360">
<div class="article-summary-box-inner">
<span><p>Scientific topics, claims and resources are increasingly debated as part of
online discourse, where prominent examples include discourse related to
COVID-19 or climate change. This has led to both significant societal impact
and increased interest in scientific online discourse from various disciplines.
For instance, communication studies aim at a deeper understanding of biases,
quality or spreading pattern of scientific information whereas computational
methods have been proposed to extract, classify or verify scientific claims
using NLP and IR techniques. However, research across disciplines currently
suffers from both a lack of robust definitions of the various forms of
science-relatedness as well as appropriate ground truth data for distinguishing
them. In this work, we contribute (a) an annotation framework and corresponding
definitions for different forms of scientific relatedness of online discourse
in Tweets, (b) an expert-annotated dataset of 1261 tweets obtained through our
labeling framework reaching an average Fleiss Kappa $\kappa$ of 0.63, (c) a
multi-label classifier trained on our data able to detect science-relatedness
with 89% F1 and also able to detect distinct forms of scientific knowledge
(claims, references). With this work we aim to lay the foundation for
developing and evaluating robust methods for analysing science as part of
large-scale online discourse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering. (arXiv:2207.01940v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01940">
<div class="article-summary-box-inner">
<span><p>We describe our two-stage system for the Multilingual Information Access
(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The
first stage consists of multilingual passage retrieval with a hybrid dense and
sparse retrieval strategy. The second stage consists of a reader which outputs
the answer from the top passages returned by the first stage. We show the
efficacy of using entity representations, sparse retrieval signals to help
dense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46
F1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On
the test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an
average F1 score of 31.61. We improve over the official baseline by over 4 F1
points on both the development and test sets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Machine Perception with Psychophysics. (arXiv:2207.02241v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02241">
<div class="article-summary-box-inner">
<span><p>{G}{ustav} Fechner's 1860 delineation of psychophysics, the measurement of
sensation in relation to its stimulus, is widely considered to be the advent of
modern psychological science. In psychophysics, a researcher parametrically
varies some aspects of a stimulus, and measures the resulting changes in a
human subject's experience of that stimulus; doing so gives insight to the
determining relationship between a sensation and the physical input that evoked
it. This approach is used heavily in perceptual domains, including signal
detection, threshold measurement, and ideal observer analysis. Scientific
fields like vision science have always leaned heavily on the methods and
procedures of psychophysics, but there is now growing appreciation of them by
machine learning researchers, sparked by widening overlap between biological
and artificial perception \cite{rojas2011automatic,
scheirer2014perceptual,escalera2014chalearn,zhang2018agil,
grieggs2021measuring}. Machine perception that is guided by behavioral
measurements, as opposed to guidance restricted to arbitrarily assigned human
labels, has significant potential to fuel further progress in artificial
intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based Surgical Skills Assessment using Long term Tool Tracking. (arXiv:2207.02247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02247">
<div class="article-summary-box-inner">
<span><p>Mastering the technical skills required to perform surgery is an extremely
challenging task. Video-based assessment allows surgeons to receive feedback on
their technical skills to facilitate learning and development. Currently, this
feedback comes primarily from manual video review, which is time-intensive and
limits the feasibility of tracking a surgeon's progress over many cases. In
this work, we introduce a motion-based approach to automatically assess
surgical skills from surgical case video feed. The proposed pipeline first
tracks surgical tools reliably to create motion trajectories and then uses
those trajectories to predict surgeon technical skill levels. The tracking
algorithm employs a simple yet effective re-identification module that improves
ID-switch compared to other state-of-the-art methods. This is critical for
creating reliable tool trajectories when instruments regularly move on- and
off-screen or are periodically obscured. The motion-based classification model
employs a state-of-the-art self-attention transformer network to capture short-
and long-term motion patterns that are essential for skill evaluation. The
proposed method is evaluated on an in-vivo (Cholec80) dataset where an
expert-rated GOALS skill assessment of the Calot Triangle Dissection is used as
a quantitative skill measure. We compare transformer-based skill assessment
with traditional machine learning approaches using the proposed and
state-of-the-art tracking. Our result suggests that using motion trajectories
from reliable tracking methods is beneficial for assessing surgeon skills based
solely on video streams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Array Camera Image Fusion using Physics-Aware Transformers. (arXiv:2207.02250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02250">
<div class="article-summary-box-inner">
<span><p>We demonstrate a physics-aware transformer for feature-based data fusion from
cameras with diverse resolution, color spaces, focal planes, focal lengths, and
exposure. We also demonstrate a scalable solution for synthetic training data
generation for the transformer using open-source computer graphics software. We
demonstrate image synthesis on arrays with diverse spectral responses,
instantaneous field of view and frame rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers. (arXiv:2207.02255v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02255">
<div class="article-summary-box-inner">
<span><p>We present OSFormer, the first one-stage transformer framework for
camouflaged instance segmentation (CIS). OSFormer is based on two key designs.
First, we design a location-sensing transformer (LST) to obtain the location
label and instance-aware parameters by introducing the location-guided queries
and the blend-convolution feedforward network. Second, we develop a
coarse-to-fine fusion (CFF) to merge diverse context information from the LST
encoder and CNN backbone. Coupling these two components enables OSFormer to
efficiently blend local features and long-range context dependencies for
predicting camouflaged instances. Compared with two-stage frameworks, our
OSFormer reaches 41% AP and achieves good convergence efficiency without
requiring enormous training data, i.e., only 3,040 samples under 60 epochs.
Code link: https://github.com/PJLallen/OSFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenLDN: Learning to Discover Novel Classes for Open-World Semi-Supervised Learning. (arXiv:2207.02261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02261">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) is one of the dominant approaches to address
the annotation bottleneck of supervised learning. Recent SSL methods can
effectively leverage a large repository of unlabeled data to improve
performance while relying on a small set of labeled data. One common assumption
in most SSL methods is that the labeled and unlabeled data are from the same
underlying data distribution. However, this is hardly the case in many
real-world scenarios, which limits their applicability. In this work, instead,
we attempt to solve the recently proposed challenging open-world SSL problem
that does not make such an assumption. In the open-world SSL problem, the
objective is to recognize samples of known classes, and simultaneously detect
and cluster samples belonging to novel classes present in unlabeled data. This
work introduces OpenLDN that utilizes a pairwise similarity loss to discover
novel classes. Using a bi-level optimization rule this pairwise similarity loss
exploits the information available in the labeled set to implicitly cluster
novel class samples, while simultaneously recognizing samples from known
classes. After discovering novel classes, OpenLDN transforms the open-world SSL
problem into a standard SSL problem to achieve additional performance gains
using existing SSL methods. Our extensive experiments demonstrate that OpenLDN
outperforms the current state-of-the-art methods on multiple popular
classification benchmarks while providing a better accuracy/training time
trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Realistic Semi-Supervised Learning. (arXiv:2207.02269v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02269">
<div class="article-summary-box-inner">
<span><p>Deep learning is pushing the state-of-the-art in many computer vision
applications. However, it relies on large annotated data repositories, and
capturing the unconstrained nature of the real-world data is yet to be solved.
Semi-supervised learning (SSL) complements the annotated training data with a
large corpus of unlabeled data to reduce annotation cost. The standard SSL
approach assumes unlabeled data are from the same distribution as annotated
data. Recently, ORCA [9] introduce a more realistic SSL problem, called
open-world SSL, by assuming that the unannotated data might contain samples
from unknown classes. This work proposes a novel approach to tackle SSL in
open-world setting, where we simultaneously learn to classify known and unknown
classes. At the core of our method, we utilize sample uncertainty and
incorporate prior knowledge about class distribution to generate reliable
pseudo-labels for unlabeled data belonging to both known and unknown classes.
Our extensive experimentation showcases the effectiveness of our approach on
several benchmark datasets, where it substantially outperforms the existing
state-of-the-art on seven diverse datasets including CIFAR-100 (17.6%),
ImageNet-100 (5.7%), and Tiny ImageNet (9.9%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection. (arXiv:2207.02279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02279">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection is a core problem in vision. Correctly detecting and
identifying anomalous behaviors in pedestrians from video data will enable
safety-critical applications such as surveillance, activity monitoring, and
human-robot interaction. In this paper, we propose to leverage trajectory
localization and prediction for unsupervised pedestrian anomaly event
detection. Different than previous reconstruction-based approaches, our
proposed framework rely on the prediction errors of normal and abnormal
pedestrian trajectories to detect anomalies spatially and temporally. We
present experimental results on real-world benchmark datasets on varying
timescales and show that our proposed trajectory-predictor-based anomaly
detection pipeline is effective and efficient at identifying anomalous
activities of pedestrians in videos. Code will be made available at
https://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiPOCO: Bi-Directional Trajectory Prediction with Pose Constraints for Pedestrian Anomaly Detection. (arXiv:2207.02281v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02281">
<div class="article-summary-box-inner">
<span><p>We present BiPOCO, a Bi-directional trajectory predictor with POse
COnstraints, for detecting anomalous activities of pedestrians in videos. In
contrast to prior work based on feature reconstruction, our work identifies
pedestrian anomalous events by forecasting their future trajectories and
comparing the predictions with their expectations. We introduce a set of novel
compositional pose-based losses with our predictor and leverage prediction
errors of each body joint for pedestrian anomaly detection. Experimental
results show that our BiPOCO approach can detect pedestrian anomalous
activities with a high detection rate (up to 87.0%) and incorporating pose
constraints helps distinguish normal and anomalous poses in prediction. This
work extends current literature of using prediction-based methods for anomaly
detection and can benefit safety-critical applications such as autonomous
driving and surveillance. Code is available at
https://github.com/akanuasiegbu/BiPOCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectivity of super resolution convolutional neural network for the enhancement of land cover classification from medium resolution satellite images. (arXiv:2207.02301v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02301">
<div class="article-summary-box-inner">
<span><p>In the modern world, satellite images play a key role in forest management
and degradation monitoring. For a precise quantification of forest land cover
changes, the availability of spatially fine resolution data is a necessity.
Since 1972, NASAs LANDSAT Satellites are providing terrestrial images covering
every corner of the earth, which have been proved to be a highly useful
resource for terrestrial change analysis and have been used in numerous other
sectors. However, freely accessible satellite images are, generally, of medium
to low resolution which is a major hindrance to the precision of the analysis.
Hence, we performed a comprehensive study to prove our point that, enhancement
of resolution by Super-Resolution Convolutional Neural Network (SRCNN) will
lessen the chance of misclassification of pixels, even under the established
recognition methods. We tested the method on original LANDSAT-7 images of
different regions of Sundarbans and their upscaled versions which were produced
by bilinear interpolation, bicubic interpolation, and SRCNN respectively and it
was discovered that SRCNN outperforms the others by a significant amount.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Ensemble Learning Approach to Lung CT Segmentation for COVID-19 Severity Assessment. (arXiv:2207.02322v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02322">
<div class="article-summary-box-inner">
<span><p>We present a novel deep learning approach to categorical segmentation of lung
CTs of COVID-19 patients. Specifically, we partition the scans into healthy
lung tissues, non-lung regions, and two different, yet visually similar,
pathological lung tissues, namely, ground-glass opacity and consolidation. This
is accomplished via a unique, end-to-end hierarchical network architecture and
ensemble learning, which contribute to the segmentation and provide a measure
for segmentation uncertainty. The proposed framework achieves competitive
results and outstanding generalization capabilities for three COVID-19
datasets. Our method is ranked second in a public Kaggle competition for
COVID-19 CT images segmentation. Moreover, segmentation uncertainty regions are
shown to correspond to the disagreements between the manual annotations of two
different radiologists. Finally, preliminary promising correspondence results
are shown for our private dataset when comparing the patients' COVID-19
severity scores (based on clinical measures), and the segmented lung
pathologies. Code and data are available at our repository:
https://github.com/talbenha/covid-seg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02327">
<div class="article-summary-box-inner">
<span><p>Diffusion MRI tractography is an advanced imaging technique for quantitative
mapping of the brain's structural connectivity. Whole brain tractography (WBT)
data contains over hundreds of thousands of individual fiber streamlines
(estimated brain connections), and this data is usually parcellated to create
compact representations for data analysis applications such as disease
classification. In this paper, we propose a novel parcellation-free WBT
analysis framework, TractoFormer, that leverages tractography information at
the level of individual fiber streamlines and provides a natural mechanism for
interpretation of results using the attention mechanism of transformers.
TractoFormer includes two main contributions. First, we propose a novel and
simple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber
spatial relationships and any feature of interest that can be computed from
individual fibers (such as FA or MD). Second, we design a network based on
vision transformers (ViTs) that includes: 1) data augmentation to overcome
model overfitting on small datasets, 2) identification of discriminative fibers
for interpretation of results, and 3) ensemble learning to leverage fiber
information from different brain regions. In a synthetic data experiment,
TractoFormer successfully identifies discriminative fibers with simulated group
differences. In a disease classification experiment comparing several methods,
TractoFormer achieves the highest accuracy in classifying schizophrenia vs
control. Discriminative fibers are identified in left hemispheric frontal and
parietal superficial white matter regions, which have previously been shown to
be affected in schizophrenia patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Grounding for VQA in Vision-Language Transformers. (arXiv:2207.02334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02334">
<div class="article-summary-box-inner">
<span><p>Transformers for visual-language representation learning have been getting a
lot of interest and shown tremendous performance on visual question answering
(VQA) and grounding. But most systems that show good performance of those tasks
still rely on pre-trained object detectors during training, which limits their
applicability to the object classes available for those detectors. To mitigate
this limitation, the following paper focuses on the problem of weakly
supervised grounding in context of visual question answering in transformers.
The approach leverages capsules by grouping each visual token in the visual
encoder and uses activations from language self-attention layers as a
text-guided selection module to mask those capsules before they are forwarded
to the next layer. We evaluate our approach on the challenging GQA as well as
VQA-HAT dataset for VQA grounding. Our experiments show that: while removing
the information of masked objects from standard transformer architectures leads
to a significant drop in performance, the integration of capsules significantly
improves the grounding ability of such systems and provides new
state-of-the-art results compared to other approaches in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02335">
<div class="article-summary-box-inner">
<span><p>Early detection of retinal diseases is one of the most important means of
preventing partial or permanent blindness in patients. In this research, a
novel multi-label classification system is proposed for the detection of
multiple retinal diseases, using fundus images collected from a variety of
sources. First, a new multi-label retinal disease dataset, the MuReD dataset,
is constructed, using a number of publicly available datasets for fundus
disease classification. Next, a sequence of post-processing steps is applied to
ensure the quality of the image data and the range of diseases, present in the
dataset. For the first time in fundus multi-label disease classification, a
transformer-based model optimized through extensive experimentation is used for
image analysis and decision making. Numerous experiments are performed to
optimize the configuration of the proposed system. It is shown that the
approach performs better than state-of-the-art works on the same task by 7.9%
and 8.1% in terms of AUC score for disease detection and disease
classification, respectively. The obtained results further support the
potential applications of transformer-based architectures in the medical
imaging field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms. (arXiv:2207.02337v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02337">
<div class="article-summary-box-inner">
<span><p>The advent of federated learning has facilitated large-scale data exchange
amongst machine learning models while maintaining privacy. Despite its brief
history, federated learning is rapidly evolving to make wider use more
practical. One of the most significant advancements in this domain is the
incorporation of transfer learning into federated learning, which overcomes
fundamental constraints of primary federated learning, particularly in terms of
security. This chapter performs a comprehensive survey on the intersection of
federated and transfer learning from a security point of view. The main goal of
this study is to uncover potential vulnerabilities and defense mechanisms that
might compromise the privacy and performance of systems that use federated and
transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalization to translation shifts: a study in architectures and augmentations. (arXiv:2207.02349v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02349">
<div class="article-summary-box-inner">
<span><p>We provide a detailed evaluation of various image classification
architectures (convolutional, vision transformer, and fully connected MLP
networks) and data augmentation techniques towards generalization to large
spacial translation shifts. We make the following observations: (a) In the
absence of data augmentation, all architectures, including convolutional
networks suffer degradation in performance when evaluated on translated test
distributions. Understandably, both the in-distribution accuracy as well as
degradation to shifts is significantly worse for non-convolutional
architectures. (b) Across all architectures, even a minimal augmentation of $4$
pixel random crop improves the robustness of performance to much larger
magnitude shifts of up to $1/4$ of image size ($8$-$16$ pixels) in the test
data -- suggesting a form of meta generalization from augmentation. For
non-convolutional architectures, while the absolute accuracy is still low, we
see dramatic improvements in robustness to large translation shifts. (c) With
sufficiently advanced augmentation ($4$ pixel
crop+RandAugmentation+Erasing+MixUp) pipeline all architectures can be trained
to have competitive performance, both in terms of in-distribution accuracy as
well as generalization to large translation shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNeRF: Stylized Neural Implicit Representations for 3D Scenes. (arXiv:2207.02363v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02363">
<div class="article-summary-box-inner">
<span><p>This paper presents a stylized novel view synthesis method. Applying
state-of-the-art stylization methods to novel views frame by frame often causes
jittering artifacts due to the lack of cross-view consistency. Therefore, this
paper investigates 3D scene stylization that provides a strong inductive bias
for consistent novel view synthesis. Specifically, we adopt the emerging neural
radiance fields (NeRF) as our choice of 3D scene representation for their
capability to render high-quality novel views for a variety of scenes. However,
as rendering a novel view from a NeRF requires a large number of samples,
training a stylized NeRF requires a large amount of GPU memory that goes beyond
an off-the-shelf GPU capacity. We introduce a new training method to address
this problem by alternating the NeRF and stylization optimization steps. Such a
method enables us to make full use of our hardware memory capacity to both
generate images at higher resolution and adopt more expressive image style
transfer methods. Our experiments show that our method produces stylized NeRFs
for a wide range of content, including indoor, outdoor and dynamic scenes, and
synthesizes high-quality novel views with cross-view consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning for Human Sensing Using Radio Signals. (arXiv:2207.02370v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02370">
<div class="article-summary-box-inner">
<span><p>There is a growing literature demonstrating the feasibility of using Radio
Frequency (RF) signals to enable key computer vision tasks in the presence of
occlusions and poor lighting. It leverages that RF signals traverse walls and
occlusions to deliver through-wall pose estimation, action recognition, scene
captioning, and human re-identification. However, unlike RGB datasets which can
be labeled by human workers, labeling RF signals is a daunting task because
such signals are not human interpretable. Yet, it is fairly easy to collect
unlabelled RF signals. It would be highly beneficial to use such unlabeled RF
data to learn useful representations in an unsupervised manner. Thus, in this
paper, we explore the feasibility of adapting RGB-based unsupervised
representation learning to RF signals. We show that while contrastive learning
has emerged as the main technique for unsupervised representation learning from
images and videos, such methods produce poor performance when applied to
sensing humans using RF signals. In contrast, predictive unsupervised learning
methods learn high-quality representations that can be used for multiple
downstream RF-based sensing tasks. Our empirical results show that this
approach outperforms state-of-the-art RF-based human sensing on various tasks,
opening the possibility of unsupervised representation learning from this novel
modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Video Segmentation via Temporal Pseudo Supervision. (arXiv:2207.02372v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02372">
<div class="article-summary-box-inner">
<span><p>Video semantic segmentation has achieved great progress under the supervision
of large amounts of labelled training data. However, domain adaptive video
segmentation, which can mitigate data labelling constraints by adapting from a
labelled source domain toward an unlabelled target domain, is largely
neglected. We design temporal pseudo supervision (TPS), a simple and effective
method that explores the idea of consistency training for learning effective
representations from unlabelled target videos. Unlike traditional consistency
training that builds consistency in spatial space, we explore consistency
training in spatiotemporal space by enforcing model consistency across
augmented video frames which helps learn from more diverse target data.
Specifically, we design cross-frame pseudo labelling to provide pseudo
supervision from previous video frames while learning from the augmented
current video frames. The cross-frame pseudo labelling encourages the network
to produce high-certainty predictions, which facilitates consistency training
with cross-frame augmentation effectively. Extensive experiments over multiple
public datasets show that TPS is simpler to implement, much more stable to
train, and achieves superior video segmentation accuracy as compared with the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DG-STFM: 3D Geometric Guided Student-Teacher Feature Matching. (arXiv:2207.02375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02375">
<div class="article-summary-box-inner">
<span><p>We tackle the essential task of finding dense visual correspondences between
a pair of images. This is a challenging problem due to various factors such as
poor texture, repetitive patterns, illumination variation, and motion blur in
practical scenarios. In contrast to methods that use dense correspondence
ground-truths as direct supervision for local feature matching training, we
train 3DG-STFM: a multi-modal matching model (Teacher) to enforce the depth
consistency under 3D dense correspondence supervision and transfer the
knowledge to 2D unimodal matching model (Student). Both teacher and student
models consist of two transformer-based matching modules that obtain dense
correspondences in a coarse-to-fine manner. The teacher model guides the
student model to learn RGB-induced depth information for the matching purpose
on both coarse and fine branches. We also evaluate 3DG-STFM on a model
compression task. To the best of our knowledge, 3DG-STFM is the first
student-teacher learning method for the local feature matching task. The
experiments show that our method outperforms state-of-the-art methods on indoor
and outdoor camera pose estimations, and homography estimation problems. Code
is available at: https://github.com/Ryan-prime/3DG-STFM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review on Deep Supervision: Theories and Applications. (arXiv:2207.02376v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02376">
<div class="article-summary-box-inner">
<span><p>Deep supervision, or known as 'intermediate supervision' or 'auxiliary
supervision', is to add supervision at hidden layers of a neural network. This
technique has been increasingly applied in deep neural network learning systems
for various computer vision applications recently. There is a consensus that
deep supervision helps improve neural network performance by alleviating the
gradient vanishing problem, as one of the many strengths of deep supervision.
Besides, in different computer vision applications, deep supervision can be
applied in different ways. How to make the most use of deep supervision to
improve network performance in different applications has not been thoroughly
investigated. In this paper, we provide a comprehensive in-depth review of deep
supervision in both theories and applications. We propose a new classification
of different deep supervision networks, and discuss advantages and limitations
of current deep supervision networks in computer vision applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising. (arXiv:2207.02377v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02377">
<div class="article-summary-box-inner">
<span><p>The acquisition conditions for low-dose and high-dose CT images are usually
different, so that the shifts in the CT numbers often occur. Accordingly,
unsupervised deep learning-based approaches, which learn the target image
distribution, often introduce CT number distortions and result in detrimental
effects in diagnostic performance. To address this, here we propose a novel
unsupervised learning approach for lowdose CT reconstruction using patch-wise
deep metric learning. The key idea is to learn embedding space by pulling the
positive pairs of image patches which shares the same anatomical structure, and
pushing the negative pairs which have same noise level each other. Thereby, the
network is trained to suppress the noise level, while retaining the original
global CT number distributions even after the image translation. Experimental
results confirm that our deep metric learning plays a critical role in
producing high quality denoised images without CT number shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI. (arXiv:2207.02390v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02390">
<div class="article-summary-box-inner">
<span><p>Fast MRI aims to reconstruct a high fidelity image from partially observed
measurements. Exuberant development in fast MRI using deep learning has been
witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer
based models, are fast-growing in natural language processing and promptly
developed for computer vision and medical image analysis due to their prominent
performance. Nevertheless, due to the complexity of the Transformer, the
application of fast MRI may not be straightforward. The main obstacle is the
computational cost of the self-attention layer, which is the core part of the
Transformer, can be expensive for high resolution MRI inputs. In this study, we
propose a new Transformer architecture for solving fast MRI that coupled
Shifted Windows Transformer with U-Net to reduce the network complexity. We
incorporate deformable attention to construe the explainability of our
reconstruction model. We empirically demonstrate that our method achieves
consistently superior performance on the fast MRI task. Besides, compared to
state-of-the-art Transformer models, our method has fewer network parameters
while revealing explainability. The code is publicly available at
https://github.com/ayanglab/SDAUT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Efficient Adversarial Attack Based on Latin Hypercube Sampling. (arXiv:2207.02391v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02391">
<div class="article-summary-box-inner">
<span><p>In order to be applicable in real-world scenario, Boundary Attacks (BAs) were
proposed and ensured one hundred percent attack success rate with only decision
information. However, existing BA methods craft adversarial examples by
leveraging a simple random sampling (SRS) to estimate the gradient, consuming a
large number of model queries. To overcome the drawback of SRS, this paper
proposes a Latin Hypercube Sampling based Boundary Attack (LHS-BA) to save
query budget. Compared with SRS, LHS has better uniformity under the same
limited number of random samples. Therefore, the average on these random
samples is closer to the true gradient than that estimated by SRS. Various
experiments are conducted on benchmark datasets including MNIST, CIFAR, and
ImageNet-1K. Experimental results demonstrate the superiority of the proposed
LHS-BA over the state-of-the-art BA methods in terms of query efficiency. The
source codes are publicly available at https://github.com/GZHU-DVL/LHS-BA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoSpeed: A Linked Autoencoder Approach for Pulse-Echo Speed-of-Sound Imaging for Medical Ultrasound. (arXiv:2207.02392v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02392">
<div class="article-summary-box-inner">
<span><p>Quantitative ultrasound, e.g., speed-of-sound (SoS) in tissues, provides
information about tissue properties that have diagnostic value. Recent studies
showed the possibility of extracting SoS information from pulse-echo ultrasound
raw data (a.k.a. RF data) using deep neural networks that are fully trained on
simulated data. These methods take sensor domain data, i.e., RF data, as input
and train a network in an end-to-end fashion to learn the implicit mapping
between the RF data domain and SoS domain. However, such networks are prone to
overfitting to simulated data which results in poor performance and instability
when tested on measured data. We propose a novel method for SoS mapping
employing learned representations from two linked autoencoders. We test our
approach on simulated and measured data acquired from human breast mimicking
phantoms. We show that SoS mapping is possible using linked autoencoders. The
proposed method has a Mean Absolute Percentage Error (MAPE) of 2.39% on the
simulated data. On the measured data, the predictions of the proposed method
are close to the expected values with MAPE of 1.1%. Compared to an end-to-end
trained network, the proposed method shows higher stability and
reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Hybrid Endoscopic Dataset for Evaluating Machine Learning-based Photometric Image Enhancement Models. (arXiv:2207.02396v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02396">
<div class="article-summary-box-inner">
<span><p>Endoscopy is the most widely used medical technique for cancer and polyp
detection inside hollow organs. However, images acquired by an endoscope are
frequently affected by illumination artefacts due to the enlightenment source
orientation. There exist two major issues when the endoscope's light source
pose suddenly changes: overexposed and underexposed tissue areas are produced.
These two scenarios can result in misdiagnosis due to the lack of information
in the affected zones or hamper the performance of various computer vision
methods (e.g., SLAM, structure from motion, optical flow) used during the non
invasive examination. The aim of this work is two-fold: i) to introduce a new
synthetically generated data-set generated by a generative adversarial
techniques and ii) and to explore both shallow based and deep learning-based
image-enhancement methods in overexposed and underexposed lighting conditions.
Best quantitative results (i.e., metric based results), were obtained by the
deep-learnnig-based LMSPEC method,besides a running time around 7.6 fps)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Transformation for Image Composition via Correspondence Learning. (arXiv:2207.02398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02398">
<div class="article-summary-box-inner">
<span><p>When using cut-and-paste to acquire a composite image, the geometry
inconsistency between foreground and background may severely harm its fidelity.
To address the geometry inconsistency in composite images, several existing
works learned to warp the foreground object for geometric correction. However,
the absence of annotated dataset results in unsatisfactory performance and
unreliable evaluation. In this work, we contribute a Spatial TRAnsformation for
virtual Try-on (STRAT) dataset covering three typical application scenarios.
Moreover, previous works simply concatenate foreground and background as input
without considering their mutual correspondence. Instead, we propose a novel
correspondence learning network (CorrelNet) to model the correspondence between
foreground and background using cross-attention maps, based on which we can
predict the target coordinate that each source coordinate of foreground should
be mapped to on the background. Then, the warping parameters of foreground
object can be derived from pairs of source and target coordinates.
Additionally, we learn a filtering mask to eliminate noisy pairs of coordinates
to estimate more accurate warping parameters. Extensive experiments on our
STRAT dataset demonstrate that our proposed CorrelNet performs more favorably
against previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Apparent Diffusion Coefficient Maps from Undersampled Radial k-Space Diffusion-Weighted MRI in Mice using a Deep CNN-Transformer Model in Conjunction with a Monoexponential Model. (arXiv:2207.02399v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02399">
<div class="article-summary-box-inner">
<span><p>Purpose: To accelerate radially sampled diffusion weighted spin-echo
(Rad-DW-SE) acquisition method for generating high quality of apparent
diffusion coefficient (ADC) maps. Methods: A deep learning method was developed
to generate accurate ADC map reconstruction from undersampled DWI data acquired
with the Rad-DW-SE method. The deep learning method integrates convolutional
neural networks (CNNs) with vison transformers to generate high quality ADC
maps from undersampled DWI data, regularized by a monoexponential ADC model
fitting term. A model was trained on DWI data of 147 mice and evaluated on DWI
data of 36 mice, with undersampling rates of 4x and 8x. Results: Ablation
studies and experimental results have demonstrated that the proposed deep
learning model can generate high quality ADC maps from undersampled DWI data,
better than alternative deep learning methods under comparison, with their
performance quantified on different levels of images, tumors, kidneys, and
muscles. Conclusions: The deep learning method with integrated CNNs and
transformers provides an effective means to accurately compute ADC maps from
undersampled DWI data acquired with the Rad-DW-SE method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chairs Can be Stood on: Overcoming Object Bias in Human-Object Interaction Detection. (arXiv:2207.02400v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02400">
<div class="article-summary-box-inner">
<span><p>Detecting Human-Object Interaction (HOI) in images is an important step
towards high-level visual comprehension. Existing work often shed light on
improving either human and object detection, or interaction recognition.
However, due to the limitation of datasets, these methods tend to fit well on
frequent interactions conditioned on the detected objects, yet largely ignoring
the rare ones, which is referred to as the object bias problem in this paper.
In this work, we for the first time, uncover the problem from two aspects:
unbalanced interaction distribution and biased model learning. To overcome the
object bias problem, we propose a novel plug-and-play Object-wise Debiasing
Memory (ODM) method for re-balancing the distribution of interactions under
detected objects. Equipped with carefully designed read and write strategies,
the proposed ODM allows rare interaction instances to be more frequently
sampled for training, thereby alleviating the object bias induced by the
unbalanced interaction distribution. We apply this method to three advanced
baselines and conduct experiments on the HICO-DET and HOI-COCO datasets. To
quantitatively study the object bias problem, we advocate a new protocol for
evaluating model performance. As demonstrated in the experimental results, our
method brings consistent and significant improvements over baselines,
especially on rare interactions under each object. In addition, when evaluating
under the conventional standard setting, our method achieves new
state-of-the-art on the two benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White Matter Tracts are Point Clouds: Neuropsychological Score Prediction and Critical Region Localization via Geometric Deep Learning. (arXiv:2207.02402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02402">
<div class="article-summary-box-inner">
<span><p>White matter tract microstructure has been shown to influence
neuropsychological scores of cognitive performance. However, prediction of
these scores from white matter tract data has not been attempted. In this
paper, we propose a deep-learning-based framework for neuropsychological score
prediction using microstructure measurements estimated from diffusion magnetic
resonance imaging (dMRI) tractography, focusing on predicting performance on a
receptive vocabulary assessment task based on a critical fiber tract for
language, the arcuate fasciculus (AF). We directly utilize information from all
points in a fiber tract, without the need to average data along the fiber as is
traditionally required by diffusion MRI tractometry methods. Specifically, we
represent the AF as a point cloud with microstructure measurements at each
point, enabling adoption of point-based neural networks. We improve prediction
performance with the proposed Paired-Siamese Loss that utilizes information
about differences between continuous neuropsychological scores. Finally, we
propose a Critical Region Localization (CRL) algorithm to localize informative
anatomical regions containing points with strong contributions to the
prediction results. Our method is evaluated on data from 806 subjects from the
Human Connectome Project dataset. Results demonstrate superior
neuropsychological score prediction performance compared to baseline methods.
We discover that critical regions in the AF are strikingly consistent across
subjects, with the highest number of strongly contributing points located in
frontal cortical regions (i.e., the rostral middle frontal, pars opercularis,
and pars triangularis), which are strongly implicated as critical areas for
language processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Model for Partial Multi-Label Image Classification with Curriculum Based Disambiguation. (arXiv:2207.02410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02410">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the partial multi-label (PML) image classification
problem, where each image is annotated with a candidate label set consists of
multiple relevant labels and other noisy labels. Existing PML methods typically
design a disambiguation strategy to filter out noisy labels by utilizing prior
knowledge with extra assumptions, which unfortunately is unavailable in many
real tasks. Furthermore, because the objective function for disambiguation is
usually elaborately designed on the whole training set, it can be hardly
optimized in a deep model with SGD on mini-batches. In this paper, for the
first time we propose a deep model for PML to enhance the representation and
discrimination ability. On one hand, we propose a novel curriculum based
disambiguation strategy to progressively identify ground-truth labels by
incorporating the varied difficulties of different classes. On the other hand,
a consistency regularization is introduced for model retraining to balance
fitting identified easy labels and exploiting potential relevant labels.
Extensive experimental results on the commonly used benchmark datasets show the
proposed method significantly outperforms the SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation. (arXiv:2207.02425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02425">
<div class="article-summary-box-inner">
<span><p>We observe that human poses exhibit strong group-wise structural correlation
and spatial coupling between keypoints due to the biological constraints of
different body parts. This group-wise structural correlation can be explored to
improve the accuracy and robustness of human pose estimation. In this work, we
develop a self-constrained prediction-verification network to characterize and
learn the structural correlation between keypoints during training. During the
inference stage, the feedback information from the verification network allows
us to perform further optimization of pose prediction, which significantly
improves the performance of human pose estimation. Specifically, we partition
the keypoints into groups according to the biological structure of human body.
Within each group, the keypoints are further partitioned into two subsets,
high-confidence base keypoints and low-confidence terminal keypoints. We
develop a self-constrained prediction-verification network to perform forward
and backward predictions between these keypoint subsets. One fundamental
challenge in pose estimation, as well as in generic prediction tasks, is that
there is no mechanism for us to verify if the obtained pose estimation or
prediction results are accurate or not, since the ground truth is not
available. Once successfully learned, the verification network serves as an
accuracy verification module for the forward pose prediction. During the
inference stage, it can be used to guide the local optimization of the pose
estimation results of low-confidence keypoints with the self-constrained loss
on high-confidence keypoints as the objective function. Our extensive
experimental results on benchmark MS COCO and CrowdPose datasets demonstrate
that the proposed method can significantly improve the pose estimation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCT-Net: Domain-Calibrated Translation for Portrait Stylization. (arXiv:2207.02426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02426">
<div class="article-summary-box-inner">
<span><p>This paper introduces DCT-Net, a novel image translation architecture for
few-shot portrait stylization. Given limited style exemplars ($\sim$100), the
new architecture can produce high-quality style transfer results with advanced
ability to synthesize high-fidelity contents and strong generality to handle
complicated scenes (e.g., occlusions and accessories). Moreover, it enables
full-body image translation via one elegant evaluation network trained by
partial observations (i.e., stylized heads). Few-shot learning based style
transfer is challenging since the learned model can easily become overfitted in
the target domain, due to the biased distribution formed by only a few training
examples. This paper aims to handle the challenge by adopting the key idea of
"calibration first, translation later" and exploring the augmented global
structure with locally-focused translation. Specifically, the proposed DCT-Net
consists of three modules: a content adapter borrowing the powerful prior from
source photos to calibrate the content distribution of target samples; a
geometry expansion module using affine transformations to release spatially
semantic constraints; and a texture translation module leveraging samples
produced by the calibrated distribution to learn a fine-grained conversion.
Experimental results demonstrate the proposed method's superiority over the
state of the art in head stylization and its effectiveness on full image
translation with adaptive deformations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAMa: Cross-view Video Geo-localization. (arXiv:2207.02431v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02431">
<div class="article-summary-box-inner">
<span><p>The existing work in cross-view geo-localization is based on images where a
ground panorama is matched to an aerial image. In this work, we focus on ground
videos instead of images which provides additional contextual cues which are
important for this task. There are no existing datasets for this problem,
therefore we propose GAMa dataset, a large-scale dataset with ground videos and
corresponding aerial images. We also propose a novel approach to solve this
problem. At clip-level, a short video clip is matched with corresponding aerial
image and is later used to get video-level geo-localization of a long video.
Moreover, we propose a hierarchical approach to further improve the clip-level
geolocalization. It is a challenging dataset, unaligned and limited field of
view, and our proposed method achieves a Top-1 recall rate of 19.4% and 45.1%
@1.0mile. Code and dataset are available at following link:
https://github.com/svyas23/GAMa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complementary Bi-directional Feature Compression for Indoor 360{\deg} Semantic Segmentation with Self-distillation. (arXiv:2207.02437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02437">
<div class="article-summary-box-inner">
<span><p>Recently, horizontal representation-based panoramic semantic segmentation
approaches outperform projection-based solutions, because the distortions can
be effectively removed by compressing the spherical data in the vertical
direction. However, these methods ignore the distortion distribution prior and
are limited to unbalanced receptive fields, e.g., the receptive fields are
sufficient in the vertical direction and insufficient in the horizontal
direction. Differently, a vertical representation compressed in another
direction can offer implicit distortion prior and enlarge horizontal receptive
fields. In this paper, we combine the two different representations and propose
a novel 360{\deg} semantic segmentation solution from a complementary
perspective. Our network comprises three modules: a feature extraction module,
a bi-directional compression module, and an ensemble decoding module. First, we
extract multi-scale features from a panorama. Then, a bi-directional
compression module is designed to compress features into two complementary
low-dimensional representations, which provide content perception and
distortion prior. Furthermore, to facilitate the fusion of bi-directional
features, we design a unique self distillation strategy in the ensemble
decoding module to enhance the interaction of different features and further
improve the performance. Experimental results show that our approach
outperforms the state-of-the-art solutions with at least 10\% improvement on
quantitative evaluations while displaying the best performance on visual
appearance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation. (arXiv:2207.02466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02466">
<div class="article-summary-box-inner">
<span><p>The inherent ambiguity in ground-truth annotations of 3D bounding boxes
caused by occlusions, signal missing, or manual annotation errors can confuse
deep 3D object detectors during training, thus deteriorating the detection
accuracy. However, existing methods overlook such issues to some extent and
treat the labels as deterministic. In this paper, we propose GLENet, a
generative label uncertainty estimation framework adapted from conditional
variational autoencoders, to model the one-to-many relationship between a
typical 3D object and its potential ground-truth bounding boxes with latent
variables. The label uncertainty generated by GLENet is a plug-and-play module
and can be conveniently integrated into existing deep 3D detectors to build
probabilistic detectors and supervise the learning of the localization
uncertainty. Besides, we propose an uncertainty-aware quality estimator
architecture in probabilistic detectors to guide the training of IoU-branch
with predicted localization uncertainty. We incorporate the proposed methods
into various popular base 3D detectors and observe that their performance is
significantly boosted to the current state-of-the-art over the Waymo Open
dataset and KITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-area Target Individual Detection with Free Drawing on Video. (arXiv:2207.02467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02467">
<div class="article-summary-box-inner">
<span><p>This paper has provided a novel design idea and some implementation methods
to make a real time detection of multi-areas with multiple detecting areas that
are generated by the real time drawing on the screen display of the video. The
drawing on the video will remain the output as polylines, and the colors of the
outlines will change when the stage of drawing or detecting is changed. The
shape of the drawn area is free to be customized and real-time effective. The
configuration of the drawn areas can be renewed and the detecting areas are
working individually. The detection result should be shown with a GUI designed
by Tkinter. The object recognition model was developed on YOLOv5 but can be
changed to others, which means the core design and implementation idea of this
paper is model-independent. With PIL and OpenCV and Tkinter, the drawing effect
is real time and efficient. The design and code of this research is basic and
can be extended to be implemented in numerous monitoring and detecting
situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Contrast MRI Segmentation Trained on Synthetic Images. (arXiv:2207.02469v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02469">
<div class="article-summary-box-inner">
<span><p>In our comprehensive experiments and evaluations, we show that it is possible
to generate multiple contrast (even all synthetically) and use synthetically
generated images to train an image segmentation engine. We showed promising
segmentation results tested on real multi-contrast MRI scans when delineating
muscle, fat, bone and bone marrow, all trained on synthetic images. Based on
synthetic image training, our segmentation results were as high as 93.91\%,
94.11\%, 91.63\%, 95.33\%, for muscle, fat, bone, and bone marrow delineation,
respectively. Results were not significantly different from the ones obtained
when real images were used for segmentation training: 94.68\%, 94.67\%,
95.91\%, and 96.82\%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stage Decision Improves Open-Set Panoptic Segmentation. (arXiv:2207.02504v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02504">
<div class="article-summary-box-inner">
<span><p>Open-set panoptic segmentation (OPS) problem is a new research direction
aiming to perform segmentation for both \known classes and \unknown classes,
i.e., the objects ("things") that are never annotated in the training set. The
main challenges of OPS are twofold: (1) the infinite possibility of the
\unknown object appearances makes it difficult to model them from a limited
number of training data. (2) at training time, we are only provided with the
"void" category, which essentially mixes the "unknown thing" and "background"
classes. We empirically find that directly using "void" category to supervise
\known class or "background" without screening will not lead to a satisfied OPS
result. In this paper, we propose a divide-and-conquer scheme to develop a
two-stage decision process for OPS. We show that by properly combining a \known
class discriminator with an additional class-agnostic object prediction head,
the OPS performance can be significantly improved. Specifically, we first
propose to create a classifier with only \known categories and let the "void"
class proposals achieve low prediction probability from those categories. Then
we distinguish the "unknown things" from the background by using the additional
object prediction head. To further boost performance, we introduce "unknown
things" pseudo-labels generated from up-to-date models and a heuristic rule to
enrich the training set. Our extensive experimental evaluation shows that our
approach significantly improves \unknown class panoptic quality, with more than
30\% relative improvements than the existing best-performed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Mitigating Flaws of Deep Perceptual Similarity Metrics. (arXiv:2207.02512v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02512">
<div class="article-summary-box-inner">
<span><p>Measuring the similarity of images is a fundamental problem to computer
vision for which no universal solution exists. While simple metrics such as the
pixel-wise L2-norm have been shown to have significant flaws, they remain
popular. One group of recent state-of-the-art metrics that mitigates some of
those flaws are Deep Perceptual Similarity (DPS) metrics, where the similarity
is evaluated as the distance in the deep features of neural networks. However,
DPS metrics themselves have been less thoroughly examined for their benefits
and, especially, their flaws. This work investigates the most common DPS
metric, where deep features are compared by spatial position, along with
metrics comparing the averaged and sorted deep features. The metrics are
analyzed in-depth to understand the strengths and weaknesses of the metrics by
using images designed specifically to challenge them. This work contributes
with new insights into the flaws of DPS, and further suggests improvements to
the metrics. An implementation of this work is available online:
https://github.com/guspih/deep_perceptual_similarity_analysis/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Encoder-Decoder Architecture for Foot Ulcer Segmentation. (arXiv:2207.02515v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02515">
<div class="article-summary-box-inner">
<span><p>Continuous monitoring of foot ulcer healing is needed to ensure the efficacy
of a given treatment and to avoid any possibility of deterioration. Foot ulcer
segmentation is an essential step in wound diagnosis. We developed a model that
is similar in spirit to the well-established encoder-decoder and residual
convolution neural networks. Our model includes a residual connection along
with a channel and spatial attention integrated within each convolution block.
A simple patch-based approach for model training, test time augmentations, and
majority voting on the obtained predictions resulted in superior performance.
Our model did not leverage any readily available backbone architecture,
pre-training on a similar external dataset, or any of the transfer learning
techniques. The total number of network parameters being around 5 million made
it a significantly lightweight model as compared with the available
state-of-the-art models used for the foot ulcer segmentation task. Our
experiments presented results at the patch-level and image-level. Applied on
publicly available Foot Ulcer Segmentation (FUSeg) Challenge dataset from
MICCAI 2021, our model achieved state-of-the-art image-level performance of
88.22% in terms of Dice similarity score and ranked second in the official
challenge leaderboard. We also showed an extremely simple solution that could
be compared against the more advanced architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Perspective Decoupled Heatmaps for 3D Robot Pose Estimation from Depth Maps. (arXiv:2207.02519v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02519">
<div class="article-summary-box-inner">
<span><p>Knowing the exact 3D location of workers and robots in a collaborative
environment enables several real applications, such as the detection of unsafe
situations or the study of mutual interactions for statistical and social
purposes. In this paper, we propose a non-invasive and light-invariant
framework based on depth devices and deep neural networks to estimate the 3D
pose of robots from an external camera. The method can be applied to any robot
without requiring hardware access to the internal states. We introduce a novel
representation of the predicted pose, namely Semi-Perspective Decoupled
Heatmaps (SPDH), to accurately compute 3D joint locations in world coordinates
adapting efficient deep networks designed for the 2D Human Pose Estimation. The
proposed approach, which takes as input a depth representation based on XYZ
coordinates, can be trained on synthetic depth data and applied to real-world
settings without the need for domain adaptation techniques. To this end, we
present the SimBa dataset, based on both synthetic and real depth images, and
use it for the experimental evaluation. Results show that the proposed
approach, made of a specific depth map representation and the SPDH, overcomes
the current state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation through Shape Modeling for Medical Image Segmentation. (arXiv:2207.02529v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02529">
<div class="article-summary-box-inner">
<span><p>Shape information is a strong and valuable prior in segmenting organs in
medical images. However, most current deep learning based segmentation
algorithms have not taken shape information into consideration, which can lead
to bias towards texture. We aim at modeling shape explicitly and using it to
help medical image segmentation. Previous methods proposed Variational
Autoencoder (VAE) based models to learn the distribution of shape for a
particular organ and used it to automatically evaluate the quality of a
segmentation prediction by fitting it into the learned shape distribution.
Based on which we aim at incorporating VAE into current segmentation pipelines.
Specifically, we propose a new unsupervised domain adaptation pipeline based on
a pseudo loss and a VAE reconstruction loss under a teacher-student learning
paradigm. Both losses are optimized simultaneously and, in return, boost the
segmentation task performance. Extensive experiments on three public Pancreas
segmentation datasets as well as two in-house Pancreas segmentation datasets
show consistent improvements with at least 2.8 points gain in the Dice score,
demonstrating the effectiveness of our method in challenging unsupervised
domain adaptation scenarios for medical image segmentation. We hope this work
will advance shape analysis and geometric learning in medical imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Regularized Multi-Scale Feature Flow for High Dynamic Range Imaging. (arXiv:2207.02539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02539">
<div class="article-summary-box-inner">
<span><p>Reconstructing ghosting-free high dynamic range (HDR) images of dynamic
scenes from a set of multi-exposure images is a challenging task, especially
with large object motion and occlusions, leading to visible artifacts using
existing methods. To address this problem, we propose a deep network that tries
to learn multi-scale feature flow guided by the regularized loss. It first
extracts multi-scale features and then aligns features from non-reference
images. After alignment, we use residual channel attention blocks to merge the
features from different images. Extensive qualitative and quantitative
comparisons show that our approach achieves state-of-the-art performance and
produces excellent results where color artifacts and geometric distortions are
significantly reduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection. (arXiv:2207.02541v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02541">
<div class="article-summary-box-inner">
<span><p>To date, the most powerful semi-supervised object detectors (SS-OD) are based
on pseudo-boxes, which need a sequence of post-processing with fine-tuned
hyper-parameters. In this work, we propose replacing the sparse pseudo-boxes
with the dense prediction as a united and straightforward form of pseudo-label.
Compared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any
post-processing method, thus retaining richer information. We also introduce a
region selection technique to highlight the key information while suppressing
the noise carried by dense labels. We name our proposed SS-OD algorithm that
leverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows
superior performance under various settings compared with the pseudo-box-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound. (arXiv:2207.02549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02549">
<div class="article-summary-box-inner">
<span><p>Accurate and consistent predictions of echocardiography parameters are
important for cardiovascular diagnosis and treatment. In particular,
segmentations of the left ventricle can be used to derive ventricular volume,
ejection fraction (EF) and other relevant measurements. In this paper we
propose a new automated method called EchoGraphs for predicting ejection
fraction and segmenting the left ventricle by detecting anatomical keypoints.
Models for direct coordinate regression based on Graph Convolutional Networks
(GCNs) are used to detect the keypoints. GCNs can learn to represent the
cardiac shape based on local appearance of each keypoint, as well as global
spatial and temporal structures of all keypoints combined. We evaluate our
EchoGraphs model on the EchoNet benchmark dataset. Compared to semantic
segmentation, GCNs show accurate segmentation and improvements in robustness
and inference runtime. EF is computed simultaneously to segmentations and our
method also obtains state-of-the-art ejection fraction estimation. Source code
is available online: https://github.com/guybenyosef/EchoGraphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is the U-Net Directional-Relationship Aware?. (arXiv:2207.02574v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02574">
<div class="article-summary-box-inner">
<span><p>CNNs are often assumed to be capable of using contextual information about
distinct objects (such as their directional relations) inside their receptive
field. However, the nature and limits of this capacity has never been explored
in full. We explore a specific type of relationship~-- directional~-- using a
standard U-Net trained to optimize a cross-entropy loss function for
segmentation. We train this network on a pretext segmentation task requiring
directional relation reasoning for success and state that, with enough data and
a sufficiently large receptive field, it succeeds to learn the proposed task.
We further explore what the network has learned by analysing scenarios where
the directional relationships are perturbed, and show that the network has
learned to reason using these relationships.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIC 4th Challenge: Semantic-Assisted Multi-Feature Encoding and Multi-Head Decoding for Dense Video Captioning. (arXiv:2207.02583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02583">
<div class="article-summary-box-inner">
<span><p>The task of Dense Video Captioning (DVC) aims to generate captions with
timestamps for multiple events in one video. Semantic information plays an
important role for both localization and description of DVC. We present a
semantic-assisted dense video captioning model based on the encoding-decoding
framework. In the encoding stage, we design a concept detector to extract
semantic information, which is then fused with multi-modal visual features to
sufficiently represent the input video. In the decoding stage, we design a
classification head, paralleled with the localization and captioning heads, to
provide semantic supervision. Our method achieves significant improvements on
the YouMakeup dataset under DVC evaluation metrics and achieves high
performance in the Makeup Dense Video Captioning (MDVC) task of PIC 4th
Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling. (arXiv:2207.02595v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02595">
<div class="article-summary-box-inner">
<span><p>Current deep video quality assessment (VQA) methods are usually with high
computational costs when evaluating high-resolution videos. This cost hinders
them from learning better video-quality-related representations via end-to-end
training. Existing approaches typically consider naive sampling to reduce the
computational cost, such as resizing and cropping. However, they obviously
corrupt quality-related information in videos and are thus not optimal for
learning good representations for VQA. Therefore, there is an eager need to
design a new quality-retained sampling scheme for VQA. In this paper, we
propose Grid Mini-patch Sampling (GMS), which allows consideration of local
quality by sampling patches at their raw resolution and covers global quality
with contextual relations via mini-patches sampled in uniform grids. These
mini-patches are spliced and aligned temporally, named as fragments. We further
build the Fragment Attention Network (FANet) specially designed to accommodate
fragments as inputs. Consisting of fragments and FANet, the proposed FrAgment
Sample Transformer for VQA (FAST-VQA) enables efficient end-to-end deep VQA and
learns effective video-quality-related representations. It improves
state-of-the-art accuracy by around 10% while reducing 99.5% FLOPs on 1080P
high-resolution videos. The newly learned video-quality-related representations
can also be transferred into smaller VQA datasets, boosting performance in
these scenarios. Extensive experiments show that FAST-VQA has good performance
on inputs of various resolutions while retaining high efficiency. We publish
our code at https://github.com/timothyhtimothy/FAST-VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting is not Understanding: Recognizing and Addressing Underspecification in Machine Learning. (arXiv:2207.02598v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02598">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) models are typically optimized for their accuracy on a
given dataset. However, this predictive criterion rarely captures all desirable
properties of a model, in particular how well it matches a domain expert's
understanding of a task. Underspecification refers to the existence of multiple
models that are indistinguishable in their in-domain accuracy, even though they
differ in other desirable properties such as out-of-distribution (OOD)
performance. Identifying these situations is critical for assessing the
reliability of ML models.
</p>
<p>We formalize the concept of underspecification and propose a method to
identify and partially address it. We train multiple models with an
independence constraint that forces them to implement different functions. They
discover predictive features that are otherwise ignored by standard empirical
risk minimization (ERM), which we then distill into a global model with
superior OOD performance. Importantly, we constrain the models to align with
the data manifold to ensure that they discover meaningful features. We
demonstrate the method on multiple datasets in computer vision (collages,
WILDS-Camelyon17, GQA) and discuss general implications of underspecification.
Most notably, in-domain performance cannot serve for OOD model selection
without additional assumptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GFNet: Geometric Flow Network for 3D Point Cloud Semantic Segmentation. (arXiv:2207.02605v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02605">
<div class="article-summary-box-inner">
<span><p>Point cloud semantic segmentation from projected views, such as range-view
(RV) and bird's-eye-view (BEV), has been intensively investigated. Different
views capture different information of point clouds and thus are complementary
to each other. However, recent projection-based methods for point cloud
semantic segmentation usually utilize a vanilla late fusion strategy for the
predictions of different views, failing to explore the complementary
information from a geometric perspective during the representation learning. In
this paper, we introduce a geometric flow network (GFNet) to explore the
geometric correspondence between different views in an align-before-fuse
manner. Specifically, we devise a novel geometric flow module (GFM) to
bidirectionally align and propagate the complementary information across
different views according to geometric relationships under the end-to-end
learning scheme. We perform extensive experiments on two widely used benchmark
datasets, SemanticKITTI and nuScenes, to demonstrate the effectiveness of our
GFNet for project-based point cloud semantic segmentation. Concretely, GFNet
not only significantly boosts the performance of each individual view but also
achieves state-of-the-art results over all existing projection-based models.
Code is available at \url{https://github.com/haibo-qiu/GFNet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseHybrid: Hybrid Anomaly Detection for Dense Open-set Recognition. (arXiv:2207.02606v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02606">
<div class="article-summary-box-inner">
<span><p>Anomaly detection can be conceived either through generative modelling of
regular training data or by discriminating with respect to negative training
data. These two approaches exhibit different failure modes. Consequently,
hybrid algorithms present an attractive research goal. Unfortunately, dense
anomaly detection requires translational equivariance and very large input
resolutions. These requirements disqualify all previous hybrid approaches to
the best of our knowledge. We therefore design a novel hybrid algorithm based
on reinterpreting discriminative logits as a logarithm of the unnormalized
joint distribution $\hat{p}(\mathbf{x}, \mathbf{y})$. Our model builds on a
shared convolutional representation from which we recover three dense
predictions: i) the closed-set class posterior $P(\mathbf{y}|\mathbf{x})$, ii)
the dataset posterior $P(d_{in}|\mathbf{x})$, iii) unnormalized data likelihood
$\hat{p}(\mathbf{x})$. The latter two predictions are trained both on the
standard training data and on a generic negative dataset. We blend these two
predictions into a hybrid anomaly score which allows dense open-set recognition
on large natural images. We carefully design a custom loss for the data
likelihood in order to avoid backpropagation through the untractable
normalizing constant $Z(\theta)$. Experiments evaluate our contributions on
standard dense anomaly detection benchmarks as well as in terms of open-mIoU -
a novel metric for dense open-set performance. Our submissions achieve
state-of-the-art performance despite neglectable computational overhead over
the standard semantic segmentation baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VMRF: View Matching Neural Radiance Fields. (arXiv:2207.02621v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02621">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) have demonstrated very impressive performance
in novel view synthesis via implicitly modelling 3D representations from
multi-view 2D images. However, most existing studies train NeRF models with
either reasonable camera pose initialization or manually-crafted camera pose
distributions which are often unavailable or hard to acquire in various
real-world data. We design VMRF, an innovative view matching NeRF that enables
effective NeRF training without requiring prior knowledge in camera poses or
camera pose distributions. VMRF introduces a view matching scheme, which
exploits unbalanced optimal transport to produce a feature transport plan for
mapping a rendered image with randomly initialized camera pose to the
corresponding real image. With the feature transport plan as the guidance, a
novel pose calibration technique is designed which rectifies the initially
randomized camera poses by predicting relative pose transformations between the
pair of rendered and real images. Extensive experiments over a number of
synthetic and real datasets show that the proposed VMRF outperforms the
state-of-the-art qualitatively and quantitatively by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowing Earlier what Right Means to You: A Comprehensive VQA Dataset for Grounding Relative Directions via Multi-Task Learning. (arXiv:2207.02624v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02624">
<div class="article-summary-box-inner">
<span><p>Spatial reasoning poses a particular challenge for intelligent agents and is
at the same time a prerequisite for their successful interaction and
communication in the physical world. One such reasoning task is to describe the
position of a target object with respect to the intrinsic orientation of some
reference object via relative directions. In this paper, we introduce
GRiD-A-3D, a novel diagnostic visual question-answering (VQA) dataset based on
abstract objects. Our dataset allows for a fine-grained analysis of end-to-end
VQA models' capabilities to ground relative directions. At the same time, model
training requires considerably fewer computational resources compared with
existing datasets, yet yields a comparable or even higher performance. Along
with the new dataset, we provide a thorough evaluation based on two widely
known end-to-end VQA architectures trained on GRiD-A-3D. We demonstrate that
within a few epochs, the subtasks required to reason over relative directions,
such as recognizing and locating objects in a scene and estimating their
intrinsic orientations, are learned in the order in which relative directions
are intuitively processed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Difference in Euclidean Norm Can Cause Semantic Divergence in Batch Normalization. (arXiv:2207.02625v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02625">
<div class="article-summary-box-inner">
<span><p>In this paper, we show that the difference in Euclidean norm of samples can
make a contribution to the semantic divergence and even confusion, after the
spatial translation and scaling transformation in batch normalization. To
address this issue, we propose an intuitive but effective method to equalize
the Euclidean norms of sample vectors. Concretely, we $l_2$-normalize each
sample vector before batch normalization, and therefore the sample vectors are
of the same magnitude. Since the proposed method combines the $l_2$
normalization and batch normalization, we name our method as $L_2$BN. The
$L_2$BN can strengthen the compactness of intra-class features and enlarge the
discrepancy of inter-class features. In addition, it can help the gradient
converge to a stable scale. The $L_2$BN is easy to implement and can exert its
effect without any additional parameters and hyper-parameters. Therefore, it
can be used as a basic normalization method for neural networks. We evaluate
the effectiveness of $L_2$BN through extensive experiments with various models
on image classification and acoustic scene classification tasks. The
experimental results demonstrate that the $L_2$BN is able to boost the
generalization ability of various neural network models and achieve
considerable performance improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Sensing Attention Network for Video-based Person Re-identification. (arXiv:2207.02631v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02631">
<div class="article-summary-box-inner">
<span><p>Video-based person re-identification (ReID) is challenging due to the
presence of various interferences in video frames. Recent approaches handle
this problem using temporal aggregation strategies. In this work, we propose a
novel Context Sensing Attention Network (CSA-Net), which improves both the
frame feature extraction and temporal aggregation steps. First, we introduce
the Context Sensing Channel Attention (CSCA) module, which emphasizes responses
from informative channels for each frame. These informative channels are
identified with reference not only to each individual frame, but also to the
content of the entire sequence. Therefore, CSCA explores both the individuality
of each frame and the global context of the sequence. Second, we propose the
Contrastive Feature Aggregation (CFA) module, which predicts frame weights for
temporal aggregation. Here, the weight for each frame is determined in a
contrastive manner: i.e., not only by the quality of each individual frame, but
also by the average quality of the other frames in a sequence. Therefore, it
effectively promotes the contribution of relatively good frames. Extensive
experimental results on four datasets show that CSA-Net consistently achieves
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network Pruning via Feature Shift Minimization. (arXiv:2207.02632v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02632">
<div class="article-summary-box-inner">
<span><p>Channel pruning is widely used to reduce the complexity of deep network
models. Recent pruning methods usually identify which parts of the network to
discard by proposing a channel importance criterion. However, recent studies
have shown that these criteria do not work well in all conditions. In this
paper, we propose a novel Feature Shift Minimization (FSM) method to compress
CNN models, which evaluates the feature shift by converging the information of
both features and filters. Specifically, we first investigate the compression
efficiency with some prevalent methods in different layer-depths and then
propose the feature shift concept. Then, we introduce an approximation method
to estimate the magnitude of the feature shift, since it is difficult to
compute it directly. Besides, we present a distribution-optimization algorithm
to compensate for the accuracy loss and improve the network compression
efficiency. The proposed method yields state-of-the-art performance on various
benchmark networks and datasets, verified by extensive experiments. The codes
can be available at \url{https://github.com/lscgx/FSM}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness of Visual Dialog. (arXiv:2207.02639v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02639">
<div class="article-summary-box-inner">
<span><p>Adversarial robustness evaluates the worst-case performance scenario of a
machine learning model to ensure its safety and reliability. This study is the
first to investigate the robustness of visually grounded dialog models towards
textual attacks. These attacks represent a worst-case scenario where the input
question contains a synonym which causes the previously correct model to return
a wrong answer. Using this scenario, we first aim to understand how multimodal
input components contribute to model robustness. Our results show that models
which encode dialog history are more robust, and when launching an attack on
history, model prediction becomes more uncertain. This is in contrast to prior
work which finds that dialog history is negligible for model performance on
this task. We also evaluate how to generate adversarial test examples which
successfully fool the model but remain undetected by the user/software
designer. We find that the textual, as well as the visual context are important
to generate plausible worst-case scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaze-Vergence-Controlled See-Through Vision in Augmented Reality. (arXiv:2207.02645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02645">
<div class="article-summary-box-inner">
<span><p>Augmented Reality (AR) see-through vision is an interesting research topic
since it enables users to see through a wall and see the occluded objects. Most
existing research focuses on the visual effects of see-through vision, while
the interaction method is less studied. However, we argue that using common
interaction modalities, e.g., midair click and speech, may not be the optimal
way to control see-through vision. This is because when we want to see through
something, it is physically related to our gaze depth/vergence and thus should
be naturally controlled by the eyes. Following this idea, this paper proposes a
novel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since
gaze depth is needed, we build a gaze tracking module with two infrared cameras
and the corresponding algorithm and assemble it into the Microsoft HoloLens 2
to achieve gaze depth estimation. We then propose two different GVC modes for
see-through vision to fit different scenarios. Extensive experimental results
demonstrate that our gaze depth estimation is efficient and accurate. By
comparing with conventional interaction modalities, our GVC techniques are also
shown to be superior in terms of efficiency and more preferred by users.
Finally, we present four example applications of gaze-vergence-controlled
see-through vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Quality Assessment of Omnidirectional Images. (arXiv:2207.02674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02674">
<div class="article-summary-box-inner">
<span><p>Omnidirectional images and videos can provide immersive experience of
real-world scenes in Virtual Reality (VR) environment. We present a perceptual
omnidirectional image quality assessment (IQA) study in this paper since it is
extremely important to provide a good quality of experience under the VR
environment. We first establish an omnidirectional IQA (OIQA) database, which
includes 16 source images and 320 distorted images degraded by 4 commonly
encountered distortion types, namely JPEG compression, JPEG2000 compression,
Gaussian blur and Gaussian noise. Then a subjective quality evaluation study is
conducted on the OIQA database in the VR environment. Considering that humans
can only see a part of the scene at one movement in the VR environment, visual
attention becomes extremely important. Thus we also track head and eye movement
data during the quality rating experiments. The original and distorted
omnidirectional images, subjective quality ratings, and the head and eye
movement data together constitute the OIQA database. State-of-the-art
full-reference (FR) IQA measures are tested on the OIQA database, and some new
observations different from traditional IQA are made.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Team PKU-WICT-MIPL PIC Makeup Temporal Video Grounding Challenge 2022 Technical Report. (arXiv:2207.02687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02687">
<div class="article-summary-box-inner">
<span><p>In this technical report, we briefly introduce the solutions of our team
`PKU-WICT-MIPL' for the PIC Makeup Temporal Video Grounding (MTVG) Challenge in
ACM-MM 2022. Given an untrimmed makeup video and a step query, the MTVG aims to
localize a temporal moment of the target makeup step in the video. To tackle
this task, we propose a phrase relationship mining framework to exploit the
temporal localization relationship relevant to the fine-grained phrase and the
whole sentence. Besides, we propose to constrain the localization results of
different step sentence queries to not overlap with each other through a
dynamic programming algorithm. The experimental results demonstrate the
effectiveness of our method. Our final submission ranked 2nd on the
leaderboard, with only a 0.55\% gap from the first.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. (arXiv:2207.02696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02696">
<div class="article-summary-box-inner">
<span><p>YOLOv7 surpasses all known object detectors in both speed and accuracy in the
range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all
known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6
object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based
detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed
and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask
R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as
well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR,
Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors
in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from
scratch without using any other datasets or pre-trained weights. Source code is
released in https://github.com/WongKinYiu/yolov7.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spike Calibration: Fast and Accurate Conversion of Spiking Neural Network for Object Detection and Segmentation. (arXiv:2207.02702v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02702">
<div class="article-summary-box-inner">
<span><p>Spiking neural network (SNN) has been attached to great importance due to the
properties of high biological plausibility and low energy consumption on
neuromorphic hardware. As an efficient method to obtain deep SNN, the
conversion method has exhibited high performance on various large-scale
datasets. However, it typically suffers from severe performance degradation and
high time delays. In particular, most of the previous work focuses on simple
classification tasks while ignoring the precise approximation to ANN output. In
this paper, we first theoretically analyze the conversion errors and derive the
harmful effects of time-varying extremes on synaptic currents. We propose the
Spike Calibration (SpiCalib) to eliminate the damage of discrete spikes to the
output distribution and modify the LIPooling to allow conversion of the
arbitrary MaxPooling layer losslessly. Moreover, Bayesian optimization for
optimal normalization parameters is proposed to avoid empirical settings. The
experimental results demonstrate the state-of-the-art performance on
classification, object detection, and segmentation tasks. To the best of our
knowledge, this is the first time to obtain SNN comparable to ANN on these
tasks simultaneously. Moreover, we only need 1/50 inference time of the
previous work on the detection task and can achieve the same performance under
0.492$\times$ energy consumption of ANN on the segmentation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Histopathology DatasetGAN: Synthesizing Large-Resolution Histopathology Datasets. (arXiv:2207.02712v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02712">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) methods are enabling an increasing number of
deep learning models to be trained on image datasets in domains where labels
are difficult to obtain. These methods, however, struggle to scale to the high
resolution of medical imaging datasets, where they are critical for achieving
good generalization on label-scarce medical image datasets. In this work, we
propose the Histopathology DatasetGAN (HDGAN) framework, an extension of the
DatasetGAN semi-supervised framework for image generation and segmentation that
scales well to large-resolution histopathology images. We make several
adaptations from the original framework, including updating the generative
backbone, selectively extracting latent features from the generator, and
switching to memory-mapped arrays. These changes reduce the memory consumption
of the framework, improving its applicability to medical imaging domains. We
evaluate HDGAN on a thrombotic microangiopathy high-resolution tile dataset,
demonstrating strong performance on the high-resolution image-annotation
generation task. We hope that this work enables more application of deep
learning models to medical datasets, in addition to encouraging more
exploration of self-supervised frameworks within the medical imaging domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes. (arXiv:2207.02715v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02715">
<div class="article-summary-box-inner">
<span><p>We present a novel approach to efficiently compute tight non-convex
enclosures of the image through neural networks with ReLU, sigmoid, or
hyperbolic tangent activation functions. In particular, we abstract the
input-output relation of each neuron by a polynomial approximation, which is
evaluated in a set-based manner using polynomial zonotopes. Our proposed method
is especially well suited for reachability analysis of neural network
controlled systems since polynomial zonotopes are able to capture the
non-convexity in both, the image through the neural network as well as the
reachable set. We demonstrate the superior performance of our approach compared
to other state of the art methods on various benchmark systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning approach for Classifying Trusses and Runners of Strawberries. (arXiv:2207.02721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02721">
<div class="article-summary-box-inner">
<span><p>The use of artificial intelligence in the agricultural sector has been
growing at a rapid rate to automate farming activities. Emergent farming
technologies focus on mapping and classification of plants, fruits, diseases,
and soil types. Although, assisted harvesting and pruning applications using
deep learning algorithms are in the early development stages, there is a demand
for solutions to automate such processes. This paper proposes the use of Deep
Learning for the classification of trusses and runners of strawberry plants
using semantic segmentation and dataset augmentation. The proposed approach is
based on the use of noises (i.e. Gaussian, Speckle, Poisson and
Salt-and-Pepper) to artificially augment the dataset and compensate the low
number of data samples and increase the overall classification performance. The
results are evaluated using mean average of precision, recall and F1 score. The
proposed approach achieved 91\%, 95\% and 92\% on precision, recall and F1
score, respectively, for truss detection using the ResNet101 with dataset
augmentation utilising Salt-and-Pepper noise; and 83\%, 53\% and 65\% on
precision, recall and F1 score, respectively, for truss detection using the
ResNet50 with dataset augmentation utilising Poisson noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Gesture Recognition with Virtual Glove Markers. (arXiv:2207.02729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02729">
<div class="article-summary-box-inner">
<span><p>Due to the universal non-verbal natural communication approach that allows
for effective communication between humans, gesture recognition technology has
been steadily developing over the previous few decades. Many different
strategies have been presented in research articles based on gesture
recognition to try to create an effective system to send non-verbal natural
communication information to computers, using both physical sensors and
computer vision. Hyper accurate real-time systems, on the other hand, have only
recently began to occupy the study field, with each adopting a range of
methodologies due to past limits such as usability, cost, speed, and accuracy.
A real-time computer vision-based human-computer interaction tool for gesture
recognition applications that acts as a natural user interface is proposed.
Virtual glove markers on users hands will be created and used as input to a
deep learning model for the real-time recognition of gestures. The results
obtained show that the proposed system would be effective in real-time
applications including social interaction through telepresence and
rehabilitation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding. (arXiv:2207.02756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02756">
<div class="article-summary-box-inner">
<span><p>In this technical report, we introduce our solution to human-centric
spatio-temporal video grounding task. We propose a concise and effective
framework named STVGFormer, which models spatiotemporal visual-linguistic
dependencies with a static branch and a dynamic branch. The static branch
performs cross-modal understanding in a single frame and learns to localize the
target object spatially according to intra-frame visual cues like object
appearances. The dynamic branch performs cross-modal understanding across
multiple frames. It learns to predict the starting and ending time of the
target moment according to dynamic visual cues like motions. Both the static
and dynamic branches are designed as cross-modal transformers. We further
design a novel static-dynamic interaction block to enable the static and
dynamic branches to transfer useful and complementary information from each
other, which is shown to be effective to improve the prediction on hard cases.
Our proposed method achieved 39.6% vIoU and won the first place in the HC-STVG
track of the 4th Person in Context Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Relighting of Real Scenes. (arXiv:2207.02774v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02774">
<div class="article-summary-box-inner">
<span><p>We introduce the task of local relighting, which changes a photograph of a
scene by switching on and off the light sources that are visible within the
image. This new task differs from the traditional image relighting problem, as
it introduces the challenge of detecting light sources and inferring the
pattern of light that emanates from them. We propose an approach for local
relighting that trains a model without supervision of any novel image dataset
by using synthetically generated image pairs from another model. Concretely, we
collect paired training images from a stylespace-manipulated GAN; then we use
these images to train a conditional image-to-image model. To benchmark local
relighting, we introduce Lonoff, a collection of 306 precisely aligned images
taken in indoor spaces with different combinations of lights switched on. We
show that our method significantly outperforms baseline methods based on GAN
inversion. Finally, we demonstrate extensions of our method that control
different light sources separately. We invite the community to tackle this new
task of local relighting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-receptive Focused Inference Network for Lightweight Image Super-Resolution. (arXiv:2207.02796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02796">
<div class="article-summary-box-inner">
<span><p>With the development of deep learning, single image super-resolution (SISR)
has achieved significant breakthroughs. Recently, methods to enhance the
performance of SISR networks based on global feature interactions have been
proposed. However, the capabilities of neurons that need to adjust their
function in response to the context dynamically are neglected. To address this
issue, we propose a lightweight Cross-receptive Focused Inference Network
(CFIN), a hybrid network composed of a Convolutional Neural Network (CNN) and a
Transformer. Specifically, a novel Cross-receptive Field Guide Transformer
(CFGT) is designed to adaptively modify the network weights by using modulated
convolution kernels combined with local representative semantic information. In
addition, a CNN-based Cross-scale Information Aggregation Module (CIAM) is
proposed to make the model better focused on potentially practical information
and improve the efficiency of the Transformer stage. Extensive experiments show
that our proposed CFIN is a lightweight and efficient SISR model, which can
achieve a good balance between computational cost and model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning. (arXiv:2207.02797v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02797">
<div class="article-summary-box-inner">
<span><p>The manifold hypothesis is a core mechanism behind the success of deep
learning, so understanding the intrinsic manifold structure of image data is
central to studying how neural networks learn from the data. Intrinsic dataset
manifolds and their relationship to learning difficulty have recently begun to
be studied for the common domain of natural images, but little such research
has been attempted for radiological images. We address this here. First, we
compare the intrinsic manifold dimensionality of radiological and natural
images. We also investigate the relationship between intrinsic dimensionality
and generalization ability over a wide range of datasets. Our analysis shows
that natural image datasets generally have a higher number of intrinsic
dimensions than radiological images. However, the relationship between
generalization ability and intrinsic dimensionality is much stronger for
medical images, which could be explained as radiological images having
intrinsic features that are more difficult to learn. These results give a more
principled underpinning for the intuition that radiological images can be more
challenging to apply deep learning to than natural image datasets common to
machine learning research. We believe rather than directly applying models
developed for natural images to the radiological imaging domain, more care
should be taken to developing architectures and algorithms that are more
tailored to the specific characteristics of this domain. The research shown in
our paper, demonstrating these characteristics and the differences from natural
images, is an important first step in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into Sequential Patches for Deepfake Detection. (arXiv:2207.02803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02803">
<div class="article-summary-box-inner">
<span><p>Recent advances in face forgery techniques produce nearly visually
untraceable deepfake videos, which could be leveraged with malicious
intentions. As a result, researchers have been devoted to deepfake detection.
Previous studies has identified the importance of local low-level cues and
temporal information in pursuit to generalize well across deepfake methods,
however, they still suffer from robustness problem against post-processings. In
this work, we propose the Local- &amp; Temporal-aware Transformer-based Deepfake
Detection (LTTD) framework, which adopts a local-to-global learning protocol
with a particular focus on the valuable temporal information within local
sequences. Specifically, we propose a Local Sequence Transformer (LST), which
models the temporal consistency on sequences of restricted spatial regions,
where low-level information is hierarchically enhanced with shallow layers of
learned 3D filters. Based on the local temporal embeddings, we then achieve the
final classification in a global contrastive way. Extensive experiments on
popular datasets validate that our approach effectively spots local forgery
cues and achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPODv2: Dense Correspondence-Based 6 DoF Pose Estimation. (arXiv:2207.02805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02805">
<div class="article-summary-box-inner">
<span><p>We propose a three-stage 6 DoF object detection method called DPODv2 (Dense
Pose Object Detector) that relies on dense correspondences. We combine a 2D
object detector with a dense correspondence estimation network and a multi-view
pose refinement method to estimate a full 6 DoF pose. Unlike other deep
learning methods that are typically restricted to monocular RGB images, we
propose a unified deep learning network allowing different imaging modalities
to be used (RGB or Depth). Moreover, we propose a novel pose refinement method,
that is based on differentiable rendering. The main concept is to compare
predicted and rendered correspondences in multiple views to obtain a pose which
is consistent with predicted correspondences in all views. Our proposed method
is evaluated rigorously on different data modalities and types of training data
in a controlled setup. The main conclusions is that RGB excels in
correspondence estimation, while depth contributes to the pose accuracy if good
3D-3D correspondences are available. Naturally, their combination achieves the
overall best performance. We perform an extensive evaluation and an ablation
study to analyze and validate the results on several challenging datasets.
DPODv2 achieves excellent results on all of them while still remaining fast and
scalable independent of the used data modality and the type of training data
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Object Pose Refinement With Differentiable Renderer. (arXiv:2207.02811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02811">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel multi-view 6 DoF object pose refinement
approach focusing on improving methods trained on synthetic data. It is based
on the DPOD detector, which produces dense 2D-3D correspondences between the
model vertices and the image pixels in each frame. We have opted for the use of
multiple frames with known relative camera transformations, as it allows
introduction of geometrical constraints via an interpretable ICP-like loss
function. The loss function is implemented with a differentiable renderer and
is optimized iteratively. We also demonstrate that a full detection and
refinement pipeline, which is trained solely on synthetic data, can be used for
auto-labeling real data. We perform quantitative evaluation on LineMOD,
Occlusion, Homebrewed and YCB-V datasets and report excellent performance in
comparison to the state-of-the-art methods trained on the synthetic and real
data. We demonstrate empirically that our approach requires only a few frames
and is robust to close camera locations and noise in extrinsic camera
calibration, making its practical usage easier and more ubiquitous.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Counterfactual Image Manipulation via CLIP. (arXiv:2207.02812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02812">
<div class="article-summary-box-inner">
<span><p>Leveraging StyleGAN's expressivity and its disentangled latent codes,
existing methods can achieve realistic editing of different visual attributes
such as age and gender of facial images. An intriguing yet challenging problem
arises: Can generative models achieve counterfactual editing against their
learnt priors? Due to the lack of counterfactual samples in natural datasets,
we investigate this problem in a text-driven manner with
Contrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic
knowledge even for various counterfactual concepts. Different from in-domain
manipulation, counterfactual manipulation requires more comprehensive
exploitation of semantic knowledge encapsulated in CLIP as well as more
delicate handling of editing directions for avoiding being stuck in local
minimum or undesired editing. To this end, we design a novel contrastive loss
that exploits predefined CLIP-space directions to guide the editing toward
desired directions from different perspectives. In addition, we design a simple
yet effective scheme that explicitly maps CLIP embeddings (of target text) to
the latent space and fuses them with latent codes for effective latent code
optimization and accurate editing. Extensive experiments show that our design
achieves accurate and realistic editing while driving by target texts with
various counterfactual concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localization Uncertainty Estimation for Anchor-Free Object Detection. (arXiv:2006.15607v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.15607">
<div class="article-summary-box-inner">
<span><p>Since many safety-critical systems, such as surgical robots and autonomous
driving cars operate in unstable environments with sensor noise and incomplete
data, it is desirable for object detectors to take the localization uncertainty
into account. However, there are several limitations of the existing
uncertainty estimation methods for anchor-based object detection. 1) They model
the uncertainty of the heterogeneous object properties with different
characteristics and scales, such as location (center point) and scale (width,
height), which could be difficult to estimate. 2) They model box offsets as
Gaussian distributions, which is not compatible with the ground truth bounding
boxes that follow the Dirac delta distribution. 3) Since anchor-based methods
are sensitive to anchor hyper-parameters, their localization uncertainty could
also be highly sensitive to the choice of hyper-parameters. To tackle these
limitations, we propose a new localization uncertainty estimation method called
UAD for anchor-free object detection. Our method captures the uncertainty in
four directions of box offsets (left, right, top, bottom) that are homogeneous,
so that it can tell which direction is uncertain, and provide a quantitative
value of uncertainty in [0, 1]. To enable such uncertainty estimation, we
design a new uncertainty loss, negative power log-likelihood loss, to measure
the localization uncertainty by weighting the likelihood loss by its IoU, which
alleviates the model misspecification problem. Furthermore, we propose an
uncertainty-aware focal loss for reflecting the estimated uncertainty to the
classification score. Experimental results on COCO datasets demonstrate that
our method significantly improves FCOS, by up to 1.8 points, without
sacrificing computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal Processing. (arXiv:2104.00253v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00253">
<div class="article-summary-box-inner">
<span><p>Camera Image Signal Processing(ISP) pipelines, including deep learning
trained versions, can get appealing results in different image signal
processing tasks. However, most if not all of these methods tend to apply a
single filter that is homogeneous over the entire image. This is also
particularly true when an encoder-decoder type deep architecture is trained for
the task. However, it is natural to view a camera image as heterogeneous, as
the color intensity and the artificial noise are distributed vastly different,
even across the two dimensional domain of a single image. Varied Moire ringing,
motion-blur, color-bleaching or lens based projection distortions can all
potentially lead to a heterogeneous image artifact filtering problem. In this
paper, we present a specific patch-based, local subspace deep neural network
that improves Camera ISP to be robust to heterogeneous artifacts (especially
image denoising). We call our three-fold deep trained model the Patch Subspace
Learning Autoencoder (PSL-AE). PSL-AE does not necessarily assume uniform image
distortion levels nor repeated nor similar artifact types within the image.
Rather, PSL-AE first diagnostically encodes patches extracted from noisy and
clean image pairs, with different artifact type and distortion levels, by
contrastive learning. Then, each image's patches are encoded into soft-clusters
in their appropriate latent sub-space, using a prior mixture model. Lastly, the
decoders of the PSL-AE are also trained in an unsupervised manner customized
for the image patches in each soft-cluster. Our experimental results
demonstrates the flexibility and performance that one can achieve through
improved heterogeneous filtering, both from synthesized artifacts but also
realistic SIDD image pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Detransformation Autoencoder for Representation Learning in Open Set Recognition. (arXiv:2105.13557v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13557">
<div class="article-summary-box-inner">
<span><p>The objective of Open set recognition (OSR) is to learn a classifier that can
reject the unknown samples while classifying the known classes accurately. In
this paper, we propose a self-supervision method, Detransformation Autoencoder
(DTAE), for the OSR problem. This proposed method engages in learning
representations that are invariant to the transformations of the input data.
Experiments on several standard image datasets indicate that the pre-training
process significantly improves the model performance in the OSR tasks.
Meanwhile, our proposed self-supervision method achieves significant gains in
detecting the unknown class and classifying the known classes. Moreover, our
analysis indicates that DTAE can yield representations that contain more target
class information and less transformation information than RotNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Wake-up: 3D Object Rigging from a Single Image. (arXiv:2108.02708v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02708">
<div class="article-summary-box-inner">
<span><p>Given a single image of a general object such as a chair, could we also
restore its articulated 3D shape similar to human modeling, so as to animate
its plausible articulations and diverse motions? This is an interesting new
question that may have numerous downstream augmented reality and virtual
reality applications. Comparing with previous efforts on object manipulation,
our work goes beyond 2D manipulation and rigid deformation, and involves
articulated manipulation. To achieve this goal, we propose an automated
approach to build such 3D generic objects from single images and embed
articulated skeletons in them. Specifically, our framework starts by
reconstructing the 3D object from an input image. Afterwards, to extract
skeletons for generic 3D objects, we develop a novel skeleton prediction method
with a multi-head structure for skeleton probability field estimation by
utilizing the deep implicit functions. A dataset of generic 3D objects with
ground-truth annotated skeletons is collected. Empirically our approach is
demonstrated with satisfactory performance on public datasets as well as our
in-house dataset; our results surpass those of the state-of-the-arts by a
noticeable margin on both 3D reconstruction and skeleton prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DexMV: Imitation Learning for Dexterous Manipulation from Human Videos. (arXiv:2108.05877v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05877">
<div class="article-summary-box-inner">
<span><p>While significant progress has been made on understanding hand-object
interactions in computer vision, it is still very challenging for robots to
perform complex dexterous manipulation. In this paper, we propose a new
platform and pipeline DexMV (Dexterous Manipulation from Videos) for imitation
learning. We design a platform with: (i) a simulation system for complex
dexterous manipulation tasks with a multi-finger robot hand and (ii) a computer
vision system to record large-scale demonstrations of a human hand conducting
the same tasks. In our novel pipeline, we extract 3D hand and object poses from
videos, and propose a novel demonstration translation method to convert human
motion to robot demonstrations. We then apply and benchmark multiple imitation
learning algorithms with the demonstrations. We show that the demonstrations
can indeed improve robot learning by a large margin and solve the complex tasks
which reinforcement learning alone cannot solve. More details can be found in
the project page: https://yzqin.github.io/dexmv
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks. (arXiv:2110.05668v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05668">
<div class="article-summary-box-inner">
<span><p>Most existing neural architecture search (NAS) benchmarks and algorithms
prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet.
This makes the performance of NAS approaches in more diverse areas poorly
understood. In this paper, we present NAS-Bench-360, a benchmark suite to
evaluate methods on domains beyond those traditionally studied in architecture
search, and use it to address the following question: do state-of-the-art NAS
methods perform well on diverse tasks? To construct the benchmark, we curate
ten tasks spanning a diverse array of application domains, dataset sizes,
problem dimensionalities, and learning objectives. Each task is carefully
chosen to interoperate with modern CNN-based search methods while possibly
being far-afield from its original development domain. To speed up and reduce
the cost of NAS research, for two of the tasks we release the precomputed
performance of 15,625 architectures comprising a standard CNN search space.
Experimentally, we show the need for more robust NAS evaluation of the kind
NAS-Bench-360 enables by showing that several modern NAS procedures perform
inconsistently across the ten tasks, with many catastrophically poor results.
We also demonstrate how NAS-Bench-360 and its associated precomputed results
will enable future scientific discoveries by testing whether several recent
hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360
is hosted at https://nb360.ml.cmu.edu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14051">
<div class="article-summary-box-inner">
<span><p>Machine learning models often encounter samples that are diverged from the
training distribution. Failure to recognize an out-of-distribution (OOD)
sample, and consequently assign that sample to an in-class label significantly
compromises the reliability of a model. The problem has gained significant
attention due to its importance for safety deploying models in open-world
settings. Detecting OOD samples is challenging due to the intractability of
modeling all possible unknown distributions. To date, several research domains
tackle the problem of detecting unfamiliar samples, including anomaly
detection, novelty detection, one-class learning, open set recognition, and
out-of-distribution detection. Despite having similar and shared concepts,
out-of-distribution, open-set, and anomaly detection have been investigated
independently. Accordingly, these research avenues have not cross-pollinated,
creating research barriers. While some surveys intend to provide an overview of
these approaches, they seem to only focus on a specific domain without
examining the relationship between different domains. This survey aims to
provide a cross-domain and comprehensive review of numerous eminent works in
respective areas while identifying their commonalities. Researchers can benefit
from the overview of research advances in different fields and develop future
methodology synergistically. Furthermore, to the best of our knowledge, while
there are surveys in anomaly detection or one-class learning, there is no
comprehensive or up-to-date survey on out-of-distribution detection, which our
survey covers extensively. Finally, having a unified cross-domain perspective,
we discuss and shed light on future lines of research, intending to bring these
fields closer together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Mask: Real-World Universal Adversarial Attack on Face Recognition Models. (arXiv:2111.10759v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10759">
<div class="article-summary-box-inner">
<span><p>Deep learning-based facial recognition (FR) models have demonstrated
state-of-the-art performance in the past few years, even when wearing
protective medical face masks became commonplace during the COVID-19 pandemic.
Given the outstanding performance of these models, the machine learning
research community has shown increasing interest in challenging their
robustness. Initially, researchers presented adversarial attacks in the digital
domain, and later the attacks were transferred to the physical domain. However,
in many cases, attacks in the physical domain are conspicuous, and thus may
raise suspicion in real-world environments (e.g., airports). In this paper, we
propose Adversarial Mask, a physical universal adversarial perturbation (UAP)
against state-of-the-art FR models that is applied on face masks in the form of
a carefully crafted pattern. In our experiments, we examined the
transferability of our adversarial mask to a wide range of FR model
architectures and datasets. In addition, we validated our adversarial mask's
effectiveness in real-world experiments (CCTV use case) by printing the
adversarial pattern on a fabric face mask. In these experiments, the FR system
was only able to identify 3.34% of the participants wearing the mask (compared
to a minimum of 83.34% with other evaluated masks). A demo of our experiments
can be found at: https://youtu.be/_TXkDO5z11w.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11191">
<div class="article-summary-box-inner">
<span><p>The paper presents a method of a Convolutional Neural Networks (CNN) model
for image classification with image preprocessing and hyperparameters tuning,
aiming at increasing the predictive performance for COVID-19 diagnosis while
avoiding deeper and thus more complex alternatives. Firstly, the CNN model
includes four similar convolutional layers followed by a flattening and two
dense layers. This work proposes a less complex solution based on simply
classifying 2D slices of CT scans using a CNN model. Despite the simplicity in
architecture, the proposed CNN model showed improved quantitative results
exceeding state-of-the-art on the dataset of images, in terms of the macro F1
score. The results were achieved on the original CT slices of the dataset.
Secondly, the original dataset was processed via anatomy-relevant masking of
slice, removing none-representative slices from the CT volume, and
hyperparameters tuning. For slice processing, a fixed-sized rectangular area
was used for cropping an anatomy-relevant region-of-interest in the images, and
a threshold based on the number of white pixels in binarized slices was
employed to remove none-representative slices from the 3D-CT scans. The CNN
model with a learning rate schedule and an exponential decay and slice flipping
techniques was deployed on the processed slices. The proposed method was used
to make predictions on the 2D slices. For final diagnosis at patient level,
majority voting was applied on the slices of each CT scan to take the
diagnosis. The macro F1 score of the proposed method well-exceeded the baseline
approach and other alternatives on the validation set as well as on a test
partition of previously unseen images from COV-19CT-DB dataset partitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Discriminative Shrinkage Deep Networks for Image Deconvolution. (arXiv:2111.13876v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13876">
<div class="article-summary-box-inner">
<span><p>Most existing methods usually formulate the non-blind deconvolution problem
into a maximum-a-posteriori framework and address it by manually designing
kinds of regularization terms and data terms of the latent clear images.
However, explicitly designing these two terms is quite challenging and usually
leads to complex optimization problems which are difficult to solve. In this
paper, we propose an effective non-blind deconvolution approach by learning
discriminative shrinkage functions to implicitly model these terms. In contrast
to most existing methods that use deep convolutional neural networks (CNNs) or
radial basis functions to simply learn the regularization term, we formulate
both the data term and regularization term and split the deconvolution model
into data-related and regularization-related sub-problems according to the
alternating direction method of multipliers. We explore the properties of the
Maxout function and develop a deep CNN model with a Maxout layer to learn
discriminative shrinkage functions to directly approximate the solutions of
these two sub-problems. Moreover, given the fast-Fourier-transform-based image
restoration usually leads to ringing artifacts while conjugate-gradient-based
approach is time-consuming, we develop the Conjugate Gradient Network to
restore the latent clear images effectively and efficiently. Experimental
results show that the proposed method performs favorably against the
state-of-the-art ones in terms of efficiency and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Minimal Misalignment at Minimal Cost in One-Stage and Anchor-Free Object Detection. (arXiv:2112.08902v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08902">
<div class="article-summary-box-inner">
<span><p>Common object detection models consist of classification and regression
branches, due to different task drivers, these two branches have different
sensibility to the features from the same scale level and the same spatial
location. The point-based prediction method, which is based on the assumption
that the high classification confidence point has the high regression quality,
leads to the misalignment problem. Our analysis shows, the problem is further
composed of scale misalignment and spatial misalignment specifically. We aim to
resolve the phenomenon at minimal cost: a minor adjustment of the head network
and a new label assignment method replacing the rigid one. Our experiments show
that, compared to the baseline FCOS, a one-stage and anchor-free object
detection model, our model consistently get around 3 AP improvement with
different backbones, demonstrating both simplicity and efficiency of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Masking for Self-Supervised Learning. (arXiv:2201.13100v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13100">
<div class="article-summary-box-inner">
<span><p>We propose ADIOS, a masked image model (MIM) framework for self-supervised
learning, which simultaneously learns a masking function and an image encoder
using an adversarial objective. The image encoder is trained to minimise the
distance between representations of the original and that of a masked image.
The masking function, conversely, aims at maximising this distance. ADIOS
consistently improves on state-of-the-art self-supervised learning (SSL)
methods on a variety of tasks and datasets -- including classification on
ImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and
iNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao
et al., 2021) -- while generating semantically meaningful masks. Unlike modern
MIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch
tokenisation construction of Vision Transformers, and can be implemented with
convolutional backbones. We further demonstrate that the masks learned by ADIOS
are more effective in improving representation learning of SSL methods than
masking schemes used in popular MIM models. Code is available at
https://github.com/YugeTen/adios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Neighbor Consistency for Noisy Labels. (arXiv:2202.02200v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02200">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have relied on large, labelled datasets to
train high-capacity models. However, collecting large datasets in a time- and
cost-efficient manner often results in label noise. We present a method for
learning from noisy labels that leverages similarities between training
examples in feature space, encouraging the prediction of each example to be
similar to its nearest neighbours. Compared to training algorithms that use
multiple models or distinct stages, our approach takes the form of a simple,
additional regularization term. It can be interpreted as an inductive version
of the classical, transductive label propagation algorithm. We thoroughly
evaluate our method on datasets evaluating both synthetic (CIFAR-10, CIFAR-100)
and realistic (mini-WebVision, WebVision, Clothing1M, mini-ImageNet-Red) noise,
and achieve competitive or state-of-the-art accuracies across all of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GroupViT: Semantic Segmentation Emerges from Text Supervision. (arXiv:2202.11094v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11094">
<div class="article-summary-box-inner">
<span><p>Grouping and recognition are important components of visual scene
understanding, e.g., for object detection and semantic segmentation. With
end-to-end deep learning systems, grouping of image regions usually happens
implicitly via top-down supervision from pixel-level recognition labels.
Instead, in this paper, we propose to bring back the grouping mechanism into
deep networks, which allows semantic segments to emerge automatically with only
text supervision. We propose a hierarchical Grouping Vision Transformer
(GroupViT), which goes beyond the regular grid structure representation and
learns to group image regions into progressively larger arbitrary-shaped
segments. We train GroupViT jointly with a text encoder on a large-scale
image-text dataset via contrastive losses. With only text supervision and
without any pixel-level annotations, GroupViT learns to group together semantic
regions and successfully transfers to the task of semantic segmentation in a
zero-shot manner, i.e., without any further fine-tuning. It achieves a
zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on
PASCAL Context datasets, and performs competitively to state-of-the-art
transfer-learning methods requiring greater levels of supervision. We
open-source our code at https://github.com/NVlabs/GroupViT .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FUNQUE: Fusion of Unified Quality Evaluators. (arXiv:2202.11241v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11241">
<div class="article-summary-box-inner">
<span><p>Fusion-based quality assessment has emerged as a powerful method for
developing high-performance quality models from quality models that
individually achieve lower performances. A prominent example of such an
algorithm is VMAF, which has been widely adopted as an industry standard for
video quality prediction along with SSIM. In addition to advancing the
state-of-the-art, it is imperative to alleviate the computational burden
presented by the use of a heterogeneous set of quality models. In this paper,
we unify "atom" quality models by computing them on a common transform domain
that accounts for the Human Visual System, and we propose FUNQUE, a quality
model that fuses unified quality evaluators. We demonstrate that in comparison
to the state-of-the-art, FUNQUE offers significant improvements in both
correlation against subjective scores and efficiency, due to computation
sharing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04203">
<div class="article-summary-box-inner">
<span><p>A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as "how
can I run the microwave for 1 minute?". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos and scripts to guide the user
step-by-step. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples derived from 100 newly filmed
first-person videos. Each question should be completed with multi-step
guidances by inferring from visual details (e.g., buttons' position) and
textural details (e.g., actions like press/turn). To address this unique task,
we developed a Question-to-Actions (Q2A) model that significantly outperforms
several baseline methods while still having large room for improvement. We
expect our task and dataset to advance Egocentric AI Assistant's development.
Our project page is available at: https://showlab.github.io/assistq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">City-wide Street-to-Satellite Image Geolocalization of a Mobile Ground Agent. (arXiv:2203.05612v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05612">
<div class="article-summary-box-inner">
<span><p>Cross-view image geolocalization provides an estimate of an agent's global
position by matching a local ground image to an overhead satellite image
without the need for GPS. It is challenging to reliably match a ground image to
the correct satellite image since the images have significant viewpoint
differences. Existing works have demonstrated localization in constrained
scenarios over small areas but have not demonstrated wider-scale localization.
Our approach, called Wide-Area Geolocalization (WAG), combines a neural network
with a particle filter to achieve global position estimates for agents moving
in GPS-denied environments, scaling efficiently to city-scale regions. WAG
introduces a trinomial loss function for a Siamese network to robustly match
non-centered image pairs and thus enables the generation of a smaller satellite
image database by coarsely discretizing the search area. A modified particle
filter weighting scheme is also presented to improve localization accuracy and
convergence. Taken together, WAG's network training and particle filter
weighting approach achieves city-scale position estimation accuracies on the
order of 20 meters, a 98% reduction compared to a baseline training and
weighting approach. Applied to a smaller-scale testing area, WAG reduces the
final position estimation error by 64% compared to a state-of-the-art baseline
from the literature. WAG's search space discretization additionally
significantly reduces storage and processing requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08344">
<div class="article-summary-box-inner">
<span><p>We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Normalized Density Map (SNDM) for Counting Microbiological Objects. (arXiv:2203.09474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09474">
<div class="article-summary-box-inner">
<span><p>The statistical properties of the density map (DM) approach to counting
microbiological objects on images are studied in detail. The DM is given by
U$^2$-Net. Two statistical methods for deep neural networks are utilized: the
bootstrap and the Monte Carlo (MC) dropout. The detailed analysis of the
uncertainties for the DM predictions leads to a deeper understanding of the DM
model's deficiencies. Based on our investigation, we propose a
self-normalization module in the network. The improved network model, called
\textit{Self-Normalized Density Map} (SNDM), can correct its output density map
by itself to accurately predict the total number of objects in the image. The
SNDM architecture outperforms the original model. Moreover, both statistical
frameworks -- bootstrap and MC dropout -- have consistent statistical results
for SNDM, which were not observed in the original model. The SNDM efficiency is
comparable with the detector-base models, such as Faster and Cascade R-CNN
detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09855">
<div class="article-summary-box-inner">
<span><p>In this paper, we formulate a potentially valuable panoramic depth completion
(PDC) task as panoramic 3D cameras often produce 360{\deg} depth with missing
data in complex scenes. Its goal is to recover dense panoramic depths from raw
sparse ones and panoramic RGB images. To deal with the PDC task, we train a
deep network that takes both depth and image as inputs for the dense panoramic
depth recovery. However, it needs to face a challenging optimization problem of
the network parameters due to its non-convex objective function. To address
this problem, we propose a simple yet effective approach termed M{^3}PT:
multi-modal masked pre-training. Specifically, during pre-training, we
simultaneously cover up patches of the panoramic RGB image and sparse depth by
shared random mask, then reconstruct the sparse depth in the masked regions. To
our best knowledge, it is the first time that we show the effectiveness of
masked pre-training in a multi-modal vision task, instead of the single-modal
task resolved by masked autoencoders (MAE). Different from MAE where
fine-tuning completely discards the decoder part of pre-training, there is no
architectural difference between the pre-training and fine-tuning stages in our
M$^{3}$PT as they only differ in the prediction density, which potentially
makes the transfer learning more convenient and effective. Extensive
experiments verify the effectiveness of M{^3}PT on three panoramic datasets.
Notably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,
51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Serves Traffic Safety Analysis: A Forward-looking Review. (arXiv:2203.10939v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10939">
<div class="article-summary-box-inner">
<span><p>This paper explores Deep Learning (DL) methods that are used or have the
potential to be used for traffic video analysis, emphasizing driving safety for
both Autonomous Vehicles (AVs) and human-operated vehicles. We present a
typical processing pipeline, which can be used to understand and interpret
traffic videos by extracting operational safety metrics and providing general
hints and guidelines to improve traffic safety. This processing framework
includes several steps, including video enhancement, video stabilization,
semantic and incident segmentation, object detection and classification,
trajectory extraction, speed estimation, event analysis, modeling and anomaly
detection. Our main goal is to guide traffic analysts to develop their own
custom-built processing frameworks by selecting the best choices for each step
and offering new designs for the lacking modules by providing a comparative
analysis of the most successful conventional and DL-based algorithms proposed
for each step. We also review existing open-source tools and public datasets
that can help train DL models. To be more specific, we review exemplary traffic
problems and mentioned requires steps for each problem. Besides, we investigate
connections to the closely related research areas of drivers' cognition
evaluation, Crowd-sourcing-based monitoring systems, Edge Computing in roadside
infrastructures, Automated Driving Systems (ADS)-equipped vehicles, and
highlight the missing gaps. Finally, we review commercial implementations of
traffic monitoring systems, their future outlook, and open problems and
remaining challenges for widespread use of such systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cell segmentation from telecentric bright-field transmitted light microscopy images using a Residual Attention U-Net: a case study on HeLa line. (arXiv:2203.12290v3 [q-bio.QM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12290">
<div class="article-summary-box-inner">
<span><p>Living cell segmentation from bright-field light microscopy images is
challenging due to the image complexity and temporal changes in the living
cells. Recently developed deep learning (DL)-based methods became popular in
medical and microscopy image segmentation tasks due to their success and
promising outcomes. The main objective of this paper is to develop a deep
learning, U-Net-based method to segment the living cells of the HeLa line in
bright-field transmitted light microscopy. To find the most suitable
architecture for our datasets, a residual attention U-Net was proposed and
compared with an attention and a simple U-Net architecture.
</p>
<p>The attention mechanism highlights the remarkable features and suppresses
activations in the irrelevant image regions. The residual mechanism overcomes
with vanishing gradient problem. The Mean-IoU score for our datasets reaches
0.9505, 0.9524, and 0.9530 for the simple, attention, and residual attention
U-Net, respectively. The most accurate semantic segmentation results was
achieved in the Mean-IoU and Dice metrics by applying the residual and
attention mechanisms together. The watershed method applied to this best --
Residual Attention -- semantic segmentation result gave the segmentation with
the specific information for each cell.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17255">
<div class="article-summary-box-inner">
<span><p>This theoretical article examines how to construct human-like working memory
and thought processes within a computer. There should be two working memory
stores, one analogous to sustained firing in association cortex, and one
analogous to synaptic potentiation in the cerebral cortex. These stores must be
constantly updated with new representations that arise from either
environmental stimulation or internal processing. They should be updated
continuously, and in an iterative fashion, meaning that, in the next state,
some items in the set of coactive items should always be retained. Thus, the
set of concepts coactive in working memory will evolve gradually and
incrementally over time. This makes each state is a revised iteration of the
preceding state and causes successive states to overlap and blend with respect
to the set of representations they contain. As new representations are added
and old ones are subtracted, some remain active for several seconds over the
course of these changes. This persistent activity, similar to that used in
artificial recurrent neural networks, is used to spread activation energy
throughout the global workspace to search for the next associative update. The
result is a chain of associatively linked intermediate states that are capable
of advancing toward a solution or goal. Iterative updating is conceptualized
here as an information processing strategy, a computational and
neurophysiological determinant of the stream of thought, and an algorithm for
designing and programming artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02810">
<div class="article-summary-box-inner">
<span><p>Face frontalization consists of synthesizing a frontally-viewed face from an
arbitrarily-viewed one. The main contribution of this paper is a frontalization
methodology that preserves non-rigid facial deformations in order to boost the
performance of visually assisted speech communication. The method alternates
between the estimation of (i)~the rigid transformation (scale, rotation, and
translation) and (ii)~the non-rigid deformation between an arbitrarily-viewed
face and a face model. The method has two important merits: it can deal with
non-Gaussian errors in the data and it incorporates a dynamical face
deformation model. For that purpose, we use the generalized Student
t-distribution in combination with a linear dynamic system in order to account
for both rigid head motions and time-varying facial deformations caused by
speech production. We propose to use the zero-mean normalized cross-correlation
(ZNCC) score to evaluate the ability of the method to preserve facial
expressions. The method is thoroughly evaluated and compared with several state
of the art methods, either based on traditional geometric models or on deep
learning. Moreover, we show that the method, when incorporated into deep
learning pipelines, namely lip reading and speech enhancement, improves word
recognition and speech intelligibilty scores by a considerable margin.
Supplemental material is accessible at
https://team.inria.fr/robotlearn/research/facefrontalization-benchmark/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion. (arXiv:2204.03842v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03842">
<div class="article-summary-box-inner">
<span><p>We consider the problem of Multi-view 3D Face Reconstruction (MVR) with
weakly supervised learning that leverages a limited number of 2D face images
(e.g. 3) to generate a high-quality 3D face model with very light annotation.
Despite their encouraging performance, present MVR methods simply concatenate
multi-view image features and pay less attention to critical areas (e.g. eye,
brow, nose, and mouth). To this end, we propose a novel model called Deep
Fusion MVR (DF-MVR) and design a multi-view encoding to a single decoding
framework with skip connections, able to extract, integrate, and compensate
deep features with attention from multi-view images. In addition, we develop a
multi-view face parse network to learn, identify, and emphasize the critical
common face area. Finally, though our model is trained with a few 2D images, it
can reconstruct an accurate 3D model even if one single 2D image is input. We
conduct extensive experiments to evaluate various multi-view 3D face
reconstruction methods. Experiments on Pixel-Face and Bosphorus datasets
indicate the superiority of our model. Without 3D landmarks annotation, DF-MVR
achieves 5.2% and 3.0% RMSE improvements over the existing best weakly
supervised MVRs respectively on Pixel-Face and Bosphorus datasets; with 3D
landmarks annotation, DF-MVR attains superior performance particularly on
Pixel-Face dataset, leading to 13.4% RMSE improvement over the best weakly
supervised MVR model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13326">
<div class="article-summary-box-inner">
<span><p>This paper describes the methods submitted for evaluation to the SHREC 2022
track on pothole and crack detection in the road pavement. A total of 7
different runs for the semantic segmentation of the road surface are compared,
6 from the participants plus a baseline method. All methods exploit Deep
Learning techniques and their performance is tested using the same environment
(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic
segmentation image/mask pairs and 797 RGB-D video clips collected with the
latest depth cameras was made available to the participants. The methods are
then evaluated on the 496 image/mask pairs in the validation set, on the 504
pairs in the test set and finally on 8 video clips. The analysis of the results
is based on quantitative metrics for image segmentation and qualitative
analysis of the video clips. The participation and the results show that the
scenario is of great interest and that the use of RGB-D data is still
challenging in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14141">
<div class="article-summary-box-inner">
<span><p>Masked image modeling (MIM) learns representations with remarkably good
fine-tuning performances, overshadowing previous prevalent pre-training
approaches such as image classification, instance contrastive learning, and
image-text alignment. In this paper, we show that the inferior fine-tuning
performance of these pre-training approaches can be significantly improved by a
simple post-processing in the form of feature distillation (FD). The feature
distillation converts the old representations to new representations that have
a few desirable properties just like those representations produced by MIM.
These properties, which we aggregately refer to as optimization friendliness,
are identified and analyzed by a set of attention- and optimization-related
diagnosis tools. With these properties, the new representations show strong
fine-tuning performance. Specifically, the contrastive self-supervised learning
methods are made as competitive in fine-tuning as the state-of-the-art masked
image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is
also significantly improved, with a CLIP ViT-L model reaching \textbf{89.0%}
top-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter
SwinV2-G model, the fine-tuning accuracy on ADE20K semantic segmentation is
improved by +1.5 mIoU to \textbf{61.4 mIoU}, creating a new record. More
importantly, our work provides a way for the future research to focus more
effort on the generality and scalability of the learnt representations without
being pre-occupied with optimization friendliness since it can be enhanced
rather easily. The code will be available at
https://github.com/SwinTransformer/Feature-Distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision GNN: An Image is Worth Graph of Nodes. (arXiv:2206.00272v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00272">
<div class="article-summary-box-inner">
<span><p>Network architecture plays a key role in the deep learning-based computer
vision system. The widely-used convolutional neural network and transformer
treat the image as a grid or sequence structure, which is not flexible to
capture irregular and complex objects. In this paper, we propose to represent
the image as a graph structure and introduce a new Vision GNN (ViG)
architecture to extract graph-level feature for visual tasks. We first split
the image to a number of patches which are viewed as nodes, and construct a
graph by connecting the nearest neighbors. Based on the graph representation of
images, we build our ViG model to transform and exchange information among all
the nodes. ViG consists of two basic modules: Grapher module with graph
convolution for aggregating and updating graph information, and FFN module with
two linear layers for node feature transformation. Both isotropic and pyramid
architectures of ViG are built with different model sizes. Extensive
experiments on image recognition and object detection tasks demonstrate the
superiority of our ViG architecture. We hope this pioneering study of GNN on
general visual tasks will provide useful inspiration and experience for future
research. The PyTorch code is available at
https://github.com/huawei-noah/Efficient-AI-Backbones and the MindSpore code is
available at https://gitee.com/mindspore/models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-aware Panoptic Segmentation. (arXiv:2206.14554v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.14554">
<div class="article-summary-box-inner">
<span><p>Reliable scene understanding is indispensable for modern autonomous systems.
Current learning-based methods typically try to maximize their performance
based on segmentation metrics that only consider the quality of the
segmentation. However, for the safe operation of a system in the real world it
is crucial to consider the uncertainty in the prediction as well. In this work,
we introduce the novel task of uncertainty-aware panoptic segmentation, which
aims to predict per-pixel semantic and instance segmentations, together with
per-pixel uncertainty estimates. We define two novel metrics to facilitate its
quantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the
panoptic Expected Calibration Error (pECE). We further propose the novel
top-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.
Our architecture employs a simple yet effective probabilistic fusion module
that leverages the predicted uncertainties. Additionally, we propose a new
Lov\'asz evidential loss function to optimize the IoU for the segmentation
utilizing the probabilities provided by deep evidential learning. Furthermore,
we provide several strong baselines combining state-of-the-art panoptic
segmentation networks with sampling-free uncertainty estimation techniques.
Extensive evaluations show that our EvPSNet achieves the new state-of-the-art
for the standard Panoptic Quality (PQ), as well as for our uncertainty-aware
panoptic metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Latent Replay for efficient Generative Rehearsal. (arXiv:2207.01562v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01562">
<div class="article-summary-box-inner">
<span><p>We introduce a new method for internal replay that modulates the frequency of
rehearsal based on the depth of the network. While replay strategies mitigate
the effects of catastrophic forgetting in neural networks, recent works on
generative replay show that performing the rehearsal only on the deeper layers
of the network improves the performance in continual learning. However, the
generative approach introduces additional computational overhead, limiting its
applications. Motivated by the observation that earlier layers of neural
networks forget less abruptly, we propose to update network layers with varying
frequency using intermediate-level features during replay. This reduces the
computational burden by omitting computations for both deeper layers of the
generator and earlier layers of the main model. We name our method Progressive
Latent Replay and show that it outperforms Internal Replay while using
significantly fewer resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Fine-Grained Sketch-Based Image Retrieval. (arXiv:2207.01723v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01723">
<div class="article-summary-box-inner">
<span><p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has
shifted towards generalising a model to new categories without any training
data from them. In real-world applications, however, a trained FG-SBIR model is
often applied to both new categories and different human sketchers, i.e.,
different drawing styles. Although this complicates the generalisation problem,
fortunately, a handful of examples are typically available, enabling the model
to adapt to the new category/style. In this paper, we offer a novel perspective
-- instead of asking for a model that generalises, we advocate for one that
quickly adapts, with just very few samples during testing (in a few-shot
manner). To solve this new problem, we introduce a novel model-agnostic
meta-learning (MAML) based framework with several key modifications: (1) As a
retrieval task with a margin-based contrastive loss, we simplify the MAML
training in the inner loop to make it more stable and tractable. (2) The margin
in our contrastive loss is also meta-learned with the rest of the model. (3)
Three additional regularisation losses are introduced in the outer loop, to
make the meta-learned FG-SBIR model more effective for category/style
adaptation. Extensive experiments on public datasets suggest a large gain over
generalisation and zero-shot based approaches, and a few strong few-shot
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian approaches for Quantifying Clinicians' Variability in Medical Image Quantification. (arXiv:2207.01868v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01868">
<div class="article-summary-box-inner">
<span><p>Medical imaging, including MRI, CT, and Ultrasound, plays a vital role in
clinical decisions. Accurate segmentation is essential to measure the structure
of interest from the image. However, manual segmentation is highly
operator-dependent, which leads to high inter and intra-variability of
quantitative measurements. In this paper, we explore the feasibility that
Bayesian predictive distribution parameterized by deep neural networks can
capture the clinicians' inter-intra variability. By exploring and analyzing
recently emerged approximate inference schemes, we evaluate whether approximate
Bayesian deep learning with the posterior over segmentations can learn
inter-intra rater variability both in segmentation and clinical measurements.
The experiments are performed with two different imaging modalities: MRI and
ultrasound. We empirically demonstrated that Bayesian predictive distribution
parameterized by deep neural networks could approximate the clinicians'
inter-intra variability. We show a new perspective in analyzing medical images
quantitatively by providing clinical measurement uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latents2Segments: Disentangling the Latent Space of Generative Models for Semantic Segmentation of Face Images. (arXiv:2207.01871v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01871">
<div class="article-summary-box-inner">
<span><p>With the advent of an increasing number of Augmented and Virtual Reality
applications that aim to perform meaningful and controlled style edits on
images of human faces, the impetus for the task of parsing face images to
produce accurate and fine-grained semantic segmentation maps is more than ever
before. Few State of the Art (SOTA) methods which solve this problem, do so by
incorporating priors with respect to facial structure or other face attributes
such as expression and pose in their deep classifier architecture. Our
endeavour in this work is to do away with the priors and complex pre-processing
operations required by SOTA multi-class face segmentation models by reframing
this operation as a downstream task post infusion of disentanglement with
respect to facial semantic regions of interest (ROIs) in the latent space of a
Generative Autoencoder model. We present results for our model's performance on
the CelebAMask-HQ and HELEN datasets. The encoded latent space of our model
achieves significantly higher disentanglement with respect to semantic ROIs
than that of other SOTA works. Moreover, it achieves a 13% faster inference
rate and comparable accuracy with respect to the publicly available SOTA for
the downstream task of semantic segmentation of face images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Robustness Analysis Against Language and Visual Perturbations. (arXiv:2207.02159v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02159">
<div class="article-summary-box-inner">
<span><p>Joint visual and language modeling on large-scale datasets has recently shown
a good progress in multi-modal tasks when compared to single modal learning.
However, robustness of these approaches against real-world perturbations has
not been studied. In this work, we perform the first extensive robustness study
of such models against various real-world perturbations focusing on video and
language. We focus on text-to-video retrieval and propose two large-scale
benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual
and 35 different textual perturbations. The study reveals some interesting
findings: 1) The studied models are more robust when text is perturbed versus
when video is perturbed 2) The transformer text encoder is more robust on
non-semantic changing text perturbations and visual perturbations compared to
word embedding approaches. 3) Using two-branch encoders in isolation is
typically more robust than when architectures use cross-attention. We hope this
study will serve as a benchmark and guide future research in robust multimodal
learning.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-07 23:08:53.787471698 UTC">2022-07-07 23:08:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>