<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-03T01:30:00Z">06-03</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">What Changed? Investigating Debiasing Methods using Causal Mediation Analysis. (arXiv:2206.00701v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00701">
<div class="article-summary-box-inner">
<span><p>Previous work has examined how debiasing language models affect downstream
tasks, specifically, how debiasing techniques influence task performance and
whether debiased models also make impartial predictions in downstream tasks or
not. However, what we don't understand well yet is why debiasing methods have
varying impacts on downstream tasks and how debiasing techniques affect
internal components of language models, i.e., neurons, layers, and attentions.
In this paper, we decompose the internal mechanisms of debiasing language
models with respect to gender by applying causal mediation analysis to
understand the influence of debiasing methods on toxicity detection as a
downstream task. Our findings suggest a need to test the effectiveness of
debiasing methods with different bias metrics, and to focus on changes in the
behavior of certain components of the models, e.g.,first two layers of language
models, and attention heads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting. (arXiv:2206.00761v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00761">
<div class="article-summary-box-inner">
<span><p>The availability of large pre-trained models is changing the landscape of
Machine Learning research and practice, moving from a training-from-scratch to
a fine-tuning paradigm. While in some applications the goal is to "nudge" the
pre-trained distribution towards preferred outputs, in others it is to steer it
towards a different distribution over the sample space. Two main paradigms have
emerged to tackle this challenge: Reward Maximization (RM) and, more recently,
Distribution Matching (DM). RM applies standard Reinforcement Learning (RL)
techniques, such as Policy Gradients, to gradually increase the reward signal.
DM prescribes to first make explicit the target distribution that the model is
fine-tuned to approximate. Here we explore the theoretical connections between
the two paradigms, and show that methods such as KL-control developed for RM
can also be construed as belonging to DM. We further observe that while DM
differs from RM, it can suffer from similar training difficulties, such as high
gradient variance. We leverage connections between the two paradigms to import
the concept of baseline into DM methods. We empirically validate the benefits
of adding a baseline on an array of controllable language generation tasks such
as constraining topic, sentiment, and gender distributions in texts sampled
from a language model. We observe superior performance in terms of constraint
satisfaction, stability and sample efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the trade-off between prediction accuracy and interpretability for topic modeling on energetic materials corpora. (arXiv:2206.00773v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00773">
<div class="article-summary-box-inner">
<span><p>As the amount and variety of energetics research increases, machine aware
topic identification is necessary to streamline future research pipelines. The
makeup of an automatic topic identification process consists of creating
document representations and performing classification. However, the
implementation of these processes on energetics research imposes new
challenges. Energetics datasets contain many scientific terms that are
necessary to understand the context of a document but may require more complex
document representations. Secondly, the predictions from classification must be
understandable and trusted by the chemists within the pipeline. In this work,
we study the trade-off between prediction accuracy and interpretability by
implementing three document embedding methods that vary in computational
complexity. With our accuracy results, we also introduce local interpretability
model-agnostic explanations (LIME) of each prediction to provide a localized
understanding of each prediction and to validate classifier decisions with our
team of energetics experts. This study was carried out on a novel labeled
energetics dataset created and validated by our team of energetics experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BayesFormer: Transformer with Uncertainty Estimation. (arXiv:2206.00826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00826">
<div class="article-summary-box-inner">
<span><p>Transformer has become ubiquitous due to its dominant performance in various
NLP and image processing tasks. However, it lacks understanding of how to
generate mathematically grounded uncertainty estimates for transformer
architectures. Models equipped with such uncertainty estimates can typically
improve predictive performance, make networks robust, avoid over-fitting and
used as acquisition function in active learning. In this paper, we introduce
BayesFormer, a Transformer model with dropouts designed by Bayesian theory. We
proposed a new theoretical framework to extend the approximate variational
inference-based dropout to Transformer-based architectures. Through extensive
experiments, we validate the proposed architecture in four paradigms and show
improvements across the board: language modeling and classification,
long-sequence understanding, machine translation and acquisition function for
active learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSTR: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation. (arXiv:2206.00847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00847">
<div class="article-summary-box-inner">
<span><p>Many scientific papers such as those in arXiv and PubMed data collections
have abstracts with varying lengths of 50-1000 words and average length of
approximately 200 words, where longer abstracts typically convey more
information about the source paper. Up to recently, scientific summarization
research has typically focused on generating short, abstract-like summaries
following the existing datasets used for scientific summarization. In domains
where the source text is relatively long-form, such as in scientific documents,
such summary is not able to go beyond the general and coarse overview and
provide salient information from the source document. The recent interest to
tackle this problem motivated curation of scientific datasets, arXiv-Long and
PubMed-Long, containing human-written summaries of 400-600 words, hence,
providing a venue for research in generating long/extended summaries. Extended
summaries facilitate a faster read while providing details beyond coarse
information. In this paper, we propose TSTR, an extractive summarizer that
utilizes the introductory information of documents as pointers to their salient
information. The evaluations on two existing large-scale extended summarization
datasets indicate statistically significant improvement in terms of Rouge and
average Rouge (F1) scores (except in one case) as compared to strong baselines
and state-of-the-art. Comprehensive human evaluations favor our generated
extended summaries in terms of cohesion and completeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MentSum: A Resource for Exploring Summarization of Mental Health Online Posts. (arXiv:2206.00856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00856">
<div class="article-summary-box-inner">
<span><p>Mental health remains a significant challenge of public health worldwide.
With increasing popularity of online platforms, many use the platforms to share
their mental health conditions, express their feelings, and seek help from the
community and counselors. Some of these platforms, such as Reachout, are
dedicated forums where the users register to seek help. Others such as Reddit
provide subreddits where the users publicly but anonymously post their mental
health distress. Although posts are of varying length, it is beneficial to
provide a short, but informative summary for fast processing by the counselors.
To facilitate research in summarization of mental health online posts, we
introduce Mental Health Summarization dataset, MentSum, containing over 24k
carefully selected user posts from Reddit, along with their short user-written
summary (called TLDR) in English from 43 mental health subreddits. This
domain-specific dataset could be of interest not only for generating short
summaries on Reddit, but also for generating summaries of posts on the
dedicated mental health forums such as Reachout. We further evaluate both
extractive and abstractive state-of-the-art summarization baselines in terms of
Rouge scores, and finally conduct an in-depth human evaluation study of both
user-written and system-generated summaries, highlighting challenges in this
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Squeezeformer: An Efficient Transformer for Automatic Speech Recognition. (arXiv:2206.00888v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00888">
<div class="article-summary-box-inner">
<span><p>The recently proposed Conformer model has become the de facto backbone model
for various downstream speech tasks based on its hybrid attention-convolution
architecture that captures both local and global features. However, through a
series of systematic studies, we find that the Conformer architecture's design
choices are not optimal. After reexamining the design choices for both the
macro and micro-architecture of Conformer, we propose the Squeezeformer model,
which consistently outperforms the state-of-the-art ASR models under the same
training schemes. In particular, for the macro-architecture, Squeezeformer
incorporates (i) the Temporal U-Net structure, which reduces the cost of the
multi-head attention modules on long sequences, and (ii) a simpler block
structure of feed-forward module, followed up by multi-head attention or
convolution modules, instead of the Macaron structure proposed in Conformer.
Furthermore, for the micro-architecture, Squeezeformer (i) simplifies the
activations in the convolutional block, (ii) removes redundant Layer
Normalization operations, and (iii) incorporates an efficient depth-wise
downsampling layer to efficiently sub-sample the input signal. Squeezeformer
achieves state-of-the-art results of 7.5%, 6.5%, and 6.0% word-error-rate on
Librispeech test-other without external language models. This is 3.1%, 1.4%,
and 0.6% better than Conformer-CTC with the same number of FLOPs. Our code is
open-sourced and available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralSympCheck: A Symptom Checking and Disease Diagnostic Neural Model with Logic Regularization. (arXiv:2206.00906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00906">
<div class="article-summary-box-inner">
<span><p>The symptom checking systems inquire users for their symptoms and perform a
rapid and affordable medical assessment of their condition. The basic symptom
checking systems based on Bayesian methods, decision trees, or information gain
methods are easy to train and do not require significant computational
resources. However, their drawbacks are low relevance of proposed symptoms and
insufficient quality of diagnostics. The best results on these tasks are
achieved by reinforcement learning models. Their weaknesses are the difficulty
of developing and training such systems and limited applicability to cases with
large and sparse decision spaces. We propose a new approach based on the
supervised learning of neural models with logic regularization that combines
the advantages of the different methods. Our experiments on real and synthetic
data show that the proposed approach outperforms the best existing methods in
the accuracy of diagnosis when the number of diagnoses and symptoms is large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The ParlaSent-BCS dataset of sentiment-annotated parliamentary debates from Bosnia-Herzegovina, Croatia, and Serbia. (arXiv:2206.00929v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00929">
<div class="article-summary-box-inner">
<span><p>Expression of sentiment in parliamentary debates is deemed to be
significantly different from that on social media or in product reviews. This
paper adds to an emerging body of research on parliamentary debates with a
dataset of sentences annotated for detection sentiment polarity in political
discourse. We sample the sentences for annotation from the proceedings of three
Southeast European parliaments: Croatia, Bosnia-Herzegovina, and Serbia. A
six-level schema is applied to the data with the aim of training a
classification model for the detection of sentiment in parliamentary
proceedings. Krippendorff's alpha measuring the inter-annotator agreement
ranges from 0.6 for the six-level annotation schema to 0.75 for the three-level
schema and 0.83 for the two-level schema. Our initial experiments on the
dataset show that transformer models perform significantly better than those
using a simpler architecture. Furthermore, regardless of the similarity of the
three languages, we observe differences in performance across different
languages. Performing parliament-specific training and evaluation shows that
the main reason for the differing performance between parliaments seems to be
the different complexity of the automatic classification task, which is not
observable in annotator performance. Language distance does not seem to play
any role neither in annotator nor in automatic classification performance. We
release the dataset and the best-performing model under permissive licences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection. (arXiv:2206.00962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00962">
<div class="article-summary-box-inner">
<span><p>We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-BEiT: Generative Vision-Language Pretraining. (arXiv:2206.01127v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01127">
<div class="article-summary-box-inner">
<span><p>We introduce a vision-language foundation model called VL-BEiT, which is a
bidirectional multimodal Transformer learned by generative pretraining. Our
minimalist solution conducts masked prediction on both monomodal and multimodal
data with a shared Transformer. Specifically, we perform masked vision-language
modeling on image-text pairs, masked language modeling on texts, and masked
image modeling on images. VL-BEiT is learned from scratch with one unified
pretraining task, one shared backbone, and one-stage training. Our method is
conceptually simple and empirically effective. Experimental results show that
VL-BEiT obtains strong results on various vision-language benchmarks, such as
visual question answering, visual reasoning, and image-text retrieval.
Moreover, our method learns transferable visual features, achieving competitive
performance on image classification, and semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vygotskian Autotelic Artificial Intelligence: Language and Culture Internalization for Human-Like AI. (arXiv:2206.01134v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01134">
<div class="article-summary-box-inner">
<span><p>Building autonomous artificial agents able to grow open-ended repertoires of
skills is one of the fundamental goals of AI. To that end, a promising
developmental approach recommends the design of intrinsically motivated agents
that learn new skills by generating and pursuing their own goals - autotelic
agents. However, existing algorithms still show serious limitations in terms of
goal diversity, exploration, generalization or skill composition. This
perspective calls for the immersion of autotelic agents into rich
socio-cultural worlds. We focus on language especially, and how its structure
and content may support the development of new cognitive functions in
artificial agents, just like it does in humans. Indeed, most of our skills
could not be learned in isolation. Formal education teaches us to reason
systematically, books teach us history, and YouTube might teach us how to cook.
Crucially, our values, traditions, norms and most of our goals are cultural in
essence. This knowledge, and some argue, some of our cognitive functions such
as abstraction, compositional imagination or relational thinking, are formed
through linguistic and cultural interactions. Inspired by the work of Vygotsky,
we suggest the design of Vygotskian autotelic agents able to interact with
others and, more importantly, able to internalize these interactions to
transform them into cognitive tools supporting the development of new cognitive
functions. This perspective paper proposes a new AI paradigm in the quest for
artificial lifelong skill discovery. It justifies the approach by uncovering
examples of new artificial cognitive functions emerging from interactions
between language and embodiment in recent works at the intersection of deep
reinforcement learning and natural language processing. Looking forward, it
highlights future opportunities and challenges for Vygotskian Autotelic AI
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation. (arXiv:2206.01137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01137">
<div class="article-summary-box-inner">
<span><p>General translation models often still struggle to generate accurate
translations in specialized domains. To guide machine translation practitioners
and characterize the effectiveness of domain adaptation methods under different
data availability scenarios, we conduct an in-depth empirical exploration of
monolingual and parallel data approaches to domain adaptation of pre-trained,
third-party, NMT models in settings where architecture change is impractical.
We compare data centric adaptation methods in isolation and combination. We
study method effectiveness in very low resource (8k parallel examples) and
moderately low resource (46k parallel examples) conditions and propose an
ensemble approach to alleviate reductions in original domain translation
quality. Our work includes three domains: consumer electronic, clinical, and
biomedical and spans four language pairs - Zh-En, Ja-En, Es-En, and Ru-En. We
also make concrete recommendations for achieving high in-domain performance and
release our consumer electronic and medical domain datasets for all languages
and make our code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01201">
<div class="article-summary-box-inner">
<span><p>This paper revisits visual representation in knowledge-based visual question
answering (VQA) and demonstrates that using regional information in a better
way can significantly improve the performance. While visual representation is
extensively studied in traditional VQA, it is under-explored in knowledge-based
VQA even though these two tasks share the common spirit, i.e., rely on visual
input to answer the question. Specifically, we observe that in most
state-of-the-art knowledge-based VQA methods: 1) visual features are extracted
either from the whole image or in a sliding window manner for retrieving
knowledge, and the important relationship within/among object regions is
neglected; 2) visual features are not well utilized in the final answering
model, which is counter-intuitive to some extent. Based on these observations,
we propose a new knowledge-based VQA method REVIVE, which tries to utilize the
explicit information of object regions not only in the knowledge retrieval
stage but also in the answering model. The key motivation is that object
regions and inherent relationships are important for knowledge-based VQA. We
perform extensive experiments on the standard OK-VQA dataset and achieve new
state-of-the-art performance, i.e., 58.0% accuracy, surpassing previous
state-of-the-art method by a large margin (+3.6%). We also conduct detailed
analysis and show the necessity of regional information in different framework
components for knowledge-based VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkillBot: Identifying Risky Content for Children in Alexa Skills. (arXiv:2102.03382v2 [cs.MA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03382">
<div class="article-summary-box-inner">
<span><p>Many households include children who use voice personal assistants (VPA) such
as Amazon Alexa. Children benefit from the rich functionalities of VPAs and
third-party apps but are also exposed to new risks in the VPA ecosystem. In
this paper, we first investigate "risky" child-directed voice apps that contain
inappropriate content or ask for personal information through voice
interactions. We build SkillBot - a natural language processing (NLP)-based
system to automatically interact with VPA apps and analyze the resulting
conversations. We find 28 risky child-directed apps and maintain a growing
dataset of 31,966 non-overlapping app behaviors collected from 3,434 Alexa
apps. Our findings suggest that although child-directed VPA apps are subject to
stricter policy requirements and more intensive vetting, children remain
vulnerable to inappropriate content and privacy violations. We then conduct a
user study showing that parents are concerned about the identified risky apps.
Many parents do not believe that these apps are available and designed for
families/kids, although these apps are actually published in Amazon's "Kids"
product category. We also find that parents often neglect basic precautions
such as enabling parental controls on Alexa devices. Finally, we identify a
novel risk in the VPA ecosystem: confounding utterances, or voice commands
shared by multiple apps that may cause a user to interact with a different app
than intended. We identify 4,487 confounding utterances, including 581 shared
by child-directed and non-child-directed apps. We find that 27% of these
confounding utterances prioritize invoking a non-child-directed app over a
child-directed app. This indicates that children are at real risk of
accidentally invoking non-child-directed apps due to confounding utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABC: Attention with Bounded-memory Control. (arXiv:2110.02488v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02488">
<div class="article-summary-box-inner">
<span><p>Transformer architectures have achieved state-of-the-art results on a variety
of sequence modeling tasks. However, their attention mechanism comes with a
quadratic complexity in sequence lengths, making the computational overhead
prohibitive, especially for long sequences. Attention context can be seen as a
random-access memory with each token taking a slot. Under this perspective, the
memory size grows linearly with the sequence length, and so does the overhead
of reading from it. One way to improve the efficiency is to bound the memory
size. We show that disparate approaches can be subsumed into one abstraction,
attention with bounded-memory control (ABC), and they vary in their
organization of the memory. ABC reveals new, unexplored possibilities. First,
it connects several efficient attention variants that would otherwise seem
apart. Second, this abstraction gives new insights--an established approach
(Wang et al., 2020b) previously thought to be not applicable in causal
attention, actually is. Last, we present a new instance of ABC, which draws
inspiration from existing ABC approaches, but replaces their heuristic
memory-organizing functions with a learned, contextualized one. Our experiments
on language modeling, machine translation, and masked language model finetuning
show that our approach outperforms previous efficient attention models;
compared to the strong transformer baselines, it significantly improves the
inference time and space efficiency with no or negligible accuracy loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data. (arXiv:2112.04351v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04351">
<div class="article-summary-box-inner">
<span><p>Background: The COVID-19 pandemic has affected our society and human
well-being in various ways. In this study, we investigate how the pandemic has
influenced people's emotions and psychological states compared to a
pre-pandemic period using real-world data from social media.
</p>
<p>Method: We collected Reddit social media data from 2019 (pre-pandemic) and
2020 (pandemic) from the subreddits communities associated with eight
universities. We applied the pre-trained Robustly Optimized BERT pre-training
approach (RoBERTa) to learn text embedding from the Reddit messages, and
leveraged the relational information among posted messages to train a graph
attention network (GAT) for sentiment classification. Finally, we applied model
stacking to combine the prediction probabilities from RoBERTa and GAT to yield
the final classification on sentiment. With the model-predicted sentiment
labels on the collected data, we used a generalized linear mixed-effects model
to estimate the effects of pandemic and in-person teaching during the pandemic
on sentiment.
</p>
<p>Results: The results suggest that the odds of negative sentiments in 2020
(pandemic) were 25.7% higher than the odds in 2019 (pre-pandemic) with a
$p$-value $&lt;0.001$; and the odds of negative sentiments associated in-person
learning were 48.3% higher than with remote learning in 2020 with a $p$-value
of 0.029.
</p>
<p>Conclusions: Our study results are consistent with the findings in the
literature on the negative impacts of the pandemic on people's emotions and
psychological states. Our study contributes to the growing real-world evidence
on the various negative impacts of the pandemic on our society; it also
provides a good example of using both ML techniques and statistical modeling
and inference to make better use of real-world data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Augmented Language Models for Cause-Effect Relation Classification. (arXiv:2112.08615v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08615">
<div class="article-summary-box-inner">
<span><p>Previous studies have shown the efficacy of knowledge augmentation methods in
pretrained language models. However, these methods behave differently across
domains and downstream tasks. In this work, we investigate the augmentation of
pretrained language models with commonsense knowledge in the cause-effect
relation classification and commonsense causal reasoning tasks. After
automatically verbalizing ATOMIC2020, a wide coverage commonsense reasoning
knowledge graph, and GLUCOSE, a dataset of implicit commonsense causal
knowledge, we continually pretrain BERT and RoBERTa with the verbalized data.
Then we evaluate the resulting models on cause-effect pair classification and
answering commonsense causal reasoning questions. Our results show that
continually pretrained language models augmented with commonsense knowledge
outperform our baselines on two commonsense causal reasoning benchmarks, COPA
and BCOPA-CE, and the Temporal and Causal Reasoning (TCR) dataset, without
additional improvement in model architecture or using quality-enhanced data for
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inherently Explainable Reinforcement Learning in Natural Language. (arXiv:2112.08907v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08907">
<div class="article-summary-box-inner">
<span><p>We focus on the task of creating a reinforcement learning agent that is
inherently explainable -- with the ability to produce immediate local
explanations by thinking out loud while performing a task and analyzing entire
trajectories post-hoc to produce causal explanations. This Hierarchically
Explainable Reinforcement Learning agent (HEX-RL), operates in Interactive
Fictions, text-based game environments in which an agent perceives and acts
upon the world using textual natural language. These games are usually
structured as puzzles or quests with long-term dependencies in which an agent
must complete a sequence of actions to succeed -- providing ideal environments
in which to test an agent's ability to explain its actions. Our agent is
designed to treat explainability as a first-class citizen, using an extracted
symbolic knowledge graph-based state representation coupled with a Hierarchical
Graph Attention mechanism that points to the facts in the internal graph
representation that most influenced the choice of actions. Experiments show
that this agent provides significantly improved explanations over strong
baselines, as rated by human participants generally unfamiliar with the
environment, while also matching state-of-the-art task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09332">
<div class="article-summary-box-inner">
<span><p>We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. By setting up the task so that it can be performed by humans, we are able
to train models on the task using imitation learning, and then optimize answer
quality with human feedback. To make human evaluation of factual accuracy
easier, models must collect references while browsing in support of their
answers. We train and evaluate our models on ELI5, a dataset of questions asked
by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior
cloning, and then performing rejection sampling against a reward model trained
to predict human preferences. This model's answers are preferred by humans 56%
of the time to those of our human demonstrators, and 69% of the time to the
highest-voted answer from Reddit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01771">
<div class="article-summary-box-inner">
<span><p>Language model (LM) pre-training is useful in many language processing tasks.
But can pre-trained LMs be further leveraged for more general machine learning
problems? We propose an approach for using LMs to scaffold learning and
generalization in general sequential decision-making problems. In this
approach, goals and observations are represented as a sequence of embeddings,
and a policy network initialized with a pre-trained LM predicts the next
action. We demonstrate that this framework enables effective combinatorial
generalization across different environments and supervisory modalities. We
begin by assuming access to a set of expert demonstrations, and show that
initializing policies with LMs and fine-tuning them via behavior cloning
improves task completion rates by 43.6% in the VirtualHome environment. We then
examine how our framework may be used in environments without pre-collected
expert data. To do this, we integrate an active data gathering procedure into
pre-trained LMs. The agent iteratively learns by interacting with the
environment, relabeling the language goal of past 'failed' experiences, and
updating the policy in a self-supervised loop. The active data gathering
procedure also enables effective combinatorial generalization, outperforming
the best baseline by 25.1%. Finally, we explain these results by investigating
three possible factors underlying the effectiveness of the LM-based policy. We
find that sequential input representations (vs. fixed-dimensional feature
vectors) and favorable weight initialization are both important for
generalization. Surprisingly, however, the format of the policy inputs encoding
(e.g. as a natural language string vs. an arbitrary sequential encoding) has
little influence. Together, these results suggest that language modeling
induces representations that are useful for modeling not just language, but
also goals and plans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What are the best systems? New perspectives on NLP Benchmarking. (arXiv:2202.03799v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03799">
<div class="article-summary-box-inner">
<span><p>In Machine Learning, a benchmark refers to an ensemble of datasets associated
with one or multiple metrics together with a way to aggregate different systems
performances. They are instrumental in (i) assessing the progress of new
methods along different axes and (ii) selecting the best systems for practical
use. This is particularly the case for NLP with the development of large
pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a
variety of tasks. While the community mainly focused on developing new datasets
and metrics, there has been little interest in the aggregation procedure, which
is often reduced to a simple average over various performance measures.
However, this procedure can be problematic when the metrics are on a different
scale, which may lead to spurious conclusions. This paper proposes a new
procedure to rank systems based on their performance across different tasks.
Motivated by the social choice theory, the final system ordering is obtained
through aggregating the rankings induced by each task and is theoretically
grounded. We conduct extensive numerical experiments (on over 270k scores) to
assess the soundness of our approach both on synthetic and real scores (e.g.
GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method
yields different conclusions on state-of-the-art systems than the
mean-aggregation procedure while being both more reliable and robust.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locating and Editing Factual Associations in GPT. (arXiv:2202.05262v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05262">
<div class="article-summary-box-inner">
<span><p>We analyze the storage and recall of factual associations in autoregressive
transformer language models, finding evidence that these associations
correspond to localized, directly-editable computations. We first develop a
causal intervention for identifying neuron activations that are decisive in a
model's factual predictions. This reveals a distinct set of steps in
middle-layer feed-forward modules that mediate factual predictions while
processing subject tokens. To test our hypothesis that these computations
correspond to factual association recall, we modify feed-forward weights to
update specific factual associations using Rank-One Model Editing (ROME). We
find that ROME is effective on a standard zero-shot relation extraction (zsRE)
model-editing task, comparable to existing methods. To perform a more sensitive
evaluation, we also evaluate ROME on a new dataset of counterfactual
assertions, on which it simultaneously maintains both specificity and
generalization, whereas other methods sacrifice one or another. Our results
confirm an important role for mid-layer feed-forward modules in storing factual
associations and suggest that direct manipulation of computational mechanisms
may be a feasible approach for model editing. The code, dataset,
visualizations, and an interactive demo notebook are available at
https://rome.baulab.info/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Conditional Masked Language Pre-training for Neural Machine Translation. (arXiv:2203.09210v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09210">
<div class="article-summary-box-inner">
<span><p>Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching &amp; masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low- to extremely high-resource
languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on
average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it
can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the
best of our knowledge, this is the first work to pre-train a unified model for
fine-tuning on both NMT tasks. Code, data, and pre-trained models are available
at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Modeling of Multi-Domain Multi-Device ASR Systems. (arXiv:2205.06655v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06655">
<div class="article-summary-box-inner">
<span><p>Modern Automatic Speech Recognition (ASR) systems often use a portfolio of
domain-specific models in order to get high accuracy for distinct user
utterance types across different devices. In this paper, we propose an
innovative approach that integrates the different per-domain per-device models
into a unified model, using a combination of domain embedding, domain experts,
mixture of experts and adversarial training. We run careful ablation studies to
show the benefit of each of these innovations in contributing to the accuracy
of the overall unified model. Experiments show that our proposed unified
modeling approach actually outperforms the carefully tuned per-domain models,
giving relative gains of up to 10% over a baseline model with negligible
increase in the number of parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RigoBERTa: A State-of-the-Art Language Model For Spanish. (arXiv:2205.10233v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10233">
<div class="article-summary-box-inner">
<span><p>This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.
RigoBERTa is trained over a well-curated corpus formed up from different
subcorpora with key features. It follows the DeBERTa architecture, which has
several advantages over other architectures of similar size as BERT or RoBERTa.
RigoBERTa performance is assessed over 13 NLU tasks in comparison with other
available Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa
outperformed the three models in 10 out of the 13 tasks, achieving new
"State-of-the-Art" results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption. (arXiv:2206.00216v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00216">
<div class="article-summary-box-inner">
<span><p>As more and more pre-trained language models adopt on-cloud deployment, the
privacy issues grow quickly, mainly for the exposure of plain-text user data
(e.g., search history, medical record, bank account). Privacy-preserving
inference of transformer models is on the demand of cloud service users. To
protect privacy, it is an attractive choice to compute only with ciphertext in
homomorphic encryption (HE). However, enabling pre-trained models inference on
ciphertext data is difficult due to the complex computations in transformer
blocks, which are not supported by current HE tools yet. In this work, we
introduce $\textit{THE-X}$, an approximation approach for transformers, which
enables privacy-preserving inference of pre-trained models developed by popular
frameworks. $\textit{THE-X}$ proposes a workflow to deal with complex
computation in transformer networks, including all the non-polynomial functions
like GELU, softmax, and LayerNorm. Experiments reveal our proposed
$\textit{THE-X}$ can enable transformer inference on encrypted data for
different downstream tasks, all with negligible performance drop but enjoying
the theory-guaranteed privacy-preserving advantage.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Driven Detection of Invertebrate Species in Deep-Sea Video. (arXiv:2206.00718v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00718">
<div class="article-summary-box-inner">
<span><p>Each year, underwater remotely operated vehicles (ROVs) collect thousands of
hours of video of unexplored ocean habitats revealing a plethora of information
regarding biodiversity on Earth. However, fully utilizing this information
remains a challenge as proper annotations and analysis require trained
scientists time, which is both limited and costly. To this end, we present a
Dataset for Underwater Substrate and Invertebrate Analysis (DUSIA), a benchmark
suite and growing large-scale dataset to train, validate, and test methods for
temporally localizing four underwater substrates as well as temporally and
spatially localizing 59 underwater invertebrate species. DUSIA currently
includes over ten hours of footage across 25 videos captured in 1080p at 30 fps
by an ROV following pre planned transects across the ocean floor near the
Channel Islands of California. Each video includes annotations indicating the
start and end times of substrates across the video in addition to counts of
species of interest. Some frames are annotated with precise bounding box
locations for invertebrate species of interest, as seen in Figure 1. To our
knowledge, DUSIA is the first dataset of its kind for deep sea exploration,
with video from a moving camera, that includes substrate annotations and
invertebrate species that are present at significant depths where sunlight does
not penetrate. Additionally, we present the novel context-driven object
detector (CDD) where we use explicit substrate classification to influence an
object detection network to simultaneously predict a substrate and species
class influenced by that substrate. We also present a method for improving
training on partially annotated bounding box frames. Finally, we offer a
baseline method for automating the counting of invertebrate species of
interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Distillation using Neural Feature Regression. (arXiv:2206.00719v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00719">
<div class="article-summary-box-inner">
<span><p>Dataset distillation aims to learn a small synthetic dataset that preserves
most of the information from the original dataset. Dataset distillation can be
formulated as a bi-level meta-learning problem where the outer loop optimizes
the meta-dataset and the inner loop trains a model on the distilled data.
Meta-gradient computation is one of the key challenges in this formulation, as
differentiating through the inner loop learning procedure introduces
significant computation and memory costs. In this paper, we address these
challenges using neural Feature Regression with Pooling (FRePo), achieving the
state-of-the-art performance with an order of magnitude less memory requirement
and two orders of magnitude faster training than previous methods. The proposed
algorithm is analogous to truncated backpropagation through time with a pool of
models to alleviate various types of overfitting in dataset distillation. FRePo
significantly outperforms the previous methods on CIFAR100, Tiny ImageNet, and
ImageNet-1K. Furthermore, we show that high-quality distilled data can greatly
improve various downstream applications, such as continual learning and
membership inference defense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascaded Video Generation for Videos In-the-Wild. (arXiv:2206.00735v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00735">
<div class="article-summary-box-inner">
<span><p>Videos can be created by first outlining a global view of the scene and then
adding local details. Inspired by this idea we propose a cascaded model for
video generation which follows a coarse to fine approach. First our model
generates a low resolution video, establishing the global scene structure,
which is then refined by subsequent cascade levels operating at larger
resolutions. We train each cascade level sequentially on partial views of the
videos, which reduces the computational complexity of our model and makes it
scalable to high-resolution videos with many frames. We empirically validate
our approach on UCF101 and Kinetics-600, for which our model is competitive
with the state-of-the-art. We further demonstrate the scaling capabilities of
our model and train a three-level model on the BDD100K dataset which generates
256x256 pixels videos with 48 frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Multiplicative Filter Networks for Multiscale Reconstruction. (arXiv:2206.00746v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00746">
<div class="article-summary-box-inner">
<span><p>Coordinate networks like Multiplicative Filter Networks (MFNs) and BACON
offer some control over the frequency spectrum used to represent continuous
signals such as images or 3D volumes. Yet, they are not readily applicable to
problems for which coarse-to-fine estimation is required, including various
inverse problems in which coarse-to-fine optimization plays a key role in
avoiding poor local minima. We introduce a new coordinate network architecture
and training scheme that enables coarse-to-fine optimization with fine-grained
control over the frequency support of learned reconstructions. This is achieved
with two key innovations. First, we incorporate skip connections so that
structure at one scale is preserved when fitting finer-scale structure. Second,
we propose a novel initialization scheme to provide control over the model
frequency spectrum at each stage of optimization. We demonstrate how these
modifications enable multiscale optimization for coarse-to-fine fitting to
natural images. We then evaluate our model on synthetically generated datasets
for the the problem of single-particle cryo-EM reconstruction. We learn high
resolution multiscale structures, on par with the state-of-the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Linear Transformer for 3D Biomedical Image Segmentation. (arXiv:2206.00771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00771">
<div class="article-summary-box-inner">
<span><p>Transformer-based neural networks have surpassed promising performance on
many biomedical image segmentation tasks due to a better global information
modeling from the self-attention mechanism. However, most methods are still
designed for 2D medical images while ignoring the essential 3D volume
information. The main challenge for 3D transformer-based segmentation methods
is the quadratic complexity introduced by the self-attention mechanism
\cite{vaswani2017attention}. In this paper, we propose a novel transformer
architecture for 3D medical image segmentation using an encoder-decoder style
architecture with linear complexity. Furthermore, we newly introduce a dynamic
token concept to further reduce the token numbers for self-attention
calculation. Taking advantage of the global information modeling, we provide
uncertainty maps from different hierarchy stages. We evaluate this method on
multiple challenging CT pancreas segmentation datasets. Our promising results
show that our novel 3D Transformer-based segmentor could provide promising
highly feasible segmentation performance and accurate uncertainty
quantification using single annotation. Code is available
https://github.com/freshman97/LinTransUNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness. (arXiv:2206.00785v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00785">
<div class="article-summary-box-inner">
<span><p>Document understanding is a key business process in the data-driven economy
since documents are central to knowledge discovery and business insights.
Converting documents into a machine-processable format is a particular
challenge here due to their huge variability in formats and complex structure.
Accordingly, many algorithms and machine-learning methods emerged to solve
particular tasks such as Optical Character Recognition (OCR), layout analysis,
table-structure recovery, figure understanding, etc. We observe the adoption of
such methods in document understanding solutions offered by all major cloud
providers. Yet, publications outlining how such services are designed and
optimized to scale in the cloud are scarce. In this paper, we focus on the case
of document conversion to illustrate the particular challenges of scaling a
complex data processing pipeline with a strong reliance on machine-learning
methods on cloud infrastructure. Our key objective is to achieve high
scalability and responsiveness for different workload profiles in a
well-defined resource budget. We outline the requirements, design, and
implementation choices of our document conversion service and reflect on the
challenges we faced. Evidence for the scaling behavior and resource efficiency
is provided for two alternative workload distribution strategies and deployment
configurations. Our best-performing method achieves sustained throughput of
over one million PDF pages per hour on 3072 CPU cores across 192 nodes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction. (arXiv:2206.00790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00790">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning for computer vision has achieved tremendous progress
and improved many downstream vision tasks such as image classification,
semantic segmentation, and object detection. Among these, generative
self-supervised vision learning approaches such as MAE and BEiT show promising
performance. However, their global masked reconstruction mechanism is
computationally demanding. To address this issue, we propose local masked
reconstruction (LoMaR), a simple yet effective approach that performs masked
reconstruction within a small window of 7$\times$7 patches on a simple
Transformer encoder, improving the trade-off between efficiency and accuracy
compared to global masked reconstruction over the entire image. Extensive
experiments show that LoMaR reaches 84.1% top-1 accuracy on ImageNet-1K
classification, outperforming MAE by 0.5%. After finetuning the pretrained
LoMaR on 384$\times$384 images, it can reach 85.4% top-1 accuracy, surpassing
MAE by 0.6%. On MS COCO, LoMaR outperforms MAE by 0.5 $\text{AP}^\text{box}$ on
object detection and 0.5 $\text{AP}^\text{mask}$ on instance segmentation.
LoMaR is especially more computation-efficient on pretraining high-resolution
images, e.g., it is 3.1$\times$ faster than MAE with 0.2% higher classification
accuracy on pretraining 448$\times$448 images. This local masked reconstruction
learning mechanism can be easily integrated into any other generative
self-supervised learning approach. Our code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale frequency separation network for image deblurring. (arXiv:2206.00798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00798">
<div class="article-summary-box-inner">
<span><p>Image deblurring aims to restore the detailed texture information or
structures from the blurry images, which has become an indispensable step in
many computer-vision tasks. Although various methods have been proposed to deal
with the image deblurring problem, most of them treated the blurry image as a
whole and neglected the characteristics of different image frequencies. In this
paper, we present a new method called multi-scale frequency separation network
(MSFS-Net) for image deblurring. MSFS-Net introduces the frequency separation
module (FSM) into an encoder-decoder network architecture to capture the low
and high-frequency information of image at multiple scales. Then, a simple
cycle-consistency strategy and a sophisticated contrastive learning module
(CLM) are respectively designed to retain the low-frequency information and
recover the high-frequency information during deblurring. At last, the features
of different scales are fused by a cross-scale feature fusion module (CSFFM).
Extensive experiments on benchmark datasets show that the proposed network
achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CcHarmony: Color-checker based Image Harmonization Dataset. (arXiv:2206.00800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00800">
<div class="article-summary-box-inner">
<span><p>Image harmonization targets at adjusting the foreground in a composite image
to make it compatible with the background, producing a more realistic and
harmonious image. Training deep image harmonization network requires abundant
training data, but it is extremely difficult to acquire training pairs of
composite images and ground-truth harmonious images. Therefore, existing works
turn to adjust the foreground appearance in a real image to create a synthetic
composite image. However, such adjustment may not faithfully reflect the
natural illumination change of foreground. In this work, we explore a novel
transitive way to construct image harmonization dataset. Specifically, based on
the existing datasets with recorded illumination information, we first convert
the foreground in a real image to the standard illumination condition, and then
convert it to another illumination condition, which is combined with the
original background to form a synthetic composite image. In this manner, we
construct an image harmonization dataset called ccHarmony, which is named after
color checker (cc). The dataset is available at
https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XBound-Former: Toward Cross-scale Boundary Modeling in Transformers. (arXiv:2206.00806v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00806">
<div class="article-summary-box-inner">
<span><p>Skin lesion segmentation from dermoscopy images is of great significance in
the quantitative analysis of skin cancers, which is yet challenging even for
dermatologists due to the inherent issues, i.e., considerable size, shape and
color variation, and ambiguous boundaries. Recent vision transformers have
shown promising performance in handling the variation through global context
modeling. Still, they have not thoroughly solved the problem of ambiguous
boundaries as they ignore the complementary usage of the boundary knowledge and
global contexts. In this paper, we propose a novel cross-scale boundary-aware
transformer, \textbf{XBound-Former}, to simultaneously address the variation
and boundary problems of skin lesion segmentation. XBound-Former is a purely
attention-based network and catches boundary knowledge via three specially
designed learners. We evaluate the model on two skin lesion datasets,
ISIC-2016\&amp;PH$^2$ and ISIC-2018, where our model consistently outperforms other
convolution- and transformer-based models, especially on the boundary-wise
metrics. We extensively verify the generalization ability of polyp lesion
segmentation that has similar characteristics, and our model can also yield
significant improvement compared to the latest models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Knowledge from Object Classification to Aesthetics Assessment. (arXiv:2206.00809v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00809">
<div class="article-summary-box-inner">
<span><p>In this work, we point out that the major dilemma of image aesthetics
assessment (IAA) comes from the abstract nature of aesthetic labels. That is, a
vast variety of distinct contents can correspond to the same aesthetic label.
On the one hand, during inference, the IAA model is required to relate various
distinct contents to the same aesthetic label. On the other hand, when
training, it would be hard for the IAA model to learn to distinguish different
contents merely with the supervision from aesthetic labels, since aesthetic
labels are not directly related to any specific content. To deal with this
dilemma, we propose to distill knowledge on semantic patterns for a vast
variety of image contents from multiple pre-trained object classification (POC)
models to an IAA model. Expecting the combination of multiple POC models can
provide sufficient knowledge on various image contents, the IAA model can
easier learn to relate various distinct contents to a limited number of
aesthetic labels. By supervising an end-to-end single-backbone IAA model with
the distilled knowledge, the performance of the IAA model is significantly
improved by 4.8% in SRCC compared to the version trained only with ground-truth
aesthetic labels. On specific categories of images, the SRCC improvement
brought by the proposed method can achieve up to 7.2%. Peer comparison also
shows that our method outperforms 10 previous IAA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling sRGB Camera Noise with Normalizing Flows. (arXiv:2206.00812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00812">
<div class="article-summary-box-inner">
<span><p>Noise modeling and reduction are fundamental tasks in low-level computer
vision. They are particularly important for smartphone cameras relying on small
sensors that exhibit visually noticeable noise. There has recently been renewed
interest in using data-driven approaches to improve camera noise models via
neural networks. These data-driven approaches target noise present in the
raw-sensor image before it has been processed by the camera's image signal
processor (ISP). Modeling noise in the RAW-rgb domain is useful for improving
and testing the in-camera denoising algorithm; however, there are situations
where the camera's ISP does not apply denoising or additional denoising is
desired when the RAW-rgb domain image is no longer available. In such cases,
the sensor noise propagates through the ISP to the final rendered image encoded
in standard RGB (sRGB). The nonlinear steps on the ISP culminate in a
significantly more complex noise distribution in the sRGB domain and existing
raw-domain noise models are unable to capture the sRGB noise distribution. We
propose a new sRGB-domain noise model based on normalizing flows that is
capable of learning the complex noise distribution found in sRGB images under
various ISO levels. Our normalizing flows-based approach outperforms other
models by a large margin in noise modeling and synthesis tasks. We also show
that image denoisers trained on noisy images synthesized with our noise model
outperforms those trained with noise from baselines models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm and Casorati Matrix Nuclear Norm Regularizations. (arXiv:2206.00831v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00831">
<div class="article-summary-box-inner">
<span><p>Low-rank tensor models have been applied in accelerating dynamic magnetic
resonance imaging (dMRI). Recently, a new tensor nuclear norm based on t-SVD
has been proposed and applied to tensor completion. Inspired by the different
properties of the tensor nuclear norm (TNN) and the Casorati matrix nuclear
norm (MNN), we introduce a combined TNN and Casorati MNN regularizations
framework to reconstruct dMRI, which we term as TMNN. The proposed method
simultaneously exploits the spatial structure and the temporal correlation of
the dynamic MR data. The optimization problem can be efficiently solved by the
alternating direction method of multipliers (ADMM). In order to further improve
the computational efficiency, we develop a fast algorithm under the Cartesian
sampling scenario. Numerical experiments based on cardiac cine MRI and
perfusion MRI data demonstrate the performance improvement over the traditional
Casorati nuclear norm regularization method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00843">
<div class="article-summary-box-inner">
<span><p>Efficient deep neural network (DNN) models equipped with compact operators
(e.g., depthwise convolutions) have shown great potential in reducing DNNs'
theoretical complexity (e.g., the total number of weights/operations) while
maintaining a decent model accuracy. However, existing efficient DNNs are still
limited in fulfilling their promise in boosting real-hardware efficiency, due
to their commonly adopted compact operators' low hardware utilization. In this
work, we open up a new compression paradigm for developing real-hardware
efficient DNNs, leading to boosted hardware efficiency while maintaining model
accuracy. Interestingly, we observe that while some DNN layers' activation
functions help DNNs' training optimization and achievable accuracy, they can be
properly removed after training without compromising the model accuracy.
Inspired by this observation, we propose a framework dubbed DepthShrinker,
which develops hardware-friendly compact networks via shrinking the basic
building blocks of existing efficient DNNs that feature irregular computation
patterns into dense ones with much improved hardware utilization and thus
real-hardware efficiency. Excitingly, our DepthShrinker framework delivers
hardware-friendly compact networks that outperform both state-of-the-art
efficient DNNs and compression techniques, e.g., a 3.06\% higher accuracy and
1.53$\times$ throughput on Tesla V100 over SOTA channel-wise pruning method
MetaPruning. Our codes are available at:
https://github.com/RICE-EIC/DepthShrinker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperspherical Consistency Regularization. (arXiv:2206.00845v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00845">
<div class="article-summary-box-inner">
<span><p>Recent advances in contrastive learning have enlightened diverse applications
across various semi-supervised fields. Jointly training supervised learning and
unsupervised learning with a shared feature encoder becomes a common scheme.
Though it benefits from taking advantage of both feature-dependent information
from self-supervised learning and label-dependent information from supervised
learning, this scheme remains suffering from bias of the classifier. In this
work, we systematically explore the relationship between self-supervised
learning and supervised learning, and study how self-supervised learning helps
robust data-efficient deep learning. We propose hyperspherical consistency
regularization (HCR), a simple yet effective plug-and-play method, to
regularize the classifier using feature-dependent information and thus avoid
bias from labels. Specifically, HCR first projects logits from the classifier
and feature projections from the projection head on the respective hypersphere,
then it enforces data points on hyperspheres to have similar structures by
minimizing binary cross entropy of pairwise distances' similarity metrics.
Extensive experiments on semi-supervised and weakly-supervised learning
demonstrate the effectiveness of our method, by showing superior performance
with HCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic MRI using Learned Transform-based Deep Tensor Low-Rank Network (DTLR-Net). (arXiv:2206.00850v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00850">
<div class="article-summary-box-inner">
<span><p>While low-rank matrix prior has been exploited in dynamic MR image
reconstruction and has obtained satisfying performance, low-rank tensors models
have recently emerged as powerful alternative representations for
three-dimensional dynamic MR datasets. In this paper, we introduce a
model-based deep learning network by learning the tensor low-rank prior of the
cardiac dynamic MR images. Instead of representing the dynamic dataset as a
low-rank tensor directly, we propose a learned transformation operator to
exploit the tensor low-rank property in a transform domain. In particular, by
generalizing the t-SVD tensor decomposition into a unitary transformed t-SVD,
we define a transformed tensor nuclear norm (TTNN) to enforce the tensor
low-rankness. The dynamic MRI reconstruction problem is thus formulated using a
TTNN regularized optimization problem. An iterative algorithm based on ADMM
used to minimize the cost is unrolled into a deep network, where the transform
is learned using convolutional neural networks (CNNs) to promote the
reconstruction quality in the feature domain. Experimental results on cardiac
cine MRI reconstruction demonstrate that the proposed framework is able to
provide improved recovery results compared with the state-of-the-art
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Generation Network for Enlarged License Plate Recognition and A Unified Dataset. (arXiv:2206.00859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00859">
<div class="article-summary-box-inner">
<span><p>License plate recognition plays a critical role in many practical
applications, but license plates of large vehicles are difficult to be
recognized due to the factors of low resolution, contamination, low
illumination, and occlusion, to name a few. To overcome the above factors, the
transportation management department generally introduces the enlarged license
plate behind the rear of a vehicle. However, enlarged license plates have high
diversity as they are non-standard in position, size, and style. Furthermore,
the background regions contain a variety of noisy information which greatly
disturbs the recognition of license plate characters. Existing works have not
studied this challenging problem. In this work, we first address the enlarged
license plate recognition problem and contribute a dataset containing 9342
images, which cover most of the challenges of real scenes. However, the created
data are still insufficient to train deep methods of enlarged license plate
recognition, and building large-scale training data is very time-consuming and
high labor cost. To handle this problem, we propose a novel task-level
disentanglement generation framework based on the Disentangled Generation
Network (DGNet), which disentangles the generation into the text generation and
background generation in an end-to-end manner to effectively ensure diversity
and integrity, for robust enlarged license plate recognition. Extensive
experiments on the created dataset are conducted, and we demonstrate the
effectiveness of the proposed approach in three representative text recognition
frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientNeRF: Efficient Neural Radiance Fields. (arXiv:2206.00878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00878">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) has been wildly applied to various tasks for
its high-quality representation of 3D scenes. It takes long per-scene training
time and per-image testing time. In this paper, we present EfficientNeRF as an
efficient NeRF-based method to represent 3D scene and synthesize novel-view
images. Although several ways exist to accelerate the training or testing
process, it is still difficult to much reduce time for both phases
simultaneously. We analyze the density and weight distribution of the sampled
points then propose valid and pivotal sampling at the coarse and fine stage,
respectively, to significantly improve sampling efficiency. In addition, we
design a novel data structure to cache the whole scene during testing to
accelerate the rendering speed. Overall, our method can reduce over 88\% of
training time, reach rendering speed of over 200 FPS, while still achieving
competitive accuracy. Experiments prove that our method promotes the
practicality of NeRF in the real world and enables many applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Systematic Knowledge of 2D Transformations. (arXiv:2206.00893v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00893">
<div class="article-summary-box-inner">
<span><p>The existing deep learning models suffer from out-of-distribution (o.o.d.)
performance drop in computer vision tasks. In comparison, humans have a
remarkable ability to interpret images, even if the scenes in the images are
rare, thanks to the systematicity of acquired knowledge. This work focuses on
1) the acquisition of systematic knowledge of 2D transformations, and 2)
architectural components that can leverage the learned knowledge in image
classification tasks in an o.o.d. setting. With a new training methodology
based on synthetic datasets that are constructed under the causal framework,
the deep neural networks acquire knowledge from semantically different domains
(e.g. even from noise), and exhibit certain level of systematicity in parameter
estimation experiments. Based on this, a novel architecture is devised
consisting of a classifier, an estimator and an identifier (abbreviated as
"CED"). By emulating the "hypothesis-verification" process in human visual
perception, CED improves the classification accuracy significantly on test sets
under covariate shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture Imagery. (arXiv:2206.00897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00897">
<div class="article-summary-box-inner">
<span><p>Unsustainable fishing practices worldwide pose a major threat to marine
resources and ecosystems. Identifying vessels that evade monitoring systems --
known as "dark vessels" -- is key to managing and securing the health of marine
environments. With the rise of satellite-based synthetic aperture radar (SAR)
imaging and modern machine learning (ML), it is now possible to automate
detection of dark vessels day or night, under all-weather conditions. SAR
images, however, require domain-specific treatment and is not widely accessible
to the ML community. Moreover, the objects (vessels) are small and sparse,
challenging traditional computer vision approaches. We present the largest
labeled dataset for training ML models to detect and characterize vessels from
SAR. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the
Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The
images are annotated using a combination of automated and manual analysis.
Co-located bathymetry and wind state rasters accompany every SAR image. We
provide an overview of the results from the xView3 Computer Vision Challenge,
an international competition using xView3-SAR for ship detection and
characterization at large scale. We release the data (https://iuu.xview.us/)
and code (https://github.com/DIUx-xView) to support ongoing development and
evaluation of ML approaches for this important application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISSU: 3D Medical Image Segmentation via Self-distilling TransUNet. (arXiv:2206.00902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00902">
<div class="article-summary-box-inner">
<span><p>U-Nets have achieved tremendous success in medical image segmentation.
Nevertheless, it may suffer limitations in global (long-range) contextual
interactions and edge-detail preservation. In contrast, Transformer has an
excellent ability to capture long-range dependencies by leveraging the
self-attention mechanism into the encoder. Although Transformer was born to
model the long-range dependency on the extracted feature maps, it still suffers
from extreme computational and spatial complexities in processing
high-resolution 3D feature maps. This motivates us to design the efficiently
Transformer-based UNet model and study the feasibility of Transformer-based
network architectures for medical image segmentation tasks. To this end, we
propose to self-distill a Transformer-based UNet for medical image
segmentation, which simultaneously learns global semantic information and local
spatial-detailed features. Meanwhile, a local multi-scale fusion block is first
proposed to refine fine-grained details from the skipped connections in the
encoder by the main CNN stem through self-distillation, only computed during
training and removed at inference with minimal overhead. Extensive experiments
on BraTS 2019 and CHAOS datasets show that our MISSU achieves the best
performance over previous state-of-the-art methods. Code and models are
available at \url{https://github.com/wangn123/MISSU.git}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask-Guided Divergence Loss Improves the Generalization and Robustness of Deep Neural Network. (arXiv:2206.00913v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00913">
<div class="article-summary-box-inner">
<span><p>Deep neural network (DNN) with dropout can be regarded as an ensemble model
consisting of lots of sub-DNNs (i.e., an ensemble sub-DNN where the sub-DNN is
the remaining part of the DNN after dropout), and through increasing the
diversity of the ensemble sub-DNN, the generalization and robustness of the DNN
can be effectively improved. In this paper, a mask-guided divergence loss
function (MDL), which consists of a cross-entropy loss term and an orthogonal
term, is proposed to increase the diversity of the ensemble sub-DNN by the
added orthogonal term. Particularly, the mask technique is introduced to assist
in generating the orthogonal term for avoiding overfitting of the diversity
learning. The theoretical analysis and extensive experiments on 4 datasets
(i.e., MNIST, FashionMNIST, CIFAR10, and CIFAR100) manifest that MDL can
improve the generalization and robustness of standard training and adversarial
training. For CIFAR10 and CIFAR100, in standard training, the maximum
improvement of accuracy is $1.38\%$ on natural data, $30.97\%$ on FGSM (i.e.,
Fast Gradient Sign Method) attack, $38.18\%$ on PGD (i.e., Projected Gradient
Descent) attack. While in adversarial training, the maximum improvement is
$1.68\%$ on natural data, $4.03\%$ on FGSM attack and $2.65\%$ on PGD attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Image Composition for Complex Scene Generation. (arXiv:2206.00923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00923">
<div class="article-summary-box-inner">
<span><p>We present a method that achieves state-of-the-art results on challenging
(few-shot) layout-to-image generation tasks by accurately modeling textures,
structures and relationships contained in a complex scene. After compressing
RGB images into patch tokens, we propose the Transformer with Focal Attention
(TwFA) for exploring dependencies of object-to-object, object-to-patch and
patch-to-patch. Compared to existing CNN-based and Transformer-based generation
models that entangled modeling on pixel-level&amp;patch-level and
object-level&amp;patch-level respectively, the proposed focal attention predicts
the current patch token by only focusing on its highly-related tokens that
specified by the spatial layout, thereby achieving disambiguation during
training. Furthermore, the proposed TwFA largely increases the data efficiency
during training, therefore we propose the first few-shot complex scene
generation strategy based on the well-trained TwFA. Comprehensive experiments
show the superiority of our method, which significantly increases both
quantitative metrics and qualitative visual realism with respect to
state-of-the-art CNN-based and transformer-based methods. Code is available at
https://github.com/JohnDreamer/TwFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FACM: Correct the Output of Deep Neural Network with Middle Layers Features against Adversarial Samples. (arXiv:2206.00924v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00924">
<div class="article-summary-box-inner">
<span><p>In the strong adversarial attacks against deep neural network (DNN), the
output of DNN will be misclassified if and only if the last feature layer of
the DNN is completely destroyed by adversarial samples, while our studies found
that the middle feature layers of the DNN can still extract the effective
features of the original normal category in these adversarial attacks. To this
end, in this paper, a middle $\bold{F}$eature layer $\bold{A}$nalysis and
$\bold{C}$onditional $\bold{M}$atching prediction distribution (FACM) model is
proposed to increase the robustness of the DNN against adversarial samples
through correcting the output of DNN with the features extracted by the middle
layers of DNN. In particular, the middle $\bold{F}$eature layer
$\bold{A}$nalysis (FA) module, the conditional matching prediction distribution
(CMPD) module and the output decision module are included in our FACM model to
collaboratively correct the classification of adversarial samples. The
experiments results show that, our FACM model can significantly improve the
robustness of the naturally trained model against various attacks, and our FA
model can significantly improve the robustness of the adversarially trained
model against white-box attacks with weak transferability and black box attacks
where FA model includes the FA module and the output decision module, not the
CMPD module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Physical Object Properties from Video. (arXiv:2206.00930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00930">
<div class="article-summary-box-inner">
<span><p>We present a novel approach to estimating physical properties of objects from
video. Our approach consists of a physics engine and a correction estimator.
Starting from the initial observed state, object behavior is simulated forward
in time. Based on the simulated and observed behavior, the correction estimator
then determines refined physical parameters for each object. The method can be
iterated for increased precision. Our approach is generic, as it allows for the
use of an arbitrary - not necessarily differentiable - physics engine and
correction estimator. For the latter, we evaluate both gradient-free
hyperparameter optimization and a deep convolutional neural network. We
demonstrate faster and more robust convergence of the learned method in several
simulated 2D scenarios focusing on bin situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00941">
<div class="article-summary-box-inner">
<span><p>Recently, diffusion models have been used to solve various inverse problems
in an unsupervised manner with appropriate modifications to the sampling
process. However, the current solvers, which recursively apply a reverse
diffusion step followed by a measurement consistency step, often produce
sub-optimal results. By studying the generative sampling path, here we show
that current solvers throw the sample path off the data manifold, and hence the
error accumulates. To address this, we propose an additional correction term
inspired by the manifold constraint, which can be used synergistically with the
previous solvers to make the iterations close to the manifold. The proposed
manifold constraint is straightforward to implement within a few lines of code,
yet boosts the performance by a surprisingly large margin. With extensive
experiments, we show that our method is superior to the previous methods both
theoretically and empirically, producing promising results in many applications
such as image inpainting, colorization, and sparse-view computed tomography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Space Particle Inference for Neural Network Ensembles. (arXiv:2206.00944v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00944">
<div class="article-summary-box-inner">
<span><p>Ensembles of deep neural networks demonstrate improved performance over
single models. For enhancing the diversity of ensemble members while keeping
their performance, particle-based inference methods offer a promising approach
from a Bayesian perspective. However, the best way to apply these methods to
neural networks is still unclear: seeking samples from the weight-space
posterior suffers from inefficiency due to the over-parameterization issues,
while seeking samples directly from the function-space posterior often results
in serious underfitting. In this study, we propose optimizing particles in the
feature space where the activation of a specific intermediate layer lies to
address the above-mentioned difficulties. Our method encourages each member to
capture distinct features, which is expected to improve ensemble prediction
robustness. Extensive evaluation on real-world datasets shows that our model
significantly outperforms the gold-standard Deep Ensembles on various metrics,
including accuracy, calibration, and robustness. Code is available at
https://github.com/DensoITLab/featurePI .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bhattacharyya Coefficient-Based Framework for Noise Model-Aware Random Walker Image Segmentation. (arXiv:2206.00947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00947">
<div class="article-summary-box-inner">
<span><p>One well established method of interactive image segmentation is the random
walker algorithm. Considerable research on this family of segmentation methods
has been continuously conducted in recent years with numerous applications.
These methods are common in using a simple Gaussian weight function which
depends on a parameter that strongly influences the segmentation performance.
In this work we propose a general framework of deriving weight functions based
on probabilistic modeling. This framework can be concretized to cope with
virtually any well-defined noise model. It eliminates the critical parameter
and thus avoids time-consuming parameter search. We derive the specific weight
functions for common noise types and show their superior performance on
synthetic data as well as different biomedical image data (MRI images from the
NYU fastMRI dataset, larvae images acquired with the FIM technique). Our
framework can also be used in multiple other applications, e.g., the graph cut
algorithm and its extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparseDet: Towards End-to-End 3D Object Detection. (arXiv:2206.00960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00960">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose SparseDet for end-to-end 3D object detection from
point cloud. Existing works on 3D object detection rely on dense object
candidates over all locations in a 3D or 2D grid following the mainstream
methods for object detection in 2D images. However, this dense paradigm
requires expertise in data to fulfill the gap between label and detection. As a
new detection paradigm, SparseDet maintains a fixed set of learnable proposals
to represent latent candidates and directly perform classification and
localization for 3D objects through stacked transformers. It demonstrates that
effective 3D object detection can be achieved with none of post-processing such
as redundant removal and non-maximum suppression. With a properly designed
network, SparseDet achieves highly competitive detection accuracy while running
with a more efficient speed of 34.5 FPS. We believe this end-to-end paradigm of
SparseDet will inspire new thinking on the sparsity of 3D object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron. (arXiv:2206.00971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00971">
<div class="article-summary-box-inner">
<span><p>Cervical cancer is the seventh most common cancer among all the cancers
worldwide and the fourth most common cancer among women. Cervical cytopathology
image classification is an important method to diagnose cervical cancer. Manual
screening of cytopathology images is time-consuming and error-prone. The
emergence of the automatic computer-aided diagnosis system solves this problem.
This paper proposes a framework called CVM-Cervix based on deep learning to
perform cervical cell classification tasks. It can analyze pap slides quickly
and accurately. CVM-Cervix first proposes a Convolutional Neural Network module
and a Visual Transformer module for local and global feature extraction
respectively, then a Multilayer Perceptron module is designed to fuse the local
and global features for the final classification. Experimental results show the
effectiveness and potential of the proposed CVM-Cervix in the field of cervical
Pap smear image classification. In addition, according to the practical needs
of clinical work, we perform a lightweight post-processing to compress the
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StopNet: Scalable Trajectory and Occupancy Prediction for Urban Autonomous Driving. (arXiv:2206.00991v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00991">
<div class="article-summary-box-inner">
<span><p>We introduce a motion forecasting (behavior prediction) method that meets the
latency requirements for autonomous driving in dense urban environments without
sacrificing accuracy. A whole-scene sparse input representation allows StopNet
to scale to predicting trajectories for hundreds of road agents with reliable
latency. In addition to predicting trajectories, our scene encoder lends itself
to predicting whole-scene probabilistic occupancy grids, a complementary output
representation suitable for busy urban environments. Occupancy grids allow the
AV to reason collectively about the behavior of groups of agents without
processing their individual trajectories. We demonstrate the effectiveness of
our sparse input representation and our model in terms of computation and
accuracy over three datasets. We further show that co-training consistent
trajectory and occupancy predictions improves upon state-of-the-art performance
under standard metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Mapping Necessary for Realistic PointGoal Navigation?. (arXiv:2206.00997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00997">
<div class="article-summary-box-inner">
<span><p>Can an autonomous agent navigate in a new environment without building an
explicit map?
</p>
<p>For the task of PointGoal navigation ('Go to $\Delta x$, $\Delta y$') under
idealized settings (no RGB-D and actuation noise, perfect GPS+Compass), the
answer is a clear 'yes' - map-less neural models composed of task-agnostic
components (CNNs and RNNs) trained with large-scale reinforcement learning
achieve 100% Success on a standard dataset (Gibson). However, for PointNav in a
realistic setting (RGB-D and actuation noise, no GPS+Compass), this is an open
question; one we tackle in this paper. The strongest published result for this
task is 71.7% Success.
</p>
<p>First, we identify the main (perhaps, only) cause of the drop in performance:
the absence of GPS+Compass. An agent with perfect GPS+Compass faced with RGB-D
sensing and actuation noise achieves 99.8% Success (Gibson-v2 val). This
suggests that (to paraphrase a meme) robust visual odometry is all we need for
realistic PointNav; if we can achieve that, we can ignore the sensing and
actuation noise.
</p>
<p>With that as our operating hypothesis, we scale the dataset and model size,
and develop human-annotation-free data-augmentation techniques to train models
for visual odometry. We advance the state of art on the Habitat Realistic
PointNav Challenge from 71% to 94% Success (+32, 4% relative) and 53% to 74%
SPL (+39, 6% relative). While our approach does not saturate or 'solve' this
dataset, this strong improvement combined with promising zero-shot sim2real
transfer (to a LoCoBot) provides evidence consistent with the hypothesis that
explicit mapping may not be necessary for navigation, even in a realistic
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing One Sided Margin Loss for Solving Classification Problems in Deep Networks. (arXiv:2206.01002v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01002">
<div class="article-summary-box-inner">
<span><p>This paper introduces a new loss function, OSM (One-Sided Margin), to solve
maximum-margin classification problems effectively. Unlike the hinge loss, in
OSM the margin is explicitly determined with corresponding hyperparameters and
then the classification problem is solved. In experiments, we observe that
using OSM loss leads to faster training speeds and better accuracies than
binary and categorical cross-entropy in several commonly used deep models for
classification and optical character recognition problems.
</p>
<p>OSM has consistently shown better classification accuracies over
cross-entropy and hinge losses for small to large neural networks. it has also
led to a more efficient training procedure. We achieved state-of-the-art
accuracies for small networks on several benchmark datasets of
CIFAR10(98.82\%), CIFAR100(91.56\%), Flowers(98.04\%), Stanford Cars(93.91\%)
with considerable improvements over other loss functions. Moreover, the
accuracies are rather better than cross-entropy and hinge loss for large
networks. Therefore, we strongly believe that OSM is a powerful alternative to
hinge and cross-entropy losses to train deep neural networks on classification
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Recurrence Modeling for Video Action Anticipation. (arXiv:2206.01009v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01009">
<div class="article-summary-box-inner">
<span><p>Forecasting future events based on evidence of current conditions is an
innate skill of human beings, and key for predicting the outcome of any
decision making. In artificial vision for example, we would like to predict the
next human action before it happens, without observing the future video frames
associated to it. Computer vision models for action anticipation are expected
to collect the subtle evidence in the preamble of the target actions. In prior
studies recurrence modeling often leads to better performance, the strong
temporal inference is assumed to be a key element for reasonable prediction. To
this end, we propose a unified recurrence modeling for video action
anticipation via message passing framework. The information flow in space-time
can be described by the interaction between vertices and edges, and the changes
of vertices for each incoming frame reflects the underlying dynamics. Our model
leverages self-attention as the building blocks for each of the message passing
functions. In addition, we introduce different edge learning strategies that
can be end-to-end optimized to gain better flexibility for the connectivity
between vertices. Our experimental results demonstrate that our proposed method
outperforms previous works on the large-scale EPIC-Kitchen dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-tailed Recognition by Learning from Latent Categories. (arXiv:2206.01010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01010">
<div class="article-summary-box-inner">
<span><p>In this work, we address the challenging task of long-tailed image
recognition. Previous long-tailed recognition methods commonly focus on the
data augmentation or re-balancing strategy of the tail classes to give more
attention to tail classes during the model training. However, due to the
limited training images for tail classes, the diversity of tail class images is
still restricted, which results in poor feature representations. In this work,
we hypothesize that common latent features among the head and tail classes can
be used to give better feature representation. Motivated by this, we introduce
a Latent Categories based long-tail Recognition (LCReg) method. Specifically,
we propose to learn a set of class-agnostic latent features shared among the
head and tail classes. Then, we implicitly enrich the training sample diversity
via applying semantic data augmentation to the latent features. Extensive
experiments on five long-tailed image recognition datasets demonstrate that our
proposed LCReg is able to significantly outperform previous methods and achieve
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suggestive Annotation of Brain MR Images with Gradient-guided Sampling. (arXiv:2206.01014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01014">
<div class="article-summary-box-inner">
<span><p>Machine learning has been widely adopted for medical image analysis in recent
years given its promising performance in image segmentation and classification
tasks. The success of machine learning, in particular supervised learning,
depends on the availability of manually annotated datasets. For medical imaging
applications, such annotated datasets are not easy to acquire, it takes a
substantial amount of time and resource to curate an annotated medical image
set. In this paper, we propose an efficient annotation framework for brain MR
images that can suggest informative sample images for human experts to
annotate. We evaluate the framework on two different brain image analysis
tasks, namely brain tumour segmentation and whole brain segmentation.
Experiments show that for brain tumour segmentation task on the BraTS 2019
dataset, training a segmentation model with only 7% suggestively annotated
image samples can achieve a performance comparable to that of training on the
full dataset. For whole brain segmentation on the MALC dataset, training with
42% suggestively annotated image samples can achieve a comparable performance
to training on the full dataset. The proposed framework demonstrates a
promising way to save manual annotation cost and improve data efficiency in
medical imaging applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Two-stream Attention Network for Video Question Answering. (arXiv:2206.01017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01017">
<div class="article-summary-box-inner">
<span><p>To date, visual question answering (VQA) (i.e., image QA and video QA) is
still a holy grail in vision and language understanding, especially for video
QA. Compared with image QA that focuses primarily on understanding the
associations between image region-level details and corresponding questions,
video QA requires a model to jointly reason across both spatial and long-range
temporal structures of a video as well as text to provide an accurate answer.
In this paper, we specifically tackle the problem of video QA by proposing a
Structured Two-stream Attention network, namely STA, to answer a free-form or
open-ended natural language question about the content of a given video. First,
we infer rich long-range temporal structures in videos using our structured
segment component and encode text features. Then, our structured two-stream
attention component simultaneously localizes important visual instance, reduces
the influence of background video and focuses on the relevant text. Finally,
the structured two-stream fusion component incorporates different segments of
query and video aware context representation and infers the answers.
Experiments on the large-scale video QA dataset \textit{TGIF-QA} show that our
proposed method significantly surpasses the best counterpart (i.e., with one
representation for the video input) by 13.0%, 13.5%, 11.0% and 0.3 for Action,
Trans., TrameQA and Count tasks. It also outperforms the best competitor (i.e.,
with two representations) on the Action, Trans., TrameQA tasks by 4.1%, 4.7%,
and 5.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Laser Spot: Robust and Covert Physical Adversarial Attack to DNNs. (arXiv:2206.01034v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01034">
<div class="article-summary-box-inner">
<span><p>Most existing deep neural networks (DNNs) are easily disturbed by slight
noise. As far as we know, there are few researches on physical adversarial
attack technology by deploying lighting equipment. The light-based physical
adversarial attack technology has excellent covertness, which brings great
security risks to many applications based on deep neural networks (such as
automatic driving technology). Therefore, we propose a robust physical
adversarial attack technology with excellent covertness, called adversarial
laser point (AdvLS), which optimizes the physical parameters of laser point
through genetic algorithm to perform physical adversarial attack. It realizes
robust and covert physical adversarial attack by using low-cost laser
equipment. As far as we know, AdvLS is the first light-based adversarial attack
technology that can perform physical adversarial attacks in the daytime. A
large number of experiments in the digital and physical environments show that
AdvLS has excellent robustness and concealment. In addition, through in-depth
analysis of the experimental data, we find that the adversarial perturbations
generated by AdvLS have superior adversarial attack migration. The experimental
results show that AdvLS impose serious interference to the advanced deep neural
networks, we call for the attention of the proposed physical adversarial attack
technology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications. (arXiv:2206.01038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01038">
<div class="article-summary-box-inner">
<span><p>To understand human behaviors, action recognition based on videos is a common
approach. Compared with image-based action recognition, videos provide much
more information. Reducing the ambiguity of actions and in the last decade,
many works focused on datasets, novel models and learning approaches have
improved video action recognition to a higher level. However, there are
challenges and unsolved problems, in particular in sports analytics where data
collection and labeling are more sophisticated, requiring sport professionals
to annotate data. In addition, the actions could be extremely fast and it
becomes difficult to recognize them. Moreover, in team sports like football and
basketball, one action could involve multiple players, and to correctly
recognize them, we need to analyse all players, which is relatively
complicated. In this paper, we present a survey on video action recognition for
sports analytics. We introduce more than ten types of sports, including team
sports, such as football, basketball, volleyball, hockey and individual sports,
such as figure skating, gymnastics, table tennis, tennis, diving and badminton.
Then we compare numerous existing frameworks for sports analysis to present
status quo of video action recognition in both team sports and individual
sports. Finally, we discuss the challenges and unsolved problems in this area
and to facilitate sports analytics, we develop a toolbox using PaddlePaddle,
which supports football, basketball, table tennis and figure skating action
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FV-UPatches: Enhancing Universality in Finger Vein Recognition. (arXiv:2206.01061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01061">
<div class="article-summary-box-inner">
<span><p>Many deep learning-based models have been introduced in finger vein
recognition in recent years. These solutions, however, suffer from data
dependency and are difficult to achieve model generalization. To address this
problem, we are inspired by the idea of domain adaptation and propose a
universal learning-based framework, which achieves generalization while
training with limited data. To reduce differences between data distributions, a
compressed U-Net is introduced as a domain mapper to map the raw region of
interest image onto a target domain. The concentrated target domain is a
unified feature space for the subsequent matching, in which a local descriptor
model SOSNet is employed to embed patches into descriptors measuring the
similarity of matching pairs. In the proposed framework, the domain mapper is
an approximation to a specific extraction function thus the training is only a
one-time effort with limited data. Moreover, the local descriptor model can be
trained to be representative enough based on a public dataset of
non-finger-vein images. The whole pipeline enables the framework to be well
generalized, making it possible to enhance universality and helps to reduce
costs of data collection, tuning and retraining. The comparable experimental
results to state-of-the-art (SOTA) performance in five public datasets prove
the effectiveness of the proposed framework. Furthermore, the framework shows
application potential in other vein-based biometric recognition as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis. (arXiv:2206.01062v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01062">
<div class="article-summary-box-inner">
<span><p>Accurate document layout analysis is a key requirement for high-quality PDF
document conversion. With the recent availability of public, large ground-truth
datasets such as PubLayNet and DocBank, deep-learning models have proven to be
very effective at layout detection and segmentation. While these datasets are
of adequate size to train such models, they severely lack in layout variability
since they are sourced from scientific article repositories such as PubMed and
arXiv only. Consequently, the accuracy of the layout segmentation drops
significantly when these models are applied on more challenging and diverse
layouts. In this paper, we present \textit{DocLayNet}, a new, publicly
available, document-layout annotation dataset in COCO format. It contains 80863
manually annotated pages from diverse data sources to represent a wide
variability in layouts. For each PDF page, the layout annotations provide
labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also
provides a subset of double- and triple-annotated pages to determine the
inter-annotator agreement. In multiple experiments, we provide baseline
accuracy scores (in mAP) for a set of popular object detection models. We also
demonstrate that these models fall approximately 10\% behind the
inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is
of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and
DocLayNet, showing that layout predictions of the DocLayNet-trained models are
more robust and thus the preferred choice for general-purpose document-layout
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning. (arXiv:2206.01088v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01088">
<div class="article-summary-box-inner">
<span><p>Cancer is a fatal disease caused by a combination of genetic diseases and a
variety of biochemical abnormalities. Lung and colon cancer have emerged as two
of the leading causes of death and disability in humans. The histopathological
detection of such malignancies is usually the most important component in
determining the best course of action. Early detection of the ailment on either
front considerably decreases the likelihood of mortality. Machine learning and
deep learning techniques can be utilized to speed up such cancer detection,
allowing researchers to study a large number of patients in a much shorter
amount of time and at a lower cost. In this research work, we introduced a
hybrid ensemble feature extraction model to efficiently identify lung and colon
cancer. It integrates deep feature extraction and ensemble learning with
high-performance filtering for cancer image datasets. The model is evaluated on
histopathological (LC25000) lung and colon datasets. According to the study
findings, our hybrid model can detect lung, colon, and (lung and colon) cancer
with accuracy rates of 99.05%, 100%, and 99.30%, respectively. The study's
findings show that our proposed strategy outperforms existing models
significantly. Thus, these models could be applicable in clinics to support the
doctor in the diagnosis of cancers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A DTCWT-SVD Based Video Watermarking resistant to frame rate conversion. (arXiv:2206.01094v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01094">
<div class="article-summary-box-inner">
<span><p>Videos can be easily tampered, copied and redistributed by attackers for
illegal and monetary usage. Such behaviors severely jeopardize the interest of
content owners. Despite huge efforts made in digital video watermarking for
copyright protection, typical distortions in video transmission including
signal attacks, geometric attacks and temporal synchronization attacks can
still easily erase the embedded signal. Among them, temporal synchronization
attacks which include frame dropping, frame insertion and frame rate conversion
is one of the most prevalent attacks. To address this issue, we present a new
video watermarking based on joint Dual-Tree Cosine Wavelet Transformation
(DTCWT) and Singular Value Decomposition (SVD), which is resistant to frame
rate conversion. We first extract a set of candidate coefficient by applying
SVD decomposition after DTCWT transform. Then, we simulate the watermark
embedding by adjusting the shape of candidate coefficient. Finally, we perform
group-level watermarking that includes moderate temporal redundancy to resist
temporal desynchronization attacks. Extensive experimental results show that
the proposed scheme is more resilient to temporal desynchronization attacks and
performs better than the existing blind video watermarking schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-fusion Semantic Segmentation Framework With GAN For SAR Images. (arXiv:2206.01096v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01096">
<div class="article-summary-box-inner">
<span><p>Deep learning based semantic segmentation is one of the popular methods in
remote sensing image segmentation. In this paper, a network based on the widely
used encoderdecoder architecture is proposed to accomplish the synthetic
aperture radar (SAR) images segmentation. With the better representation
capability of optical images, we propose to enrich SAR images with generated
optical images via the generative adversative network (GAN) trained by numerous
SAR and optical images. These optical images can be used as expansions of
original SAR images, thus ensuring robust result of segmentation. Then the
optical images generated by the GAN are stitched together with the
corresponding real images. An attention module following the stitched data is
used to strengthen the representation of the objects. Experiments indicate that
our method is efficient compared to other commonly used methods
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection. (arXiv:2206.01102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01102">
<div class="article-summary-box-inner">
<span><p>We propose a stealthy clean-label video backdoor attack against Deep Learning
(DL)-based models aiming at detecting a particular class of spoofing attacks,
namely video rebroadcast attacks. The injected backdoor does not affect
spoofing detection in normal conditions, but induces a misclassification in the
presence of a specific triggering signal. The proposed backdoor relies on a
temporal trigger altering the average chrominance of the video sequence. The
backdoor signal is designed by taking into account the peculiarities of the
Human Visual System (HVS) to reduce the visibility of the trigger, thus
increasing the stealthiness of the backdoor. To force the network to look at
the presence of the trigger in the challenging clean-label scenario, we choose
the poisoned samples used for the injection of the backdoor following a
so-called Outlier Poisoning Strategy (OPS). According to OPS, the triggering
signal is inserted in the training samples that the network finds more
difficult to classify. The effectiveness of the proposed backdoor attack and
its generality are validated experimentally on different datasets and
anti-spoofing rebroadcast detection architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise2NoiseFlow: Realistic Camera Noise Modeling without Clean Images. (arXiv:2206.01103v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01103">
<div class="article-summary-box-inner">
<span><p>Image noise modeling is a long-standing problem with many applications in
computer vision. Early attempts that propose simple models, such as
signal-independent additive white Gaussian noise or the heteroscedastic
Gaussian noise model (a.k.a., camera noise level function) are not sufficient
to learn the complex behavior of the camera sensor noise. Recently, more
complex learning-based models have been proposed that yield better results in
noise synthesis and downstream tasks, such as denoising. However, their
dependence on supervised data (i.e., paired clean images) is a limiting factor
given the challenges in producing ground-truth images. This paper proposes a
framework for training a noise model and a denoiser simultaneously while
relying only on pairs of noisy images rather than noisy/clean paired image
data. We apply this framework to the training of the Noise Flow architecture.
The noise synthesis and density estimation results show that our framework
outperforms previous signal-processing-based noise models and is on par with
its supervised counterpart. The trained denoiser is also shown to significantly
improve upon both supervised and weakly supervised baseline denoising
approaches. The results indicate that the joint training of a denoiser and a
noise model yields significant improvements in the denoiser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01118">
<div class="article-summary-box-inner">
<span><p>Diabetic retinopathy is an eye-related pathology creating abnormalities and
causing visual impairment, proper treatment of which requires identifying
irregularities. This research uses a hemorrhage detection method and compares
classification of conventional and deep features. Especially, method identifies
hemorrhage connected with blood vessels or reside at retinal border and
reported challenging. Initially, adaptive brightness adjustment and contrast
enhancement rectify degraded images. Prospective locations of hemorrhages are
estimated by a Gaussian matched filter, entropy thresholding, and morphological
operation. Hemorrhages are segmented by a novel technique based on regional
variance of intensities. Features are then extracted by conventional methods
and deep models for training support vector machines, and results evaluated.
Evaluation metrics for each model are promising, but findings suggest that
comparatively, deep models are more effective than conventional features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix Conditioning Unifies Language and Label Supervision. (arXiv:2206.01125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01125">
<div class="article-summary-box-inner">
<span><p>Vision-language contrastive learning suggests a new learning paradigm by
leveraging a large amount of image-caption-pair data. The caption supervision
excels at providing wide coverage in vocabulary that enables strong zero-shot
image recognition performance. On the other hand, label supervision offers to
learn more targeted visual representations that are label-oriented and can
cover rare categories. To gain the complementary advantages of both kinds of
supervision for contrastive image-caption pre-training, recent works have
proposed to convert class labels into a sentence with pre-defined templates
called prompts. However, a naive unification of the real caption and the prompt
sentences could lead to a complication in learning, as the distribution shift
in text may not be handled properly in the language encoder. In this work, we
propose a simple yet effective approach to unify these two types of supervision
using prefix tokens that inform a language encoder of the type of the input
sentence (e.g., caption or prompt) at training time. Our method is generic and
can be easily integrated into existing VL pre-training objectives such as CLIP
or UniCL. In experiments, we show that this simple technique dramatically
improves the performance in zero-shot image recognition accuracy of the
pre-trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-BEiT: Generative Vision-Language Pretraining. (arXiv:2206.01127v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01127">
<div class="article-summary-box-inner">
<span><p>We introduce a vision-language foundation model called VL-BEiT, which is a
bidirectional multimodal Transformer learned by generative pretraining. Our
minimalist solution conducts masked prediction on both monomodal and multimodal
data with a shared Transformer. Specifically, we perform masked vision-language
modeling on image-text pairs, masked language modeling on texts, and masked
image modeling on images. VL-BEiT is learned from scratch with one unified
pretraining task, one shared backbone, and one-stage training. Our method is
conceptually simple and empirically effective. Experimental results show that
VL-BEiT obtains strong results on various vision-language benchmarks, such as
visual question answering, visual reasoning, and image-text retrieval.
Moreover, our method learns transferable visual features, achieving competitive
performance on image classification, and semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives. (arXiv:2206.01136v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01136">
<div class="article-summary-box-inner">
<span><p>Transformer, the latest technological advance of deep learning, has gained
prevalence in natural language processing or computer vision. Since medical
imaging bear some resemblance to computer vision, it is natural to inquire
about the status quo of Transformers in medical imaging and ask the question:
can the Transformer models transform medical imaging? In this paper, we attempt
to make a response to the inquiry. After a brief introduction of the
fundamentals of Transformers, especially in comparison with convolutional
neural networks (CNNs), and highlighting key defining properties that
characterize the Transformers, we offer a comprehensive review of the
state-of-the-art Transformer-based approaches for medical imaging and exhibit
current research progresses made in the areas of medical image segmentation,
recognition, detection, registration, reconstruction, enhancement, etc. In
particular, what distinguishes our review lies in its organization based on the
Transformer's key defining properties, which are mostly derived from comparing
the Transformer and CNN, and its type of architecture, which specifies the
manner in which the Transformer and CNN are combined, all helping the readers
to best understand the rationale behind the reviewed approaches. We conclude
with discussions of future perspectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Active Fine-Grained Recognition. (arXiv:2206.01153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01153">
<div class="article-summary-box-inner">
<span><p>As fine-grained visual classification (FGVC) being developed for decades,
great works related have exposed a key direction -- finding discriminative
local regions and revealing subtle differences. However, unlike identifying
visual contents within static images, for recognizing objects in the real
physical world, discriminative information is not only present within seen
local regions but also hides in other unseen perspectives. In other words, in
addition to focusing on the distinguishable part from the whole, for efficient
and accurate recognition, it is required to infer the key perspective with a
few glances, e.g., people may recognize a "Benz AMG GT" with a glance of its
front and then know that taking a look at its exhaust pipe can help to tell
which year's model it is. In this paper, back to reality, we put forward the
problem of active fine-grained recognition (AFGR) and complete this study in
three steps: (i) a hierarchical, multi-view, fine-grained vehicle dataset is
collected as the testbed, (ii) a simple experiment is designed to verify that
different perspectives contribute differently for FGVC and different categories
own different discriminative perspective, (iii) a policy-gradient-based
framework is adopted to achieve efficient recognition with active view
selection. Comprehensive experiments demonstrate that the proposed method
delivers a better performance-efficient trade-off than previous FGVC methods
and advanced neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DE-Net: Dynamic Text-guided Image Editing Adversarial Networks. (arXiv:2206.01160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01160">
<div class="article-summary-box-inner">
<span><p>Text-guided image editing models have shown remarkable results. However,
there remain two problems. First, they employ fixed manipulation modules for
various editing requirements (e.g., color changing, texture changing, content
adding and removing), which result in over-editing or insufficient editing.
Second, they do not clearly distinguish between text-required parts and
text-irrelevant parts, which leads to inaccurate editing. To solve these
limitations, we propose: (i) a Dynamic Editing Block (DEBlock) which combines
spatial- and channel-wise manipulations dynamically for various editing
requirements. (ii) a Combination Weights Predictor (CWP) which predicts the
combination weights for DEBlock according to the inference on text and visual
features. (iii) a Dynamic text-adaptive Convolution Block (DCBlock) which
queries source image features to distinguish text-required parts and
text-irrelevant parts. Extensive experiments demonstrate that our DE-Net
achieves excellent performance and manipulates source images more effectively
and accurately. Code is available at \url{https://github.com/tobran/DE-Net}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Relevance Maps of Vision Transformers Improves Robustness. (arXiv:2206.01161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01161">
<div class="article-summary-box-inner">
<span><p>It has been observed that visual classification models often rely mostly on
the image background, neglecting the foreground, which hurts their robustness
to distribution changes. To alleviate this shortcoming, we propose to monitor
the model's relevancy signal and manipulate it such that the model is focused
on the foreground object. This is done as a finetuning step, involving
relatively few samples consisting of pairs of images and their associated
foreground masks. Specifically, we encourage the model's relevancy map (i) to
assign lower relevance to background regions, (ii) to consider as much
information as possible from the foreground, and (iii) we encourage the
decisions to have high confidence. When applied to Vision Transformer (ViT)
models, a marked improvement in robustness to domain shifts is observed.
Moreover, the foreground masks can be obtained automatically, from a
self-supervised variant of the ViT model itself; therefore no additional
supervision is required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning on Implicit Neural Datasets. (arXiv:2206.01178v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01178">
<div class="article-summary-box-inner">
<span><p>Implicit neural representations (INRs) have become fast, lightweight tools
for storing continuous data, but to date there is no general method for
learning directly with INRs as a data representation. We introduce a principled
deep learning framework for learning and inference directly with INRs of any
type without reverting to grid-based features or operations. Our INR-Nets
evaluate INRs on a low discrepancy sequence, enabling quasi-Monte Carlo (QMC)
integration throughout the network. We prove INR-Nets are universal
approximators on a large class of maps between $L^2$ functions. Additionally,
INR-Nets have convergent gradients under the empirical measure, enabling
backpropagation. We design INR-Nets as a continuous generalization of discrete
networks, enabling them to be initialized with pre-trained models. We
demonstrate learning of INR-Nets on classification (INR$\to$label) and
segmentation (INR$\to$INR) tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01191">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,
achieving promising results on various benchmarks. However, due to the massive
number of parameters and model design, e.g., attention mechanism, ViT-based
models are generally times slower than lightweight convolutional networks.
Therefore, the deployment of ViT for real-time applications is particularly
challenging, especially on resource-constrained hardware such as mobile
devices. Recent efforts try to reduce the computation complexity of ViT through
network architecture search or hybrid design with MobileNet block, yet the
inference speed is still unsatisfactory. This leads to an important question:
can transformers run as fast as MobileNet while obtaining high performance? To
answer this, we first revisit the network architecture and operators used in
ViT-based models and identify inefficient designs. Then we introduce a
dimension-consistent pure transformer (without MobileNet blocks) as design
paradigm. Finally, we perform latency-driven slimming to get a series of final
models dubbed EfficientFormer. Extensive experiments show the superiority of
EfficientFormer in performance and speed on mobile devices. Our fastest model,
EfficientFormer-L1, achieves 79.2% top-1 accuracy on ImageNet-1K with only 1.6
ms inference latency on iPhone 12 (compiled with CoreML), which is even a bit
faster than MobileNetV2 (1.7 ms, 71.8% top-1), and our largest model,
EfficientFormer-L7, obtains 83.3% accuracy with only 7.0 ms latency. Our work
proves that properly designed transformers can reach extremely low latency on
mobile devices while maintaining high performance
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hard Negative Sampling Strategies for Contrastive Representation Learning. (arXiv:2206.01197v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01197">
<div class="article-summary-box-inner">
<span><p>One of the challenges in contrastive learning is the selection of appropriate
\textit{hard negative} examples, in the absence of label information. Random
sampling or importance sampling methods based on feature similarity often lead
to sub-optimal performance. In this work, we introduce UnReMix, a hard negative
sampling strategy that takes into account anchor similarity, model uncertainty
and representativeness. Experimental results on several benchmarks show that
UnReMix improves negative sample selection, and subsequently downstream
performance when compared to state-of-the-art contrastive learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization. (arXiv:2206.01198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01198">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) and network pruning are widely studied
efficient AI techniques, but not yet perfect. NAS performs exhaustive candidate
architecture search, incurring tremendous search cost. Though (structured)
pruning can simply shrink model dimension, it remains unclear how to decide the
per-layer sparsity automatically and optimally. In this work, we revisit the
problem of layer-width optimization and propose Pruning-as-Search (PaS), an
end-to-end channel pruning method to search out desired sub-network
automatically and efficiently. Specifically, we add a depth-wise binary
convolution to learn pruning policies directly through gradient descent. By
combining the structural reparameterization and PaS, we successfully searched
out a new family of VGG-like and lightweight networks, which enable the
flexibility of arbitrary width with respect to each layer instead of each
stage. Experimental results show that our proposed architecture outperforms
prior arts by around $1.0\%$ top-1 accuracy under similar inference speed on
ImageNet-1000 classification task. Furthermore, we demonstrate the
effectiveness of our width search on complex tasks including instance
segmentation and image translation. Code and models are released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01201">
<div class="article-summary-box-inner">
<span><p>This paper revisits visual representation in knowledge-based visual question
answering (VQA) and demonstrates that using regional information in a better
way can significantly improve the performance. While visual representation is
extensively studied in traditional VQA, it is under-explored in knowledge-based
VQA even though these two tasks share the common spirit, i.e., rely on visual
input to answer the question. Specifically, we observe that in most
state-of-the-art knowledge-based VQA methods: 1) visual features are extracted
either from the whole image or in a sliding window manner for retrieving
knowledge, and the important relationship within/among object regions is
neglected; 2) visual features are not well utilized in the final answering
model, which is counter-intuitive to some extent. Based on these observations,
we propose a new knowledge-based VQA method REVIVE, which tries to utilize the
explicit information of object regions not only in the knowledge retrieval
stage but also in the answering model. The key motivation is that object
regions and inherent relationships are important for knowledge-based VQA. We
perform extensive experiments on the standard OK-VQA dataset and achieve new
state-of-the-art performance, i.e., 58.0% accuracy, surpassing previous
state-of-the-art method by a large margin (+3.6%). We also conduct detailed
analysis and show the necessity of regional information in different framework
components for knowledge-based VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features. (arXiv:2206.01202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01202">
<div class="article-summary-box-inner">
<span><p>Recent studies show that paddings in convolutional neural networks encode
absolute position information which can negatively affect the model performance
for certain tasks. However, existing metrics for quantifying the strength of
positional information remain unreliable and frequently lead to erroneous
results. To address this issue, we propose novel metrics for measuring (and
visualizing) the encoded positional information. We formally define the encoded
information as PPP (Position-information Pattern from Padding) and conduct a
series of experiments to study its properties as well as its formation. The
proposed metrics measure the presence of positional information more reliably
than the existing metrics based on PosENet and a test in F-Conv. We also
demonstrate that for any extant (and proposed) padding schemes, PPP is
primarily a learning artifact and is less dependent on the characteristics of
the underlying padding schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Instance Segmentation of 3D Scenes Through Weak Bounding Box Supervision. (arXiv:2206.01203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01203">
<div class="article-summary-box-inner">
<span><p>Current 3D segmentation methods heavily rely on large-scale point-cloud
datasets, which are notoriously laborious to annotate. Few attempts have been
made to circumvent the need for dense per-point annotations. In this work, we
look at weakly-supervised 3D instance semantic segmentation. The key idea is to
leverage 3D bounding box labels which are easier and faster to annotate.
Indeed, we show that it is possible to train dense segmentation models using
only weak bounding box labels. At the core of our method, Box2Mask, lies a deep
model, inspired by classical Hough voting, that directly votes for bounding box
parameters, and a clustering method specifically tailored to bounding box
votes. This goes beyond commonly used center votes, which would not fully
exploit the bounding box annotations. On ScanNet test, our weakly supervised
model attains leading performance among other weakly supervised approaches (+18
mAP50). Remarkably, it also achieves 97% of the performance of fully supervised
models. To prove the practicality of our approach, we show segmentation results
on the recently released ARKitScenes dataset which is annotated with 3D
bounding boxes only, and obtain, for the first time, compelling 3D instance
segmentation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Siamese Image Modeling for Self-Supervised Vision Representation Learning. (arXiv:2206.01204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01204">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has delivered superior performance on a
variety of downstream vision tasks. Two main-stream SSL frameworks have been
proposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM).
ID pulls together the representations of different views from the same image,
while avoiding feature collapse. It does well on linear probing but is inferior
in detection performance. On the other hand, MIM reconstructs the original
content given a masked image. It excels at dense prediction but fails to
perform well on linear probing. Their distinctions are caused by neglecting the
representation requirements of either semantic alignment or spatial
sensitivity. Specifically, we observe that (1) semantic alignment demands
semantically similar views to be projected into nearby representation, which
can be achieved by contrasting different views with strong augmentations; (2)
spatial sensitivity requires to model the local structure within an image.
Predicting dense representations with masked image is therefore beneficial
because it models the conditional distribution of image content. Driven by
these analysis, we propose Siamese Image Modeling (SIM), which predicts the
dense representations of an augmented view, based on another masked view from
the same image but with different augmentations. Our method uses a Siamese
network with two branches. The online branch encodes the first view, and
predicts the second view's representation according to the relative positions
between these two views. The target branch produces the target by encoding the
second view. In this way, we are able to achieve comparable linear probing and
dense prediction performances with ID and MIM, respectively. We also
demonstrate that decent linear probing result can be obtained without a global
loss. Code shall be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07719">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection (FSOD) aims to detect objects using only a few
examples. How to adapt state-of-the-art object detectors to the few-shot domain
remains challenging. Object proposal is a key ingredient in modern object
detectors. However, the quality of proposals generated for few-shot classes
using existing methods is far worse than that of many-shot classes, e.g.,
missing boxes for few-shot classes due to misclassification or inaccurate
spatial locations with respect to true objects. To address the noisy proposal
problem, we propose a novel meta-learning based FSOD model by jointly
optimizing the few-shot proposal generation and fine-grained few-shot proposal
classification. To improve proposal generation for few-shot classes, we propose
to learn a lightweight metric-learning based prototype matching network,
instead of the conventional simple linear object/nonobject classifier, e.g.,
used in RPN. Our non-linear classifier with the feature fusion network could
improve the discriminative prototype matching and the proposal recall for
few-shot classes. To improve the fine-grained few-shot proposal classification,
we propose a novel attentive feature alignment method to address the spatial
misalignment between the noisy proposals and few-shot classes, thus improving
the performance of few-shot object detection. Meanwhile we learn a separate
Faster R-CNN detection head for many-shot base classes and show strong
performance of maintaining base-classes knowledge. Our model achieves
state-of-the-art performance on multiple FSOD benchmarks over most of the shots
and metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning. (arXiv:2106.05682v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05682">
<div class="article-summary-box-inner">
<span><p>The capability of the traditional semi-supervised learning (SSL) methods is
far from real-world application due to severely biased pseudo-labels caused by
(1) class imbalance and (2) class distribution mismatch between labeled and
unlabeled data. This paper addresses such a relatively under-explored problem.
First, we propose a general pseudo-labeling framework that class-adaptively
blends the semantic pseudo-label from a similarity-based classifier to the
linear one from the linear classifier, after making the observation that both
types of pseudo-labels have complementary properties in terms of bias. We
further introduce a novel semantic alignment loss to establish balanced feature
representation to reduce the biased predictions from the classifier. We term
the whole framework as Distribution-Aware Semantics-Oriented (DASO)
Pseudo-label. We conduct extensive experiments in a wide range of imbalanced
benchmarks: CIFAR10/100-LT, STL10-LT, and large-scale long-tailed Semi-Aves
with open-set class, and demonstrate that, the proposed DASO framework reliably
improves SSL learners with unlabeled data especially when both (1) class
imbalance and (2) distribution mismatch dominate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Knowledge Distillation With Noise Elimination for RGB-D Salient Object Detection. (arXiv:2106.09517v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09517">
<div class="article-summary-box-inner">
<span><p>RGB-D salient object detection (SOD) demonstrates its superiority on
detecting in complex environments due to the additional depth information
introduced in the data. Inevitably, an independent stream is introduced to
extract features from depth images, leading to extra computation and
parameters. This methodology sacrifices the model size to improve the detection
accuracy which may impede the practical application of SOD problems. To tackle
this dilemma, we propose a dynamic distillation method along with a lightweight
structure, which significantly reduces the computational burden while
maintaining validity. This method considers the factors of both teacher and
student performance within the training stage and dynamically assigns the
distillation weight instead of applying a fixed weight on the student model. We
also investigate the issue of RGB-D early fusion strategy in distillation and
propose a simple noise elimination method to mitigate the impact of distorted
training data caused by low quality depth maps. Extensive experiments are
conducted on five public datasets to demonstrate that our method can achieve
competitive performance with a fast inference speed (136FPS) compared to 10
prior methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01378">
<div class="article-summary-box-inner">
<span><p>In the past few years, transformers have achieved promising performances on
various computer vision tasks. Unfortunately, the immense inference overhead of
most existing vision transformers withholds their from being deployed on edge
devices such as cell phones and smart watches. Knowledge distillation is a
widely used paradigm for compressing cumbersome architectures via transferring
information to a compact student. However, most of them are designed for
convolutional neural networks (CNNs), which do not fully investigate the
character of vision transformer (ViT). In this paper, we utilize the
patch-level information and propose a fine-grained manifold distillation
method. Specifically, we train a tiny student model to match a pre-trained
teacher model in the patch-level manifold space. Then, we decouple the manifold
matching loss into three terms with careful design to further reduce the
computational costs for the patch relationship. Equipped with the proposed
method, a DeiT-Tiny model containing 5M parameters achieves 76.5% top-1
accuracy on ImageNet-1k, which is +2.0% higher than previous distillation
approaches. Transfer learning results on other classification benchmarks and
downstream vision tasks also demonstrate the superiority of our method over the
state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards real-world navigation with deep differentiable planners. (arXiv:2108.05713v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05713">
<div class="article-summary-box-inner">
<span><p>We train embodied neural networks to plan and navigate unseen complex 3D
environments, emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment, the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning, we focus on differentiable planners such as Value
Iteration Networks (VIN), which are trained offline from safe expert
demonstrations. Although they work well in small simulations, we address two
major limitations that hinder their deployment. First, we observed that current
differentiable planners struggle to plan long-term in environments with a high
branching complexity. While they should ideally learn to assign low rewards to
obstacles to avoid collisions, we posit that the constraints imposed on the
network are not strong enough to guarantee the network to learn sufficiently
large penalties for every possible collision. We thus impose a structural
constraint on the value iteration, which explicitly learns to model any
impossible actions. Secondly, we extend the model to work with a limited
perspective camera under translation and rotation, which is crucial for real
robot deployment. Many VIN-like planners assume a 360 degrees or overhead view
without rotation. In contrast, our method uses a memory-efficient lattice map
to aggregate CNN embeddings of partial observations, and models the rotational
dynamics explicitly using a 3D state-space grid (translation and rotation). Our
proposals significantly improve semantic navigation and exploration on several
2D and 3D environments, succeeding in settings that are otherwise challenging
for this class of methods. As far as we know, we are the first to successfully
perform differentiable planning on the difficult Active Vision Dataset,
consisting of real images captured from a robot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11941">
<div class="article-summary-box-inner">
<span><p>Accurately segmenting teeth and identifying the corresponding anatomical
landmarks on dental mesh models are essential in computer-aided orthodontic
treatment. Manually performing these two tasks is time-consuming, tedious, and,
more importantly, highly dependent on orthodontists' experiences due to the
abnormality and large-scale variance of patients' teeth. Some machine
learning-based methods have been designed and applied in the orthodontic field
to automatically segment dental meshes (e.g., intraoral scans). In contrast,
the number of studies on tooth landmark localization is still limited. This
paper proposes a two-stage framework based on mesh deep learning (called
TS-MDL) for joint tooth labeling and landmark identification on raw intraoral
scans. Our TS-MDL first adopts an end-to-end \emph{i}MeshSegNet method (i.e., a
variant of the existing MeshSegNet with both improved accuracy and efficiency)
to label each tooth on the downsampled scan. Guided by the segmentation
outputs, our TS-MDL further selects each tooth's region of interest (ROI) on
the original mesh to construct a light-weight variant of the pioneering
PointNet (i.e., PointNet-Reg) for regressing the corresponding landmark
heatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing
promising segmentation and localization performance. Specifically,
\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice
similarity coefficient (DSC) at \textcolor[rgb]{0,0,0}{$0.964\pm0.054$},
significantly outperforming the original MeshSegNet. In the second stage,
PointNet-Reg achieved a mean absolute error (MAE) of $0.597\pm0.761 \, mm$ in
distances between the prediction and ground truth for $66$ landmarks, which is
superior compared with other networks for landmark detection. All these results
suggest the potential usage of our TS-MDL in orthodontics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02711">
<div class="article-summary-box-inner">
<span><p>Recently, GAN inversion methods combined with Contrastive Language-Image
Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts.
However, their applications to diverse real images are still difficult due to
the limited GAN inversion capability. Specifically, these approaches often have
difficulties in reconstructing images with novel poses, views, and highly
variable contents compared to the training data, altering object identity, or
producing unwanted image artifacts. To mitigate these problems and enable
faithful manipulation of real images, we propose a novel method, dubbed
DiffusionCLIP, that performs text-driven image manipulation using diffusion
models. Based on full inversion capability and high-quality image generation
power of recent diffusion models, our method performs zero-shot image
manipulation successfully even between unseen domains and takes another step
towards general application by manipulating images from a widely varying
ImageNet dataset. Furthermore, we propose a novel noise combination method that
allows straightforward multi-attribute manipulation. Extensive experiments and
human evaluation confirmed robust and superior manipulation performance of our
methods compared to the existing baselines. Code is available at
https://github.com/gwang-kim/DiffusionCLIP.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03605">
<div class="article-summary-box-inner">
<span><p>The literature on adversarial attacks in computer vision typically focuses on
pixel-level perturbations. These tend to be very difficult to interpret. Recent
work that manipulates the latent representations of image generators to create
"feature-level" adversarial perturbations gives us an opportunity to explore
interpretable adversarial attacks. We make three contributions. First, we
observe that feature-level attacks provide useful classes of inputs for
studying the representations in models. Second, we show that these adversaries
are versatile and highly robust. We demonstrate that they can be used to
produce targeted, universal, disguised, physically-realizable, and black-box
attacks at the ImageNet scale. Third, we show how these adversarial images can
be used as a practical interpretability tool for identifying bugs in networks.
We use these adversaries to make predictions about spurious associations
between features and classes which we then test by designing "copy/paste"
attacks in which one natural image is pasted into another to cause a targeted
misclassification. Our results indicate that feature-level attacks are a
promising approach for rigorous interpretability research. They support the
design of tools to better understand what a model has learned and diagnose
brittle feature associations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04079">
<div class="article-summary-box-inner">
<span><p>Accurate and reliable lane detection is vital for the safe performance of
lane-keeping assistance and lane departure warning systems. However, under
certain challenging circumstances, it is difficult to get satisfactory
performance in accurately detecting the lanes from one single image as mostly
done in current literature. Since lane markings are continuous lines, the lanes
that are difficult to be accurately detected in the current single image can
potentially be better deduced if information from previous frames is
incorporated. This study proposes a novel hybrid spatial-temporal (ST)
sequence-to-one deep learning architecture. This architecture makes full use of
the ST information in multiple continuous image frames to detect the lane
markings in the very last frame. Specifically, the hybrid model integrates the
following aspects: (a) the single image feature extraction module equipped with
the spatial convolutional neural network; (b) the ST feature integration module
constructed by ST recurrent neural network; (c) the encoder-decoder structure,
which makes this image segmentation problem work in an end-to-end supervised
learning format. Extensive experiments reveal that the proposed model
architecture can effectively handle challenging driving scenes and outperforms
available state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11524">
<div class="article-summary-box-inner">
<span><p>A key component of understanding hand-object interactions is the ability to
identify the active object -- the object that is being manipulated by the human
hand. In order to accurately localize the active object, any method must reason
using information encoded by each image pixel, such as whether it belongs to
the hand, the object, or the background. To leverage each pixel as evidence to
determine the bounding box of the active object, we propose a pixel-wise voting
function. Our pixel-wise voting function takes an initial bounding box as input
and produces an improved bounding box of the active object as output. The
voting function is designed so that each pixel inside of the input bounding box
votes for an improved bounding box, and the box with the majority vote is
selected as the output. We call the collection of bounding boxes generated
inside of the voting function, the Relational Box Field, as it characterizes a
field of bounding boxes defined in relationship to the current bounding box.
While our voting function is able to improve the bounding box of the active
object, one round of voting is typically not enough to accurately localize the
active object. Therefore, we repeatedly apply the voting function to
sequentially improve the location of the bounding box. However, since it is
known that repeatedly applying a one-step predictor (i.e., auto-regressive
processing with our voting function) can cause a data distribution shift, we
mitigate this issue using reinforcement learning (RL). We adopt standard RL to
learn the voting function parameters and show that it provides a meaningful
improvement over a standard supervised learning approach. We perform
experiments on two large-scale datasets: 100DOH and MECCANO, improving AP50
performance by 8% and 30%, respectively, over the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically Explainable CNN for SAR Image Classification. (arXiv:2110.14144v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14144">
<div class="article-summary-box-inner">
<span><p>Integrating the special electromagnetic characteristics of Synthetic Aperture
Radar (SAR) in deep neural networks is essential in order to enhance the
explainability and physics awareness of deep learning. In this paper, we first
propose a novel physically explainable convolutional neural network for SAR
image classification, namely physics guided and injected learning (PGIL). It
comprises three parts: (1) explainable models (XM) to provide prior physics
knowledge, (2) physics guided network (PGN) to encode the knowledge into
physics-aware features, and (3) physics injected network (PIN) to adaptively
introduce the physics-aware features into classification pipeline for label
prediction. A hybrid Image-Physics SAR dataset format is proposed for
evaluation, with both Sentinel-1 and Gaofen-3 SAR data being experimented. The
results show that the proposed PGIL substantially improve the classification
performance in case of limited labeled data compared with the counterpart
data-driven CNN and other pre-training methods. Additionally, the physics
explanations are discussed to indicate the interpretability and the physical
consistency preserved in the predictions. We deem the proposed method would
promote the development of physically explainable deep learning in SAR image
interpretation field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining machine learning with physics: A framework for tracking and sorting multiple dark solitons. (arXiv:2111.04881v2 [cond-mat.quant-gas] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04881">
<div class="article-summary-box-inner">
<span><p>In ultracold-atom experiments, data often comes in the form of images which
suffer information loss inherent in the techniques used to prepare and measure
the system. This is particularly problematic when the processes of interest are
complicated, such as interactions among excitations in Bose-Einstein
condensates (BECs). In this paper, we describe a framework combining machine
learning (ML) models with physics-based traditional analyses to identify and
track multiple solitonic excitations in images of BECs. We use an ML-based
object detector to locate the solitonic excitations and develop a
physics-informed classifier to sort solitonic excitations into physically
motivated subcategories. Lastly, we introduce a quality metric quantifying the
likelihood that a specific feature is a longitudinal soliton. Our trained
implementation of this framework, SolDet, is publicly available as an
open-source python package. SolDet is broadly applicable to feature
identification in cold-atom images when trained on a suitable user-provided
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework. (arXiv:2112.05141v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05141">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has shown its great potential to extract powerful
visual representations without human annotations. Various works are proposed to
deal with self-supervised learning from different perspectives: (1) contrastive
learning methods (e.g., MoCo, SimCLR) utilize both positive and negative
samples to guide the training direction; (2) asymmetric network methods (e.g.,
BYOL, SimSiam) get rid of negative samples via the introduction of a predictor
network and the stop-gradient operation; (3) feature decorrelation methods
(e.g., Barlow Twins, VICReg) instead aim to reduce the redundancy between
feature dimensions. These methods appear to be quite different in the designed
loss functions from various motivations. The final accuracy numbers also vary,
where different networks and tricks are utilized in different works. In this
work, we demonstrate that these methods can be unified into the same form.
Instead of comparing their loss functions, we derive a unified formula through
gradient analysis. Furthermore, we conduct fair and detailed experiments to
compare their performances. It turns out that there is little gap between these
methods, and the use of momentum encoder is the key factor to boost
performance. From this unified framework, we propose UniGrad, a simple but
effective gradient form for self-supervised learning. It does not require a
memory bank or a predictor network, but can still achieve state-of-the-art
performance and easily adopt other training strategies. Extensive experiments
on linear evaluation and many downstream tasks also show its effectiveness.
Code shall be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation. (arXiv:2112.13985v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13985">
<div class="article-summary-box-inner">
<span><p>Text-guided image manipulation tasks have recently gained attention in the
vision-and-language community. While most of the prior studies focused on
single-turn manipulation, our goal in this paper is to address the more
challenging multi-turn image manipulation (MTIM) task. Previous models for this
task successfully generate images iteratively, given a sequence of instructions
and a previously generated image. However, this approach suffers from
under-generation and a lack of generated quality of the objects that are
described in the instructions, which consequently degrades the overall
performance. To overcome these problems, we present a novel architecture called
a Visually Guided Language Attention GAN (LatteGAN). Here, we address the
limitations of the previous approaches by introducing a Visually Guided
Language Attention (Latte) module, which extracts fine-grained text
representations for the generator, and a Text-Conditioned U-Net discriminator
architecture, which discriminates both the global and local representations of
fake or real images. Extensive experiments on two distinct MTIM datasets,
CoDraw and i-CLEVR, demonstrate the state-of-the-art performance of the
proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08418">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning has achieved remarkable successes in medical image
analysis. Although deep neural networks generate clinically important
predictions, they have inherent uncertainty. Such uncertainty is a major
barrier to report these predictions with confidence. In this paper, we propose
a novel yet simple Bayesian inference approach called SoftDropConnect (SDC) to
quantify the network uncertainty in medical imaging tasks with gliomas
segmentation and metastases classification as initial examples. Our key idea is
that during training and testing SDC modulates network parameters continuously
so as to allow affected information processing channels still in operation,
instead of disabling them as Dropout or DropConnet does. When compared with
three popular Bayesian inference methods including Bayes By Backprop, Dropout,
and DropConnect, our SDC method (SDC-W after optimization) outperforms the
three competing methods with a substantial margin. Quantitatively, our proposed
method generates substantial improvements in prediction accuracy (by 3.4%,
2.5%, and 6.7% respectively for whole tumor segmentation in terms of dice
score; and by 11.7%, 3.9%, and 8.7% respectively for brain metastases
classification) and greatly reduced epistemic and aleatoric uncertainties. Our
approach promises to deliver better diagnostic performance and make medical AI
imaging more explainable and trustworthy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Lambda: Disentangling Dynamic Task Relationships. (arXiv:2202.03091v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03091">
<div class="article-summary-box-inner">
<span><p>Understanding the structure of multiple related tasks allows for multi-task
learning to improve the generalisation ability of one or all of them. However,
it usually requires training each pairwise combination of tasks together in
order to capture task relationships, at an extremely high computational cost.
In this work, we learn task relationships via an automated weighting framework,
named Auto-Lambda. Unlike previous methods where task relationships are assumed
to be fixed, Auto-Lambda is a gradient-based meta learning framework which
explores continuous, dynamic task relationships via task-specific weightings,
and can optimise any choice of combination of tasks through the formulation of
a meta-loss; where the validation loss automatically influences task weightings
throughout training. We apply the proposed framework to both multi-task and
auxiliary learning problems in computer vision and robotics, and show that
Auto-Lambda achieves state-of-the-art performance, even when compared to
optimisation strategies designed specifically for each problem and data domain.
Finally, we observe that Auto-Lambda can discover interesting learning
behaviors, leading to new insights in multi-task learning. Code is available at
https://github.com/lorenmt/auto-lambda.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncalibrated Models Can Improve Human-AI Collaboration. (arXiv:2202.05983v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05983">
<div class="article-summary-box-inner">
<span><p>In many practical applications of AI, an AI model is used as a decision aid
for human users. The AI provides advice that a human (sometimes) incorporates
into their decision-making process. The AI advice is often presented with some
measure of "confidence" that the human can use to calibrate how much they
depend on or trust the advice. In this paper, we demonstrate that human-AI
performance can be improved by calibrating this confidence to the humans using
the advice. In practice, this means presenting calibrated AI models as more or
less confident than they actually are. We show empirically that this can
improve human-AI performance (measured as the accuracy and confidence of the
human's final prediction after seeing the AI advice). We first train a model to
predict human incorporation of AI advice using data from thousands of human
interactions. This enables us to explicitly estimate how to transform the AI's
prediction confidence, making the AI uncalibrated, in order to improve the
final human prediction. We empirically validate our results across four
different tasks--dealing with images, text and tabular data--involving hundreds
of human participants. We further support our findings with simulation
analysis. Our findings suggest the importance of and a framework for jointly
optimizing the human-AI system in contrast to the standard paradigm of
optimizing the AI model alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rare Gems: Finding Lottery Tickets at Initialization. (arXiv:2202.12002v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12002">
<div class="article-summary-box-inner">
<span><p>Large neural networks can be pruned to a small fraction of their original
size, with little loss in accuracy, by following a time-consuming "train,
prune, re-train" approach. Frankle &amp; Carbin conjecture that we can avoid this
by training "lottery tickets", i.e., special sparse subnetworks found at
initialization, that can be trained to high accuracy. However, a subsequent
line of work by Frankle et al. and Su et al. presents concrete evidence that
current algorithms for finding trainable networks at initialization, fail
simple baseline comparisons, e.g., against training random sparse subnetworks.
Finding lottery tickets that train to better accuracy compared to simple
baselines remains an open problem. In this work, we resolve this open problem
by proposing Gem-Miner which finds lottery tickets at initialization that beat
current baselines. Gem-Miner finds lottery tickets trainable to accuracy
competitive or better than Iterative Magnitude Pruning (IMP), and does so up to
$19\times$ faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training privacy-preserving video analytics pipelines by suppressing features that reveal information about private attributes. (arXiv:2203.02635v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02635">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are increasingly deployed for scene analytics, including
to evaluate the attention and reaction of people exposed to out-of-home
advertisements. However, the features extracted by a deep neural network that
was trained to predict a specific, consensual attribute (e.g. emotion) may also
encode and thus reveal information about private, protected attributes (e.g.
age or gender). In this work, we focus on such leakage of private information
at inference time. We consider an adversary with access to the features
extracted by the layers of a deployed neural network and use these features to
predict private attributes. To prevent the success of such an attack, we modify
the training of the network using a confusion loss that encourages the
extraction of features that make it difficult for the adversary to accurately
predict private attributes. We validate this training approach on image-based
tasks using a publicly available dataset. Results show that, compared to the
original network, the proposed PrivateNet can reduce the leakage of private
information of a state-of-the-art emotion recognition classifier by 2.88% for
gender and by 13.06% for age group, with a minimal effect on task accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04203">
<div class="article-summary-box-inner">
<span><p>A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as "how
can I run the microwave for 1 minute?". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos and scripts to guide the user
step-by-step. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples derived from 100 newly filmed
first-person videos. Each question should be completed with multi-step
guidances by inferring from visual details (e.g., buttons' position) and
textural details (e.g., actions like press/turn). To address this unique task,
we developed a Question-to-Actions (Q2A) model that significantly outperforms
several baseline methods while still having large room for improvement. We
expect our task and dataset to advance Egocentric AI Assistant's development.
Our project page is available at: https://showlab.github.io/assistq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Efficient Person Re-Identification with Diverse Cluster-Based Pair Selection. (arXiv:2203.05395v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05395">
<div class="article-summary-box-inner">
<span><p>Person Re-identification (Re-ID) has attracted great attention due to its
promising real-world applications. However, in practice, it is always costly to
annotate the training data to train a Re-ID model, and it still remains
challenging to reduce the annotation cost while maintaining the performance for
the Re-ID task. To solve this problem, we propose the Annotation Efficient
Person Re-Identification method to select image pairs from an alternative pair
set according to the fallibility and diversity of pairs, and train the Re-ID
model based on the annotation. Specifically, we design an annotation and
training framework to firstly reduce the size of the alternative pair set by
clustering all images considering the locality of features, secondly select
images pairs from intra-/inter-cluster samples for human to annotate, thirdly
re-assign clusters according to the annotation, and finally train the model
with the re-assigned clusters. During the pair selection, we seek for valuable
pairs according to pairs' fallibility and diversity, which includes an
intra-cluster criterion to construct image pairs with the most chaotic samples
and the representative samples within clusters, an inter-cluster criterion to
construct image pairs between clusters based on the second-order Wasserstein
distance, and a diversity criterion for clusterbased pair selection. Combining
all criteria above, a greedy strategy is developed to solve the pair selection
problem. Finally, the above
clustering-selecting-annotating-reassigning-training procedure will be repeated
until the annotation budget is reached. Extensive experiments on three widely
adopted Re-ID datasets show that we can greatly reduce the annotation cost
while achieving better performance compared with state-of-the-art works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Font Generation with Missing Impression Labels. (arXiv:2203.10348v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10348">
<div class="article-summary-box-inner">
<span><p>Our goal is to generate fonts with specific impressions, by training a
generative adversarial network with a font dataset with impression labels. The
main difficulty is that font impression is ambiguous and the absence of an
impression label does not always mean that the font does not have the
impression. This paper proposes a font generation model that is robust against
missing impression labels. The key ideas of the proposed method are (1)a
co-occurrence-based missing label estimator and (2)an impression label space
compressor. The first is to interpolate missing impression labels based on the
co-occurrence of labels in the dataset and use them for training the model as
completed label conditions. The second is an encoder-decoder module to compress
the high-dimensional impression space into low-dimensional. We proved that the
proposed model generates high-quality font images using multi-label data with
missing labels through qualitative and quantitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APP: Anytime Progressive Pruning. (arXiv:2204.01640v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01640">
<div class="article-summary-box-inner">
<span><p>With the latest advances in deep learning, there has been a lot of focus on
the online learning paradigm due to its relevance in practical settings.
Although many methods have been investigated for optimal learning settings in
scenarios where the data stream is continuous over time, sparse networks
training in such settings have often been overlooked. In this paper, we explore
the problem of training a neural network with a target sparsity in a particular
case of online learning: the anytime learning at macroscale paradigm (ALMA). We
propose a novel way of progressive pruning, referred to as \textit{Anytime
Progressive Pruning} (APP); the proposed approach significantly outperforms the
baseline dense and Anytime OSP models across multiple architectures and
datasets under short, moderate, and long-sequence training. Our method, for
example, shows an improvement in accuracy of $\approx 7\%$ and a reduction in
the generalization gap by $\approx 22\%$, while being $\approx 1/3$ rd the size
of the dense baseline model in few-shot restricted imagenet training. We
further observe interesting nonmonotonic transitions in the generalization gap
in the high number of megabatches-based ALMA. The code and experiment
dashboards can be accessed at
\url{https://github.com/landskape-ai/Progressive-Pruning} and
\url{https://wandb.ai/landskape/APP}, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12693">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has achieved a great success in the representation
learning of visual and textual data. However, the current methods are mainly
validated on the well-curated datasets, which do not exhibit the real-world
long-tailed distribution. Recent attempts to consider self-supervised
long-tailed learning are made by rebalancing in the loss perspective or the
model perspective, resembling the paradigms in the supervised long-tailed
learning. Nevertheless, without the aid of labels, these explorations have not
shown the expected significant promise due to the limitation in tail sample
discovery or the heuristic structure design. Different from previous works, we
explore this direction from an alternative perspective, i.e., the data
perspective, and propose a novel Boosted Contrastive Learning (BCL) method.
Specifically, BCL leverages the memorization effect of deep neural networks to
automatically drive the information discrepancy of the sample views in
contrastive learning, which is more efficient to enhance the long-tailed
learning in the label-unaware context. Extensive experiments on a range of
benchmark datasets demonstrate the effectiveness of BCL over several
state-of-the-art methods. Our code is available at
https://github.com/MediaBrain-SJTU/BCL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13219">
<div class="article-summary-box-inner">
<span><p>Obtaining gold standard annotated data for object detection is often costly,
involving human-level effort. Semi-supervised object detection algorithms solve
the problem with a small amount of gold-standard labels and a large unlabelled
dataset used to generate silver-standard labels. But training on the silver
standard labels does not produce good results, because they are
machine-generated annotations. In this work, we design a modified loss function
to train on large silver standard annotated sets generated by a weak annotator.
We include a confidence metric associated with the annotation as an additional
term in the loss function, signifying the quality of the annotation. We test
the effectiveness of our approach on various test sets and use numerous
variations to compare the results with some of the current approaches to object
detection. In comparison with the baseline where no confidence metric is used,
we achieved a 4% gain in mAP with 25% labeled data and 10% gain in mAP with 50%
labeled data by using the proposed confidence metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient textual explanations for complex road and traffic scenarios based on semantic segmentation. (arXiv:2205.14118v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14118">
<div class="article-summary-box-inner">
<span><p>The complex driving environment brings great challenges to the visual
perception of autonomous vehicles. It's essential to extract clear and
explainable information from the complex road and traffic scenarios and offer
clues to decision and control. However, the previous scene explanation had been
implemented as a separate model. The black box model makes it difficult to
interpret the driving environment. It cannot detect comprehensive textual
information and requires a high computational load and time consumption. Thus,
this study proposed a comprehensive and efficient textual explanation model.
From 336k video frames of the driving environment, critical images of complex
road and traffic scenarios were selected into a dataset. Through transfer
learning, this study established an accurate and efficient segmentation model
to obtain the critical traffic elements in the environment. Based on the
XGBoost algorithm, a comprehensive model was developed. The model provided
textual information about states of traffic elements, the motion of conflict
objects, and scenario complexity. The approach was verified on the real-world
road. It improved the perception accuracy of critical traffic elements to
78.8%. The time consumption reached 13 minutes for each epoch, which was 11.5
times more efficient than the pre-trained network. The textual information
analyzed from the model was also accordant with reality. The findings offer
clear and explainable information about the complex driving environment, which
lays a foundation for subsequent decision and control. It can improve the
visual perception ability and enrich the prior knowledge and judgments of
complex traffic situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Lip Region-of-Interest Sufficient for Lipreading?. (arXiv:2205.14295v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14295">
<div class="article-summary-box-inner">
<span><p>Lip region-of-interest (ROI) is conventionally used for visual input in the
lipreading task. Few works have adopted the entire face as visual input because
lip-excluded parts of the face are usually considered to be redundant and
irrelevant to visual speech recognition. However, faces contain much more
detailed information than lips, such as speakers' head pose, emotion, identity
etc. We argue that such information might benefit visual speech recognition if
a powerful feature extractor employing the entire face is trained. In this
work, we propose to adopt the entire face for lipreading with self-supervised
learning. AV-HuBERT, an audio-visual multi-modal self-supervised learning
framework, was adopted in our experiments. Our experimental results showed that
adopting the entire face achieved 16% relative word error rate (WER) reduction
on the lipreading task, compared with the baseline method using lip as visual
input. Without self-supervised pretraining, the model with face input achieved
a higher WER than that using lip input in the case of limited training data (30
hours), while a slightly lower WER when using large amount of training data
(433 hours).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepRM: Deep Recurrent Matching for 6D Pose Refinement. (arXiv:2205.14474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14474">
<div class="article-summary-box-inner">
<span><p>Precise 6D pose estimation of rigid objects from RGB images is a critical but
challenging task in robotics and augmented reality. To address this problem, we
propose DeepRM, a novel recurrent network architecture for 6D pose refinement.
DeepRM leverages initial coarse pose estimates to render synthetic images of
target objects. The rendered images are then matched with the observed images
to predict a rigid transform for updating the previous pose estimate. This
process is repeated to incrementally refine the estimate at each iteration.
LSTM units are used to propagate information through each refinement step,
significantly improving overall performance. In contrast to many 2-stage
Perspective-n-Point based solutions, DeepRM is trained end-to-end, and uses a
scalable backbone that can be tuned via a single parameter for accuracy and
efficiency. During training, a multi-scale optical flow head is added to
predict the optical flow between the observed and synthetic images. Optical
flow prediction stabilizes the training process, and enforces the learning of
features that are relevant to the task of pose estimation. Our results
demonstrate that DeepRM achieves state-of-the-art performance on two widely
accepted challenging datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Batch Normalization Is Blind to the First and Second Derivatives of the Loss. (arXiv:2205.15146v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15146">
<div class="article-summary-box-inner">
<span><p>In this paper, we prove the effects of the BN operation on the
back-propagation of the first and second derivatives of the loss. When we do
the Taylor series expansion of the loss function, we prove that the BN
operation will block the influence of the first-order term and most influence
of the second-order term of the loss. We also find that such a problem is
caused by the standardization phase of the BN operation. Experimental results
have verified our theoretical conclusions, and we have found that the BN
operation significantly affects feature representations in specific tasks,
where losses of different samples share similar analytic formulas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial synthesis based data-augmentation for code-switched spoken language identification. (arXiv:2205.15747v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15747">
<div class="article-summary-box-inner">
<span><p>Spoken Language Identification (LID) is an important sub-task of Automatic
Speech Recognition(ASR) that is used to classify the language(s) in an audio
segment. Automatic LID plays an useful role in multilingual countries. In
various countries, identifying a language becomes hard, due to the multilingual
scenario where two or more than two languages are mixed together during
conversation. Such phenomenon of speech is called as code-mixing or
code-switching. This nature is followed not only in India but also in many
Asian countries. Such code-mixed data is hard to find, which further reduces
the capabilities of the spoken LID. Hence, this work primarily addresses this
problem using data augmentation as a solution on the on the data scarcity of
the code-switched class. This study focuses on Indic language code-mixed with
English. Spoken LID is performed on Hindi, code-mixed with English. This
research proposes Generative Adversarial Network (GAN) based data augmentation
technique performed using Mel spectrograms for audio data. GANs have already
been proven to be accurate in representing the real data distribution in the
image domain. Proposed research exploits these capabilities of GANs in speech
domains such as speech classification, automatic speech recognition, etc. GANs
are trained to generate Mel spectrograms of the minority code-mixed class which
are then used to augment data for the classifier. Utilizing GANs give an
overall improvement on Unweighted Average Recall by an amount of 3.5% as
compared to a Convolutional Recurrent Neural Network (CRNN) classifier used as
the baseline reference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images. (arXiv:2206.00252v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00252">
<div class="article-summary-box-inner">
<span><p>Identifying the type of kidney stones can allow urologists to determine their
formation cause, improving the early prescription of appropriate treatments to
diminish future relapses. However, currently, the associated ex-vivo diagnosis
(known as morpho-constitutional analysis, MCA) is time-consuming, expensive,
and requires a great deal of experience, as it requires a visual analysis
component that is highly operator dependant. Recently, machine learning methods
have been developed for in-vivo endoscopic stone recognition. Shallow methods
have been demonstrated to be reliable and interpretable but exhibit low
accuracy, while deep learning-based methods yield high accuracy but are not
explainable. However, high stake decisions require understandable
computer-aided diagnosis (CAD) to suggest a course of action based on
reasonable evidence, rather than merely prescribe one. Herein, we investigate
means for learning part-prototypes (PPs) that enable interpretable models. Our
proposal suggests a classification for a kidney stone patch image and provides
explanations in a similar way as those used on the MCA method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Invariant Visual Representations for Compositional Zero-Shot Learning. (arXiv:2206.00415v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00415">
<div class="article-summary-box-inner">
<span><p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions
using knowledge learned from seen attribute-object compositions in the training
set. Previous works mainly project an image and a composition into a common
embedding space to measure their compatibility score. However, both attributes
and objects share the visual representations learned above, leading the model
to exploit spurious correlations and bias towards seen pairs. Instead, we
reconsider CZSL as an out-of-distribution generalization problem. If an object
is treated as a domain, we can learn object-invariant features to recognize the
attributes attached to any object reliably. Similarly, attribute-invariant
features can also be learned when recognizing the objects with attributes as
domains. Specifically, we propose an invariant feature learning framework to
align different domains at the representation and gradient levels to capture
the intrinsic characteristics associated with the tasks. Experiments on two
CZSL benchmarks demonstrate that the proposed method significantly outperforms
the previous state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines. (arXiv:2206.00535v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00535">
<div class="article-summary-box-inner">
<span><p>Deepfakes pose a serious threat to our digital society by fueling the spread
of misinformation. It is essential to develop techniques that both detect them,
and effectively alert the human user to their presence. Here, we introduce a
novel deepfake detection framework that meets both of these needs. Our approach
learns to generate attention maps of video artifacts, semi-supervised on human
annotations. These maps make two contributions. First, they improve the
accuracy and generalizability of a deepfake classifier, demonstrated across
several deepfake detection datasets. Second, they allow us to generate an
intuitive signal for the human user, in the form of "Deepfake Caricatures":
transformations of the original deepfake video where attended artifacts are
exacerbated to improve human recognition. Our approach, based on a mixture of
human and artificial supervision, aims to further the development of
countermeasures against fake visual content, and grants humans the ability to
make their own judgment when presented with dubious visual media.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-04 23:07:45.608289569 UTC">2022-06-04 23:07:45 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>