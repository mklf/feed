<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-17T01:30:00Z">09-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Register Projection for Headline Part of Speech Tagging. (arXiv:2109.07483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07483">
<div class="article-summary-box-inner">
<span><p>Part of speech (POS) tagging is a familiar NLP task. State of the art taggers
routinely achieve token-level accuracies of over 97% on news body text,
evidence that the problem is well understood. However, the register of English
news headlines, "headlinese", is very different from the register of long-form
text, causing POS tagging models to underperform on headlines. In this work, we
automatically annotate news headlines with POS tags by projecting predicted
tags from corresponding sentences in news bodies. We train a multi-domain POS
tagger on both long-form and headline text and show that joint training on both
registers improves over training on just one or naively concatenating training
sets. We evaluate on a newly-annotated corpus of over 5,248 English news
headlines from the Google sentence compression corpus, and show that our model
yields a 23% relative error reduction per token and 19% per headline. In
addition, we demonstrate that better headline POS tags can improve the
performance of a syntax-based open information extraction system. We make POSH,
the POS-tagged Headline corpus, available to encourage research in improved NLP
models for news headlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Euclidean and Hyperbolic Embeddings on the WordNet Nouns Hypernymy Graph. (arXiv:2109.07488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07488">
<div class="article-summary-box-inner">
<span><p>Nickel and Kiela (2017) present a new method for embedding tree nodes in the
Poincare ball, and suggest that these hyperbolic embeddings are far more
effective than Euclidean embeddings at embedding nodes in large, hierarchically
structured graphs like the WordNet nouns hypernymy tree. This is especially
true in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek
to reproduce their experiments on embedding and reconstructing the WordNet
nouns hypernymy graph. Counter to what they report, we find that Euclidean
embeddings are able to represent this tree at least as well as Poincare
embeddings, when allowed at least 50 dimensions. We note that this does not
diminish the significance of their work given the impressive performance of
hyperbolic embeddings in very low-dimensional settings. However, given the wide
influence of their work, our aim here is to present an updated and more
accurate comparison between the Euclidean and hyperbolic embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets. (arXiv:2109.07494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07494">
<div class="article-summary-box-inner">
<span><p>For interpreting the behavior of a probabilistic model, it is useful to
measure a model's calibration--the extent to which it produces reliable
confidence scores. We address the open problem of calibration for tagging
models with sparse tagsets, and recommend strategies to measure and reduce
calibration error (CE) in such models. We show that several post-hoc
recalibration techniques all reduce calibration error across the marginal
distribution for two existing sequence taggers. Moreover, we propose tag
frequency grouping (TFG) as a way to measure calibration error in different
frequency bands. Further, recalibrating each group separately promotes a more
equitable reduction of calibration error across the tag frequency spectrum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue State Tracking with a Language Model using Schema-Driven Prompting. (arXiv:2109.07506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07506">
<div class="article-summary-box-inner">
<span><p>Task-oriented conversational systems often use dialogue state tracking to
represent the user's intentions, which involves filling in values of
pre-defined slots. Many approaches have been proposed, often using
task-specific architectures with special-purpose classifiers. Recently, good
results have been obtained using more general architectures based on pretrained
language models. Here, we introduce a new variation of the language modeling
approach that uses schema-driven prompting to provide task-aware history
encoding that is used for both categorical and non-categorical slots. We
further improve performance by augmenting the prompting with schema
descriptions, a naturally occurring source of in-domain knowledge. Our purely
generative system achieves state-of-the-art performance on MultiWOZ 2.2 and
achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M.
The data and code will be available at
https://github.com/chiahsuan156/DST-as-Prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tied & Reduced RNN-T Decoder. (arXiv:2109.07513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07513">
<div class="article-summary-box-inner">
<span><p>Previous works on the Recurrent Neural Network-Transducer (RNN-T) models have
shown that, under some conditions, it is possible to simplify its prediction
network with little or no loss in recognition accuracy (<a href="/abs/2003.07705">arXiv:2003.07705</a>
[eess.AS], [2], <a href="/abs/2012.06749">arXiv:2012.06749</a> [cs.CL]). This is done by limiting the context
size of previous labels and/or using a simpler architecture for its layers
instead of LSTMs. The benefits of such changes include reduction in model size,
faster inference and power savings, which are all useful for on-device
applications.
</p>
<p>In this work, we study ways to make the RNN-T decoder (prediction network +
joint network) smaller and faster without degradation in recognition
performance. Our prediction network performs a simple weighted averaging of the
input embeddings, and shares its embedding matrix weights with the joint
network's output layer (a.k.a. weight tying, commonly used in language modeling
<a href="/abs/1611.01462">arXiv:1611.01462</a> [cs.LG]). This simple design, when used in conjunction with
additional Edit-based Minimum Bayes Risk (EMBR) training, reduces the RNN-T
Decoder from 23M parameters to just 2M, without affecting word-error rate
(WER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text as Causal Mediators: Research Design for Causal Estimates of Differential Treatment of Social Groups via Language Aspects. (arXiv:2109.07542v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07542">
<div class="article-summary-box-inner">
<span><p>Using observed language to understand interpersonal interactions is important
in high-stakes decision making. We propose a causal research design for
observational (non-experimental) data to estimate the natural direct and
indirect effects of social group signals (e.g. race or gender) on speakers'
responses with separate aspects of language as causal mediators. We illustrate
the promises and challenges of this framework via a theoretical case study of
the effect of an advocate's gender on interruptions from justices during U.S.
Supreme Court oral arguments. We also discuss challenges conceptualizing and
operationalizing causal variables such as gender and language that comprise of
many components, and we articulate technical open challenges such as temporal
dependence between language mediators in conversational settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"It doesn't look good for a date": Transforming Critiques into Preferences for Conversational Recommendation Systems. (arXiv:2109.07576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07576">
<div class="article-summary-box-inner">
<span><p>Conversations aimed at determining good recommendations are iterative in
nature. People often express their preferences in terms of a critique of the
current recommendation (e.g., "It doesn't look good for a date"), requiring
some degree of common sense for a preference to be inferred. In this work, we
present a method for transforming a user critique into a positive preference
(e.g., "I prefer more romantic") in order to retrieve reviews pertaining to
potentially better recommendations (e.g., "Perfect for a romantic dinner"). We
leverage a large neural language model (LM) in a few-shot setting to perform
critique-to-preference transformation, and we test two methods for retrieving
recommendations: one that matches embeddings, and another that fine-tunes an LM
for the task. We instantiate this approach in the restaurant domain and
evaluate it using a new dataset of restaurant critiques. In an ablation study,
we show that utilizing critique-to-preference transformation improves
recommendations, and that there are at least three general cases that explain
this improved performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An influencer-based approach to understanding radical right viral tweets. (arXiv:2109.07588v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07588">
<div class="article-summary-box-inner">
<span><p>Radical right influencers routinely use social media to spread highly
divisive, disruptive and anti-democratic messages. Assessing and countering the
challenge that such content poses is crucial for ensuring that online spaces
remain open, safe and accessible. Previous work has paid little attention to
understanding factors associated with radical right content that goes viral. We
investigate this issue with a new dataset ROT which provides insight into the
content, engagement and followership of a set of 35 radical right influencers.
It includes over 50,000 original entries and over 40 million retweets, quotes,
replies and mentions. We use a multilevel model to measure engagement with
tweets, which are nested in each influencer. We show that it is crucial to
account for the influencer-level structure, and find evidence of the importance
of both influencer- and content-level factors, including the number of
followers each influencer has, the type of content (original posts, quotes and
replies), the length and toxicity of content, and whether influencers request
retweets. We make ROT available for other researchers to use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning. (arXiv:2109.07589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07589">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) in Few-Shot setting is imperative for entity
tagging in low resource domains. Existing approaches only learn class-specific
semantic features and intermediate representations from source domains. This
affects generalizability to unseen target domains, resulting in suboptimal
performances. To this end, we present CONTaiNER, a novel contrastive learning
technique that optimizes the inter-token distribution distance for Few-Shot
NER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a
generalized objective of differentiating between token categories based on
their Gaussian-distributed embeddings. This effectively alleviates overfitting
issues originating from training domains. Our experiments in several
traditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large
scale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER
outperforms previous methods by 3%-13% absolute F1 points while showing
consistent performance trends, even in challenging scenarios where previous
approaches could not achieve appreciable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Data Selection and Fine Tuning for Domain Adaptation. (arXiv:2109.07591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07591">
<div class="article-summary-box-inner">
<span><p>Domain adaptation of neural networks commonly relies on three training
phases: pretraining, selected data training and then fine tuning. Data
selection improves target domain generalization by training further on
pretraining data identified by relying on a small sample of target domain data.
This work examines the benefit of data selection for language modeling and
machine translation. Our experiments assess the complementarity of selection
with fine tuning and result in practical recommendations: (i) selected data
must be similar to the fine-tuning domain but not so much as to erode the
complementary effect of fine-tuning; (ii) there is a trade-off between
selecting little data for fast but limited progress or much data for slow but
long lasting progress; (iii) data selection can be applied early during
pretraining, with performance gains comparable to long pretraining session;
(iv) data selection from domain classifiers is often more effective than the
popular contrastive data selection method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification. (arXiv:2109.07604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07604">
<div class="article-summary-box-inner">
<span><p>Traditional hand-crafted linguistically-informed features have often been
used for distinguishing between translated and original non-translated texts.
By contrast, to date, neural architectures without manual feature engineering
have been less explored for this task. In this work, we (i) compare the
traditional feature-engineering-based approach to the feature-learning-based
one and (ii) analyse the neural architectures in order to investigate how well
the hand-crafted features explain the variance in the neural models'
predictions. We use pre-trained neural word embeddings, as well as several
end-to-end neural architectures in both monolingual and multilingual settings
and compare them to feature-engineering-based SVM classifiers. We show that (i)
neural architectures outperform other approaches by more than 20 accuracy
points, with the BERT-based model performing the best in both the monolingual
and multilingual settings; (ii) while many individual hand-crafted
translationese features correlate with neural model predictions, feature
importance analysis shows that the most important features for neural and
classical architectures differ; and (iii) our multilingual experiments provide
empirical evidence for translationese universals across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ontology-Based Information Extraction System for Residential Land Use Suitability Analysis. (arXiv:2109.07672v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07672">
<div class="article-summary-box-inner">
<span><p>We propose an Ontology-Based Information Extraction (OBIE) system to automate
the extraction of the criteria and values applied in Land Use Suitability
Analysis (LUSA) from bylaw and regulation documents related to the geographic
area of interest. The results obtained by our proposed LUSA OBIE system (land
use suitability criteria and their values) are presented as an ontology
populated with instances of the extracted criteria and property values. This
latter output ontology is incorporated into a Multi-Criteria Decision Making
(MCDM) model applied for constructing suitability maps for different kinds of
land uses. The resulting maps may be the final desired product or can be
incorporated into the cellular automata urban modeling and simulation for
predicting future urban growth. A case study has been conducted where the
output from LUSA OBIE is applied to help produce a suitability map for the City
of Regina, Saskatchewan, to assist in the identification of suitable areas for
residential development. A set of Saskatchewan bylaw and regulation documents
were downloaded and input to the LUSA OBIE system. We accessed the extracted
information using both the populated LUSA ontology and the set of annotated
documents. In this regard, the LUSA OBIE system was effective in producing a
final suitability map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset. (arXiv:2109.07679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07679">
<div class="article-summary-box-inner">
<span><p>Reasoning over commonsense knowledge bases (CSKB) whose elements are in the
form of free-text is an important yet hard task in NLP. While CSKB completion
only fills the missing links within the domain of the CSKB, CSKB population is
alternatively proposed with the goal of reasoning unseen assertions from
external resources. In this task, CSKBs are grounded to a large-scale
eventuality (activity, state, and event) graph to discriminate whether novel
triples from the eventuality graph are plausible or not. However, existing
evaluations on the population task are either not accurate (automatic
evaluation with randomly sampled negative examples) or of small scale (human
annotation). In this paper, we benchmark the CSKB population task with a new
large-scale dataset by first aligning four popular CSKBs, and then presenting a
high-quality human-annotated evaluation set to probe neural models' commonsense
reasoning ability. We also propose a novel inductive commonsense reasoning
model that reasons over graphs. Experimental results show that generalizing
commonsense reasoning on unseen assertions is inherently a hard task. Models
achieving high accuracy during training perform poorly on the evaluation set,
with a large gap between human performance. We will make the data publicly
available for future contributions. Codes and data are available at
https://github.com/HKUST-KnowComp/CSKB-Population.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07680">
<div class="article-summary-box-inner">
<span><p>Identification of user's opinions from natural language text has become an
exciting field of research due to its growing applications in the real world.
The research field is known as sentiment analysis and classification, where
aspect category detection (ACD) and aspect category polarity (ACP) are two
important sub-tasks of aspect-based sentiment analysis. The goal in ACD is to
specify which aspect of the entity comes up in opinion while ACP aims to
specify the polarity of each aspect category from the ACD task. The previous
works mostly propose separate solutions for these two sub-tasks. This paper
focuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The
proposed method carries out multi-label classification where four different
deep models were employed and comparatively evaluated to examine their
performance. A dataset of Persian reviews was collected from CinemaTicket
website including 2200 samples from 14 categories. The developed models were
evaluated using the collected dataset in terms of example-based and label-based
metrics. The results indicate the high applicability and preference of the CNN
and GRU models in comparison to LSTM and Bi-LSTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Few-shot Multilingual Learners. (arXiv:2109.07684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07684">
<div class="article-summary-box-inner">
<span><p>General-purpose language models have demonstrated impressive capabilities,
performing on par with state-of-the-art approaches on a range of downstream
natural language processing (NLP) tasks and benchmarks when inferring
instructions from very few examples. Here, we evaluate the multilingual skills
of the GPT and T5 models in conducting multi-class classification on
non-English languages without any parameter updates. We show that, given a few
English examples as context, pre-trained language models can predict not only
English test samples but also non-English ones. Finally, we find the in-context
few-shot cross-lingual prediction results of language models are significantly
better than random prediction, and they are competitive compared to the
existing state-of-the-art cross-lingual models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferable Persona-Grounded Dialogues via Grounded Minimal Edits. (arXiv:2109.07713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07713">
<div class="article-summary-box-inner">
<span><p>Grounded dialogue models generate responses that are grounded on certain
concepts. Limited by the distribution of grounded dialogue data, models trained
on such data face the transferability challenges in terms of the data
distribution and the type of grounded concepts. To address the challenges, we
propose the grounded minimal editing framework, which minimally edits existing
responses to be grounded on the given concept. Focusing on personas, we propose
Grounded Minimal Editor (GME), which learns to edit by disentangling and
recombining persona-related and persona-agnostic parts of the response. To
evaluate persona-grounded minimal editing, we present the PersonaMinEdit
dataset, and experimental results show that GME outperforms competitive
baselines by a large margin. To evaluate the transferability, we experiment on
the test set of BlendedSkillTalk and show that GME can edit dialogue models'
responses to largely improve their persona consistency while preserving the use
of knowledge and empathy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sister Help: Data Augmentation for Frame-Semantic Role Labeling. (arXiv:2109.07725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07725">
<div class="article-summary-box-inner">
<span><p>While FrameNet is widely regarded as a rich resource of semantics in natural
language processing, a major criticism concerns its lack of coverage and the
relative paucity of its labeled data compared to other commonly used lexical
resources such as PropBank and VerbNet. This paper reports on a pilot study to
address these gaps. We propose a data augmentation approach, which uses
existing frame-specific annotation to automatically annotate other lexical
units of the same frame which are unannotated. Our rule-based approach defines
the notion of a sister lexical unit and generates frame-specific augmented data
for training. We present experiments on frame-semantic role labeling which
demonstrate the importance of this data augmentation: we obtain a large
improvement to prior results on frame identification and argument
identification for FrameNet, utilizing both full-text and lexicographic
annotations under FrameNet. Our findings on data augmentation highlight the
value of automatic resource creation for improved models in frame-semantic
parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOVER: Mask, Over-generate and Rank for Hyperbole Generation. (arXiv:2109.07726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07726">
<div class="article-summary-box-inner">
<span><p>Despite being a common figure of speech, hyperbole is under-researched with
only a few studies addressing its identification task. In this paper, we
introduce a new task of hyperbole generation to transfer a literal sentence
into its hyperbolic paraphrase. To tackle the lack of available hyperbolic
sentences, we construct HYPO-XL, the first large-scale hyperbole corpus
containing 17,862 hyperbolic sentences in a non-trivial way. Based on our
corpus, we propose an unsupervised method for hyperbole generation with no need
for parallel literal-hyperbole pairs. During training, we fine-tune BART to
infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we
mask part of an input literal sentence and over-generate multiple possible
hyperbolic versions. Then a BERT-based ranker selects the best candidate by
hyperbolicity and paraphrase quality. Human evaluation results show that our
model is capable of generating hyperbolic paraphrase sentences and outperforms
several baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws for Neural Machine Translation. (arXiv:2109.07740v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07740">
<div class="article-summary-box-inner">
<span><p>We present an empirical study of scaling properties of encoder-decoder
Transformer models used in neural machine translation (NMT). We show that
cross-entropy loss as a function of model size follows a certain scaling law.
Specifically (i) We propose a formula which describes the scaling behavior of
cross-entropy loss as a bivariate function of encoder and decoder size, and
show that it gives accurate predictions under a variety of scaling approaches
and languages; we show that the total number of parameters alone is not
sufficient for such purposes. (ii) We observe different power law exponents
when scaling the decoder vs scaling the encoder, and provide recommendations
for optimal allocation of encoder/decoder capacity based on this observation.
(iii) We also report that the scaling behavior of the model is acutely
influenced by composition bias of the train/test sets, which we define as any
deviation from naturally generated text (either via machine generated or human
translated text). We observe that natural text on the target side enjoys
scaling, which manifests as successful reduction of the cross-entropy loss.
(iv) Finally, we investigate the relationship between the cross-entropy loss
and the quality of the generated translations. We find two different behaviors,
depending on the nature of the test data. For test sets which were originally
translated from target language to source language, both loss and BLEU score
improve as model size increases. In contrast, for test sets originally
translated from source language to target language, the loss improves, but the
BLEU score stops improving after a certain threshold. We release generated text
from all models used in this study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish Biomedical Language Models. (arXiv:2109.07765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07765">
<div class="article-summary-box-inner">
<span><p>We introduce CoWeSe (the Corpus Web Salud Espa\~nol), the largest Spanish
biomedical corpus to date, consisting of 4.5GB (about 750M tokens) of clean
plain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains
executed in 2020. The corpus is openly available and already preprocessed.
CoWeSe is an important resource for biomedical and health NLP in Spanish and
has already been employed to train domain-specific language models and to
produce word embbedings. We released the CoWeSe corpus under a Creative Commons
Attribution 4.0 International license, both in Zenodo
(\url{https://zenodo.org/record/4561971\#.YTI5SnVKiEA}).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07779">
<div class="article-summary-box-inner">
<span><p>Researches on dialogue empathy aim to endow an agent with the capacity of
accurate understanding and proper responding for emotions. Existing models for
empathetic dialogue generation focus on the emotion flow in one direction, that
is, from the context to response. We argue that conducting an empathetic
conversation is a bidirectional process, where empathy occurs when the emotions
of two interlocutors could converge on the same point, i.e., reaching an
emotion consensus. Besides, we also find that the empathetic dialogue corpus is
extremely limited, which further restricts the model performance. To address
the above issues, we propose a dual-generative model, Dual-Emp, to
simultaneously construct the emotion consensus and utilize some external
unpaired data. Specifically, our model integrates a forward dialogue model, a
backward dialogue model, and a discrete latent variable representing the
emotion consensus into a unified architecture. Then, to alleviate the
constraint of paired data, we extract unpaired emotional data from open-domain
conversations and employ Dual-Emp to produce pseudo paired empathetic samples,
which is more efficient and low-cost than the human annotation. Automatic and
human evaluations demonstrate that our method outperforms competitive baselines
in producing coherent and empathetic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural Machine Translation by Bidirectional Training. (arXiv:2109.07780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07780">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective pretraining strategy -- bidirectional
training (BiT) for neural machine translation. Specifically, we bidirectionally
update the model parameters at the early stage and then tune the model
normally. To achieve bidirectional updating, we simply reconstruct the training
samples from "src$\rightarrow$tgt" to "src+tgt$\rightarrow$tgt+src" without any
complicated model modifications. Notably, our approach does not increase any
parameters or training steps, requiring the parallel data merely. Experimental
results show that BiT pushes the SOTA neural machine translation performance
across 15 translation tasks on 8 language pairs (data sizes range from 160K to
38M) significantly higher. Encouragingly, our proposed model can complement
existing data manipulation strategies, i.e. back translation, data
distillation, and data diversification. Extensive analyses show that our
approach functions as a novel bilingual code-switcher, obtaining better
bilingual alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transductive Learning for Unsupervised Text Style Transfer. (arXiv:2109.07812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07812">
<div class="article-summary-box-inner">
<span><p>Unsupervised style transfer models are mainly based on an inductive learning
approach, which represents the style as embeddings, decoder parameters, or
discriminator parameters and directly applies these general rules to the test
cases. However, the lacking of parallel corpus hinders the ability of these
inductive learning methods on this task. As a result, it is likely to cause
severe inconsistent style expressions, like `the salad is rude`. To tackle this
problem, we propose a novel transductive learning approach in this paper, based
on a retrieval-based context-aware style representation. Specifically, an
attentional encoder-decoder with a retriever framework is utilized. It involves
top-K relevant sentences in the target style in the transfer process. In this
way, we can learn a context-aware style embedding to alleviate the above
inconsistency problem. In this paper, both sparse (BM25) and dense retrieval
functions (MIPS) are used, and two objective functions are designed to
facilitate joint learning. Experimental results show that our method
outperforms several strong baselines. The proposed transductive learning
approach is general and effective to the task of unsupervised style transfer,
and we will apply it to the other two typical methods in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reframing Instructional Prompts to GPTk's Language. (arXiv:2109.07830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07830">
<div class="article-summary-box-inner">
<span><p>How can model designers turn task instructions into effective prompts for
language models? Backed by extensive empirical analysis on GPT3, we observe
important features for successful instructional prompts, and propose several
reframing techniques for model designers to create such prompts. For example, a
complex task can be decomposed into multiple simpler tasks. We experiment over
12 NLP tasks across 6 diverse categories (question generation, classification,
etc.). Our results show that reframing improves few-shot learning performance
by 14\% while reducing sample complexity over existing few-shot baselines. The
performance gains are particularly important on large language models, such as
GPT3 where tuning models or prompts on large datasets is not feasible.
Furthermore, we observe that such gains are not limited to GPT3; the reframed
tasks remain superior over raw instructions across different model
architectures, underscoring the cross-model generality of these guidelines. We
hope these empirical-driven techniques will pave way for more effective ways to
prompt LMs in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings. (arXiv:2109.07833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07833">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) requires models to learn and apply
commonsense knowledge. These reasoning abilities are particularly important for
explainable NLI systems that generate a natural language explanation in
addition to their label prediction. The integration of external knowledge has
been shown to improve NLI systems, here we investigate whether it can also
improve their explanation capabilities. For this, we investigate different
sources of external knowledge and evaluate the performance of our models on
in-domain data as well as on special transfer datasets that are designed to
assess fine-grained reasoning capabilities. We find that different sources of
knowledge have a different effect on reasoning abilities, for example, implicit
knowledge stored in language models can hinder reasoning on numbers and
negations. Finally, we conduct the largest and most fine-grained explainable
NLI crowdsourcing study to date. It reveals that even large differences in
automatic performance scores do neither reflect in human ratings of label,
explanation, commonsense nor grammar correctness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation. (arXiv:2109.07848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07848">
<div class="article-summary-box-inner">
<span><p>Temporary syntactic ambiguities arise when the beginning of a sentence is
compatible with multiple syntactic analyses. We inspect to which extent neural
language models (LMs) exhibit uncertainty over such analyses when processing
temporarily ambiguous inputs, and how that uncertainty is modulated by
disambiguating cues. We probe the LM's expectations by generating from it: we
use stochastic decoding to derive a set of sentence completions, and estimate
the probability that the LM assigns to each interpretation based on the
distribution of parses across completions. Unlike scoring-based methods for
targeted syntactic evaluation, this technique makes it possible to explore
completions that are not hypothesized in advance by the researcher. We apply
this method to study the behavior of two LMs (GPT2 and an LSTM) on three types
of temporary ambiguity, using materials from human sentence processing
experiments. We find that LMs can track multiple analyses simultaneously; the
degree of uncertainty varies across constructions and contexts. As a response
to disambiguating cues, the LMs often select the correct interpretation, but
occasional errors point to potential areas of improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translation Transformers Rediscover Inherent Data Domains. (arXiv:2109.07864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07864">
<div class="article-summary-box-inner">
<span><p>Many works proposed methods to improve the performance of Neural Machine
Translation (NMT) models in a domain/multi-domain adaptation scenario. However,
an understanding of how NMT baselines represent text domain information
internally is still lacking. Here we analyze the sentence representations
learned by NMT Transformers and show that these explicitly include the
information on text domains, even after only seeing the input sentences without
domains labels. Furthermore, we show that this internal information is enough
to cluster sentences by their underlying domains without supervision. We show
that NMT models produce clusters better aligned to the actual domains compared
to pre-trained language models (LMs). Notably, when computed on document-level,
NMT cluster-to-domain correspondence nears 100%. We use these findings together
with an approach to NMT domain adaptation using automatically extracted
domains. Whereas previous work relied on external LMs for text clustering, we
propose re-using the NMT model as a source of unsupervised clusters. We perform
an extensive experimental study comparing two approaches across two data
scenarios, three language pairs, and both sentence-level and document-level
clustering, showing equal or significantly superior performance compared to
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Humanly Certifying Superhuman Classifiers. (arXiv:2109.07867v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07867">
<div class="article-summary-box-inner">
<span><p>Estimating the performance of a machine learning system is a longstanding
challenge in artificial intelligence research. Today, this challenge is
especially relevant given the emergence of systems which appear to increasingly
outperform human beings. In some cases, this "superhuman" performance is
readily demonstrated; for example by defeating legendary human players in
traditional two player games. On the other hand, it can be challenging to
evaluate classification models that potentially surpass human performance.
Indeed, human annotations are often treated as a ground truth, which implicitly
assumes the superiority of the human over any models trained on human
annotations. In reality, human annotators can make mistakes and be subjective.
Evaluating the performance with respect to a genuine oracle may be more
objective and reliable, even when querying the oracle is expensive or
impossible. In this paper, we first raise the challenge of evaluating the
performance of both humans and models with respect to an oracle which is
unobserved. We develop a theory for estimating the accuracy compared to the
oracle, using only imperfect human annotations for reference. Our analysis
provides a simple recipe for detecting and certifying superhuman performance in
this setting, which we believe will assist in understanding the stage of
current research on classification. We validate the convergence of the bounds
and the assumptions of our theory on carefully designed toy experiments with
known oracles. Moreover, we demonstrate the utility of our theory by
meta-analyzing large-scale natural language processing tasks, for which an
oracle does not exist, and show that under our assumptions a number of models
from recent years are with high probability superhuman.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity Recognition. (arXiv:2109.07877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07877">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models lead Named Entity Recognition (NER) into a new
era, while some more knowledge is needed to improve their performance in
specific problems. In Chinese NER, character substitution is a complicated
linguistic phenomenon. Some Chinese characters are quite similar for sharing
the same components or having similar pronunciations. People replace characters
in a named entity with similar characters to generate a new collocation but
referring to the same object. It becomes even more common in the Internet age
and is often used to avoid Internet censorship or just for fun. Such character
substitution is not friendly to those pre-trained language models because the
new collocations are occasional. As a result, it always leads to unrecognizable
or recognition errors in the NER task. In this paper, we propose a new method,
Multi-Feature Fusion Embedding for Chinese Named Entity Recognition (MFE-NER),
to strengthen the language pattern of Chinese and handle the character
substitution problem in Chinese Named Entity Recognition. MFE fuses semantic,
glyph, and phonetic features together. In the glyph domain, we disassemble
Chinese characters into components to denote structure features so that
characters with similar structures can have close embedding space
representation. Meanwhile, an improved phonetic system is also proposed in our
work, making it reasonable to calculate phonetic similarity among Chinese
characters. Experiments demonstrate that our method improves the overall
performance of Chinese NER and especially performs well in informal language
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surveying the Research on Fake News in Social Media: a Tale of Networks and Language. (arXiv:2109.07909v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07909">
<div class="article-summary-box-inner">
<span><p>The history of journalism and news diffusion is tightly coupled with the
effort to dispel hoaxes, misinformation, propaganda, unverified rumours, poor
reporting, and messages containing hate and divisions. With the explosive
growth of online social media and billions of individuals engaged with
consuming, creating, and sharing news, this ancient problem has surfaced with a
renewed intensity threatening our democracies, public health, and news outlets
credibility. This has triggered many researchers to develop new methods for
studying, understanding, detecting, and preventing fake-news diffusion; as a
consequence, thousands of scientific papers have been published in a relatively
short period, making researchers of different disciplines to struggle in search
of open problems and most relevant trends. The aim of this survey is threefold:
first, we want to provide the researchers interested in this multidisciplinary
and challenging area with a network-based analysis of the existing literature
to assist them with a visual exploration of papers that can be of interest;
second, we present a selection of the main results achieved so far adopting the
network as an unifying framework to represent and make sense of data, to model
diffusion processes, and to evaluate different debunking strategies. Finally,
we present an outline of the most relevant research trends focusing on the
moving target of fake-news, bots, and trolls identification by means of data
mining and text technologies; despite scholars working on computational
linguistics and networks traditionally belong to different scientific
communities, we expect that forthcoming computational approaches to prevent
fake news from polluting the social media must be developed using hybrid and
up-to-date methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (arXiv:2109.07926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07926">
<div class="article-summary-box-inner">
<span><p>Recently more attention has been given to adversarial attacks on neural
networks for natural language processing (NLP). A central research topic has
been the investigation of search algorithms and search constraints, accompanied
by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth
order optimization-based attacks and compare with the benchmark results in the
TextAttack framework. Surprisingly, we find that optimization-based methods do
not yield any improvement in a constrained setup and slightly benefit from
approximate gradient information only in unconstrained setups where search
spaces are larger. In contrast, simple heuristics exploiting nearest neighbors
without querying the target function yield substantial success rates in
constrained setups, and nearly full success rate in unconstrained setups, at an
order of magnitude fewer queries. We conclude from these results that current
TextAttack benchmark tasks are too easy and constraints are too strict,
preventing meaningful research on black-box adversarial text attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization. (arXiv:2109.07943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07943">
<div class="article-summary-box-inner">
<span><p>Existing summarization systems mostly generate summaries purely relying on
the content of the source document. However, even for humans, we usually need
some references or exemplars to help us fully understand the source document
and write summaries in a particular format. But how to find the high-quality
exemplars and incorporate them into summarization systems is still challenging
and worth exploring. In this paper, we propose RetrievalSum, a novel retrieval
enhanced abstractive summarization framework consisting of a dense Retriever
and a Summarizer. At first, several closely related exemplars are retrieved as
supplementary input to help the generation model understand the text more
comprehensively. Furthermore, retrieved exemplars can also play a role in
guiding the model to capture the writing style of a specific corpus. We
validate our method on a wide range of summarization datasets across multiple
domains and two backbone models: BERT and BART. Results show that our framework
obtains significant improvement by 1.38~4.66 in ROUGE-1 score when compared
with the powerful pre-trained models, and achieve new state-of-the-art on
BillSum. Human evaluation demonstrates that our retrieval enhanced model can
better capture the domain-specific writing style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Attribute Injection for Pretrained Language Models. (arXiv:2109.07953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07953">
<div class="article-summary-box-inner">
<span><p>Metadata attributes (e.g., user and product IDs from reviews) can be
incorporated as additional inputs to neural-based NLP models, by modifying the
architecture of the models, in order to improve their performance. Recent
models however rely on pretrained language models (PLMs), where previously used
techniques for attribute injection are either nontrivial or ineffective. In
this paper, we propose a lightweight and memory-efficient method to inject
attributes to PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules,
to include attributes both independently of or jointly with the text. To limit
the increase of parameters especially when the attribute vocabulary is large,
we use low-rank approximations and hypercomplex multiplications, significantly
decreasing the total parameters. We also introduce training mechanisms to
handle domains in which attributes can be multi-labeled or sparse. Extensive
experiments and analyses on eight datasets from different domains show that our
method outperforms previous attribute injection methods and achieves
state-of-the-art performance on various datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Unsupervised Question Answering via Summarization-Informed Question Generation. (arXiv:2109.07954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07954">
<div class="article-summary-box-inner">
<span><p>Question Generation (QG) is the task of generating a plausible question for a
given &lt;passage, answer&gt; pair. Template-based QG uses linguistically-informed
heuristics to transform declarative sentences into interrogatives, whereas
supervised QG uses existing Question Answering (QA) datasets to train a system
to generate a question given a passage and an answer. A disadvantage of the
heuristic approach is that the generated questions are heavily tied to their
declarative counterparts. A disadvantage of the supervised approach is that
they are heavily tied to the domain/language of the QA dataset used as training
data. In order to overcome these shortcomings, we propose an unsupervised QG
method which uses questions generated heuristically from summaries as a source
of training data for a QG system. We make use of freely available news summary
data, transforming declarative summary sentences into appropriate questions
using heuristics informed by dependency parsing, named entity recognition and
semantic role labeling. The resulting questions are then combined with the
original news articles to train an end-to-end neural QG model. We extrinsically
evaluate our approach using unsupervised QA: our QG model is used to generate
synthetic QA pairs for training a QA model. Experimental results show that,
trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model
substantially outperforms previous unsupervised models on three in-domain
datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain
datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TruthfulQA: Measuring How Models Mimic Human Falsehoods. (arXiv:2109.07958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07958">
<div class="article-summary-box-inner">
<span><p>We propose a benchmark to measure whether a language model is truthful in
generating answers to questions. The benchmark comprises 817 questions that
span 38 categories, including health, law, finance and politics. We crafted
questions that some humans would answer falsely due to a false belief or
misconception. To perform well, models must avoid generating false answers
learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a
T5-based model. The best model was truthful on 58% of questions, while human
performance was 94%. Models generated many false answers that mimic popular
misconceptions and have the potential to deceive humans. The largest models
were generally the least truthful. For example, the 6B-parameter GPT-J model
was 17% less truthful than its 125M-parameter counterpart. This contrasts with
other NLP tasks, where performance improves with model size. However, this
result is expected if false answers are learned from the training distribution.
We suggest that scaling up models alone is less promising for improving
truthfulness than fine-tuning using training objectives other than imitation of
text from the web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alquist 4.0: Towards Social Intelligence Using Generative Models and Dialogue Personalization. (arXiv:2109.07968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07968">
<div class="article-summary-box-inner">
<span><p>The open domain-dialogue system Alquist has a goal to conduct a coherent and
engaging conversation that can be considered as one of the benchmarks of social
intelligence. The fourth version of the system, developed within the Alexa
Prize Socialbot Grand Challenge 4, brings two main innovations. The first
addresses coherence, and the second addresses the engagingness of the
conversation. For innovations regarding coherence, we propose a novel hybrid
approach combining hand-designed responses and a generative model. The proposed
approach utilizes hand-designed dialogues, out-of-domain detection, and a
neural response generator. Hand-designed dialogues walk the user through
high-quality conversational flows. The out-of-domain detection recognizes that
the user diverges from the predefined flow and prevents the system from
producing a scripted response that might not make sense for unexpected user
input. Finally, the neural response generator generates a response based on the
context of the dialogue that correctly reacts to the unexpected user input and
returns the dialogue to the boundaries of hand-designed dialogues. The
innovations for engagement that we propose are mostly inspired by the famous
exploration-exploitation dilemma. To conduct an engaging conversation with the
dialogue partners, one has to learn their preferences and interests --
exploration. Moreover, to engage the partner, we have to utilize the knowledge
we have already learned -- exploitation. In this work, we present the
principles and inner workings of individual components of the open-domain
dialogue system Alquist developed within the Alexa Prize Socialbot Grand
Challenge 4 and the experiments we have conducted to evaluate them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Language Models Know the Way to Rome?. (arXiv:2109.07971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07971">
<div class="article-summary-box-inner">
<span><p>The global geometry of language models is important for a range of
applications, but language model probes tend to evaluate rather local
relations, for which ground truths are easily obtained. In this paper we
exploit the fact that in geography, ground truths are available beyond local
relations. In a series of experiments, we evaluate the extent to which language
model representations of city and country names are isomorphic to real-world
geography, e.g., if you tell a language model where Paris and Berlin are, does
it know the way to Rome? We find that language models generally encode limited
geographic information, but with larger models performing the best, suggesting
that geographic knowledge can be induced from higher-order co-occurrence
statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07983">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations for understanding the behavior of black box models
has gained a lot of attention recently as they provide potential for recourse.
In this paper, we propose a method Contrastive Attributed explanations for Text
(CAT) which provides contrastive explanations for natural language text data
with a novel twist as we build and exploit attribute classifiers leading to
more semantically meaningful explanations. To ensure that our contrastive
generated text has the fewest possible edits with respect to the original text,
while also being fluent and close to a human generated contrastive, we resort
to a minimal perturbation approach regularized using a BERT language model and
attribute classifiers trained on available attributes. We show through
qualitative examples and a user study that our method not only conveys more
insight because of these attributes, but also leads to better quality
(contrastive) text. Moreover, quantitatively we show that our method is more
efficient than other state-of-the-art methods with it also scoring higher on
benchmark metrics such as flip rate, (normalized) Levenstein distance, fluency
and content preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-aware Entity Typing in Knowledge Graphs. (arXiv:2109.07990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07990">
<div class="article-summary-box-inner">
<span><p>Knowledge graph entity typing aims to infer entities' missing types in
knowledge graphs which is an important but under-explored issue. This paper
proposes a novel method for this task by utilizing entities' contextual
information. Specifically, we design two inference mechanisms: i) N2T:
independently use each neighbor of an entity to infer its type; ii) Agg2T:
aggregate the neighbors of an entity to infer its type. Those mechanisms will
produce multiple inference results, and an exponentially weighted pooling
method is used to generate the final inference result. Furthermore, we propose
a novel loss function to alleviate the false-negative problem during training.
Experiments on two real-world KGs demonstrate the effectiveness of our method.
The source code and data of this paper can be obtained from
https://github.com/CCIIPLab/CET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowMAN: Weakly Supervised Multinomial Adversarial Networks. (arXiv:2109.07994v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07994">
<div class="article-summary-box-inner">
<span><p>The absence of labeled data for training neural models is often addressed by
leveraging knowledge about the specific task, resulting in heuristic but noisy
labels. The knowledge is captured in labeling functions, which detect certain
regularities or patterns in the training samples and annotate corresponding
labels for training. This process of weakly supervised training may result in
an over-reliance on the signals captured by the labeling functions and hinder
models to exploit other signals or to generalize well. We propose KnowMAN, an
adversarial scheme that enables to control influence of signals associated with
specific labeling functions. KnowMAN forces the network to learn
representations that are invariant to those signals and to pick up other
signals that are more generally associated with an output label. KnowMAN
strongly improves results compared to direct weakly supervised learning with a
pre-trained transformer language model and a feature-based baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NiuTrans System for the WMT21 Efficiency Task. (arXiv:2109.08003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08003">
<div class="article-summary-box-inner">
<span><p>This paper describes the NiuTrans system for the WMT21 translation efficiency
task (<a href="http://statmt.org/wmt21/efficiency-task.html">this http URL</a>). Following last year's
work, we explore various techniques to improve efficiency while maintaining
translation quality. We investigate the combinations of lightweight Transformer
architectures and knowledge distillation strategies. Also, we improve the
translation efficiency with graph optimization, low precision, dynamic
batching, and parallel pre/post-processing. Our system can translate 247,000
words per second on an NVIDIA A100, being 3$\times$ faster than last year's
system. Our system is the fastest and has the lowest memory consumption on the
GPU-throughput track. The code, model, and pipeline will be available at
NiuTrans.NMT (https://github.com/NiuTrans/NiuTrans.NMT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NiuTrans System for WNGT 2020 Efficiency Task. (arXiv:2109.08008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08008">
<div class="article-summary-box-inner">
<span><p>This paper describes the submissions of the NiuTrans Team to the WNGT 2020
Efficiency Shared Task. We focus on the efficient implementation of deep
Transformer models \cite{wang-etal-2019-learning, li-etal-2019-niutrans} using
NiuTensor (https://github.com/NiuTrans/NiuTensor), a flexible toolkit for NLP
tasks. We explored the combination of deep encoder and shallow decoder in
Transformer models via model compression and knowledge distillation. The neural
machine translation decoding also benefits from FP16 inference, attention
caching, dynamic batching, and batch pruning. Our systems achieve promising
results in both translation quality and efficiency, e.g., our fastest system
can translate more than 40,000 tokens per second with an RTX 2080 Ti while
maintaining 42.9 BLEU on \textit{newstest2018}. The code, models, and docker
images are available at NiuTrans.NMT
(https://github.com/NiuTrans/NiuTrans.NMT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propaganda Techniques in Memes. (arXiv:2109.08013v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08013">
<div class="article-summary-box-inner">
<span><p>Propaganda can be defined as a form of communication that aims to influence
the opinions or the actions of people towards a specific goal; this is achieved
by means of well-defined rhetorical and psychological devices. Propaganda, in
the form we know it today, can be dated back to the beginning of the 17th
century. However, it is with the advent of the Internet and the social media
that it has started to spread on a much larger scale than before, thus becoming
major societal and political issue. Nowadays, a large fraction of propaganda in
social media is multimodal, mixing textual with visual content. With this in
mind, here we propose a new multi-label multimodal task: detecting the type of
propaganda techniques used in memes. We further create and release a new corpus
of 950 memes, carefully annotated with 22 propaganda techniques, which can
appear in the text, in the image, or in both. Our analysis of the corpus shows
that understanding both modalities together is essential for detecting these
techniques. This is further confirmed in our experiments with several
state-of-the-art multimodal models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Concept of Semantic Value in Social Network Analysis: an Application to Comparative Mythology. (arXiv:2109.08023v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08023">
<div class="article-summary-box-inner">
<span><p>Human sciences have traditionally relied on human reasoning and intelligence
to infer knowledge from a wide range of sources, such as oral and written
narrations, reports, and traditions. Here we develop an extension of classical
social network analysis approaches to incorporate the concept of meaning in
each actor, as a mean to quantify and infer further knowledge from the original
source of the network. This extension is based on a new affinity function, the
semantic affinity, that establishes fuzzy-like relationships between the
different actors in the network, using combinations of affinity functions. We
also propose a new heuristic algorithm based on the shortest capacity problem
to compute this affinity function. We use these concept of meaning and semantic
affinity to analyze and compare the gods and heroes from three different
classical mythologies: Greek, Celtic and Nordic. We study the relationships of
each individual mythology and those of common structure that is formed when we
fuse the three of them. We show a strong connection between the Celtic and
Nordic gods and that Greeks put more emphasis on heroic characters rather than
deities. Our approach provides a technique to highlight and quantify important
relationships in the original domain of the network not deducible from its
structural properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locating Language-Specific Information in Contextualized Embeddings. (arXiv:2109.08040v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08040">
<div class="article-summary-box-inner">
<span><p>Multilingual pretrained language models (MPLMs) exhibit multilinguality and
are well suited for transfer across languages. Most MPLMs are trained in an
unsupervised fashion and the relationship between their objective and
multilinguality is unclear. More specifically, the question whether MPLM
representations are language-agnostic or they simply interleave well with
learned task prediction heads arises. In this work, we locate language-specific
information in MPLMs and identify its dimensionality and the layers where this
information occurs. We show that language-specific information is scattered
across many dimensions, which can be projected into a linear subspace. Our
study contributes to a better understanding of MPLM representations, going
beyond treating them as unanalyzable blobs of information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Error Type Annotation for Arabic. (arXiv:2109.08068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08068">
<div class="article-summary-box-inner">
<span><p>We present ARETA, an automatic error type annotation system for Modern
Standard Arabic. We design ARETA to address Arabic's morphological richness and
orthographic ambiguity. We base our error taxonomy on the Arabic Learner Corpus
(ALC) Error Tagset with some modifications. ARETA achieves a performance of
85.8% (micro average F1 score) on a manually annotated blind test portion of
ALC. We also demonstrate ARETA's usability by applying it to a number of
submissions from the QALB 2014 shared task for Arabic grammatical error
correction. The resulting analyses give helpful insights on the strengths and
weaknesses of different submissions, which is more useful than the opaque M2
scoring metrics used in the shared task. ARETA employs a large Arabic
morphological analyzer, but is completely unsupervised otherwise. We make ARETA
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Open Information Extraction using Question Generation and Reading Comprehension. (arXiv:2109.08079v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08079">
<div class="article-summary-box-inner">
<span><p>Typically, Open Information Extraction (OpenIE) focuses on extracting
triples, representing a subject, a relation, and the object of the relation.
However, most of the existing techniques are based on a predefined set of
relations in each domain which limits their applicability to newer domains
where these relations may be unknown such as financial documents. This paper
presents a zero-shot open information extraction technique that extracts the
entities (value) and their descriptions (key) from a sentence, using off the
shelf machine reading comprehension (MRC) Model. The input questions to this
model are created using a novel noun phrase generation method. This method
takes the context of the sentence into account and can create a wide variety of
questions making our technique domain independent. Given the questions and the
sentence, our technique uses the MRC model to extract entities (value). The
noun phrase corresponding to the question, with the highest confidence, is
taken as the description (key).
</p>
<p>This paper also introduces the EDGAR10-Q dataset which is based on publicly
available financial documents from corporations listed in US securities and
exchange commission (SEC). The dataset consists of paragraphs, tagged values
(entities), and their keys (descriptions) and is one of the largest among
entity extraction datasets. This dataset will be a valuable addition to the
research community, especially in the financial domain. Finally, the paper
demonstrates the efficacy of the proposed technique on the EDGAR10-Q and Ade
corpus drug dosage datasets, where it obtained 86.84 % and 97% accuracy,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection. (arXiv:2109.08113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08113">
<div class="article-summary-box-inner">
<span><p>Much of natural language processing is focused on leveraging large capacity
language models, typically trained over single messages with a task of
predicting one or more tokens. However, modeling human language at
higher-levels of context (i.e., sequences of messages) is under-explored. In
stance detection and other social media tasks where the goal is to predict an
attribute of a message, we have contextual data that is loosely semantically
connected by authorship. Here, we introduce Message-Level Transformer (MeLT) --
a hierarchical message-encoder pre-trained over Twitter and applied to the task
of stance prediction. We focus on stance prediction as a task benefiting from
knowing the context of the message (i.e., the sequence of previous messages).
The model is trained using a variant of masked-language modeling; where instead
of predicting tokens, it seeks to generate an entire masked (aggregated)
message vector via reconstruction loss. We find that applying this pre-trained
masked message-level transformer to the downstream task of stance detection
achieves F1 performance of 67%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Online Hate Speech through the Causal Lens. (arXiv:2109.08120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08120">
<div class="article-summary-box-inner">
<span><p>The societal issue of digital hostility has previously attracted a lot of
attention. The topic counts an ample body of literature, yet remains prominent
and challenging as ever due to its subjective nature. We posit that a better
understanding of this problem will require the use of causal inference
frameworks. This survey summarises the relevant research that revolves around
estimations of causal effects related to online hate speech. Initially, we
provide an argumentation as to why re-establishing the exploration of hate
speech in causal terms is of the essence. Following that, we give an overview
of the leading studies classified with respect to the direction of their
outcomes, as well as an outline of all related research, and a summary of open
research problems that can influence future work on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Tri-training of Dependency Parsers. (arXiv:2109.08122v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08122">
<div class="article-summary-box-inner">
<span><p>We compare two orthogonal semi-supervised learning techniques, namely
tri-training and pretrained word embeddings, in the task of dependency parsing.
We explore language-specific FastText and ELMo embeddings and multilingual BERT
embeddings. We focus on a low resource scenario as semi-supervised learning can
be expected to have the most impact here. Based on treebank size and available
ELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and
Vietnamese. Furthermore, we include English in a simulated low-resource
setting. We find that pretrained word embeddings make more effective use of
unlabelled data than tri-training but that the two approaches can be
successfully combined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Summary Evaluation Survive Translation to Other Languages?. (arXiv:2109.08129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08129">
<div class="article-summary-box-inner">
<span><p>The creation of a large summarization quality dataset is a considerable,
expensive, time-consuming effort, requiring careful planning and setup. It
includes producing human-written and machine-generated summaries and evaluation
of the summaries by humans, preferably by linguistic experts, and by automatic
evaluation tools. If such effort is made in one language, it would be
beneficial to be able to use it in other languages. To investigate how much we
can trust the translation of such dataset without repeating human annotations
in another language, we translated an existing English summarization dataset,
SummEval dataset, to four different languages and analyzed the scores from the
automatic evaluation metrics in translated languages, as well as their
correlation with human annotations in the source language. Our results reveal
that although translation changes the absolute value of automatic scores, the
scores keep the same rank order and approximately the same correlations with
human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase Retrieval Learns Passage Retrieval, Too. (arXiv:2109.08133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08133">
<div class="article-summary-box-inner">
<span><p>Dense retrieval methods have shown great promise over sparse retrieval
methods in a range of NLP problems. Among them, dense phrase retrieval-the most
fine-grained retrieval unit-is appealing because phrases can be directly used
as the output for question answering and slot filling tasks. In this work, we
follow the intuition that retrieving phrases naturally entails retrieving
larger text blocks and study whether phrase retrieval can serve as the basis
for coarse-level retrieval including passages and documents. We first observe
that a dense phrase-retrieval system, without any retraining, already achieves
better passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage
retrievers, which also helps achieve superior end-to-end QA performance with
fewer passages. Then, we provide an interpretation for why phrase-level
supervision helps learn better fine-grained entailment compared to
passage-level supervision, and also show that phrase retrieval can be improved
to achieve competitive performance in document-retrieval tasks such as entity
linking and knowledge-grounded dialogue. Finally, we demonstrate how phrase
filtering and vector quantization can reduce the size of our index by 4-10x,
making dense phrase retrieval a practical and versatile solution in
multi-granularity retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resources for Turkish Dependency Parsing: Introducing the BOUN Treebank and the BoAT Annotation Tool. (arXiv:2002.10416v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.10416">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the resources that we developed for Turkish
dependency parsing, which include a novel manually annotated treebank (BOUN
Treebank), along with the guidelines we adopted, and a new annotation tool
(BoAT). The manual annotation process we employed was shaped and implemented by
a team of four linguists and five Natural Language Processing (NLP)
specialists. Decisions regarding the annotation of the BOUN Treebank were made
in line with the Universal Dependencies (UD) framework as well as our recent
efforts for unifying the Turkish UD treebanks through manual re-annotation. To
the best of our knowledge, BOUN Treebank is the largest Turkish treebank. It
contains a total of 9,761 sentences from various topics including biographical
texts, national newspapers, instructional texts, popular culture articles, and
essays. In addition, we report the parsing results of a state-of-the-art
dependency parser obtained over the BOUN Treebank as well as two other
treebanks in Turkish. Our results demonstrate that the unification of the
Turkish annotation scheme and the introduction of a more comprehensive treebank
lead to improved performance with regard to dependency parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regex Queries over Incomplete Knowledge Bases. (arXiv:2005.00480v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00480">
<div class="article-summary-box-inner">
<span><p>We propose the novel task of answering regular expression queries (containing
disjunction ($\vee$) and Kleene plus ($+$) operators) over incomplete KBs. The
answer set of these queries potentially has a large number of entities, hence
previous works for single-hop queries in KBC that model a query as a point in
high-dimensional space are not as effective. In response, we develop RotatE-Box
-- a novel combination of RotatE and box embeddings. It can model more
relational inference patterns compared to existing embedding based models.
Furthermore, we define baseline approaches for embedding based KBC models to
handle regex operators. We demonstrate performance of RotatE-Box on two new
regex-query datasets introduced in this paper, including one where the queries
are harvested based on actual user query logs. We find that our final
RotatE-Box model significantly outperforms models based on just RotatE and just
box embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Learning for End-to-End Automatic Speech Recognition. (arXiv:2005.04288v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04288">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an incremental learning method for end-to-end
Automatic Speech Recognition (ASR) which enables an ASR system to perform well
on new tasks while maintaining the performance on its originally learned ones.
To mitigate catastrophic forgetting during incremental learning, we design a
novel explainability-based knowledge distillation for ASR models, which is
combined with a response-based knowledge distillation to maintain the original
model's predictions and the "reason" for the predictions. Our method works
without access to the training data of original tasks, which addresses the
cases where the previous data is no longer available or joint training is
costly. Results on a multi-stage sequential training task show that our method
outperforms existing ones in mitigating forgetting. Furthermore, in two
practical scenarios, compared to the target-reference joint training method,
the performance drop of our method is 0.02% Character Error Rate (CER), which
is 97% smaller than the drops of the baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04480">
<div class="article-summary-box-inner">
<span><p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality
Estimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven
language pairs, with human labels for up to 10,000 translations per language
pair in the following formats: sentence-level direct assessments and
post-editing effort, and word-level good/bad labels. It also contains the
post-edited sentences, as well as titles of the articles where the sentences
were extracted from, and the neural MT models used to translate the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Zero-shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders. (arXiv:2011.01097v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01097">
<div class="article-summary-box-inner">
<span><p>Current end-to-end approaches to Spoken Language Translation (SLT) rely on
limited training resources, especially for multilingual settings. On the other
hand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on
higher-quality and more massive data sets. Our proposed method extends a
MultiNMT architecture based on language-specific encoders-decoders to the task
of Multilingual SLT (MultiSLT). Our method entirely eliminates the dependency
from MultiSLT data and it is able to translate while training only on ASR and
MultiNMT data.
</p>
<p>Our experiments on four different languages show that coupling the speech
encoder to the MultiNMT architecture produces similar quality translations
compared to a bilingual baseline ($\pm 0.2$ BLEU) while effectively allowing
for zero-shot MultiSLT. Additionally, we propose using an Adapter module for
coupling the speech inputs. This Adapter module produces consistent
improvements up to +6 BLEU points on the proposed architecture and +1 BLEU
point on the end-to-end baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Wish I Would Have Loved This One, But I Didn't -- A Multilingual Dataset for Counterfactual Detection in Product Reviews. (arXiv:2104.06893v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06893">
<div class="article-summary-box-inner">
<span><p>Counterfactual statements describe events that did not or cannot take place.
We consider the problem of counterfactual detection (CFD) in product reviews.
For this purpose, we annotate a multilingual CFD dataset from Amazon product
reviews covering counterfactual statements written in English, German, and
Japanese languages. The dataset is unique as it contains counterfactuals in
multiple languages, covers a new application area of e-commerce reviews, and
provides high quality professional annotations. We train CFD models using
different text representation methods and classifiers. We find that these
models are robust against the selectional biases introduced due to cue
phrase-based sentence selection. Moreover, our CFD dataset is compatible with
prior datasets and can be merged to learn accurate CFD models. Applying machine
translation on English counterfactual examples to create multilingual data
performs poorly, demonstrating the language-specificity of this problem, which
has been ignored so far.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TWEAC: Transformer with Extendable QA Agent Classifiers. (arXiv:2104.07081v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07081">
<div class="article-summary-box-inner">
<span><p>Question answering systems should help users to access knowledge on a broad
range of topics and to answer a wide array of different questions. Most systems
fall short of this expectation as they are only specialized in one particular
setting, e.g., answering factual questions with Wikipedia data. To overcome
this limitation, we propose composing multiple QA agents within a meta-QA
system. We argue that there exist a wide range of specialized QA agents in
literature. Thus, we address the central research question of how to
effectively and efficiently identify suitable QA agents for any given question.
We study both supervised and unsupervised approaches to address this challenge,
showing that TWEAC -- Transformer with Extendable Agent Classifiers -- achieves
the best performance overall with 94% accuracy. We provide extensive insights
on the scalability of TWEAC, demonstrating that it scales robustly to over 100
QA agents with each providing just 1000 examples of questions they can answer.
Our code and data is available:
https://github.com/UKPLab/TWEAC-qa-agent-selection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. (arXiv:2104.08678v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08678">
<div class="article-summary-box-inner">
<span><p>Despite recent progress, state-of-the-art question answering models remain
vulnerable to a variety of adversarial attacks. While dynamic adversarial data
collection, in which a human annotator tries to write examples that fool a
model-in-the-loop, can improve model robustness, this process is expensive
which limits the scale of the collected data. In this work, we are the first to
use synthetic adversarial data generation to make question answering models
more robust to human adversaries. We develop a data generation pipeline that
selects source passages, identifies candidate answers, generates questions,
then finally filters or re-labels them to improve quality. Using this approach,
we amplify a smaller human-written adversarial dataset to a much larger set of
synthetic question-answer pairs. By incorporating our synthetic data, we
improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve
model generalisation on nine of the twelve MRQA datasets. We further conduct a
novel human-in-the-loop evaluation to show that our models are considerably
more robust to new human-written adversarial examples: crowdworkers can fool
our model only 8.8% of the time on average, compared to 17.6% for a model
trained without synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete representations in neural models of spoken language. (arXiv:2105.05582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05582">
<div class="article-summary-box-inner">
<span><p>The distributed and continuous representations used by neural networks are at
odds with representations employed in linguistics, which are typically
symbolic. Vector quantization has been proposed as a way to induce discrete
neural representations that are closer in nature to their linguistic
counterparts. However, it is not clear which metrics are the best-suited to
analyze such discrete representations. We compare the merits of four commonly
used metrics in the context of weakly supervised models of spoken language. We
compare the results they show when applied to two different models, while
systematically studying the effect of the placement and size of the
discretization layer. We find that different evaluation regimes can give
inconsistent results. While we can attribute them to the properties of the
different metrics in most cases, one point of concern remains: the use of
minimal pairs of phoneme triples as stimuli disadvantages larger discrete unit
inventories, unlike metrics applied to complete utterances. Furthermore, while
in general vector quantization induces representations that correlate with
units posited in linguistics, the strength of this correlation is only
moderate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divided We Rule: Influencer Polarization on Twitter During Political Crises in India. (arXiv:2105.08361v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08361">
<div class="article-summary-box-inner">
<span><p>Influencers are key to the nature and networks of information propagation on
social media. Influencers are particularly important in political discourse
through their engagement with issues, and may derive their legitimacy either
solely or in large part through online operation, or have an offline sphere of
expertise such as entertainers, journalists etc. To quantify influencers'
political engagement and polarity, we use Google's Universal Sentence Encoder
(USE) to encode the tweets of 6k influencers and 26k Indian politicians during
political crises in India. We then obtain aggregate vector representations of
the influencers based on their tweet embeddings, which alongside retweet graphs
help compute their stance and polarity with respect to these political issues.
We find that influencers engage with the topics in a partisan manner, with
polarized influencers being rewarded with increased retweeting and following.
Moreover, we observe that specific groups of influencers are consistently
polarized across all events. We conclude by discussing how our study provides
insights into the political schisms of present-day India, but also offers a
means to study the role of influencers in exacerbating political polarization
in other contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Directed Acyclic Graph Network for Conversational Emotion Recognition. (arXiv:2105.12907v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12907">
<div class="article-summary-box-inner">
<span><p>The modeling of conversational context plays a vital role in emotion
recognition from conversation (ERC). In this paper, we put forward a novel idea
of encoding the utterances with a directed acyclic graph (DAG) to better model
the intrinsic structure within a conversation, and design a directed acyclic
neural network, namely DAG-ERC, to implement this idea. In an attempt to
combine the strengths of conventional graph-based neural models and
recurrence-based neural models, DAG-ERC provides a more intuitive way to model
the information flow between long-distance conversation background and nearby
context. Extensive experiments are conducted on four ERC benchmarks with
state-of-the-art models employed as baselines for comparison. The empirical
results demonstrate the superiority of this new model and confirm the
motivation of the directed acyclic graph architecture for ERC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05707">
<div class="article-summary-box-inner">
<span><p>Fact verification has attracted a lot of attention in the machine learning
and natural language processing communities, as it is one of the key methods
for detecting misinformation. Existing large-scale benchmarks for this task
have focused mostly on textual sources, i.e. unstructured information, and thus
ignored the wealth of information available in structured formats, such as
tables. In this paper we introduce a novel dataset and benchmark, Fact
Extraction and VERification Over Unstructured and Structured information
(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated
with evidence in the form of sentences and/or cells from tables in Wikipedia,
as well as a label indicating whether this evidence supports, refutes, or does
not provide enough information to reach a verdict. Furthermore, we detail our
efforts to track and minimize the biases present in the dataset and could be
exploited by models, e.g. being able to predict the label without using
evidence. Finally, we develop a baseline for verifying claims against text and
tables which predicts both the correct evidence and verdict for 18% of the
claims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06922">
<div class="article-summary-box-inner">
<span><p>How to effectively incorporate cross-utterance information cues into a neural
language model (LM) has emerged as one of the intriguing issues for automatic
speech recognition (ASR). Existing research efforts on improving
contextualization of an LM typically regard previous utterances as a sequence
of additional input and may fail to capture complex global structural
dependencies among these utterances. In view of this, we in this paper seek to
represent the historical context information of an utterance as
graph-structured data so as to distill cross-utterances, global word
interaction relationships. To this end, we apply a graph convolutional network
(GCN) on the resulting graph to obtain the corresponding GCN embeddings of
historical words. GCN has recently found its versatile applications on
social-network analysis, text summarization, and among others due mainly to its
ability of effectively capturing rich relational information among elements.
However, GCN remains largely underexplored in the context of ASR, especially
for dealing with conversational speech. In addition, we frame ASR N-best
reranking as a prediction problem, leveraging bidirectional encoder
representations from transformers (BERT) as the vehicle to not only seize the
local intrinsic word regularity patterns inherent in a candidate hypothesis but
also incorporate the cross-utterance, historical word interaction cues
distilled by GCN for promoting performance. Extensive experiments conducted on
the AMI benchmark dataset seem to confirm the pragmatic utility of our methods,
in relation to some current top-of-the-line methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07340">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the original BERT (i.e., BASE BERT), researchers
have developed various customized BERT models with improved performance for
specific domains and tasks by exploiting the benefits of transfer learning. Due
to the nature of mathematical texts, which often use domain specific vocabulary
along with equations and math symbols, we posit that the development of a new
BERT model for mathematics would be useful for many mathematical downstream
tasks. In this resource paper, we introduce our multi-institutional effort
(i.e., two learning platforms and three academic institutions in the US) toward
this need: MathBERT, a model created by pre-training the BASE BERT model on a
large mathematical corpus ranging from pre-kindergarten (pre-k), to
high-school, to college graduate level mathematical content. In addition, we
select three general NLP tasks that are often used in mathematics education:
prediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge
tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our
experiments show that MathBERT outperforms prior best methods by 1.2-22% and
BASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific
vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT
pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT
vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the
participated leaning platforms: Stride, Inc, a commercial educational resource
provider, and ASSISTments.org, a free online educational platform. We release
MathBERT for public usage at: https://github.com/tbs17/MathBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01366">
<div class="article-summary-box-inner">
<span><p>Despite their practical success, modern seq2seq architectures are unable to
generalize systematically on several SCAN tasks. Hence, it is not clear if
SCAN-style compositional generalization is useful in realistic NLP tasks. In
this work, we study the benefit that such compositionality brings about to
several machine translation tasks. We present several focused modifications of
Transformer that greatly improve generalization capabilities on SCAN and select
one that remains on par with a vanilla Transformer on a standard machine
translation (MT) task. Next, we study its performance in low-resource settings
and on a newly introduced distribution-shifted English-French translation task.
Overall, we find that improvements of a SCAN-capable model do not directly
transfer to the resource-rich MT setup. In contrast, in the low-resource setup,
general modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a
vanilla Transformer. Similarly, an improvement of 14% in an accuracy-based
metric is achieved in the introduced compositional English-French translation
task. This provides experimental evidence that the compositional generalization
assessed in SCAN is particularly useful in resource-starved and domain-shifted
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07610">
<div class="article-summary-box-inner">
<span><p>This paper improves the robustness of the pretrained language model, BERT,
against word substitution-based adversarial attacks by leveraging
self-supervised contrastive learning with adversarial perturbations. One
advantage of our method compared to previous works is that it is capable of
improving model robustness without using any labels. Additionally, we also
create an adversarial attack for word-level adversarial training on BERT. The
attack is efficient, allowing adversarial training for BERT on adversarial
examples generated \textit{on the fly} during training. Experimental results
show that our method improves the robustness of BERT against four different
word substitution-based adversarial attacks. Additionally, combining our method
with adversarial training gives higher robustness than adversarial training
alone. Furthermore, to understand why our method can improve the model
robustness against adversarial attacks, we study vector representations of
clean examples and their corresponding adversarial examples before and after
applying our method. As our method improves model robustness with unlabeled raw
data, it opens up the possibility of using large text datasets to train robust
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00412">
<div class="article-summary-box-inner">
<span><p>In multimodal sentiment analysis (MSA), the performance of a model highly
depends on the quality of synthesized embeddings. These embeddings are
generated from the upstream process called multimodal fusion, which aims to
extract and combine the input unimodal raw data to produce a richer multimodal
representation. Previous work either back-propagates the task loss or
manipulates the geometric property of feature spaces to produce favorable
fusion results, which neglects the preservation of critical task-related
information that flows from input to the fusion results. In this work, we
propose a framework named MultiModal InfoMax (MMIM), which hierarchically
maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)
and between multimodal fusion result and unimodal input in order to maintain
task-related information through multimodal fusion. The framework is jointly
trained with the main task (MSA) to improve the performance of the downstream
MSA task. To address the intractable issue of MI bounds, we further formulate a
set of computationally simple parametric and non-parametric methods to
approximate their truth value. Experimental results on the two widely used
datasets demonstrate the efficacy of our approach. The implementation of this
work is publicly available at
https://github.com/declare-lab/Multimodal-Infomax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Establishing Interlingua in Multilingual Language Models. (arXiv:2109.01207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01207">
<div class="article-summary-box-inner">
<span><p>Large multilingual language models show remarkable zero-shot cross-lingual
transfer performance on a range of tasks. Follow-up works hypothesized that
these models internally project representations of different languages into a
shared interlingual space. However, they produced contradictory results. In
this paper, we correct the famous prior work claiming that "BERT is not an
Interlingua" and show that with the proper choice of sentence representation
different languages actually do converge to a shared space in such language
models. Furthermore, we demonstrate that this convergence pattern is robust
across four measures of correlation similarity and six mBERT-like models. We
then extend our analysis to 28 diverse languages and find that the interlingual
space exhibits a particular structure similar to the linguistic relatedness of
languages. We also highlight a few outlier languages that seem to fail to
converge to the shared space. The code for replicating our results is available
at the following URL: https://github.com/maksym-del/interlingua.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03772">
<div class="article-summary-box-inner">
<span><p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous
challenge since it involves multiple speakers at one dialogue, resulting in
intricate speaker information flows and noisy dialogue contexts. To alleviate
such difficulties, previous models focus on how to incorporate these
information using complex graph-based modules and additional manually labeled
data, which is usually rare in real scenarios. In this paper, we design two
labour-free self- and pseudo-self-supervised prediction tasks on speaker and
key-utterance to implicitly model the speaker information flows, and capture
salient clues in a long dialogue. Experimental results on two benchmark
datasets have justified the effectiveness of our method over competitive
baselines and current state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03910">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage large language models (LMs) to perform zero-shot
text style transfer. We present a prompting method that we call augmented
zero-shot learning, which frames style transfer as a sentence rewriting task
and requires only a natural language instruction, without model fine-tuning or
exemplars in the target style. Augmented zero-shot learning is simple and
demonstrates promising results not just on standard style transfer tasks such
as sentiment, but also on arbitrary transformations such as "make this
melodramatic" or "insert a metaphor."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05494">
<div class="article-summary-box-inner">
<span><p>Building an automatic speech recognition (ASR) system from scratch requires a
large amount of annotated speech data, which is difficult to collect in many
languages. However, there are cases where the low-resource language shares a
common acoustic space with a high-resource language having enough annotated
data to build an ASR. In such cases, we show that the domain-independent
acoustic models learned from the high-resource language through unsupervised
domain adaptation (UDA) schemes can enhance the performance of the ASR in the
low-resource language. We use the specific example of Hindi in the source
domain and Sanskrit in the target domain. We explore two architectures: i)
domain adversarial training using gradient reversal layer (GRL) and ii) domain
separation networks (DSN). The GRL and DSN architectures give absolute
improvements of 6.71% and 7.32%, respectively, in word error rate over the
baseline deep neural network model when trained on just 5.5 hours of data in
the target domain. We also show that choosing a proper language (Telugu) in the
source domain can bring further improvement. The results suggest that UDA
schemes can be helpful in the development of ASR systems for low-resource
languages, mitigating the hassle of collecting large amounts of annotated
speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05611">
<div class="article-summary-box-inner">
<span><p>We propose a novel scheme to use the Levenshtein Transformer to perform the
task of word-level quality estimation. A Levenshtein Transformer is a natural
fit for this task: trained to perform decoding in an iterative manner, a
Levenshtein Transformer can learn to post-edit without explicit supervision. To
further minimize the mismatch between the translation task and the word-level
QE task, we propose a two-stage transfer learning procedure on both augmented
data and human post-editing data. We also propose heuristics to construct
reference labels that are compatible with subword-level finetuning and
inference. Results on WMT 2020 QE shared task dataset show that our proposed
method has superior data efficiency under the data-constrained setting and
competitive performance under the unconstrained setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talking Space: inference from spatial linguistic meanings. (arXiv:2109.06554v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06554">
<div class="article-summary-box-inner">
<span><p>This paper concerns the intersection of natural language and the physical
space around us in which we live, that we observe and/or imagine things within.
Many important features of language have spatial connotations, for example,
many prepositions (like in, next to, after, on, etc.) are fundamentally
spatial. Space is also a key factor of the meanings of many
words/phrases/sentences/text, and space is a, if not the key, context for
referencing (e.g. pointing) and embodiment.
</p>
<p>We propose a mechanism for how space and linguistic structure can be made to
interact in a matching compositional fashion. Examples include Cartesian space,
subway stations, chesspieces on a chess-board, and Penrose's staircase. The
starting point for our construction is the DisCoCat model of compositional
natural language meaning, which we relax to accommodate physical space. We
address the issue of having multiple agents/objects in a space, including the
case that each agent has different capabilities with respect to that space,
e.g., the specific moves each chesspiece can make, or the different velocities
one may be able to reach.
</p>
<p>Once our model is in place, we show how inferences drawing from the structure
of physical space can be made. We also how how linguistic model of space can
interact with other such models related to our senses and/or embodiment, such
as the conceptual spaces of colour, taste and smell, resulting in a rich
compositional model of meaning that is close to human experience and embodiment
in the world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation. (arXiv:2109.07222v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07222">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have shown remarkable results on various NLP
tasks. Nevertheless, due to their bulky size and slow inference speed, it is
hard to deploy them on edge devices. In this paper, we have a critical insight
that improving the feed-forward network (FFN) in BERT has a higher gain than
improving the multi-head attention (MHA) since the computational cost of FFN is
2$\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to
designing efficient FFN as opposed to previous works that pay attention to MHA.
Since FFN comprises a multilayer perceptron (MLP) that is essential in BERT
optimization, we further design a thorough search space towards an advanced MLP
and perform a coarse-to-fine mechanism to search for an efficient BERT
architecture. Moreover, to accelerate searching and enhance model
transferability, we employ a novel warm-up knowledge distillation strategy at
each search stage. Extensive experiments show our searched EfficientBERT is
6.9$\times$ smaller and 4.4$\times$ faster than BERT$\rm_{BASE}$, and has
competitive performances on GLUE and SQuAD Benchmarks. Concretely,
EfficientBERT attains a 77.7 average score on GLUE \emph{test}, 0.7 higher than
MobileBERT$\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0
\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.
The code is released at https://github.com/cheneydon/efficient-bert.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop. (arXiv:1907.11184v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.11184">
<div class="article-summary-box-inner">
<span><p>While the role of humans is increasingly recognized in machine learning
community, representation of and interaction with models in current
human-in-the-loop machine learning (HITL-ML) approaches are too low-level and
far-removed from human's conceptual models. We demonstrate HEIDL, a prototype
HITL-ML system that exposes the machine-learned model through high-level,
explainable linguistic expressions formed of predicates representing semantic
structure of text. In HEIDL, human's role is elevated from simply evaluating
model predictions to interpreting and even updating the model logic directly by
enabling interaction with rule predicates themselves. Raising the currency of
interaction to such semantic levels calls for new interaction paradigms between
humans and machines that result in improved productivity for text analytics
model development process. Moreover, by involving humans in the process, the
human-machine co-created models generalize better to unseen data as domain
experts are able to instill their expertise by extrapolating from what has been
learned by automated algorithms from few labelled data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Contrastive Learning for Decentralized Unlabeled Medical Images. (arXiv:2109.07504v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07504">
<div class="article-summary-box-inner">
<span><p>A label-efficient paradigm in computer vision is based on self-supervised
contrastive pre-training on unlabeled data followed by fine-tuning with a small
number of labels. Making practical use of a federated computing environment in
the clinical domain and learning on medical images poses specific challenges.
In this work, we propose FedMoCo, a robust federated contrastive learning (FCL)
framework, which makes efficient use of decentralized unlabeled medical data.
FedMoCo has two novel modules: metadata transfer, an inter-node statistical
data augmentation module, and self-adaptive aggregation, an aggregation module
based on representational similarity analysis. To the best of our knowledge,
this is the first FCL work on medical images. Our experiments show that FedMoCo
can consistently outperform FedAvg, a seminal federated learning framework, in
extracting meaningful representations for downstream tasks. We further show
that FedMoCo can substantially reduce the amount of labeled data required in a
downstream task, such as COVID-19 detection, to achieve a reasonable
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Aggregate and Refine Noisy Labels for Visual Sentiment Analysis. (arXiv:2109.07509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07509">
<div class="article-summary-box-inner">
<span><p>Visual sentiment analysis has received increasing attention in recent years.
However, the quality of the dataset is a concern because the sentiment labels
are crowd-sourcing, subjective, and prone to mistakes. This poses a severe
threat to the data-driven models including the deep neural networks which would
generalize poorly on the testing cases if they are trained to over-fit the
samples with noisy sentiment labels. Inspired by the recent progress on
learning with noisy labels, we propose a robust learning method to perform
robust visual sentiment analysis. Our method relies on an external memory to
aggregate and filter noisy labels during training and thus can prevent the
model from overfitting the noisy cases. The memory is composed of the
prototypes with corresponding labels, both of which can be updated online. We
establish a benchmark for visual sentiment analysis with label noise using
publicly available datasets. The experiment results of the proposed benchmark
settings comprehensively show the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose Transformers (POTR): Human Motion Prediction with Non-Autoregressive Transformers. (arXiv:2109.07531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07531">
<div class="article-summary-box-inner">
<span><p>We propose to leverage Transformer architectures for non-autoregressive human
motion prediction. Our approach decodes elements in parallel from a query
sequence, instead of conditioning on previous predictions such as
instate-of-the-art RNN-based approaches. In such a way our approach is less
computational intensive and potentially avoids error accumulation to long term
elements in the sequence. In that context, our contributions are fourfold: (i)
we frame human motion prediction as a sequence-to-sequence problem and propose
a non-autoregressive Transformer to infer the sequences of poses in parallel;
(ii) we propose to decode sequences of 3D poses from a query sequence generated
in advance with elements from the input sequence;(iii) we propose to perform
skeleton-based activity classification from the encoder memory, in the hope
that identifying the activity can improve predictions;(iv) we show that despite
its simplicity, our approach achieves competitive results in two public
datasets, although surprisingly more for short term predictions rather than for
long term ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching. (arXiv:2109.07547v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07547">
<div class="article-summary-box-inner">
<span><p>We introduce RAFT-Stereo, a new deep architecture for rectified stereo based
on the optical flow network RAFT. We introduce multi-level convolutional GRUs,
which more efficiently propagate information across the image. A modified
version of RAFT-Stereo can perform accurate real-time inference. RAFT-stereo
ranks first on the Middlebury leaderboard, outperforming the next best method
on 1px error by 29% and outperforms all published work on the ETH3D two-view
stereo benchmark. Code is available at
https://github.com/princeton-vl/RAFT-Stereo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Regularization in DCE-MR Image Reconstruction for Functional Imaging of Kidneys. (arXiv:2109.07548v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07548">
<div class="article-summary-box-inner">
<span><p>Kidney DCE-MRI aims at both qualitative assessment of kidney anatomy and
quantitative assessment of kidney function by estimating the tracer kinetic
(TK) model parameters. Accurate estimation of TK model parameters requires an
accurate measurement of the arterial input function (AIF) with high temporal
resolution. Accelerated imaging is used to achieve high temporal resolution,
which yields under-sampling artifacts in the reconstructed images. Compressed
sensing (CS) methods offer a variety of reconstruction options. Most commonly,
sparsity of temporal differences is encouraged for regularization to reduce
artifacts. Increasing regularization in CS methods removes the ambient
artifacts but also over-smooths the signal temporally which reduces the
parameter estimation accuracy. In this work, we propose a single image trained
deep neural network to reduce MRI under-sampling artifacts without reducing the
accuracy of functional imaging markers. Instead of regularizing with a penalty
term in optimization, we promote regularization by generating images from a
lower dimensional representation. In this manuscript we motivate and explain
the lower dimensional input design. We compare our approach to CS
reconstructions with multiple regularization weights. Proposed approach results
in kidney biomarkers that are highly correlated with the ground truth markers
estimated using the CS reconstruction which was optimized for functional
analysis. At the same time, the proposed approach reduces the artifacts in the
reconstructed images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pathology Deep Learning System Capable of Triage of Melanoma Specimens Utilizing Dermatopathologist Consensus as Ground Truth. (arXiv:2109.07554v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07554">
<div class="article-summary-box-inner">
<span><p>Although melanoma occurs more rarely than several other skin cancers,
patients' long term survival rate is extremely low if the diagnosis is missed.
Diagnosis is complicated by a high discordance rate among pathologists when
distinguishing between melanoma and benign melanocytic lesions. A tool that
allows pathology labs to sort and prioritize melanoma cases in their workflow
could improve turnaround time by prioritizing challenging cases and routing
them directly to the appropriate subspecialist. We present a pathology deep
learning system (PDLS) that performs hierarchical classification of digitized
whole slide image (WSI) specimens into six classes defined by their
morphological characteristics, including classification of "Melanocytic
Suspect" specimens likely representing melanoma or severe dysplastic nevi. We
trained the system on 7,685 images from a single lab (the reference lab),
including the the largest set of triple-concordant melanocytic specimens
compiled to date, and tested the system on 5,099 images from two distinct
validation labs. We achieved Area Underneath the ROC Curve (AUC) values of 0.93
classifying Melanocytic Suspect specimens on the reference lab, 0.95 on the
first validation lab, and 0.82 on the second validation lab. We demonstrate
that the PDLS is capable of automatically sorting and triaging skin specimens
with high sensitivity to Melanocytic Suspect cases and that a pathologist would
only need between 30% and 60% of the caseload to address all melanoma
specimens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid ICP. (arXiv:2109.07559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07559">
<div class="article-summary-box-inner">
<span><p>ICP algorithms typically involve a fixed choice of data association method
and a fixed choice of error metric. In this paper, we propose Hybrid ICP, a
novel and flexible ICP variant which dynamically optimises both the data
association method and error metric based on the live image of an object and
the current ICP estimate. We show that when used for object pose estimation,
Hybrid ICP is more accurate and more robust to noise than other commonly used
ICP variants. We also consider the setting where ICP is applied sequentially
with a moving camera, and we study the trade-off between the accuracy of each
ICP estimate and the number of ICP estimates available within a fixed amount of
time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Multisensory Foresight for Embodied Agents. (arXiv:2109.07561v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07561">
<div class="article-summary-box-inner">
<span><p>Predicting future sensory states is crucial for learning agents such as
robots, drones, and autonomous vehicles. In this paper, we couple multiple
sensory modalities with exploratory actions and propose a predictive neural
network architecture to address this problem. Most existing approaches rely on
large, manually annotated datasets, or only use visual data as a single
modality. In contrast, the unsupervised method presented here uses multi-modal
perceptions for predicting future visual frames. As a result, the proposed
model is more comprehensive and can better capture the spatio-temporal dynamics
of the environment, leading to more accurate visual frame prediction. The other
novelty of our framework is the use of sub-networks dedicated to anticipating
future haptic, audio, and tactile signals. The framework was tested and
validated with a dataset containing 4 sensory modalities (vision, haptic,
audio, and tactile) on a humanoid robot performing 9 behaviors multiple times
on a large set of objects. While the visual information is the dominant
modality, utilizing the additional non-visual modalities improves the accuracy
of predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting 3D shapes, masks, and properties of materials, liquids, and objects inside transparent containers, using the TransProteus CGI dataset. (arXiv:2109.07577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07577">
<div class="article-summary-box-inner">
<span><p>We present TransProteus, a dataset, and methods for predicting the 3D
structure, masks, and properties of materials, liquids, and objects inside
transparent vessels from a single image without prior knowledge of the image
source and camera parameters. Manipulating materials in transparent containers
is essential in many fields and depends heavily on vision. This work supplies a
new procedurally generated dataset consisting of 50k images of liquids and
solid objects inside transparent containers. The image annotations include 3D
models, material properties (color/transparency/roughness...), and segmentation
masks for the vessel and its content. The synthetic (CGI) part of the dataset
was procedurally generated using 13k different objects, 500 different
environments (HDRI), and 1450 material textures (PBR) combined with simulated
liquids and procedurally generated vessels. In addition, we supply 104
real-world images of objects inside transparent vessels with depth maps of both
the vessel and its content. We propose a camera agnostic method that predicts
3D models from an image as an XYZ map. This allows the trained net to predict
the 3D model as a map with XYZ coordinates per pixel without prior knowledge of
the image source. To calculate the training loss, we use the distance between
pairs of points inside the 3D model instead of the absolute XYZ coordinates.
This makes the loss function translation invariant. We use this to predict 3D
models of vessels and their content from a single image. Finally, we
demonstrate a net that uses a single image to predict the material properties
of the vessel content and surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UCP-Net: Unstructured Contour Points for Instance Segmentation. (arXiv:2109.07592v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07592">
<div class="article-summary-box-inner">
<span><p>The goal of interactive segmentation is to assist users in producing
segmentation masks as fast and as accurately as possible. Interactions have to
be simple and intuitive and the number of interactions required to produce a
satisfactory segmentation mask should be as low as possible. In this paper, we
propose a novel approach to interactive segmentation based on unconstrained
contour clicks for initial segmentation and segmentation refinement. Our method
is class-agnostic and produces accurate segmentation masks (IoU &gt; 85%) for a
lower number of user interactions than state-of-the-art methods on popular
segmentation datasets (COCO MVal, SBD and Berkeley).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partner-Assisted Learning for Few-Shot Image Classification. (arXiv:2109.07607v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07607">
<div class="article-summary-box-inner">
<span><p>Few-shot Learning has been studied to mimic human visual capabilities and
learn effective models without the need of exhaustive human annotation. Even
though the idea of meta-learning for adaptation has dominated the few-shot
learning methods, how to train a feature extractor is still a challenge. In
this paper, we focus on the design of training strategy to obtain an elemental
representation such that the prototype of each novel class can be estimated
from a few labeled samples. We propose a two-stage training scheme,
Partner-Assisted Learning (PAL), which first trains a partner encoder to model
pair-wise similarities and extract features serving as soft-anchors, and then
trains a main encoder by aligning its outputs with soft-anchors while
attempting to maximize classification performance. Two alignment constraints
from logit-level and feature-level are designed individually. For each few-shot
task, we perform prototype classification. Our method consistently outperforms
the state-of-the-art method on four benchmarks. Detailed ablation studies of
PAL are provided to justify the selection of each component involved in
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication. (arXiv:2109.07644v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07644">
<div class="article-summary-box-inner">
<span><p>Employing Vehicle-to-Vehicle communication to enhance perception performance
in self-driving technology has attracted considerable attention recently;
however, the absence of a suitable open dataset for benchmarking algorithms has
made it difficult to develop and assess cooperative perception technologies. To
this end, we present the first large-scale open simulated dataset for
Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 111,464
frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns
in CARLA and a digital town of Culver City, Los Angeles. We then construct a
comprehensive benchmark with a total of 16 implemented models to evaluate
several information fusion strategies~(i.e. early, late, and intermediate
fusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose
a new Attentive Intermediate Fusion pipeline to aggregate information from
multiple connected vehicles. Our experiments show that the proposed pipeline
can be easily integrated with existing 3D LiDAR detectors and achieve
outstanding performance even with large compression rates. To encourage more
researchers to investigate Vehicle-to-Vehicle perception, we will release the
dataset, benchmark methods, and all related codes in
https://mobility-lab.seas.ucla.edu/opv2v/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">METEOR: A Massive Dense & Heterogeneous Behavior Dataset for Autonomous Driving. (arXiv:2109.07648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07648">
<div class="article-summary-box-inner">
<span><p>We present a new and complex traffic dataset, METEOR, which captures traffic
patterns in unstructured scenarios in India. METEOR consists of more than 1000
one-minute video clips, over 2 million annotated frames with ego-vehicle
trajectories, and more than 13 million bounding boxes for surrounding vehicles
or traffic agents. METEOR is a unique dataset in terms of capturing the
heterogeneity of microscopic and macroscopic traffic characteristics.
Furthermore, we provide annotations for rare and interesting driving behaviors
such as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane
changing, running traffic signals, driving in the wrong lanes, taking wrong
turns, lack of right-of-way rules at intersections, etc. We also present
diverse traffic scenarios corresponding to rainy weather, nighttime driving,
driving in rural areas with unmarked roads, and high-density traffic scenarios.
We use our novel dataset to evaluate the performance of object detection and
behavior prediction algorithms. We show that state-of-the-art object detectors
fail in these challenging conditions and also propose a new benchmark test:
action-behavior prediction with a baseline mAP score of 70.74.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Fusion Network for RGBT Tracking. (arXiv:2109.07662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07662">
<div class="article-summary-box-inner">
<span><p>For both visible and infrared images have their own advantages and
disadvantages, RGBT tracking has attracted more and more attention. The key
points of RGBT tracking lie in feature extraction and feature fusion of visible
and infrared images. Current RGBT tracking methods mostly pay attention to both
individual features (features extracted from images of a single camera) and
common features (features extracted and fused from an RGB camera and a thermal
camera), while pay less attention to the different and dynamic contributions of
individual features and common features for different sequences of registered
image pairs. This paper proposes a novel RGBT tracking method, called Dynamic
Fusion Network (DFNet), which adopts a two-stream structure, in which two
non-shared convolution kernels are employed in each layer to extract individual
features. Besides, DFNet has shared convolution kernels for each layer to
extract common features. Non-shared convolution kernels and shared convolution
kernels are adaptively weighted and summed according to different image pairs,
so that DFNet can deal with different contributions for different sequences.
DFNet has a fast speed, which is 28.658 FPS. The experimental results show that
when DFNet only increases the Mult-Adds of 0.02% than the
non-shared-convolution-kernel-based fusion method, Precision Rate (PR) and
Success Rate (SR) reach 88.1% and 71.9% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIN Road Mapper: Extracting Roads from Aerial Images via Spatial and Interaction Space Graph Reasoning for Autonomous Driving. (arXiv:2109.07701v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07701">
<div class="article-summary-box-inner">
<span><p>Road extraction is an essential step in building autonomous navigation
systems. Detecting road segments is challenging as they are of varying widths,
bifurcated throughout the image, and are often occluded by terrain, cloud, or
other weather conditions. Using just convolution neural networks (ConvNets) for
this problem is not effective as it is inefficient at capturing distant
dependencies between road segments in the image which is essential to extract
road connectivity. To this end, we propose a Spatial and Interaction Space
Graph Reasoning (SPIN) module which when plugged into a ConvNet performs
reasoning over graphs constructed on spatial and interaction spaces projected
from the feature maps. Reasoning over spatial space extracts dependencies
between different spatial regions and other contextual information. Reasoning
over a projected interaction space helps in appropriate delineation of roads
from other topographies present in the image. Thus, SPIN extracts long-range
dependencies between road segments and effectively delineates roads from other
semantics. We also introduce a SPIN pyramid which performs SPIN graph reasoning
across multiple scales to extract multi-scale features. We propose a network
based on stacked hourglass modules and SPIN pyramid for road segmentation which
achieves better performance compared to existing methods. Moreover, our method
is computationally efficient and significantly boosts the convergence speed
during training, making it feasible for applying on large-scale high-resolution
aerial images. Code available at:
https://github.com/wgcban/SPIN_RoadMapper.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation. (arXiv:2109.07702v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07702">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation has significantly benefitted thanks to deep
learning architectures. Furthermore, semi-supervised learning (SSL) has
recently been a growing trend for improving a model's overall performance by
leveraging abundant unlabeled data. Moreover, learning multiple tasks within
the same model further improves model generalizability. To generate smoother
and accurate segmentation masks from 3D cardiac MR images, we present a
Multi-task Cross-task learning consistency approach to enforce the correlation
between the pixel-level (segmentation) and the geometric-level (distance map)
tasks. Our extensive experimentation with varied quantities of labeled data in
the training sets justifies the effectiveness of our model for the segmentation
of the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR)
images. With the incorporation of uncertainty estimates to detect failures in
the segmentation masks generated by CNNs, our study further showcases the
potential of our model to flag low-quality segmentation from a given model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROS-X-Habitat: Bridging the ROS Ecosystem with Embodied AI. (arXiv:2109.07703v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07703">
<div class="article-summary-box-inner">
<span><p>We introduce ROS-X-Habitat, a software interface that bridges the AI Habitat
platform for embodied reinforcement learning agents with other robotics
resources via ROS. This interface not only offers standardized communication
protocols between embodied agents and simulators, but also enables
physics-based simulation. With this interface, roboticists are able to train
their own Habitat RL agents in another simulation environment or to develop
their own robotic algorithms inside Habitat Sim. Through in silico experiments,
we demonstrate that ROS-X-Habitat has minimal impact on the navigation
performance and simulation speed of Habitat agents; that a standard set of ROS
mapping, planning and navigation tools can run in the Habitat simulator, and
that a Habitat agent can run in the standard ROS simulator Gazebo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Pruning of Pointwise Convolutions in the Frequency Domain. (arXiv:2109.07707v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07707">
<div class="article-summary-box-inner">
<span><p>Depthwise separable convolutions and frequency-domain convolutions are two
recent ideas for building efficient convolutional neural networks. They are
seemingly incompatible: the vast majority of operations in depthwise separable
CNNs are in pointwise convolutional layers, but pointwise layers use 1x1
kernels, which do not benefit from frequency transformation. This paper unifies
these two ideas by transforming the activations, not the kernels. Our key
insights are that 1) pointwise convolutions commute with frequency
transformation and thus can be computed in the frequency domain without
modification, 2) each channel within a given layer has a different level of
sensitivity to frequency domain pruning, and 3) each channel's sensitivity to
frequency pruning is approximately monotonic with respect to frequency. We
leverage this knowledge by proposing a new technique which wraps each pointwise
layer in a discrete cosine transform (DCT) which is truncated to selectively
prune coefficients above a given threshold as per the needs of each channel. To
learn which frequencies should be pruned from which channels, we introduce a
novel learned parameter which specifies each channel's pruning threshold. We
add a new regularization term which incentivizes the model to decrease the
number of retained frequencies while still maintaining task accuracy. Unlike
weight pruning techniques which rely on sparse operators, our contiguous
frequency band pruning results in fully dense computation. We apply our
technique to MobileNetV2 and in the process reduce computation time by 22% and
incur &lt;1% accuracy degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Activation based Gradient Output Sparsity to Accelerate Backpropagation in CNNs. (arXiv:2109.07710v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07710">
<div class="article-summary-box-inner">
<span><p>Machine/deep-learning (ML/DL) based techniques are emerging as a driving
force behind many cutting-edge technologies, achieving high accuracy on
computer vision workloads such as image classification and object detection.
However, training these models involving large parameters is both
time-consuming and energy-hogging. In this regard, several prior works have
advocated for sparsity to speed up the of DL training and more so, the
inference phase. This work begins with the observation that during training,
sparsity in the forward and backward passes are correlated. In that context, we
investigate two types of sparsity (input and output type) inherent in gradient
descent-based optimization algorithms and propose a hardware micro-architecture
to leverage the same. Our experimental results use five state-of-the-art CNN
models on the Imagenet dataset, and show back propagation speedups in the range
of 1.69$\times$ to 5.43$\times$, compared to the dense baseline execution. By
exploiting sparsity in both the forward and backward passes, speedup
improvements range from 1.68$\times$ to 3.30$\times$ over the sparsity-agnostic
baseline execution. Our work also achieves significant reduction in training
iteration time over several previously proposed dense as well as sparse
accelerator based platforms, in addition to achieving order of magnitude energy
efficiency improvements over GPU based execution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepMTS: Deep Multi-task Learning for Survival Prediction in Patients with Advanced Nasopharyngeal Carcinoma using Pretreatment PET/CT. (arXiv:2109.07711v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07711">
<div class="article-summary-box-inner">
<span><p>Nasopharyngeal Carcinoma (NPC) is a worldwide malignant epithelial cancer.
Survival prediction is a major concern for NPC patients, as it provides early
prognostic information that is needed to guide treatments. Recently, deep
learning, which leverages Deep Neural Networks (DNNs) to learn deep
representations of image patterns, has been introduced to the survival
prediction in various cancers including NPC. It has been reported that
image-derived end-to-end deep survival models have the potential to outperform
clinical prognostic indicators and traditional radiomics-based survival models
in prognostic performance. However, deep survival models, especially 3D models,
require large image training data to avoid overfitting. Unfortunately, medical
image data is usually scarce, especially for Positron Emission
Tomography/Computed Tomography (PET/CT) due to the high cost of PET/CT
scanning. Compared to Magnetic Resonance Imaging (MRI) or Computed Tomography
(CT) providing only anatomical information of tumors, PET/CT that provides both
anatomical (from CT) and metabolic (from PET) information is promising to
achieve more accurate survival prediction. However, we have not identified any
3D end-to-end deep survival model that applies to small PET/CT data of NPC
patients. In this study, we introduced the concept of multi-task leaning into
deep survival models to address the overfitting problem resulted from small
data. Tumor segmentation was incorporated as an auxiliary task to enhance the
model's efficiency of learning from scarce PET/CT data. Based on this idea, we
proposed a 3D end-to-end Deep Multi-Task Survival model (DeepMTS) for joint
survival prediction and tumor segmentation. Our DeepMTS can jointly learn
survival prediction and tumor segmentation using PET/CT data of only 170
patients with advanced NPC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Object Detection by Attending to Per-Sample-Prototype. (arXiv:2109.07734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07734">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection aims to detect instances of specific categories in
a query image with only a handful of support samples. Although this takes less
effort than obtaining enough annotated images for supervised object detection,
it results in a far inferior performance compared to the conventional object
detection methods. In this paper, we propose a meta-learning-based approach
that considers the unique characteristics of each support sample. Rather than
simply averaging the information of the support samples to generate a single
prototype per category, our method can better utilize the information of each
support sample by treating each support sample as an individual prototype.
Specifically, we introduce two types of attention mechanisms for aggregating
the query and support feature maps. The first is to refine the information of
few-shot samples by extracting shared information between the support samples
through attention. Second, each support sample is used as a class code to
leverage the information by comparing similarities between each support feature
and query features. Our proposed method is complementary to the previous
methods, making it easy to plug and play for further improvement. We have
evaluated our method on PASCAL VOC and COCO benchmarks, and the results verify
the effectiveness of our method. In particular, the advantages of our method
are maximized when there is more diversity among support data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Machine Learning Methods for Predicting the Evolution of Brain Connectivity from a Baseline Timepoint. (arXiv:2109.07739v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07739">
<div class="article-summary-box-inner">
<span><p>Predicting the evolution of the brain network, also called connectome, by
foreseeing changes in the connectivity weights linking pairs of anatomical
regions makes it possible to spot connectivity-related neurological disorders
in earlier stages and detect the development of potential connectomic
anomalies. Remarkably, such a challenging prediction problem remains least
explored in the predictive connectomics literature. It is a known fact that
machine learning (ML) methods have proven their predictive abilities in a wide
variety of computer vision problems. However, ML techniques specifically
tailored for the prediction of brain connectivity evolution trajectory from a
single timepoint are almost absent. To fill this gap, we organized a Kaggle
competition where 20 competing teams designed advanced machine learning
pipelines for predicting the brain connectivity evolution from a single
timepoint. The competing teams developed their ML pipelines with a combination
of data pre-processing, dimensionality reduction, and learning methods.
Utilizing an inclusive evaluation approach, we ranked the methods based on two
complementary evaluation metrics (mean absolute error (MAE) and Pearson
Correlation Coefficient (PCC)) and their performances using different training
and testing data perturbation strategies (single random split and
cross-validation). The final rank was calculated using the rank product for
each competing team across all evaluation measures and validation strategies.
In support of open science, the developed 20 ML pipelines along with the
connectomic dataset are made available on GitHub. The outcomes of this
competition are anticipated to lead to the further development of predictive
models that can foresee the evolution of brain connectivity over time, as well
as other types of networks (e.g., genetic networks).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Partially Observable Visual Navigation in a Diverse Environment. (arXiv:2109.07752v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07752">
<div class="article-summary-box-inner">
<span><p>How can a robot navigate successfully in a rich and diverse environment,
indoors or outdoors, along an office corridor or a trail in the park, on the
flat ground, the staircase, or the elevator, etc.? To this end, this work aims
at three challenges: (i) complex visual observations, (ii) partial
observability of local sensing, and (iii) multimodal navigation behaviors that
depend on both the local environment and the high-level goal. We propose a
novel neural network (NN) architecture to represent a local controller and
leverage the flexibility of the end-to-end approach to learn a powerful policy.
To tackle complex visual observations, we extract multiscale spatial
information through convolution layers. To deal with partial observability, we
encode rich history information in LSTM-like modules. Importantly, we integrate
the two into a single unified architecture that exploits convolutional memory
cells to track the observation history at multiple spatial scales, which can
capture the complex spatiotemporal dependencies between observations and
controls. We additionally condition the network on the high-level goal in order
to generate different navigation behavior modes. Specifically, we propose to
use independent memory cells for different modes to prevent mode collapse in
the learned policy. We implemented the NN controller on the SPOT robot and
evaluate it on three challenging tasks with partial observations: adversarial
pedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. Our
model significantly outperforms CNNs, conventional LSTMs, or the ablated
versions of our model. A demo video will be publicly available, showing our
SPOT robot traversing many different locations on our university campus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask-Guided Feature Extraction and Augmentation for Ultra-Fine-Grained Visual Categorization. (arXiv:2109.07755v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07755">
<div class="article-summary-box-inner">
<span><p>While the fine-grained visual categorization (FGVC) problems have been
greatly developed in the past years, the Ultra-fine-grained visual
categorization (Ultra-FGVC) problems have been understudied. FGVC aims at
classifying objects from the same species (very similar categories), while the
Ultra-FGVC targets at more challenging problems of classifying images at an
ultra-fine granularity where even human experts may fail to identify the visual
difference. The challenges for Ultra-FGVC mainly comes from two aspects: one is
that the Ultra-FGVC often arises overfitting problems due to the lack of
training samples; and another lies in that the inter-class variance among
images is much smaller than normal FGVC tasks, which makes it difficult to
learn discriminative features for each class. To solve these challenges, a
mask-guided feature extraction and feature augmentation method is proposed in
this paper to extract discriminative and informative regions of images which
are then used to augment the original feature map. The advantage of the
proposed method is that the feature detection and extraction model only
requires a small amount of target region samples with bounding boxes for
training, then it can automatically locate the target area for a large number
of images in the dataset at a high detection accuracy. Experimental results on
two public datasets and ten state-of-the-art benchmark methods consistently
demonstrate the effectiveness of the proposed method both visually and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Semantic Contrast for Self-Supervised Visual Representation Learning. (arXiv:2109.07756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07756">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning for visual pre-training has achieved
remarkable success with sample (instance or pixel) discrimination and semantics
discovery of instance, whereas there still exists a non-negligible gap between
pre-trained model and downstream dense prediction tasks. Concretely, these
downstream tasks require more accurate representation, in other words, the
pixels from the same object must belong to a shared semantic category, which is
lacking in the previous methods. In this work, we present Dense Semantic
Contrast (DSC) for modeling semantic category decision boundaries at a dense
level to meet the requirement of these tasks. Furthermore, we propose a dense
cross-image semantic contrastive learning framework for multi-granularity
representation learning. Specially, we explicitly explore the semantic
structure of the dataset by mining relations among pixels from different
perspectives. For intra-image relation modeling, we discover pixel neighbors
from multiple views. And for inter-image relations, we enforce pixel
representation from the same semantic class to be more similar than the
representation from different classes in one mini-batch. Experimental results
show that our DSC model outperforms state-of-the-art methods when transferring
to downstream dense prediction tasks, including object detection, semantic
segmentation, and instance segmentation. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Non-Line-of-Sight Photography. (arXiv:2109.07783v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07783">
<div class="article-summary-box-inner">
<span><p>Non-line-of-sight (NLOS) imaging is based on capturing the multi-bounce
indirect reflections from the hidden objects. Active NLOS imaging systems rely
on the capture of the time of flight of light through the scene, and have shown
great promise for the accurate and robust reconstruction of hidden scenes
without the need for specialized scene setups and prior assumptions. Despite
that existing methods can reconstruct 3D geometries of the hidden scene with
excellent depth resolution, accurately recovering object textures and
appearance with high lateral resolution remains an challenging problem. In this
work, we propose a new problem formulation, called NLOS photography, to
specifically address this deficiency. Rather than performing an intermediate
estimate of the 3D scene geometry, our method follows a data-driven approach
and directly reconstructs 2D images of a NLOS scene that closely resemble the
pictures taken with a conventional camera from the location of the relay wall.
This formulation largely simplifies the challenging reconstruction problem by
bypassing the explicit modeling of 3D geometry, and enables the learning of a
deep model with a relatively small training dataset. The results are NLOS
reconstructions of unprecedented lateral resolution and image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHFC: Multi-Head Feature Collaboration for Few-Shot Learning. (arXiv:2109.07785v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07785">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to address the data-scarce problem. A standard
FSL framework is composed of two components: (1) Pre-train. Employ the base
data to generate a CNN-based feature extraction model (FEM). (2) Meta-test.
Apply the trained FEM to acquire the novel data's features and recognize them.
FSL relies heavily on the design of the FEM. However, various FEMs have
distinct emphases. For example, several may focus more attention on the contour
information, whereas others may lay particular emphasis on the texture
information. The single-head feature is only a one-sided representation of the
sample. Besides the negative influence of cross-domain (e.g., the trained FEM
can not adapt to the novel class flawlessly), the distribution of novel data
may have a certain degree of deviation compared with the ground truth
distribution, which is dubbed as distribution-shift-problem (DSP). To address
the DSP, we propose Multi-Head Feature Collaboration (MHFC) algorithm, which
attempts to project the multi-head features (e.g., multiple features extracted
from a variety of FEMs) to a unified space and fuse them to capture more
discriminative information. Typically, first, we introduce a subspace learning
method to transform the multi-head features to aligned low-dimensional
representations. It corrects the DSP via learning the feature with more
powerful discrimination and overcomes the problem of inconsistent measurement
scales from different head features. Then, we design an attention block to
update combination weights for each head feature automatically. It
comprehensively considers the contribution of various perspectives and further
improves the discrimination of features. We evaluate the proposed method on
five benchmark datasets (including cross-domain experiments) and achieve
significant improvements of 2.1%-7.8% compared with state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marginal MAP Estimation for Inverse RL under Occlusion with Observer Noise. (arXiv:2109.07788v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07788">
<div class="article-summary-box-inner">
<span><p>We consider the problem of learning the behavioral preferences of an expert
engaged in a task from noisy and partially-observable demonstrations. This is
motivated by real-world applications such as a line robot learning from
observing a human worker, where some observations are occluded by environmental
objects that cannot be removed. Furthermore, robotic perception tends to be
imperfect and noisy. Previous techniques for inverse reinforcement learning
(IRL) take the approach of either omitting the missing portions or inferring it
as part of expectation-maximization, which tends to be slow and prone to local
optima. We present a new method that generalizes the well-known Bayesian
maximum-a-posteriori (MAP) IRL method by marginalizing the occluded portions of
the trajectory. This is additionally extended with an observation model to
account for perception noise. We show that the marginal MAP (MMAP) approach
significantly improves on the previous IRL technique under occlusion in both
formative evaluations on a toy problem and in a summative evaluation on an
onion sorting line task by a robot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning. (arXiv:2109.07799v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07799">
<div class="article-summary-box-inner">
<span><p>Automatic transcription of scene understanding in images and videos is a step
towards artificial general intelligence. Image captioning is a nomenclature for
describing meaningful information in an image using computer vision techniques.
Automated image captioning techniques utilize encoder and decoder architecture,
where the encoder extracts features from an image and the decoder generates a
transcript. In this work, we investigate two unexplored ideas for image
captioning using transformers: First, we demonstrate the enforcement of using
objects' relevance in the surrounding environment. Second, learning an explicit
association between labels and language constructs. We propose label-attention
Transformer with geometrically coherent objects (LATGeO). The proposed
technique acquires a proposal of geometrically coherent objects using a deep
neural network (DNN) and generates captions by investigating their
relationships using a label-attention module. Object coherence is defined using
the localized ratio of the geometrical properties of the proposals. The
label-attention module associates the extracted objects classes to the
available dictionary using self-attention layers. The experimentation results
show that objects' relevance in surroundings and binding of their visual
feature with their geometrically localized ratios combined with its associated
labels help in defining meaningful captions. The proposed framework is tested
on the MSCOCO dataset, and a thorough evaluation resulting in overall better
quantitative scores pronounces its superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compact Binary Fingerprint for Image Copy Re-Ranking. (arXiv:2109.07802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07802">
<div class="article-summary-box-inner">
<span><p>Image copy detection is challenging and appealing topic in computer vision
and signal processing. Recent advancements in multimedia have made distribution
of image across the global easy and fast: that leads to many other issues such
as forgery and image copy retrieval.
</p>
<p>Local keypoint descriptors such as SIFT are used to represent the images, and
based on those descriptors matching, images are matched and retrieved. Features
are quantized so that searching/matching may be made feasible for large
databases at the cost of accuracy loss. In this paper, we propose binary
feature that is obtained by quantizing the SIFT into binary, and rank list is
re-examined to remove the false positives. Experiments on challenging dataset
shows the gain in accuracy and time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection Accuracy for Evaluating Compositional Explanations of Units. (arXiv:2109.07804v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07804">
<div class="article-summary-box-inner">
<span><p>The recent success of deep learning models in solving complex problems and in
different domains has increased interest in understanding what they learn.
Therefore, different approaches have been employed to explain these models, one
of which uses human-understandable concepts as explanations. Two examples of
methods that use this approach are Network Dissection and Compositional
explanations. The former explains units using atomic concepts, while the latter
makes explanations more expressive, replacing atomic concepts with logical
forms. While intuitively, logical forms are more informative than atomic
concepts, it is not clear how to quantify this improvement, and their
evaluation is often based on the same metric that is optimized during the
search-process and on the usage of hyper-parameters to be tuned. In this paper,
we propose to use as evaluation metric the Detection Accuracy, which measures
units' consistency of detection of their assigned explanations. We show that
this metric (1) evaluates explanations of different lengths effectively, (2)
can be used as a stopping criterion for the compositional explanation search,
eliminating the explanation length hyper-parameter, and (3) exposes new
specialized units whose length 1 explanations are the perceptual abstractions
of their longer explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Assignment Distillation for Object Detection. (arXiv:2109.07843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07843">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation methods are proved to be promising in improving the
performance of neural networks and no additional computational expenses are
required during the inference time. For the sake of boosting the accuracy of
object detection, a great number of knowledge distillation methods have been
proposed particularly designed for object detection. However, most of these
methods only focus on feature-level distillation and label-level distillation,
leaving the label assignment step, a unique and paramount procedure for object
detection, by the wayside. In this work, we come up with a simple but effective
knowledge distillation approach focusing on label assignment in object
detection, in which the positive and negative samples of student network are
selected in accordance with the predictions of teacher network. Our method
shows encouraging results on the MSCOCO2017 benchmark, and can not only be
applied to both one-stage detectors and two-stage detectors but also be
utilized orthogonally with other knowledge distillation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-aware Padding for Semantic Segmentation. (arXiv:2109.07854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07854">
<div class="article-summary-box-inner">
<span><p>Zero padding is widely used in convolutional neural networks to prevent the
size of feature maps diminishing too fast. However, it has been claimed to
disturb the statistics at the border. As an alternative, we propose a
context-aware (CA) padding approach to extend the image. We reformulate the
padding problem as an image extrapolation problem and illustrate the effects on
the semantic segmentation task. Using context-aware padding, the ResNet-based
segmentation model achieves higher mean Intersection-Over-Union than the
traditional zero padding on the Cityscapes and the dataset of DeepGlobe
satellite imaging challenge. Furthermore, our padding does not bring noticeable
overhead during training and testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Continual Learning Algorithms by Generating 3D Virtual Environments. (arXiv:2109.07855v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07855">
<div class="article-summary-box-inner">
<span><p>Continual learning refers to the ability of humans and animals to
incrementally learn over time in a given environment. Trying to simulate this
learning process in machines is a challenging task, also due to the inherent
difficulty in creating conditions for designing continuously evolving dynamics
that are typical of the real-world. Many existing research works usually
involve training and testing of virtual agents on datasets of static images or
short videos, considering sequences of distinct learning tasks. However, in
order to devise continual learning algorithms that operate in more realistic
conditions, it is fundamental to gain access to rich, fully customizable and
controlled experimental playgrounds. Focussing on the specific case of vision,
we thus propose to leverage recent advances in 3D virtual environments in order
to approach the automatic generation of potentially life-long dynamic scenes
with photo-realistic appearance. Scenes are composed of objects that move along
variable routes with different and fully customizable timings, and randomness
can also be included in their evolution. A novel element of this paper is that
scenes are described in a parametric way, thus allowing the user to fully
control the visual complexity of the input stream the agent perceives. These
general principles are concretely implemented exploiting a recently published
3D virtual environment. The user can generate scenes without the need of having
strong skills in computer graphics, since all the generation facilities are
exposed through a simple high-level Python interface. We publicly share the
proposed generator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Humanly Certifying Superhuman Classifiers. (arXiv:2109.07867v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07867">
<div class="article-summary-box-inner">
<span><p>Estimating the performance of a machine learning system is a longstanding
challenge in artificial intelligence research. Today, this challenge is
especially relevant given the emergence of systems which appear to increasingly
outperform human beings. In some cases, this "superhuman" performance is
readily demonstrated; for example by defeating legendary human players in
traditional two player games. On the other hand, it can be challenging to
evaluate classification models that potentially surpass human performance.
Indeed, human annotations are often treated as a ground truth, which implicitly
assumes the superiority of the human over any models trained on human
annotations. In reality, human annotators can make mistakes and be subjective.
Evaluating the performance with respect to a genuine oracle may be more
objective and reliable, even when querying the oracle is expensive or
impossible. In this paper, we first raise the challenge of evaluating the
performance of both humans and models with respect to an oracle which is
unobserved. We develop a theory for estimating the accuracy compared to the
oracle, using only imperfect human annotations for reference. Our analysis
provides a simple recipe for detecting and certifying superhuman performance in
this setting, which we believe will assist in understanding the stage of
current research on classification. We validate the convergence of the bounds
and the assumptions of our theory on carefully designed toy experiments with
known oracles. Moreover, we demonstrate the utility of our theory by
meta-analyzing large-scale natural language processing tasks, for which an
oracle does not exist, and show that under our assumptions a number of models
from recent years are with high probability superhuman.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainability Requires Interactivity. (arXiv:2109.07869v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07869">
<div class="article-summary-box-inner">
<span><p>When explaining the decisions of deep neural networks, simple stories are
tempting but dangerous. Especially in computer vision, the most popular
explanation approaches give a false sense of comprehension to its users and
provide an overly simplistic picture. We introduce an interactive framework to
understand the highly complex decision boundaries of modern vision models. It
allows the user to exhaustively inspect, probe, and test a network's decisions.
Across a range of case studies, we compare the power of our interactive
approach to static explanation methods, showing how these can lead a user
astray, with potentially severe consequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resolution based Feature Distillation for Cross Resolution Person Re-Identification. (arXiv:2109.07871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07871">
<div class="article-summary-box-inner">
<span><p>Person re-identification (re-id) aims to retrieve images of same identities
across different camera views. Resolution mismatch occurs due to varying
distances between person of interest and cameras, this significantly degrades
the performance of re-id in real world scenarios. Most of the existing
approaches resolve the re-id task as low resolution problem in which a low
resolution query image is searched in a high resolution images gallery. Several
approaches apply image super resolution techniques to produce high resolution
images but ignore the multiple resolutions of gallery images which is a better
realistic scenario. In this paper, we introduce channel correlations to improve
the learning of features from the degraded data. In addition, to overcome the
problem of multiple resolutions we propose a Resolution based Feature
Distillation (RFD) approach. Such an approach learns resolution invariant
features by filtering the resolution related features from the final feature
vectors that are used to compute the distance matrix. We tested the proposed
approach on two synthetically created datasets and on one original multi
resolution dataset with real degradation. Our approach improves the performance
when multiple resolutions occur in the gallery and have comparable results in
case of single resolution (low resolution re-id).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SketchHairSalon: Deep Sketch-based Hair Image Synthesis. (arXiv:2109.07874v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07874">
<div class="article-summary-box-inner">
<span><p>Recent deep generative models allow real-time generation of hair images from
sketch inputs. Existing solutions often require a user-provided binary mask to
specify a target hair shape. This not only costs users extra labor but also
fails to capture complicated hair boundaries. Those solutions usually encode
hair structures via orientation maps, which, however, are not very effective to
encode complex structures. We observe that colored hair sketches already
implicitly define target hair shapes as well as hair appearance and are more
flexible to depict hair structures than orientation maps. Based on these
observations, we present SketchHairSalon, a two-stage framework for generating
realistic hair images directly from freehand sketches depicting desired hair
structure and appearance. At the first stage, we train a network to predict a
hair matte from an input hair sketch, with an optional set of non-hair strokes.
At the second stage, another network is trained to synthesize the structure and
appearance of hair images from the input sketch and the generated matte. To
make the networks in the two stages aware of long-term dependency of strokes,
we apply self-attention modules to them. To train these networks, we present a
new dataset containing thousands of annotated hair sketch-image pairs and
corresponding hair mattes. Two efficient methods for sketch completion are
proposed to automatically complete repetitive braided parts and hair strokes,
respectively, thus reducing the workload of users. Based on the trained
networks and the two sketch completion strategies, we build an intuitive
interface to allow even novice users to design visually pleasing hair images
exhibiting various hair structures and appearance via freehand sketches. The
qualitative and quantitative evaluations show the advantages of the proposed
system over the existing or alternative solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Medical Pre-Diagnosis System for Histopathological Image of Breast Cancer. (arXiv:2109.07878v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07878">
<div class="article-summary-box-inner">
<span><p>This paper constructs a novel intelligent medical diagnosis system, which can
realize automatic communication and breast cancer pathological image
recognition. This system contains two main parts, including a pre-training
chatbot called M-Chatbot and an improved neural network model of
EfficientNetV2-S named EfficientNetV2-SA, in which the activation function in
top layers is replaced by ACON-C. Using information retrieval mechanism,
M-Chatbot instructs patients to send breast pathological image to
EfficientNetV2-SA network, and then the classifier trained by transfer learning
will return the diagnosis results. We verify the performance of our chatbot and
classification on the extrinsic metrics and BreaKHis dataset, respectively. The
task completion rate of M-Chatbot reached 63.33\%. For the BreaKHis dataset,
the highest accuracy of EfficientNetV2-SA network have achieved 84.71\%. All
these experimental results illustrate that the proposed model can improve the
accuracy performance of image recognition and our new intelligent medical
diagnosis system is successful and efficient in providing automatic diagnosis
of breast cancer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated risk classification of colon biopsies based on semantic segmentation of histopathology images. (arXiv:2109.07892v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07892">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) can potentially support histopathologists in the
diagnosis of a broad spectrum of cancer types. In colorectal cancer (CRC), AI
can alleviate the laborious task of characterization and reporting on resected
biopsies, including polyps, the numbers of which are increasing as a result of
CRC population screening programs, ongoing in many countries all around the
globe. Here, we present an approach to address two major challenges in
automated assessment of CRC histopathology whole-slide images. First, we
present an AI-based method to segment multiple tissue compartments in the
H\&amp;E-stained whole-slide image, which provides a different, more perceptible
picture of tissue morphology and composition. We test and compare a panel of
state-of-the-art loss functions available for segmentation models, and provide
indications about their use in histopathology image segmentation, based on the
analysis of a) a multi-centric cohort of CRC cases from five medical centers in
the Netherlands and Germany, and b) two publicly available datasets on
segmentation in CRC. Second, we use the best performing AI model as the basis
for a computer-aided diagnosis system (CAD) that classifies colon biopsies into
four main categories that are relevant pathologically. We report the
performance of this system on an independent cohort of more than 1,000
patients. The results show the potential of such an AI-based system to assist
pathologists in diagnosis of CRC in the context of population screening. We
have made the segmentation model available for research use on
https://grand-challenge.org/algorithms/colon-tissue-segmentation/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterogeneous Relational Complement for Vehicle Re-identification. (arXiv:2109.07894v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07894">
<div class="article-summary-box-inner">
<span><p>The crucial problem in vehicle re-identification is to find the same vehicle
identity when reviewing this object from cross-view cameras, which sets a
higher demand for learning viewpoint-invariant representations. In this paper,
we propose to solve this problem from two aspects: constructing robust feature
representations and proposing camera-sensitive evaluations. We first propose a
novel Heterogeneous Relational Complement Network (HRCN) by incorporating
region-specific features and cross-level features as complements for the
original high-level output. Considering the distributional differences and
semantic misalignment, we propose graph-based relation modules to embed these
heterogeneous features into one unified high-dimensional space. On the other
hand, considering the deficiencies of cross-camera evaluations in existing
measures (i.e., CMC and AP), we then propose a Cross-camera Generalization
Measure (CGM) to improve the evaluations by introducing position-sensitivity
and cross-camera generalization penalties. We further construct a new benchmark
of existing models with our proposed CGM and experimental results reveal that
our proposed HRCN model achieves new state-of-the-art in VeRi-776, VehicleID,
and VERI-Wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Computer Science Can Aid Forest Restoration. (arXiv:2109.07898v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07898">
<div class="article-summary-box-inner">
<span><p>The world faces two interlinked crises: climate change and loss of
biodiversity. Forest restoration on degraded lands and surplus croplands can
play a significant role both in sequestering carbon and re-establishing
bio-diversity. There is a considerable body of research and practice that
addresses forest restoration. However, there has been little work by computer
scientists to bring powerful computational techniques to bear on this important
area of work, perhaps due to a lack of awareness. In an attempt to bridge this
gap, we present our vision of how techniques from computer science, broadly
speaking, can aid current practice in forest restoration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition. (arXiv:2109.07916v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07916">
<div class="article-summary-box-inner">
<span><p>Using mel-spectrograms over conventional MFCCs features, we assess the
abilities of convolutional neural networks to accurately recognize and classify
emotions from speech data. We introduce FSER, a speech emotion recognition
model trained on four valid speech databases, achieving a high-classification
accuracy of 95,05\%, over 8 different emotion classes: anger, anxiety, calm,
disgust, happiness, neutral, sadness, surprise. On each benchmark dataset, FSER
outperforms the best models introduced so far, achieving a state-of-the-art
performance. We show that FSER stays reliable, independently of the language,
sex identity, and any other external factor. Additionally, we describe how FSER
could potentially be used to improve mental and emotional health care and how
our analysis and findings serve as guidelines and benchmarks for further works
in the same direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M2RNet: Multi-modal and Multi-scale Refined Network for RGB-D Salient Object Detection. (arXiv:2109.07922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07922">
<div class="article-summary-box-inner">
<span><p>Salient object detection is a fundamental topic in computer vision. Previous
methods based on RGB-D often suffer from the incompatibility of multi-modal
feature fusion and the insufficiency of multi-scale feature aggregation. To
tackle these two dilemmas, we propose a novel multi-modal and multi-scale
refined network (M2RNet). Three essential components are presented in this
network. The nested dual attention module (NDAM) explicitly exploits the
combined features of RGB and depth flows. The adjacent interactive aggregation
module (AIAM) gradually integrates the neighbor features of high, middle and
low levels. The joint hybrid optimization loss (JHOL) makes the predictions
have a prominent outline. Extensive experiments demonstrate that our method
outperforms other state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifting 2D Object Locations to 3D by Discounting LiDAR Outliers across Objects and Views. (arXiv:2109.07945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07945">
<div class="article-summary-box-inner">
<span><p>We present a system for automatic converting of 2D mask object predictions
and raw LiDAR point clouds into full 3D bounding boxes of objects. Because the
LiDAR point clouds are partial, directly fitting bounding boxes to the point
clouds is meaningless. Instead, we suggest that obtaining good results requires
sharing information between \emph{all} objects in the dataset jointly, over
multiple frames. We then make three improvements to the baseline. First, we
address ambiguities in predicting the object rotations via direct optimization
in this space while still backpropagating rotation prediction through the
model. Second, we explicitly model outliers and task the network with learning
their typical patterns, thus better discounting them. Third, we enforce
temporal consistency when video data is available. With these contributions,
our method significantly outperforms previous work despite the fact that those
methods use significantly more complex pipelines, 3D models and additional
human-annotated external sources of prior information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection. (arXiv:2109.07950v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07950">
<div class="article-summary-box-inner">
<span><p>With the increased deployment of face recognition systems in our daily lives,
face presentation attack detection (PAD) is attracting a lot of attention and
playing a key role in securing face recognition systems. Despite the great
performance achieved by the hand-crafted and deep learning based methods in
intra-dataset evaluations, the performance drops when dealing with unseen
scenarios. In this work, we propose a dual-stream convolution neural networks
(CNNs) framework. One stream adapts four learnable frequency filters to learn
features in the frequency domain, which are less influenced variations in
sensors/illuminations. The other stream leverage the RGB images to complement
the features of the frequency domain. Moreover, we propose a hierarchical
attention module integration to join the information from the two streams at
different stages by considering the nature of deep features in different layers
of the CNN. The proposed method is evaluated in the intra-dataset and
cross-dataset setups and the results demonstrates that our proposed approach
enhances the generalizability in most experimental setups in comparison to
state-of-the-art, including the methods designed explicitly for domain
adaption/shift problem. We successfully prove the design of our proposed PAD
solution in a step-wise ablation study that involves our proposed learnable
frequency decomposition, our hierarchical attention module design, and the used
loss function. Training codes and pre-trained models are publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of Tencent Multi-modal Ads Video Understanding Challenge. (arXiv:2109.07951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07951">
<div class="article-summary-box-inner">
<span><p>Multi-modal Ads Video Understanding Challenge is the first grand challenge
aiming to comprehensively understand ads videos. Our challenge includes two
tasks: video structuring in the temporal dimension and multi-modal video
classification. It asks the participants to accurately predict both the scene
boundaries and the multi-label categories of each scene based on a fine-grained
and ads-related category hierarchy. Therefore, our task has four distinguishing
features from previous ones: ads domain, multi-modal information, temporal
segmentation, and multi-label classification. It will advance the foundation of
ads video understanding and have a significant impact on many ads applications
like video recommendation. This paper presents an overview of our challenge,
including the background of ads videos, an elaborate description of task and
dataset, evaluation protocol, and our proposed baseline. By ablating the key
components of our baseline, we would like to reveal the main challenges of this
task and provide useful guidance for future research of this area. In this
paper, we give an extended version of our challenge overview. The dataset will
be publicly available at https://algo.qq.com/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality-aware Cine Cardiac MRI Reconstruction and Analysis from Undersampled k-space Data. (arXiv:2109.07955v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07955">
<div class="article-summary-box-inner">
<span><p>Cine cardiac MRI is routinely acquired for the assessment of cardiac health,
but the imaging process is slow and typically requires several breath-holds to
acquire sufficient k-space profiles to ensure good image quality. Several
undersampling-based reconstruction techniques have been proposed during the
last decades to speed up cine cardiac MRI acquisition. However, the
undersampling factor is commonly fixed to conservative values before
acquisition to ensure diagnostic image quality, potentially leading to
unnecessarily long scan times. In this paper, we propose an end-to-end
quality-aware cine short-axis cardiac MRI framework that combines image
acquisition and reconstruction with downstream tasks such as segmentation,
volume curve analysis and estimation of cardiac functional parameters. The goal
is to reduce scan time by acquiring only a fraction of k-space data to enable
the reconstruction of images that can pass quality control checks and produce
reliable estimates of cardiac functional parameters. The framework consists of
a deep learning model for the reconstruction of 2D+t cardiac cine MRI images
from undersampled data, an image quality-control step to detect good quality
reconstructions, followed by a deep learning model for bi-ventricular
segmentation, a quality-control step to detect good quality segmentations and
automated calculation of cardiac functional parameters. To demonstrate the
feasibility of the proposed approach, we perform simulations using a cohort of
selected participants from the UK Biobank (n=270), 200 healthy subjects and 70
patients with cardiomyopathies. Our results show that we can produce
quality-controlled images in a scan time reduced from 12 to 4 seconds per
slice, enabling reliable estimates of cardiac functional parameters such as
ejection fraction within 5% mean absolute error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real Time Monocular Vehicle Velocity Estimation using Synthetic Data. (arXiv:2109.07957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07957">
<div class="article-summary-box-inner">
<span><p>Vision is one of the primary sensing modalities in autonomous driving. In
this paper we look at the problem of estimating the velocity of road vehicles
from a camera mounted on a moving car. Contrary to prior methods that train
end-to-end deep networks that estimate the vehicles' velocity from the video
pixels, we propose a two-step approach where first an off-the-shelf tracker is
used to extract vehicle bounding boxes and then a small neural network is used
to regress the vehicle velocity from the tracked bounding boxes. Surprisingly,
we find that this still achieves state-of-the-art estimation performance with
the significant benefit of separating perception from dynamics estimation via a
clean, interpretable and verifiable interface which allows us distill the
statistics which are crucial for velocity estimation. We show that the latter
can be used to easily generate synthetic training data in the space of bounding
boxes and use this to improve the performance of our method further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R3LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package. (arXiv:2109.07982v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07982">
<div class="article-summary-box-inner">
<span><p>In this letter, we propose a novel LiDAR-Inertial-Visual sensor fusion
framework termed R3LIVE, which takes advantage of measurement of LiDAR,
inertial, and visual sensors to achieve robust and accurate state estimation.
R3LIVE is contained of two subsystems, the LiDAR-inertial odometry (LIO) and
visual-inertial odometry (VIO). The LIO subsystem (FAST-LIO) takes advantage of
the measurement from LiDAR and inertial sensors and builds the geometry
structure of (i.e. the position of 3D points) global maps. The VIO subsystem
utilizes the data of visual-inertial sensors and renders the map's texture
(i.e. the color of 3D points). More specifically, the VIO subsystem fuses the
visual data directly and effectively by minimizing the frame-to-map photometric
error. The developed system R3LIVE is developed based on our previous work
R2LIVE, with careful architecture design and implementation. Experiment results
show that the resultant system achieves more robustness and higher accuracy in
state estimation than current counterparts (see our attached video).
</p>
<p>R3LIVE is a versatile and well-engineered system toward various possible
applications, which can not only serve as a SLAM system for real-time robotic
applications, but can also reconstruct the dense, precise, RGB-colored 3D maps
for applications like surveying and mapping. Moreover, to make R3LIVE more
extensible, we develop a series of offline utilities for reconstructing and
texturing meshes, which further minimizes the gap between R3LIVE and various of
3D applications such as simulators, video games and etc (see our demos video).
To share our findings and make contributions to the community, we open source
R3LIVE on our Github, including all of our codes, software utilities, and the
mechanical design of our device.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing Perceptual Adversarial Patches for Crowd Counting. (arXiv:2109.07986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07986">
<div class="article-summary-box-inner">
<span><p>Crowd counting, which is significantly important for estimating the number of
people in safety-critical scenes, has been shown to be vulnerable to
adversarial examples in the physical world (e.g., adversarial patches). Though
harmful, adversarial examples are also valuable for assessing and better
understanding model robustness. However, existing adversarial example
generation methods in crowd counting scenarios lack strong transferability
among different black-box models. Motivated by the fact that transferability is
positively correlated to the model-invariant characteristics, this paper
proposes the Perceptual Adversarial Patch (PAP) generation framework to learn
the shared perceptual features between models by exploiting both the model
scale perception and position perception. Specifically, PAP exploits
differentiable interpolation and density attention to help learn the invariance
between models during training, leading to better transferability. In addition,
we surprisingly found that our adversarial patches could also be utilized to
benefit the performance of vanilla models for alleviating several challenges
including cross datasets and complex backgrounds. Extensive experiments under
both digital and physical world scenarios demonstrate the effectiveness of our
PAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations. (arXiv:2109.07991v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07991">
<div class="article-summary-box-inner">
<span><p>Multisensory object-centric perception, reasoning, and interaction have been
a key research topic in recent years. However, the progress in these directions
is limited by the small set of objects available -- synthetic objects are not
realistic enough and are mostly centered around geometry, while real object
datasets such as YCB are often practically challenging and unstable to acquire
due to international shipping, inventory, and financial cost. We present
ObjectFolder, a dataset of 100 virtualized objects that addresses both
challenges with two key innovations. First, ObjectFolder encodes the visual,
auditory, and tactile sensory data for all objects, enabling a number of
multisensory object recognition tasks, beyond existing datasets that focus
purely on object geometry. Second, ObjectFolder employs a uniform,
object-centric, and implicit representation for each object's visual textures,
acoustic simulations, and tactile readings, making the dataset flexible to use
and easy to share. We demonstrate the usefulness of our dataset as a testbed
for multisensory perception and control by evaluating it on a variety of
benchmark tasks, including instance recognition, cross-sensory retrieval, 3D
reconstruction, and robotic grasping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propaganda Techniques in Memes. (arXiv:2109.08013v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08013">
<div class="article-summary-box-inner">
<span><p>Propaganda can be defined as a form of communication that aims to influence
the opinions or the actions of people towards a specific goal; this is achieved
by means of well-defined rhetorical and psychological devices. Propaganda, in
the form we know it today, can be dated back to the beginning of the 17th
century. However, it is with the advent of the Internet and the social media
that it has started to spread on a much larger scale than before, thus becoming
major societal and political issue. Nowadays, a large fraction of propaganda in
social media is multimodal, mixing textual with visual content. With this in
mind, here we propose a new multi-label multimodal task: detecting the type of
propaganda techniques used in memes. We further create and release a new corpus
of 950 memes, carefully annotated with 22 propaganda techniques, which can
appear in the text, in the image, or in both. Our analysis of the corpus shows
that understanding both modalities together is essential for detecting these
techniques. This is further confirmed in our experiments with several
state-of-the-art multimodal models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The pitfalls of using open data to develop deep learning solutions for COVID-19 detection in chest X-rays. (arXiv:2109.08020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08020">
<div class="article-summary-box-inner">
<span><p>Since the emergence of COVID-19, deep learning models have been developed to
identify COVID-19 from chest X-rays. With little to no direct access to
hospital data, the AI community relies heavily on public data comprising
numerous data sources. Model performance results have been exceptional when
training and testing on open-source data, surpassing the reported capabilities
of AI in pneumonia-detection prior to the COVID-19 outbreak. In this study
impactful models are trained on a widely used open-source data and tested on an
external test set and a hospital dataset, for the task of classifying chest
X-rays into one of three classes: COVID-19, non-COVID pneumonia and
no-pneumonia. Classification performance of the models investigated is
evaluated through ROC curves, confusion matrices and standard classification
metrics. Explainability modules are implemented to explore the image features
most important to classification. Data analysis and model evaluations show that
the popular open-source dataset COVIDx is not representative of the real
clinical problem and that results from testing on this are inflated. Dependence
on open-source data can leave models vulnerable to bias and confounding
variables, requiring careful analysis to develop clinically useful/viable AI
tools for COVID-19 detection in chest X-rays.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer. (arXiv:2109.08024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08024">
<div class="article-summary-box-inner">
<span><p>We propose a semi-supervised network for wide-angle portraits correction.
Wide-angle images often suffer from skew and distortion affected by perspective
distortion, especially noticeable at the face regions. Previous deep learning
based approaches require the ground-truth correction flow maps for the training
guidance. However, such labels are expensive, which can only be obtained
manually. In this work, we propose a semi-supervised scheme, which can consume
unlabeled data in addition to the labeled data for improvements. Specifically,
our semi-supervised scheme takes the advantages of the consistency mechanism,
with several novel components such as direction and range consistency (DRC) and
regression consistency (RC). Furthermore, our network, named as Multi-Scale
Swin-Unet (MS-Unet), is built upon the multi-scale swin transformer block
(MSTB), which can learn both local-scale and long-range semantic information
effectively. In addition, we introduce a high-quality unlabeled dataset with
rich scenarios for the training. Extensive experiments demonstrate that the
proposed method is superior over the state-of-the-art methods and other
representative baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architecture Search in operational context: a remote sensing case-study. (arXiv:2109.08028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08028">
<div class="article-summary-box-inner">
<span><p>Deep learning has become in recent years a cornerstone tool fueling key
innovations in the industry, such as autonomous driving. To attain good
performances, the neural network architecture used for a given application must
be chosen with care. These architectures are often handcrafted and therefore
prone to human biases and sub-optimal selection. Neural Architecture Search
(NAS) is a framework introduced to mitigate such risks by jointly optimizing
the network architectures and its weights. Albeit its novelty, it was applied
on complex tasks with significant results - e.g. semantic image segmentation.
In this technical paper, we aim to evaluate its ability to tackle a challenging
operational task: semantic segmentation of objects of interest in satellite
imagery. Designing a NAS framework is not trivial and has strong dependencies
to hardware constraints. We therefore motivate our NAS approach selection and
provide corresponding implementation details. We also present novel ideas to
carry out other such use-case studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering. (arXiv:2109.08029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08029">
<div class="article-summary-box-inner">
<span><p>Integrating outside knowledge for reasoning in visio-linguistic tasks such as
visual question answering (VQA) is an open problem. Given that pretrained
language models have been shown to include world knowledge, we propose to use a
unimodal (text-only) train and inference procedure based on automatic
off-the-shelf captioning of images and pretrained language models. Our results
on a visual question answering task which requires external knowledge (OK-VQA)
show that our text-only model outperforms pretrained multimodal (image-text)
models of comparable number of parameters. In contrast, our model is less
effective in a standard VQA task (VQA 2.0) confirming that our text-only method
is specially effective for tasks requiring external knowledge. In addition, we
show that our unimodal model is complementary to multimodal models in both
OK-VQA and VQA 2.0, and yield the best result to date in OK-VQA among systems
not using external knowledge graphs, and comparable to systems that do use
them. Our qualitative analysis on OK-VQA reveals that automatic captions often
fail to capture relevant information in the images, which seems to be balanced
by the better inference ability of the text-only language models. Our work
opens up possibilities to further improve inference in visio-linguistic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Temporal Sentence Grounding in Videos. (arXiv:2109.08039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08039">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding in videos~(TSGV), which aims to localize one
target segment from an untrimmed video with respect to a given sentence query,
has drawn increasing attentions in the research community over the past few
years. Different from the task of temporal action localization, TSGV is more
flexible since it can locate complicated activities via natural languages,
without restrictions from predefined action categories. Meanwhile, TSGV is more
challenging since it requires both textual and visual understanding for
semantic alignment between two modalities~(i.e., text and video). In this
survey, we give a comprehensive overview for TSGV, which i) summarizes the
taxonomy of existing methods, ii) provides a detailed description of the
evaluation protocols~(i.e., datasets and metrics) to be used in TSGV, and iii)
in-depth discusses potential problems of current benchmarking designs and
research directions for further investigations. To the best of our knowledge,
this is the first systematic survey on temporal sentence grounding. More
specifically, we first discuss existing TSGV approaches by grouping them into
four categories, i.e., two-stage methods, end-to-end methods, reinforcement
learning-based methods, and weakly supervised methods. Then we present the
benchmark datasets and evaluation metrics to assess current research progress.
Finally, we discuss some limitations in TSGV through pointing out potential
problems improperly resolved in the current evaluation protocols, which may
push forwards more cutting edge research in TSGV. Besides, we also share our
insights on several promising directions, including three typical tasks with
new and practical settings based on TSGV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Dataset For Large-scale 3D Facial Emotion Recognition. (arXiv:2109.08043v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08043">
<div class="article-summary-box-inner">
<span><p>The tremendous development in deep learning has led facial expression
recognition (FER) to receive much attention in the past few years. Although 3D
FER has an inherent edge over its 2D counterpart, work on 2D images has
dominated the field. The main reason for the slow development of 3D FER is the
unavailability of large training and large test datasets. Recognition
accuracies have already saturated on existing 3D emotion recognition datasets
due to their small gallery sizes. Unlike 2D photographs, 3D facial scans are
not easy to collect, causing a bottleneck in the development of deep 3D FER
networks and datasets. In this work, we propose a method for generating a large
dataset of 3D faces with labeled emotions. We also develop a deep convolutional
neural network(CNN) for 3D FER trained on 624,000 3D facial scans. The test
data comprises 208,000 3D facial scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eformer: Edge Enhancement based Transformer for Medical Image Denoising. (arXiv:2109.08044v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08044">
<div class="article-summary-box-inner">
<span><p>In this work, we present Eformer - Edge enhancement based transformer, a
novel architecture that builds an encoder-decoder network using transformer
blocks for medical image denoising. Non-overlapping window-based self-attention
is used in the transformer block that reduces computational requirements. This
work further incorporates learnable Sobel-Feldman operators to enhance edges in
the image and propose an effective way to concatenate them in the intermediate
layers of our architecture. The experimental analysis is conducted by comparing
deterministic learning and residual learning for the task of medical image
denoising. To defend the effectiveness of our approach, our model is evaluated
on the AAPM-Mayo Clinic Low-Dose CT Grand Challenge Dataset and achieves
state-of-the-art performance, $i.e.$, 43.487 PSNR, 0.0067 RMSE, and 0.9861
SSIM. We believe that our work will encourage more research in
transformer-based architectures for medical image denoising using residual
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotation Averaging in a Split Second: A Primal-Dual Method and a Closed-Form for Cycle Graphs. (arXiv:2109.08046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08046">
<div class="article-summary-box-inner">
<span><p>A cornerstone of geometric reconstruction, rotation averaging seeks the set
of absolute rotations that optimally explains a set of measured relative
orientations between them. In spite of being an integral part of bundle
adjustment and structure-from-motion, averaging rotations is both a non-convex
and high-dimensional optimization problem. In this paper, we address it from a
maximum likelihood estimation standpoint and make a twofold contribution.
Firstly, we set forth a novel initialization-free primal-dual method which we
show empirically to converge to the global optimum. Further, we derive what is
to our knowledge, the first optimal closed-form solution for rotation averaging
in cycle graphs and contextualize this result within spectral graph theory. Our
proposed methods achieve a significant gain both in precision and performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Raising context awareness in motion forecasting. (arXiv:2109.08048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08048">
<div class="article-summary-box-inner">
<span><p>Learning-based trajectory prediction models have encountered great success,
with the promise of leveraging contextual information in addition to motion
history. Yet, we find that state-of-the-art forecasting methods tend to overly
rely on the agent's dynamics, failing to exploit the semantic cues provided at
its input. To alleviate this issue, we introduce CAB, a motion forecasting
model equipped with a training procedure designed to promote the use of
semantic contextual information. We also introduce two novel metrics --
dispersion and convergence-to-range -- to measure the temporal consistency of
successive forecasts, which we found missing in standard metrics. Our method is
evaluated on the widely adopted nuScenes Prediction benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Machine Learning Framework for Automatic Prediction of Human Semen Motility. (arXiv:2109.08049v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08049">
<div class="article-summary-box-inner">
<span><p>In the field of reproductive health, a vital aspect for the detection of male
fertility issues is the analysis of human semen quality. Two factors of
importance are the morphology and motility of the sperm cells. While the former
describes defects in different parts of a spermatozoon, the latter measures the
efficient movement of cells. For many non-human species, so-called
Computer-Aided Sperm Analysis systems work well for assessing these
characteristics from microscopic video recordings but struggle with human sperm
samples which generally show higher degrees of debris and dead spermatozoa, as
well as lower overall sperm motility. Here, machine learning methods that
harness large amounts of training data to extract salient features could
support physicians with the detection of fertility issues or in vitro
fertilisation procedures. In this work, the overall motility of given sperm
samples is predicted with the help of a machine learning framework integrating
unsupervised methods for feature extraction with downstream regression models.
The models evaluated herein improve on the state-of-the-art for video-based
sperm-motility prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Visual Representation Learning for Fashion Compatibility. (arXiv:2109.08052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08052">
<div class="article-summary-box-inner">
<span><p>We consider the problem of complementary fashion prediction. Existing
approaches focus on learning an embedding space where fashion items from
different categories that are visually compatible are closer to each other.
However, creating such labeled outfits is intensive and also not feasible to
generate all possible outfit combinations, especially with large fashion
catalogs. In this work, we propose a semi-supervised learning approach where we
leverage large unlabeled fashion corpus to create pseudo-positive and
pseudo-negative outfits on the fly during training. For each labeled outfit in
a training batch, we obtain a pseudo-outfit by matching each item in the
labeled outfit with unlabeled items. Additionally, we introduce consistency
regularization to ensure that representation of the original images and their
transformations are consistent to implicitly incorporate colour and other
important attributes through self-supervision. We conduct extensive experiments
on Polyvore, Polyvore-D and our newly created large-scale Fashion Outfits
datasets, and show that our approach with only a fraction of labeled examples
performs on-par with completely supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Urdu text in natural scene images: a new dataset and preliminary text detection. (arXiv:2109.08060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08060">
<div class="article-summary-box-inner">
<span><p>Text detection in natural scene images for content analysis is an interesting
task. The research community has seen some great developments for
English/Mandarin text detection. However, Urdu text extraction in natural scene
images is a task not well addressed. In this work, firstly, a new dataset is
introduced for Urdu text in natural scene images. The dataset comprises of 500
standalone images acquired from real scenes. Secondly, the channel enhanced
Maximally Stable Extremal Region (MSER) method is applied to extract Urdu text
regions as candidates in an image. Two-stage filtering mechanism is applied to
eliminate non-candidate regions. In the first stage, text and noise are
classified based on their geometric properties. In the second stage, a support
vector machine classifier is trained to discard non-text candidate regions.
After this, text candidate regions are linked using centroid-based vertical and
horizontal distances. Text lines are further analyzed by a different classifier
based on HOG features to remove non-text regions. Extensive experimentation is
performed on the locally developed dataset to evaluate the performance. The
experimental results show good performance on test set images. The dataset will
be made available for research use. To the best of our knowledge, the work is
the first of its kind for the Urdu language and would provide a good dataset
for free research use and serve as a baseline performance on the task of Urdu
text extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invertable Frowns: Video-to-Video Facial Emotion Translation. (arXiv:2109.08061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08061">
<div class="article-summary-box-inner">
<span><p>We present Wav2Lip-Emotion, a video-to-video translation architecture that
modifies facial expressions of emotion in videos of speakers. Previous work
modifies emotion in images, uses a single image to produce a video with
animated emotion, or puppets facial expressions in videos with landmarks from a
reference video. However, many use cases such as modifying an actor's
performance in post-production, coaching individuals to be more animated
speakers, or touching up emotion in a teleconference require a video-to-video
translation approach. We explore a method to maintain speakers' lip movements,
identity, and pose while translating their expressed emotion. Our approach
extends an existing multi-modal lip synchronization architecture to modify the
speaker's emotion using L1 reconstruction and pre-trained emotion objectives.
We also propose a novel automated emotion evaluation approach and corroborate
it with a user study. These find that we succeed in modifying emotion while
maintaining lip synchronization. Visual quality is somewhat diminished, with a
trade off between greater emotion modification and visual quality between model
variants. Nevertheless, we demonstrate (1) that facial expressions of emotion
can be modified with nothing other than L1 reconstruction and pre-trained
emotion objectives and (2) that our automated emotion evaluation approach
aligns with human judgements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisUnknown: Distilling Unknown Factors for Disentanglement Learning. (arXiv:2109.08090v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08090">
<div class="article-summary-box-inner">
<span><p>Disentangling data into interpretable and independent factors is critical for
controllable generation tasks. With the availability of labeled data,
supervision can help enforce the separation of specific factors as expected.
However, it is often expensive or even impossible to label every single factor
to achieve fully-supervised disentanglement. In this paper, we adopt a general
setting where all factors that are hard to label or identify are encapsulated
as a single unknown factor. Under this setting, we propose a flexible
weakly-supervised multi-factor disentanglement framework DisUnknown, which
Distills Unknown factors for enabling multi-conditional generation regarding
both labeled and unknown factors. Specifically, a two-stage training approach
is adopted to first disentangle the unknown factor with an effective and robust
training method, and then train the final generator with the proper
disentanglement of all labeled factors utilizing the unknown distillation. To
demonstrate the generalization capacity and scalability of our method, we
evaluate it on multiple benchmark datasets qualitatively and quantitatively and
further apply it to various real-world applications on complicated datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aesthetics and neural network image representations. (arXiv:2109.08103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08103">
<div class="article-summary-box-inner">
<span><p>We analyze the spaces of images encoded by generative networks of the BigGAN
architecture. We find that generic multiplicative perturbations away from the
photo-realistic point often lead to images which appear as "artistic
renditions" of the corresponding objects. This demonstrates an emergence of
aesthetic properties directly from the structure of the photo-realistic
environment coupled with its neural network parametrization. Moreover,
modifying a deep semantic part of the neural network encoding leads to the
appearance of symbolic visual representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural \'{E}tendue Expander for Ultra-Wide-Angle High-Fidelity Holographic Display. (arXiv:2109.08123v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08123">
<div class="article-summary-box-inner">
<span><p>Holographic displays can generate light fields by dynamically modulating the
wavefront of a coherent beam of light using a spatial light modulator,
promising rich virtual and augmented reality applications. However, the limited
spatial resolution of existing dynamic spatial light modulators imposes a tight
bound on the diffraction angle. As a result, today's holographic displays
possess low \'{e}tendue, which is the product of the display area and the
maximum solid angle of diffracted light. The low \'{e}tendue forces a sacrifice
of either the field of view (FOV) or the display size. In this work, we lift
this limitation by presenting neural \'{e}tendue expanders. This new breed of
optical elements, which is learned from a natural image dataset, enables higher
diffraction angles for ultra-wide FOV while maintaining both a compact form
factor and the fidelity of displayed contents to human viewers. With neural
\'{e}tendue expanders, we achieve 64$\times$ \'{e}tendue expansion of natural
images with reconstruction quality (measured in PSNR) over 29dB on simulated
retinal-resolution images. As a result, the proposed approach with expansion
factor 64$\times$ enables high-fidelity ultra-wide-angle holographic projection
of natural images using an 8K-pixel SLM, resulting in a 18.5 mm eyebox size and
2.18 steradians FOV, covering 85\% of the human stereo FOV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Transformer Model for 3D Object Detection. (arXiv:2109.08141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08141">
<div class="article-summary-box-inner">
<span><p>We propose 3DETR, an end-to-end Transformer based object detection model for
3D point clouds. Compared to existing detection methods that employ a number of
3D-specific inductive biases, 3DETR requires minimal modifications to the
vanilla Transformer block. Specifically, we find that a standard Transformer
with non-parametric queries and Fourier positional embeddings is competitive
with specialized architectures that employ libraries of 3D-specific operators
with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and
easy to implement, enabling further improvements by incorporating 3D domain
knowledge. Through extensive experiments, we show 3DETR outperforms the
well-established and highly optimized VoteNet baselines on the challenging
ScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks
beyond detection, and can serve as a building block for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation. (arXiv:1904.03215v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.03215">
<div class="article-summary-box-inner">
<span><p>Deep learning has enabled impressive progress in the accuracy of semantic
segmentation. Yet, the ability to estimate uncertainty and detect failure is
key for safety-critical applications like autonomous driving. Existing
uncertainty estimates have mostly been evaluated on simple tasks, and it is
unclear whether these methods generalize to more complex scenarios. We present
Fishyscapes, the first public benchmark for uncertainty estimation in a
real-world task of semantic segmentation for urban driving. It evaluates
pixel-wise uncertainty estimates towards the detection of anomalous objects in
front of the vehicle. We~adapt state-of-the-art methods to recent semantic
segmentation models and compare approaches based on softmax confidence,
Bayesian learning, and embedding density. Our results show that anomaly
detection is far from solved even for ordinary situations, while our benchmark
allows measuring advancements beyond the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Out-of-Distribution Detection with Divergence Guarantee in Deep Generative Models. (arXiv:2002.03328v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03328">
<div class="article-summary-box-inner">
<span><p>Recent research has revealed that deep generative models including flow-based
models and Variational autoencoders may assign higher likelihood to
out-of-distribution (OOD) data than in-distribution (ID) data. However, we
cannot sample out OOD data from the model. This counterintuitive phenomenon has
not been satisfactorily explained. In this paper, we prove theorems to
investigate the divergences in flow-based model and give two explanations to
the above phenomenon from divergence and geometric perspectives, respectively.
Based on our analysis, we propose two group anomaly detection methods.
Furthermore, we decompose the KL divergence and propose a point-wise anomaly
detection method. We have conducted extensive experiments on prevalent
benchmarks to evaluate our methods. For group anomaly detection (GAD), our
method can achieve near 100\% AUROC on all problems and has robustness against
data manipulations. On the contrary, the state-of-the-art (SOTA) GAD method
performs not better than random guessing for challenging problems and can be
attacked by data manipulation in almost all cases. For point-wise anomaly
detection (PAD), our method is comparable to the SOTA PAD method on one
category of problems and outperforms the baseline significantly on another
category of problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular, One-stage, Regression of Multiple 3D People. (arXiv:2008.12272v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12272">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the regression of multiple 3D people from a single RGB
image. Existing approaches predominantly follow a multi-stage pipeline that
first detects people in bounding boxes and then independently regresses their
3D body meshes. In contrast, we propose to Regress all meshes in a One-stage
fashion for Multiple 3D People (termed ROMP). The approach is conceptually
simple, bounding box-free, and able to learn a per-pixel representation in an
end-to-end manner. Our method simultaneously predicts a Body Center heatmap and
a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel
level. Through a body-center-guided sampling process, the body mesh parameters
of all people in the image are easily extracted from the Mesh Parameter map.
Equipped with such a fine-grained representation, our one-stage framework is
free of the complex multi-stage process and more robust to occlusion. Compared
with state-of-the-art methods, ROMP achieves superior performance on the
challenging multi-person benchmarks, including 3DPW and CMU Panoptic.
Experiments on crowded/occluded datasets demonstrate the robustness under
various types of occlusion. The released code is the first real-time
implementation of monocular multi-person 3D mesh regression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Biases and Effectiveness of Content-Style Disentanglement. (arXiv:2008.12378v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12378">
<div class="article-summary-box-inner">
<span><p>A recent spate of state-of-the-art semi- and un-supervised solutions
disentangle and encode image "content" into a spatial tensor and image
appearance or "style" into a vector, to achieve good performance in spatially
equivariant tasks (e.g. image-to-image translation). To achieve this, they
employ different model design, learning objective, and data biases. While
considerable effort has been made to measure disentanglement in vector
representations, and assess its impact on task performance, such analysis for
(spatial) content - style disentanglement is lacking. In this paper, we conduct
an empirical study to investigate the role of different biases in content-style
disentanglement settings and unveil the relationship between the degree of
disentanglement and task performance. In particular, we consider the setting
where we: (i) identify key design choices and learning constraints for three
popular content-style disentanglement models; (ii) relax or remove such
constraints in an ablation fashion; and (iii) use two metrics to measure the
degree of disentanglement and assess its effect on each task performance. Our
experiments reveal that there is a "sweet spot" between disentanglement, task
performance and - surprisingly - content interpretability, suggesting that
blindly forcing for higher disentanglement can hurt model performance and
content factors semanticness. Our findings, as well as the used
task-independent metrics, can be used to guide the design and selection of new
models for tasks where content-style representations are useful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pedestrian Trajectory Prediction with Convolutional Neural Networks. (arXiv:2010.05796v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05796">
<div class="article-summary-box-inner">
<span><p>Predicting the future trajectories of pedestrians is a challenging problem
that has a range of application, from crowd surveillance to autonomous driving.
In literature, methods to approach pedestrian trajectory prediction have
evolved, transitioning from physics-based models to data-driven models based on
recurrent neural networks. In this work, we propose a new approach to
pedestrian trajectory prediction, with the introduction of a novel 2D
convolutional model. This new model outperforms recurrent models, and it
achieves state-of-the-art results on the ETH and TrajNet datasets. We also
present an effective system to represent pedestrian positions and powerful data
augmentation techniques, such as the addition of Gaussian noise and the use of
random rotations, which can be applied to any model. As an additional
exploratory analysis, we present experimental results on the inclusion of
occupancy methods to model social information, which empirically show that
these methods are ineffective in capturing social interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows. (arXiv:2101.05796v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05796">
<div class="article-summary-box-inner">
<span><p>The difficulty of obtaining paired data remains a major bottleneck for
learning image restoration and enhancement models for real-world applications.
Current strategies aim to synthesize realistic training data by modeling noise
and degradations that appear in real-world settings. We propose DeFlow, a
method for learning stochastic image degradations from unpaired data. Our
approach is based on a novel unpaired learning formulation for conditional
normalizing flows. We model the degradation process in the latent space of a
shared flow encoder-decoder network. This allows us to learn the conditional
distribution of a noisy image given the clean input by solely minimizing the
negative log-likelihood of the marginal distributions. We validate our DeFlow
formulation on the task of joint image restoration and super-resolution. The
models trained with the synthetic data generated by DeFlow outperform previous
learnable approaches on three recent datasets. Code and trained models are
available at: https://github.com/volflow/DeFlow
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Awareness Attention for Few-Shot Object Detection. (arXiv:2102.12152v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12152">
<div class="article-summary-box-inner">
<span><p>While recent progress has significantly boosted few-shot classification (FSC)
performance, few-shot object detection (FSOD) remains challenging for modern
learning systems. Existing FSOD systems follow FSC approaches, ignoring
critical issues such as spatial variability and uncertain representations, and
consequently result in low performance. Observing this, we propose a novel
\textbf{Dual-Awareness Attention (DAnA)} mechanism that enables networks to
adaptively interpret the given support images. DAnA transforms support images
into \textbf{query-position-aware} (QPA) features, guiding detection networks
precisely by assigning customized support information to each local region of
the query. In addition, the proposed DAnA component is flexible and adaptable
to multiple existing object detection frameworks. By adopting DAnA,
conventional object detection networks, Faster R-CNN and RetinaNet, which are
not designed explicitly for few-shot learning, reach state-of-the-art
performance in FSOD tasks. In comparison with previous methods, our model
significantly increases the performance by 47\% (+6.9 AP), showing remarkable
ability under various evaluation settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Field Convolutions for Surface CNNs. (arXiv:2104.03916v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03916">
<div class="article-summary-box-inner">
<span><p>We present a novel surface convolution operator acting on vector fields that
is based on a simple observation: instead of combining neighboring features
with respect to a single coordinate parameterization defined at a given point,
we have every neighbor describe the position of the point within its own
coordinate frame. This formulation combines intrinsic spatial convolution with
parallel transport in a scattering operation while placing no constraints on
the filters themselves, providing a definition of convolution that commutes
with the action of isometries, has increased descriptive potential, and is
robust to noise and other nuisance factors. The result is a rich notion of
convolution which we call field convolution, well-suited for CNNs on surfaces.
Field convolutions are flexible, straight-forward to incorporate into surface
learning frameworks, and their highly discriminating nature has cascading
effects throughout the learning pipeline. Using simple networks constructed
from residual field convolution blocks, we achieve state-of-the-art results on
standard benchmarks in fundamental geometry processing tasks, such as shape
classification, segmentation, correspondence, and sparse matching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NURBS-Diff: A differentiable programming module for NURBS. (arXiv:2104.14547v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14547">
<div class="article-summary-box-inner">
<span><p>Boundary representations (B-reps) using Non-Uniform Rational B-splines
(NURBS) are the de facto standard used in CAD, but their utility in deep
learning-based approaches is not well researched. We propose a differentiable
NURBS module to integrate the NURBS representation of CAD models with deep
learning methods. We mathematically define the derivatives of the NURBS curves
or surfaces with respect to the input parameters. These derivatives are used to
define an approximate Jacobian that can be used to perform the "backward"
evaluation used while training deep learning models. We have implemented our
NURBS module using GPU-accelerated algorithms and integrated it with PyTorch, a
popular deep learning framework. We demonstrate the efficacy of our NURBS
module in performing CAD operations such as curve or surface fitting and
surface offsetting. Further, we show its utility in deep learning for
unsupervised point cloud reconstruction. These examples show that our module
performs better for certain deep learning frameworks and can be directly
integrated with any deep-learning framework requiring NURBS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Pursuit of Knowledge: Discovering and Localizing Novel Categories using Dual Memory. (arXiv:2105.01652v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01652">
<div class="article-summary-box-inner">
<span><p>We tackle object category discovery, which is the problem of discovering and
localizing novel objects in a large unlabeled dataset. While existing methods
show results on datasets with less cluttered scenes and fewer object instances
per image, we present our results on the challenging COCO dataset. Moreover, we
argue that, rather than discovering new categories from scratch, discovery
algorithms can benefit from identifying what is already known and focusing
their attention on the unknown. We propose a method that exploits prior
knowledge about certain object types to discover new categories by leveraging
two memory modules, namely Working and Semantic memory. We show the performance
of our detector on the COCO minival dataset to demonstrate its in-the-wild
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EchoCP: An Echocardiography Dataset in Contrast Transthoracic Echocardiography for Patent Foramen Ovale Diagnosis. (arXiv:2105.08267v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08267">
<div class="article-summary-box-inner">
<span><p>Patent foramen ovale (PFO) is a potential separation between the septum,
primum and septum secundum located in the anterosuperior portion of the atrial
septum. PFO is one of the main factors causing cryptogenic stroke which is the
fifth leading cause of death in the United States. For PFO diagnosis, contrast
transthoracic echocardiography (cTTE) is preferred as being a more robust
method compared with others. However, the current PFO diagnosis through cTTE is
extremely slow as it is proceeded manually by sonographers on echocardiography
videos. Currently there is no publicly available dataset for this important
topic in the community. In this paper, we present EchoCP, as the first
echocardiography dataset in cTTE targeting PFO diagnosis.
</p>
<p>EchoCP consists of 30 patients with both rest and Valsalva maneuver videos
which covers various PFO grades. We further establish an automated baseline
method for PFO diagnosis based on the state-of-the-art cardiac chamber
segmentation technique, which achieves 0.89 average mean Dice score, but only
0.60/0.67 mean accuracies for PFO diagnosis, leaving large room for
improvement. We hope that the challenging EchoCP dataset can stimulate further
research and lead to innovative and generic solutions that would have an impact
in multiple domains. Our dataset is released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04144">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks may perform poorly when the test and train data
are from different domains. While this problem can be mitigated by using the
target domain data to align the source and target domain feature
representations, the target domain data may be unavailable due to privacy
concerns. Consequently, there is a need for methods that generalize well
without access to target domain data during training. In this work, we propose
an adversarial hallucination approach, which combines a class-wise
hallucination module and a semantic segmentation module. Since the segmentation
performance varies across different classes, we design a semantic-conditioned
style hallucination layer to adaptively stylize each class. The classwise
stylization parameters are generated from the semantic knowledge in the
segmentation probability maps of the source domain image. Both modules compete
adversarially, with the hallucination module generating increasingly
'difficult' style images to challenge the segmentation module. In response, the
segmentation module improves its performance as it is trained with generated
samples at an appropriate class-wise difficulty level. Experiments on state of
the art domain adaptation work demonstrate the efficacy of our proposed method
when no target domain data are available for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separating Boundary Points via Structural Regularization for Very Compact Clusters. (arXiv:2106.05430v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05430">
<div class="article-summary-box-inner">
<span><p>Clustering algorithms have significantly improved along with Deep Neural
Networks which provide effective representation of data. Existing methods are
built upon deep autoencoder and self-training process that leverages the
distribution of cluster assignments of samples. However, as the fundamental
objective of the autoencoder is focused on efficient data reconstruction, the
learnt space may be sub-optimal for clustering. Moreover, it requires highly
effective codes (i.e., representation) of data, otherwise the initial cluster
centers often cause stability issues during self-training. Many
state-of-the-art clustering algorithms use convolution operation to extract
efficient codes but their applications are limited to image data. In this
regard, we propose an end-to-end deep clustering algorithm, i.e., Very Compact
Clusters (VCC). VCC takes advantage of distributions of local relationships of
samples near the boundary of clusters, so that they can be properly separated
and pulled to cluster centers to form compact clusters. Experimental results on
various datasets illustrate that our proposed approach achieves competitive
clustering performance against most of the state-of-the-art clustering methods
for both image and non-image data, and its results can be easily qualitatively
seen in the learnt low-dimensional space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Meta-learning with Disentanglement for Domain-generalised Medical Image Segmentation. (arXiv:2106.13292v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13292">
<div class="article-summary-box-inner">
<span><p>Generalising deep models to new data from new centres (termed here domains)
remains a challenge. This is largely attributed to shifts in data statistics
(domain shifts) between source and unseen domains. Recently, gradient-based
meta-learning approaches where the training data are split into meta-train and
meta-test sets to simulate and handle the domain shifts during training have
shown improved generalisation performance. However, the current fully
supervised meta-learning approaches are not scalable for medical image
segmentation, where large effort is required to create pixel-wise annotations.
Meanwhile, in a low data regime, the simulated domain shifts may not
approximate the true domain shifts well across source and unseen domains. To
address this problem, we propose a novel semi-supervised meta-learning
framework with disentanglement. We explicitly model the representations related
to domain shifts. Disentangling the representations and combining them to
reconstruct the input image allows unlabeled data to be used to better
approximate the true domain shifts for meta-learning. Hence, the model can
achieve better generalisation performance, especially when there is a limited
amount of labeled data. Experiments show that the proposed method is robust on
different segmentation tasks and achieves state-of-the-art generalisation
performance on two public benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals. (arXiv:2106.15004v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15004">
<div class="article-summary-box-inner">
<span><p>Accurately predicting the future motion of surrounding vehicles requires
reasoning about the inherent uncertainty in driving behavior. This uncertainty
can be loosely decoupled into lateral (e.g., keeping lane, turning) and
longitudinal (e.g., accelerating, braking). We present a novel method that
combines learned discrete policy rollouts with a focused decoder on subsets of
the lane graph. The policy rollouts explore different goals given current
observations, ensuring that the model captures lateral variability.
Longitudinal variability is captured by our latent variable model decoder that
is conditioned on various subsets of the lane graph. Our model achieves
state-of-the-art performance on the nuScenes motion prediction dataset, and
qualitatively demonstrates excellent scene compliance. Detailed ablations
highlight the importance of the policy rollouts and the decoder architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Micro-expression Recognition: A Survey. (arXiv:2107.02823v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02823">
<div class="article-summary-box-inner">
<span><p>Micro-expressions (MEs) are involuntary facial movements revealing people's
hidden feelings in high-stake situations and have practical importance in
medical treatment, national security, interrogations and many human-computer
interaction systems. Early methods for MER mainly based on traditional
appearance and geometry features. Recently, with the success of deep learning
(DL) in various fields, neural networks have received increasing interests in
MER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid
facial movements, leading to difficult data collection, thus have small-scale
datasets. DL based MER becomes challenging due to above ME characters. To date,
various DL approaches have been proposed to solve the ME issues and improve MER
performance. In this survey, we provide a comprehensive review of deep
micro-expression recognition (MER), including datasets, deep MER pipeline, and
the bench-marking of most influential methods. This survey defines a new
taxonomy for the field, encompassing all aspects of MER based on DL. For each
aspect, the basic approaches and advanced developments are summarized and
discussed. In addition, we conclude the remaining challenges and and potential
directions for the design of robust deep MER systems. To the best of our
knowledge, this is the first survey of deep MER methods, and this survey can
serve as a reference point for future MER research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Transfer and Interference in Multi-Domain Learning. (arXiv:2107.05445v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05445">
<div class="article-summary-box-inner">
<span><p>Humans are incredibly good at transferring knowledge from one domain to
another, enabling rapid learning of new tasks. Likewise, transfer learning has
enabled enormous success in many computer vision problems using pretraining.
However, the benefits of transfer in multi-domain learning, where a network
learns multiple tasks defined by different datasets, has not been adequately
studied. Learning multiple domains could be beneficial or these domains could
interfere with each other given limited network capacity. In this work, we
decipher the conditions where interference and knowledge transfer occur in
multi-domain learning. We propose new metrics disentangling interference and
transfer and set up experimental protocols. We further examine the roles of
network capacity, task grouping, and dynamic loss weighting in reducing
interference and facilitating transfer. We demonstrate our findings on the
CIFAR-100, MiniPlaces, and Tiny-ImageNet datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Coarse-to-Fine Approach in Single Image Deblurring. (arXiv:2108.05054v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05054">
<div class="article-summary-box-inner">
<span><p>Coarse-to-fine strategies have been extensively used for the architecture
design of single image deblurring networks. Conventional methods typically
stack sub-networks with multi-scale input images and gradually improve
sharpness of images from the bottom sub-network to the top sub-network,
yielding inevitably high computational costs. Toward a fast and accurate
deblurring network design, we revisit the coarse-to-fine strategy and present a
multi-input multi-output U-net (MIMO-UNet). The MIMO-UNet has three distinct
features. First, the single encoder of the MIMO-UNet takes multi-scale input
images to ease the difficulty of training. Second, the single decoder of the
MIMO-UNet outputs multiple deblurred images with different scales to mimic
multi-cascaded U-nets using a single U-shaped network. Last, asymmetric feature
fusion is introduced to merge multi-scale features in an efficient manner.
Extensive experiments on the GoPro and RealBlur datasets demonstrate that the
proposed network outperforms the state-of-the-art methods in terms of both
accuracy and computational complexity. Source code is available for research
purposes at https://github.com/chosj95/MIMO-UNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voxel-wise Cross-Volume Representation Learning for 3D Neuron Reconstruction. (arXiv:2108.06522v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06522">
<div class="article-summary-box-inner">
<span><p>Automatic 3D neuron reconstruction is critical for analysing the morphology
and functionality of neurons in brain circuit activities. However, the
performance of existing tracing algorithms is hinged by the low image quality.
Recently, a series of deep learning based segmentation methods have been
proposed to improve the quality of raw 3D optical image stacks by removing
noises and restoring neuronal structures from low-contrast background. Due to
the variety of neuron morphology and the lack of large neuron datasets, most of
current neuron segmentation models rely on introducing complex and
specially-designed submodules to a base architecture with the aim of encoding
better feature representations. Though successful, extra burden would be put on
computation during inference. Therefore, rather than modifying the base
network, we shift our focus to the dataset itself. The encoder-decoder backbone
used in most neuron segmentation models attends only intra-volume voxel points
to learn structural features of neurons but neglect the shared intrinsic
semantic features of voxels belonging to the same category among different
volumes, which is also important for expressive representation learning. Hence,
to better utilise the scarce dataset, we propose to explicitly exploit such
intrinsic features of voxels through a novel voxel-level cross-volume
representation learning paradigm on the basis of an encoder-decoder
segmentation model. Our method introduces no extra cost during inference.
Evaluated on 42 3D neuron images from BigNeuron project, our proposed method is
demonstrated to improve the learning ability of the original segmentation model
and further enhancing the reconstruction performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Tutorial on Learning Disentangled Representations in the Imaging Domain. (arXiv:2108.12043v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12043">
<div class="article-summary-box-inner">
<span><p>Disentangled representation learning has been proposed as an approach to
learning general representations. This can be done in the absence of, or with
limited, annotations. A good general representation can be readily fine-tuned
for new target tasks using modest amounts of data, or even be used directly in
unseen domains achieving remarkable performance in the corresponding task. This
alleviation of the data and annotation requirements offers tantalising
prospects for tractable and affordable applications in computer vision and
healthcare. Finally, disentangled representations can offer model
explainability and can help us understand the underlying causal relations of
the factors of variation, increasing their suitability for real-world
deployment. In this tutorial paper, we will offer an overview of the
disentangled representation learning, its building blocks and criteria, and
discuss applications in computer vision and medical imaging. We conclude our
tutorial by presenting the identified opportunities for the integration of
recent machine learning advances into disentanglement, as well as the remaining
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Cloud Pre-training by Mixing and Disentangling. (arXiv:2109.00452v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00452">
<div class="article-summary-box-inner">
<span><p>The annotation for large-scale point clouds is still time-consuming and
unavailable for many real-world tasks. Point cloud pre-training is one
potential solution for obtaining a scalable model for fast adaptation.
Therefore, in this paper, we investigate a new self-supervised learning
approach, called Mixing and Disentangling (MD), for point cloud pre-training.
As the name implies, we explore how to separate the original point cloud from
the mixed point cloud, and leverage this challenging task as a pretext
optimization objective for model training. Considering the limited training
data in the original dataset, which is much less than prevailing ImageNet, the
mixing process can efficiently generate more high-quality samples. We build one
baseline network to verify our intuition, which simply contains two modules,
encoder and decoder. Given a mixed point cloud, the encoder is first
pre-trained to extract the semantic embedding. Then an instance-adaptive
decoder is harnessed to disentangle the point clouds according to the
embedding. Albeit simple, the encoder is inherently able to capture the point
cloud keypoints after training and can be fast adapted to downstream tasks
including classification and segmentation by the pre-training and fine-tuning
paradigm. Extensive experiments on two datasets show that the encoder + ours
(MD) significantly surpasses that of the encoder trained from scratch and
converges quickly. In ablation studies, we further study the effect of each
component and discuss the advantages of the proposed self-supervised learning
strategy. We hope this self-supervised learning attempt on point clouds can
pave the way for reducing the deeply-learned model dependence on large-scale
labeled data and saving a lot of annotation costs in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ErfAct: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04386">
<div class="article-summary-box-inner">
<span><p>An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct-1 and ErfAct-2. Experiments suggest that the proposed functions improve
the network performance significantly compared to the widely used activations
like ReLU, Swish, and Mish. Replacing ReLU by ErfAct-1 and ErfAct-2, we have
5.21% and 5.04% improvement for top-1 accuracy on PreactResNet-34 network in
CIFAR100 dataset, 2.58% and 2.76% improvement for top-1 accuracy on
PreactResNet-34 network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiresolution Deep Implicit Functions for 3D Shape Representation. (arXiv:2109.05591v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05591">
<div class="article-summary-box-inner">
<span><p>We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical
representation that can recover fine geometry detail, while being able to
perform global operations such as shape completion. Our model represents a
complex 3D shape with a hierarchy of latent grids, which can be decoded into
different levels of detail and also achieve better accuracy. For shape
completion, we propose latent grid dropout to simulate partial data in the
latent space and therefore defer the completing functionality to the decoder
side. This along with our multires design significantly improves the shape
completion quality under decoder-only latent optimization. To the best of our
knowledge, MDIF is the first deep implicit function model that can at the same
time (1) represent different levels of detail and allow progressive decoding;
(2) support both encoder-decoder inference and decoder-only latent
optimization, and fulfill multiple applications; (3) perform detailed
decoder-only shape completion. Experiments demonstrate its superior performance
against prior art in various 3D reconstruction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07036">
<div class="article-summary-box-inner">
<span><p>Recently, DETR pioneered the solution of vision tasks with transformers, it
directly translates the image feature map into the object detection result.
Though effective, translating the full feature map can be costly due to
redundant computation on some area like the background. In this work, we
encapsulate the idea of reducing spatial redundancy into a novel poll and pool
(PnP) sampling module, with which we build an end-to-end PnP-DETR architecture
that adaptively allocates its computation spatially to be more efficient.
Concretely, the PnP module abstracts the image feature map into fine foreground
object feature vectors and a small number of coarse background contextual
feature vectors. The transformer models information interaction within the
fine-coarse feature space and translates the features into the detection
result. Moreover, the PnP-augmented model can instantly achieve various desired
trade-offs between performance and computation with a single model by varying
the sampled feature length, without requiring to train multiple models as
existing methods. Thus it offers greater flexibility for deployment in diverse
scenarios with varying computation constraint. We further validate the
generalizability of the PnP module on panoptic segmentation and the recent
transformer-based image recognition model ViT and show consistent efficiency
gain. We believe our method makes a step for efficient visual analysis with
transformers, wherein spatial redundancy is commonly observed. Code will be
available at \url{https://github.com/twangnh/pnp-detr}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGRNet: Adaptive Graph Representation Learning and Reasoning for Face Parsing. (arXiv:2101.07034v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07034">
<div class="article-summary-box-inner">
<span><p>Face parsing infers a pixel-wise label to each facial component, which has
drawn much attention recently.Previous methods have shown their success in face
parsing, which however overlook the correlation among facial components.As a
matter of fact, the component-wise relationship is a critical clue in
discriminating ambiguous pixels in facial area.To address this issue, we
propose adaptive graph representation learning and reasoning over facial
components, aiming to learn representative vertices that describe each
component, exploit the component-wise relationship and thereby produce accurate
parsing results against ambiguity. In particular, we devise an adaptive and
differentiable graph abstraction method to represent the components on a graph
via pixel-to-vertex projection under the initial condition of a predicted
parsing map, where pixel features within a certain facial region are aggregated
onto a vertex. Further, we explicitly incorporate the image edge as a prior in
the model, which helps to discriminate edge and non-edge pixels during the
projection, thus leading to refined parsing results along the edges.Then, our
model learns and reasons over the relations among components by propagating
information across vertices on the graph. Finally, the refined vertex features
are projected back to pixel grids for the prediction of the final parsing
map.To train our model, we propose a discriminative loss to penalize small
distances between vertices in the feature space, which leads to distinct
vertices with strong semantics. Experimental results show the superior
performance of the proposed model on multiple face parsing datasets, along with
the validation on the human parsing task to demonstrate the generalizability of
our model.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-17 23:02:35.436742485 UTC">2021-09-17 23:02:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>