<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-27T01:30:00Z">01-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Convex Polytope Modelling for Unsupervised Derivation of Semantic Structure for Data-efficient Natural Language Understanding. (arXiv:2201.10588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10588">
<div class="article-summary-box-inner">
<span><p>Popular approaches for Natural Language Understanding (NLU) usually rely on a
huge amount of annotated data or handcrafted rules, which is laborious and not
adaptive to domain extension. We recently proposed a
Convex-Polytopic-Model-based framework that shows great potential in
automatically extracting semantic patterns by exploiting the raw dialog corpus.
The extracted semantic patterns can be used to generate semantic frames, which
is essential in assisting NLU tasks. This paper further studies the CPM model
in depth and visualizes its high interpretability and transparency at various
levels. We show that this framework can exploit
</p>
<p>semantic-frame-related features in the corpus, reveal the underlying semantic
structure of the utterances, and boost the performance of the state-of-the-art
NLU model with minimal supervision. We conduct our experiments on the ATIS (Air
Travel Information System) corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOM-LM: Learning Generalizable Representations for HTML Documents. (arXiv:2201.10608v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10608">
<div class="article-summary-box-inner">
<span><p>HTML documents are an important medium for disseminating information on the
Web for human consumption. An HTML document presents information in multiple
text formats including unstructured text, structured key-value pairs, and
tables. Effective representation of these documents is essential for machine
understanding to enable a wide range of applications, such as Question
Answering, Web Search, and Personalization. Existing work has either
represented these documents using visual features extracted by rendering them
in a browser, which is typically computationally expensive, or has simply
treated them as plain text documents, thereby failing to capture useful
information presented in their HTML structure. We argue that the text and HTML
structure together convey important semantics of the content and therefore
warrant a special treatment for their representation learning. In this paper,
we introduce a novel representation learning approach for web pages, dubbed
DOM-LM, which addresses the limitations of existing approaches by encoding both
text and DOM tree structure with a transformer-based encoder and learning
generalizable representations for HTML documents via self-supervised
pre-training. We evaluate DOM-LM on a variety of webpage understanding tasks,
including Attribute Extraction, Open Information Extraction, and Question
Answering. Our extensive experiments show that DOM-LM consistently outperforms
all baselines designed for these tasks. In particular, DOM-LM demonstrates
better generalization performance both in few-shot and zero-shot settings,
making it attractive for making it suitable for real-world application settings
with limited labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The ABBE Corpus: Animate Beings Being Emotional. (arXiv:2201.10618v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10618">
<div class="article-summary-box-inner">
<span><p>Emotion detection is an established NLP task of demonstrated utility for text
understanding. However, basic emotion detection leaves out key information,
namely, who is experiencing the emotion in question. For example, it may be the
author, the narrator, or a character; or the emotion may correspond to
something the audience is supposed to feel, or even be unattributable to a
specific being, e.g., when emotions are being discussed per se. We provide the
ABBE corpus -- Animate Beings Being Emotional -- a new double-annotated corpus
of texts that captures this key information for one class of emotion
experiencer, namely, animate beings in the world described by the text. Such a
corpus is useful for developing systems that seek to model or understand this
specific type of expressed emotion. Our corpus contains 30 chapters, comprising
134,513 words, drawn from the Corpus of English Novels, and contains 2,010
unique emotion expressions attributable to 2,227 animate beings. The emotion
expressions are categorized according to Plutchik's 8-category emotion model,
and the overall inter-annotator agreement for the annotations was 0.83 Cohen's
Kappa, indicating excellent agreement. We describe in detail our annotation
scheme and procedure, and also release the corpus for use by other researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Strategy for Multilingual Grammatical Error Correction with Pre-trained Cross-Lingual Language Model. (arXiv:2201.10707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10707">
<div class="article-summary-box-inner">
<span><p>Synthetic data construction of Grammatical Error Correction (GEC) for
non-English languages relies heavily on human-designed and language-specific
rules, which produce limited error-corrected patterns. In this paper, we
propose a generic and language-independent strategy for multilingual GEC, which
can train a GEC system effectively for a new non-English language with only two
easy-to-access resources: 1) a pretrained cross-lingual language model (PXLM)
and 2) parallel translation data between English and the language. Our approach
creates diverse parallel GEC data without any language-specific operations by
taking the non-autoregressive translation generated by PXLM and the gold
translation as error-corrected sentence pairs. Then, we reuse PXLM to
initialize the GEC model and pretrain it with the synthetic data generated by
itself, which yields further improvement. We evaluate our approach on three
public benchmarks of GEC in different languages. It achieves the
state-of-the-art results on the NLPCC 2018 Task 2 dataset (Chinese) and obtains
competitive performance on Falko-Merlin (German) and RULEC-GEC (Russian).
Further analysis demonstrates that our data construction method is
complementary to rule-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models. (arXiv:2201.10716v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10716">
<div class="article-summary-box-inner">
<span><p>Neural network models have achieved state-of-the-art performance on
grapheme-to-phoneme (G2P) conversion. However, their performance relies on
large-scale pronunciation dictionaries, which may not be available for a lot of
languages. Inspired by the success of the pre-trained language model BERT, this
paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which
is built by self-supervised training on a large, language-specific word list
with only grapheme information. Furthermore, two approaches are developed to
incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e.,
fine-tuning GBERT or fusing GBERT into the Transformer model by attention.
Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean
datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our
GBERT-based G2P models under both medium-resource and low-resource data
conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Decoder Transformer For end-to-end Mandarin Chinese Speech Recognition with Pinyin and Character. (arXiv:2201.10792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10792">
<div class="article-summary-box-inner">
<span><p>End-to-end automatic speech recognition (ASR) has achieved promising results.
However, most existing end-to-end ASR methods neglect the use of specific
language characteristics. For Mandarin Chinese ASR tasks, pinyin and character
as writing and spelling systems respectively are mutual promotion in the
Mandarin Chinese language. Based on the above intuition, we investigate types
of related models that are suitable but not for joint pinyin-character ASR and
propose a novel Mandarin Chinese ASR model with dual-decoder Transformer
according to the characteristics of the pinyin transcripts and character
transcripts. Specifically, the joint pinyin-character layer-wise linear
interactive (LWLI) module and phonetic posteriorgrams adapter (PPGA) are
proposed to achieve inter-layer multi-level interaction by adaptively fusing
pinyin and character information. Furthermore, a two-stage training strategy is
proposed to make training more stable and faster convergence. The results on
the test sets of AISHELL-1 dataset show that the proposed
Speech-Pinyin-Character-Interaction (SPCI) model without a language model
achieves 9.85% character error rate (CER) on the test set, which is 17.71%
relative reduction compared to baseline models based on Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automated Question-Answering Framework Based on Evolution Algorithm. (arXiv:2201.10797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10797">
<div class="article-summary-box-inner">
<span><p>Building a deep learning model for a Question-Answering (QA) task requires a
lot of human effort, it may need several months to carefully tune various model
architectures and find a best one. It's even harder to find different excellent
models for multiple datasets. Recent works show that the best model structure
is related to the dataset used, and one single model cannot adapt to all tasks.
In this paper, we propose an automated Question-Answering framework, which
could automatically adjust network architecture for multiple datasets. Our
framework is based on an innovative evolution algorithm, which is stable and
suitable for multiple dataset scenario. The evolution algorithm for search
combine prior knowledge into initial population and use a performance estimator
to avoid inefficient mutation by predicting the performance of candidate model
architecture. The prior knowledge used in initial population could improve the
final result of the evolution algorithm. The performance estimator could
quickly filter out models with bad performance in population as the number of
trials increases, to speed up the convergence. Our framework achieves 78.9 EM
and 86.1 F1 on SQuAD 1.1, 69.9 EM and 72.5 F1 on SQuAD 2.0. On NewsQA dataset,
the found model achieves 47.0 EM and 62.9 F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Both the validity of the cultural tightness index and the association with creativity and order are spurious -- a comment on Jackson et al. (arXiv:2201.10812v1 [stat.AP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10812">
<div class="article-summary-box-inner">
<span><p>It was recently suggested in a study published in Nature Human Behaviour that
the historical loosening of American culture was associated with a trade-off
between higher creativity and lower order. To this end, Jackson et al. generate
a linguistic index of cultural tightness based on the Google Books Ngram corpus
and use this index to show that American norms loosened between 1800 and 2000.
While we remain agnostic toward a potential loosening of American culture and a
statistical association with creativity/order, we show here that the methods
used by Jackson et al. are neither suitable for testing the validity of the
index nor for establishing possible relationships with creativity/order.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeRetriever: Unimodal and Bimodal Contrastive Learning. (arXiv:2201.10866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10866">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose the CodeRetriever model, which combines the
unimodal and bimodal contrastive learning to train function-level code semantic
representations, specifically for the code search task. For unimodal
contrastive learning, we design a semantic-guided method to build positive code
pairs based on the documentation and function name. For bimodal contrastive
learning, we leverage the documentation and in-line comments of code to build
text-code pairs. Both contrastive objectives can fully leverage the large-scale
code corpus for pre-training. Experimental results on several public
benchmarks, (i.e., CodeSearch, CoSQA, etc.) demonstrate the effectiveness of
CodeRetriever in the zero-shot setting. By fine-tuning with domain/language
specified downstream data, CodeRetriever achieves the new state-of-the-art
performance with significant improvement over existing code pre-trained models.
We will make the code, model checkpoint, and constructed datasets publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Norwegian Parliamentary Speech Corpus. (arXiv:2201.10881v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10881">
<div class="article-summary-box-inner">
<span><p>The Norwegian Parliamentary Speech Corpus (NPSC) is a speech dataset with
recordings of meetings from Stortinget, the Norwegian parliament. It is the
first, publicly available dataset containing unscripted, Norwegian speech
designed for training of automatic speech recognition (ASR) systems. The
recordings are manually transcribed and annotated with language codes and
speakers, and there are detailed metadata about the speakers. The
transcriptions exist in both normalized and non-normalized form, and
non-standardized words are explicitly marked and annotated with standardized
equivalents. To test the usefulness of this dataset, we have compared an ASR
system trained on the NPSC with a baseline system trained on only
manuscript-read speech. These systems were tested on an independent dataset
containing spontaneous, dialectal speech. The NPSC-trained system performed
significantly better, with a 22.9% relative improvement in word error rate
(WER). Moreover, training on the NPSC is shown to have a "democratizing" effect
in terms of dialects, as improvements are generally larger for dialects with
higher WER from the baseline system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10890">
<div class="article-summary-box-inner">
<span><p>Human education system trains one student by multiple experts.
Mixture-of-experts (MoE) is a powerful sparse architecture including multiple
experts. However, sparse MoE model is hard to implement, easy to overfit, and
not hardware-friendly. In this work, inspired by human education model, we
propose a novel task, knowledge integration, to obtain a dense student model
(OneS) as knowledgeable as one sparse MoE. We investigate this task by
proposing a general training framework including knowledge gathering and
knowledge distillation. Specifically, we first propose Singular Value
Decomposition Knowledge Gathering (SVD-KG) to gather key knowledge from
different pretrained experts. We then refine the dense student model by
knowledge distillation to offset the noise from gathering. On ImageNet, our
OneS preserves $61.7\%$ benefits from MoE. OneS can achieve $78.4\%$ top-1
accuracy with only $15$M parameters. On four natural language processing
datasets, OneS obtains $88.2\%$ MoE benefits and outperforms SoTA by $51.7\%$
using the same architecture and training data. In addition, compared with the
MoE counterpart, OneS can achieve $3.7 \times$ inference speedup due to the
hardware-friendly architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pair-Level Supervised Contrastive Learning for Natural Language Inference. (arXiv:2201.10927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10927">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) is an increasingly important task for
natural language understanding, which requires one to infer the relationship
between the sentence pair (premise and hypothesis). Many recent works have used
contrastive learning by incorporating the relationship of the sentence pair
from NLI datasets to learn sentence representation. However, these methods only
focus on comparisons with sentence-level representations. In this paper, we
propose a Pair-level Supervised Contrastive Learning approach (PairSCL). We
adopt a cross attention module to learn the joint representations of the
sentence pairs. A contrastive learning objective is designed to distinguish the
varied classes of sentence pairs by pulling those in one class together and
pushing apart the pairs in other classes. We evaluate PairSCL on two public
datasets of NLI where the accuracy of PairSCL outperforms other methods by 2.1%
on average. Furthermore, our method outperforms the previous state-of-the-art
method on seven transfer tasks of text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning for Food Review and Recommendation. (arXiv:2201.10978v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10978">
<div class="article-summary-box-inner">
<span><p>Food reviews and recommendations have always been important for online food
service websites. However, reviewing and recommending food is not simple as it
is likely to be overwhelmed by disparate contexts and meanings. In this paper,
we use different deep learning approaches to address the problems of sentiment
analysis, automatic review tag generation, and retrieval of food reviews. We
propose to develop a web-based food review system at Nanyang Technological
University (NTU) named NTU Food Hunter, which incorporates different deep
learning approaches that help users with food selection. First, we implement
the BERT and LSTM deep learning models into the system for sentiment analysis
of food reviews. Then, we develop a Part-of-Speech (POS) algorithm to
automatically identify and extract adjective-noun pairs from the review content
for review tag generation based on POS tagging and dependency parsing. Finally,
we also train a RankNet model for the re-ranking of the retrieval results to
improve the accuracy in our Solr-based food reviews search system. The
experimental results show that our proposed deep learning approaches are
promising for the applications of real-world problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter-Demographer: A Flow-based Tool to Enrich Twitter Data. (arXiv:2201.10986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10986">
<div class="article-summary-box-inner">
<span><p>Twitter data have become essential to Natural Language Processing (NLP) and
social science research, driving various scientific discoveries in recent
years. However, the textual data alone are often not enough to conduct studies:
especially social scientists need more variables to perform their analysis and
control for various factors. How we augment this information, such as users'
location, age, or tweet sentiment, has ramifications for anonymity and
reproducibility, and requires dedicated effort. This paper describes
Twitter-Demographer, a simple, flow-based tool to enrich Twitter data with
additional information about tweets and users. Twitter-Demographer is aimed at
NLP practitioners and (computational) social scientists who want to enrich
their datasets with aggregated information, facilitating reproducibility, and
providing algorithmic privacy-by-design measures for pseudo-anonymity. We
discuss our design choices, inspired by the flow-based programming paradigm, to
use black-box components that can easily be chained together and extended. We
also analyze the ethical issues related to the use of this tool, and the
built-in measures to facilitate pseudo-anonymity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating language-biased image classification based on semantic representations. (arXiv:2201.11014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11014">
<div class="article-summary-box-inner">
<span><p>Humans show language-biased image recognition for a word-embedded image,
known as picture-word interference. Such interference depends on hierarchical
semantic categories and reflects that human language processing highly
interacts with visual processing. Similar to humans, recent artificial models
jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased
image classification. Exploring whether the bias leads to interferences similar
to those observed in humans can contribute to understanding how much the model
acquires hierarchical semantic representations from joint learning of language
and vision. The present study introduces methodological tools from the
cognitive science literature to assess the biases of artificial models.
Specifically, we introduce a benchmark task to test whether words superimposed
on images can distort the image classification across different category levels
and, if it can, whether the perturbation is due to the shared semantic
representation between language and vision. Our dataset is a set of
word-embedded images and consists of a mixture of natural image datasets and
hierarchical word labels with superordinate/basic category levels. Using this
benchmark test, we evaluate the CLIP model. We show that presenting words
distorts the image classification by the model across different category
levels, but the effect does not depend on the semantic relationship between
images and embedded words. This suggests that the semantic word representation
in the CLIP visual processing is not shared with the image representation,
although the word representation strongly dominates for word-embedded images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCAI-QReCC Shared Task on Conversational Question Answering. (arXiv:2201.11094v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11094">
<div class="article-summary-box-inner">
<span><p>Search-Oriented Conversational AI (SCAI) is an established venue that
regularly puts a spotlight upon the recent work advancing the field of
conversational search. SCAI'21 was organised as an independent on-line event
and featured a shared task on conversational question answering. Since all of
the participant teams experimented with answer generation models for this task,
we identified evaluation of answer correctness in this settings as the major
challenge and a current research gap. Alongside the automatic evaluation, we
conducted two crowdsourcing experiments to collect annotations for answer
plausibility and faithfulness. As a result of this shared task, the original
conversational QA dataset used for evaluation was further extended with
alternative correct answers produced by the participant systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11114">
<div class="article-summary-box-inner">
<span><p>Some neurons in deep networks specialize in recognizing highly specific
perceptual, structural, or semantic features of inputs. In computer vision,
techniques exist for identifying neurons that respond to individual concept
categories like colors, textures, and object classes. But these techniques are
limited in scope, labeling only a small subset of neurons and behaviors in any
network. Is a richer characterization of neuron-level computation possible? We
introduce a procedure (called MILAN, for mutual-information-guided linguistic
annotation of neurons) that automatically labels neurons with open-ended,
compositional, natural language descriptions. Given a neuron, MILAN generates a
description by searching for a natural language string that maximizes pointwise
mutual information with the image regions in which the neuron is active. MILAN
produces fine-grained descriptions that capture categorical, relational, and
logical structure in learned features. These descriptions obtain high agreement
with human-generated feature descriptions across a diverse set of model
architectures and tasks, and can aid in understanding and controlling learned
models. We highlight three applications of natural language neuron
descriptions. First, we use MILAN for analysis, characterizing the distribution
and importance of neurons selective for attribute, category, and relational
information in vision models. Second, we use MILAN for auditing, surfacing
neurons sensitive to protected categories like race and gender in models
trained on datasets intended to obscure these features. Finally, we use MILAN
for editing, improving robustness in an image classifier by deleting neurons
sensitive to text features spuriously correlated with class labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CsFEVER and CTKFacts: Czech Datasets for Fact Verification. (arXiv:2201.11115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11115">
<div class="article-summary-box-inner">
<span><p>In this paper we present two Czech datasets aimed for training automated
fact-checking machine learning models. Specifically we deal with the task of
assessment of a textual claim veracity w.r.t. to a (presumably) verified
corpus. The output of the system is the claim classification SUPPORTS or
REFUTES complemented with evidence documents or NEI (Not Enough Info) alone. In
the first place we publish CsFEVER of approximately 112k claims which is an
automatically generated Czech version of the well-known Wikipedia-based FEVER
dataset. We took a hybrid approach of machine translation and language
alignment, where the same method (and tools we provide) can be easily applied
to other languages. The second dataset CTKFacts of 3,097 claims is built on the
corpus of approximately two million Czech News Agency news reports. We present
an extended methodology based on the FEVER approach. Most notably, we describe
a method to automatically generate wider claim contexts (dictionaries) for
non-hyperlinked corpora. The datasets are analyzed for spurious cues, which are
annotation patterns leading to model overfitting. CTKFacts is further examined
for inter-annotator agreement, and a typology of common annotator errors is
extracted. Finally, we provide baseline models for all stages of the
fact-checking pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Representations for Modeling Variation in Speech. (arXiv:2011.12649v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12649">
<div class="article-summary-box-inner">
<span><p>Variation in speech is often quantified by comparing phonetic transcriptions
of the same utterance. However, manually transcribing speech is time-consuming
and error prone. As an alternative, therefore, we investigate the extraction of
acoustic embeddings from several self-supervised neural models. We use these
representations to compute word-based pronunciation differences between
non-native and native speakers of English, and between Norwegian dialect
speakers. For comparison with several earlier studies, we evaluate how well
these differences match human perception by comparing them with available human
judgements of similarity. We show that speech representations extracted from a
specific type of neural model (i.e. Transformers) lead to a better match with
human perception than two earlier approaches on the basis of phonetic
transcriptions and MFCC-based acoustic features. We furthermore find that
features from the neural models can generally best be extracted from one of the
middle hidden layers than from the final layer. We also demonstrate that neural
speech representations not only capture segmental differences, but also
intonational and durational differences that cannot adequately be represented
by a set of discrete symbols used in phonetic transcriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09966">
<div class="article-summary-box-inner">
<span><p>Sender-receiver interactions, and specifically persuasion games, are widely
researched in economic modeling and artificial intelligence. However, in the
classic persuasion games setting, the messages sent from the expert to the
decision-maker (DM) are abstract or well-structured signals rather than natural
language messages. This paper addresses the use of natural language in
persuasion games. For this purpose, we conduct an online repeated interaction
experiment. At each trial of the interaction, an informed expert aims to sell
an uninformed decision-maker a vacation in a hotel, by sending her a review
that describes the hotel. While the expert is exposed to several scored
reviews, the decision-maker observes only the single review sent by the expert,
and her payoff in case she chooses to take the hotel is a random draw from the
review score distribution available to the expert only. We also compare the
behavioral patterns in this experiment to the equivalent patterns in similar
experiments where the communication is based on the numerical values of the
reviews rather than the reviews' text, and observe substantial differences
which can be explained through an equilibrium analysis of the game. We consider
a number of modeling approaches for our verbal communication setup, differing
from each other in the model type (deep neural network vs. linear classifier),
the type of features used by the model (textual, behavioral or both) and the
source of the textual features (DNN-based vs. hand-crafted). Our results
demonstrate that given a prefix of the interaction sequence, our models can
predict the future decisions of the decision-maker, particularly when a
sequential modeling approach and hand-crafted textual features are applied.
Further analysis of the hand-crafted textual features allows us to make initial
observations about the aspects of text that drive decision making in our setup
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav2vec-Switch: Contrastive Learning from Original-noisy Speech Pairs for Robust Speech Recognition. (arXiv:2110.04934v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04934">
<div class="article-summary-box-inner">
<span><p>The goal of self-supervised learning (SSL) for automatic speech recognition
(ASR) is to learn good speech representations from a large amount of unlabeled
speech for the downstream ASR task. However, most SSL frameworks do not
consider noise robustness which is crucial for real-world applications. In this
paper we propose wav2vec-Switch, a method to encode noise robustness into
contextualized representations of speech via contrastive learning.
Specifically, we feed original-noisy speech pairs simultaneously into the
wav2vec 2.0 network. In addition to the existing contrastive learning task, we
switch the quantized representations of the original and noisy speech as
additional prediction targets of each other. By doing this, it enforces the
network to have consistent predictions for the original and noisy speech, thus
allows to learn contextualized representation with noise robustness. Our
experiments on synthesized and real noisy data show the effectiveness of our
method: it achieves 2.9--4.9% relative word error rate (WER) reduction on the
synthesized noisy LibriSpeech data without deterioration on the original data,
and 5.7% on CHiME-4 real 1-channel noisy data compared to a data augmentation
baseline even with a strong language model for decoding. Our results on CHiME-4
can match or even surpass those with well-designed speech enhancement
components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. (arXiv:2110.06800v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06800">
<div class="article-summary-box-inner">
<span><p>Zero/few-shot transfer to unseen services is a critical challenge in
task-oriented dialogue research. The Schema-Guided Dialogue (SGD) dataset
introduced a paradigm for enabling models to support an unlimited number of
services without additional data collection or re-training through the use of
schemas. Schemas describe APIs in natural language, which models consume to
understand the services they need to support. However, the impact of the choice
of language in these schemas on model performance remains unexplored. We
address this by releasing SGD-X, a benchmark for measuring the robustness of
dialogue systems to linguistic variations in schemas. SGD-X extends the SGD
dataset with crowdsourced variants for every schema, where variants are
semantically similar yet stylistically diverse. We evaluate two top-performing
dialogue state tracking models on SGD-X and observe that neither generalizes
well across schema variants, measured by joint goal accuracy and a novel metric
for measuring schema sensitivity. Finally, we present a simple model-agnostic
data augmentation method to improve schema robustness and zero-shot
generalization to unseen services.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Processing for Smart Healthcare. (arXiv:2110.15803v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15803">
<div class="article-summary-box-inner">
<span><p>Smart healthcare has achieved significant progress in recent years. Emerging
artificial intelligence (AI) technologies enable various smart applications
across various healthcare scenarios. As an essential technology powered by AI,
natural language processing (NLP) plays a key role in smart healthcare due to
its capability of analysing and understanding human language. In this work, we
review existing studies that concern NLP for smart healthcare from the
perspectives of technique and application. We focus on feature extraction and
modelling for various NLP tasks encountered in smart healthcare from a
technical point of view. In the context of smart healthcare applications
employing NLP techniques, the elaboration largely attends to representative
smart healthcare scenarios, including clinical practice, hospital management,
personal care, public health, and drug development. We further discuss the
limitations of current works and identify the directions for future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01140">
<div class="article-summary-box-inner">
<span><p>The rapid mutation of the influenza virus threatens public health.
Reassortment among viruses with different hosts can lead to a fatal pandemic.
However, it is difficult to detect the original host of the virus during or
after an outbreak as influenza viruses can circulate between different species.
Therefore, early and rapid detection of the viral host would help reduce the
further spread of the virus. We use various machine learning models with
features derived from the position-specific scoring matrix (PSSM) and features
learned from word embedding and word encoding to infer the origin host of
viruses. The results show that the performance of the PSSM-based model reaches
the MCC around 95%, and the F1 around 96%. The MCC obtained using the model
with word embedding is around 96%, and the F1 is around 97%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention. (arXiv:2201.10375v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10375">
<div class="article-summary-box-inner">
<span><p>With recent advancements in voice cloning, the performance of speech
synthesis for a target speaker has been rendered similar to the human level.
However, autoregressive voice cloning systems still suffer from text alignment
failures, resulting in an inability to synthesize long sentences. In this work,
we propose a variant of attention-based text-to-speech system that can
reproduce a target voice from a few seconds of reference speech and generalize
to very long utterances as well. The proposed system is based on three
independently trained components: a speaker encoder, synthesizer and universal
vocoder. Generalization to long utterances is realized using an energy-based
attention mechanism known as Dynamic Convolution Attention, in combination with
a set of modifications proposed for the synthesizer based on Tacotron 2.
Moreover, effective zero-shot speaker adaptation is achieved by conditioning
both the synthesizer and vocoder on a speaker encoder that has been pretrained
on a large corpus of diverse data. We compare several implementations of voice
cloning systems in terms of speech naturalness, speaker similarity, alignment
consistency and ability to synthesize long utterances, and conclude that the
proposed model can produce intelligible synthetic speech for extremely long
utterances, while preserving a high extent of naturalness and similarity for
short texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection. (arXiv:2201.10474v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10474">
<div class="article-summary-box-inner">
<span><p>Language models increasingly rely on massive web dumps for diverse text data.
However, these sources are rife with undesirable content. As such, resources
like Wikipedia, books, and newswire often serve as anchors for automatically
selecting web text most suitable for language modeling, a process typically
referred to as quality filtering. Using a new dataset of U.S. high school
newspaper articles -- written by students from across the country -- we
investigate whose language is preferred by the quality filter used for GPT-3.
We find that newspapers from larger schools, located in wealthier, educated,
and urban ZIP codes are more likely to be classified as high quality. We then
demonstrate that the filter's measurement of quality is unaligned with other
sensible metrics, such as factuality or literary acclaim. We argue that
privileging any corpus as high quality entails a language ideology, and more
care is needed to construct training corpora for language models, with better
transparency and justification for the inclusion or exclusion of various texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models. (arXiv:2201.08471v1 [cs.IR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08471">
<div class="article-summary-box-inner">
<span><p>The advent of transformer-based models such as BERT has led to the rise of
neural ranking models. These models have improved the effectiveness of
retrieval systems well beyond that of lexical term matching models such as
BM25. While monolingual retrieval tasks have benefited from large-scale
training collections such as MS MARCO and advances in neural architectures,
cross-language retrieval tasks have fallen behind these advancements. This
paper introduces ColBERT-X, a generalization of the ColBERT
multi-representation dense retrieval model that uses the XLM-RoBERTa (XLM-R)
encoder to support cross-language information retrieval (CLIR). ColBERT-X can
be trained in two ways. In zero-shot training, the system is trained on the
English MS MARCO collection, relying on the XLM-R encoder for cross-language
mappings. In translate-train, the system is trained on the MS MARCO English
queries coupled with machine translations of the associated MS MARCO passages.
Results on ad hoc document ranking tasks in several languages demonstrate
substantial and statistically significant improvements of these trained dense
retrieval models over traditional lexical CLIR baselines.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Jacobian Computation for Cumulative B-splines on SE(3) and Application to Continuous-Time Object Tracking. (arXiv:2201.10602v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10602">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a method that estimates the $SE(3)$ continuous
trajectories (orientation and translation) of the dynamic rigid objects present
in a scene, from multiple RGB-D views. Specifically, we fit the object
trajectories to cumulative B-Splines curves, which allow us to interpolate, at
any intermediate time stamp, not only their poses but also their linear and
angular velocities and accelerations. Additionally, we derive in this work the
analytical $SE(3)$ Jacobians needed by the optimization, being applicable to
any other approach that uses this type of curves. To the best of our knowledge
this is the first work that proposes 6-DoF continuous-time object tracking,
which we endorse with significant computational cost reduction thanks to our
analytical derivations. We evaluate our proposal in synthetic data and in a
public benchmark, showing competitive results in localization and significant
improvements in velocity estimation in comparison to discrete-time approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation via Semi-supervised Learning and Label Fusion. (arXiv:2201.10647v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10647">
<div class="article-summary-box-inner">
<span><p>Automatic methods to segment the vestibular schwannoma (VS) tumors and the
cochlea from magnetic resonance imaging (MRI) are critical to VS treatment
planning. Although supervised methods have achieved satisfactory performance in
VS segmentation, they require full annotations by experts, which is laborious
and time-consuming. In this work, we aim to tackle the VS and cochlea
segmentation problem in an unsupervised domain adaptation setting. Our proposed
method leverages both the image-level domain alignment to minimize the domain
divergence and semi-supervised training to further boost the performance.
Furthermore, we propose to fuse the labels predicted from multiple models via
noisy label correction. In the MICCAI 2021 crossMoDA challenge, our results on
the final evaluation leaderboard showed that our proposed method has achieved
promising segmentation performance with mean dice score of 79.9% and 82.5% and
ASSD of 1.29 mm and 0.18 mm for VS tumor and cochlea, respectively. The cochlea
ASSD achieved by our method has outperformed all other competing methods as
well as the supervised nnU-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Task Interaction Network for Multi-Task Learning. (arXiv:2201.10649v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10649">
<div class="article-summary-box-inner">
<span><p>Multitask learning (MTL) has recently gained a lot of popularity as a
learning paradigm that can lead to improved per-task performance while also
using fewer per-task model parameters compared to single task learning. One of
the biggest challenges regarding MTL networks involves how to share features
across tasks. To address this challenge, we propose the Attentive Task
Interaction Network (ATI-Net). ATI-Net employs knowledge distillation of the
latent features for each task, then combines the feature maps to provide
improved contextualized information to the decoder. This novel approach to
introducing knowledge distillation into an attention based multitask network
outperforms state of the art MTL baselines such as the standalone MTAN and
PAD-Net, with roughly the same number of model parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Visual Image: Automated Diagnosis of Pigmented Skin Lesions Combining Clinical Image Features with Patient Data. (arXiv:2201.10650v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10650">
<div class="article-summary-box-inner">
<span><p>kin cancer is considered one of the most common type of cancer in several
countries. Due to the difficulty and subjectivity in the clinical diagnosis of
skin lesions, Computer-Aided Diagnosis systems are being developed for assist
experts to perform more reliable diagnosis. The clinical analysis and diagnosis
of skin lesions relies not only on the visual information but also on the
context information provided by the patient. This work addresses the problem of
pigmented skin lesions detection from smartphones captured images. In addition
to the features extracted from images, patient context information was
collected to provide a more accurate diagnosis. The experiments showed that the
combination of visual features with context information improved final results.
Experimental results are very promising and comparable to experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SA-VQA: Structured Alignment of Visual and Semantic Representations for Visual Question Answering. (arXiv:2201.10654v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10654">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) attracts much attention from both industry
and academia. As a multi-modality task, it is challenging since it requires not
only visual and textual understanding, but also the ability to align
cross-modality representations. Previous approaches extensively employ
entity-level alignments, such as the correlations between the visual regions
and their semantic labels, or the interactions across question words and object
features. These attempts aim to improve the cross-modality representations,
while ignoring their internal relations. Instead, we propose to apply
structured alignments, which work with graph representation of visual and
textual content, aiming to capture the deep connections between the visual and
textual modalities. Nevertheless, it is nontrivial to represent and integrate
graphs for structured alignments. In this work, we attempt to solve this issue
by first converting different modality entities into sequential nodes and the
adjacency graph, then incorporating them for structured alignments. As
demonstrated in our experimental results, such a structured alignment improves
reasoning performance. In addition, our model also exhibits better
interpretability for each generated answer. The proposed model, without any
pretraining, outperforms the state-of-the-art methods on GQA dataset, and beats
the non-pretrained state-of-the-art methods on VQA-v2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGA-VQA: Multi-Granularity Alignment for Visual Question Answering. (arXiv:2201.10656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10656">
<div class="article-summary-box-inner">
<span><p>Learning to answer visual questions is a challenging task since the
multi-modal inputs are within two feature spaces. Moreover, reasoning in visual
question answering requires the model to understand both image and question,
and align them in the same space, rather than simply memorize statistics about
the question-answer pairs. Thus, it is essential to find component connections
between different modalities and within each modality to achieve better
attention. Previous works learned attention weights directly on the features.
However, the improvement is limited since these two modality features are in
two domains: image features are highly diverse, lacking structure and
grammatical rules as language, and natural language features have a higher
probability of missing detailed information. To better learn the attention
between visual and text, we focus on how to construct input stratification and
embed structural information to improve the alignment between different level
components. We propose Multi-Granularity Alignment architecture for Visual
Question Answering task (MGA-VQA), which learns intra- and inter-modality
correlations by multi-granularity alignment, and outputs the final result by
the decision fusion module. In contrast to previous works, our model splits
alignment into different levels to achieve learning better correlations without
needing additional data and annotations. The experiments on the VQA-v2 and GQA
datasets demonstrate that our model significantly outperforms non-pretrained
state-of-the-art methods on both datasets without extra pretraining data and
annotations. Moreover, it even achieves better results over the pre-trained
methods on GQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Neural Networks for Segmentation Understand Insideness?. (arXiv:2201.10664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10664">
<div class="article-summary-box-inner">
<span><p>The insideness problem is an aspect of image segmentation that consists of
determining which pixels are inside and outside a region. Deep Neural Networks
(DNNs) excel in segmentation benchmarks, but it is unclear if they have the
ability to solve the insideness problem as it requires evaluating long-range
spatial dependencies. In this paper, the insideness problem is analysed in
isolation, without texture or semantic cues, such that other aspects of
segmentation do not interfere in the analysis. We demonstrate that DNNs for
segmentation with few units have sufficient complexity to solve insideness for
any curve. Yet, such DNNs have severe problems with learning general solutions.
Only recurrent networks trained with small images learn solutions that
generalize well to almost any curve. Recurrent networks can decompose the
evaluation of long-range dependencies into a sequence of local operations, and
learning with small images alleviates the common difficulties of training
recurrent networks with a large number of unrolling steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Writer Recognition Using Off-line Handwritten Single Block Characters. (arXiv:2201.10665v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10665">
<div class="article-summary-box-inner">
<span><p>Block characters are often used when filling paper forms for a variety of
purposes. We investigate if there is biometric information contained within
individual digits of handwritten text. In particular, we use personal identity
numbers consisting of the six digits of the date of birth, DoB. We evaluate two
recognition approaches, one based on handcrafted features that compute contour
directional measurements, and another based on deep features from a ResNet50
model. We use a self-captured database of 317 individuals and 4920 written DoBs
in total. Results show the presence of identity-related information in a piece
of handwritten information as small as six digits with the DoB. We also analyze
the impact of the amount of enrolment samples, varying its number between one
and ten. Results with such small amount of data are promising. With ten
enrolment samples, the Top-1 accuracy with deep features is around 94%, and
reaches nearly 100% by Top-10. The verification accuracy is more modest, with
EER&gt;20%with any given feature and enrolment set size, showing that there is
still room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Adversarial Training for Semi-supervised Breast Mass Classification. (arXiv:2201.10675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10675">
<div class="article-summary-box-inner">
<span><p>This study aims to develop a novel computer-aided diagnosis (CAD) scheme for
mammographic breast mass classification using semi-supervised learning.
Although supervised deep learning has achieved huge success across various
medical image analysis tasks, its success relies on large amounts of
high-quality annotations, which can be challenging to acquire in practice. To
overcome this limitation, we propose employing a semi-supervised method, i.e.,
virtual adversarial training (VAT), to leverage and learn useful information
underlying in unlabeled data for better classification of breast masses.
Accordingly, our VAT-based models have two types of losses, namely supervised
and virtual adversarial losses. The former loss acts as in supervised
classification, while the latter loss aims at enhancing model robustness
against virtual adversarial perturbation, thus improving model
generalizability. To evaluate the performance of our VAT-based CAD scheme, we
retrospectively assembled a total of 1024 breast mass images, with equal number
of benign and malignant masses. A large CNN and a small CNN were used in this
investigation, and both were trained with and without the adversarial loss.
When the labeled ratios were 40% and 80%, VAT-based CNNs delivered the highest
classification accuracy of 0.740 and 0.760, respectively. The experimental
results suggest that the VAT-based CAD scheme can effectively utilize
meaningful knowledge from unlabeled data to better classify mammographic breast
mass images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimation of Spectral Biophysical Skin Properties from Captured RGB Albedo. (arXiv:2201.10695v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10695">
<div class="article-summary-box-inner">
<span><p>We present a new method to reconstruct and manipulate the spectral properties
of human skin from simple RGB albedo captures. To this end, we leverage Monte
Carlo light simulation over an accurate biophysical human skin layering model
parameterized by its most important components, thereby covering a plausible
range of colors. The practical complexity of the model allows us to learn the
inverse mapping from any albedo to its most probable associated skin
properties. Our technique can faithfully reproduce any skin type, being
expressive enough to automatically handle more challenging areas like the lips
or imperfections in the face. Thanks to the smoothness of the skin parameters
maps recovered, the albedo can be robustly edited through meaningful
biophysical properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Image Deblurring: A Survey. (arXiv:2201.10700v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10700">
<div class="article-summary-box-inner">
<span><p>Image deblurring is a classic problem in low-level computer vision, which
aims to recover a sharp image from a blurred input image. Recent advances in
deep learning have led to significant progress in solving this problem, and a
large number of deblurring networks have been proposed. This paper presents a
comprehensive and timely survey of recently published deep-learning based image
deblurring approaches, aiming to serve the community as a useful literature
review. We start by discussing common causes of image blur, introduce benchmark
datasets and performance metrics, and summarize different problem formulations.
Next we present a taxonomy of methods using convolutional neural networks (CNN)
based on architecture, loss function, and application, offering a detailed
review and comparison. In addition, we discuss some domain-specific deblurring
applications including face images, text, and stereo image pairs. We conclude
by discussing key challenges and future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection via Reverse Distillation from One-Class Embedding. (arXiv:2201.10703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10703">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) achieves promising results on the challenging
problem of unsupervised anomaly detection (AD).The representation discrepancy
of anomalies in the teacher-student (T-S) model provides essential evidence for
AD. However, using similar or identical architectures to build the teacher and
student models in previous studies hinders the diversity of anomalous
representations. To tackle this problem, we propose a novel T-S model
consisting of a teacher encoder and a student decoder and introduce a simple
yet effective "reverse distillation" paradigm accordingly. Instead of receiving
raw images directly, the student network takes teacher model's one-class
embedding as input and targets to restore the teacher's multiscale
representations. Inherently, knowledge distillation in this study starts from
abstract, high-level presentations to low-level features. In addition, we
introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S
model. The obtained compact embedding effectively preserves essential
information on normal patterns, but abandons anomaly perturbations. Extensive
experimentation on AD and one-class novelty detection benchmarks shows that our
method surpasses SOTA performance, demonstrating our proposed approach's
effectiveness and generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Data-Driven STAP Radar. (arXiv:2201.10712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10712">
<div class="article-summary-box-inner">
<span><p>Using an amalgamation of techniques from classical radar, computer vision,
and deep learning, we characterize our ongoing data-driven approach to
space-time adaptive processing (STAP) radar. We generate a rich example dataset
of received radar signals by randomly placing targets of variable strengths in
a predetermined region using RFView, a site-specific radio frequency modeling
and simulation tool developed by ISL Inc. For each data sample within this
region, we generate heatmap tensors in range, azimuth, and elevation of the
output power of a minimum variance distortionless response (MVDR) beamformer,
which can be replaced with a desired test statistic. These heatmap tensors can
be thought of as stacked images, and in an airborne scenario, the moving radar
creates a sequence of these time-indexed image stacks, resembling a video. Our
goal is to use these images and videos to detect targets and estimate their
locations, a procedure reminiscent of computer vision algorithms for object
detection$-$namely, the Faster Region-Based Convolutional Neural Network
(Faster R-CNN). The Faster R-CNN consists of a proposal generating network for
determining regions of interest (ROI), a regression network for positioning
anchor boxes around targets, and an object classification algorithm; it is
developed and optimized for natural images. Our ongoing research will develop
analogous tools for heatmap images of radar data. In this regard, we will
generate a large, representative adaptive radar signal processing database for
training and testing, analogous in spirit to the COCO dataset for natural
images. As a preliminary example, we present a regression network in this paper
for estimating target locations to demonstrate the feasibility of and
significant improvements provided by our data-driven approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Generation with Self Pixel-wise Normalization. (arXiv:2201.10725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10725">
<div class="article-summary-box-inner">
<span><p>Region-adaptive normalization (RAN) methods have been widely used in the
generative adversarial network (GAN)-based image-to-image translation
technique. However, since these approaches need a mask image to infer the
pixel-wise affine transformation parameters, they cannot be applied to the
general image generation models having no paired mask images. To resolve this
problem, this paper presents a novel normalization method, called self
pixel-wise normalization (SPN), which effectively boosts the generative
performance by performing the pixel-adaptive affine transformation without the
mask image. In our method, the transforming parameters are derived from a
self-latent mask that divides the feature map into the foreground and
background regions. The visualization of the self-latent masks shows that SPN
effectively captures a single object to be generated as the foreground. Since
the proposed method produces the self-latent mask without external data, it is
easily applicable in the existing generative models. Extensive experiments on
various datasets reveal that the proposed method significantly improves the
performance of image generation technique in terms of Frechet inception
distance (FID) and Inception score (IS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Vision Transformers with Only 2040 Images. (arXiv:2201.10728v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10728">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViTs) is emerging as an alternative to convolutional
neural networks (CNNs) for visual recognition. They achieve competitive results
with CNNs but the lack of the typical convolutional inductive bias makes them
more data-hungry than common CNNs. They are often pretrained on JFT-300M or at
least ImageNet and few works study training ViTs with limited data. In this
paper, we investigate how to train ViTs with limited data (e.g., 2040 images).
We give theoretical analyses that our method (based on parametric instance
discrimination) is superior to other methods in that it can capture both
feature alignment and instance similarities. We achieve state-of-the-art
results when training from scratch on 7 small datasets under various ViT
backbones. We also investigate the transferring ability of small datasets and
find that representations learned from small datasets can even improve
large-scale ImageNet training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating the Mutual Error Amplification for Semi-Supervised Object Detection. (arXiv:2201.10734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10734">
<div class="article-summary-box-inner">
<span><p>Semi-supervised object detection (SSOD) has achieved substantial progress in
recent years. However, it is observed that the performances of self-labeling
SSOD methods remain limited. Based on our experimental analysis, we reveal that
the reason behind such phenomenon lies in the mutual error amplification
between the pseudo labels and the trained detector. In this study, we propose a
Cross Teaching (CT) method, aiming to mitigate the mutual error amplification
by introducing a rectification mechanism of pseudo labels. CT simultaneously
trains multiple detectors with an identical structure but different parameter
initialization. In contrast to existing mutual teaching methods that directly
treat predictions from other detectors as pseudo labels, we propose the Label
Rectification Module (LRM), where the bounding boxes predicted by one detector
are rectified by using the corresponding boxes predicted by all other detectors
with higher confidence scores. In this way, CT can enhance the pseudo label
quality compared with self-labeling and existing mutual teaching methods, and
reasonably mitigate the mutual error amplification. Over two popular detector
structures, i.e., SSD300 and Faster-RCNN-FPN, the proposed CT method obtains
consistent improvements and outperforms the state-of-the-art SSOD methods by
2.2% absolute mAP improvements on the Pascal VOC and MS-COCO benchmarks. The
code is available at github.com/machengcheng2016/CrossTeaching-SSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Convolution Auto-encoder Network for Infrared and Visible Image Fusion. (arXiv:2201.10736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10736">
<div class="article-summary-box-inner">
<span><p>Background: Leaning redundant and complementary relationships is a critical
step in the human visual system. Inspired by the infrared cognition ability of
crotalinae animals, we design a joint convolution auto-encoder (JCAE) network
for infrared and visible image fusion. Methods: Our key insight is to feed
infrared and visible pair images into the network simultaneously and separate
an encoder stream into two private branches and one common branch, the private
branch works for complementary features learning and the common branch does for
redundant features learning. We also build two fusion rules to integrate
redundant and complementary features into their fused feature which are then
fed into the decoder layer to produce the final fused image. We detail the
structure, fusion rule and explain its multi-task loss function. Results: Our
JCAE network achieves good results in terms of both subjective effect and
objective evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10737">
<div class="article-summary-box-inner">
<span><p>Transformers have made remarkable progress towards modeling long-range
dependencies within the medical image analysis domain. However, current
transformer-based models suffer from several disadvantages: 1) existing methods
fail to capture the important features of the images due to the naive
tokenization scheme; 2) the models suffer from information loss because they
only consider single-scale feature representations; and 3) the segmentation
label maps generated by the models are not accurate enough without considering
rich semantic contexts and anatomical textures. In this work, we present
CA-GANformer, a novel type of generative adversarial transformers, for medical
image segmentation. First, we take advantage of the pyramid structure to
construct multi-scale representations and handle multi-scale variations. We
then design a novel class-aware transformer module to better learn the
discriminative regions of objects with semantic structures. Lastly, we utilize
an adversarial training strategy that boosts segmentation accuracy and
correspondingly allows a transformer-based discriminator to capture high-level
semantically correlated contents and low-level anatomical features. Our
experiments demonstrate that CA-GANformer dramatically outperforms previous
state-of-the-art transformer-based approaches on three benchmarks, obtaining
absolute 2.54%-5.88% improvements in Dice over previous models. Further
qualitative experiments provide a more detailed picture of the model's inner
workings, shed light on the challenges in improved transparency, and
demonstrate that transfer learning can greatly improve performance and reduce
the size of medical image datasets in training, making CA-GANformer a strong
starting point for downstream medical image analysis tasks. Codes and models
will be available to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infrared and visible image fusion based on Multi-State Contextual Hidden Markov Model. (arXiv:2201.10739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10739">
<div class="article-summary-box-inner">
<span><p>The traditional two-state hidden Markov model divides the high frequency
coefficients only into two states (large and small states). Such scheme is
prone to produce an inaccurate statistical model for the high frequency subband
and reduces the quality of fusion result. In this paper, a fine-grained
multi-state contextual hidden Markov model (MCHMM) is proposed for infrared and
visible image fusion in the non-subsampled Shearlet domain, which takes full
consideration of the strong correlations and level of details of NSST
coefficients. To this end, an accurate soft context variable is designed
correspondingly from the perspective of context correlation. Then, the
statistical features provided by MCHMM are utilized for the fusion of high
frequency subbands. To ensure the visual quality, a fusion strategy based on
the difference in regional energy is proposed as well for lowfrequency
subbands. Experimental results demonstrate that the proposed method can achieve
a superior performance compared with other fusion methods in both subjective
and objective aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution. (arXiv:2201.10747v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10747">
<div class="article-summary-box-inner">
<span><p>Unsupervised real world super resolution (USR) aims at restoring
high-resolution (HR) images given low-resolution (LR) inputs when paired data
is unavailable. One of the most common approaches is synthesizing noisy LR
images using GANs and utilizing a synthetic dataset to train the model in a
supervised manner. The goal of modeling the degradation generator is to
approximate the distribution of LR images given a HR image. Previous works
simply assumed the conditional distribution as a delta function and learned the
deterministic mapping from HR image to a LR image. Instead, we propose the
probabilistic degradation generator. Our degradation generator is a deep
hierarchical latent variable model and more suitable for modeling the complex
distribution. Furthermore, we train multiple degradation generators to enhance
the mode coverage and apply the novel collaborative learning. We outperform
several baselines on benchmark datasets in terms of PSNR and SSIM and
demonstrate the robustness of our method on unseen data distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Image Inpainting Using Semantic Guidance. (arXiv:2201.10753v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10753">
<div class="article-summary-box-inner">
<span><p>Image inpainting approaches have achieved significant progress with the help
of deep neural networks. However, existing approaches mainly focus on
leveraging the priori distribution learned by neural networks to produce a
single inpainting result or further yielding multiple solutions, where the
controllability is not well studied. This paper develops a novel image
inpainting approach that enables users to customize the inpainting result by
their own preference or memory. Specifically, our approach is composed of two
stages that utilize the prior of neural network and user's guidance to jointly
inpaint corrupted images. In the first stage, an autoencoder based on a novel
external spatial attention mechanism is deployed to produce reconstructed
features of the corrupted image and a coarse inpainting result that provides
semantic mask as the medium for user interaction. In the second stage, a
semantic decoder that takes the reconstructed features as prior is adopted to
synthesize a fine inpainting result guided by user's customized semantic mask,
so that the final inpainting result will share the same content with user's
guidance while the textures and colors reconstructed in the first stage are
preserved. Extensive experiments demonstrate the superiority of our approach in
terms of inpainting quality and controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Study of Image Classification Model Sensitivity to Foregrounds, Backgrounds, and Visual Attributes. (arXiv:2201.10766v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10766">
<div class="article-summary-box-inner">
<span><p>While datasets with single-label supervision have propelled rapid advances in
image classification, additional annotations are necessary in order to
quantitatively assess how models make predictions. To this end, for a subset of
ImageNet samples, we collect segmentation masks for the entire object and $18$
informative attributes. We call this dataset RIVAL10 (RIch Visual Attributes
with Localization), consisting of roughly $26k$ instances over $10$ classes.
Using RIVAL10, we evaluate the sensitivity of a broad set of models to noise
corruptions in foregrounds, backgrounds and attributes. In our analysis, we
consider diverse state-of-the-art architectures (ResNets, Transformers) and
training procedures (CLIP, SimCLR, DeiT, Adversarial Training). We find that,
somewhat surprisingly, in ResNets, adversarial training makes models more
sensitive to the background compared to foreground than standard training.
Similarly, contrastively-trained models also have lower relative foreground
sensitivity in both transformers and ResNets. Lastly, we observe intriguing
adaptive abilities of transformers to increase relative foreground sensitivity
as corruption level increases. Using saliency methods, we automatically
discover spurious features that drive the background sensitivity of models and
assess alignment of saliency maps with foregrounds. Finally, we quantitatively
study the attribution problem for neural features by comparing feature saliency
with ground-truth localization of semantic attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction. (arXiv:2201.10776v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10776">
<div class="article-summary-box-inner">
<span><p>Multi-contrast MRI (MC-MRI) captures multiple complementary imaging
modalities to aid in radiological decision-making. Given the need for lowering
the time cost of multiple acquisitions, current deep accelerated MRI
reconstruction networks focus on exploiting the redundancy between multiple
contrasts. However, existing works are largely supervised with paired data
and/or prohibitively expensive fully-sampled MRI sequences. Further,
reconstruction networks typically rely on convolutional architectures which are
limited in their capacity to model long-range interactions and may lead to
suboptimal recovery of fine anatomical detail. To these ends, we present a
dual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI
reconstruction. DSFormer develops a deep conditional cascade transformer (DCCT)
consisting of several cascaded Swin transformer reconstruction networks
(SwinRN) trained under two deep conditioning strategies to enable MC-MRI
information sharing. We further present a dual-domain (image and k-space)
self-supervised learning strategy for DCCT to alleviate the costs of acquiring
fully sampled training data. DSFormer generates high-fidelity reconstructions
which experimentally outperform current fully-supervised baselines. Moreover,
we find that DSFormer achieves nearly the same performance when trained either
with full supervision or with our proposed dual-domain self-supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASFD: Automatic and Scalable Face Detector. (arXiv:2201.10781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10781">
<div class="article-summary-box-inner">
<span><p>Along with current multi-scale based detectors, Feature Aggregation and
Enhancement (FAE) modules have shown superior performance gains for
cutting-edge object detection. However, these hand-crafted FAE modules show
inconsistent improvements on face detection, which is mainly due to the
significant distribution difference between its training and applying corpus,
COCO vs. WIDER Face. To tackle this problem, we essentially analyse the effect
of data distribution, and consequently propose to search an effective FAE
architecture, termed AutoFAE by a differentiable architecture search, which
outperforms all existing FAE modules in face detection with a considerable
margin. Upon the found AutoFAE and existing backbones, a supernet is further
built and trained, which automatically obtains a family of detectors under the
different complexity constraints. Extensive experiments conducted on popular
benchmarks, WIDER Face and FDDB, demonstrate the state-of-the-art
performance-efficiency trade-off for the proposed automatic and scalable face
detector (ASFD) family. In particular, our strong ASFD-D6 outperforms the best
competitor with AP 96.7/96.2/92.1 on WIDER Face test, and the lightweight
ASFD-D0 costs about 3.1 ms, more than 320 FPS, on the V100 GPU with
VGA-resolution images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised 3D Semantic Representation Learning for Vision-and-Language Navigation. (arXiv:2201.10788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10788">
<div class="article-summary-box-inner">
<span><p>In the Vision-and-Language Navigation task, the embodied agent follows
linguistic instructions and navigates to a specific goal. It is important in
many practical scenarios and has attracted extensive attention from both
computer vision and robotics communities. However, most existing works only use
RGB images but neglect the 3D semantic information of the scene. To this end,
we develop a novel self-supervised training framework to encode the voxel-level
3D semantic reconstruction into a 3D semantic representation. Specifically, a
region query task is designed as the pretext task, which predicts the presence
or absence of objects of a particular class in a specific 3D region. Then, we
construct an LSTM-based navigation model and train it with the proposed 3D
semantic representations and BERT language features on vision-language pairs.
Experiments show that the proposed approach achieves success rates of 68% and
66% on the validation unseen and test unseen splits of the R2R dataset
respectively, which are superior to most of RGB-based methods utilizing
vision-language transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism. (arXiv:2201.10801v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10801">
<div class="article-summary-box-inner">
<span><p>Attention mechanism has been widely believed as the key to success of vision
transformers (ViTs), since it provides a flexible and powerful way to model
spatial relationships. However, is the attention mechanism truly an
indispensable part of ViT? Can it be replaced by some other alternatives? To
demystify the role of attention mechanism, we simplify it into an extremely
simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift
operation. It does not contain any parameter or arithmetic calculation. The
only operation is to exchange a small portion of the channels between
neighboring features. Based on this simple operation, we construct a new
backbone network, namely ShiftViT, where the attention layers in ViT are
substituted by shift operations. Surprisingly, ShiftViT works quite well in
several mainstream tasks, e.g., classification, detection, and segmentation.
The performance is on par with or even better than the strong baseline Swin
Transformer. These results suggest that the attention mechanism might not be
the vital factor that makes ViT successful. It can be even replaced by a
zero-parameter operation. We should pay more attentions to the remaining parts
of ViT in the future work. Code is available at github.com/microsoft/SPACH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoDistill: Learning Spatial Features for Monocular 3D Object Detection. (arXiv:2201.10830v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10830">
<div class="article-summary-box-inner">
<span><p>3D object detection is a fundamental and challenging task for 3D scene
understanding, and the monocular-based methods can serve as an economical
alternative to the stereo-based or LiDAR-based methods. However, accurately
detecting objects in the 3D space from a single image is extremely difficult
due to the lack of spatial cues. To mitigate this issue, we propose a simple
and effective scheme to introduce the spatial information from LiDAR signals to
the monocular 3D detectors, without introducing any extra cost in the inference
phase. In particular, we first project the LiDAR signals into the image plane
and align them with the RGB images. After that, we use the resulting data to
train a 3D detector (LiDAR Net) with the same architecture as the baseline
model. Finally, this LiDAR Net can serve as the teacher to transfer the learned
knowledge to the baseline model. Experimental results show that the proposed
method can significantly boost the performance of the baseline model and ranks
the $1^{st}$ place among all monocular-based methods on the KITTI benchmark.
Besides, extensive ablation studies are conducted, which further prove the
effectiveness of each part of our designs and illustrate what the baseline
model has learned from the LiDAR Net. Our code will be released at
\url{https://github.com/monster-ghost/MonoDistill}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PARS: Pseudo-Label Aware Robust Sample Selection for Learning with Noisy Labels. (arXiv:2201.10836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10836">
<div class="article-summary-box-inner">
<span><p>Acquiring accurate labels on large-scale datasets is both time consuming and
expensive. To reduce the dependency of deep learning models on learning from
clean labeled data, several recent research efforts are focused on learning
with noisy labels. These methods typically fall into three design categories to
learn a noise robust model: sample selection approaches, noise robust loss
functions, or label correction methods. In this paper, we propose PARS:
Pseudo-Label Aware Robust Sample Selection, a hybrid approach that combines the
best from all three worlds in a joint-training framework to achieve robustness
to noisy labels. Specifically, PARS exploits all training samples using both
the raw/noisy labels and estimated/refurbished pseudo-labels via self-training,
divides samples into an ambiguous and a noisy subset via loss analysis, and
designs label-dependent noise-aware loss functions for both sets of filtered
labels. Results show that PARS significantly outperforms the state of the art
on extensive studies on the noisy CIFAR-10 and CIFAR-100 datasets, particularly
on challenging high-noise and low-resource settings. In particular, PARS
achieved an absolute 12% improvement in test accuracy on the CIFAR-100 dataset
with 90% symmetric label noise, and an absolute 27% improvement in test
accuracy when only 1/5 of the noisy labels are available during training as an
additional restriction. On a real-world noisy dataset, Clothing1M, PARS
achieves competitive results to the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Depth Estimation Setups from Stereo Endoscopy and Optical Tracking for Point Measurements. (arXiv:2201.10848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10848">
<div class="article-summary-box-inner">
<span><p>To support minimally-invasive intraoperative mitral valve repair,
quantitative measurements from the valve can be obtained using an infra-red
tracked stylus. It is desirable to view such manually measured points together
with the endoscopic image for further assistance. Therefore, hand-eye
calibration is required that links both coordinate systems and is a
prerequisite to project the points onto the image plane. A complementary
approach to this is to use a vision-based endoscopic stereo-setup to detect and
triangulate points of interest, to obtain the 3D coordinates. In this paper, we
aim to compare both approaches on a rigid phantom and two patient-individual
silicone replica which resemble the intraoperative scenario. The preliminary
results indicate that 3D landmark estimation, either labeled manually or
through partly automated detection with a deep learning approach, provides more
accurate triangulated depth measurements when performed with a tailored
image-based method than with stylus measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Knee Osteoarthritis Progression from Structural MRI using Deep Learning. (arXiv:2201.10849v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10849">
<div class="article-summary-box-inner">
<span><p>Accurate prediction of knee osteoarthritis (KOA) progression from structural
MRI has a potential to enhance disease understanding and support clinical
trials. Prior art focused on manually designed imaging biomarkers, which may
not fully exploit all disease-related information present in MRI scan. In
contrast, our method learns relevant representations from raw data end-to-end
using Deep Learning, and uses them for progression prediction. The method
employs a 2D CNN to process the data slice-wise and aggregate the extracted
features using a Transformer. Evaluated on a large cohort (n=4,866), the
proposed method outperforms conventional 2D and 3D CNN-based models and
achieves average precision of $0.58\pm0.03$ and ROC AUC of $0.78\pm0.01$. This
paper sets a baseline on end-to-end KOA progression prediction from structural
MRI. Our code is publicly available at
https://github.com/MIPT-Oulu/OAProgressionMR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing the diversity of representations learned by Bayesian neural networks. (arXiv:2201.10859v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10859">
<div class="article-summary-box-inner">
<span><p>Explainable artificial intelligence (XAI) aims to make learning machines less
opaque, and offers researchers and practitioners various tools to reveal the
decision-making strategies of neural networks. In this work, we investigate how
XAI methods can be used for exploring and visualizing the diversity of feature
representations learned by Bayesian neural networks (BNNs). Our goal is to
provide a global understanding of BNNs by making their decision-making
strategies a) visible and tangible through feature visualizations and b)
quantitatively measurable with a distance measure learned by contrastive
learning. Our work provides new insights into the posterior distribution in
terms of human-understandable feature information with regard to the underlying
decision-making strategies. Our main findings are the following: 1) global XAI
methods can be applied to explain the diversity of decision-making strategies
of BNN instances, 2) Monte Carlo dropout exhibits increased diversity in
feature representations compared to the multimodal posterior approximation of
MultiSWAG, 3) the diversity of learned feature representations highly
correlates with the uncertainty estimates, and 4) the inter-mode diversity of
the multimodal posterior decreases as the network width increases, while the
intra-mode diversity increases. Our findings are consistent with the recent
deep neural networks theory, providing additional intuitions about what the
theory implies in terms of humanly understandable concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Issues of TrueDepth Sensor Data for Computer Vision Tasks Across Different iPad Generations. (arXiv:2201.10865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10865">
<div class="article-summary-box-inner">
<span><p>In 2017 Apple introduced the TrueDepth sensor with the iPhone X release.
Although its primary use case is biometric face recognition, the exploitation
of accurate depth data for other computer vision tasks like segmentation,
portrait image generation and metric 3D reconstruction seems natural and lead
to the development of various applications. In this report, we investigate the
reliability of TrueDepth data - accessed through two different APIs - on
various devices including different iPhone and iPad generations and reveal two
different and significant issues on all tested iPads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransPPG: Two-stream Transformer for Remote Heart Rate Estimate. (arXiv:2201.10873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10873">
<div class="article-summary-box-inner">
<span><p>Non-contact facial video-based heart rate estimation using remote
photoplethysmography (rPPG) has shown great potential in many applications
(e.g., remote health care) and achieved creditable results in constrained
scenarios. However, practical applications require results to be accurate even
under complex environment with head movement and unstable illumination.
Therefore, improving the performance of rPPG in complex environment has become
a key challenge. In this paper, we propose a novel video embedding method that
embeds each facial video sequence into a feature map referred to as Multi-scale
Adaptive Spatial and Temporal Map with Overlap (MAST_Mop), which contains not
only vital information but also surrounding information as reference, which
acts as the mirror to figure out the homogeneous perturbations imposed on
foreground and background simultaneously, such as illumination instability.
Correspondingly, we propose a two-stream Transformer model to map the MAST_Mop
into heart rate (HR), where one stream follows the pulse signal in the facial
area while the other figures out the perturbation signal from the surrounding
region such that the difference of the two channels leads to adaptive noise
cancellation. Our approach significantly outperforms all current
state-of-the-art methods on two public datasets MAHNOB-HCI and VIPL-HR. As far
as we know, it is the first work with Transformer as backbone to capture the
temporal dependencies in rPPGs and apply the two stream scheme to figure out
the interference from backgrounds as mirror of the corresponding perturbation
on foreground signals for noise tolerating.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperparameter Optimization for COVID-19 Chest X-Ray Classification. (arXiv:2201.10885v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10885">
<div class="article-summary-box-inner">
<span><p>Despite the introduction of vaccines, Coronavirus disease (COVID-19) remains
a worldwide dilemma, continuously developing new variants such as Delta and the
recent Omicron. The current standard for testing is through polymerase chain
reaction (PCR). However, PCRs can be expensive, slow, and/or inaccessible to
many people. X-rays on the other hand have been readily used since the early
20th century and are relatively cheaper, quicker to obtain, and typically
covered by health insurance. With a careful selection of model,
hyperparameters, and augmentations, we show that it is possible to develop
models with 83% accuracy in binary classification and 64% in multi-class for
detecting COVID-19 infections from chest x-rays.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Student Knows All Experts Know: From Sparse to Dense. (arXiv:2201.10890v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10890">
<div class="article-summary-box-inner">
<span><p>Human education system trains one student by multiple experts.
Mixture-of-experts (MoE) is a powerful sparse architecture including multiple
experts. However, sparse MoE model is hard to implement, easy to overfit, and
not hardware-friendly. In this work, inspired by human education model, we
propose a novel task, knowledge integration, to obtain a dense student model
(OneS) as knowledgeable as one sparse MoE. We investigate this task by
proposing a general training framework including knowledge gathering and
knowledge distillation. Specifically, we first propose Singular Value
Decomposition Knowledge Gathering (SVD-KG) to gather key knowledge from
different pretrained experts. We then refine the dense student model by
knowledge distillation to offset the noise from gathering. On ImageNet, our
OneS preserves $61.7\%$ benefits from MoE. OneS can achieve $78.4\%$ top-1
accuracy with only $15$M parameters. On four natural language processing
datasets, OneS obtains $88.2\%$ MoE benefits and outperforms SoTA by $51.7\%$
using the same architecture and training data. In addition, compared with the
MoE counterpart, OneS can achieve $3.7 \times$ inference speedup due to the
hardware-friendly architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speeding up Heterogeneous Federated Learning with Sequentially Trained Superclients. (arXiv:2201.10899v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10899">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) allows training machine learning models in
privacy-constrained scenarios by enabling the cooperation of edge devices
without requiring local data sharing. This approach raises several challenges
due to the different statistical distribution of the local datasets and the
clients' computational heterogeneity. In particular, the presence of highly
non-i.i.d. data severely impairs both the performance of the trained neural
network and its convergence rate, increasing the number of communication rounds
requested to reach a performance comparable to that of the centralized
scenario. As a solution, we propose FedSeq, a novel framework leveraging the
sequential training of subgroups of heterogeneous clients, i.e. superclients,
to emulate the centralized paradigm in a privacy-compliant way. Given a fixed
budget of communication rounds, we show that FedSeq outperforms or match
several state-of-the-art federated algorithms in terms of final performance and
speed of convergence. Finally, our method can be easily integrated with other
approaches available in the literature. Empirical results show that combining
existing algorithms with FedSeq further improves its final performance and
convergence speed. We test our method on CIFAR-10 and CIFAR-100 and prove its
effectiveness in both i.i.d. and non-i.i.d. scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bayesian Based Deep Unrolling Algorithm for Single-Photon Lidar Systems. (arXiv:2201.10910v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10910">
<div class="article-summary-box-inner">
<span><p>Deploying 3D single-photon Lidar imaging in real world applications faces
multiple challenges including imaging in high noise environments. Several
algorithms have been proposed to address these issues based on statistical or
learning-based frameworks. Statistical methods provide rich information about
the inferred parameters but are limited by the assumed model correlation
structures, while deep learning methods show state-of-the-art performance but
limited inference guarantees, preventing their extended use in critical
applications. This paper unrolls a statistical Bayesian algorithm into a new
deep learning architecture for robust image reconstruction from single-photon
Lidar data, i.e., the algorithm's iterative steps are converted into neural
network layers. The resulting algorithm benefits from the advantages of both
statistical and learning based frameworks, providing best estimates with
improved network interpretability. Compared to existing learning-based
solutions, the proposed architecture requires a reduced number of trainable
parameters, is more robust to noise and mismodelling effects, and provides
richer information about the estimates including uncertainty measures. Results
on synthetic and real data show competitive results regarding the quality of
the inference and computational complexity when compared to state-of-the-art
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting 3D Adversarial Attacks with Attacking On Frequency. (arXiv:2201.10937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10937">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks. Recently, 3D adversarial attacks, especially adversarial attacks on
point clouds, have elicited mounting interest. However, adversarial point
clouds obtained by previous methods show weak transferability and are easy to
defend. To address these problems, in this paper we propose a novel point cloud
attack (dubbed AOF) that pays more attention on the low-frequency component of
point clouds. We combine the losses from point cloud and its low-frequency
component to craft adversarial samples. Extensive experiments validate that AOF
can improve the transferability significantly compared to state-of-the-art
(SOTA) attacks, and is more robust to SOTA 3D defense methods. Otherwise,
compared to clean point clouds, adversarial point clouds obtained by AOF
contain more deformation than outlier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Projective Urban Texturing. (arXiv:2201.10938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10938">
<div class="article-summary-box-inner">
<span><p>This paper proposes a method for automatic generation of textures for 3D city
meshes in immersive urban environments. Many recent pipelines capture or
synthesize large quantities of city geometry using scanners or procedural
modeling pipelines. Such geometry is intricate and realistic, however the
generation of photo-realistic textures for such large scenes remains a problem.
We propose to generate textures for input target 3D meshes driven by the
textural style present in readily available datasets of panoramic photos
capturing urban environments. Re-targeting such 2D datasets to 3D geometry is
challenging because the underlying shape, size, and layout of the urban
structures in the photos do not correspond to the ones in the target meshes.
Photos also often have objects (e.g., trees, vehicles) that may not even be
present in the target geometry.To address these issues we present a method,
called Projective Urban Texturing (PUT), which re-targets textural style from
real-world panoramic images to unseen urban meshes. PUT relies on contrastive
and adversarial training of a neural architecture designed for unpaired
image-to-texture translation. The generated textures are stored in a texture
atlas applied to the target 3D mesh geometry. To promote texture consistency,
PUT employs an iterative procedure in which texture synthesis is conditioned on
previously generated, adjacent textures. We demonstrate both quantitative and
qualitative evaluation of the generated textures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-based Video Reconstruction via Potential-assisted Spiking Neural Network. (arXiv:2201.10943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10943">
<div class="article-summary-box-inner">
<span><p>Neuromorphic vision sensor is a new bio-inspired imaging paradigm that
reports asynchronous, continuously per-pixel brightness changes called `events'
with high temporal resolution and high dynamic range. So far, the event-based
image reconstruction methods are based on artificial neural networks (ANN) or
hand-crafted spatiotemporal smoothing techniques. In this paper, we first
implement the image reconstruction work via fully spiking neural network (SNN)
architecture. As the bio-inspired neural networks, SNNs operating with
asynchronous binary spikes distributed over time, can potentially lead to
greater computational efficiency on event-driven hardware. We propose a novel
Event-based Video reconstruction framework based on a fully Spiking Neural
Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and
Membrane Potential (MP) neuron. We find that the spiking neurons have the
potential to store useful temporal information (memory) to complete such
time-dependent tasks. Furthermore, to better utilize the temporal information,
we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane
potential of spiking neuron. The proposed neuron is referred as Adaptive
Membrane Potential (AMP) neuron, which adaptively updates the membrane
potential according to the input spikes. The experimental results demonstrate
that our models achieve comparable performance to ANN-based models on IJRR,
MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are
19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN
architectures, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Deep Learning on Edge Devices through Filter Pruning and Knowledge Transfer. (arXiv:2201.10947v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10947">
<div class="article-summary-box-inner">
<span><p>Deep learning models have introduced various intelligent applications to edge
devices, such as image classification, speech recognition, and augmented
reality. There is an increasing need of training such models on the devices in
order to deliver personalized, responsive, and private learning. To address
this need, this paper presents a new solution for deploying and training
state-of-the-art models on the resource-constrained devices. First, the paper
proposes a novel filter-pruning-based model compression method to create
lightweight trainable models from large models trained in the cloud, without
much loss of accuracy. Second, it proposes a novel knowledge transfer method to
enable the on-device model to update incrementally in real time or near real
time using incremental learning on new data and enable the on-device model to
learn the unseen categories with the help of the in-cloud model in an
unsupervised fashion. The results show that 1) our model compression method can
remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy
of over 90% on CIFAR-10; 2) our knowledge transfer method enables the
compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good
accuracy on old categories; 3) it allows the compressed models to converge
within real time (three to six minutes) on the edge for incremental learning
tasks; 4) it enables the model to classify unseen categories of data (78.92%
Top-1 accuracy) that it is never trained with.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Tasks Siamese Transformer Framework for Building Damage Assessment. (arXiv:2201.10953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10953">
<div class="article-summary-box-inner">
<span><p>Accurate and fine-grained information about the extent of damage to buildings
is essential for humanitarian relief and disaster response. However, as the
most commonly used architecture in remote sensing interpretation tasks,
Convolutional Neural Networks (CNNs) have limited ability to model the
non-local relationship between pixels. Recently, Transformer architecture first
proposed for modeling long-range dependency in natural language processing has
shown promising results in computer vision tasks. Considering the frontier
advances of Transformer architecture in the computer vision field, in this
paper, we present the first attempt at designing a Transformer-based damage
assessment architecture (DamFormer). In DamFormer, a siamese Transformer
encoder is first constructed to extract non-local and representative deep
features from input multitemporal image-pairs. Then, a multitemporal fusion
module is designed to fuse information for downstream tasks. Finally, a
lightweight dual-tasks decoder aggregates multi-level features for final
prediction. To the best of our knowledge, it is the first time that such a deep
Transformer-based network is proposed for multitemporal remote sensing
interpretation tasks. The experimental results on the large-scale damage
assessment dataset xBD demonstrate the potential of the Transformer-based
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compose Diversified Prompts for Image Emotion Classification. (arXiv:2201.10963v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10963">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) represents the latest
incarnation of pre-trained vision-language models. Although CLIP has recently
shown its superior power on a wide range of downstream vision-language tasks
like Visual Question Answering, it is still underexplored for Image Emotion
Classification (IEC). Adapting CLIP to the IEC task has three significant
challenges, tremendous training objective gap between pretraining and IEC,
shared suboptimal and invariant prompts for all instances. In this paper, we
propose a general framework that shows how CLIP can be effectively applied to
IEC. We first introduce a prompt tuning method that mimics the pretraining
objective of CLIP and thus can leverage the rich image and text semantics
entailed in CLIP. Then we automatically compose instance-specific prompts by
conditioning them on the categories and image contents of instances,
diversifying prompts and avoiding suboptimal problems. Evaluations on six
widely-used affective datasets demonstrate that our proposed method outperforms
the state-of-the-art methods to a large margin (i.e., up to 9.29% accuracy gain
on EmotionROI dataset) on IEC tasks, with only a few parameters trained. Our
codes will be publicly available for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Robust are Discriminatively Trained Zero-Shot Learning Models?. (arXiv:2201.10972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10972">
<div class="article-summary-box-inner">
<span><p>Data shift robustness is an active research topic, however, it has been
primarily investigated from a fully supervised perspective, and robustness of
zero-shot learning (ZSL) models have been largely neglected. In this paper, we
present a novel analysis on the robustness of discriminative ZSL to image
corruptions. We leverage the well-known label embedding model and subject it to
a large set of common corruptions and defenses. In order to realize the
corruption analysis, we curate and release the first ZSL corruption robustness
datasets SUN-C, CUB-C and AWA2-C. We analyse our results by taking into account
the dataset characteristics, class imbalance, class transition trends between
seen and unseen classes and the discrepancies between ZSL and GZSL
performances. Our results show that discriminative ZSL suffer from corruptions
and this trend is further exacerbated by the severe class imbalance and model
weakness inherent in ZSL methods. We then combine our findings with those based
on adversarial attacks in ZSL, and highlight the different effects of
corruptions and adversarial examples, such as the pseudo-robustness effect
present under adversarial attacks. We also obtain new strong baselines for the
label embedding model with certain corruption robustness enhancement methods.
Finally, our experiments show that although existing methods to improve
robustness somewhat work for ZSL models, they do not produce a tangible effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Liver and Hepatic Lesion Segmentation using a Hybrid CNN with Transformer Layers. (arXiv:2201.10981v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10981">
<div class="article-summary-box-inner">
<span><p>Deep learning-based segmentation of the liver and hepatic lesions therein
steadily gains relevance in clinical practice due to the increasing incidence
of liver cancer each year. Whereas various network variants with overall
promising results in the field of medical image segmentation have been
developed over the last years, almost all of them struggle with the challenge
of accurately segmenting hepatic lesions. This lead to the idea of combining
elements of convolutional and transformerbased architectures to overcome the
existing limitations. This work presents a hybrid network called SWTR-Unet,
consisting of a pretrained ResNet, transformer blocks as well as a common
Unet-style decoder path. This network was applied to clinical liver MRI, as
well as to the publicly available CT data of the liver tumor segmentation
(LiTS) challenge. Additionally, multiple state-of-the-art networks were
implemented and applied to both datasets, ensuring a direct comparability.
Furthermore, correlation analysis and an ablation study were carried out, to
investigate various influencing factors on the segmentation accuracy of our
presented method. With Dice similarity scores of averaged 98 +- 2 % for liver
and 81 +- 28 % lesion segmentation on the MRI dataset and 97 +- 2 % and 79 +-
25 %, respectively on the CT dataset, the proposed SWTR-Unet outperforms each
of the additionally implemented state-of-the-art networks. The achieved
segmentation accuracy was found to be on par with manually performed expert
segmentations as indicated by interobserver variabilities for liver lesion
segmentation. In conclusion, the presented method could save valuable time and
resources in clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jalisco's multiclass land cover analysis and classification using a novel lightweight convnet with real-world multispectral and relief data. (arXiv:2201.10985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10985">
<div class="article-summary-box-inner">
<span><p>The understanding of global climate change, agriculture resilience, and
deforestation control rely on the timely observations of the Land Use and Land
Cover Change (LULCC). Recently, some deep learning (DL) methods have been
adapted to make an automatic classification of Land Cover (LC) for global and
homogeneous data. However, most of these DL models can not apply effectively to
real-world data. i.e. a large number of classes, multi-seasonal data, diverse
climate regions, high imbalance label dataset, and low-spatial resolution. In
this work, we present our novel lightweight (only 89k parameters) Convolution
Neural Network (ConvNet) to make LC classification and analysis to handle these
problems for the Jalisco region. In contrast to the global approaches, the
regional data provide the context-specificity that is required for policymakers
to plan the land use and management, conservation areas, or ecosystem services.
In this work, we combine three real-world open data sources to obtain 13
channels. Our embedded analysis anticipates the limited performance in some
classes and gives us the opportunity to group the most similar, as a result,
the test accuracy performance increase from 73 % to 83 %. We hope that this
research helps other regional groups with limited data sources or computational
resources to attain the United Nations Sustainable Development Goal (SDG)
concerning Life on Land.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning To Recognize Procedural Activities with Distant Supervision. (arXiv:2201.10990v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10990">
<div class="article-summary-box-inner">
<span><p>In this paper we consider the problem of classifying fine-grained, multi-step
activities (e.g., cooking different recipes, making disparate home
improvements, creating various forms of arts and crafts) from long videos
spanning up to several minutes. Accurately categorizing these activities
requires not only recognizing the individual steps that compose the task but
also capturing their temporal dependencies. This problem is dramatically
different from traditional action classification, where models are typically
optimized on videos that span only a few seconds and that are manually trimmed
to contain simple atomic actions. While step annotations could enable the
training of models to recognize the individual steps of procedural activities,
existing large-scale datasets in this area do not include such segment labels
due to the prohibitive cost of manually annotating temporal boundaries in long
videos. To address this issue, we propose to automatically identify steps in
instructional videos by leveraging the distant supervision of a textual
knowledge base (wikiHow) that includes detailed descriptions of the steps
needed for the execution of a wide variety of complex activities. Our method
uses a language model to match noisy, automatically-transcribed speech from the
video to step descriptions in the knowledge base. We demonstrate that video
models trained to recognize these automatically-labeled steps (without manual
supervision) yield a representation that achieves superior generalization
performance on four downstream tasks: recognition of procedural activities,
step classification, step forecasting and egocentric video classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One shot PACS: Patient specific Anatomic Context and Shape prior aware recurrent registration-segmentation of longitudinal thoracic cone beam CTs. (arXiv:2201.11000v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11000">
<div class="article-summary-box-inner">
<span><p>Image-guided adaptive lung radiotherapy requires accurate tumor and organs
segmentation from during treatment cone-beam CT (CBCT) images. Thoracic CBCTs
are hard to segment because of low soft-tissue contrast, imaging artifacts,
respiratory motion, and large treatment induced intra-thoracic anatomic
changes. Hence, we developed a novel Patient-specific Anatomic Context and
Shape prior or PACS-aware 3D recurrent registration-segmentation network for
longitudinal thoracic CBCT segmentation. Segmentation and registration networks
were concurrently trained in an end-to-end framework and implemented with
convolutional long-short term memory models. The registration network was
trained in an unsupervised manner using pairs of planning CT (pCT) and CBCT
images and produced a progressively deformed sequence of images. The
segmentation network was optimized in a one-shot setting by combining
progressively deformed pCT (anatomic context) and pCT delineations (shape
context) with CBCT images. Our method, one-shot PACS was significantly more
accurate (p$&lt;$0.001) for tumor (DSC of 0.83 $\pm$ 0.08, surface DSC [sDSC] of
0.97 $\pm$ 0.06, and Hausdorff distance at $95^{th}$ percentile [HD95] of
3.97$\pm$3.02mm) and the esophagus (DSC of 0.78 $\pm$ 0.13, sDSC of
0.90$\pm$0.14, HD95 of 3.22$\pm$2.02) segmentation than multiple methods.
Ablation tests and comparative experiments were also done.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-rater Comparative Study of Automatic Target Localization Methods for Epilepsy Deep Brain Stimulation Procedures. (arXiv:2201.11002v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11002">
<div class="article-summary-box-inner">
<span><p>Epilepsy is the fourth most common neurological disorder and affects people
of all ages worldwide. Deep Brain Stimulation (DBS) has emerged as an
alternative treatment option when anti-epileptic drugs or resective surgery
cannot lead to satisfactory outcomes. To facilitate the planning of the
procedure and for its standardization, it is desirable to develop an algorithm
to automatically localize the DBS stimulation target, i.e., Anterior Nucleus of
Thalamus (ANT), which is a challenging target to plan. In this work, we perform
an extensive comparative study by benchmarking various localization methods for
ANT-DBS. Specifically, the methods involved in this study include traditional
registration method and deep-learning-based methods including heatmap matching
and differentiable spatial to numerical transform (DSNT). Our experimental
results show that the deep-learning (DL)-based localization methods that are
trained with pseudo labels can achieve a performance that is comparable to the
inter-rater and intra-rater variability and that they are orders of magnitude
faster than traditional methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Compressible and Learnable Image Transformation with Secret Key and Its Applications. (arXiv:2201.11006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11006">
<div class="article-summary-box-inner">
<span><p>This article presents an overview of image transformation with a secret key
and its applications. Image transformation with a secret key enables us not
only to protect visual information on plain images but also to embed unique
features controlled with a key into images. In addition, numerous encryption
methods can generate encrypted images that are compressible and learnable for
machine learning. Various applications of such transformation have been
developed by using these properties. In this paper, we focus on a class of
image transformation referred to as learnable image encryption, which is
applicable to privacy-preserving machine learning and adversarially robust
defense. Detailed descriptions of both transformation algorithms and
performances are provided. Moreover, we discuss robustness against various
attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating language-biased image classification based on semantic representations. (arXiv:2201.11014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11014">
<div class="article-summary-box-inner">
<span><p>Humans show language-biased image recognition for a word-embedded image,
known as picture-word interference. Such interference depends on hierarchical
semantic categories and reflects that human language processing highly
interacts with visual processing. Similar to humans, recent artificial models
jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased
image classification. Exploring whether the bias leads to interferences similar
to those observed in humans can contribute to understanding how much the model
acquires hierarchical semantic representations from joint learning of language
and vision. The present study introduces methodological tools from the
cognitive science literature to assess the biases of artificial models.
Specifically, we introduce a benchmark task to test whether words superimposed
on images can distort the image classification across different category levels
and, if it can, whether the perturbation is due to the shared semantic
representation between language and vision. Our dataset is a set of
word-embedded images and consists of a mixture of natural image datasets and
hierarchical word labels with superordinate/basic category levels. Using this
benchmark test, we evaluate the CLIP model. We show that presenting words
distorts the image classification by the model across different category
levels, but the effect does not depend on the semantic relationship between
images and embedded words. This suggests that the semantic word representation
in the CLIP visual processing is not shared with the image representation,
although the word representation strongly dominates for word-embedded images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation. (arXiv:2201.11037v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11037">
<div class="article-summary-box-inner">
<span><p>Automatic diabetic retinopathy (DR) lesions segmentation makes great sense of
assisting ophthalmologists in diagnosis. Although many researches have been
conducted on this task, most prior works paid too much attention to the designs
of networks instead of considering the pathological association for lesions.
Through investigating the pathogenic causes of DR lesions in advance, we found
that certain lesions are closed to specific vessels and present relative
patterns to each other. Motivated by the observation, we propose a relation
transformer block (RTB) to incorporate attention mechanisms at two main levels:
a self-attention transformer exploits global dependencies among lesion
features, while a cross-attention transformer allows interactions between
lesion and vessel features by integrating valuable vascular information to
alleviate ambiguity in lesion detection caused by complex fundus structures. In
addition, to capture the small lesion patterns first, we propose a global
transformer block (GTB) which preserves detailed information in deep network.
By integrating the above blocks of dual-branches, our network segments the four
kinds of lesions simultaneously. Comprehensive experiments on IDRiD and DDR
datasets well demonstrate the superiority of our approach, which achieves
competitive performance compared to state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum Capsule Networks. (arXiv:2201.11091v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11091">
<div class="article-summary-box-inner">
<span><p>Capsule networks are a class of neural networks that achieved promising
results on many computer vision tasks. However, baseline capsule networks have
failed to reach state-of-the-art results on more complex datasets due to the
high computation and memory requirements. We tackle this problem by proposing a
new network architecture, called Momentum Capsule Network (MoCapsNet).
MoCapsNets are inspired by Momentum ResNets, a type of network that applies
reversible residual building blocks. Reversible networks allow for
recalculating activations of the forward pass in the backpropagation algorithm,
so those memory requirements can be drastically reduced. In this paper, we
provide a framework on how invertible residual building blocks can be applied
to capsule networks. We will show that MoCapsNet beats the accuracy of baseline
capsule networks on MNIST, SVHN and CIFAR-10 while using considerably less
memory. The source code is available on https://github.com/moejoe95/MoCapsNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Attention Neural Bag-of-Features. (arXiv:2201.11092v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11092">
<div class="article-summary-box-inner">
<span><p>In this work, we propose several attention formulations for multivariate
sequence data. We build on top of the recently introduced 2D-Attention and
reformulate the attention learning methodology by quantifying the relevance of
feature/temporal dimensions through latent spaces based on self-attention
rather than learning them directly. In addition, we propose a joint
feature-temporal attention mechanism that learns a joint 2D attention mask
highlighting relevant information without treating feature and temporal
representations independently. The proposed approaches can be used in various
architectures and we specifically evaluate their application together with
Neural Bag of Features feature extraction module. Experiments on several
sequence data analysis tasks show the improved performance yielded by our
approach compared to standard methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-attention fusion for audiovisual emotion recognition with incomplete data. (arXiv:2201.11095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11095">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of multimodal data analysis with a use
case of audiovisual emotion recognition. We propose an architecture capable of
learning from raw data and describe three variants of it with distinct modality
fusion mechanisms. While most of the previous works consider the ideal scenario
of presence of both modalities at all times during inference, we evaluate the
robustness of the model in the unconstrained settings where one modality is
absent or noisy, and propose a method to mitigate these limitations in a form
of modality dropout. Most importantly, we find that following this approach not
only improves performance drastically under the absence/noisy representations
of one modality, but also improves the performance in a standard ideal setting,
outperforming the competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Instance Distillation for Object Detection in Autonomous Driving. (arXiv:2201.11097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11097">
<div class="article-summary-box-inner">
<span><p>In recent years, knowledge distillation (KD) has been widely used as an
effective way to derive efficient models. Through imitating a large teacher
model, a lightweight student model can achieve comparable performance with more
efficiency. However, most existing knowledge distillation methods are focused
on classification tasks. Only a limited number of studies have applied
knowledge distillation to object detection, especially in time-sensitive
autonomous driving scenarios. We propose the Adaptive Instance Distillation
(AID) method to selectively impart knowledge from the teacher to the student
for improving the performance of knowledge distillation. Unlike previous KD
methods that treat all instances equally, our AID can attentively adjust the
distillation weights of instances based on the teacher model's prediction loss.
We verified the effectiveness of our AID method through experiments on the
KITTI and the COCO traffic datasets. The results show that our method improves
the performance of existing state-of-the-art attention-guided and non-local
distillation methods and achieves better distillation results on both
single-stage and two-stage detectors. Compared to the baseline, our AID led to
an average of 2.7% and 2.05% mAP increases for single-stage and two-stage
detectors, respectively. Furthermore, our AID is also shown to be useful for
self-distillation to improve the teacher model's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Compressing Subset Pruning for Semantic Image Segmentation. (arXiv:2201.11103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11103">
<div class="article-summary-box-inner">
<span><p>State-of-the-art semantic segmentation models are characterized by high
parameter counts and slow inference times, making them unsuitable for
deployment in resource-constrained environments. To address this challenge, we
propose \textsc{Auto-Compressing Subset Pruning}, \acosp, as a new online
compression method. The core of \acosp consists of learning a channel selection
mechanism for individual channels of each convolution in the segmentation model
based on an effective temperature annealing schedule. We show a crucial
interplay between providing a high-capacity model at the beginning of training
and the compression pressure forcing the model to compress concepts into
retained channels. We apply \acosp to \segnet and \pspnet architectures and
show its success when trained on the \camvid, \city, \voc, and \ade datasets.
The results are competitive with existing baselines for compression of
segmentation models at low compression ratios and outperform them significantly
at high compression ratios, yielding acceptable results even when removing more
than $93\%$ of the parameters. In addition, \acosp is conceptually simple, easy
to implement, and can readily be generalized to other data modalities, tasks,
and architectures. Our code is available at
\url{https://github.com/merantix/acosp}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Descriptions of Deep Visual Features. (arXiv:2201.11114v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11114">
<div class="article-summary-box-inner">
<span><p>Some neurons in deep networks specialize in recognizing highly specific
perceptual, structural, or semantic features of inputs. In computer vision,
techniques exist for identifying neurons that respond to individual concept
categories like colors, textures, and object classes. But these techniques are
limited in scope, labeling only a small subset of neurons and behaviors in any
network. Is a richer characterization of neuron-level computation possible? We
introduce a procedure (called MILAN, for mutual-information-guided linguistic
annotation of neurons) that automatically labels neurons with open-ended,
compositional, natural language descriptions. Given a neuron, MILAN generates a
description by searching for a natural language string that maximizes pointwise
mutual information with the image regions in which the neuron is active. MILAN
produces fine-grained descriptions that capture categorical, relational, and
logical structure in learned features. These descriptions obtain high agreement
with human-generated feature descriptions across a diverse set of model
architectures and tasks, and can aid in understanding and controlling learned
models. We highlight three applications of natural language neuron
descriptions. First, we use MILAN for analysis, characterizing the distribution
and importance of neurons selective for attribute, category, and relational
information in vision models. Second, we use MILAN for auditing, surfacing
neurons sensitive to protected categories like race and gender in models
trained on datasets intended to obscure these features. Finally, we use MILAN
for editing, improving robustness in an image classifier by deleting neurons
sensitive to text features spuriously correlated with class labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating a Fusion Image: One's Identity and Another's Shape. (arXiv:1804.07455v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.07455">
<div class="article-summary-box-inner">
<span><p>Generating a novel image by manipulating two input images is an interesting
research problem in the study of generative adversarial networks (GANs). We
propose a new GAN-based network that generates a fusion image with the identity
of input image x and the shape of input image y. Our network can simultaneously
train on more than two image datasets in an unsupervised manner. We define an
identity loss LI to catch the identity of image x and a shape loss LS to get
the shape of y. In addition, we propose a novel training method called
Min-Patch training to focus the generator on crucial parts of an image, rather
than its entirety. We show qualitative results on the VGG Youtube Pose dataset,
Eye dataset (MPIIGaze and UnityEyes), and the Photo-Sketch-Cartoon dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-driven Adversarial Examples Recognition Framework via Adversarial Feature Genome. (arXiv:1812.10085v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.10085">
<div class="article-summary-box-inner">
<span><p>Adversarial examples pose many security threats to convolutional neural
networks (CNNs). Most defense algorithms prevent these threats by finding
differences between the original images and adversarial examples. However, the
found differences do not contain features about the classes, so these defense
algorithms can only detect adversarial examples without recovering the correct
labels. In this regard, we propose the Adversarial Feature Genome (AFG), a
novel type of data that contains both the differences and features about
classes. This method is inspired by an observed phenomenon, namely the
Adversarial Feature Separability (AFS), where the difference between the
feature maps of the original images and adversarial examples becomes larger
with deeper layers. On top of that, we further develop an adversarial example
recognition framework that detects adversarial examples and can recover the
correct labels. In the experiments, the detection and classification of
adversarial examples by AFGs has an accuracy of more than 90.01\% in various
attack scenarios. To the best of our knowledge, our method is the first method
that focuses on both attack detecting and recovering. AFG gives a new
data-driven perspective to improve the robustness of CNNs. The source code is
available at https://github.com/GeoX-Lab/Adv_Fea_Genome.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Network for Video Relocalization. (arXiv:2007.09877v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.09877">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on video relocalization task, which uses a query
video clip as input to retrieve a semantic relative video clip in another
untrimmed long video. we find that in video relocalization datasets, there
exists a phenomenon showing that there does not exist consistent relationship
between feature similarity by frame and feature similarity by video, which
affects the feature fusion among frames. However, existing video relocalization
methods do not fully consider it. Taking this phenomenon into account, in this
article, we treat video features as a graph by concatenating the query video
feature and proposal video feature along time dimension, where each timestep is
treated as a node, each row of the feature matrix is treated as feature of each
node. Then, with the power of graph neural networks, we propose a Multi-Graph
Feature Fusion Module to fuse the relation feature of this graph. After
evaluating our method on ActivityNet v1.2 dataset and Thumos14 dataset, we find
that our proposed method outperforms the state of art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12750">
<div class="article-summary-box-inner">
<span><p>Ultrafast ultrasound (US) revolutionized biomedical imaging with its
capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough
modalities such as shear-wave elastography and functional US neuroimaging. Yet,
it suffers from strong diffraction artifacts, mainly caused by grating lobes,
side lobes, or edge waves. Multiple acquisitions are typically required to
obtain a sufficient image quality, at the cost of a reduced frame rate. To
answer the increasing demand for high-quality imaging from single unfocused
acquisitions, we propose a two-step convolutional neural network (CNN)-based
image reconstruction method, compatible with real-time imaging. A low-quality
estimate is obtained by means of a backprojection-based operation, akin to
conventional delay-and-sum beamforming, from which a high-quality image is
restored using a residual CNN with multi-scale and multi-channel filtering
properties, trained specifically to remove the diffraction artifacts inherent
to ultrafast US imaging. To account for both the high dynamic range and the
oscillating properties of radio frequency US images, we introduce the mean
signed logarithmic absolute error (MSLAE) as training loss function.
Experiments were conducted with a linear transducer array, in single plane wave
(PW) imaging. Trainings were performed on a simulated dataset, crafted to
contain a wide diversity of structures and echogenicities. Extensive numerical
evaluations demonstrate that the proposed approach can reconstruct images from
single PWs with a quality similar to that of gold-standard synthetic aperture
imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo
experiments show that trainings carried out on simulated data perform well in
experimental settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ES Attack: Model Stealing against Deep Neural Networks without Data Hurdles. (arXiv:2009.09560v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09560">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have become the essential components for various
commercialized machine learning services, such as Machine Learning as a Service
(MLaaS). Recent studies show that machine learning services face severe privacy
threats - well-trained DNNs owned by MLaaS providers can be stolen through
public APIs, namely model stealing attacks. However, most existing works
undervalued the impact of such attacks, where a successful attack has to
acquire confidential training data or auxiliary data regarding the victim DNN.
In this paper, we propose ES Attack, a novel model stealing attack without any
data hurdles. By using heuristically generated synthetic data, ES Attack
iteratively trains a substitute model and eventually achieves a functionally
equivalent copy of the victim DNN. The experimental results reveal the severity
of ES Attack: i) ES Attack successfully steals the victim model without data
hurdles, and ES Attack even outperforms most existing model stealing attacks
using auxiliary data in terms of model accuracy; ii) most countermeasures are
ineffective in defending ES Attack; iii) ES Attack facilitates further attacks
relying on the stolen model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-World Semi-Supervised Learning. (arXiv:2102.03526v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03526">
<div class="article-summary-box-inner">
<span><p>A fundamental limitation of applying semi-supervised learning in real-world
settings is the assumption that unlabeled test data contains only classes
previously encountered in the labeled training data. However, this assumption
rarely holds for data in-the-wild, where instances belonging to novel classes
may appear at testing time. Here, we introduce a novel open-world
semi-supervised learning setting that formalizes the notion that novel classes
may appear in the unlabeled test data. In this novel setting, the goal is to
solve the class distribution mismatch between labeled and unlabeled data, where
at the test time every input instance either needs to be classified into one of
the existing classes or a new unseen class needs to be initialized. To tackle
this challenging problem, we propose ORCA, an end-to-end deep learning approach
that introduces uncertainty adaptive margin mechanism to circumvent the bias
towards seen classes caused by learning discriminative features for seen
classes faster than for the novel classes. In this way, ORCA reduces the gap
between intra-class variance of seen with respect to novel classes. Experiments
on image classification datasets and a single-cell annotation dataset
demonstrate that ORCA consistently outperforms alternative baselines, achieving
25% improvement on seen and 96% improvement on novel classes of the ImageNet
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Transformer for Accurate and Reliable Salient Object Detection. (arXiv:2104.10127v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10127">
<div class="article-summary-box-inner">
<span><p>In this paper, we conduct extensive research on exploring the contribution of
transformers to salient object detection, achieving both accurate and reliable
saliency predictions. We first investigate transformers for accurate salient
object detection with deterministic neural networks, and explain that the
effective structure modeling and global context modeling abilities lead to its
superior performance compared with the CNN based frameworks. Then, we design
stochastic networks to evaluate the transformers' ability in reliable salient
object detection. We observe that both CNN and transformer based frameworks
suffer greatly from the over-confidence issue, where the models tend to
generate wrong predictions with high confidence, leading to over-confident
predictions or a poorly-calibrated model. To estimate the calibration degree of
both CNN- and transformer-based frameworks for reliable saliency prediction, we
introduce generative adversarial network (GAN) based models to identify the
over-confident regions by sampling from the latent space. Specifically, we
present the inferential generative adversarial network (iGAN). Different from
the conventional GAN based framework, which defines the distribution of the
latent variable as fixed standard normal distribution N(0,1), the proposed
"iGAN" infers the latent variable by gradient-based Markov Chain Monte Carlo
(MCMC), namely Langevin dynamics. We apply the proposed inferential generative
adversarial network (iGAN) to both fully and weakly supervised salient object
detection, and explain that iGAN within the transformer framework leads to both
accurate and reliable salient object detection. The source code and
experimental results are publicly available via our project page:
https://github.com/fupiao1998/TrasformerSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning Ternary Quantization. (arXiv:2107.10998v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10998">
<div class="article-summary-box-inner">
<span><p>Inference time, model size, and accuracy are three key factors in deep model
compression.
</p>
<p>Most of the existing work addresses these three key factors separately as it
is difficult to optimize them all at the same time.
</p>
<p>For example, low-bit quantization aims at obtaining a faster model; weight
sharing quantization aims at improving compression ratio and accuracy; and
mixed-precision quantization aims at balancing accuracy and inference time. To
simultaneously optimize bit-width, model size, and accuracy, we propose pruning
ternary quantization (PTQ): a simple, effective, symmetric ternary quantization
method. We integrate L2 normalization, pruning, and the weight decay term to
reduce the weight discrepancy in the gradient estimator during quantization,
thus producing highly compressed ternary weights. Our method brings the highest
test accuracy and the highest compression ratio. For example, it produces a
939kb (49$\times$) 2bit ternary ResNet-18 model with only 4\% accuracy drop on
the ImageNet dataset. It compresses 170MB Mask R-CNN to 5MB (34$\times$) with
only 2.8\% average precision drop. Our method is verified on image
classification, object detection/segmentation tasks with different network
structures such as ResNet-18, ResNet-50, and MobileNetV2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11845">
<div class="article-summary-box-inner">
<span><p>This letter is concerned with image classification with deep convolutional
neural networks (CNNs). The focus is on the following question: given a set of
candidate CNN models, how to select the right one with the best generalization
property for the current task? Present model selection methods require access
to a batch of labeled data for computing a pre-specified performance metric,
such as the cross-entropy loss, the classification error rate, the negative
log-likelihood. In many practical cases, labels are not available in time as
labeling itself is a time-consuming and expensive task. To this end, this
letter presents an approach to CNN model selection using only unlabeled data.
This method is developed based on a principle termed consistent relative
confidence. The effectiveness and efficiency of the proposed method are
demonstrated by experiments using benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01174">
<div class="article-summary-box-inner">
<span><p>Image reconstruction for positron emission tomography (PET) is challenging
because of the ill-conditioned tomographic problem and low counting statistics.
Kernel methods address this challenge by using kernel representation to
incorporate image prior information in the forward model of iterative PET image
reconstruction. Existing kernel methods construct the kernels commonly using an
empirical process, which may lead to suboptimal performance. In this paper, we
describe the equivalence between the kernel representation and a trainable
neural network model. A deep kernel method is then proposed by exploiting a
deep neural network to enable automated learning of an optimized kernel model
and is directly applicable to single subjects. The training process utilizes
available image prior data to seek the best way to form a set of robust kernels
optimally rather than empirically. The results from computer simulations and a
real patient dataset demonstrate that the proposed deep kernel method can
outperform the existing kernel method and neural network method for dynamic PET
image reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sparse Masks for Diffusion-based Image Inpainting. (arXiv:2110.02636v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02636">
<div class="article-summary-box-inner">
<span><p>Diffusion-based inpainting is a powerful tool for the reconstruction of
images from sparse data. Its quality strongly depends on the choice of known
data. Optimising their spatial location -- the inpainting mask -- is
challenging. A commonly used tool for this task are stochastic optimisation
strategies. However, they are slow as they compute multiple inpainting results.
We provide a remedy in terms of a learned mask generation model. By emulating
the complete inpainting pipeline with two networks for mask generation and
neural surrogate inpainting, we obtain a model for highly efficient adaptive
mask generation. Experiments indicate that our model can achieve competitive
quality with an acceleration by as much as four orders of magnitude. Our
findings serve as a basis for making diffusion-based inpainting more attractive
for applications such as image compression, where fast encoding is highly
desirable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAda! Temporally-Adaptive Convolutions for Video Understanding. (arXiv:2110.06178v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06178">
<div class="article-summary-box-inner">
<span><p>Spatial convolutions are widely used in numerous deep video models. It
fundamentally assumes spatio-temporal invariance, i.e., using shared weights
for every location in different frames. This work presents Temporally-Adaptive
Convolutions (TAdaConv) for video understanding, which shows that adaptive
weight calibration along the temporal dimension is an efficient way to
facilitate modelling complex temporal dynamics in videos. Specifically,
TAdaConv empowers the spatial convolutions with temporal modelling abilities by
calibrating the convolution weights for each frame according to its local and
global temporal context. Compared to previous temporal modelling operations,
TAdaConv is more efficient as it operates over the convolution kernels instead
of the features, whose dimension is an order of magnitude smaller than the
spatial resolutions. Further, the kernel calibration brings an increased model
capacity. We construct TAda2D networks by replacing the 2D convolutions in
ResNet with TAdaConv, which leads to at least on par or better performance
compared to state-of-the-art approaches on multiple video action recognition
and localization benchmarks. We also demonstrate that as a readily plug-in
operation with negligible computation overhead, TAdaConv can effectively
improve many existing video models with a convincing margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three approaches to facilitate DNN generalization to objects in out-of-distribution orientations and illuminations. (arXiv:2111.00131v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00131">
<div class="article-summary-box-inner">
<span><p>The training data distribution is often biased towards objects in certain
orientations and illumination conditions. While humans have a remarkable
capability of recognizing objects in out-of-distribution (OoD) orientations and
illuminations, Deep Neural Networks (DNNs) severely suffer in this case, even
when large amounts of training examples are available. In this paper, we
investigate three different approaches to improve DNNs in recognizing objects
in OoD orientations and illuminations. Namely, these are (i) training much
longer after convergence of the in-distribution (InD) validation accuracy,
i.e., late-stopping, (ii) tuning the momentum parameter of the batch
normalization layers, and (iii) enforcing invariance of the neural activity in
an intermediate layer to orientation and illumination conditions. Each of these
approaches substantially improves the DNN's OoD accuracy (more than 20% in some
cases). We report results in four datasets: two datasets are modified from the
MNIST and iLab datasets, and the other two are novel (one of 3D rendered cars
and another of objects taken from various controlled orientations and
illumination conditions). These datasets allow to study the effects of
different amounts of bias and are challenging as DNNs perform poorly in OoD
conditions. Finally, we demonstrate that even though the three approaches focus
on different aspects of DNNs, they all tend to lead to the same underlying
neural mechanism to enable OoD accuracy gains --individual neurons in the
intermediate layers become more selective to a category and also invariant to
OoD orientations and illuminations. We anticipate this study to be a basis for
further improvement of deep neural networks' OoD generalization performance,
which is highly demanded to achieve safe and fair AI applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does a Face Mask Protect my Privacy?: Deep Learning to Predict Protected Attributes from Masked Face Images. (arXiv:2112.07879v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07879">
<div class="article-summary-box-inner">
<span><p>Contactless and efficient systems are implemented rapidly to advocate
preventive methods in the fight against the COVID-19 pandemic. Despite the
positive benefits of such systems, there is potential for exploitation by
invading user privacy. In this work, we analyse the privacy invasiveness of
face biometric systems by predicting privacy-sensitive soft-biometrics using
masked face images. We train and apply a CNN based on the ResNet-50
architecture with 20,003 synthetic masked images and measure the privacy
invasiveness. Despite the popular belief of the privacy benefits of wearing a
mask among people, we show that there is no significant difference to privacy
invasiveness when a mask is worn. In our experiments we were able to accurately
predict sex (94.7%),race (83.1%) and age (MAE 6.21 and RMSE 8.33) from masked
face images. Our proposed approach can serve as a baseline utility to evaluate
the privacy-invasiveness of artificial intelligence systems that make use of
privacy-sensitive information. We open-source all contributions for
re-producibility and broader use by the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSegFormer: Interactive Image Segmentation with Transformers. (arXiv:2112.11325v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11325">
<div class="article-summary-box-inner">
<span><p>We propose iSegFormer, a novel transformer-based approach for interactive
image segmentation. iSegFormer is built upon existing segmentation transformers
with user clicks as an additional input, allowing users to interactively and
iteratively refine the segmentation mask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Saliency based Feature Fusion Model for EEG Emotion Estimation. (arXiv:2201.03891v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03891">
<div class="article-summary-box-inner">
<span><p>Among the different modalities to assess emotion, electroencephalogram (EEG),
representing the electrical brain activity, achieved motivating results over
the last decade. Emotion estimation from EEG could help in the diagnosis or
rehabilitation of certain diseases. In this paper, we propose a dual model
considering two different representations of EEG feature maps: 1) a sequential
based representation of EEG band power, 2) an image-based representation of the
feature vectors. We also propose an innovative method to combine the
information based on a saliency analysis of the image-based model to promote
joint learning of both model parts. The model has been evaluated on four
publicly available datasets and achieves similar results to the
state-of-the-art approaches. It outperforms results for two of the proposed
datasets with a lower standard deviation that reflects higher stability. For
sake of reproducibility, the codes and models proposed in this paper are
available at https://github.com/VDelv/Emotion-EEG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Proposals Refinement for 3D Object Detection. (arXiv:2201.07070v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07070">
<div class="article-summary-box-inner">
<span><p>Recent advances in 3D object detection is made by developing the refinement
stage for voxel-based Region Proposal Networks (RPN) to better strike the
balance between accuracy and efficiency. A popular approach among
state-of-the-art frameworks is to divide proposals, or Regions of Interest
(ROI), into grids and extract feature for each grid location before
synthesizing them to form ROI feature. While achieving impressive performances,
such an approach involves a number of hand crafted components (e.g. grid
sampling, set abstraction) which requires expert knowledge to be tuned
correctly. This paper proposes a data-driven approach to ROI feature computing
named APRO3D-Net which consists of a voxel-based RPN and a refinement stage
made of Vector Attention. Unlike the original multi-head attention, Vector
Attention assigns different weights to different channels within a point
feature, thus being able to capture a more sophisticated relation between
pooled points and ROI. Experiments on KITTI \textit{validation} set show that
our method achieves competitive performance of 84.84 AP for class Car at
Moderate difficulty while having the least parameters compared to closely
related methods and attaining a quasi-real time inference speed at 15 FPS on
NVIDIA V100 GPU. The code is released in
https://github.com/quan-dao/APRO3D-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Rendering for Integral Imaging Light Field Displays Based on a Voxel-Pixel Lookup Table. (arXiv:2201.08266v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08266">
<div class="article-summary-box-inner">
<span><p>A real-time elemental image array (EIA) generation method which does not
sacrifice accuracy nor rely on high-performance hardware is developed, through
raytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both
offline and online working flow, experiments verified the effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation. (arXiv:2201.08582v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08582">
<div class="article-summary-box-inner">
<span><p>Current research on deep learning for medical image segmentation exposes
their limitations in learning either global semantic information or local
contextual information. To tackle these issues, a novel network named
SegTransVAE is proposed in this paper. SegTransVAE is built upon
encoder-decoder architecture, exploiting transformer with the variational
autoencoder (VAE) branch to the network to reconstruct the input images jointly
with segmentation. To the best of our knowledge, this is the first method
combining the success of CNN, transformer, and VAE. Evaluation on various
recently introduced datasets shows that SegTransVAE outperforms previous
methods in Dice Score and $95\%$-Haudorff Distance while having comparable
inference time to a simple CNN-based architecture network. The source code is
available at: https://github.com/itruonghai/SegTransVAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image Enhancement. (arXiv:2201.10294v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10294">
<div class="article-summary-box-inner">
<span><p>Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps
in different energy channels, reflecting energy properties of the scanned
object. Due to the limited photon numbers and the non-ideal detector response
of each energy channel, the reconstructed images usually contain much noise.
With the development of Deep Learning (DL) technique, different kinds of
DL-based models have been proposed for noise reduction. However, most of the
models require clean data set as the training labels, which are not always
available in medical imaging field. Inspiring by the similarities of each
channel's reconstructed image, we proposed a self-supervised learning based
PCCT image enhancement framework via multi-spectral channels (S2MS). In S2MS
framework, both the input and output labels are noisy images. Specifically, one
single channel image was used as output while images of other single channels
and channel-sum image were used as input to train the network, which can fully
use the spectral data information without extra cost. The simulation results
based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS
model can suppress the noise and preserve details more effectively in
comparison with the traditional DL models, which has potential to improve the
image quality of PCCT in clinical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plaque segmentation via masking of the artery wall. (arXiv:2201.10424v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10424">
<div class="article-summary-box-inner">
<span><p>The presence of plaques in the coronary arteries are a major risk to the
patients' life. In particular, non-calcified plaques pose a great challenge, as
they are harder to detect and more likely to rupture than calcified plaques.
While current deep learning techniques allow precise segmentation of regular
images, the performance in medical images is still low, caused mostly by
blurriness and ambiguous voxel intensities of unrelated parts that fall on the
same range. In this paper, we propose a novel methodology for segmenting
calcified and non-calcified plaques in CCTA-CPR scans of coronary arteries. The
input slices are masked so only the voxels within the wall vessel are
considered for segmentation. We also provide an exhaustive evaluation by
applying different types of masks, in order to validate the potential of vessel
masking for plaque segmentation. Our methodology results in a prominent boost
in segmentation performance, in both quantitative and qualitative evaluation,
achieving accurate plaque shapes even for the challenging non-calcified
plaques. We believe our findings can lead the future research for
high-performance plaque segmentation.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-27 23:07:00.771921397 UTC">2022-01-27 23:07:00 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>