<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-03T01:30:00Z">03-03</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conformer Based Acoustic Model for Robust Automatic Speech Recognition. (arXiv:2203.00725v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00725">
<div class="article-summary-box-inner">
<span><p>This study addresses robust automatic speech recognition (ASR) by introducing
a Conformer-based acoustic model. The proposed model builds on a
state-of-the-art recognition system using a bi-directional long short-term
memory (BLSTM) model with utterance-wise dropout and iterative speaker
adaptation, but employs a Conformer encoder instead of the BLSTM network. The
Conformer encoder uses a convolution-augmented attention mechanism for acoustic
modeling. The proposed system is evaluated on the monaural ASR task of the
CHiME-4 corpus. Coupled with utterance-wise normalization and speaker
adaptation, our model achieves $6.25\%$ word error rate, which outperforms the
previous best system by $8.4\%$ relatively. In addition, the proposed
Conformer-based model is $18.3\%$ smaller in model size and reduces training
time by $88.5\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots. (arXiv:2203.00732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00732">
<div class="article-summary-box-inner">
<span><p>Few-shot table-to-text generation is a task of composing fluent and faithful
sentences to convey table content using limited data. Despite many efforts
having been made towards generating impressive fluent sentences by fine-tuning
powerful pre-trained language models, the faithfulness of generated content
still needs to be improved. To this end, this paper proposes a novel approach
Attend, Memorize and Generate (called AMG), inspired by the text generation
process of humans. In particular, AMG (1) attends over the multi-granularity of
context using a novel strategy based on table slot level and traditional
token-by-token level attention to exploit both the table structure and natural
linguistic information; (2) dynamically memorizes the table slot allocation
states; and (3) generates faithful sentences according to both the context and
memory allocation states. Comprehensive experiments with human evaluation on
three domains (i.e., humans, songs, and books) of the Wiki dataset show that
our model can generate higher qualified texts when compared with several
state-of-the-art baselines, in both fluency and faithfulness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models. (arXiv:2203.00748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00748">
<div class="article-summary-box-inner">
<span><p>Building huge and highly capable language models has been a trend in the past
years. Despite their great performance, they incur high computational cost. A
common solution is to apply model compression or choose light-weight
architectures, which often need a separate fixed-size model for each desirable
computational budget, and may lose performance in case of heavy compression.
This paper proposes an effective dynamic inference approach, called E-LANG,
which distributes the inference between large accurate Super-models and
light-weight Swift models. To this end, a decision making module routes the
inputs to Super or Swift models based on the energy characteristics of the
representations in the latent space. This method is easily adoptable and
architecture agnostic. As such, it can be applied to black-box pre-trained
models without a need for architectural manipulations, reassembling of modules,
or re-training. Unlike existing methods that are only applicable to
encoder-only backbones and classification tasks, our method also works for
encoder-decoder structures and sequence-to-sequence tasks such as translation.
The E-LANG performance is verified through a set of experiments with T5 and
BERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B
with an average computations speed-up of 3.3$\times$ on GLUE and 2.9$\times$ on
SuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2$\times$ less
computations. Code and demo are available in the supplementary materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperPrompt: Prompt-based Task-Conditioning of Transformers. (arXiv:2203.00759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00759">
<div class="article-summary-box-inner">
<span><p>Prompt-Tuning is a new paradigm for finetuning pre-trained language models in
a parameter-efficient way. Here, we explore the use of HyperNetworks to
generate hyper-prompts: we propose HyperPrompt, a novel architecture for
prompt-based task-conditioning of self-attention in Transformers. The
hyper-prompts are end-to-end learnable via generation by a HyperNetwork.
HyperPrompt allows the network to learn task-specific feature maps where the
hyper-prompts serve as task global memories for the queries to attend to, at
the same time enabling flexible information sharing among tasks. We show that
HyperPrompt is competitive against strong multi-task learning baselines with as
few as $0.14\%$ of additional task-conditioning parameters, achieving great
parameter and computational efficiency. Through extensive empirical
experiments, we demonstrate that HyperPrompt can achieve superior performances
over strong T5 multi-task learning baselines and parameter-efficient adapter
variants including Prompt-Tuning and HyperFormer++ on Natural Language
Understanding benchmarks of GLUE and SuperGLUE across many model sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Analysis for Text with Side Data. (arXiv:2203.00762v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00762">
<div class="article-summary-box-inner">
<span><p>Although latent factor models (e.g., matrix factorization) obtain good
performance in predictions, they suffer from several problems including
cold-start, non-transparency, and suboptimal recommendations. In this paper, we
employ text with side data to tackle these limitations. We introduce a hybrid
generative probabilistic model that combines a neural network with a latent
topic model, which is a four-level hierarchical Bayesian model. In the model,
each document is modeled as a finite mixture over an underlying set of topics
and each topic is modeled as an infinite mixture over an underlying set of
topic probabilities. Furthermore, each topic probability is modeled as a finite
mixture over side data. In the context of text, the neural network provides an
overview distribution about side data for the corresponding text, which is the
prior distribution in LDA to help perform topic grouping. The approach is
evaluated on several different datasets, where the model is shown to outperform
standard LDA and Dirichlet-multinomial regression (DMR) in terms of topic
grouping, model perplexity, classification and comment generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Sentence Knowledge Selection in Open-Domain Dialogue. (arXiv:2203.00763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00763">
<div class="article-summary-box-inner">
<span><p>Incorporating external knowledge sources effectively in conversations is a
longstanding problem in open-domain dialogue research. The existing literature
on open-domain knowledge selection is limited and makes certain brittle
assumptions on knowledge sources to simplify the overall task (Dinan et al.,
2019), such as the existence of a single relevant knowledge sentence per
context. In this work, we evaluate the existing state of open-domain
conversation knowledge selection, showing where the existing methodologies
regarding data and evaluation are flawed. We then improve on them by proposing
a new framework for collecting relevant knowledge, and create an augmented
dataset based on the Wizard of Wikipedia (WOW) corpus, which we call WOW++.
WOW++ averages 8 relevant knowledge sentences per dialogue context, embracing
the inherent ambiguity of open-domain dialogue knowledge selection. We then
benchmark various knowledge ranking algorithms on this augmented dataset with
both intrinsic evaluation and extrinsic measures of response quality, showing
that neural rerankers that use WOW++ can outperform rankers trained on standard
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Level Supervised Contrastive Learning for Response Selection in Multi-Turn Dialogue. (arXiv:2203.00793v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00793">
<div class="article-summary-box-inner">
<span><p>Selecting an appropriate response from many candidates given the utterances
in a multi-turn dialogue is the key problem for a retrieval-based dialogue
system. Existing work formalizes the task as matching between the utterances
and a candidate and uses the cross-entropy loss in learning of the model. This
paper applies contrastive learning to the problem by using the supervised
contrastive loss. In this way, the learned representations of positive examples
and representations of negative examples can be more distantly separated in the
embedding space, and the performance of matching can be enhanced. We further
develop a new method for supervised contrastive learning, referred to as
two-level supervised contrastive learning, and employ the method in response
selection in multi-turn dialogue. Our method exploits two techniques: sentence
token shuffling (STS) and sentence re-ordering (SR) for supervised contrastive
learning. Experimental results on three benchmark datasets demonstrate that the
proposed method significantly outperforms the contrastive learning baseline and
the state-of-the-art methods for the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSAM: A Two-Stream Attention Model for Causal Emotion Entailment. (arXiv:2203.00819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00819">
<div class="article-summary-box-inner">
<span><p>Causal Emotion Entailment (CEE) aims to discover the potential causes behind
an emotion in a conversational utterance. Previous works formalize CEE as
independent utterance pair classification problems, with emotion and speaker
information neglected. From a new perspective, this paper considers CEE in a
joint framework. We classify multiple utterances synchronously to capture the
correlations between utterances in a global view and propose a Two-Stream
Attention Model (TSAM) to effectively model the speaker's emotional influences
in the conversational history. Specifically, the TSAM comprises three modules:
Emotion Attention Network (EAN), Speaker Attention Network (SAN), and
interaction module. The EAN and SAN incorporate emotion and speaker information
in parallel, and the subsequent interaction module effectively interchanges
relevant information between the EAN and SAN via a mutual BiAffine
transformation. Experimental results on a benchmark dataset demonstrate that
our model achieves new State-Of-The-Art (SOTA) performance and outperforms
baselines remarkably.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Contextual Spelling Correction for Customization of End-to-end Speech Recognition Systems. (arXiv:2203.00888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00888">
<div class="article-summary-box-inner">
<span><p>Contextual biasing is an important and challenging task for end-to-end
automatic speech recognition (ASR) systems, which aims to achieve better
recognition performance by biasing the ASR system to particular context phrases
such as person names, music list, proper nouns, etc. Existing methods mainly
include contextual LM biasing and adding bias encoder into end-to-end ASR
models. In this work, we introduce a novel approach to do contextual biasing by
adding a contextual spelling correction model on top of the end-to-end ASR
system. We incorporate contextual information into a sequence-to-sequence
spelling correction model with a shared context encoder. Our proposed model
includes two different mechanisms: autoregressive (AR) and non-autoregressive
(NAR). We propose filtering algorithms to handle large-size context lists, and
performance balancing mechanisms to control the biasing degree of the model. We
demonstrate the proposed model is a general biasing solution which is
domain-insensitive and can be adopted in different scenarios. Experiments show
that the proposed method achieves as much as 51% relative word error rate (WER)
reduction over ASR system and outperforms traditional biasing methods. Compared
to the AR solution, the proposed NAR model reduces model size by 43.2% and
speeds up inference by 2.1 times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Prompts Solve NLP Tasks Using Natural Language?. (arXiv:2203.00902v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00902">
<div class="article-summary-box-inner">
<span><p>Thanks to the advanced improvement of large pre-trained language models,
prompt-based fine-tuning is shown to be effective on a variety of downstream
tasks. Though many prompting methods have been investigated, it remains unknown
which type of prompts are the most effective among three types of prompts
(i.e., human-designed prompts, schema prompts and null prompts). In this work,
we empirically compare the three types of prompts under both few-shot and
fully-supervised settings. Our experimental results show that schema prompts
are the most effective in general. Besides, the performance gaps tend to
diminish when the scale of training data grows large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking. (arXiv:2203.00991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00991">
<div class="article-summary-box-inner">
<span><p>Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling
errors, which are mainly caused by the phonological or visual similarity.
Recently, pre-trained language models (PLMs) promote the progress of CSC task.
However, there exists a gap between the learned knowledge of PLMs and the goal
of CSC task. PLMs focus on the semantics in text and tend to correct the
erroneous characters to semantically proper or commonly used ones, but these
aren't the ground-truth corrections. To address this issue, we propose an
Error-driven COntrastive Probability Optimization (ECOPO) framework for CSC
task. ECOPO refines the knowledge representations of PLMs, and guides the model
to avoid predicting these common characters through an error-driven way.
Particularly, ECOPO is model-agnostic and it can be combined with existing CSC
methods to achieve better performance. Extensive experiments and detailed
analyses on SIGHAN datasets demonstrate that ECOPO is simple yet effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs. (arXiv:2203.01044v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01044">
<div class="article-summary-box-inner">
<span><p>Entity alignment, aiming to identify equivalent entities across different
knowledge graphs (KGs), is a fundamental problem for constructing Web-scale
KGs. Over the course of its development, the label supervision has been
considered necessary for accurate alignments. Inspired by the recent progress
of self-supervised learning, we explore the extent to which we can get rid of
supervision for entity alignment. Commonly, the label information (positive
entity pairs) is used to supervise the process of pulling the aligned entities
in each positive pair closer. However, our theoretical analysis suggests that
the learning of entity alignment can actually benefit more from pushing
unlabeled negative pairs far away from each other than pulling labeled positive
pairs close. By leveraging this discovery, we develop the self-supervised
learning objective for entity alignment. We present SelfKG with efficient
strategies to optimize this objective for aligning entities without label
supervision. Extensive experiments on benchmark datasets demonstrate that
SelfKG without supervision can match or achieve comparable results with
state-of-the-art supervised baselines. The performance of SelfKG suggests that
self-supervised learning offers great potential for entity alignment in KGs.
The code and data are available at https://github.com/THUDM/SelfKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges. (arXiv:2203.01054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01054">
<div class="article-summary-box-inner">
<span><p>As an important fine-grained sentiment analysis problem, aspect-based
sentiment analysis (ABSA), aiming to analyze and understand people's opinions
at the aspect level, has been attracting considerable interest in the last
decade. To handle ABSA in different scenarios, various tasks have been
introduced for analyzing different sentiment elements and their relations,
including the aspect term, aspect category, opinion term, and sentiment
polarity. Unlike early ABSA works focusing on a single sentiment element, many
compound ABSA tasks involving multiple elements have been studied in recent
years for capturing more complete aspect-level sentiment information. However,
a systematic review of various ABSA tasks and their corresponding solutions is
still lacking, which we aim to fill in this survey. More specifically, we
provide a new taxonomy for ABSA which organizes existing studies from the axes
of concerned sentiment elements, with an emphasis on recent advances of
compound ABSA tasks. From the perspective of solutions, we summarize the
utilization of pre-trained language models for ABSA, which improved the
performance of ABSA to a new stage. Besides, techniques for building more
practical ABSA systems in cross-domain/lingual scenarios are discussed.
Finally, we review some emerging topics and discuss some open challenges to
outlook potential future directions of ABSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discontinuous Constituency and BERT: A Case Study of Dutch. (arXiv:2203.01063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01063">
<div class="article-summary-box-inner">
<span><p>In this paper, we set out to quantify the syntactic capacity of BERT in the
evaluation regime of non-context free patterns, as occurring in Dutch. We
devise a test suite based on a mildly context-sensitive formalism, from which
we derive grammars that capture the linguistic phenomena of control verb
nesting and verb raising. The grammars, paired with a small lexicon, provide us
with a large collection of naturalistic utterances, annotated with verb-subject
pairings, that serve as the evaluation test bed for an attention-based span
selection probe. Our results, backed by extensive analysis, suggest that the
models investigated fail in the implicit acquisition of the dependencies
examined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models. (arXiv:2203.01104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01104">
<div class="article-summary-box-inner">
<span><p>The state-of-the-art Mixture-of-Experts (short as MoE) architecture has
achieved several remarkable successes in terms of increasing model capacity.
However, MoE has been hindered widespread adoption due to complexity,
communication costs, and training instability. Here we present a novel MoE
architecture based on matrix product operators (MPO) from quantum many-body
physics. It can decompose an original matrix into central tensors (containing
the core information) and auxiliary tensors (with only a small proportion of
parameters). With the decomposed MPO structure, we can reduce the parameters of
the original MoE architecture by sharing a global central tensor across experts
and keeping expert-specific auxiliary tensors. We also design the gradient mask
strategy for the tensor structure of MPO to alleviate the overfitting problem.
Experiments on the three well-known downstream natural language datasets based
on GPT2 show improved performance and efficiency in increasing model capacity
(7.26x fewer parameters with the same amount of experts). We additionally
demonstrate an improvement in the positive transfer effects of our approach for
multi-task learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Hate Speech Detection with Cross-Domain Transfer. (arXiv:2203.01111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01111">
<div class="article-summary-box-inner">
<span><p>The performance of hate speech detection models relies on the datasets on
which the models are trained. Existing datasets are mostly prepared with a
limited number of instances or hate domains that define hate topics. This
hinders large-scale analysis and transfer learning with respect to hate
domains. In this study, we construct large-scale tweet datasets for hate speech
detection in English and a low-resource language, Turkish, consisting of
human-labeled 100k tweets per each. Our datasets are designed to have equal
number of tweets distributed over five domains. The experimental results
supported by statistical tests show that Transformer-based language models
outperform conventional bag-of-words and neural models by at least 5% in
English and 10% in Turkish for large-scale hate speech detection. The
performance is also scalable to different training sizes, such that 98% of
performance in English, and 97% in Turkish, are recovered when 20% of training
instances are used. We further examine the generalization ability of
cross-domain transfer among hate domains. We show that 96% of the performance
of a target domain in average is recovered by other domains for English, and
92% for Turkish. Gender and religion are more successful to generalize to other
domains, while sports fail most.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mukayese: Turkish NLP Strikes Back. (arXiv:2203.01215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01215">
<div class="article-summary-box-inner">
<span><p>Having sufficient resources for language X lifts it from the under-resourced
languages class, but not necessarily from the under-researched class. In this
paper, we address the problem of the absence of organized benchmarks in the
Turkish language. We demonstrate that languages such as Turkish are left behind
the state-of-the-art in NLP applications. As a solution, we present Mukayese, a
set of NLP benchmarks for the Turkish language that contains several NLP tasks.
We work on one or more datasets for each benchmark and present two or more
baselines. Moreover, we present four new benchmarking datasets in Turkish for
language modeling, sentence segmentation, and spell checking. All datasets and
baselines are available under: https://github.com/alisafaya/mukayese
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\texttt{py-irt}$: A Scalable Item Response Theory Library for Python. (arXiv:2203.01282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01282">
<div class="article-summary-box-inner">
<span><p>$\texttt{py-irt}$ is a Python library for fitting Bayesian Item Response
Theory (IRT) models. $\texttt{py-irt}$ estimates latent traits of subjects and
items, making it appropriate for use in IRT tasks as well as ideal-point
models. $\texttt{py-irt}$ is built on top of the Pyro and PyTorch frameworks
and uses GPU-accelerated training to scale to large data sets. Code,
documentation, and examples can be found at https://github.com/nd-ball/py-irt.
$\texttt{py-irt}$ can be installed from the GitHub page or the Python Package
Index (PyPI).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Providing Insights for Open-Response Surveys via End-to-End Context-Aware Clustering. (arXiv:2203.01294v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01294">
<div class="article-summary-box-inner">
<span><p>Teachers often conduct surveys in order to collect data from a predefined
group of students to gain insights into topics of interest. When analyzing
surveys with open-ended textual responses, it is extremely time-consuming,
labor-intensive, and difficult to manually process all the responses into an
insightful and comprehensive report. In the analysis step, traditionally, the
teacher has to read each of the responses and decide on how to group them in
order to extract insightful information. Even though it is possible to group
the responses only using certain keywords, such an approach would be limited
since it not only fails to account for embedded contexts but also cannot detect
polysemous words or phrases and semantics that are not expressible in single
words. In this work, we present a novel end-to-end context-aware framework that
extracts, aggregates, and abbreviates embedded semantic patterns in
open-response survey data. Our framework relies on a pre-trained natural
language model in order to encode the textual data into semantic vectors. The
encoded vectors then get clustered either into an optimally tuned number of
groups or into a set of groups with pre-specified titles. In the former case,
the clusters are then further analyzed to extract a representative set of
keywords or summary sentences that serve as the labels of the clusters. In our
framework, for the designated clusters, we finally provide context-aware
wordclouds that demonstrate the semantically prominent keywords within each
group. Honoring user privacy, we have successfully built the on-device
implementation of our framework suitable for real-time analysis on mobile
devices and have tested it on a synthetic dataset. Our framework reduces the
costs at-scale by automating the process of extracting the most insightful
information pieces from survey data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01311">
<div class="article-summary-box-inner">
<span><p>Learning multimodal representations involves discovering correspondences and
integrating information from multiple heterogeneous sources of data. While
recent research has begun to explore the design of more general-purpose
multimodal models (contrary to prior focus on domain and modality-specific
architectures), these methods are still largely focused on a small set of
modalities in the language, vision, and audio space. In order to accelerate
generalization towards diverse and understudied modalities, we investigate
methods for high-modality (a large set of diverse modalities) and
partially-observable (each task only defined on a small subset of modalities)
scenarios. To tackle these challenges, we design a general multimodal model
that enables multitask and transfer learning: multitask learning with shared
parameters enables stable parameter counts (addressing scalability), and
cross-modal transfer learning enables information sharing across modalities and
tasks (addressing partial observability). Our resulting model generalizes
across text, image, video, audio, time-series, sensors, tables, and set
modalities from different research areas, improves the tradeoff between
performance and efficiency, transfers to new modalities and tasks, and reveals
surprising insights on the nature of information sharing in multitask models.
We release our code and benchmarks which we hope will present a unified
platform for subsequent theoretical and empirical analysis:
https://github.com/pliang279/HighMMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Institutional Grammar 2.0 Codebook. (arXiv:2008.08937v4 [cs.MA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08937">
<div class="article-summary-box-inner">
<span><p>The Grammar of Institutions, or Institutional Grammar, is an established
approach to encode policy information in terms of institutional statements
based on a set of pre-defined syntactic components. This codebook provides
coding guidelines for a revised version of the Institutional Grammar, the
Institutional Grammar 2.0 (IG 2.0). IG 2.0 is a specification that aims at
facilitating the encoding of policy to meet varying analytical objectives. To
this end, it revises the grammar with respect to comprehensiveness,
flexibility, and specificity by offering multiple levels of expressiveness (IG
Core, IG Extended, IG Logico). In addition to the encoding of regulative
statements, it further introduces the encoding of constitutive institutional
statements, as well as statements that exhibit both constitutive and regulative
characteristics. Introducing those aspects, the codebook initially covers
fundamental concepts of IG 2.0, before providing an overview of pre-coding
steps relevant for document preparation. Detailed coding guidelines are
provided for both regulative and constitutive statements across all levels of
expressiveness, along with the encoding guidelines for statements of mixed form
-- hybrid and polymorphic institutional statements. The document further
provides an overview of taxonomies used in the encoding process and referred to
throughout the codebook. The codebook concludes with a summary and discussion
of relevant considerations to facilitate the coding process. An initial
Reader's Guide helps the reader tailor the content to her interest.
</p>
<p>Note that this codebook specifically focuses on operational aspects of IG 2.0
in the context of policy coding. Links to additional resources such as the
underlying scientific literature (that offers a comprehensive treatment of the
underlying theoretical concepts) are referred to in the DOI and the concluding
section of the codebook.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing deep neural networks with morphological information. (arXiv:2011.12432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12432">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches are superior in NLP due to their ability to extract
informative features and patterns from languages. The two most successful
neural architectures are LSTM and transformers, used in large pretrained
language models such as BERT. While cross-lingual approaches are on the rise,
most current NLP techniques are designed and applied to English, and
less-resourced languages are lagging behind. In morphologically rich languages,
information is conveyed through morphology, e.g., through affixes modifying
stems of words. Existing neural approaches do not explicitly use the
information on word morphology. We analyse the effect of adding morphological
features to LSTM and BERT models. As a testbed, we use three tasks available in
many less-resourced languages: named entity recognition (NER), dependency
parsing (DP), and comment filtering (CF). We construct baselines involving LSTM
and BERT models, which we adjust by adding additional input in the form of part
of speech (POS) tags and universal features. We compare models across several
languages from different language families. Our results suggest that adding
morphological features has mixed effects depending on the quality of features
and the task. The features improve the performance of LSTM-based models on the
NER and DP tasks, while they do not benefit the performance on the CF task. For
BERT-based models, the morphological features only improve the performance on
DP when they are of high quality while not showing practical improvement when
they are predicted. Even for high-quality features, the improvements are less
pronounced in language-specific BERT variants compared to massively
multilingual BERT models. As in NER and CF datasets manually checked features
are not available, we only experiment with predicted features and find that
they do not cause any practical improvement in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariance, encodings, and generalization: learning identity effects with neural networks. (arXiv:2101.08386v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08386">
<div class="article-summary-box-inner">
<span><p>Often in language and other areas of cognition, whether two components of an
object are identical or not determines if it is well formed. We call such
constraints identity effects. When developing a system to learn well-formedness
from examples, it is easy enough to build in an identify effect. But can
identity effects be learned from the data without explicit guidance? We provide
a framework in which we can rigorously prove that algorithms satisfying simple
criteria cannot make the correct inference. We then show that a broad class of
learning algorithms including deep feedforward neural networks trained via
gradient-based algorithms (such as stochastic gradient descent or the Adam
method) satisfy our criteria, dependent on the encoding of inputs. In some
broader circumstances we are able to provide adversarial examples that the
network necessarily classifies incorrectly. Finally, we demonstrate our theory
with computational experiments in which we explore the effect of different
input encodings on the ability of algorithms to generalize to novel inputs.
This allows us to show similar effects to those predicted by theory for more
realistic methods that violate some of the conditions of our theoretical
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Effectively Adapting Pretrained Language Models for Active Learning. (arXiv:2104.08320v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08320">
<div class="article-summary-box-inner">
<span><p>Recent Active Learning (AL) approaches in Natural Language Processing (NLP)
proposed using off-the-shelf pretrained language models (LMs). In this paper,
we argue that these LMs are not adapted effectively to the downstream task
during AL and we explore ways to address this issue. We suggest to first adapt
the pretrained LM to the target task by continuing training with all the
available unlabeled data and then use it for AL. We also propose a simple yet
effective fine-tuning method to ensure that the adapted LM is properly trained
in both low and high resource scenarios during AL. Our experiments demonstrate
that our approach provides substantial data efficiency improvements compared to
the standard fine-tuning approach, suggesting that a poor training strategy can
be catastrophic for AL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting intermediate convolutional layers of CNNs trained on raw speech. (arXiv:2104.09489v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09489">
<div class="article-summary-box-inner">
<span><p>This paper presents a technique to interpret and visualize intermediate
layers in CNNs trained on raw speech data in an unsupervised manner. We argue
that averaging over feature maps after ReLU activation in each convolutional
layer yields interpretable time-series data. By linearly interpolating
individual latent variables to marginal levels outside of the training range,
we further argue that we are able to observe a causal relationship between
individual latent variables that encode linguistically meaningful units and
activations in intermediate convolutional layers. The proposed technique allows
acoustic analysis of intermediate layers that parallels the acoustic analysis
of human speech data: we can extract F0, intensity, duration, formants, and
other acoustic properties from intermediate layers in order to test where and
how CNNs encode various types of information. Observing the causal effect
between linear interpolation and the resulting changes in intermediate layers
can reveal how individual variables get transformed into spikes in activation
in intermediate layers.We train and probe internal representations on two
models -- a bare WaveGAN architecture and a ciwGAN extension which forces the
Generator to output informative data and results in emergence of linguistically
meaningful representations. Interpretation and visualization is performed for
three basic acoustic properties of speech: periodic vibration (corresponding to
vowels), aperiodic noise vibration (corresponding to fricatives), and silence
(corresponding to stops). The proposal also allows testing of higher-level
morphophonological alternations such as reduplication (copying). In short,
using the proposed technique, we can analyze how linguistically meaningful
units in speech get encoded in different convolutional layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07306">
<div class="article-summary-box-inner">
<span><p>A major challenge in structured prediction is to represent the
interdependencies within output structures. When outputs are structured as
sequences, linear-chain conditional random fields (CRFs) are a widely used
model class which can learn \textit{local} dependencies in the output. However,
the CRF's Markov assumption makes it impossible for CRFs to represent
distributions with \textit{nonlocal} dependencies, and standard CRFs are unable
to respect nonlocal constraints of the data (such as global arity constraints
on output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show
empirically that it can be substantially better in practice. Additionally, we
demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF
into a deep neural model for semantic role labeling, exceeding state-of-the-art
results on a standard dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient DP-SGD Mechanism for Large Scale NLP Models. (arXiv:2107.14586v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14586">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have drastically improved performance on
many Natural Language Understanding (NLU) tasks. However, the data used to
train NLU models may contain private information such as addresses or phone
numbers, particularly when drawn from human subjects. It is desirable that
underlying models do not expose private information contained in the training
data. Differentially Private Stochastic Gradient Descent (DP-SGD) has been
proposed as a mechanism to build privacy-preserving models. However, DP-SGD can
be prohibitively slow to train. In this work, we propose a more efficient
DP-SGD for training using a GPU infrastructure and apply it to fine-tuning
models based on LSTM and transformer architectures. We report faster training
times, alongside accuracy, theoretical privacy guarantees and success of
Membership inference attacks for our models and observe that fine-tuning with
proposed variant of DP-SGD can yield competitive models without significant
degradation in training time and improvement in privacy protection. We also
make observations such as looser theoretical $\epsilon, \delta$ can translate
into significant practical privacy gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00087">
<div class="article-summary-box-inner">
<span><p>Figurative language is ubiquitous in English. Yet, the vast majority of NLP
research focuses on literal language. Existing text representations by design
rely on compositionality, while figurative language is often non-compositional.
In this paper, we study the interpretation of two non-compositional figurative
languages (idioms and similes). We collected datasets of fictional narratives
containing a figurative expression along with crowd-sourced plausible and
implausible continuations relying on the correct interpretation of the
expression. We then trained models to choose or generate the plausible
continuation. Our experiments show that models based solely on pre-trained
language models perform substantially worse than humans on these tasks. We
additionally propose knowledge-enhanced models, adopting human strategies for
interpreting figurative language types : inferring meaning from the context and
relying on the constituent words' literal meanings. The knowledge-enhanced
models improve the performance on both the discriminative and generative tasks,
further bridging the gap from human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08186">
<div class="article-summary-box-inner">
<span><p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.
FaST-VGS is a Transformer-based model for learning the associations between raw
speech waveforms and visual images. The model unifies dual-encoder and
cross-attention architectures into a single model, reaping the superior
retrieval speed of the former along with the accuracy of the latter. FaST-VGS
achieves state-of-the-art speech-image retrieval accuracy on benchmark
datasets, and its learned representations exhibit strong performance on the
ZeroSpeech 2021 phonetic and semantic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03342">
<div class="article-summary-box-inner">
<span><p>In this paper, we formulate a novel task to synthesize speech in sync with a
silent pre-recorded video, denoted as automatic voice over (AVO). Unlike
traditional speech synthesis, AVO seeks to generate not only human-sounding
speech, but also perfect lip-speech synchronization. A natural solution to AVO
is to condition the speech rendering on the temporal progression of lip
sequence in the video. We propose a novel text-to-speech model that is
conditioned on visual input, named VisualTTS, for accurate lip-speech
synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)
textual-visual attention, and 2) visual fusion strategy during acoustic
decoding, which both contribute to forming accurate alignment between the input
text content and lip motion in input lip sequence. Experimental results show
that VisualTTS achieves accurate lip-speech synchronization and outperforms all
baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Chinese Biomedical Language Models via Multi-Level Text Discrimination. (arXiv:2110.07244v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07244">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized
the field of NLP, not only in the general domain but also in the biomedical
domain. Most prior efforts in building biomedical PLMs have resorted simply to
domain adaptation and focused mainly on English. In this work we introduce
eHealth, a Chinese biomedical PLM built from scratch with a new pre-training
framework. This new framework pre-trains eHealth as a discriminator through
both token- and sequence-level discrimination. The former is to detect input
tokens corrupted by a generator and recover their original identities from
plausible candidates, while the latter is to further distinguish corruptions of
a same original sequence from those of others. As such, eHealth can learn
language semantics at both token and sequence levels. Extensive experiments on
11 Chinese biomedical language understanding tasks of various forms verify the
effectiveness and superiority of our approach. We release the pre-trained model
at \url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and
will also release the code later.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Descriptive Patterns and Their Application to Characterizing Classification Errors. (arXiv:2110.09599v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09599">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep learning methods achieve human-like performance on many
tasks, but make errors nevertheless. Characterizing these errors in easily
interpretable terms gives insight into whether a classifier is prone to making
systematic errors, but also gives a way to act and improve the classifier. We
propose to discover those feature-value combinations (ie. patterns) that
strongly correlate with correct resp. erroneous predictions to obtain a global
and interpretable description for arbitrary classifiers. We show this is an
instance of the more general label description problem, which we formulate in
terms of the Minimum Description Length principle. To discover a good pattern
set, we develop the efficient Premise algorithm. Through an extensive set of
experiments we show it performs very well in practice on both synthetic and
real-world data. Unlike existing solutions, it ably recovers ground truth
patterns, even on highly imbalanced data over many features. Through two case
studies on Visual Question Answering and Named Entity Recognition, we confirm
that Premise gives clear and actionable insight into the systematic errors made
by modern NLP classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Siamese Bi-encoder Neural Ranking Model Using Lightweight Fine-Tuning. (arXiv:2110.14943v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14943">
<div class="article-summary-box-inner">
<span><p>A BERT-based Neural Ranking Model (NRM) can be either a crossencoder or a
bi-encoder. Between the two, bi-encoder is highly efficient because all the
documents can be pre-processed before the actual query time. In this work, we
show two approaches for improving the performance of BERT-based bi-encoders.
The first approach is to replace the full fine-tuning step with a lightweight
fine-tuning. We examine lightweight fine-tuning methods that are adapter-based,
prompt-based, and hybrid of the two. The second approach is to develop
semi-Siamese models where queries and documents are handled with a limited
amount of difference. The limited difference is realized by learning two
lightweight fine-tuning modules, where the main language model of BERT is kept
common for both query and document. We provide extensive experiment results for
monoBERT, TwinBERT, and ColBERT where three performance metrics are evaluated
over Robust04, ClueWeb09b, and MS-MARCO datasets. The results confirm that both
lightweight fine-tuning and semi-Siamese are considerably helpful for improving
BERT-based bi-encoders. In fact, lightweight fine-tuning is helpful for
crossencoder, too
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressively Optimized Bi-Granular Document Representation for Scalable Embedding Based Retrieval. (arXiv:2201.05409v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05409">
<div class="article-summary-box-inner">
<span><p>Ad-hoc search calls for the selection of appropriate answers from a
massive-scale corpus. Nowadays, the embedding-based retrieval (EBR) becomes a
promising solution, where deep learning based document representation and ANN
search techniques are allied to handle this task. However, a major challenge is
that the ANN index can be too large to fit into memory, given the considerable
size of answer corpus. In this work, we tackle this problem with Bi-Granular
Document Representation, where the lightweight sparse embeddings are indexed
and standby in memory for coarse-grained candidate search, and the heavyweight
dense embeddings are hosted in disk for fine-grained post verification. For the
best of retrieval accuracy, a Progressive Optimization framework is designed.
The sparse embeddings are learned ahead for high-quality search of candidates.
Conditioned on the candidate distribution induced by the sparse embeddings, the
dense embeddings are continuously learned to optimize the discrimination of
ground-truth from the shortlisted candidates. Besides, two techniques: the
contrastive quantization and the locality-centric sampling are introduced for
the learning of sparse and dense embeddings, which substantially contribute to
their performances. Thanks to the above features, our method effectively
handles massive-scale EBR with strong advantages in accuracy: with up to +4.3%
recall gain on million-scale corpus, and up to +17.5% recall gain on
billion-scale corpus. Besides, Our method is applied to a major sponsored
search platform with substantial gains on revenue (+1.95%), Recall (+1.01%) and
CTR (+0.49%). Our code is available at https://github.com/microsoft/BiDR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples. (arXiv:2201.05979v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05979">
<div class="article-summary-box-inner">
<span><p>Unsupervised sentence embedding aims to obtain the most appropriate embedding
for a sentence to reflect its semantic. Contrastive learning has been
attracting developing attention. For a sentence, current models utilize diverse
data augmentation methods to generate positive samples, while consider other
independent sentences as negative samples. Then they adopt InfoNCE loss to pull
the embeddings of positive pairs gathered, and push those of negative pairs
scattered. Although these models have made great progress on sentence
embedding, we argue that they may suffer from feature suppression. The models
fail to distinguish and decouple textual similarity and semantic similarity.
And they may overestimate the semantic similarity of any pairs with similar
textual regardless of the actual semantic difference between them. This is
because positive pairs in unsupervised contrastive learning come with similar
and even the same textual through data augmentation. To alleviate feature
suppression, we propose contrastive learning for unsupervised sentence
embedding with soft negative samples (SNCSE). Soft negative samples share
highly similar textual but have surely and apparently different semantic with
the original samples. Specifically, we take the negation of original sentences
as soft negative samples, and propose Bidirectional Margin Loss (BML) to
introduce them into traditional contrastive learning framework, which merely
involves positive and negative samples. Our experimental results show that
SNCSE can obtain state-of-the-art performance on semantic textual similarity
(STS) task with average Spearman's correlation coefficient of 78.97% on
BERTbase and 79.23% on RoBERTabase. Besides, we adopt rank-based error analysis
method to detect the weakness of SNCSE for future study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving End-to-End Contextual Speech Recognition with Fine-Grained Contextual Knowledge Selection. (arXiv:2201.12806v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12806">
<div class="article-summary-box-inner">
<span><p>Nowadays, most methods in end-to-end contextual speech recognition bias the
recognition process towards contextual knowledge. Since all-neural contextual
biasing methods rely on phrase-level contextual modeling and attention-based
relevance modeling, they may encounter confusion between similar
context-specific phrases, which hurts predictions at the token level. In this
work, we focus on mitigating confusion problems with fine-grained contextual
knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge
to reduce the uncertainty of token predictions. Specifically, we first apply
phrase selection to narrow the range of phrase candidates, and then conduct
token attention on the tokens in the selected phrase candidates. Moreover, we
re-normalize the attention weights of most relevant phrases in inference to
obtain more focused phrase-level contextual representations, and inject
position information to better discriminate phrases or tokens. On LibriSpeech
and an in-house 160,000-hour dataset, we explore the proposed methods based on
a controllable all-neural biasing method, collaborative decoding (ColDec). The
proposed methods provide at most 6.1% relative word error rate reduction on
LibriSpeech and 16.4% relative character error rate reduction on the in-house
dataset over ColDec.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JARVix at SemEval-2022 Task 2: It Takes One to Know One? Idiomaticity Detection using Zero and One Shot Learning. (arXiv:2202.02394v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02394">
<div class="article-summary-box-inner">
<span><p>Large Language Models have been successful in a wide variety of Natural
Language Processing tasks by capturing the compositionality of the text
representations. In spite of their great success, these vector representations
fail to capture meaning of idiomatic multi-word expressions (MWEs). In this
paper, we focus on the detection of idiomatic expressions by using binary
classification. We use a dataset consisting of the literal and idiomatic usage
of MWEs in English and Portuguese. Thereafter, we perform the classification in
two different settings: zero shot and one shot, to determine if a given
sentence contains an idiom or not. N shot classification for this task is
defined by N number of common idioms between the training and testing sets. In
this paper, we train multiple Large Language Models in both the settings and
achieve an F1 score (macro) of 0.73 for the zero shot setting and an F1 score
(macro) of 0.85 for the one shot setting. An implementation of our work can be
found at
https://github.com/ashwinpathak20/Idiomaticity_Detection_Using_Few_Shot_Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03543">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge
and SUPERB benchmark. Our submissions are based on the recently proposed
FaST-VGS model, which is a Transformer-based model that learns to associate raw
speech waveforms with semantically related images, all without the use of any
transcriptions of the speech. Additionally, we introduce a novel extension of
this model, FaST-VGS+, which is learned in a multi-task fashion with a masked
language modeling objective in addition to the visual grounding objective. On
ZeroSpeech 2021, we show that our models perform competitively on the ABX task,
outperform all other concurrent submissions on the Syntactic and Semantic
tasks, and nearly match the best system on the Lexical task. On the SUPERB
benchmark, we show that our models also achieve strong performance, in some
cases even outperforming the popular wav2vec2.0 model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Self Shuffle Language. (arXiv:2202.07988v2 [math.CO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07988">
<div class="article-summary-box-inner">
<span><p>The shuffle product \(u\shuffle v\) of two words \(u\) and \(v\) is the set
of all words which can be obtained by interleaving \(u\) and \(v\). Motivated
by the paper \emph{The Shuffle Product: New Research Directions} by Restivo
(2015) we investigate a special case of the shuffle product. In this work we
consider the shuffle of a word with itself called the \emph{self shuffle} or
\emph{shuffle square}, showing first that the self shuffle language and the
shuffle of the language are in general different sets. We prove that the
language of all words arising as a self shuffle of some word is context
sensitive but not context free. Furthermore, we show that the self shuffle \(w
\shuffle w\) uniquely determines \(w\).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Evaluation of Large Language Models of Code. (arXiv:2202.13169v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13169">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) of code have recently shown tremendous promise in
completing code and synthesizing code from natural language descriptions.
However, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,
2021)) are not publicly available, leaving many questions about their model and
data design decisions. We aim to fill in some of these blanks through a
systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,
GPT-NeoX-20B, and CodeParrot, across various programming languages. Although
Codex itself is not open-source, we find that existing open-source models do
achieve close results in some programming languages, although targeted mainly
for natural language modeling. We further identify an important missing piece
in the form of a large open-source model trained exclusively on a multi-lingual
corpus of code. We release a new model, PolyCoder, with 2.7B parameters based
on the GPT-2 architecture, which was trained on 249GB of code across 12
programming languages on a single machine. In the C programming language,
PolyCoder outperforms all models including Codex. Our trained models are
open-source and publicly available at https://github.com/VHellendoorn/Code-LMs,
which enables future research and application in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Beauty in Songs: Neural Singing Voice Beautifier. (arXiv:2202.13277v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13277">
<div class="article-summary-box-inner">
<span><p>We are interested in a novel task, singing voice beautifying (SVB). Given the
singing voice of an amateur singer, SVB aims to improve the intonation and
vocal tone of the voice, while keeping the content and vocal timbre. Current
automatic pitch correction techniques are immature, and most of them are
restricted to intonation but ignore the overall aesthetic quality. Hence, we
introduce Neural Singing Voice Beautifier (NSVB), the first generative model to
solve the SVB task, which adopts a conditional variational autoencoder as the
backbone and learns the latent representations of vocal tone. In NSVB, we
propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic
Time Warping (SADTW), which ameliorates the robustness of existing time-warping
approaches, to synchronize the amateur recording with the template pitch curve.
Furthermore, we propose a latent-mapping algorithm in the latent space to
convert the amateur vocal tone to the professional one. To achieve this, we
also propose a new dataset containing parallel singing recordings of both
amateur and professional versions. Extensive experiments on both Chinese and
English songs demonstrate the effectiveness of our methods in terms of both
objective and subjective metrics. Audio samples are available
at~\url{https://neuralsvb.github.io}. Codes:
\url{https://github.com/MoonInTheRiver/NeuralSVB}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring and Adapting Chinese GPT to Pinyin Input Method. (arXiv:2203.00249v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00249">
<div class="article-summary-box-inner">
<span><p>While GPT has become the de-facto method for text generation tasks, its
application to pinyin input method remains unexplored. In this work, we make
the first exploration to leverage Chinese GPT for pinyin input method. We find
that a frozen GPT achieves state-of-the-art performance on perfect pinyin.
However, the performance drops dramatically when the input includes abbreviated
pinyin. A reason is that an abbreviated pinyin can be mapped to many perfect
pinyin, which links to even larger number of Chinese characters. We mitigate
this issue with two strategies, including enriching the context with pinyin and
optimizing the training process to help distinguish homophones. To further
facilitate the evaluation of pinyin input method, we create a dataset
consisting of 270K instances from 15 domains. Results show that our approach
improves performance on abbreviated pinyin across all domains. Model analysis
demonstrates that both strategies contribute to the performance boost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Is Whole Word Masking Always Better for Chinese BERT?": Probing on Chinese Grammatical Error Correction. (arXiv:2203.00286v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00286">
<div class="article-summary-box-inner">
<span><p>Whole word masking (WWM), which masks all subwords corresponding to a word at
once, makes a better English BERT model. For the Chinese language, however,
there is no subword because each token is an atomic character. The meaning of a
word in Chinese is different in that a word is a compositional unit consisting
of multiple characters. Such difference motivates us to investigate whether WWM
leads to better context understanding ability for Chinese BERT. To achieve
this, we introduce two probing tasks related to grammatical error correction
and ask pretrained models to revise or insert tokens in a masked language
modeling manner. We construct a dataset including labels for 19,075 tokens in
10,448 sentences. We train three Chinese BERT models with standard
character-level masking (CLM), WWM, and a combination of CLM and WWM,
respectively. Our major findings are as follows: First, when one character
needs to be inserted or replaced, the model trained with CLM performs the best.
Second, when more than one character needs to be handled, WWM is the key to
better performance. Finally, when being fine-tuned on sentence-level downstream
tasks, models trained with different masking strategies perform comparably.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training. (arXiv:2203.00648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00648">
<div class="article-summary-box-inner">
<span><p>Human speech data comprises a rich set of domain factors such as accent,
syntactic and semantic variety, or acoustic environment. Previous work explores
the effect of domain mismatch in automatic speech recognition between
pre-training and fine-tuning as a whole but does not dissect the contribution
of individual factors. In this paper, we present a controlled study to better
understand the effect of such factors on the performance of pre-trained
representations. To do so, we pre-train models either on modified natural
speech or synthesized audio, with a single domain factor modified, and then
measure performance on automatic speech recognition after fine tuning. Results
show that phonetic domain factors play an important role during pre-training
while grammatical and syntactic factors are far less important. To our
knowledge, this is the first study to better understand the domain
characteristics in self-supervised pre-training for speech.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Knock, knock. Who's there? -- Identifying football player jersey numbers with synthetic data. (arXiv:2203.00734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00734">
<div class="article-summary-box-inner">
<span><p>Automatic player identification is an essential and complex task in sports
video analysis. Different strategies have been devised over the years, but
identification based on jersey numbers is one of the most common approaches
given its versatility and relative simplicity. However, automatic detection of
jersey numbers is still challenging due to changing camera angles, low video
resolution, small object size in wide-range shots and transient changes in the
player's posture and movement. In this paper we present a novel approach for
jersey number identification in a small, highly imbalanced dataset from the
Seattle Seahawks practice videos. Our results indicate that simple models can
achieve an acceptable performance on the jersey number detection task and that
synthetic data can improve the performance dramatically (accuracy increase of
~9% overall, ~18% on low frequency numbers) making our approach achieve state
of the art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Skeleton-based Human Motion Prediction with Manifold-Aware GAN. (arXiv:2203.00736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00736">
<div class="article-summary-box-inner">
<span><p>In this work we propose a novel solution for 3D skeleton-based human motion
prediction. The objective of this task consists in forecasting future human
poses based on a prior skeleton pose sequence. This involves solving two main
challenges still present in recent literature; (1) discontinuity of the
predicted motion which results in unrealistic motions and (2) performance
deterioration in long-term horizons resulting from error accumulation across
time. We tackle these issues by using a compact manifold-valued representation
of 3D human skeleton motion. Specifically, we model the temporal evolution of
the 3D poses as trajectory, what allows us to map human motions to single
points on a sphere manifold. Using such a compact representation avoids error
accumulation and provides robust representation for long-term prediction while
ensuring the smoothness and the coherence of the whole motion. To learn these
non-Euclidean representations, we build a manifold-aware Wasserstein generative
adversarial model that captures the temporal and spatial dependencies of human
motion through different losses. Experiments have been conducted on CMU MoCap
and Human 3.6M datasets and demonstrate the superiority of our approach over
the state-of-the-art both in short and long term horizons. The smoothness of
the generated motion is highlighted in the qualitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Runtime Detection of Executional Errors in Robot-Assisted Surgery. (arXiv:2203.00737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00737">
<div class="article-summary-box-inner">
<span><p>Despite significant developments in the design of surgical robots and
automated techniques for objective evaluation of surgical skills, there are
still challenges in ensuring safety in robot-assisted minimally-invasive
surgery (RMIS). This paper presents a runtime monitoring system for the
detection of executional errors during surgical tasks through the analysis of
kinematic data. The proposed system incorporates dual Siamese neural networks
and knowledge of surgical context, including surgical tasks and gestures, their
distributional similarities, and common error modes, to learn the differences
between normal and erroneous surgical trajectories from small training
datasets. We evaluate the performance of the error detection using Siamese
networks compared to single CNN and LSTM networks trained with different levels
of contextual knowledge and training data, using the dry-lab demonstrations of
the Suturing and Needle Passing tasks from the JIGSAWS dataset. Our results
show that gesture specific task nonspecific Siamese networks obtain micro F1
scores of 0.94 (Siamese-CNN) and 0.95 (Siamese-LSTM), and perform better than
single CNN (0.86) and LSTM (0.87) networks. These Siamese networks also
outperform gesture nonspecific task specific Siamese-CNN and Siamese-LSTM
models for Suturing and Needle Passing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">There is a Time and Place for Reasoning Beyond the Image. (arXiv:2203.00758v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00758">
<div class="article-summary-box-inner">
<span><p>Images are often more significant than only the pixels to human eyes, as we
can infer, associate, and reason with contextual information from other sources
to establish a more complete picture. For example, in Figure 1, we can find a
way to identify the news articles related to the picture through segment-wise
understandings on the signs, the buildings, the crowds, and more. This tells us
the time when and the location where the image is taken, which will help us in
subsequent tasks, such as evidence retrieval for criminal activities, automatic
storyline construction, and upper-stream processing such as image clustering.
In this work, we formulate this problem and introduce TARA: a dataset with 16k
images with their associated news, time and location automatically extracted
from New York Times (NYT), and an additional 61k examples as distant
supervision from WIT. On top of the extractions, we present a crowdsourced
subset in which images are believed to be feasible to find their
spatio-temporal information for evaluation purpose. We show that there exists a
70% gap between a state-of-the-art joint model and human performance, which is
slightly filled by our proposed model that uses segment-wise reasoning,
motivating higher-level vision-language joint models that can conduct
open-ended reasoning with world knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tricks and Plugins to GBM on Images and Sequences. (arXiv:2203.00761v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00761">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) and transformers, which are composed of
multiple processing layers and blocks to learn the representations of data with
multiple abstract levels, are the most successful machine learning models in
recent years. However, millions of parameters and many blocks make them
difficult to be trained, and sometimes several days or weeks are required to
find an ideal architecture or tune the parameters. Within this paper, we
propose a new algorithm for boosting Deep Convolutional Neural Networks
(BoostCNN) to combine the merits of dynamic feature selection and BoostCNN, and
another new family of algorithms combining boosting and transformers. To learn
these new models, we introduce subgrid selection and importance sampling
strategies and propose a set of algorithms to incorporate boosting weights into
a deep learning architecture based on a least squares objective function. These
algorithms not only reduce the required manual effort for finding an
appropriate network architecture but also result in superior performance and
lower running time. Experiments show that the proposed methods outperform
benchmarks on several fine-grained classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Cost On-device Partial Domain Adaptation (LoCO-PDA): Enabling efficient CNN retraining on edge devices. (arXiv:2203.00772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00772">
<div class="article-summary-box-inner">
<span><p>With the increased deployment of Convolutional Neural Networks (CNNs) on edge
devices, the uncertainty of the observed data distribution upon deployment has
led researchers to to utilise large and extensive datasets such as ILSVRC'12 to
train CNNs. Consequently, it is likely that the observed data distribution upon
deployment is a subset of the training data distribution. In such cases, not
adapting a network to the observed data distribution can cause performance
degradation due to negative transfer and alleviating this is the focus of
Partial Domain Adaptation (PDA). Current works targeting PDA do not focus on
performing the domain adaptation on an edge device, adapting to a changing
target distribution or reducing the cost of deploying the adapted network. This
work proposes a novel PDA methodology that targets all of these directions and
opens avenues for on-device PDA. LoCO-PDA adapts a deployed network to the
observed data distribution by enabling it to be retrained on an edge device.
Across subsets of the ILSVRC12 dataset, LoCO-PDA improves classification
accuracy by 3.04pp on average while achieving up to 15.1x reduction in
retraining memory consumption and 2.07x improvement in inference latency on the
NVIDIA Jetson TX2. The work is open-sourced at \emph{link removed for
anonymity}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image analysis for automatic measurement of crustose lichens. (arXiv:2203.00787v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00787">
<div class="article-summary-box-inner">
<span><p>Lichens, organisms resulting from a symbiosis between a fungus and an algae,
are frequently used as age estimators, especially in recent geological deposits
and archaeological structures, using the correlation between lichen size and
age. Current non-automated manual lichen and measurement (with ruler, calipers
or using digital image processing tools) is a time-consuming and laborious
process, especially when the number of samples is high.
</p>
<p>This work presents a workflow and set of image acquisition and processing
tools developed to efficiently identify lichen thalli in flat rocky surfaces,
and to produce relevant lichen size statistics (percentage cover, number of
thalli, their area and perimeter).
</p>
<p>The developed workflow uses a regular digital camera for image capture along
with specially designed targets to allow for automatic image correction and
scale assignment. After this step, lichen identification is done in a flow
comprising assisted image segmentation and classification based on interactive
foreground extraction tool (GrabCut) and automatic classification of images
using Simple Linear Iterative Clustering (SLIC) for image segmentation and
Support Vector Machines (SV) and Random Forest classifiers.
</p>
<p>Initial evaluation shows promising results. The manual classification of
images (for training) using GrabCut show an average speedup of 4 if compared
with currently used techniques and presents an average precision of 95\%. The
automatic classification using SLIC and SVM with default parameters produces
results with average precision higher than 70\%. The developed system is
flexible and allows a considerable reduction of processing time, the workflow
allows it applicability to data sets of new lichen populations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Physical Threat Monitoring System Aided by Virtual Building Simulation. (arXiv:2203.00789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00789">
<div class="article-summary-box-inner">
<span><p>With increasing physical threats in recent years targeted at critical
infrastructures, it is crucial to establish a reliable threat monitoring system
integrating video surveillance and digital sensors based on cutting-edge
technologies. A physical threat monitoring solution unifying the floorplan,
cameras, and sensors for smart buildings has been set up in our study. Computer
vision and deep learning models are used for video streams analysis. When a
threat is detected by a rule engine based on the real-time analysis results
combining with feedback from related digital sensors, an alert is sent to the
Video Management System so that human operators can take further action. A
physical threat monitoring system typically needs to address complex and even
destructive incidents, such as fire, which is unrealistic to simulate in real
life. Restrictions imposed during the Covid-19 pandemic and privacy concerns
have added to the challenges. Our study utilises the Unreal Engine to simulate
some typical suspicious and intrusion scenes with photorealistic qualities in
the context of a virtual building. Add-on programs are implemented to transfer
the video stream from virtual PTZ cameras to the Milestone Video Management
System and enable users to control those cameras from the graphic client
application. Virtual sensors such as fire alarms, temperature sensors and door
access controls are implemented similarly, fulfilling the same programmatic VMS
interface as real-life sensors. Thanks to this simulation system's
extensibility and repeatability, we have consolidated this unified physical
threat monitoring system and verified its effectiveness and user-friendliness.
Both the simulated Unreal scenes and the software add-ons developed during this
study are highly modulated and thereby are ready for reuse in future projects
in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stable, accurate and efficient deep neural networks for inverse problems with analysis-sparse models. (arXiv:2203.00804v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00804">
<div class="article-summary-box-inner">
<span><p>Solving inverse problems is a fundamental component of science, engineering
and mathematics. With the advent of deep learning, deep neural networks have
significant potential to outperform existing state-of-the-art, model-based
methods for solving inverse problems. However, it is known that current
data-driven approaches face several key issues, notably instabilities and
hallucinations, with potential impact in critical tasks such as medical
imaging. This raises the key question of whether or not one can construct
stable and accurate deep neural networks for inverse problems. In this work, we
present a novel construction of an accurate, stable and efficient neural
network for inverse problems with general analysis-sparse models. To construct
the network, we unroll NESTA, an accelerated first-order method for convex
optimization. Combined with a compressed sensing analysis, we prove accuracy
and stability. Finally, a restart scheme is employed to enable exponential
decay of the required network depth, yielding a shallower, and consequently
more efficient, network. We showcase this approach in the case of Fourier
imaging, and verify its stability and performance via a series of numerical
experiments. The key impact of this work is to provide theoretical guarantees
for computing and developing stable neural networks in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InCloud: Incremental Learning for Point Cloud Place Recognition. (arXiv:2203.00807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00807">
<div class="article-summary-box-inner">
<span><p>Place recognition is a fundamental component of robotics, and has seen
tremendous improvements through the use of deep learning models in recent
years. Networks can experience significant drops in performance when deployed
in unseen or highly dynamic environments, and require additional training on
the collected data. However naively fine-tuning on new training distributions
can cause severe degradation of performance on previously visited domains, a
phenomenon known as catastrophic forgetting. In this paper we address the
problem of incremental learning for point cloud place recognition and introduce
InCloud, a structure-aware distillation-based approach which preserves the
higher-order structure of the network's embedding space. We introduce several
challenging new benchmarks on four popular and large-scale LiDAR datasets
(Oxford, MulRan, In-house and KITTI) showing broad improvements in point cloud
place recognition performance over a variety of network architectures. To the
best of our knowledge, this work is the first to effectively apply incremental
learning for point cloud place recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-aware multi-object self-supervision for monocular depth prediction. (arXiv:2203.00809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00809">
<div class="article-summary-box-inner">
<span><p>This paper proposes a self-supervised monocular image-to-depth prediction
framework that is trained with an end-to-end photometric loss that handles not
only 6-DOF camera motion but also 6-DOF moving object instances.
Self-supervision is performed by warping the images across a video sequence
using depth and scene motion including object instances. One novelty of the
proposed method is the use of a multi-head attention of the transformer network
that matches moving objects across time and models their interaction and
dynamics. This enables accurate and robust pose estimation for each object
instance. Most image-to-depth predication frameworks make the assumption of
rigid scenes, which largely degrades their performance with respect to dynamic
objects. Only a few SOTA papers have accounted for dynamic objects. The
proposed method is shown to largely outperform these methods on standard
benchmarks and the impact of the dynamic motion on these benchmarks is exposed.
Furthermore, the proposed image-to-depth prediction framework is also shown to
outperform SOTA video-to-depth prediction frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Seatbelt Detection and Usage Recognition for Driver Monitoring Systems. (arXiv:2203.00810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00810">
<div class="article-summary-box-inner">
<span><p>Wearing a seatbelt appropriately while driving can reduce serious
crash-related injuries or deaths by about half. However, current seatbelt
reminder system has multiple shortcomings, such as can be easily fooled by a
"Seatbelt Warning Stopper", and cannot recognize incorrect usages for example
seating in front of a buckled seatbelt or wearing a seatbelt under the arm.
General seatbelt usage recognition has many challenges, to name a few, lacking
of color information in Infrared (IR) cameras, strong distortion caused by wide
Field of View (FoV) fisheye lens, low contrast between belt and its background,
occlusions caused by hands or hair, and imaging blurry. In this paper, we
introduce a novel general seatbelt detection and usage recognition framework to
resolve the above challenges. Our method consists of three components: a local
predictor, a global assembler, and a shape modeling process. Our approach can
be applied to the driver in the Driver Monitoring System (DMS) or general
passengers in the Occupant Monitoring System (OMS) for various camera
modalities. Experiment results on both DMS and OMS are provided to demonstrate
the accuracy and robustness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification. (arXiv:2203.00828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00828">
<div class="article-summary-box-inner">
<span><p>Although accurate and fast point cloud classification is a fundamental task
in 3D applications, it is difficult to achieve this purpose due to the
irregularity and disorder of point clouds that make it challenging to achieve
effective and efficient global discriminative feature learning. Lately, 3D
Transformers have been adopted to improve point cloud processing. Nevertheless,
massive Transformer layers tend to incur huge computational and memory costs.
This paper presents a novel hierarchical framework that incorporates
convolution with Transformer for point cloud classification, named 3D
Convolution-Transformer Network (3DCTN), to combine the strong and efficient
local feature learning ability of convolution with the remarkable global
context modeling capability of Transformer. Our method has two main modules
operating on the downsampling point sets, and each module consists of a
multi-scale local feature aggregating (LFA) block and a global feature learning
(GFL) block, which are implemented by using Graph Convolution and Transformer
respectively. We also conduct a detailed investigation on a series of
Transformer variants to explore better performance for our network. Various
experiments on ModelNet40 demonstrate that our method achieves state-of-the-art
classification performance, in terms of both accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSC Loss: A Gaussian Score Calibrating Loss for Deep Learning. (arXiv:2203.00833v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00833">
<div class="article-summary-box-inner">
<span><p>Cross entropy (CE) loss integrated with softmax is an orthodox component in
most classification-based frameworks, but it fails to obtain an accurate
probability distribution of predicted scores that is critical for further
decision-making of poor-classified samples. The prediction score calibration
provides a solution to learn the distribution of predicted scores which can
explicitly make the model obtain a discriminative representation. Considering
the entropy function can be utilized to measure the uncertainty of predicted
scores. But, the gradient variation of it is not in line with the expectations
of model optimization. To this end, we proposed a general Gaussian Score
Calibrating (GSC) loss to calibrate the predicted scores produced by the deep
neural networks (DNN). Extensive experiments on over 10 benchmark datasets
demonstrate that the proposed GSC loss can yield consistent and significant
performance boosts in a variety of visual tasks. Notably, our label-independent
GSC loss can be embedded into common improved methods based on the CE loss
easily.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion. (arXiv:2203.00838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00838">
<div class="article-summary-box-inner">
<span><p>A well-known challenge in applying deep-learning methods to omnidirectional
images is spherical distortion. In dense regression tasks such as depth
estimation, where structural details are required, using a vanilla CNN layer on
the distorted 360 image results in undesired information loss. In this paper,
we propose a 360 monocular depth estimation pipeline, \textit{OmniFusion}, to
tackle the spherical distortion issue. Our pipeline transforms a 360 image into
less-distorted perspective patches (i.e. tangent images) to obtain patch-wise
predictions via CNN, and then merge the patch-wise results for final output. To
handle the discrepancy between patch-wise predictions which is a major issue
affecting the merging quality, we propose a new framework with the following
key components. First, we propose a geometry-aware feature fusion mechanism
that combines 3D geometric features with 2D image features to compensate for
the patch-wise discrepancy. Second, we employ the self-attention-based
transformer architecture to conduct a global aggregation of patch-wise
information, which further improves the consistency. Last, we introduce an
iterative depth refinement mechanism, to further refine the estimated depth
based on the more accurate geometric features. Experiments show that our method
greatly mitigates the distortion issue, and achieves state-of-the-art
performances on several 360 monocular depth estimation benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00843">
<div class="article-summary-box-inner">
<span><p>3D dense captioning aims to describe individual objects by natural language
in 3D scenes, where 3D scenes are usually represented as RGB-D scans or point
clouds. However, only exploiting single modal information, e.g., point cloud,
previous approaches fail to produce faithful descriptions. Though aggregating
2D features into point clouds may be beneficial, it introduces an extra
computational burden, especially in inference phases. In this study, we
investigate a cross-modal knowledge transfer using Transformer for 3D dense
captioning, X-Trans2Cap, to effectively boost the performance of single-modal
3D caption through knowledge distillation using a teacher-student framework. In
practice, during the training phase, the teacher network exploits auxiliary 2D
modality and guides the student network that only takes point clouds as input
through the feature consistency constraints. Owing to the well-designed
cross-modal feature fusion module and the feature alignment in the training
phase, X-Trans2Cap acquires rich appearance information embedded in 2D images
with ease. Thus, a more faithful caption can be generated only using point
clouds during the inference. Qualitative and quantitative results confirm that
X-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,
about +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can No-reference features help in Full-reference image quality estimation?. (arXiv:2203.00845v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00845">
<div class="article-summary-box-inner">
<span><p>Development of perceptual image quality assessment (IQA) metrics has been of
significant interest to computer vision community. The aim of these metrics is
to model quality of an image as perceived by humans. Recent works in
Full-reference IQA research perform pixelwise comparison between deep features
corresponding to query and reference images for quality prediction. However,
pixelwise feature comparison may not be meaningful if distortion present in
query image is severe. In this context, we explore utilization of no-reference
features in Full-reference IQA task. Our model consists of both full-reference
and no-reference branches. Full-reference branches use both distorted and
reference images, whereas No-reference branch only uses distorted image. Our
experiments show that use of no-reference features boosts performance of image
quality assessment. Our model achieves higher SRCC and KRCC scores than a
number of state-of-the-art algorithms on KADID-10K and PIPAL datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clean-Annotation Backdoor Attack against Lane Detection Systems in the Wild. (arXiv:2203.00858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00858">
<div class="article-summary-box-inner">
<span><p>We present the first backdoor attack against the lane detection systems in
the physical world. Modern autonomous vehicles adopt various deep learning
methods to train lane detection models, making it challenging to devise a
universal backdoor attack technique. In our solution, (1) we propose a novel
semantic trigger design, which leverages the traffic cones with specific poses
and locations to activate the backdoor. Such trigger can be easily realized
under the physical setting, and looks very natural not to be detected. (2) We
introduce a new clean-annotation approach to generate poisoned samples. These
samples have correct annotations but are still capable of embedding the
backdoor to the model. Comprehensive evaluations on public datasets and
physical autonomous vehicles demonstrate that our backdoor attack is effective,
stealthy and robust.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video. (arXiv:2203.00859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00859">
<div class="article-summary-box-inner">
<span><p>Recent transformer-based solutions have been introduced to estimate 3D human
pose from 2D keypoint sequence by considering body joints among all frames
globally to learn spatio-temporal correlation. We observe that the motions of
different joints differ significantly. However, the previous methods cannot
efficiently model the solid inter-frame correspondence of each joint, leading
to insufficient learning of spatial-temporal correlation. We propose MixSTE
(Mixed Spatio-Temporal Encoder), which has a temporal transformer block to
separately model the temporal motion of each joint and a spatial transformer
block to learn inter-joint spatial correlation. These two blocks are utilized
alternately to obtain better spatio-temporal feature encoding. In addition, the
network output is extended from the central frame to entire frames of the input
video, thereby improving the coherence between the input and output sequences.
Extensive experiments are conducted on three benchmarks (i.e. Human3.6M,
MPI-INF-3DHP, and HumanEva) to evaluate the proposed method. The results show
that our model outperforms the state-of-the-art approach by 10.9% P-MPJPE and
7.6% MPJPE on the Human3.6M dataset. Code is available in our supplementary
materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention. (arXiv:2203.00860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00860">
<div class="article-summary-box-inner">
<span><p>DETR is the first fully end-to-end detector that predicts a final set of
predictions without post-processing. However, it suffers from problems such as
low performance and slow convergence. A series of works aim to tackle these
issues in different ways, but the computational cost is yet expensive due to
the sophisticated encoder-decoder architecture. To alleviate this issue, we
propose a decoder-only detector called D^2ETR. In the absence of encoder, the
decoder directly attends to the fine-fused feature maps generated by the
Transformer backbone with a novel computationally efficient cross-scale
attention module. D^2ETR demonstrates low computational complexity and high
detection accuracy in evaluations on the COCO benchmark, outperforming DETR and
its variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Styleverse: Towards Identity Stylization across Heterogeneous Domains. (arXiv:2203.00861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00861">
<div class="article-summary-box-inner">
<span><p>We propose a new challenging task namely IDentity Stylization (IDS) across
heterogeneous domains. IDS focuses on stylizing the content identity, rather
than completely swapping it using the reference identity. We use an effective
heterogeneous-network-based framework $Styleverse$ that uses a single
domain-aware generator to exploit the Metaverse of diverse heterogeneous faces,
based on the proposed dataset FS13 with limited data. FS13 means 13 kinds of
Face Styles considering diverse lighting conditions, art representations and
life dimensions. Previous similar tasks, \eg, image style transfer can handle
textural style transfer based on a reference image. This task usually ignores
the high structure-aware facial area and high-fidelity preservation of the
content. However, Styleverse intends to controllably create topology-aware
faces in the Parallel Style Universe, where the source facial identity is
adaptively styled via AdaIN guided by the domain-aware and reference-aware
style embeddings from heterogeneous pretrained models. We first establish the
IDS quantitative benchmark as well as the qualitative Styleverse matrix.
Extensive experiments demonstrate that Styleverse achieves higher-fidelity
identity stylization compared with other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEA: Bridging the Gap Between One- and Two-stage Detector Distillation via SEmantic-aware Alignment. (arXiv:2203.00862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00862">
<div class="article-summary-box-inner">
<span><p>We revisit the one- and two-stage detector distillation tasks and present a
simple and efficient semantic-aware framework to fill the gap between them. We
address the pixel-level imbalance problem by designing the category anchor to
produce a representative pattern for each category and regularize the
topological distance between pixels and category anchors to further tighten
their semantic bonds. We name our method SEA (SEmantic-aware Alignment)
distillation given the nature of abstracting dense fine-grained information by
semantic reliance to well facilitate distillation efficacy. SEA is well adapted
to either detection pipeline and achieves new state-of-the-art results on the
challenging COCO object detection task on both one- and two-stage detectors.
Its superior performance on instance segmentation further manifests the
generalization ability. Both 2x-distilled RetinaNet and FCOS with ResNet50-FPN
outperform their corresponding 3x ResNet101-FPN teacher, arriving 40.64 and
43.06 AP, respectively. Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding. (arXiv:2203.00867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00867">
<div class="article-summary-box-inner">
<span><p>Image inpainting has made significant advances in recent years. However, it
is still challenging to recover corrupted images with both vivid textures and
reasonable structures. Some specific methods only tackle regular textures while
losing holistic structures due to the limited receptive fields of convolutional
neural networks (CNNs). On the other hand, attention-based models can learn
better long-range dependency for the structure recovery, but they are limited
by the heavy computation for inference with large image sizes. To address these
issues, we propose to leverage an additional structure restorer to facilitate
the image inpainting incrementally. The proposed model restores holistic image
structures with a powerful attention-based transformer model in a fixed
low-resolution sketch space. Such a grayscale space is easy to be upsampled to
larger scales to convey correct structural information. Our structure restorer
can be integrated with other pretrained inpainting models efficiently with the
zero-initialized residual addition. Furthermore, a masking positional encoding
strategy is utilized to improve the performance with large irregular masks.
Extensive experiments on various datasets validate the efficacy of our model
compared with other competitors. Our codes are released in
https://github.com/DQiaole/ZITS_inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Optimized Deep Convolution Neural Network based Learning Model for Object Detection. (arXiv:2203.00869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00869">
<div class="article-summary-box-inner">
<span><p>Object identification is one of the most fundamental and difficult issues in
computer vision. It aims to discover object instances in real pictures from a
huge number of established categories. In recent years, deep learning-based
object detection techniques that developed from computer vision have grabbed
the public's interest. Object recognition methods based on deep learning
frameworks have quickly become a popular way to interpret moving images
acquired by various sensors. Due to its vast variety of applications for
various computer vision tasks such as activity or event detection,
content-based image retrieval, and scene understanding, academics have spent
decades attempting to solve this problem. With this goal in mind, a unique deep
learning classification technique is used to create an autonomous object
detecting system. The noise destruction and normalising operations, which are
carried out using gaussian filter and contrast normalisation techniques,
respectively, are the first steps in the study activity. The pre-processed
picture is next subjected to entropy-based segmentation algorithms, which
separate the image's significant areas in order to distinguish between distinct
occurrences. The classification challenge is completed by the suggested Hybrid
Optimized Dense Convolutional Neural Network (HODCNN). The major goal of this
framework is to aid in the precise recognition of distinct items from the
gathered input frames. The suggested system's performance is assessed by
comparing it to existing machine learning and deep learning methodologies. The
experimental findings reveal that the suggested framework has a detection
accuracy of 0.9864, which is greater than current techniques. As a result, the
suggested object detection model outperforms other current methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Voxel Fusion for 3D Object Detection. (arXiv:2203.00871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00871">
<div class="article-summary-box-inner">
<span><p>Camera and LiDAR sensor modalities provide complementary appearance and
geometric information useful for detecting 3D objects for autonomous vehicle
applications. However, current fusion models underperform state-of-art
LiDAR-only methods on 3D object detection benchmarks. Our proposed solution,
Dense Voxel Fusion (DVF) is a sequential fusion method that generates
multi-scale multi-modal dense voxel feature representations, improving
expressiveness in low point density regions. To enhance multi-modal learning,
we train directly with ground truth 2D bounding box labels, avoiding noisy,
detector-specific, 2D predictions. Additionally, we use LiDAR ground truth
sampling to simulate missed 2D detections and to accelerate training
convergence. Both DVF and the multi-modal training approaches can be applied to
any voxel-based LiDAR backbone without introducing additional learnable
parameters. DVF outperforms existing sparse fusion detectors, ranking $1^{st}$
among all published fusion methods on KITTI's 3D car detection benchmark at the
time of submission and significantly improves 3D vehicle detection performance
of voxel-based methods on the Waymo Open Dataset. We also show that our
proposed multi-modal training strategy results in better generalization
compared to training using erroneous 2D predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Split Semantic Detection Algorithm for Psychological Sandplay Image. (arXiv:2203.00907v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00907">
<div class="article-summary-box-inner">
<span><p>Psychological sandplay, as an important psychological analysis tool, is a
visual scene constructed by the tester selecting and placing sand objects
(e.g., sand, river, human figures, animals, vegetation, buildings, etc.). As
the projection of the tester's inner world, it contains high-level semantic
information reflecting the tester's thoughts and feelings. Most of the existing
computer vision technologies focus on the objective basic semantics (e.g.,
object's name, attribute, boundingbox, etc.) in the natural image, while few
related works pay attention to the subjective psychological semantics (e.g.,
emotion, thoughts, feelings, etc.) in the artificial image. We take the latter
semantics as the research object, take "split" (a common psychological
semantics reflecting the inner integration of testers) as the research goal,
and use the method of machine learning to realize the automatic detection of
split semantics, so as to explore the application of machine learning in the
detection of subjective psychological semantics of sandplay images. To this
end, we present a feature dimensionality reduction and extraction algorithm to
obtain a one-dimensional vector representing the split feature, and build the
split semantic detector based on Multilayer Perceptron network to get the
detection results. Experimental results on the real sandplay datasets show the
effectiveness of our proposed algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle Idempotence. (arXiv:2203.00911v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00911">
<div class="article-summary-box-inner">
<span><p>Deep learning based single image super-resolution models have been widely
studied and superb results are achieved in upscaling low-resolution images with
fixed scale factor and downscaling degradation kernel. To improve real world
applicability of such models, there are growing interests to develop models
optimized for arbitrary upscaling factors. Our proposed method is the first to
treat arbitrary rescaling, both upscaling and downscaling, as one unified
process. Using joint optimization of both directions, the proposed model is
able to learn upscaling and downscaling simultaneously and achieve
bidirectional arbitrary image rescaling. It improves the performance of current
arbitrary upscaling models by a large margin while at the same time learns to
maintain visual perception quality in downscaled images. The proposed model is
further shown to be robust in cycle idempotence test, free of severe
degradations in reconstruction accuracy when the downscaling-to-upscaling cycle
is applied repetitively. This robustness is beneficial for image rescaling in
the wild when this cycle could be applied to one image for multiple times. It
also performs well on tests with arbitrary large scales and asymmetric scales,
even when the model is not trained with such tasks. Extensive experiments are
conducted to demonstrate the superior performance of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Principled Design of Image Representation: Towards Forensic Tasks. (arXiv:2203.00913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00913">
<div class="article-summary-box-inner">
<span><p>Image forensics is a rising topic as the trustworthy multimedia content is
critical for modern society. Like other vision-related applications, forensic
analysis relies heavily on the proper image representation. Despite the
importance, current theoretical understanding for such representation remains
limited, with varying degrees of neglect for its key role. For this gap, we
attempt to investigate the forensic-oriented image representation as a distinct
problem, from the perspectives of theory, implementation, and application. Our
work starts from the abstraction of basic principles that the representation
for forensics should satisfy, especially revealing the criticality of
robustness, interpretability, and coverage. At the theoretical level, we
propose a new representation framework for forensics, called Dense Invariant
Representation (DIR), which is characterized by stable description with
mathematical guarantees. At the implementation level, the discrete calculation
problems of DIR are discussed, and the corresponding accurate and fast
solutions are designed with generic nature and constant complexity. We
demonstrate the above arguments on the dense-domain pattern detection and
matching experiments, providing comparison results with state-of-the-art
descriptors. Also, at the application level, the proposed DIR is initially
explored in passive and active forensics, namely copy-move forgery detection
and perceptual hashing, exhibiting the benefits in fulfilling the requirements
of such forensic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PUFA-GAN: A Frequency-Aware Generative Adversarial Network for 3D Point Cloud Upsampling. (arXiv:2203.00914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00914">
<div class="article-summary-box-inner">
<span><p>We propose a generative adversarial network for point cloud upsampling, which
can not only make the upsampled points evenly distributed on the underlying
surface but also efficiently generate clean high frequency regions. The
generator of our network includes a dynamic graph hierarchical residual
aggregation unit and a hierarchical residual aggregation unit for point feature
extraction and upsampling, respectively. The former extracts multiscale
point-wise descriptive features, while the latter captures rich feature details
with hierarchical residuals. To generate neat edges, our discriminator uses a
graph filter to extract and retain high frequency points. The generated high
resolution point cloud and corresponding high frequency points help the
discriminator learn the global and high frequency properties of the point
cloud. We also propose an identity distribution loss function to make sure that
the upsampled points remain on the underlying surface of the input low
resolution point cloud. To assess the regularity of the upsampled points in
high frequency regions, we introduce two evaluation metrics. Objective and
subjective results demonstrate that the visual quality of the upsampled point
clouds generated by our method is better than that of the state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translation Invariant Global Estimation of Heading Angle Using Sinogram of LiDAR Point Cloud. (arXiv:2203.00924v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00924">
<div class="article-summary-box-inner">
<span><p>Global point cloud registration is an essential module for localization, of
which the main difficulty exists in estimating the rotation globally without
initial value. With the aid of gravity alignment, the degree of freedom in
point cloud registration could be reduced to 4DoF, in which only the heading
angle is required for rotation estimation. In this paper, we propose a fast and
accurate global heading angle estimation method for gravity-aligned point
clouds. Our key idea is that we generate a translation invariant representation
based on Radon Transform, allowing us to solve the decoupled heading angle
globally with circular cross-correlation. Besides, for heading angle estimation
between point clouds with different distributions, we implement this heading
angle estimator as a differentiable module to train a feature extraction
network end- to-end. The experimental results validate the effectiveness of the
proposed method in heading angle estimation and show better performance
compared with other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameterized Image Quality Score Distribution Prediction. (arXiv:2203.00926v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00926">
<div class="article-summary-box-inner">
<span><p>Recently, image quality has been generally describedby a mean opinion score
(MOS). However, we observe that thequality scores of an image given by a group
of subjects are verysubjective and diverse. Thus it is not enough to use a MOS
todescribe the image quality. In this paper, we propose to describeimage
quality using a parameterized distribution rather thana MOS, and an objective
method is also proposed to predictthe image quality score distribution (IQSD).
At first, the LIVEdatabase is re-recorded. Specifically, we have invited a
largegroup of subjects to evaluate the quality of all images in theLIVE
database, and each image is evaluated by a large numberof subjects (187 valid
subjects), whose scores can form a reliableIQSD. By analyzing the obtained
subjective quality scores, wefind that the IQSD can be well modeled by an alpha
stable model,and it can reflect much more information than a single MOS, suchas
the skewness of opinion score, the subject diversity and themaximum probability
score for an image. Therefore, we proposeto model the IQSD using the alpha
stable model. Moreover, wepropose a framework and an algorithm to predict the
alphastable model based IQSD, where quality features are extractedfrom each
image based on structural information and statisticalinformation, and support
vector regressors are trained to predictthe alpha stable model parameters.
Experimental results verifythe feasibility of using alpha stable model to
describe the IQSD,and prove the effectiveness of objective alpha stable model
basedIQSD prediction method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration. (arXiv:2203.00927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00927">
<div class="article-summary-box-inner">
<span><p>Traditional video-based human activity recognition has experienced remarkable
progress linked to the rise of deep learning, but this effect was slower as it
comes to the downstream task of driver behavior understanding. Understanding
the situation inside the vehicle cabin is essential for Advanced Driving
Assistant System (ADAS) as it enables identifying distraction, predicting
driver's intent and leads to more convenient human-vehicle interaction. At the
same time, driver observation systems face substantial obstacles as they need
to capture different granularities of driver states, while the complexity of
such secondary activities grows with the rising automation and increased driver
freedom. Furthermore, a model is rarely deployed under conditions identical to
the ones in the training set, as sensor placements and types vary from vehicle
to vehicle, constituting a substantial obstacle for real-life deployment of
data-driven models. In this work, we present a novel vision-based framework for
recognizing secondary driver behaviours based on visual transformers and an
additional augmented feature distribution calibration module. This module
operates in the latent feature-space enriching and diversifying the training
set at feature-level in order to improve generalization to novel data
appearances, (e.g., sensor changes) and general feature quality. Our framework
consistently leads to better recognition rates, surpassing previous
state-of-the-art results of the public Drive&amp;Act benchmark on all granularity
levels. Our code will be made publicly available at
https://github.com/KPeng9510/TransDARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaPose: Parameter and Domain Randomization Optimization for Pose Estimation using Synthetic Data. (arXiv:2203.00945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00945">
<div class="article-summary-box-inner">
<span><p>Pose estimation is the task of determining the 6D position of an object in a
scene. Pose estimation aid the abilities and flexibility of robotic set-ups.
However, the system must be configured towards the use case to perform
adequately. This configuration is time-consuming and limits the usability of
pose estimation and, thereby, robotic systems.
</p>
<p>Deep learning is a method to overcome this configuration procedure by
learning parameters directly from the dataset. However, obtaining this training
data can also be very time-consuming. The use of synthetic training data avoids
this data collection problem, but a configuration of the training procedure is
necessary to overcome the domain gap problem. Additionally, the pose estimation
parameters also need to be configured. This configuration is jokingly known as
grad student descent as parameters are manually adjusted until satisfactory
results are obtained.
</p>
<p>This paper presents a method for automatic configuration using only synthetic
data. This is accomplished by learning the domain randomization during network
training, and then using the domain randomization to optimize the pose
estimation parameters. The developed approach shows state-of-the-art
performance of 82.0 % recall on the challenging OCCLUSION dataset,
outperforming all previous methods with a large margin. These results prove the
validity of automatic set-up of pose estimation using purely synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CD-GAN: a robust fusion-based generative adversarial network for unsupervised change detection between heterogeneous images. (arXiv:2203.00948v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00948">
<div class="article-summary-box-inner">
<span><p>In the context of Earth observation, the detection of changes is performed
from multitemporal images acquired by sensors with possibly different
characteristics and modalities. Even when restricting to the optical modality,
this task has proved to be challenging as soon as the sensors provide images of
different spatial and/or spectral resolutions. This paper proposes a novel
unsupervised change detection method dedicated to such so-called heterogeneous
optical images. This method capitalizes on recent advances which frame the
change detection problem into a robust fusion framework. More precisely, we
show that a deep adversarial network designed and trained beforehand to fuse a
pair of multiband images can be easily complemented by a network with the same
architecture to perform change detection. The resulting overall architecture
itself follows an adversarial strategy where the fusion network and the
additional network are interpreted as essential building blocks of a generator.
A comparison with state-of-the-art change detection methods demonstrate the
versatility and the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sketched RT3D: How to reconstruct billions of photons per second. (arXiv:2203.00952v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00952">
<div class="article-summary-box-inner">
<span><p>Single-photon light detection and ranging (lidar) captures depth and
intensity information of a 3D scene. Reconstructing a scene from observed
photons is a challenging task due to spurious detections associated with
background illumination sources. To tackle this problem, there is a plethora of
3D reconstruction algorithms which exploit spatial regularity of natural scenes
to provide stable reconstructions. However, most existing algorithms have
computational and memory complexity proportional to the number of recorded
photons. This complexity hinders their real-time deployment on modern lidar
arrays which acquire billions of photons per second. Leveraging a recent lidar
sketching framework, we show that it is possible to modify existing
reconstruction algorithms such that they only require a small sketch of the
photon information. In particular, we propose a sketched version of a recent
state-of-the-art algorithm which uses point cloud denoisers to provide
spatially regularized reconstructions. A series of experiments performed on
real lidar datasets demonstrates a significant reduction of execution time and
memory requirements, while achieving the same reconstruction performance than
in the full data case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRASP EARTH: Intuitive Software for Discovering Changes on the Planet. (arXiv:2203.00955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00955">
<div class="article-summary-box-inner">
<span><p>Detecting changes on the Earth, such as urban development, deforestation, or
natural disaster, is one of the research fields that is attracting a great deal
of attention. One promising tool to solve these problems is satellite imagery.
However, satellite images require huge amount of storage, therefore users are
required to set Area of Interests first, which was not suitable for detecting
potential areas for disaster or development. To tackle with this problem, we
develop the novel tool, namely GRASP EARTH, which is the simple change
detection application based on Google Earth Engine. GRASP EARTH allows us to
handle satellite imagery easily and it has used for disaster monitoring and
urban development monitoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Moving-Object Tracking with FMCW LiDAR. (arXiv:2203.00959v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00959">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a learning-based moving-object tracking method
utilizing our newly developed LiDAR sensor, Frequency Modulated Continuous Wave
(FMCW) LiDAR. Compared with most existing commercial LiDAR sensors, our FMCW
LiDAR can provide additional Doppler velocity information to each 3D point of
the point clouds. Benefiting from this, we can generate instance labels as
ground truth in a semi-automatic manner. Given the labels, we propose a
contrastive learning framework, which pulls together the features from the same
instance in embedding space and pushes apart the features from different
instances, to improve the tracking quality. Extensive experiments are conducted
on our recorded driving data, and the results show that our method outperforms
the baseline methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy for Image Recognition without Convolutions. (arXiv:2203.00960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00960">
<div class="article-summary-box-inner">
<span><p>With the achievements of Transformer in the field of natural language
processing, the encoder-decoder and the attention mechanism in Transformer have
been applied to computer vision. Recently, in multiple tasks of computer vision
(image classification, object detection, semantic segmentation, etc.),
state-of-the-art convolutional neural networks have introduced some concepts of
Transformer. This proves that Transformer has a good prospect in the field of
image recognition. After Vision Transformer was proposed, more and more works
began to use self-attention to completely replace the convolutional layer. This
work is based on Vision Transformer, combined with the pyramid architecture,
using Split-transform-merge to propose the group encoder and name the network
architecture Aggregated Pyramid Vision Transformer (APVT). We perform image
classification tasks on the CIFAR-10 dataset and object detection tasks on the
COCO 2017 dataset. Compared with other network architectures that use
Transformer as the backbone, APVT has excellent results while reducing the
computational cost. We hope this improved strategy can provide a reference for
future Transformer research in computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class Re-Activation Maps for Weakly-Supervised Semantic Segmentation. (arXiv:2203.00962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00962">
<div class="article-summary-box-inner">
<span><p>Extracting class activation maps (CAM) is arguably the most standard step of
generating pseudo masks for weakly-supervised semantic segmentation (WSSS).
Yet, we find that the crux of the unsatisfactory pseudo masks is the binary
cross-entropy loss (BCE) widely used in CAM. Specifically, due to the
sum-over-class pooling nature of BCE, each pixel in CAM may be responsive to
multiple classes co-occurring in the same receptive field. As a result, given a
class, its hot CAM pixels may wrongly invade the area belonging to other
classes, or the non-hot ones may be actually a part of the class. To this end,
we introduce an embarrassingly simple yet surprisingly effective method:
Reactivating the converged CAM with BCE by using softmax cross-entropy loss
(SCE), dubbed \textbf{ReCAM}. Given an image, we use CAM to extract the feature
pixels of each single class, and use them with the class label to learn another
fully-connected layer (after the backbone) with SCE. Once converged, we extract
ReCAM in the same way as in CAM. Thanks to the contrastive nature of SCE, the
pixel response is disentangled into different classes and hence less mask
ambiguity is expected. The evaluation on both PASCAL VOC and MS~COCO shows that
ReCAM not only generates high-quality masks, but also supports plug-and-play in
any CAM variant with little overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training. (arXiv:2203.00972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00972">
<div class="article-summary-box-inner">
<span><p>The paper presents a simple and effective learning-based method for computing
a discriminative 3D point cloud descriptor for place recognition purposes.
Recent state-of-the-art methods have relatively complex architectures such as
multi-scale oyramid of point Transformers combined with a pyramid of feature
aggregation modules. Our method uses a simple and efficient 3D convolutional
feature extraction, based on a sparse voxelized representation, enhanced with
channel attention blocks. We employ recent advances in image retrieval and
propose a modified version of a loss function based on a differentiable average
precision approximation. Such loss function requires training with very large
batches for the best results. This is enabled by using multistaged
backpropagation. Experimental evaluation on the popular benchmarks proves the
effectiveness of our approach, with a consistent improvement over the state of
the art
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TableFormer: Table Structure Understanding with Transformers. (arXiv:2203.01017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01017">
<div class="article-summary-box-inner">
<span><p>Tables organize valuable content in a concise and compact representation.
This content is extremely valuable for systems such as search engines,
Knowledge Graph's, etc, since they enhance their predictive capabilities.
Unfortunately, tables come in a large variety of shapes and sizes. Furthermore,
they can have complex column/row-header configurations, multiline rows,
different variety of separation lines, missing entries, etc. As such, the
correct identification of the table-structure from an image is a non-trivial
task. In this paper, we present a new table-structure identification model. The
latter improves the latest end-to-end deep learning model (i.e.
encoder-dual-decoder from PubTabNet) in two significant ways. First, we
introduce a new object detection decoder for table-cells. In this way, we can
obtain the content of the table-cells from programmatic PDF's directly from the
PDF source and avoid the training of the custom OCR decoders. This
architectural change leads to more accurate table-content extraction and allows
us to tackle non-english tables. Second, we replace the LSTM decoders with
transformer based decoders. This upgrade improves significantly the previous
state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple
tables and from 88.7% to 95% on complex tables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asynchronous Optimisation for Event-based Visual Odometry. (arXiv:2203.01037v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01037">
<div class="article-summary-box-inner">
<span><p>Event cameras open up new possibilities for robotic perception due to their
low latency and high dynamic range. On the other hand, developing effective
event-based vision algorithms that fully exploit the beneficial properties of
event cameras remains work in progress. In this paper, we focus on event-based
visual odometry (VO). While existing event-driven VO pipelines have adopted
continuous-time representations to asynchronously process event data, they
either assume a known map, restrict the camera to planar trajectories, or
integrate other sensors into the system. Towards map-free event-only monocular
VO in SE(3), we propose an asynchronous structure-from-motion optimisation
back-end. Our formulation is underpinned by a principled joint optimisation
problem involving non-parametric Gaussian Process motion modelling and
incremental maximum a posteriori inference. A high-performance incremental
computation engine is employed to reason about the camera trajectory with every
incoming event. We demonstrate the robustness of our asynchronous back-end in
comparison to frame-based methods which depend on accurate temporal
accumulation of measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-based material analysis of ancient historical documents. (arXiv:2203.01042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01042">
<div class="article-summary-box-inner">
<span><p>Researchers continually perform corroborative tests to classify ancient
historical documents based on the physical materials of their writing surfaces.
However, these tests, often performed on-site, requires actual access to the
manuscript objects. The procedures involve a considerable amount of time and
cost, and can damage the manuscripts. Developing a technique to classify such
documents using only digital images can be very useful and efficient. In order
to tackle this problem, this study uses images of a famous historical
collection, the Dead Sea Scrolls, to propose a novel method to classify the
materials of the manuscripts. The proposed classifier uses the two-dimensional
Fourier Transform to identify patterns within the manuscript surfaces.
Combining a binary classification system employing the transform with a
majority voting process is shown to be effective for this classification task.
This pilot study shows a successful classification percentage of up to 97% for
a confined amount of manuscripts produced from either parchment or papyrus
material. Feature vectors based on Fourier-space grid representation
outperformed a concentric Fourier-space format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D object reconstruction and 6D-pose estimation from 2D shape for robotic grasping of objects. (arXiv:2203.01051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01051">
<div class="article-summary-box-inner">
<span><p>We propose a method for 3D object reconstruction and 6D-pose estimation from
2D images that uses knowledge about object shape as the primary key. In the
proposed pipeline, recognition and labeling of objects in 2D images deliver 2D
segment silhouettes that are compared with the 2D silhouettes of projections
obtained from various views of a 3D model representing the recognized object
class. By computing transformation parameters directly from the 2D images, the
number of free parameters required during the registration process is reduced,
making the approach feasible. Furthermore, 3D transformations and projective
geometry are employed to arrive at a full 3D reconstruction of the object in
camera space using a calibrated set up. Inclusion of a second camera allows
resolving remaining ambiguities. The method is quantitatively evaluated using
synthetic data and tested with real data, and additional results for the
well-known Linemod data set are shown. In robot experiments, successful
grasping of objects demonstrates its usability in real-world environments, and,
where possible, a comparison with other methods is provided. The method is
applicable to scenarios where 3D object models, e.g., CAD-models or point
clouds, are available and precise pixel-wise segmentation maps of 2D images can
be obtained. Different from other methods, the method does not use 3D depth for
training, widening the domain of application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Anomaly Detection from Time-of-Flight Depth Images. (arXiv:2203.01052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01052">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection (VAD) addresses the problem of automatically finding
anomalous events in video data. The primary data modalities on which current
VAD systems work on are monochrome or RGB images. Using depth data in this
context instead is still hardly explored in spite of depth images being a
popular choice in many other computer vision research areas and the increasing
availability of inexpensive depth camera hardware. We evaluate the application
of existing autoencoder-based methods on depth video and propose how the
advantages of using depth data can be leveraged by integration into the loss
function. Training is done unsupervised using normal sequences without need for
any additional annotations. We show that depth allows easy extraction of
auxiliary information for scene analysis in the form of a foreground mask and
demonstrate its beneficial effect on the anomaly detection performance through
evaluation on a large public dataset, for which we are also the first ones to
present results on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colar: Effective and Efficient Online Action Detection by Consulting Exemplars. (arXiv:2203.01057v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01057">
<div class="article-summary-box-inner">
<span><p>Online action detection has attracted increasing research interests in recent
years. Current works model historical dependencies and anticipate future to
perceive the action evolution within a video segment and improve the detection
accuracy. However, the existing paradigm ignores category-level modeling and
does not pay sufficient attention to efficiency. Considering a category, its
representative frames exhibit various characteristics. Thus, the category-level
modeling can provide complementary guidance to the temporal dependencies
modeling. In this paper, we develop an effective exemplar-consultation
mechanism that first measures the similarity between a frame and exemplary
frames, and then aggregates exemplary features based on the similarity weights.
This is also an efficient mechanism as both similarity measurement and feature
aggregation require limited computations. Based on the exemplar-consultation
mechanism, the long-term dependencies can be captured by regarding historical
frames as exemplars, and the category-level modeling can be achieved by
regarding representative frames from a category as exemplars. Due to the
complementarity from the category-level modeling, our method employs a
lightweight architecture but achieves new high performance on three benchmarks.
In addition, using a spatio-temporal network to tackle video frames, our method
spends 9.8 seconds to dispose of a one-minute video and achieves comparable
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation. (arXiv:2203.01072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01072">
<div class="article-summary-box-inner">
<span><p>This paper proposes a universal framework, called OVE6D, for model-based 6D
object pose estimation from a single depth image and a target object mask. Our
model is trained using purely synthetic data rendered from ShapeNet, and,
unlike most of the existing methods, it generalizes well on new real-world
objects without any fine-tuning. We achieve this by decomposing the 6D pose
into viewpoint, in-plane rotation around the camera optical axis and
translation, and introducing novel lightweight modules for estimating each
component in a cascaded manner. The resulting network contains less than 4M
parameters while demonstrating excellent performance on the challenging T-LESS
and Occluded LINEMOD datasets without any dataset-specific training. We show
that OVE6D outperforms some contemporary deep learning-based pose estimation
methods specifically trained for individual objects or datasets with real-world
training data.
</p>
<p>The implementation and the pre-trained model will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation. (arXiv:2203.01074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01074">
<div class="article-summary-box-inner">
<span><p>Environment perception in autonomous driving vehicles often heavily relies on
deep neural networks (DNNs), which are subject to domain shifts, leading to a
significantly decreased performance during DNN deployment. Usually, this
problem is addressed by unsupervised domain adaptation (UDA) approaches trained
either simultaneously on source and target domain datasets or even source-free
only on target data in an offline fashion. In this work, we further expand a
source-free UDA approach to a continual and therefore online-capable UDA on a
single-image basis for semantic segmentation. Accordingly, our method only
requires the pre-trained model from the supplier (trained in the source domain)
and the current (unlabeled target domain) camera image. Our method Continual
BatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch
normalization layers, using target domain images in an unsupervised fashion,
which yields consistent performance improvements during inference. Thereby, in
contrast to existing works, our approach can be applied to improve a DNN
continuously on a single-image basis during deployment without access to source
data, without algorithmic delay, and nearly without computational overhead. We
show the consistent effectiveness of our method across a wide variety of
source/target domain settings for semantic segmentation. As part of this work,
our code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-based Large-scale 3D Semantic Mapping for Autonomous Driving Applications. (arXiv:2203.01087v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01087">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a complete pipeline for 3D semantic mapping solely
based on a stereo camera system. The pipeline comprises a direct sparse visual
odometry front-end as well as a back-end for global optimization including GNSS
integration, and semantic 3D point cloud labeling. We propose a simple but
effective temporal voting scheme which improves the quality and consistency of
the 3D point labels. Qualitative and quantitative evaluations of our pipeline
are performed on the KITTI-360 dataset. The results show the effectiveness of
our proposed voting scheme and the capability of our pipeline for efficient
large-scale 3D semantic mapping. The large-scale mapping capabilities of our
pipeline is furthermore demonstrated by presenting a very large-scale semantic
map covering 8000 km of roads generated from data collected by a fleet of
vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape constrained CNN for segmentation guided prediction of myocardial shape and pose parameters in cardiac MRI. (arXiv:2203.01089v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01089">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation using convolutional neural networks (CNNs) is the
state-of-the-art for many medical image segmentation tasks including myocardial
segmentation in cardiac MR images. However, the predicted segmentation maps
obtained from such standard CNN do not allow direct quantification of regional
shape properties such as regional wall thickness. Furthermore, the CNNs lack
explicit shape constraints, occasionally resulting in unrealistic
segmentations. In this paper, we use a CNN to predict shape parameters of an
underlying statistical shape model of the myocardium learned from a training
set of images. Additionally, the cardiac pose is predicted, which allows to
reconstruct the myocardial contours. The integrated shape model regularizes the
predicted contours and guarantees realistic shapes. We enforce robustness of
shape and pose prediction by simultaneously performing pixel-wise semantic
segmentation during training and define two loss functions to impose
consistency between the two predicted representations: one distance-based loss
and one overlap-based loss. We evaluated the proposed method in a 5-fold cross
validation on an in-house clinical dataset with 75 subjects and on the ACDC and
LVQuan19 public datasets. We show the benefits of simultaneous semantic
segmentation and the two newly defined loss functions for the prediction of
shape parameters. Our method achieved a correlation of 99% for left ventricular
(LV) area on the three datasets, between 91% and 97% for myocardial area,
98-99% for LV dimensions and between 80% and 92% for regional wall thickness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generalized Approach for Cancellable Template and Its Realization for Minutia Cylinder-Code. (arXiv:2203.01095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01095">
<div class="article-summary-box-inner">
<span><p>Hashing technology gains much attention in protecting the biometric template
lately. For instance, Index-of-Max (IoM), a recent reported hashing technique,
is a ranking-based locality sensitive hashing technique, which illustrates the
feasibility to protect the ordered and fixed-length biometric template.
However, biometric templates are not always in the form of ordered and
fixed-length, rather it may be an unordered and variable size point set e.g.
fingerprint minutiae, which restricts the usage of the traditional hashing
technology. In this paper, we proposed a generalized version of IoM hashing
namely gIoM, and therefore the unordered and variable size biometric template
can be used. We demonstrate a realization using a well-known variable size
feature vector, fingerprint Minutia Cylinder-Code (MCC). The gIoM transforms
MCC into index domain to form indexing-based feature representation.
Consequently, the inversion of MCC from the transformed representation is
computational infeasible, thus to achieve non-invertibility while the
performance is preserved. Public fingerprint databases FVC2002 and FVC2004 are
employed for experiment as benchmark to demonstrate a fair comparison with
other methods. Moreover, the security and privacy analysis suggest that gIoM
meets the criteria of template protection: non-invertibility, revocability, and
non-linkability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Scene Flow Estimation with 4D Automotive Radar. (arXiv:2203.01137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01137">
<div class="article-summary-box-inner">
<span><p>Scene flow allows autonomous vehicles to reason about the arbitrary motion of
multiple independent objects which is the key to long-term mobile autonomy.
While estimating the scene flow from LiDAR has progressed recently, it remains
largely unknown how to estimate the scene flow from a 4D radar - an
increasingly popular automotive sensor for its robustness against adverse
weather and lighting conditions. Compared with the LiDAR point clouds, radar
data are drastically sparser, noisier and in much lower resolution. Annotated
datasets for radar scene flow are also in absence and costly to acquire in the
real world. These factors jointly pose the radar scene flow estimation as a
challenging problem. This work aims to address the above challenges and
estimate scene flow from 4D radar point clouds by leveraging self-supervised
learning. A robust scene flow estimation architecture and three novel losses
are bespoken designed to cope with intractable radar data. Real-world
experimental results validate that our method is able to robustly estimate the
radar scene flow in the wild and effectively supports the downstream task of
motion segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Lidar-Based Semantic Segmentation of Top-View Grid Maps by Learning Features in Complementary Representations. (arXiv:2203.01151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01151">
<div class="article-summary-box-inner">
<span><p>In this paper we introduce a novel way to predict semantic information from
sparse, single-shot LiDAR measurements in the context of autonomous driving. In
particular, we fuse learned features from complementary representations. The
approach is aimed specifically at improving the semantic segmentation of
top-view grid maps. Towards this goal the 3D LiDAR point cloud is projected
onto two orthogonal 2D representations. For each representation a tailored deep
learning architecture is developed to effectively extract semantic information
which are fused by a superordinate deep neural network. The contribution of
this work is threefold: (1) We examine different stages within the segmentation
network for fusion. (2) We quantify the impact of embedding different features.
(3) We use the findings of this survey to design a tailored deep neural network
architecture leveraging respective advantages of different representations. Our
method is evaluated using the SemanticKITTI dataset which provides a point-wise
semantic annotation of more than 23.000 LiDAR measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisARM: Displacement Aware Relation Module for 3D Detection. (arXiv:2203.01152v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01152">
<div class="article-summary-box-inner">
<span><p>We introduce Displacement Aware Relation Module (DisARM), a novel neural
network module for enhancing the performance of 3D object detection in point
cloud scenes. The core idea of our method is that contextual information is
critical to tell the difference when the instance geometry is incomplete or
featureless. We find that relations between proposals provide a good
representation to describe the context. However, adopting relations between all
the object or patch proposals for detection is inefficient, and an imbalanced
combination of local and global relations brings extra noise that could mislead
the training. Rather than working with all relations, we found that training
with relations only between the most representative ones, or anchors, can
significantly boost the detection performance. A good anchor should be
semantic-aware with no ambiguity and independent with other anchors as well. To
find the anchors, we first perform a preliminary relation anchor module with an
objectness-aware sampling approach and then devise a displacement-based module
for weighing the relation importance for better utilization of contextual
information. This lightweight relation module leads to significantly higher
accuracy of object instance detection when being plugged into the
state-of-the-art detectors. Evaluations on the public benchmarks of real-world
scenes show that our method achieves state-of-the-art performance on both SUN
RGB-D and ScanNet V2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying multi-angled parallelism to Spanish topographical maps. (arXiv:2203.01169v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01169">
<div class="article-summary-box-inner">
<span><p>Multi-Angled Parallelism (MAP) is a method to recognize lines in binary
images. It is suitable to be implemented in parallel processing and image
processing hardware. The binary image is transformed into directional planes,
upon which, directional operators of erosion-dilation are iteratively applyed.
From a set of basic operators, more complex ones are created, which let to
extract the several types of lines. Each type is extracted with a different set
of operations and so the lines are identified when extracted. In this paper, an
overview of MAP is made, and it is adapted to line recognition in Spanish
topographical maps, with the double purpose of testing the method in a real
case and studying the process of adapting it to a custom application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Adversarial Perturbations in Multi-Task Perception. (arXiv:2203.01177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01177">
<div class="article-summary-box-inner">
<span><p>While deep neural networks (DNNs) achieve impressive performance on
environment perception tasks, their sensitivity to adversarial perturbations
limits their use in practical applications. In this paper, we (i) propose a
novel adversarial perturbation detection scheme based on multi-task perception
of complex vision tasks (i.e., depth estimation and semantic segmentation).
Specifically, adversarial perturbations are detected by inconsistencies between
extracted edges of the input image, the depth output, and the segmentation
output. To further improve this technique, we (ii) develop a novel edge
consistency loss between all three modalities, thereby improving their initial
consistency which in turn supports our detection scheme. We verify our
detection scheme's effectiveness by employing various known attacks and image
noises. In addition, we (iii) develop a multi-task adversarial attack, aiming
at fooling both tasks as well as our detection scheme. Experimental evaluation
on the Cityscapes and KITTI datasets shows that under an assumption of a 5%
false positive rate up to 100% of images are correctly detected as
adversarially perturbed, depending on the strength of the perturbation. Code
will be available on github. A short video at https://youtu.be/KKa6gOyWmH4
provides qualitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Robust Ground Surface Estimation from LIDAR Measurements using Uniform B-Splines. (arXiv:2203.01180v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01180">
<div class="article-summary-box-inner">
<span><p>We propose a fast and robust method to estimate the ground surface from LIDAR
measurements on an automated vehicle. The ground surface is modeled as a UBS
which is robust towards varying measurement densities and with a single
parameter controlling the smoothness prior. We model the estimation process as
a robust LS optimization problem which can be reformulated as a linear problem
and thus solved efficiently. Using the SemanticKITTI data set, we conduct a
quantitative evaluation by classifying the point-wise semantic annotations into
ground and non-ground points. Finally, we validate the approach on our research
vehicle in real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Feature Encoding for GNNs on Road Networks. (arXiv:2203.01187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01187">
<div class="article-summary-box-inner">
<span><p>In this work, we present a novel approach to learning an encoding of visual
features into graph neural networks with the application on road network data.
We propose an architecture that combines state-of-the-art vision backbone
networks with graph neural networks. More specifically, we perform a road type
classification task on an Open Street Map road network through encoding of
satellite imagery using various ResNet architectures. Our architecture further
enables fine-tuning and a transfer-learning approach is evaluated by
pretraining on the NWPU-RESISC45 image classification dataset for remote
sensing and comparing them to purely ImageNet-pretrained ResNet models as
visual feature encoders. The results show not only that the visual feature
encoders are superior to low-level visual features, but also that the
fine-tuning of the visual feature encoder to a general remote sensing dataset
such as NWPU-RESISC45 can further improve the performance of a GNN on a machine
learning task like road type classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization of Deep Networks for Estimating Physical Properties of Containers and Fillings. (arXiv:2203.01192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01192">
<div class="article-summary-box-inner">
<span><p>We present methods to estimate the physical properties of household
containers and their fillings manipulated by humans. We use a lightweight,
pre-trained convolutional neural network with coordinate attention as a
backbone model of the pipelines to accurately locate the object of interest and
estimate the physical properties in the CORSMAL Containers Manipulation (CCM)
dataset. We address the filling type classification with audio data and then
combine this information from audio with video modalities to address the
filling level classification. For the container capacity, dimension, and mass
estimation, we present a data augmentation and consistency measurement to
alleviate the over-fitting issue in the CCM dataset caused by the limited
number of containers. We augment the training data using an
object-of-interest-based re-scaling that increases the variety of physical
values of the containers. We then perform the consistency measurement to choose
a model with low prediction variance in the same containers under different
scenes, which ensures the generalization ability of the model. Our method
improves the generalization ability of the models to estimate the property of
the containers that were not previously seen in the training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAE-iForest: Auto-encoding Reconstruction and Isolation-based Anomalies Detecting Fallen Objects on Road Surface. (arXiv:2203.01193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01193">
<div class="article-summary-box-inner">
<span><p>In road monitoring, it is an important issue to detect changes in the road
surface at an early stage to prevent damage to third parties. The target of the
falling object may be a fallen tree due to the external force of a flood or an
earthquake, and falling rocks from a slope. Generative deep learning is
possible to flexibly detect anomalies of the falling objects on the road
surface. We prototype a method that combines auto-encoding reconstruction and
isolation-based anomaly detector in application for road surface monitoring.
Actually, we apply our method to a set of test images that fallen objects is
located on the raw inputs added with fallen stone and plywood, and that snow is
covered on the winter road. Finally we mention the future works for practical
purpose application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Container Localisation and Mass Estimation with an RGB-D Camera. (arXiv:2203.01207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01207">
<div class="article-summary-box-inner">
<span><p>In the research area of human-robot interactions, the automatic estimation of
the mass of a container manipulated by a person leveraging only visual
information is a challenging task. The main challenges consist of occlusions,
different filling materials and lighting conditions. The mass of an object
constitutes key information for the robot to correctly regulate the force
required to grasp the container. We propose a single RGB-D camera-based method
to locate a manipulated container and estimate its empty mass i.e.,
independently of the presence of the content. The method first automatically
selects a number of candidate containers based on the distance with the fixed
frontal view, then averages the mass predictions of a lightweight model to
provide the final estimation. Results on the CORSMAL Containers Manipulation
dataset show that the proposed method estimates empty container mass obtaining
a score of 71.08% under different lighting or filling conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A simple and universal rotation equivariant point-cloud network. (arXiv:2203.01216v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01216">
<div class="article-summary-box-inner">
<span><p>Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Tracker with Pixel and Instance for Video Panoptic Segmentation. (arXiv:2203.01217v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01217">
<div class="article-summary-box-inner">
<span><p>Video Panoptic Segmentation (VPS) requires generating consistent panoptic
segmentation and tracking identities to all pixels across video frames.
Existing methods are mainly based on the trained instance embedding to maintain
consistent panoptic segmentation. However, they inevitably struggle to cope
with the challenges of small objects, similar appearance but inconsistent
identities, occlusion, and strong instance contour deformations. To address
these problems, we present HybridTracker, a lightweight and joint tracking
model attempting to eliminate the limitations of the single tracker.
HybridTracker performs pixel tracker and instance tracker in parallel to obtain
the association matrices, which are fused into a matching matrix. In the
instance tracker, we design a differentiable matching layer, ensuring the
stability of inter-frame matching. In the pixel tracker, we compute the dice
coefficient of the same instance of different frames given the estimated
optical flow, forming the Intersection Over Union (IoU) matrix. We additionally
propose mutual check and temporal consistency constraints during inference to
settle the occlusion and contour deformation challenges. Extensive experiments
demonstrate that HybridTracker outperforms state-of-the-art methods on
Cityscapes-VPS and VIPER datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Question Answering: Datasets, Algorithms and Challenges. (arXiv:2203.01225v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01225">
<div class="article-summary-box-inner">
<span><p>Video Question Answering (VideoQA) aims to answer natural language questions
according to the given videos. It has earned increasing attention with recent
research trends in joint vision and language understanding. Yet, compared with
ImageQA, VideoQA is largely underexplored and progresses slowly. Although
different algorithms have continually been proposed and shown success on
different VideoQA datasets, we find that there lacks a meaningful survey to
categorize them, which seriously impedes its advancements. This paper thus
provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on
the datasets, algorithms, and unique challenges. We then point out the research
trend of studying beyond factoid QA to inference QA towards the cognition of
video contents, Finally, we conclude some promising directions for future
exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable IFS Fractals. (arXiv:2203.01231v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01231">
<div class="article-summary-box-inner">
<span><p>I present my explorations in rendering Iterated Function System (IFS)
fractals using a differentiable rendering pipeline. Differentiable rendering is
a recent innovation at the intersection of graphics and machine learning. This
opens up many possibilities for generating fractals that meet particular
criteria. In this paper I show how my method can be used to generate an IFS
fractal that resembles a target image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H4D: Human 4D Modeling by Learning Neural Compositional Representation. (arXiv:2203.01247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01247">
<div class="article-summary-box-inner">
<span><p>Despite the impressive results achieved by deep learning based 3D
reconstruction, the techniques of directly learning to model the 4D human
captures with detailed geometry have been less studied. This work presents a
novel framework that can effectively learn a compact and compositional
representation for dynamic human by exploiting the human body prior from the
widely-used SMPL parametric model. Particularly, our representation, named H4D,
represents dynamic 3D human over a temporal span into the latent spaces
encoding shape, initial pose, motion and auxiliary information. A simple yet
effective linear motion model is proposed to provide a rough and regularized
motion estimation, followed by per-frame compensation for pose and geometry
details with the residual encoded in the auxiliary code. Technically, we
introduce novel GRU-based architectures to facilitate learning and improve the
representation capability. Extensive experiments demonstrate our method is not
only efficacy in recovering dynamic human with accurate motion and detailed
geometry, but also amenable to various 4D human related tasks, including motion
retargeting, motion completion and future prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Query-based Paradigm for Point Cloud Understanding. (arXiv:2203.01252v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01252">
<div class="article-summary-box-inner">
<span><p>3D point cloud understanding is an important component in autonomous driving
and robotics. In this paper, we present a novel Embedding-Querying paradigm
(EQ-Paradigm) for 3D understanding tasks including detection, segmentation and
classification. EQ-Paradigm is a unified paradigm that enables the combination
of any existing 3D backbone architectures with different task heads. Under the
EQ-Paradigm, the input is firstly encoded in the embedding stage with an
arbitrary feature extraction architecture, which is independent of tasks and
heads. Then, the querying stage enables the encoded features to be applicable
for diverse task heads. This is achieved by introducing an intermediate
representation, i.e., Q-representation, in the querying stage to serve as a
bridge between the embedding stage and task heads. We design a novel Q-Net as
the querying stage network. Extensive experimental results on various 3D tasks
show that EQ-Paradigm in tandem with Q-Net is a general and effective pipeline,
which enables a flexible collaboration of backbones and heads, and further
boosts the performance of the state-of-the-art methods. All codes and models
will be published soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Transformer for Deepfake Detection. (arXiv:2203.01265v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01265">
<div class="article-summary-box-inner">
<span><p>The fast evolution and widespread of deepfake techniques in real-world
scenarios require stronger generalization abilities of face forgery detectors.
Some works capture the features that are unrelated to method-specific
artifacts, such as clues of blending boundary, accumulated up-sampling, to
strengthen the generalization ability. However, the effectiveness of these
methods can be easily corrupted by post-processing operations such as
compression. Inspired by transfer learning, neural networks pre-trained on
other large-scale face-related tasks may provide useful features for deepfake
detection. For example, lip movement has been proved to be a kind of robust and
good-transferring highlevel semantic feature, which can be learned from the
lipreading task. However, the existing method pre-trains the lip feature
extraction model in a supervised manner, which requires plenty of human
resources in data annotation and increases the difficulty of obtaining training
data. In this paper, we propose a self-supervised transformer based
audio-visual contrastive learning method. The proposed method learns mouth
motion representations by encouraging the paired video and audio
representations to be close while unpaired ones to be diverse. After
pre-training with our method, the model will then be partially fine-tuned for
deepfake detection task. Extensive experiments show that our self-supervised
method performs comparably or even better than the supervised pre-training
counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Temporal Interpolation of Radar-based Precipitation. (arXiv:2203.01277v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01277">
<div class="article-summary-box-inner">
<span><p>When providing the boundary conditions for hydrological flood models and
estimating the associated risk, interpolating precipitation at very high
temporal resolutions (e.g. 5 minutes) is essential not to miss the cause of
flooding in local regions. In this paper, we study optical flow-based
interpolation of globally available weather radar images from satellites. The
proposed approach uses deep neural networks for the interpolation of multiple
video frames, while terrain information is combined with temporarily
coarse-grained precipitation radar observation as inputs for self-supervised
training. An experiment with the Meteonet radar precipitation dataset for the
flood risk simulation in Aude, a department in Southern France (2018),
demonstrated the advantage of the proposed method over a linear interpolation
baseline, with up to 20% error reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADVISE: ADaptive Feature Relevance and VISual Explanations for Convolutional Neural Networks. (arXiv:2203.01289v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01289">
<div class="article-summary-box-inner">
<span><p>To equip Convolutional Neural Networks (CNNs) with explainability, it is
essential to interpret how opaque models take specific decisions, understand
what causes the errors, improve the architecture design, and identify unethical
biases in the classifiers. This paper introduces ADVISE, a new explainability
method that quantifies and leverages the relevance of each unit of the feature
map to provide better visual explanations. To this end, we propose using
adaptive bandwidth kernel density estimation to assign a relevance score to
each unit of the feature map with respect to the predicted class. We also
propose an evaluation protocol to quantitatively assess the visual
explainability of CNN models. We extensively evaluate our idea in the image
classification task using AlexNet, VGG16, ResNet50, and Xception pretrained on
ImageNet. We compare ADVISE with the state-of-the-art visual explainable
methods and show that the proposed method outperforms competing approaches in
quantifying feature-relevance and visual explainability while maintaining
competitive time complexity. Our experiments further show that ADVISE fulfils
the sensitivity and implementation independence axioms while passing the sanity
checks. The implementation is accessible for reproducibility purposes on
https://github.com/dehshibi/ADVISE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement. (arXiv:2203.01296v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01296">
<div class="article-summary-box-inner">
<span><p>Low-Light Image Enhancement is a computer vision task which intensifies the
dark images to appropriate brightness. It can also be seen as an ill-posed
problem in image restoration domain. With the success of deep neural networks,
the convolutional neural networks surpass the traditional algorithm-based
methods and become the mainstream in the computer vision area. To advance the
performance of enhancement algorithms, we propose an image enhancement network
(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use
a half wavelet attention block on M-Net+ to enrich the features from wavelet
domain. Furthermore, our HWMNet has competitive performance results on two
image enhancement datasets in terms of quantitative metrics and visual quality.
The source code and pretrained model are available at
https://github.com/FanChiMao/HWMNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01305">
<div class="article-summary-box-inner">
<span><p>We present in this paper a novel denoising training method to speedup DETR
(DEtection TRansformer) training and offer a deepened understanding of the slow
convergence issue of DETR-like methods. We show that the slow convergence
results from the instability of bipartite graph matching which causes
inconsistent optimization goals in early training stages. To address this
issue, except for the Hungarian loss, our method additionally feeds
ground-truth bounding boxes with noises into Transformer decoder and trains the
model to reconstruct the original boxes, which effectively reduces the
bipartite graph matching difficulty and leads to a faster convergence. Our
method is universal and can be easily plugged into any DETR-like methods by
adding dozens of lines of code to achieve a remarkable improvement. As a
result, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the
same setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and
$50$ epochs of training respectively) among DETR-like methods with ResNet-$50$
backbone. Compared with the baseline under the same setting, DN-DETR achieves
comparable performance with $50\%$ training epochs. Code is available at
\url{https://github.com/FengLi-ust/DN-DETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. (arXiv:2203.01311v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01311">
<div class="article-summary-box-inner">
<span><p>Learning multimodal representations involves discovering correspondences and
integrating information from multiple heterogeneous sources of data. While
recent research has begun to explore the design of more general-purpose
multimodal models (contrary to prior focus on domain and modality-specific
architectures), these methods are still largely focused on a small set of
modalities in the language, vision, and audio space. In order to accelerate
generalization towards diverse and understudied modalities, we investigate
methods for high-modality (a large set of diverse modalities) and
partially-observable (each task only defined on a small subset of modalities)
scenarios. To tackle these challenges, we design a general multimodal model
that enables multitask and transfer learning: multitask learning with shared
parameters enables stable parameter counts (addressing scalability), and
cross-modal transfer learning enables information sharing across modalities and
tasks (addressing partial observability). Our resulting model generalizes
across text, image, video, audio, time-series, sensors, tables, and set
modalities from different research areas, improves the tradeoff between
performance and efficiency, transfers to new modalities and tasks, and reveals
surprising insights on the nature of information sharing in multitask models.
We release our code and benchmarks which we hope will present a unified
platform for subsequent theoretical and empirical analysis:
https://github.com/pliang279/HighMMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Protecting Celebrities with Identity Consistency Transformer. (arXiv:2203.01318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01318">
<div class="article-summary-box-inner">
<span><p>In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Methods and Applications for Region of Interest Detection in Dermoscopic Images. (arXiv:1807.10711v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1807.10711">
<div class="article-summary-box-inner">
<span><p>Rapid growth in the development of medical imaging analysis technology has
been propelled by the great interest in improving computer-aided diagnosis and
detection (CAD) systems for three popular image visualization tasks:
classification, segmentation, and Region of Interest (ROI) detection. However,
a limited number of datasets with ground truth annotations are available for
developing segmentation and ROI detection of lesions, as expert annotations are
laborious and expensive. Detecting the ROI is vital to locate lesions
accurately. In this paper, we propose the use of two deep object detection
meta-architectures (Faster R-CNN Inception-V2 and SSD Inception-V2) to develop
robust ROI detection of skin lesions in dermoscopic datasets (2017 ISIC
Challenge, PH2, and HAM10000), and compared the performance with
state-of-the-art segmentation algorithm (DeeplabV3+). To further demonstrate
the potential of our work, we built a smartphone application for real-time
automated detection of skin lesions based on this methodology. In addition, we
developed an automated natural data-augmentation method from ROI detection to
produce augmented copies of dermoscopic images, as a pre-processing step in the
segmentation of skin lesions to further improve the performance of the current
state-of-the-art deep learning algorithm. Our proposed ROI detection has the
potential to more appropriately streamline dermatology referrals and reduce
unnecessary biopsies in the diagnosis of skin cancer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ricci Curvature Based Volumetric Segmentation of the Auditory Ossicles. (arXiv:2006.14788v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14788">
<div class="article-summary-box-inner">
<span><p>The auditory ossicles that are located in the middle ear are the smallest
bones in the human body. Their damage will result in hearing loss. It is
therefore important to be able to automatically diagnose ossicles' diseases
based on Computed Tomography (CT) 3D imaging. However CT images usually include
the whole head area, which is much larger than the bones of interest, thus the
localization of the ossicles, followed by segmentation, both play a significant
role in automatic diagnosis. The commonly employed local segmentation methods
require manually selected initial points, which is a highly time consuming
process. We therefore propose a completely automatic method to locate the
ossicles which requires neither templates, nor manual labels. It relies solely
on the connective properties of the auditory ossicles themselves, and their
relationship with the surrounding tissue fluid. For the segmentation task, we
define a novel energy function and obtain the shape of the ossicles from the 3D
CT image by minimizing this new energy. Compared to the state-of-the-art
methods which usually use the gradient operator and some normalization terms,
we propose to add a Ricci curvature term to the commonly employed energy
function. We compare our proposed method with the state-of-the-art methods and
show that the performance of discrete Forman-Ricci curvature is superior to the
others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manipulation-Oriented Object Perception in Clutter through Affordance Coordinate Frames. (arXiv:2010.08202v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08202">
<div class="article-summary-box-inner">
<span><p>In order to enable robust operation in unstructured environments, robots
should be able to generalize manipulation actions to novel object instances.
For example, to pour and serve a drink, a robot should be able to recognize
novel containers which afford the task. Most importantly, robots should be able
to manipulate these novel containers to fulfill the task. To achieve this, we
aim to provide robust and generalized perception of object affordances and
their associated manipulation poses for reliable manipulation. In this work, we
combine the notions of affordance and category-level pose, and introduce the
Affordance Coordinate Frame (ACF). With ACF, we represent each object class in
terms of individual affordance parts and the compatibility between them, where
each part is associated with a part category-level pose for robot manipulation.
In our experiments, we demonstrate that ACF outperforms state-of-the-art
methods for object detection, as well as category-level pose estimation for
object parts. We further demonstrate the applicability of ACF to robot
manipulation tasks through experiments in a simulated environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Efficient GANs for Image Translation via Differentiable Masks and co-Attention Distillation. (arXiv:2011.08382v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08382">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) have been widely-used in image
translation, but their high computation and storage costs impede the deployment
on mobile devices. Prevalent methods for CNN compression cannot be directly
applied to GANs due to the peculiarties of GAN tasks and the unstable
adversarial training. To solve these, in this paper, we introduce a novel GAN
compression method, termed DMAD, by proposing a Differentiable Mask and a
co-Attention Distillation. The former searches for a light-weight generator
architecture in a training-adaptive manner. To overcome channel inconsistency
when pruning the residual connections, an adaptive cross-block group sparsity
is further incorporated. The latter simultaneously distills informative
attention maps from both the generator and discriminator of a pre-trained model
to the searched generator, effectively stabilizing the adversarial training of
our light-weight model. Experiments show that DMAD can reduce the Multiply
Accumulate Operations (MACs) of CycleGAN by 13x and that of Pix2Pix by 4x while
retaining a comparable performance against the full model. Our code can be
available at https://github.com/SJLeo/DMAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05567">
<div class="article-summary-box-inner">
<span><p>Model explanations such as saliency maps can improve user trust in AI by
highlighting important features for a prediction. However, these become
distorted and misleading when explaining predictions of images that are subject
to systematic error (bias) by perturbations and corruptions. Furthermore, the
distortions persist despite model fine-tuning on images biased by different
factors (blur, color temperature, day/night). We present Debiased-CAM to
recover explanation faithfulness across various bias types and levels by
training a multi-input, multi-task model with auxiliary tasks for explanation
and bias level predictions. In simulation studies, the approach not only
enhanced prediction accuracy, but also generated highly faithful explanations
about these predictions as if the images were unbiased. In user studies,
debiased explanations improved user task performance, perceived truthfulness
and perceived helpfulness. Debiased training can provide a versatile platform
for robust performance and explanation faithfulness for a wide range of
applications with data biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Class-Agnostic Pseudo Mask Generation for Box-Supervised Semantic Segmentation. (arXiv:2103.05463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05463">
<div class="article-summary-box-inner">
<span><p>Recently, several weakly supervised learning methods have been devoted to
utilize bounding box supervision for training deep semantic segmentation
models. Most existing methods usually leverage the generic proposal generators
(e.g., dense CRF and MCG) to produce enhanced segmentation masks for further
training segmentation models. These proposal generators, however, are generic
and not specifically designed for box-supervised semantic segmentation, thereby
leaving some leeway for improving segmentation performance. In this paper, we
aim at seeking for a more accurate learning-based class-agnostic pseudo mask
generator tailored to box-supervised semantic segmentation. To this end, we
resort to a pixel-level annotated auxiliary dataset where the class labels are
non-overlapped with those of the box-annotated dataset. For learning pseudo
mask generator from the auxiliary dataset, we present a bi-level optimization
formulation. In particular, the lower subproblem is used to learn
box-supervised semantic segmentation, while the upper subproblem is used to
learn an optimal class-agnostic pseudo mask generator. The learned pseudo
segmentation mask generator can then be deployed to the box-annotated dataset
for improving weakly supervised semantic segmentation. Experiments on PASCAL
VOC 2012 dataset show that the learned pseudo mask generator is effective in
boosting segmentation performance, and our method can further close the
performance gap between box-supervised and fully-supervised models. Our code
will be made publicly available at
https://github.com/Vious/LPG_BBox_Segmentation .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepChange: A Large Long-Term Person Re-Identification Benchmark with Clothes Change. (arXiv:2105.14685v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14685">
<div class="article-summary-box-inner">
<span><p>Existing person re-identification (re-id) works mostly consider short-term
application scenarios without clothes change. In real-world, however, we often
dress differently across space and time. To solve this contrast, a few recent
attempts have been made on long-term re-id with clothes change. Currently, one
of the most significant limitations in this field is the lack of a large
realistic benchmark. In this work, we contribute a large, realistic long-term
person re-identification benchmark, named as DeepChange. It has several unique
characteristics: (1) Realistic and rich personal appearance (e.g., clothes and
hair style) and variations: Highly diverse clothes change and styles, with
varying reappearing gaps in time from minutes to seasons, different weather
conditions (e.g., sunny, cloudy, windy, rainy, snowy, extremely cold) and
events (e.g., working, leisure, daily activities). (2) Rich camera setups: Raw
videos were recorded by 17 outdoor varying resolution cameras operating in a
real-world surveillance system. (3) The currently largest number of (17)
cameras, (1, 121) identities, and (178, 407) bounding boxes, over the longest
time span (12 months). Further, we investigate multimodal fusion strategies for
tackling the clothes change challenge. Extensive experiments show that our
fusion models outperform a wide variety of state-of-the-art models on
DeepChange. Our dataset and documents are available at
https://github.com/PengBoXiangShang/deepchange.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse PointPillars: Maintaining and Exploiting Input Sparsity to Improve Runtime on Embedded Systems. (arXiv:2106.06882v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06882">
<div class="article-summary-box-inner">
<span><p>Bird's Eye View (BEV) is a popular representation for processing 3D point
clouds, and by its nature is fundamentally sparse. Motivated by the
computational limitations of mobile robot platforms, we create a fast,
high-performance BEV 3D object detector that maintains and exploits this input
sparsity to decrease runtimes over non-sparse baselines and avoids the tradeoff
between pseudoimage area and runtime. We present results on KITTI, a canonical
3D detection dataset, and Matterport-Chair, a novel Matterport3D-derived chair
detection dataset from scenes in real furnished homes. We evaluate runtime
characteristics using a desktop GPU, an embedded ML accelerator, and a robot
CPU, demonstrating that our method results in significant detection speedups
(2X or more) for embedded systems with only a modest decrease in detection
quality. Our work represents a new approach for practitioners to optimize
models for embedded systems by maintaining and exploiting input sparsity
throughout their entire pipeline to reduce runtime and resource usage while
preserving detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration. (arXiv:2107.02433v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02433">
<div class="article-summary-box-inner">
<span><p>In order to tackle the difficulty associated with the ill-posed nature of the
image registration problem, regularization is often used to constrain the
solution space. For most learning-based registration approaches, the
regularization usually has a fixed weight and only constrains the spatial
transformation. Such convention has two limitations: (i) Besides the laborious
grid search for the optimal fixed weight, the regularization strength of a
specific image pair should be associated with the content of the images, thus
the "one value fits all" training scheme is not ideal; (ii) Only spatially
regularizing the transformation may neglect some informative clues related to
the ill-posedness. In this study, we propose a mean-teacher based registration
framework, which incorporates an additional temporal consistency regularization
term by encouraging the teacher model's prediction to be consistent with that
of the student model. More importantly, instead of searching for a fixed
weight, the teacher enables automatically adjusting the weights of the spatial
regularization and the temporal consistency regularization by taking advantage
of the transformation uncertainty and appearance uncertainty. Extensive
experiments on the challenging abdominal CT-MRI registration show that our
training strategy can promisingly advance the original learning-based method in
terms of efficient hyperparameter tuning and a better tradeoff between accuracy
and smoothness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Local-Global Contextual Adaptation for Multi-Person Pose Estimation. (arXiv:2109.03622v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03622">
<div class="article-summary-box-inner">
<span><p>This paper studies the problem of multi-person pose estimation in a bottom-up
fashion. With a new and strong observation that the localization issue of the
center-offset formulation can be remedied in a local-window search scheme in an
ideal situation, we propose a multi-person pose estimation approach, dubbed as
LOGO-CAP, by learning the LOcal-GlObal Contextual Adaptation for human Pose.
Specifically, our approach learns the keypoint attraction maps (KAMs) from the
local keypoints expansion maps (KEMs) in small local windows in the first step,
which are subsequently treated as dynamic convolutional kernels on the
keypoints-focused global heatmaps for contextual adaptation, achieving accurate
multi-person pose estimation. Our method is end-to-end trainable with near
real-time inference speed in a single forward pass, obtaining state-of-the-art
performance on the COCO keypoint benchmark for bottom-up human pose estimation.
With the COCO trained model, our method also outperforms prior arts by a large
margin on the challenging OCHuman dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotionHint: Self-Supervised Monocular Visual Odometry with Motion Constraints. (arXiv:2109.06768v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06768">
<div class="article-summary-box-inner">
<span><p>We present a novel self-supervised algorithm named MotionHint for monocular
visual odometry (VO) that takes motion constraints into account. A key aspect
of our approach is to use an appropriate motion model that can help existing
self-supervised monocular VO (SSM-VO) algorithms to overcome issues related to
the local minima within their self-supervised loss functions. The motion model
is expressed with a neural network named PPnet. It is trained to coarsely
predict the next pose of the camera and the uncertainty of this prediction. Our
self-supervised approach combines the original loss and the motion loss, which
is the weighted difference between the prediction and the generated ego-motion.
Taking two existing SSM-VO systems as our baseline, we evaluate our MotionHint
algorithm on the standard KITTI benchmark. Experimental results show that our
MotionHint algorithm can be easily applied to existing open-sourced
state-of-the-art SSM-VO systems to greatly improve the performance by reducing
the resulting ATE by up to 28.73%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07036">
<div class="article-summary-box-inner">
<span><p>Recently, DETR pioneered the solution of vision tasks with transformers, it
directly translates the image feature map into the object detection result.
Though effective, translating the full feature map can be costly due to
redundant computation on some area like the background. In this work, we
encapsulate the idea of reducing spatial redundancy into a novel poll and pool
(PnP) sampling module, with which we build an end-to-end PnP-DETR architecture
that adaptively allocates its computation spatially to be more efficient.
Concretely, the PnP module abstracts the image feature map into fine foreground
object feature vectors and a small number of coarse background contextual
feature vectors. The transformer models information interaction within the
fine-coarse feature space and translates the features into the detection
result. Moreover, the PnP-augmented model can instantly achieve various desired
trade-offs between performance and computation with a single model by varying
the sampled feature length, without requiring to train multiple models as
existing methods. Thus it offers greater flexibility for deployment in diverse
scenarios with varying computation constraint. We further validate the
generalizability of the PnP module on panoptic segmentation and the recent
transformer-based image recognition model ViT and show consistent efficiency
gain. We believe our method makes a step for efficient visual analysis with
transformers, wherein spatial redundancy is commonly observed. Code will be
available at \url{https://github.com/twangnh/pnp-detr}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S3LAM: Structured Scene SLAM. (arXiv:2109.07339v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07339">
<div class="article-summary-box-inner">
<span><p>We propose a new SLAM system that uses the semantic segmentation of objects
and structures in the scene. Semantic information is relevant as it contains
high level information which may make SLAM more accurate and robust. Our
contribution is twofold: i) A new SLAM system based on ORB-SLAM2 that creates a
semantic map made of clusters of points corresponding to objects instances and
structures in the scene. ii) A modification of the classical Bundle Adjustment
formulation to constrain each cluster using geometrical priors, which improves
both camera localization and reconstruction and enables a better understanding
of the scene. We evaluate our approach on sequences from several public
datasets and show that it improves camera pose estimation with respect to state
of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Human Pose Triangulation. (arXiv:2110.00280v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00280">
<div class="article-summary-box-inner">
<span><p>We address the problem of generalizability for multi-view 3D human pose
estimation. The standard approach is to first detect 2D keypoints in images and
then apply triangulation from multiple views. Even though the existing methods
achieve remarkably accurate 3D pose estimation on public benchmarks, most of
them are limited to a single spatial camera arrangement and their number.
Several methods address this limitation but demonstrate significantly degraded
performance on novel views. We propose a stochastic framework for human pose
triangulation and demonstrate a superior generalization across different camera
arrangements on two public datasets. In addition, we apply the same approach to
the fundamental matrix estimation problem, showing that the proposed method can
successfully apply to other computer vision problems. The stochastic framework
achieves more than 8.8% improvement on the 3D pose estimation task, compared to
the state-of-the-art, and more than 30% improvement for fundamental matrix
estimation, compared to a standard algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector-quantized Image Modeling with Improved VQGAN. (arXiv:2110.04627v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04627">
<div class="article-summary-box-inner">
<span><p>Pretraining language models with next-token prediction on massive text
corpora has delivered phenomenal zero-shot, few-shot, transfer learning and
multi-tasking capabilities on both generative and discriminative language
tasks. Motivated by this success, we explore a Vector-quantized Image Modeling
(VIM) approach that involves pretraining a Transformer to predict rasterized
image tokens autoregressively. The discrete image tokens are encoded from a
learned Vision-Transformer-based VQGAN (ViT-VQGAN). We first propose multiple
improvements over vanilla VQGAN from architecture to codebook learning,
yielding better efficiency and reconstruction fidelity. The improved ViT-VQGAN
further improves vector-quantized image modeling tasks, including
unconditional, class-conditioned image generation and unsupervised
representation learning. When trained on ImageNet at 256x256 resolution, we
achieve Inception Score (IS) of 175.1 and Fr'echet Inception Distance (FID) of
4.17, a dramatic improvement over the vanilla VQGAN, which obtains 70.6 and
17.04 for IS and FID, respectively. Based on ViT-VQGAN and unsupervised
pretraining, we further evaluate the pretrained Transformer by averaging
intermediate features, similar to Image GPT (iGPT). This ImageNet-pretrained
VIM-L significantly beats iGPT-L on linear-probe accuracy from 60.3% to 72.2%
for a similar model size. ViM-L also outperforms iGPT-XL which is trained with
extra web image data and larger model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Optimal Correlational Object Search. (arXiv:2110.09991v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09991">
<div class="article-summary-box-inner">
<span><p>In realistic applications of object search, robots will need to locate target
objects in complex environments while coping with unreliable sensors,
especially for small or hard-to-detect objects. In such settings, correlational
information can be valuable for planning efficiently. Previous approaches that
consider correlational information typically resort to ad-hoc, greedy search
strategies. We introduce the Correlational Object Search POMDP (COS-POMDP),
which models correlations while preserving optimal solutions with a reduced
state space. We propose a hierarchical planning algorithm to scale up
COS-POMDPs for practical domains. Our evaluation, conducted with the AI2-THOR
household simulator and the YOLOv5 object detector, shows that our method finds
objects more successfully and efficiently compared to baselines,particularly
for hard-to-detect objects such as srub brush and remote control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Image Patch is a Wave: Quantum Inspired Vision MLP. (arXiv:2111.12294v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12294">
<div class="article-summary-box-inner">
<span><p>In the field of computer vision, recent works show that a pure MLP
architecture mainly stacked by fully-connected layers can achieve competing
performance with CNN and transformer. An input image of vision MLP is usually
split into multiple tokens (patches), while the existing MLP models directly
aggregate them with fixed weights, neglecting the varying semantic information
of tokens from different images. To dynamically aggregate tokens, we propose to
represent each token as a wave function with two parts, amplitude and phase.
Amplitude is the original feature and the phase term is a complex value
changing according to the semantic contents of input images. Introducing the
phase term can dynamically modulate the relationship between tokens and fixed
weights in MLP. Based on the wave-like token representation, we establish a
novel Wave-MLP architecture for vision tasks. Extensive experiments demonstrate
that the proposed Wave-MLP is superior to the state-of-the-art MLP
architectures on various vision tasks such as image classification, object
detection and semantic segmentation. The source code will be available at
https://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch and
https://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGCI-System: Towards Sample-Efficient, Generalizable, Compositional, and Incremental Robot Learning. (arXiv:2111.14693v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14693">
<div class="article-summary-box-inner">
<span><p>Building general-purpose robots to perform a diverse range of tasks in a
large variety of environments in the physical world at the human level is
extremely challenging. It requires the robot learning to be sample-efficient,
generalizable, compositional, and incremental. In this work, we introduce a
systematic learning framework called SAGCI-system towards achieving these above
four requirements. Our system first takes the raw point clouds gathered by the
camera mounted on the robot's wrist as the inputs and produces initial modeling
of the surrounding environment represented as a file of Unified Robot
Description Format (URDF). Our system adopts a learning-augmented
differentiable simulation that loads the URDF. The robot then utilizes the
interactive perception to interact with the environment to online verify and
modify the URDF. Leveraging the differentiable simulation, we propose a
model-based learning algorithm combining object-centric and robot-centric
stages to efficiently produce policies to accomplish manipulation tasks. We
apply our system to perform articulated object manipulation tasks, both in the
simulation and the real world. Extensive experiments demonstrate the
effectiveness of our proposed learning framework. Supplemental materials and
videos are available on https://sites.google.com/view/egci.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Camera Self-Calibration from Video. (arXiv:2112.03325v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03325">
<div class="article-summary-box-inner">
<span><p>Camera calibration is integral to robotics and computer vision algorithms
that seek to infer geometric properties of the scene from visual input streams.
In practice, calibration is a laborious procedure requiring specialized data
collection and careful tuning. This process must be repeated whenever the
parameters of the camera change, which can be a frequent occurrence for mobile
robots and autonomous vehicles. In contrast, self-supervised depth and
ego-motion estimation approaches can bypass explicit calibration by inferring
per-frame projection models that optimize a view synthesis objective. In this
paper, we extend this approach to explicitly calibrate a wide range of cameras
from raw videos in the wild. We propose a learning algorithm to regress
per-sequence calibration parameters using an efficient family of general camera
models. Our procedure achieves self-calibration results with sub-pixel
reprojection error, outperforming other learning-based methods. We validate our
approach on a wide variety of camera geometries, including perspective,
fisheye, and catadioptric. Finally, we show that our approach leads to
improvements in the downstream task of depth estimation, achieving
state-of-the-art results on the EuRoC dataset with greater computational
efficiency than contemporary methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering. (arXiv:2112.04312v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04312">
<div class="article-summary-box-inner">
<span><p>In this work we develop a generalizable and efficient Neural Radiance Field
(NeRF) pipeline for high-fidelity free-viewpoint human body synthesis under
settings with sparse camera views. Though existing NeRF-based methods can
synthesize rather realistic details for human body, they tend to produce poor
results when the input has self-occlusion, especially for unseen humans under
sparse views. Moreover, these methods often require a large number of sampling
points for rendering, which leads to low efficiency and limits their real-world
applicability. To address these challenges, we propose a Geometry-guided
Progressive NeRF~(GP-NeRF). In particular, to better tackle self-occlusion, we
devise a geometry-guided multi-view feature integration approach that utilizes
the estimated geometry prior to integrate the incomplete information from input
views and construct a complete geometry volume for the target human body.
Meanwhile, for achieving higher rendering efficiency, we introduce a
geometry-guided progressive rendering pipeline, which leverages the geometric
feature volume and the predicted density values to progressively reduce the
number of sampling points and speed up the rendering process. Experiments on
the ZJU-MoCap and THUman datasets show that our method outperforms the
state-of-the-arts significantly across multiple generalization settings, while
the time cost is reduced &gt;70% via applying our efficient progressive rendering
pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. (arXiv:2112.05139v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05139">
<div class="article-summary-box-inner">
<span><p>We present CLIP-NeRF, a multi-modal 3D object manipulation method for neural
radiance fields (NeRF). By leveraging the joint language-image embedding space
of the recent Contrastive Language-Image Pre-Training (CLIP) model, we propose
a unified framework that allows manipulating NeRF in a user-friendly way, using
either a short text prompt or an exemplar image. Specifically, to combine the
novel view synthesis capability of NeRF and the controllable manipulation
ability of latent representations from generative models, we introduce a
disentangled conditional NeRF architecture that allows individual control over
both shape and appearance. This is achieved by performing the shape
conditioning via applying a learned deformation field to the positional
encoding and deferring color conditioning to the volumetric rendering stage. To
bridge this disentangled latent representation to the CLIP embedding, we design
two code mappers that take a CLIP embedding as input and update the latent
codes to reflect the targeted editing. The mappers are trained with a
CLIP-based matching loss to ensure the manipulation accuracy. Furthermore, we
propose an inverse optimization method that accurately projects an input image
to the latent codes for manipulation to enable editing on real images. We
evaluate our approach by extensive experiments on a variety of text prompts and
exemplar images and also provide an intuitive interface for interactive
editing. Our implementation is available at
https://cassiepython.github.io/clipnerf/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HairCLIP: Design Your Hair by Text and Reference Image. (arXiv:2112.05142v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05142">
<div class="article-summary-box-inner">
<span><p>Hair editing is an interesting and challenging problem in computer vision and
graphics. Many existing methods require well-drawn sketches or masks as
conditional inputs for editing, however these interactions are neither
straightforward nor efficient. In order to free users from the tedious
interaction process, this paper proposes a new hair editing interaction mode,
which enables manipulating hair attributes individually or jointly based on the
texts or reference images provided by users. For this purpose, we encode the
image and text conditions in a shared embedding space and propose a unified
hair editing framework by leveraging the powerful image text representation
capability of the Contrastive Language-Image Pre-Training (CLIP) model. With
the carefully designed network structures and loss functions, our framework can
perform high-quality hair editing in a disentangled manner. Extensive
experiments demonstrate the superiority of our approach in terms of
manipulation accuracy, visual realism of editing results, and irrelevant
attribute preservation. Project repo is https://github.com/wty-ustc/HairCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to integrate vision data into road network data. (arXiv:2112.10624v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10624">
<div class="article-summary-box-inner">
<span><p>Road networks are the core infrastructure for connected and autonomous
vehicles, but creating meaningful representations for machine learning
applications is a challenging task. In this work, we propose to integrate
remote sensing vision data into road network data for improved embeddings with
graph neural networks. We present a segmentation of road edges based on
spatio-temporal road and traffic characteristics, which allows to enrich the
attribute set of road networks with visual features of satellite imagery and
digital surface models. We show that both, the segmentation and the integration
of vision data can increase performance on a road type classification task, and
we achieve state-of-the-art performance on the OSM+DiDi Chuxing dataset on
Chengdu, China.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond. (arXiv:2201.03176v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03176">
<div class="article-summary-box-inner">
<span><p>Pedestrian detection is the cornerstone of many vision based applications,
starting from object tracking to video surveillance and more recently,
autonomous driving. With the rapid development of deep learning in object
detection, pedestrian detection has achieved very good performance in
traditional single-dataset training and evaluation setting. However, in this
study on generalizable pedestrian detectors, we show that, current pedestrian
detectors poorly handle even small domain shifts in cross-dataset evaluation.
We attribute the limited generalization to two main factors, the method and the
current sources of data. Regarding the method, we illustrate that biasness
present in the design choices (e.g anchor settings) of current pedestrian
detectors are the main contributing factor to the limited generalization. Most
modern pedestrian detectors are tailored towards target dataset, where they do
achieve high performance in traditional single training and testing pipeline,
but suffer a degrade in performance when evaluated through cross-dataset
evaluation. Consequently, a general object detector performs better in
cross-dataset evaluation compared with state of the art pedestrian detectors,
due to its generic design. As for the data, we show that the autonomous driving
benchmarks are monotonous in nature, that is, they are not diverse in scenarios
and dense in pedestrians. Therefore, benchmarks curated by crawling the web
(which contain diverse and dense scenarios), are an efficient source of
pre-training for providing a more robust representation. Accordingly, we
propose a progressive fine-tuning strategy which improves generalization. Code
and models can accessed at https://github.com/hasanirtiza/Pedestron.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A ConvNet for the 2020s. (arXiv:2201.03545v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03545">
<div class="article-summary-box-inner">
<span><p>The "Roaring 20s" of visual recognition began with the introduction of Vision
Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art
image classification model. A vanilla ViT, on the other hand, faces
difficulties when applied to general computer vision tasks such as object
detection and semantic segmentation. It is the hierarchical Transformers (e.g.,
Swin Transformers) that reintroduced several ConvNet priors, making
Transformers practically viable as a generic vision backbone and demonstrating
remarkable performance on a wide variety of vision tasks. However, the
effectiveness of such hybrid approaches is still largely credited to the
intrinsic superiority of Transformers, rather than the inherent inductive
biases of convolutions. In this work, we reexamine the design spaces and test
the limits of what a pure ConvNet can achieve. We gradually "modernize" a
standard ResNet toward the design of a vision Transformer, and discover several
key components that contribute to the performance difference along the way. The
outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt.
Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably
with Transformers in terms of accuracy and scalability, achieving 87.8%
ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection
and ADE20K segmentation, while maintaining the simplicity and efficiency of
standard ConvNets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHSnet: Multi-head and Spatial Attention Network with False-Positive Reduction for Pulmonary Nodules Detection. (arXiv:2201.13392v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13392">
<div class="article-summary-box-inner">
<span><p>The mortality of lung cancer has ranked high among cancers for many years.
Early detection of lung cancer is critical for disease prevention, cure, and
mortality rate reduction. However, existing detection methods on pulmonary
nodules introduce an excessive number of false positive proposals in order to
achieve high sensitivity, which is not practical in clinical situations. In
this paper, we propose the multi-head detection and spatial
squeeze-and-attention network, MHSnet, to detect pulmonary nodules, in order to
aid doctors in the early diagnosis of lung cancers. Specifically, we first
introduce multi-head detectors and skip connections to customize for the
variety of nodules in sizes, shapes and types and capture multi-scale features.
Then, we implement a spatial attention module to enable the network to focus on
different regions differently inspired by how experienced clinicians screen CT
images, which results in fewer false positive proposals. Lastly, we present a
lightweight but effective false positive reduction module with the Linear
Regression model to cut down the number of false positive proposals, without
any constraints on the front network. Extensive experimental results compared
with the state-of-the-art models have shown the superiority of the MHSnet in
terms of the average FROC, sensitivity and especially false discovery rate
(2.98% and 2.18% improvement in terms of average FROC and sensitivity, 5.62%
and 28.33% decrease in terms of false discovery rate and average candidates per
scan). The false positive reduction module significantly decreases the average
number of candidates generated per scan by 68.11% and the false discovery rate
by 13.48%, which is promising to reduce distracted proposals for the downstream
tasks based on the detection results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Crafting Better Contrastive Views for Siamese Representation Learning. (arXiv:2202.03278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03278">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised contrastive learning methods greatly benefit from the
Siamese structure that aims at minimizing distances between positive pairs. For
high performance Siamese representation learning, one of the keys is to design
good contrastive pairs. Most previous works simply apply random sampling to
make different crops of the same image, which overlooks the semantic
information that may degrade the quality of views. In this work, we propose
ContrastiveCrop, which could effectively generate better crops for Siamese
representation learning. Firstly, a semantic-aware object localization strategy
is proposed within the training process in a fully unsupervised manner. This
guides us to generate contrastive views which could avoid most false positives
(i.e., object vs. background). Moreover, we empirically find that views with
similar appearances are trivial for the Siamese model training. Thus, a
center-suppressed sampling is further designed to enlarge the variance of
crops. Remarkably, our method takes a careful consideration of positive pairs
for contrastive learning with negligible extra training overhead. As a
plug-and-play and framework-agnostic module, ContrastiveCrop consistently
improves SimCLR, MoCo, BYOL, SimSiam by 0.4% ~ 2.0% classification accuracy on
CIFAR-10, CIFAR-100, Tiny ImageNet and STL-10. Superior results are also
achieved on downstream detection and segmentation tasks when pre-trained on
ImageNet-1K.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling. (arXiv:2202.03543v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03543">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge
and SUPERB benchmark. Our submissions are based on the recently proposed
FaST-VGS model, which is a Transformer-based model that learns to associate raw
speech waveforms with semantically related images, all without the use of any
transcriptions of the speech. Additionally, we introduce a novel extension of
this model, FaST-VGS+, which is learned in a multi-task fashion with a masked
language modeling objective in addition to the visual grounding objective. On
ZeroSpeech 2021, we show that our models perform competitively on the ABX task,
outperform all other concurrent submissions on the Syntactic and Semantic
tasks, and nearly match the best system on the Lexical task. On the SUPERB
benchmark, we show that our models also achieve strong performance, in some
cases even outperforming the popular wav2vec2.0 model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Texture Information into Dimensionality Reduction for High-Dimensional Images. (arXiv:2202.09179v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09179">
<div class="article-summary-box-inner">
<span><p>High-dimensional imaging is becoming increasingly relevant in many fields
from astronomy and cultural heritage to systems biology. Visual exploration of
such high-dimensional data is commonly facilitated by dimensionality reduction.
However, common dimensionality reduction methods do not include spatial
information present in images, such as local texture features, into the
construction of low-dimensional embeddings. Consequently, exploration of such
data is typically split into a step focusing on the attribute space followed by
a step focusing on spatial information, or vice versa. In this paper, we
present a method for incorporating spatial neighborhood information into
distance-based dimensionality reduction methods, such as t-Distributed
Stochastic Neighbor Embedding (t-SNE). We achieve this by modifying the
distance measure between high-dimensional attribute vectors associated with
each pixel such that it takes the pixel's spatial neighborhood into account.
Based on a classification of different methods for comparing image patches, we
explore a number of different approaches. We compare these approaches from a
theoretical and experimental point of view. Finally, we illustrate the value of
the proposed methods by qualitative and quantitative evaluation on synthetic
data and two real-world use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Language Pre-Training with Triple Contrastive Learning. (arXiv:2202.10401v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10401">
<div class="article-summary-box-inner">
<span><p>Vision-language representation learning largely benefits from image-text
alignment through contrastive losses (e.g., InfoNCE loss). The success of this
alignment strategy is attributed to its capability in maximizing the mutual
information (MI) between an image and its matched text. However, simply
performing cross-modal alignment (CMA) ignores data potential within each
modality, which may result in degraded representations. For instance, although
CMA-based models are able to map image-text pairs close together in the
embedding space, they fail to ensure that similar inputs from the same modality
stay close by. This problem can get even worse when the pre-training data is
noisy. In this paper, we propose triple contrastive learning (TCL) for
vision-language pre-training by leveraging both cross-modal and intra-modal
self-supervision. Besides CMA, TCL introduces an intra-modal contrastive
objective to provide complementary benefits in representation learning. To take
advantage of localized and structural information from image and text input,
TCL further maximizes the average MI between local regions of image/text and
their global summary. To the best of our knowledge, ours is the first work that
takes into account local structure information for multi-modality
representation learning. Experimental evaluations show that our approach is
competitive and achieve the new state of the art on various common down-stream
vision-language tasks such as image-text retrieval and visual question
answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?. (arXiv:2202.12426v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12426">
<div class="article-summary-box-inner">
<span><p>While several works have studied the vulnerability of automated FRS and have
proposed morphing attack detection (MAD) methods, very few have focused on
studying the human ability to detect morphing attacks. The examiner/observer's
face morph detection ability is based on their observation, domain knowledge,
experience, and familiarity with the problem, and no works report the detailed
findings from observers who check identity documents as a part of their
everyday professional life. This work creates a new benchmark database of
realistic morphing attacks from 48 unique subjects leading to 400 morphed
images presented to the observers in a Differential-MAD (D-MAD) setting. Unlike
the existing databases, the newly created morphed image database has been
created with careful considerations to age, gender and ethnicity to create
realistic morph attacks. Further, unlike the previous works, we also capture
ten images from Automated Border Control (ABC) gates to mimic the realistic
D-MAD setting leading to 400 probe images in border crossing scenarios. The
newly created dataset is further used to study the ability of human observers'
ability to detect morphed images. In addition, a new dataset of 180 morphed
images is also created using the FRGCv2 dataset under the Single Image-MAD
(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a
new evaluation platform is created to conduct S-MAD and D-MAD analysis. The
benchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who
are primarily governmental employees from more than 40 countries. The analysis
provides interesting insights and points to expert observers' missing
competence and failure to detect a considerable amount of morphing attacks.
Human observers tend to detect morphed images to a lower accuracy as compared
to the automated MAD algorithms evaluated in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12785">
<div class="article-summary-box-inner">
<span><p>Calibrated confidence estimates obtained from neural networks are crucial,
particularly for safety-critical applications such as autonomous driving or
medical image diagnosis. However, although the task of confidence calibration
has been investigated on classification problems, thorough investigations on
object detection and segmentation problems are still missing. Therefore, we
focus on the investigation of confidence calibration for object detection and
segmentation models in this chapter. We introduce the concept of multivariate
confidence calibration that is an extension of well-known calibration methods
to the task of object detection and segmentation. This allows for an extended
confidence calibration that is also aware of additional features such as
bounding box/pixel position, shape information, etc. Furthermore, we extend the
expected calibration error (ECE) to measure miscalibration of object detection
and segmentation models. We examine several network architectures on MS COCO as
well as on Cityscapes and show that especially object detection as well as
instance segmentation models are intrinsically miscalibrated given the
introduced definition of calibration. Using our proposed calibration methods,
we have been able to improve calibration so that it also has a positive impact
on the quality of segmentation masks as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Multi-scale SwinTransformer-HTC with Data augmentation in CoNIC Challenge. (arXiv:2202.13588v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13588">
<div class="article-summary-box-inner">
<span><p>Colorectal cancer is one of the most common cancers worldwide, so early
pathological examination is very important. However, it is time-consuming and
labor-intensive to identify the number and type of cells on H&amp;E images in
clinical. Therefore, automatic segmentation and classification task and
counting the cellular composition of H&amp;E images from pathological sections is
proposed by CoNIC Challenge 2022. We proposed a multi-scale Swin transformer
with HTC for this challenge, and also applied the known normalization methods
to generate more augmentation data. Finally, our strategy showed that the
multi-scale played a crucial role to identify different scale features and the
augmentation arose the recognition of model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Recurrent Fusion for Indoor Localization. (arXiv:2203.00510v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00510">
<div class="article-summary-box-inner">
<span><p>This paper considers indoor localization using multi-modal wireless signals
including Wi-Fi, inertial measurement unit (IMU), and ultra-wideband (UWB). By
formulating the localization as a multi-modal sequence regression problem, a
multi-stream recurrent fusion method is proposed to combine the current hidden
state of each modality in the context of recurrent neural networks while
accounting for the modality uncertainty which is directly learned from its own
immediate past states. The proposed method was evaluated on the large-scale
SPAWC2021 multi-modal localization dataset and compared with a wide range of
baseline methods including the trilateration method, traditional fingerprinting
methods, and convolution network-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding. (arXiv:2203.00680v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00680">
<div class="article-summary-box-inner">
<span><p>Manual annotation of large-scale point cloud dataset for varying tasks such
as 3D object classification, segmentation and detection is often laborious
owing to the irregular structure of point clouds. Self-supervised learning,
which operates without any human labeling, is a promising approach to address
this issue. We observe in the real world that humans are capable of mapping the
visual concepts learnt from 2D images to understand the 3D world. Encouraged by
this insight, we propose CrossPoint, a simple cross-modal contrastive learning
approach to learn transferable 3D point cloud representations. It enables a
3D-2D correspondence of objects by maximizing agreement between point clouds
and the corresponding rendered 2D image in the invariant space, while
encouraging invariance to transformations in the point cloud modality. Our
joint training objective combines the feature correspondences within and across
modalities, thus ensembles a rich learning signal from both 3D point cloud and
2D image modalities in a self-supervised fashion. Experimental results show
that our approach outperforms the previous unsupervised learning methods on a
diverse range of downstream tasks including 3D object classification and
segmentation. Further, the ablation studies validate the potency of our
approach for a better point cloud understanding. Code and pretrained models are
available at <a href="http://github.com/MohamedAfham/CrossPoint.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-03 23:07:38.299206568 UTC">2022-03-03 23:07:38 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>