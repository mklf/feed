<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-30T01:30:00Z">05-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dialogue Representations from Consecutive Utterances. (arXiv:2205.13568v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13568">
<div class="article-summary-box-inner">
<span><p>Learning high-quality dialogue representations is essential for solving a
variety of dialogue-oriented tasks, especially considering that dialogue
systems often suffer from data scarcity. In this paper, we introduce Dialogue
Sentence Embedding (DSE), a self-supervised contrastive learning method that
learns effective dialogue representations suitable for a wide range of dialogue
tasks. DSE learns from dialogues by taking consecutive utterances of the same
dialogue as positive pairs for contrastive learning. Despite its simplicity,
DSE achieves significantly better representation capability than other dialogue
representation and universal sentence representation models. We evaluate DSE on
five downstream dialogue tasks that examine dialogue representation at
different semantic granularities. Experiments in few-shot and zero-shot
settings show that DSE outperforms baselines by a large margin. For example, it
achieves 13 average performance improvement over the strongest unsupervised
baseline in 1-shot intent classification on 6 datasets. We also provide
analyses on the benefits and limitations of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Dialogue Transcription Error Correction using Seq2Seq Models. (arXiv:2205.13572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13572">
<div class="article-summary-box-inner">
<span><p>Good communication is critical to good healthcare. Clinical dialogue is a
conversation between health practitioners and their patients, with the explicit
goal of obtaining and sharing medical information. This information contributes
to medical decision-making regarding the patient and plays a crucial role in
their healthcare journey. The reliance on note taking and manual scribing
processes are extremely inefficient and leads to manual transcription errors
when digitizing notes. Automatic Speech Recognition (ASR) plays a significant
role in speech-to-text applications, and can be directly used as a text
generator in conversational applications. However, recording clinical dialogue
presents a number of general and domain-specific challenges. In this paper, we
present a seq2seq learning approach for ASR transcription error correction of
clinical dialogues. We introduce a new Gastrointestinal Clinical Dialogue (GCD)
Dataset which was gathered by healthcare professionals from a NHS Inflammatory
Bowel Disease clinic and use this in a comparative study with four commercial
ASR systems. Using self-supervision strategies, we fine-tune a seq2seq model on
a mask-filling task using a domain-specific PubMed dataset which we have shared
publicly for future research. The BART model fine-tuned for mask-filling was
able to correct transcription errors and achieve lower word error rates for
three out of four commercial ASR outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Decoding in Large Language Models. (arXiv:2205.13621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13621">
<div class="article-summary-box-inner">
<span><p>Recent large-scale natural language processing (NLP) systems use a
pre-trained Large Language Model (LLM) on massive and diverse corpora as a
headstart. In practice, the pre-trained model is adapted to a wide array of
tasks via fine-tuning on task-specific datasets. LLMs, while effective, have
been shown to memorize instances of training data thereby potentially revealing
private information processed during pre-training. The potential leakage might
further propagate to the downstream tasks for which LLMs are fine-tuned. On the
other hand, privacy-preserving algorithms usually involve retraining from
scratch, which is prohibitively expensive for LLMs. In this work, we propose a
simple, easy to interpret, and computationally lightweight perturbation
mechanism to be applied to an already trained model at the decoding stage. Our
perturbation mechanism is model-agnostic and can be used in conjunction with
any LLM. We provide theoretical analysis showing that the proposed mechanism is
differentially private, and experimental results showing a privacy-utility
trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quark: Controllable Text Generation with Reinforced Unlearning. (arXiv:2205.13636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13636">
<div class="article-summary-box-inner">
<span><p>Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Adapters for Personalized Speech Recognition in Neural Transducers. (arXiv:2205.13660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13660">
<div class="article-summary-box-inner">
<span><p>Personal rare word recognition in end-to-end Automatic Speech Recognition
(E2E ASR) models is a challenge due to the lack of training data. A standard
way to address this issue is with shallow fusion methods at inference time.
However, due to their dependence on external language models and the
deterministic approach to weight boosting, their performance is limited. In
this paper, we propose training neural contextual adapters for personalization
in neural transducer based ASR models. Our approach can not only bias towards
user-defined words, but also has the flexibility to work with pretrained ASR
models. Using an in-house dataset, we demonstrate that contextual adapters can
be applied to any general purpose pretrained ASR model to improve
personalization. Our method outperforms shallow fusion, while retaining
functionality of the pretrained models by not altering any of the model
weights. We further show that the adapter style training is superior to
full-fine-tuning of the ASR models on datasets with user-defined content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Normalization for Streaming Speech Recognition in a Modular Framework. (arXiv:2205.13674v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13674">
<div class="article-summary-box-inner">
<span><p>We introduce the Globally Normalized Autoregressive Transducer (GNAT) for
addressing the label bias problem in streaming speech recognition. Our solution
admits a tractable exact computation of the denominator for the sequence-level
normalization. Through theoretical and empirical results, we demonstrate that
by switching to a globally normalized model, the word error rate gap between
streaming and non-streaming speech-recognition models can be greatly reduced
(by more than 50\% on the Librispeech dataset). This model is developed in a
modular framework which encompasses all the common neural speech recognition
models. The modularity of this framework enables controlled comparison of
modelling choices and creation of new models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiJoNLP at SemEval-2022 Task 2: Detecting Idiomaticity of Multiword Expressions using Multilingual Pretrained Language Models. (arXiv:2205.13708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13708">
<div class="article-summary-box-inner">
<span><p>This paper describes an approach to detect idiomaticity only from the
contextualized representation of a MWE over multilingual pretrained language
models. Our experiments find that larger models are usually more effective in
idiomaticity detection. However, using a higher layer of the model may not
guarantee a better performance. In multilingual scenarios, the convergence of
different languages are not consistent and rich-resource languages have big
advantages over other languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Foundation Models Help Us Achieve Perfect Secrecy?. (arXiv:2205.13722v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13722">
<div class="article-summary-box-inner">
<span><p>A key promise of machine learning is the ability to assist users with
personal tasks. Because the personal context required to make accurate
predictions is often sensitive, we require systems that protect privacy. A gold
standard privacy-preserving system will satisfy perfect secrecy, meaning that
interactions with the system provably reveal no additional private information
to adversaries. This guarantee should hold even as we perform multiple personal
tasks over the same underlying data. However, privacy and quality appear to be
in tension in existing systems for personal tasks. Neural models typically
require lots of training to perform well, while individual users typically hold
a limited scale of data, so the systems propose to learn from the aggregate
data of multiple users. This violates perfect secrecy and instead, in the last
few years, academics have defended these solutions using statistical notions of
privacy -- i.e., the probability of learning private information about a user
should be reasonably low. Given the vulnerabilities of these solutions, we
explore whether the strong perfect secrecy guarantee can be achieved using
recent zero-to-few sample adaptation techniques enabled by foundation models.
In response, we propose FOCUS, a framework for personal tasks. Evaluating on
popular privacy benchmarks, we find the approach, satisfying perfect secrecy,
competes with strong collaborative learning baselines on 6 of 7 tasks. We
empirically analyze the proposal, highlighting the opportunities and
limitations across task types, and model inductive biases and sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Long Programming Languages with Structure-Aware Sparse Attention. (arXiv:2205.13730v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13730">
<div class="article-summary-box-inner">
<span><p>Programming-based Pre-trained Language Models (PPLMs) such as CodeBERT have
achieved great success in many downstream code-related tasks. Since the memory
and computational complexity of self-attention in the Transformer grow
quadratically with the sequence length, PPLMs typically limit the code length
to 512. However, codes in real-world applications are generally long, such as
code searches, which cannot be processed efficiently by existing PPLMs. To
solve this problem, in this paper, we present SASA, a Structure-Aware Sparse
Attention mechanism, which reduces the complexity and improves performance for
long code understanding tasks. The key components in SASA are top-$k$ sparse
attention and Abstract Syntax Tree (AST)-based structure-aware attention. With
top-$k$ sparse attention, the most crucial attention relation can be obtained
with a lower computational cost. As the code structure represents the logic of
the code statements, which is a complement to the code sequence
characteristics, we further introduce AST structures into attention. Extensive
experiments on CodeXGLUE tasks show that SASA achieves better performance than
the competing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLU for Game-based Learning in Real: Initial Evaluations. (arXiv:2205.13754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13754">
<div class="article-summary-box-inner">
<span><p>Intelligent systems designed for play-based interactions should be
contextually aware of the users and their surroundings. Spoken Dialogue Systems
(SDS) are critical for these interactive agents to carry out effective
goal-oriented communication with users in real-time. For the real-world (i.e.,
in-the-wild) deployment of such conversational agents, improving the Natural
Language Understanding (NLU) module of the goal-oriented SDS pipeline is
crucial, especially with limited task-specific datasets. This study explores
the potential benefits of a recently proposed transformer-based multi-task NLU
architecture, mainly to perform Intent Recognition on small-size
domain-specific educational game datasets. The evaluation datasets were
collected from children practicing basic math concepts via play-based
interactions in game-based learning settings. We investigate the NLU
performances on the initial proof-of-concept game datasets versus the
real-world deployment datasets and observe anticipated performance drops
in-the-wild. We have shown that compared to the more straightforward baseline
approaches, Dual Intent and Entity Transformer (DIET) architecture is robust
enough to handle real-world data to a large extent for the Intent Recognition
task on these domain-specific in-the-wild game datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGLU 2022: Interactive Grounded Language Understanding in a Collaborative Environment at NeurIPS 2022. (arXiv:2205.13771v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13771">
<div class="article-summary-box-inner">
<span><p>Human intelligence has the remarkable ability to adapt to new tasks and
environments quickly. Starting from a very young age, humans acquire new skills
and learn how to solve new tasks either by imitating the behavior of others or
by following provided natural language instructions. To facilitate research in
this direction, we propose IGLU: Interactive Grounded Language Understanding in
a Collaborative Environment. The primary goal of the competition is to approach
the problem of how to develop interactive embodied agents that learn to solve a
task while provided with grounded natural language instructions in a
collaborative environment. Understanding the complexity of the challenge, we
split it into sub-tasks to make it feasible for participants.
</p>
<p>This research challenge is naturally related, but not limited, to two fields
of study that are highly relevant to the NeurIPS community: Natural Language
Understanding and Generation (NLU/G) and Reinforcement Learning (RL).
Therefore, the suggested challenge can bring two communities together to
approach one of the crucial challenges in AI. Another critical aspect of the
challenge is the dedication to perform a human-in-the-loop evaluation as a
final evaluation for the agents developed by contestants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Based Automatic Personality Prediction Using KGrAt-Net; A Knowledge Graph Attention Network Classifier. (arXiv:2205.13780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13780">
<div class="article-summary-box-inner">
<span><p>Nowadays, a tremendous amount of human communications take place on the
Internet-based communication infrastructures, like social networks, email,
forums, organizational communication platforms, etc. Indeed, the automatic
prediction or assessment of individuals' personalities through their written or
exchanged text would be advantageous to ameliorate the relationships among
them. To this end, this paper aims to propose KGrAt-Net which is a Knowledge
Graph Attention Network text classifier. For the first time, it applies the
knowledge graph attention network to perform Automatic Personality Prediction
(APP), according to the Big Five personality traits. After performing some
preprocessing activities, first, it tries to acquire a knowingful
representation of the knowledge behind the concepts in the input text through
building its equivalent knowledge graph. A knowledge graph is a graph-based
data model that formally represents the semantics of the existing concepts in
the input text and models the knowledge behind them. Then, applying the
attention mechanism, it efforts to pay attention to the most relevant parts of
the graph to predict the personality traits of the input text. The results
demonstrated that KGrAt-Net considerably improved the personality prediction
accuracies. Furthermore, KGrAt-Net also uses the knowledge graphs' embeddings
to enrich the classification, which makes it even more accurate in APP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nearest Neighbor Zero-Shot Inference. (arXiv:2205.13792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13792">
<div class="article-summary-box-inner">
<span><p>We introduce kNN-Prompt, a simple and effective technique to use k-nearest
neighbor (kNN) retrieval augmentation (Khandelwal et al., 2021) for zero-shot
inference with language models (LMs). Key to our approach is the introduction
of fuzzy verbalizers which leverage the sparse kNN distribution for downstream
tasks by automatically associating each classification label with a set of
natural language tokens. Across eleven diverse end-tasks (spanning text
classification, fact retrieval and question answering), using kNN-Prompt with
GPT-2 Large yields significant performance boosts over zero-shot baselines (14%
absolute improvement over the base LM on average). Extensive experiments show
that kNN-Prompt is effective for domain adaptation with no further training,
and that the benefits of retrieval increase with the size of the model used for
kNN retrieval. Overall, we show that augmenting a language model with retrieval
can bring significant gains for zero-shot inference, with the possibility that
larger retrieval models may yield even greater benefits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semeval-2022 Task 1: CODWOE -- Comparing Dictionaries and Word Embeddings. (arXiv:2205.13858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13858">
<div class="article-summary-box-inner">
<span><p>Word embeddings have advanced the state of the art in NLP across numerous
tasks. Understanding the contents of dense neural representations is of utmost
interest to the computational semantics community. We propose to focus on
relating these opaque word vectors with human-readable definitions, as found in
dictionaries. This problem naturally divides into two subtasks: converting
definitions into embeddings, and converting embeddings into definitions. This
task was conducted in a multilingual setting, using comparable sets of
embeddings trained homogeneously.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Automate Follow-up Question Generation using Process Knowledge for Depression Triage on Reddit Posts. (arXiv:2205.13884v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13884">
<div class="article-summary-box-inner">
<span><p>Conversational Agents (CAs) powered with deep language models (DLMs) have
shown tremendous promise in the domain of mental health. Prominently, the CAs
have been used to provide informational or therapeutic services to patients.
However, the utility of CAs to assist in mental health triaging has not been
explored in the existing work as it requires a controlled generation of
follow-up questions (FQs), which are often initiated and guided by the mental
health professionals (MHPs) in clinical settings. In the context of depression,
our experiments show that DLMs coupled with process knowledge in a mental
health questionnaire generate 12.54% and 9.37% better FQs based on similarity
and longest common subsequence matches to questions in the PHQ-9 dataset
respectively, when compared with DLMs without process knowledge support.
Despite coupling with process knowledge, we find that DLMs are still prone to
hallucination, i.e., generating redundant, irrelevant, and unsafe FQs. We
demonstrate the challenge of using existing datasets to train a DLM for
generating FQs that adhere to clinical process knowledge. To address this
limitation, we prepared an extended PHQ-9 based dataset, PRIMATE, in
collaboration with MHPs. PRIMATE contains annotations regarding whether a
particular question in the PHQ-9 dataset has already been answered in the
user's initial description of the mental health condition. We used PRIMATE to
train a DLM in a supervised setting to identify which of the PHQ-9 questions
can be answered directly from the user's post and which ones would require more
information from the user. Using performance analysis based on MCC scores, we
show that PRIMATE is appropriate for identifying questions in PHQ-9 that could
guide generative DLMs towards controlled FQ generation suitable for aiding
triaging. Dataset created as a part of this research:
https://github.com/primate-mh/Primate2022
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoInHindi: A Multi-label Emotion and Intensity Annotated Dataset in Hindi for Emotion Recognition in Dialogues. (arXiv:2205.13908v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13908">
<div class="article-summary-box-inner">
<span><p>The long-standing goal of Artificial Intelligence (AI) has been to create
human-like conversational systems. Such systems should have the ability to
develop an emotional connection with the users, hence emotion recognition in
dialogues is an important task. Emotion detection in dialogues is a challenging
task because humans usually convey multiple emotions with varying degrees of
intensities in a single utterance. Moreover, emotion in an utterance of a
dialogue may be dependent on previous utterances making the task more complex.
Emotion recognition has always been in great demand. However, most of the
existing datasets for multi-label emotion and intensity detection in
conversations are in English. To this end, we create a large conversational
dataset in Hindi named EmoInHindi for multi-label emotion and intensity
recognition in conversations containing 1,814 dialogues with a total of 44,247
utterances. We prepare our dataset in a Wizard-of-Oz manner for mental health
and legal counselling of crime victims. Each utterance of the dialogue is
annotated with one or more emotion categories from the 16 emotion classes
including neutral, and their corresponding intensity values. We further propose
strong contextual baselines that can detect emotion(s) and the corresponding
intensity of an utterance given the conversational context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense and Named Entity Aware Knowledge Grounded Dialogue Generation. (arXiv:2205.13928v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13928">
<div class="article-summary-box-inner">
<span><p>Grounding dialogue on external knowledge and interpreting linguistic patterns
in dialogue history context, such as ellipsis, anaphora, and co-references is
critical for dialogue comprehension and generation. In this paper, we present a
novel open-domain dialogue generation model which effectively utilizes the
large-scale commonsense and named entity based knowledge in addition to the
unstructured topic-specific knowledge associated with each utterance. We
enhance the commonsense knowledge with named entity-aware structures using
co-references. Our proposed model utilizes a multi-hop attention layer to
preserve the most accurate and critical parts of the dialogue history and the
associated knowledge. In addition, we employ a Commonsense and Named Entity
Enhanced Attention Module, which starts with the extracted triples from various
sources and gradually finds the relevant supporting set of triples using
multi-hop attention with the query vector obtained from the interactive
dialogue-knowledge module. Empirical results on two benchmark dataset
demonstrate that our model significantly outperforms the state-of-the-art
methods in terms of both automatic evaluation metrics and human judgment. Our
code is publicly available at
\href{https://github.com/deekshaVarshney/CNTF}{https://github.com/deekshaVarshney/CNTF};
\href{https://www.iitp.ac.in/~ai-nlp-ml/resources/codes/CNTF.zip}{https://www.iitp.ac.in/-ai-nlp-ml/resources/
codes/CNTF.zip}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Punctuation Restoration in Spanish Customer Support Transcripts using Transfer Learning. (arXiv:2205.13961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13961">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems typically produce unpunctuated
transcripts that have poor readability. In addition, building a punctuation
restoration system is challenging for low-resource languages, especially for
domain-specific applications. In this paper, we propose a Spanish punctuation
restoration system designed for a real-time customer support transcription
service. To address the data sparsity of Spanish transcripts in the customer
support domain, we introduce two transfer-learning-based strategies: 1) domain
adaptation using out-of-domain Spanish text data; 2) cross-lingual transfer
learning leveraging in-domain English transcript data. Our experiment results
show that these strategies improve the accuracy of the Spanish punctuation
restoration system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14014">
<div class="article-summary-box-inner">
<span><p>Transformers have made progress in miscellaneous tasks, but suffer from
quadratic computational and memory complexities. Recent works propose sparse
Transformers with attention on sparse graphs to reduce complexity and remain
strong performance. While effective, the crucial parts of how dense a graph
needs to be to perform well are not fully explored. In this paper, we propose
Normalized Information Payload (NIP), a graph scoring function measuring
information transfer on graph, which provides an analysis tool for trade-offs
between performance and complexity. Guided by this theoretical analysis, we
present Hypercube Transformer, a sparse Transformer that models token
interactions in a hypercube and shows comparable or even better results with
vanilla Transformer while yielding $O(N\log N)$ complexity with sequence length
$N$. Experiments on tasks requiring various sequence lengths lay validation for
our graph function well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StereoKG: Data-Driven Knowledge Graph Construction for Cultural Knowledge and Stereotypes. (arXiv:2205.14036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14036">
<div class="article-summary-box-inner">
<span><p>Analyzing ethnic or religious bias is important for improving fairness,
accountability, and transparency of natural language processing models.
However, many techniques rely on human-compiled lists of bias terms, which are
expensive to create and are limited in coverage. In this study, we present a
fully data-driven pipeline for generating a knowledge graph (KG) of cultural
knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5
nationalities and can easily be extended to include more entities. Our human
evaluation shows that the majority (59.2%) of non-singleton entries are
coherent and complete stereotypes. We further show that performing intermediate
masked language model training on the verbalized KG leads to a higher level of
cultural awareness in the model and has the potential to increase
classification performance on knowledge-crucial samples on a related task,
i.e., hate speech detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UAlberta at SemEval 2022 Task 2: Leveraging Glosses and Translations for Multilingual Idiomaticity Detection. (arXiv:2205.14084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14084">
<div class="article-summary-box-inner">
<span><p>We describe the University of Alberta systems for the SemEval-2022 Task 2 on
multilingual idiomaticity detection. Working under the assumption that
idiomatic expressions are noncompositional, our first method integrates
information on the meanings of the individual words of an expression into a
binary classifier. Further hypothesizing that literal and idiomatic expressions
translate differently, our second method translates an expression in context,
and uses a lexical knowledge base to determine if the translation is literal.
Our approaches are grounded in linguistic phenomena, and leverage existing
sources of lexical knowledge. Our results offer support for both approaches,
particularly the former.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patching Leaks in the Charformer for Efficient Character-Level Generation. (arXiv:2205.14086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14086">
<div class="article-summary-box-inner">
<span><p>Character-based representations have important advantages over subword-based
ones for morphologically rich languages. They come with increased robustness to
noisy input and do not need a separate tokenization step. However, they also
have a crucial disadvantage: they notably increase the length of text
sequences. The GBST method from Charformer groups (aka downsamples) characters
to solve this, but allows information to leak when applied to a Transformer
decoder. We solve this information leak issue, thereby enabling character
grouping in the decoder. We show that Charformer downsampling has no apparent
benefits in NMT over previous downsampling methods in terms of translation
quality, however it can be trained roughly 30% faster. Promising performance on
English--Turkish translation indicate the potential of character-level models
for morphologically-rich languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior. (arXiv:2205.14140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14140">
<div class="article-summary-box-inner">
<span><p>The increasing size and complexity of modern ML systems has improved their
predictive capabilities but made their behavior harder to explain. Many
techniques for model explanation have been developed in response, but we lack
clear criteria for assessing these techniques. In this paper, we cast model
explanation as the causal inference problem of estimating causal effects of
real-world concepts on the output behavior of ML models given actual input
data. We introduce CEBaB, a new benchmark dataset for assessing concept-based
explanation methods in Natural Language Processing (NLP). CEBaB consists of
short restaurant reviews with human-generated counterfactual reviews in which
an aspect (food, noise, ambiance, service) of the dining experience was
modified. Original and counterfactual reviews are annotated with
multiply-validated sentiment ratings at the aspect-level and review-level. The
rich structure of CEBaB allows us to go beyond input features to study the
effects of abstract, real-world concepts on model behavior. We use CEBaB to
compare the quality of a range of concept-based explanation methods covering
different assumptions and conceptions of the problem, and we seek to establish
natural metrics for comparative assessments of these methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05268">
<div class="article-summary-box-inner">
<span><p>Recently generating natural language explanations has shown very promising
results in not only offering interpretable explanations but also providing
additional information and supervision for prediction. However, existing
approaches usually require a large set of human annotated explanations for
training while collecting a large set of explanations is not only time
consuming but also expensive. In this paper, we develop a general framework for
interpretable natural language understanding that requires only a small set of
human annotated explanations for training. Our framework treats natural
language explanations as latent variables that model the underlying reasoning
process of a neural model. We develop a variational EM framework for
optimization where an explanation generation module and an
explanation-augmented prediction module are alternatively optimized and
mutually enhance each other. Moreover, we further propose an explanation-based
self-training method under this framework for semi-supervised learning. It
alternates between assigning pseudo-labels to unlabeled data and generating new
explanations to iteratively improve each other. Experiments on two natural
language understanding tasks demonstrate that our framework can not only make
effective predictions in both supervised and semi-supervised settings, but also
generate good natural language explanation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03824">
<div class="article-summary-box-inner">
<span><p>We show that Transformer encoder architectures can be sped up, with limited
accuracy costs, by replacing the self-attention sublayers with simple linear
transformations that "mix" input tokens. These linear mixers, along with
standard nonlinearities in feed-forward layers, prove competent at modeling
semantic relationships in several text classification tasks. Most surprisingly,
we find that replacing the self-attention sublayer in a Transformer encoder
with a standard, unparameterized Fourier Transform achieves 92-97% of the
accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on
GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input
lengths, our FNet model is significantly faster: when compared to the
"efficient" Transformers on the Long Range Arena benchmark, FNet matches the
accuracy of the most accurate models, while outpacing the fastest models across
all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).
Finally, FNet has a light memory footprint and is particularly efficient at
smaller model sizes; for a fixed speed and accuracy budget, small FNet models
outperform Transformer counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13856">
<div class="article-summary-box-inner">
<span><p>Large-scale models for learning fixed-dimensional cross-lingual sentence
representations like LASER (Artetxe and Schwenk, 2019b) lead to significant
improvement in performance on downstream tasks. However, further increases and
modifications based on such large-scale models are usually impractical due to
memory limitations. In this work, we introduce a lightweight dual-transformer
architecture with just 2 layers for generating memory-efficient cross-lingual
sentence representations. We explore different training tasks and observe that
current cross-lingual training tasks leave a lot to be desired for this shallow
architecture. To ameliorate this, we propose a novel cross-lingual language
model, which combines the existing single-word masked language model with the
newly proposed cross-lingual token-level reconstruction task. We further
augment the training task by the introduction of two computationally-lite
sentence-level contrastive learning tasks to enhance the alignment of
cross-lingual sentence representation space, which compensates for the learning
bottleneck of the lightweight transformer for generative tasks. Our comparisons
with competing models on cross-lingual sentence retrieval and multilingual
document classification confirm the effectiveness of the newly proposed
training tasks for a shallow model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Korean-English Machine Translation with Multiple Tokenization Strategy. (arXiv:2105.14274v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14274">
<div class="article-summary-box-inner">
<span><p>This work was conducted to find out how tokenization methods affect the
training results of machine translation models. In this work, alphabet
tokenization, morpheme tokenization, and BPE tokenization were applied to
Korean as the source language and English as the target language respectively,
and the comparison experiment was conducted by repeating 50,000 epochs of each
9 models using the Transformer neural network. As a result of measuring the
BLEU scores of the experimental models, the model that applied BPE tokenization
to Korean and morpheme tokenization to English recorded 35.73, showing the best
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar Accuracy Evaluation (GAE): Quantifiable Quantitative Evaluation of Machine Translation Models. (arXiv:2105.14277v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14277">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) refers to the operation of expressing the
calculation results of a system in human language. Since the quality of
generated sentences from an NLG model cannot be fully represented using only
quantitative evaluation, they are evaluated using qualitative evaluation by
humans in which the meaning or grammar of a sentence is scored according to a
subjective criterion. Nevertheless, the existing evaluation methods have a
problem as a large score deviation occurs depending on the criteria of
evaluators. In this paper, we propose Grammar Accuracy Evaluation (GAE) that
can provide the specific evaluating criteria. As a result of analyzing the
quality of machine translation by BLEU and GAE, it was confirmed that the BLEU
score does not represent the absolute performance of machine translation models
and GAE compensates for the shortcomings of BLEU with flexible evaluation of
alternative synonyms and changes in sentence structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assisting Decision Making in Scholarly Peer Review: A Preference Learning Perspective. (arXiv:2109.01190v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01190">
<div class="article-summary-box-inner">
<span><p>Peer review is the primary means of quality control in academia; as an
outcome of a peer review process, program and area chairs make acceptance
decisions for each paper based on the review reports and scores they received.
Quality of scientific work is multi-faceted; coupled with the subjectivity of
reviewing, this makes final decision making difficult and time-consuming. To
support this final step of peer review, we formalize it as a paper ranking
problem. We introduce a novel, multi-faceted generic evaluation framework for
ranking submissions based on peer reviews that takes into account
effectiveness, efficiency and fairness. We propose a preference learning
perspective on the task that considers both review texts and scores to
alleviate the inevitable bias and noise in reviews. Our experiments on peer
review data from the ACL 2018 conference demonstrate the superiority of our
preference-learning-based approach over baselines and prior work, while
highlighting the importance of using both review texts and scores to rank
submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02358">
<div class="article-summary-box-inner">
<span><p>We present a unified Vision-Language pretrained Model (VLMo) that jointly
learns a dual encoder and a fusion encoder with a modular Transformer network.
Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,
where each block contains a pool of modality-specific experts and a shared
self-attention layer. Because of the modeling flexibility of MoME, pretrained
VLMo can be fine-tuned as a fusion encoder for vision-language classification
tasks, or used as a dual encoder for efficient image-text retrieval. Moreover,
we propose a stagewise pre-training strategy, which effectively leverages
large-scale image-only and text-only data besides image-text pairs.
Experimental results show that VLMo achieves state-of-the-art results on
various vision-language tasks, including VQA, NLVR2 and image-text retrieval.
The code and pretrained models are available at https://aka.ms/vlmo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\textsc{CIS}^2}: A Simplified Commonsense Inference Evaluation for Story Prose. (arXiv:2202.07880v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07880">
<div class="article-summary-box-inner">
<span><p>Transformers have been showing near-human performance on a variety of tasks,
but they are not without their limitations. We discuss the issue of conflating
results of transformers that are instructed to do multiple tasks
simultaneously. In particular, we focus on the domain of commonsense reasoning
within story prose, which we call contextual commonsense inference (CCI). We
look at the GLUCOSE (Mostafazadeh et al. 2020) dataset and task for predicting
implicit commonsense inferences between story sentences. Since the GLUCOSE task
simultaneously generates sentences and predicts the CCI relation, there is a
conflation in the results. Is the model really measuring CCI or is its ability
to generate grammatical text carrying the results? In this paper, we introduce
the task contextual commonsense inference in sentence selection
($\textsc{CIS}^2}), a simplified task that avoids conflation by eliminating
language generation altogether. Our findings emphasize the necessity of future
work to disentangle language generation from the desired NLP tasks at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning English with Peppa Pig. (arXiv:2202.12917v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12917">
<div class="article-summary-box-inner">
<span><p>Recent computational models of the acquisition of spoken language via
grounding in perception exploit associations between the spoken and visual
modalities and learn to represent speech and visual data in a joint vector
space. A major unresolved issue from the point of ecological validity is the
training data, typically consisting of images or videos paired with spoken
descriptions of what is depicted. Such a setup guarantees an unrealistically
strong correlation between speech and the visual data. In the real world the
coupling between the linguistic and the visual modality is loose, and often
confounded by correlations with non-semantic aspects of the speech signal. Here
we address this shortcoming by using a dataset based on the children's cartoon
Peppa Pig. We train a simple bi-modal architecture on the portion of the data
consisting of dialog between characters, and evaluate on segments containing
descriptive narrations. Despite the weak and confounded signal in this training
data our model succeeds at learning aspects of the visual semantics of spoken
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00598">
<div class="article-summary-box-inner">
<span><p>Large pretrained (e.g., "foundation") models exhibit distinct capabilities
depending on the domain of data they are trained on. While these domains are
generic, they may only barely overlap. For example, visual-language models
(VLMs) are trained on Internet-scale image captions, but large language models
(LMs) are further trained on Internet-scale text with no images (e.g.,
spreadsheets, SAT questions, code). As a result, these models store different
forms of commonsense knowledge across different domains. In this work, we show
that this diversity is symbiotic, and can be leveraged through Socratic Models
(SMs): a modular framework in which multiple pretrained models may be composed
zero-shot i.e., via multimodal-informed prompting, to exchange information with
each other and capture new multimodal capabilities, without requiring
finetuning. With minimal engineering, SMs are not only competitive with
state-of-the-art zero-shot image captioning and video-to-text retrieval, but
also enable new applications such as (i) answering free-form questions about
egocentric video, (ii) engaging in multimodal assistive dialogue with people
(e.g., for cooking recipes) by interfacing with external APIs and databases
(e.g., web search), and (iii) robot perception and planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of Neural Transducer for Speech Recognition. (arXiv:2204.10586v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10586">
<div class="article-summary-box-inner">
<span><p>As one of the most popular sequence-to-sequence modeling approaches for
speech recognition, the RNN-Transducer has achieved evolving performance with
more and more sophisticated neural network models of growing size and
increasing training epochs. While strong computation resources seem to be the
prerequisite of training superior models, we try to overcome it by carefully
designing a more efficient training pipeline. In this work, we propose an
efficient 3-stage progressive training pipeline to build highly-performing
neural transducer models from scratch with very limited computation resources
in a reasonable short time period. The effectiveness of each stage is
experimentally verified on both Librispeech and Switchboard corpora. The
proposed pipeline is able to train transducer models approaching
state-of-the-art performance with a single GPU in just 2-3 weeks. Our best
conformer transducer achieves 4.1% WER on Librispeech test-other with only 35
epochs of training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021. (arXiv:2205.02388v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02388">
<div class="article-summary-box-inner">
<span><p>Human intelligence has the remarkable ability to quickly adapt to new tasks
and environments. Starting from a very young age, humans acquire new skills and
learn how to solve new tasks either by imitating the behavior of others or by
following provided natural language instructions. To facilitate research in
this direction, we propose \emph{IGLU: Interactive Grounded Language
Understanding in a Collaborative Environment}.
</p>
<p>The primary goal of the competition is to approach the problem of how to
build interactive agents that learn to solve a task while provided with
grounded natural language instructions in a collaborative environment.
Understanding the complexity of the challenge, we split it into sub-tasks to
make it feasible for participants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of COVID-19 Pandemic on LGBTQ Online Communities. (arXiv:2205.09511v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09511">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has disproportionately impacted the lives of
minorities, such as members of the LGBTQ community (lesbian, gay, bisexual,
transgender, and queer) due to pre-existing social disadvantages and health
disparities. Although extensive research has been carried out on the impact of
the COVID-19 pandemic on different aspects of the general population's lives,
few studies are focused on the LGBTQ population. In this paper, we identify a
group of Twitter users who self-disclose to belong to the LGBTQ community. We
develop and evaluate two sets of machine learning classifiers using a
pre-pandemic and a during pandemic dataset to identify Twitter posts exhibiting
minority stress, which is a unique pressure faced by the members of the LGBTQ
population due to their sexual and gender identities. For this task, we collect
a set of 20,593,823 posts by 7,241 self-disclosed LGBTQ users and annotate a
randomly selected subset of 2800 posts. We demonstrate that our best
pre-pandemic and during pandemic models show strong and stable performance for
detecting posts that contain minority stress. We investigate the linguistic
differences in minority stress posts across pre- and during-pandemic periods.
We find that anger words are strongly associated with minority stress during
the COVID-19 pandemic. We explore the impact of the pandemic on the emotional
states of the LGBTQ population by conducting controlled comparisons with the
general population. We adopt propensity score-based matching to perform a
causal analysis. The results show that the LBGTQ population have a greater
increase in the usage of cognitive words and worsened observable attribute in
the usage of positive emotion words than the group of the general population
with similar pre-pandemic behavioral attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let the Model Decide its Curriculum for Multitask Learning. (arXiv:2205.09898v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09898">
<div class="article-summary-box-inner">
<span><p>Curriculum learning strategies in prior multi-task learning approaches
arrange datasets in a difficulty hierarchy either based on human perception or
by exhaustively searching the optimal arrangement. However, human perception of
difficulty may not always correlate well with machine interpretation leading to
poor performance and exhaustive search is computationally expensive. Addressing
these concerns, we propose two classes of techniques to arrange training
instances into a learning curriculum based on difficulty scores computed via
model-based approaches. The two classes i.e Dataset-level and Instance-level
differ in granularity of arrangement. Through comprehensive experiments with 12
datasets, we show that instance-level and dataset-level techniques result in
strong representations as they lead to an average performance improvement of
4.17% and 3.15% over their respective baselines. Furthermore, we find that most
of this improvement comes from correctly answering the difficult instances,
implying a greater efficacy of our techniques on difficult tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11211">
<div class="article-summary-box-inner">
<span><p>End-to-End Speech Translation (E2E-ST) has received increasing attention due
to the potential of its less error propagation, lower latency, and fewer
parameters. However, the effectiveness of neural-based approaches to this task
is severely limited by the available training corpus, especially for domain
adaptation where in-domain triplet training data is scarce or nonexistent. In
this paper, we propose a novel non-parametric method that leverages
domain-specific text translation corpus to achieve domain adaptation for the
E2E-ST system. To this end, we first incorporate an additional encoder into the
pre-trained E2E-ST model to realize text translation modelling, and then unify
the decoder's output representation for text and speech translation tasks by
reducing the correspondent representation mismatch in available triplet
training data. During domain adaptation, a k-nearest-neighbor (kNN) classifier
is introduced to produce the final translation distribution using the external
datastore built by the domain-specific text translation corpus, while the
universal output representation is adopted to perform a similarity search.
Experiments on the Europarl-ST benchmark demonstrate that when in-domain text
translation data is involved only, our proposed approach significantly improves
baseline by 12.82 BLEU on average in all translation directions, even
outperforming the strong in-domain fine-tuning method.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal Assignment and Pseudo-Label Refinement. (arXiv:2205.13579v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13579">
<div class="article-summary-box-inner">
<span><p>Recent works on unsupervised domain adaptation (UDA) focus on the selection
of good pseudo-labels as surrogates for the missing labels in the target data.
However, source domain bias that deteriorates the pseudo-labels can still exist
since the shared network of the source and target domains are typically used
for the pseudo-label selections. The suboptimal feature space source-to-target
domain alignment can also result in unsatisfactory performance. In this paper,
we propose CA-UDA to improve the quality of the pseudo-labels and UDA results
with optimal assignment, a pseudo-label refinement strategy and class-aware
domain alignment. We use an auxiliary network to mitigate the source domain
bias for pseudo-label refinement. Our intuition is that the underlying
semantics in the target domain can be fully exploited to help refine the
pseudo-labels that are inferred from the source features under domain shift.
Furthermore, our optimal assignment can optimally align features in the
source-to-target domains and our class-aware domain alignment can
simultaneously close the domain gap while preserving the classification
decision boundaries. Extensive experiments on several benchmark datasets show
that our method can achieve state-of-the-art performance in the image
classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harnessing Artificial Intelligence to Infer Novel Spatial Biomarkers for the Diagnosis of Eosinophilic Esophagitis. (arXiv:2205.13583v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13583">
<div class="article-summary-box-inner">
<span><p>Eosinophilic esophagitis (EoE) is a chronic allergic inflammatory condition
of the esophagus associated with elevated esophageal eosinophils. Second only
to gastroesophageal reflux disease, EoE is one of the leading causes of chronic
refractory dysphagia in adults and children. EoE diagnosis requires enumerating
the density of esophageal eosinophils in esophageal biopsies, a somewhat
subjective task that is time-consuming, thus reducing the ability to process
the complex tissue structure. Previous artificial intelligence (AI) approaches
that aimed to improve histology-based diagnosis focused on recapitulating
identification and quantification of the area of maximal eosinophil density.
However, this metric does not account for the distribution of eosinophils or
other histological features, over the whole slide image. Here, we developed an
artificial intelligence platform that infers local and spatial biomarkers based
on semantic segmentation of intact eosinophils and basal zone distributions.
Besides the maximal density of eosinophils (referred to as Peak Eosinophil
Count [PEC]) and a maximal basal zone fraction, we identify two additional
metrics that reflect the distribution of eosinophils and basal zone fractions.
This approach enables a decision support system that predicts EoE activity and
classifies the histological severity of EoE patients. We utilized a cohort that
includes 1066 biopsy slides from 400 subjects to validate the system's
performance and achieved a histological severity classification accuracy of
86.70%, sensitivity of 84.50%, and specificity of 90.09%. Our approach
highlights the importance of systematically analyzing the distribution of
biopsy features over the entire slide and paves the way towards a personalized
decision support system that will assist not only in counting cells but can
also potentially improve diagnosis and provide treatment prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VectorAdam for Rotation Equivariant Geometry Optimization. (arXiv:2205.13599v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13599">
<div class="article-summary-box-inner">
<span><p>The rise of geometric problems in machine learning has necessitated the
development of equivariant methods, which preserve their output under the
action of rotation or some other transformation. At the same time, the Adam
optimization algorithm has proven remarkably effective across machine learning
and even traditional tasks in geometric optimization. In this work, we observe
that naively applying Adam to optimize vector-valued data is not rotation
equivariant, due to per-coordinate moment updates, and in fact this leads to
significant artifacts and biases in practice. We propose to resolve this
deficiency with VectorAdam, a simple modification which makes Adam
rotation-equivariant by accounting for the vector structure of optimization
variables. We demonstrate this approach on problems in machine learning and
traditional geometric optimization, showing that equivariant VectorAdam
resolves the artifacts and biases of traditional Adam when applied to
vector-valued data, with equivalent or even improved rates of convergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Circumventing Backdoor Defenses That Are Based on Latent Separability. (arXiv:2205.13613v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13613">
<div class="article-summary-box-inner">
<span><p>Deep learning models are vulnerable to backdoor poisoning attacks. In
particular, adversaries can embed hidden backdoors into a model by only
modifying a very small portion of its training data. On the other hand, it has
also been commonly observed that backdoor poisoning attacks tend to leave a
tangible signature in the latent space of the backdoored model i.e. poison
samples and clean samples form two separable clusters in the latent space.
These observations give rise to the popularity of latent separability
assumption, which states that the backdoored DNN models will learn separable
latent representations for poison and clean populations. A number of popular
defenses (e.g. Spectral Signature, Activation Clustering, SCAn, etc.) are
exactly built upon this assumption. However, in this paper, we show that the
latent separation can be significantly suppressed via designing adaptive
backdoor poisoning attacks with more sophisticated poison strategies, which
consequently render state-of-the-art defenses based on this assumption less
effective (and often completely fail). More interestingly, we find that our
adaptive attacks can even evade some other typical backdoor defenses that do
not explicitly build on this separability assumption. Our results show that
adaptive backdoor poisoning attacks that can breach the latent separability
assumption should be seriously considered for evaluating existing and future
defenses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fight Poison with Poison: Detecting Backdoor Poison Samples via Decoupling Benign Correlations. (arXiv:2205.13616v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13616">
<div class="article-summary-box-inner">
<span><p>In this work, we study poison samples detection for defending against
backdoor poisoning attacks on deep neural networks (DNNs). A principled idea
underlying prior arts on this problem is to utilize the backdoored models'
distinguishable behaviors on poison and clean populations to distinguish
between these two different populations themselves and remove the identified
poison. Many prior arts build their detectors upon a latent separability
assumption, which states that backdoored models trained on the poisoned dataset
will learn separable latent representations for backdoor and clean samples.
Although such separation behaviors empirically exist for many existing attacks,
there is no control on the separability and the extent of separation can vary a
lot across different poison strategies, datasets, as well as the training
configurations of backdoored models. Worse still, recent adaptive poison
strategies can greatly reduce the "distinguishable behaviors" and consequently
render most prior arts less effective (or completely fail). We point out that
these limitations directly come from the passive reliance on some
distinguishable behaviors that are not controlled by defenders. To mitigate
such limitations, in this work, we propose the idea of active defense -- rather
than passively assuming backdoored models will have certain distinguishable
behaviors on poison and clean samples, we propose to actively enforce the
trained models to behave differently on these two different populations.
Specifically, we introduce confusion training as a concrete instance of active
defense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denial-of-Service Attack on Object Detection Model Using Universal Adversarial Perturbation. (arXiv:2205.13618v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13618">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks against deep learning-based object detectors have been
studied extensively in the past few years. The proposed attacks aimed solely at
compromising the models' integrity (i.e., trustworthiness of the model's
prediction), while adversarial attacks targeting the models' availability, a
critical aspect in safety-critical domains such as autonomous driving, have not
been explored by the machine learning research community. In this paper, we
propose NMS-Sponge, a novel approach that negatively affects the decision
latency of YOLO, a state-of-the-art object detector, and compromises the
model's availability by applying a universal adversarial perturbation (UAP). In
our experiments, we demonstrate that the proposed UAP is able to increase the
processing time of individual frames by adding "phantom" objects while
preserving the detection of the original objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Neural Autoencoder for Sensory Neuroprostheses and Its Applications in Bionic Vision. (arXiv:2205.13623v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13623">
<div class="article-summary-box-inner">
<span><p>Sensory neuroprostheses are emerging as a promising technology to restore
lost sensory function or augment human capacities. However, sensations elicited
by current devices often appear artificial and distorted. Although current
models can often predict the neural or perceptual response to an electrical
stimulus, an optimal stimulation strategy solves the inverse problem: what is
the required stimulus to produce a desired response? Here we frame this as an
end-to-end optimization problem, where a deep neural network encoder is trained
to invert a known, fixed forward model that approximates the underlying
biological system. As a proof of concept, we demonstrate the effectiveness of
our hybrid neural autoencoder (HNA) on the use case of visual neuroprostheses.
We found that HNA is able to produce high-fidelity stimuli from the MNIST and
COCO datasets that outperform conventional encoding strategies and surrogate
techniques across all tested conditions. Overall this is an important step
towards the long-standing challenge of restoring high-quality vision to people
living with incurable blindness and may prove a promising solution for a
variety of neuroprosthetic technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation. (arXiv:2205.13629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13629">
<div class="article-summary-box-inner">
<span><p>Robust environment perception for autonomous vehicles is a tremendous
challenge, which makes a diverse sensor set with e.g. camera, lidar and radar
crucial. In the process of understanding the recorded sensor data, 3D semantic
segmentation plays an important role. Therefore, this work presents a
pyramid-based deep fusion architecture for lidar and camera to improve 3D
semantic segmentation of traffic scenes. Individual sensor backbones extract
feature maps of camera images and lidar point clouds. A novel Pyramid Fusion
Backbone fuses these feature maps at different scales and combines the
multimodal features in a feature pyramid to compute valuable multimodal,
multi-scale features. The Pyramid Fusion Head aggregates these pyramid features
and further refines them in a late fusion step, incorporating the final
features of the sensor backbones. The approach is evaluated on two challenging
outdoor datasets and different fusion strategies and setups are investigated.
It outperforms recent range view based lidar approaches as well as all so far
proposed fusion strategies and architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-temporally separable non-linear latent factor learning: an application to somatomotor cortex fMRI data. (arXiv:2205.13640v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13640">
<div class="article-summary-box-inner">
<span><p>Functional magnetic resonance imaging (fMRI) data contain complex
spatiotemporal dynamics, thus researchers have developed approaches that reduce
the dimensionality of the signal while extracting relevant and interpretable
dynamics. Models of fMRI data that can perform whole-brain discovery of
dynamical latent factors are understudied. The benefits of approaches such as
linear independent component analysis models have been widely appreciated,
however, nonlinear extensions of these models present challenges in terms of
identification. Deep learning methods provide a way forward, but new methods
for efficient spatial weight-sharing are critical to deal with the high
dimensionality of the data and the presence of noise. Our approach generalizes
weight sharing to non-Euclidean neuroimaging data by first performing spectral
clustering based on the structural and functional similarity between voxels.
The spectral clusters and their assignments can then be used as patches in an
adapted multi-layer perceptron (MLP)-mixer model to share parameters among
input points. To encourage temporally independent latent factors, we use an
additional total correlation term in the loss. Our approach is evaluated on
data with multiple motor sub-tasks to assess whether the model captures
disentangled latent factors that correspond to each sub-task. Then, to assess
the latent factors we find further, we compare the spatial location of each
latent factor to the motor homunculus. Finally, we show that our approach
captures task effects better than the current gold standard of source signal
separation, independent component analysis (ICA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Membership Inference Attack Using Self Influence Functions. (arXiv:2205.13680v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13680">
<div class="article-summary-box-inner">
<span><p>Member inference (MI) attacks aim to determine if a specific data sample was
used to train a machine learning model. Thus, MI is a major privacy threat to
models trained on private sensitive data, such as medical records. In MI
attacks one may consider the black-box settings, where the model's parameters
and activations are hidden from the adversary, or the white-box case where they
are available to the attacker. In this work, we focus on the latter and present
a novel MI attack for it that employs influence functions, or more specifically
the samples' self-influence scores, to perform the MI prediction. We evaluate
our attack on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets, using versatile
architectures such as AlexNet, ResNet, and DenseNet. Our attack method achieves
new state-of-the-art results for both training with and without data
augmentations. Code is available at
https://github.com/giladcohen/sif_mi_attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANISE: Assembly-based Neural Implicit Surface rEconstruction. (arXiv:2205.13682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13682">
<div class="article-summary-box-inner">
<span><p>We present ANISE, a method that reconstructs a 3D shape from partial
observations (images or sparse point clouds) using a part-aware neural implicit
shape representation. It is formulated as an assembly of neural implicit
functions, each representing a different shape part. In contrast to previous
approaches, the prediction of this representation proceeds in a coarse-to-fine
manner. Our network first predicts part transformations which are associated
with part neural implicit functions conditioned on those transformations. The
part implicit functions can then be combined into a single, coherent shape,
enabling part-aware shape reconstructions from images and point clouds. Those
reconstructions can be obtained in two ways: (i) by directly decoding combining
the refined part implicit functions; or (ii) by using part latents to query
similar parts in a part database and assembling them in a single shape. We
demonstrate that, when performing reconstruction by decoding part
representations into implicit functions, our method achieves state-of-the-art
part-aware reconstruction results from both images and sparse point clouds.
When reconstructing shapes by assembling parts queried from a dataset, our
approach significantly outperforms traditional shape retrieval methods even
when significantly restricting the size of the shape database. We present our
results in well-known sparse point cloud reconstruction and single-view
reconstruction benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences. (arXiv:2205.13713v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13713">
<div class="article-summary-box-inner">
<span><p>Point cloud sequences are irregular and unordered in the spatial dimension
while exhibiting regularities and order in the temporal dimension. Therefore,
existing grid based convolutions for conventional video processing cannot be
directly applied to spatio-temporal modeling of raw point cloud sequences. In
this paper, we propose a point spatio-temporal (PST) convolution to achieve
informative representations of point cloud sequences. The proposed PST
convolution first disentangles space and time in point cloud sequences. Then, a
spatial convolution is employed to capture the local structure of points in the
3D space, and a temporal convolution is used to model the dynamics of the
spatial regions along the time dimension. Furthermore, we incorporate the
proposed PST convolution into a deep network, namely PSTNet, to extract
features of point cloud sequences in a hierarchical manner. Extensive
experiments on widely-used 3D action recognition and 4D semantic segmentation
datasets demonstrate the effectiveness of PSTNet to model point cloud
sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Abstract Reasoning with Dual-Contrast Network. (arXiv:2205.13720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13720">
<div class="article-summary-box-inner">
<span><p>As a step towards improving the abstract reasoning capability of machines, we
aim to solve Raven's Progressive Matrices (RPM) with neural networks, since
solving RPM puzzles is highly correlated with human intelligence. Unlike
previous methods that use auxiliary annotations or assume hidden rules to
produce appropriate feature representation, we only use the ground truth answer
of each question for model learning, aiming for an intelligent agent to have a
strong learning capability with a small amount of supervision. Based on the RPM
problem formulation, the correct answer filled into the missing entry of the
third row/column has to best satisfy the same rules shared between the first
two rows/columns. Thus we design a simple yet effective Dual-Contrast Network
(DCNet) to exploit the inherent structure of RPM puzzles. Specifically, a rule
contrast module is designed to compare the latent rules between the filled
row/column and the first two rows/columns; a choice contrast module is designed
to increase the relative differences between candidate choices. Experimental
results on the RAVEN and PGM datasets show that DCNet outperforms the
state-of-the-art methods by a large margin of 5.77%. Further experiments on few
training samples and model generalization also show the effectiveness of DCNet.
Code is available at https://github.com/visiontao/dcnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DLTTA: Dynamic Learning Rate for Test-time Adaptation on Cross-domain Medical Images. (arXiv:2205.13723v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13723">
<div class="article-summary-box-inner">
<span><p>Test-time adaptation (TTA) has increasingly been an important topic to
efficiently tackle the cross-domain distribution shift at test time for medical
images from different institutions. Previous TTA methods have a common
limitation of using a fixed learning rate for all the test samples. Such a
practice would be sub-optimal for TTA, because test data may arrive
sequentially therefore the scale of distribution shift would change frequently.
To address this problem, we propose a novel dynamic learning rate adjustment
method for test-time adaptation, called DLTTA, which dynamically modulates the
amount of weights update for each test image to account for the differences in
their distribution shift. Specifically, our DLTTA is equipped with a memory
bank based estimation scheme to effectively measure the discrepancy of a given
test sample. Based on this estimated discrepancy, a dynamic learning rate
adjustment strategy is then developed to achieve a suitable degree of
adaptation for each test sample. The effectiveness and general applicability of
our DLTTA is extensively demonstrated on three tasks including retinal optical
coherence tomography (OCT) segmentation, histopathological image
classification, and prostate 3D MRI segmentation. Our method achieves effective
and fast test-time adaptation with consistent performance improvement over
current state-of-the-art test-time adaptation methods. Code is available at:
https://github.com/med-air/DLTTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V-Doc : Visual questions answers with Documents. (arXiv:2205.13724v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13724">
<div class="article-summary-box-inner">
<span><p>We propose V-Doc, a question-answering tool using document images and PDF,
mainly for researchers and general non-deep learning experts looking to
generate, process, and understand the document visual question answering tasks.
The V-Doc supports generating and using both extractive and abstractive
question-answer pairs using documents images. The extractive QA selects a
subset of tokens or phrases from the document contents to predict the answers,
while the abstractive QA recognises the language in the content and generates
the answer based on the trained model. Both aspects are crucial to
understanding the documents, especially in an image format. We include a
detailed scenario of question generation for the abstractive QA task. V-Doc
supports a wide range of datasets and models, and is highly extensible through
a declarative, framework-agnostic platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Reconstruction of Multi Branch Feature Multiplexing Fusion Network with Mixed Multi-layer Attention. (arXiv:2205.13738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13738">
<div class="article-summary-box-inner">
<span><p>Image super-resolution reconstruction achieves better results than
traditional methods with the help of the powerful nonlinear representation
ability of convolution neural network. However, some existing algorithms also
have some problems, such as insufficient utilization of phased features,
ignoring the importance of early phased feature fusion to improve network
performance, and the inability of the network to pay more attention to
high-frequency information in the reconstruction process. To solve these
problems, we propose a multi-branch feature multiplexing fusion network with
mixed multi-layer attention (MBMFN), which realizes the multiple utilization of
features and the multistage fusion of different levels of features. To further
improve the networks performance, we propose a lightweight enhanced residual
channel attention (LERCA), which can not only effectively avoid the loss of
channel information but also make the network pay more attention to the key
channel information and benefit from it. Finally, the attention mechanism is
introduced into the reconstruction process to strengthen the restoration of
edge texture and other details. A large number of experiments on several
benchmark sets show that, compared with other advanced reconstruction
algorithms, our algorithm produces highly competitive objective indicators and
restores more image detail texture information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Instance Representation Banks for Aerial Scene Classification. (arXiv:2205.13744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13744">
<div class="article-summary-box-inner">
<span><p>Aerial scenes are more complicated in terms of object distribution and
spatial arrangement than natural scenes due to the bird view, and thus remain
challenging to learn discriminative scene representation. Recent solutions
design \textit{local semantic descriptors} so that region of interests (RoIs)
can be properly highlighted. However, each local descriptor has limited
description capability and the overall scene representation remains to be
refined. In this paper, we solve this problem by designing a novel
representation set named \textit{instance representation bank} (IRB), which
unifies multiple local descriptors under the multiple instance learning (MIL)
formulation. This unified framework is not trivial as all the local semantic
descriptors can be aligned to the same scene scheme, enhancing the scene
representation capability. Specifically, our IRB learning framework consists of
a backbone, an instance representation bank, a semantic fusion module and a
scene scheme alignment loss function. All the components are organized in an
end-to-end manner. Extensive experiments on three aerial scene benchmarks
demonstrate that our proposed method outperforms the state-of-the-art
approaches by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Awareness Multiple Instance Neural Network. (arXiv:2205.13750v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13750">
<div class="article-summary-box-inner">
<span><p>Multiple instance learning is qualified for many pattern recognition tasks
with weakly annotated data. The combination of artificial neural network and
multiple instance learning offers an end-to-end solution and has been widely
utilized. However, challenges remain in two-folds. Firstly, current MIL pooling
operators are usually pre-defined and lack flexibility to mine key instances.
Secondly, in current solutions, the bag-level representation can be inaccurate
or inaccessible. To this end, we propose an attention awareness multiple
instance neural network framework in this paper. It consists of an
instance-level classifier, a trainable MIL pooling operator based on spatial
attention and a bag-level classification layer. Exhaustive experiments on a
series of pattern recognition tasks demonstrate that our framework outperforms
many state-of-the-art MIL methods and validates the effectiveness of our
proposed attention MIL pooling operators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIGMO: Categorical invariant representations in a deep generative framework. (arXiv:2205.13758v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13758">
<div class="article-summary-box-inner">
<span><p>Data of general object images have two most common structures: (1) each
object of a given shape can be rendered in multiple different views, and (2)
shapes of objects can be categorized in such a way that the diversity of shapes
is much larger across categories than within a category. Existing deep
generative models can typically capture either structure, but not both. In this
work, we introduce a novel deep generative model, called CIGMO, that can learn
to represent category, shape, and view factors from image data. The model is
comprised of multiple modules of shape representations that are each
specialized to a particular category and disentangled from view representation,
and can be learned using a group-based weakly supervised learning method. By
empirical investigation, we show that our model can effectively discover
categories of object shapes despite large view variation and quantitatively
supersede various previous methods including the state-of-the-art invariant
clustering algorithm. Further, we show that our approach using
category-specialization can enhance the learned shape representation to better
perform down-stream tasks such as one-shot object identification as well as
shape-view disentanglement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Convolutional One-Stage 3D Object Detection on LiDAR Range Images. (arXiv:2205.13764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13764">
<div class="article-summary-box-inner">
<span><p>We present a simple yet effective fully convolutional one-stage 3D object
detector for LiDAR point clouds of autonomous driving scenes, termed
FCOS-LiDAR. Unlike the dominant methods that use the bird-eye view (BEV), our
proposed detector detects objects from the range view (RV, a.k.a. range image)
of the LiDAR points. Due to the range view's compactness and compatibility with
the LiDAR sensors' sampling process on self-driving cars, the range view-based
object detector can be realized by solely exploiting the vanilla 2D
convolutions, departing from the BEV-based methods which often involve
complicated voxelization operations and sparse convolutions.
</p>
<p>For the first time, we show that an RV-based 3D detector with standard 2D
convolutions alone can achieve comparable performance to state-of-the-art
BEV-based detectors while being significantly faster and simpler. More
importantly, almost all previous range view-based detectors only focus on
single-frame point clouds, since it is challenging to fuse multi-frame point
clouds into a single range view. In this work, we tackle this challenging issue
with a novel range view projection mechanism, and for the first time
demonstrate the benefits of fusing multi-frame point clouds for a range-view
based detector. Extensive experiments on nuScenes show the superiority of our
proposed method and we believe that our work can be strong evidence that an
RV-based 3D detector can compare favourably with the current mainstream
BEV-based detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-aware Dense Representation Learning for Remote Sensing Image Change Detection. (arXiv:2205.13769v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13769">
<div class="article-summary-box-inner">
<span><p>Training deep learning-based change detection (CD) model heavily depends on
labeled data. Contemporary transfer learning-based methods to alleviate the CD
label insufficiency mainly upon ImageNet pre-training. A recent trend is using
remote sensing (RS) data to obtain in-domain representations via supervised or
self-supervised learning (SSL). Here, different from traditional supervised
pre-training that learns the mapping from image to label, we leverage semantic
supervision in a contrastive manner. There are typically multiple objects of
interest (e.g., buildings) distributed in varying locations in RS images. We
propose dense semantic-aware pre-training for RS image CD via sampling multiple
class-balanced points. Instead of manipulating image-level representations that
lack spatial information, we constrain pixel-level cross-view consistency and
cross-semantic discrimination to learn spatially-sensitive features, thus
benefiting downstream dense CD. Apart from learning illumination invariant
features, we fulfill consistent foreground features insensitive to irrelevant
background changes via a synthetic view using background swapping. We
additionally achieve discriminative representations to distinguish foreground
land-covers and others. We collect large-scale image-mask pairs freely
available in the RS community for pre-training. Extensive experiments on three
CD datasets verify the effectiveness of our method. Ours significantly
outperforms ImageNet, in-domain supervision, and several SSL methods. Empirical
results indicate ours well alleviates data insufficiency in CD. Notably, we
achieve competitive results using only 20% training data than baseline (random)
using 100% data. Both quantitative and qualitative results demonstrate the
generalization ability of our pre-trained model to downstream images even
remaining domain gaps with the pre-training data. Our Code will make public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEAF + AIO: Edge-Assisted Energy-Aware Object Detection for Mobile Augmented Reality. (arXiv:2205.13770v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13770">
<div class="article-summary-box-inner">
<span><p>Today very few deep learning-based mobile augmented reality (MAR)
applications are applied in mobile devices because they are significantly
energy-guzzling. In this paper, we design an edge-based energy-aware MAR system
that enables MAR devices to dynamically change their configurations, such as
CPU frequency, computation model size, and image offloading frequency based on
user preferences, camera sampling rates, and available radio resources. Our
proposed dynamic MAR configuration adaptations can minimize the per frame
energy consumption of multiple MAR clients without degrading their preferred
MAR performance metrics, such as latency and detection accuracy. To thoroughly
analyze the interactions among MAR configurations, user preferences, camera
sampling rate, and energy consumption, we propose, to the best of our
knowledge, the first comprehensive analytical energy model for MAR devices.
Based on the proposed analytical model, we design a LEAF optimization algorithm
to guide the MAR configuration adaptation and server radio resource allocation.
An image offloading frequency orchestrator, coordinating with the LEAF, is
developed to adaptively regulate the edge-based object detection invocations
and to further improve the energy efficiency of MAR devices. Extensive
evaluations are conducted to validate the performance of the proposed
analytical model and algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of COVID-19 Patients with their Severity Level from Chest CT Scans using Transfer Learning. (arXiv:2205.13774v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13774">
<div class="article-summary-box-inner">
<span><p>Background and Objective: During pandemics, the use of artificial
intelligence (AI) approaches combined with biomedical science play a
significant role in reducing the burden on the healthcare systems and
physicians. The rapid increment in cases of COVID-19 has led to an increase in
demand for hospital beds and other medical equipment. However, since medical
facilities are limited, it is recommended to diagnose patients as per the
severity of the infection. Keeping this in mind, we share our research in
detecting COVID-19 as well as assessing its severity using chest-CT scans and
Deep Learning pre-trained models. Dataset: We have collected a total of 1966 CT
Scan images for three different class labels, namely, Non-COVID, Severe COVID,
and Non-Severe COVID, out of which 714 CT images belong to the Non-COVID
category, 713 CT images are for Non-Severe COVID category and 539 CT images are
of Severe COVID category. Methods: All of the images are initially
pre-processed using the Contrast Limited Histogram Equalization (CLAHE)
approach. The pre-processed images are then fed into the VGG-16 network for
extracting features. Finally, the retrieved characteristics are categorized and
the accuracy is evaluated using a support vector machine (SVM) with 10-fold
cross-validation (CV). Result and Conclusion: In our study, we have combined
well-known strategies for pre-processing, feature extraction, and
classification which brings us to a remarkable success rate of disease and its
severity recognition with an accuracy of 96.05% (97.7% for Non-Severe COVID-19
images and 93% for Severe COVID-19 images). Our model can therefore help
radiologists detect COVID-19 and the extent of its severity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Long-Tailed Visual Recognition. (arXiv:2205.13775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13775">
<div class="article-summary-box-inner">
<span><p>The heavy reliance on data is one of the major reasons that currently limit
the development of deep learning. Data quality directly dominates the effect of
deep learning models, and the long-tailed distribution is one of the factors
affecting data quality. The long-tailed phenomenon is prevalent due to the
prevalence of power law in nature. In this case, the performance of deep
learning models is often dominated by the head classes while the learning of
the tail classes is severely underdeveloped. In order to learn adequately for
all classes, many researchers have studied and preliminarily addressed the
long-tailed problem. In this survey, we focus on the problems caused by
long-tailed data distribution, sort out the representative long-tailed visual
recognition datasets and summarize some mainstream long-tailed studies.
Specifically, we summarize these studies into ten categories from the
perspective of representation learning, and outline the highlights and
limitations of each category. Besides, we have studied four quantitative
metrics for evaluating the imbalance, and suggest using the Gini coefficient to
evaluate the long-tailedness of a dataset. Based on the Gini coefficient, we
quantitatively study 20 widely-used and large-scale visual datasets proposed in
the last decade, and find that the long-tailed phenomenon is widespread and has
not been fully studied. Finally, we provide several future directions for the
development of long-tailed learning to provide more ideas for readers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework. (arXiv:2205.13790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13790">
<div class="article-summary-box-inner">
<span><p>Fusing the camera and LiDAR information has become a de-facto standard for 3D
object detection tasks. Current methods rely on point clouds from the LiDAR
sensor as queries to leverage the feature from the image space. However, people
discover that this underlying assumption makes the current fusion framework
infeasible to produce any prediction when there is a LiDAR malfunction,
regardless of minor or major. This fundamentally limits the deployment
capability to realistic autonomous driving scenarios. In contrast, we propose a
surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera
stream does not depend on the input of LiDAR data, thus addressing the downside
of previous methods. We empirically show that our framework surpasses the
state-of-the-art methods under the normal training settings. Under the
robustness training settings that simulate various LiDAR malfunctions, our
framework significantly surpasses the state-of-the-art methods by 15.7% to
28.9% mAP. To the best of our knowledge, we are the first to handle realistic
LiDAR malfunction and can be deployed to realistic scenarios without any
post-processing procedure. The code is available at
https://github.com/ADLab-AutoDrive/BEVFusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Morphing: Fooling a Face Recognition System Is Simple!. (arXiv:2205.13796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13796">
<div class="article-summary-box-inner">
<span><p>State-of-the-art face recognition (FR) approaches have shown remarkable
results in predicting whether two faces belong to the same identity, yielding
accuracies between 92% and 100% depending on the difficulty of the protocol.
However, the accuracy drops substantially when exposed to morphed faces,
specifically generated to look similar to two identities. To generate morphed
faces, we integrate a simple pretrained FR model into a generative adversarial
network (GAN) and modify several loss functions for face morphing. In contrast
to previous works, our approach and analyses are not limited to pairs of
frontal faces with the same ethnicity and gender. Our qualitative and
quantitative results affirm that our approach achieves a seamless change
between two faces even in unconstrained scenarios. Despite using features from
a simpler FR model for face morphing, we demonstrate that even recent FR
systems struggle to distinguish the morphed face from both identities obtaining
an accuracy of only 55-70%. Besides, we provide further insights into how
knowing the FR system makes it particularly vulnerable to face morphing
attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions. (arXiv:2205.13803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13803">
<div class="article-summary-box-inner">
<span><p>A significant gap remains between today's visual pattern recognition models
and human-level visual cognition especially when it comes to few-shot learning
and compositional reasoning of novel concepts. We introduce Bongard-HOI, a new
visual reasoning benchmark that focuses on compositional learning of
human-object interactions (HOIs) from natural images. It is inspired by two
desirable characteristics from the classical Bongard problems (BPs): 1)
few-shot concept learning, and 2) context-dependent reasoning. We carefully
curate the few-shot instances with hard negatives, where positive and negative
images only disagree on action labels, making mere recognition of object
categories insufficient to complete our benchmarks. We also design multiple
test sets to systematically study the generalization of visual learning models,
where we vary the overlap of the HOI concepts between the training and test
sets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a
substantial challenge to today's visual recognition models. The
state-of-the-art HOI detection model achieves only 62% accuracy on few-shot
binary prediction while even amateur human testers on MTurk have 91% accuracy.
With the Bongard-HOI benchmark, we hope to further advance research efforts in
visual reasoning, especially in holistic perception-reasoning systems and
better representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-ViT: High Performance Linear Vision Transformer without Softmax. (arXiv:2205.13805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13805">
<div class="article-summary-box-inner">
<span><p>Vision transformers have become one of the most important models for computer
vision tasks. Although they outperform prior works, they require heavy
computational resources on a scale that is quadratic to the number of tokens,
$N$. This is a major drawback of the traditional self-attention (SA) algorithm.
Here, we propose the X-ViT, ViT with a novel SA mechanism that has linear
complexity. The main approach of this work is to eliminate nonlinearity from
the original SA. We factorize the matrix multiplication of the SA mechanism
without complicated linear approximation. By modifying only a few lines of code
from the original SA, the proposed models outperform most transformer-based
models on image classification and dense prediction tasks on most capacity
regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Look at Improving Robustness in Visual-inertial SLAM by Moment Matching. (arXiv:2205.13821v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13821">
<div class="article-summary-box-inner">
<span><p>The fusion of camera sensor and inertial data is a leading method for
ego-motion tracking in autonomous and smart devices. State estimation
techniques that rely on non-linear filtering are a strong paradigm for solving
the associated information fusion task. The de facto inference method in this
space is the celebrated extended Kalman filter (EKF), which relies on
first-order linearizations of both the dynamical and measurement model. This
paper takes a critical look at the practical implications and limitations posed
by the EKF, especially under faulty visual feature associations and the
presence of strong confounding noise. As an alternative, we revisit the assumed
density formulation of Bayesian filtering and employ a moment matching
(unscented Kalman filtering) approach to both visual-inertial odometry and
visual SLAM. Our results highlight important aspects in robustness both in
dynamics propagation and visual measurement updates, and we show
state-of-the-art results on EuRoC MAV drone data benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Fetal Ultrasound Video Model Match Human Observers in Biometric Measurements. (arXiv:2205.13835v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13835">
<div class="article-summary-box-inner">
<span><p>Objective. This work investigates the use of deep convolutional neural
networks (CNN) to automatically perform measurements of fetal body parts,
including head circumference, biparietal diameter, abdominal circumference and
femur length, and to estimate gestational age and fetal weight using fetal
ultrasound videos. Approach. We developed a novel multi-task CNN-based
spatio-temporal fetal US feature extraction and standard plane detection
algorithm (called FUVAI) and evaluated the method on 50 freehand fetal US video
scans. We compared FUVAI fetal biometric measurements with measurements made by
five experienced sonographers at two time points separated by at least two
weeks. Intra- and inter-observer variabilities were estimated. Main results. We
found that automated fetal biometric measurements obtained by FUVAI were
comparable to the measurements performed by experienced sonographers The
observed differences in measurement values were within the range of inter- and
intra-observer variability. Moreover, analysis has shown that these differences
were not statistically significant when comparing any individual medical expert
to our model. Significance. We argue that FUVAI has the potential to assist
sonographers who perform fetal biometric measurements in clinical settings by
providing them with suggestions regarding the best measuring frames, along with
automated measurements. Moreover, FUVAI is able perform these tasks in just a
few seconds, which is a huge difference compared to the average of six minutes
taken by sonographers. This is significant, given the shortage of medical
experts capable of interpreting fetal ultrasound images in numerous countries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textural-Structural Joint Learning for No-Reference Super-Resolution Image Quality Assessment. (arXiv:2205.13847v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13847">
<div class="article-summary-box-inner">
<span><p>Image super-resolution (SR) has been widely investigated in recent years.
However, it is challenging to fairly estimate the performances of various SR
methods, as the lack of reliable and accurate criteria for perceptual quality.
Existing SR image quality assessment (IQA) metrics usually concentrate on the
specific kind of degradation without distinguishing the visual sensitive areas,
which have no adaptive ability to describe the diverse SR degeneration
situations. In this paper, we focus on the textural and structural degradation
of image SR which acts as a critical role for visual perception, and design a
dual stream network to jointly explore the textural and structural information
for quality prediction, dubbed TSNet. By mimicking the human vision system
(HVS) that pays more attention to the significant areas of the image, we
develop the spatial attention mechanism to make the visual-sensitive areas more
distinguishable, which improves the prediction accuracy. Feature normalization
(F-Norm) is also developed to investigate the inherent spatial correlation of
SR features and boost the network representation capacity. Experimental results
show the proposed TSNet predicts the visual quality more accurate than the
state-of-the-art IQA methods, and demonstrates better consistency with the
human's perspective. The source code will be made available at
<a href="http://github.com/yuqing-liu-dut/NRIQA_SR.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Patterns in Visualized Data by Adding Redundant Visual Information. (arXiv:2205.13856v1 [stat.CO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13856">
<div class="article-summary-box-inner">
<span><p>We present "PATRED", a technique that uses the addition of redundant
information to facilitate the detection of specific, generally described
patterns in line-charts during the visual exploration of the charts. We
compared different versions of this technique, that differed in the way
redundancy was added, using nine distance metrics (such as Euclidean, Pearson,
Mutual Information and Jaccard) with judgments from data scientists which
served as the "ground truth". Results were analyzed with correlations (R2), F1
scores and Mutual Information with the average ranking by the data scientists.
Some distance metrics consistently benefit from the addition of redundant
information, while others are only enhanced for specific types of data
perturbations. The results demonstrate the value of adding redundancy to
improve the identification of patterns in time-series data during visual
exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrackNet: A Triplet metric-based method for Multi-Target Multi-Camera Vehicle Tracking. (arXiv:2205.13857v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13857">
<div class="article-summary-box-inner">
<span><p>We present TrackNet, a method for Multi-Target Multi-Camera (MTMC) vehicle
tracking from traffic video sequences. Cross-camera vehicle tracking has proved
to be a challenging task due to perspective, scale and speed variance, as well
occlusions and noise conditions. Our method is based on a modular approach that
first detects vehicles frame-by-frame using Faster R-CNN, then tracks
detections through single camera using Kalman filter, and finally matches
tracks by a triplet metric learning strategy. We conduct experiments on
TrackNet within the AI City Challenge framework, and present competitive IDF1
results of 0.4733.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Deep Learning Segmentation and Multigrader-annotated Mandibular Canals of Multicenter CBCT scans. (arXiv:2205.13874v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13874">
<div class="article-summary-box-inner">
<span><p>Deep learning approach has been demonstrated to automatically segment the
bilateral mandibular canals from CBCT scans, yet systematic studies of its
clinical and technical validation are scarce. To validate the mandibular canal
localization accuracy of a deep learning system (DLS) we trained it with 982
CBCT scans and evaluated using 150 scans of five scanners from clinical
workflow patients of European and Southeast Asian Institutes, annotated by four
radiologists. The interobserver variability was compared to the variability
between the DLS and the radiologists. In addition, the generalization of DLS to
CBCT scans from scanners not used in the training data was examined to evaluate
the out-of-distribution generalization capability. The DLS had lower
variability to the radiologists than the interobserver variability between them
and it was able to generalize to three new devices. For the radiologists'
consensus segmentation, used as gold standard, the DLS had a symmetric mean
curve distance of 0.39 mm compared to those of the individual radiologists with
0.62 mm, 0.55 mm, 0.47 mm, and 0.42 mm. The DLS showed comparable or slightly
better performance in the segmentation of the mandibular canal with the
radiologists and generalization capability to new scanners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TraClets: Harnessing the power of computer vision for trajectory classification. (arXiv:2205.13880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13880">
<div class="article-summary-box-inner">
<span><p>Due to the advent of new mobile devices and tracking sensors in recent years,
huge amounts of data are being produced every day. Therefore, novel
methodologies need to emerge that dive through this vast sea of information and
generate insights and meaningful information. To this end, researchers have
developed several trajectory classification algorithms over the years that are
able to annotate tracking data. Similarly, in this research, a novel
methodology is presented that exploits image representations of trajectories,
called TraClets, in order to classify trajectories in an intuitive humans way,
through computer vision techniques. Several real-world datasets are used to
evaluate the proposed approach and compare its classification performance to
other state-of-the-art trajectory classification algorithms. Experimental
results demonstrate that TraClets achieves a classification performance that is
comparable to, or in most cases, better than the state-of-the-art, acting as a
universal, high-accuracy approach for trajectory classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Domain Generalization. (arXiv:2205.13913v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13913">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) is a fundamental yet very challenging research
topic in machine learning. The existing arts mainly focus on learning
domain-invariant features with limited source domains in a static model.
Unfortunately, there is a lack of training-free mechanism to adjust the model
when generalized to the agnostic target domains. To tackle this problem, we
develop a brand-new DG variant, namely Dynamic Domain Generalization (DDG), in
which the model learns to twist the network parameters to adapt the data from
different domains. Specifically, we leverage a meta-adjuster to twist the
network parameters based on the static model with respect to different data
from different domains. In this way, the static model is optimized to learn
domain-shared features, while the meta-adjuster is designed to learn
domain-specific features. To enable this process, DomainMix is exploited to
simulate data from diverse domains during teaching the meta-adjuster to adapt
to the upcoming agnostic target domains. This learning mechanism urges the
model to generalize to different agnostic target domains via adjusting the
model without training. Extensive experiments demonstrate the effectiveness of
our proposed method. Code is available at: https://github.com/MetaVisionLab/DDG
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DILG: Irregular Latent Grids for 3D Generative Modeling. (arXiv:2205.13914v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13914">
<div class="article-summary-box-inner">
<span><p>We propose a new representation for encoding 3D shapes as neural fields. The
representation is designed to be compatible with the transformer architecture
and to benefit both shape reconstruction and shape generation. Existing works
on neural fields are grid-based representations with latents defined on a
regular grid. In contrast, we define latents on irregular grids, enabling our
representation to be sparse and adaptive. In the context of shape
reconstruction from point clouds, our shape representation built on irregular
grids improves upon grid-based methods in terms of reconstruction accuracy. For
shape generation, our representation promotes high-quality shape generation
using auto-regressive probabilistic models. We show different applications that
improve over the current state of the art. First, we show results for
probabilistic shape reconstruction from a single higher resolution image.
Second, we train a probabilistic model conditioned on very low resolution
images. Third, we apply our model to category-conditioned generation. All
probabilistic experiments confirm that we are able to generate detailed and
high quality shapes to yield the new state of the art in generative 3D shape
modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREAM: Weakly Supervised Object Localization via Class RE-Activation Mapping. (arXiv:2205.13922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13922">
<div class="article-summary-box-inner">
<span><p>Weakly Supervised Object Localization (WSOL) aims to localize objects with
image-level supervision. Existing works mainly rely on Class Activation Mapping
(CAM) derived from a classification model. However, CAM-based methods usually
focus on the most discriminative parts of an object (i.e., incomplete
localization problem). In this paper, we empirically prove that this problem is
associated with the mixup of the activation values between less discriminative
foreground regions and the background. To address it, we propose Class
RE-Activation Mapping (CREAM), a novel clustering-based approach to boost the
activation values of the integral object regions. To this end, we introduce
class-specific foreground and background context embeddings as cluster
centroids. A CAM-guided momentum preservation strategy is developed to learn
the context embeddings during training. At the inference stage, the
re-activation mapping is formulated as a parameter estimation problem under
Gaussian Mixture Model, which can be solved by deriving an unsupervised
Expectation-Maximization based soft-clustering algorithm. By simply integrating
CREAM into various WSOL approaches, our method significantly improves their
performance. CREAM achieves the state-of-the-art performance on CUB, ILSVRC and
OpenImages benchmark datasets. Code will be available at
https://github.com/Jazzcharles/CREAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep face recognition with clustering based domain adaptation. (arXiv:2205.13937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13937">
<div class="article-summary-box-inner">
<span><p>Despite great progress in face recognition tasks achieved by deep convolution
neural networks (CNNs), these models often face challenges in real world tasks
where training images gathered from Internet are different from test images
because of different lighting condition, pose and image quality. These factors
increase domain discrepancy between training (source domain) and testing
(target domain) database and make the learnt models degenerate in application.
Meanwhile, due to lack of labeled target data, directly fine-tuning the
pre-learnt models becomes intractable and impractical. In this paper, we
propose a new clustering-based domain adaptation method designed for face
recognition task in which the source and target domain do not share any
classes. Our method effectively learns the discriminative target feature by
aligning the feature domain globally, and, at the meantime, distinguishing the
target clusters locally. Specifically, it first learns a more reliable
representation for clustering by minimizing global domain discrepancy to reduce
domain gaps, and then applies simplified spectral clustering method to generate
pseudo-labels in the domain-invariant feature space, and finally learns
discriminative target representation. Comprehensive experiments on widely-used
GBU, IJB-A/B/C and RFW databases clearly demonstrate the effectiveness of our
newly proposed approach. State-of-the-art performance of GBU data set is
achieved by only unsupervised adaptation from the target training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13943">
<div class="article-summary-box-inner">
<span><p>Masked image modeling (MIM), an emerging self-supervised pre-training method,
has shown impressive success across numerous downstream vision tasks with
Vision transformers (ViT). Its underlying idea is simple: a portion of the
input image is randomly masked out and then reconstructed via the pre-text
task. However, why MIM works well is not well explained, and previous studies
insist that MIM primarily works for the Transformer family but is incompatible
with CNNs. In this paper, we first study interactions among patches to
understand what knowledge is learned and how it is acquired via the MIM task.
We observe that MIM essentially teaches the model to learn better middle-level
interactions among patches and extract more generalized features. Based on this
fact, we propose an Architecture-Agnostic Masked Image Modeling framework
(A$^2$MIM), which is compatible with not only Transformers but also CNNs in a
unified way. Extensive experiments on popular benchmarks show that our A$^2$MIM
learns better representations and endows the backbone model with the stronger
capability to transfer to various downstream tasks for both Transformers and
CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cycle Label-Consistent Networks for Unsupervised Domain Adaptation. (arXiv:2205.13957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13957">
<div class="article-summary-box-inner">
<span><p>Domain adaptation aims to leverage a labeled source domain to learn a
classifier for the unlabeled target domain with a different distribution.
Previous methods mostly match the distribution between two domains by global or
class alignment. However, global alignment methods cannot achieve a
fine-grained class-to-class overlap; class alignment methods supervised by
pseudo-labels cannot guarantee their reliability. In this paper, we propose a
simple yet efficient domain adaptation method, i.e. Cycle Label-Consistent
Network (CLCN), by exploiting the cycle consistency of classification label,
which applies dual cross-domain nearest centroid classification procedures to
generate a reliable self-supervised signal for the discrimination in the target
domain. The cycle label-consistent loss reinforces the consistency between
ground-truth labels and pseudo-labels of source samples leading to
statistically similar latent representations between source and target domains.
This new loss can easily be added to any existing classification network with
almost no computational overhead. We demonstrate the effectiveness of our
approach on MNIST-USPS-SVHN, Office-31, Office-Home and Image CLEF-DA
benchmarks. Results validate that the proposed method can alleviate the
negative influence of falsely-labeled samples and learn more discriminative
features, leading to the absolute improvement over source-only model by 9.4% on
Office-31 and 6.3% on Image CLEF-DA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video2StyleGAN: Disentangling Local and Global Variations in a Video. (arXiv:2205.13996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13996">
<div class="article-summary-box-inner">
<span><p>Image editing using a pretrained StyleGAN generator has emerged as a powerful
paradigm for facial editing, providing disentangled controls over age,
expression, illumination, etc. However, the approach cannot be directly adopted
for video manipulations. We hypothesize that the main missing ingredient is the
lack of fine-grained and disentangled control over face location, face pose,
and local facial expressions. In this work, we demonstrate that such a
fine-grained control is indeed achievable using pretrained StyleGAN by working
across multiple (latent) spaces (namely, the positional space, the W+ space,
and the S space) and combining the optimization results across the multiple
spaces. Building on this enabling component, we introduce Video2StyleGAN that
takes a target image and driving video(s) to reenact the local and global
locations and expressions from the driving video in the identity of the target
image. We evaluate the effectiveness of our method over multiple challenging
scenarios and demonstrate clear improvements over alternative approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Future Transformer for Long-term Action Anticipation. (arXiv:2205.14022v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14022">
<div class="article-summary-box-inner">
<span><p>The task of predicting future actions from a video is crucial for a
real-world agent interacting with others. When anticipating actions in the
distant future, we humans typically consider long-term relations over the whole
sequence of actions, i.e., not only observed actions in the past but also
potential actions in the future. In a similar spirit, we propose an end-to-end
attention model for action anticipation, dubbed Future Transformer (FUTR), that
leverages global attention over all input frames and output tokens to predict a
minutes-long sequence of future actions. Unlike the previous autoregressive
models, the proposed method learns to predict the whole sequence of future
actions in parallel decoding, enabling more accurate and fast inference for
long-term anticipation. We evaluate our method on two standard benchmarks for
long-term action anticipation, Breakfast and 50 Salads, achieving
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lesion classification by model-based feature extraction: A differential affine invariant model of soft tissue elasticity. (arXiv:2205.14029v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14029">
<div class="article-summary-box-inner">
<span><p>The elasticity of soft tissues has been widely considered as a characteristic
property to differentiate between healthy and vicious tissues and, therefore,
motivated several elasticity imaging modalities, such as Ultrasound
Elastography, Magnetic Resonance Elastography, and Optical Coherence
Elastography. This paper proposes an alternative approach of modeling the
elasticity using Computed Tomography (CT) imaging modality for model-based
feature extraction machine learning (ML) differentiation of lesions. The model
describes a dynamic non-rigid (or elastic) deformation in differential manifold
to mimic the soft tissues elasticity under wave fluctuation in vivo. Based on
the model, three local deformation invariants are constructed by two tensors
defined by the first and second order derivatives from the CT images and used
to generate elastic feature maps after normalization via a novel signal
suppression method. The model-based elastic image features are extracted from
the feature maps and fed to machine learning to perform lesion classifications.
Two pathologically proven image datasets of colon polyps (44 malignant and 43
benign) and lung nodules (46 malignant and 20 benign) were used to evaluate the
proposed model-based lesion classification. The outcomes of this modeling
approach reached the score of area under the curve of the receiver operating
characteristics of 94.2 % for the polyps and 87.4 % for the nodules, resulting
in an average gain of 5 % to 30 % over ten existing state-of-the-art lesion
classification methods. The gains by modeling tissue elasticity for ML
differentiation of lesions are striking, indicating the great potential of
exploring the modeling strategy to other tissue properties for ML
differentiation of lesions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Pedestrian Attribute Recognition with Group Optimization Reward. (arXiv:2205.14042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14042">
<div class="article-summary-box-inner">
<span><p>Pedestrian Attribute Recognition (PAR) is a challenging task in intelligent
video surveillance. Two key challenges in PAR include complex alignment
relations between images and attributes, and imbalanced data distribution.
Existing approaches usually formulate PAR as a recognition task. Different from
them, this paper addresses it as a decision-making task via a reinforcement
learning framework. Specifically, PAR is formulated as a Markov decision
process (MDP) by designing ingenious states, action space, reward function and
state transition. To alleviate the inter-attribute imbalance problem, we apply
an Attribute Grouping Strategy (AGS) by dividing all attributes into subgroups
according to their region and category information. Then we employ an agent to
recognize each group of attributes, which is trained with Deep Q-learning
algorithm. We also propose a Group Optimization Reward (GOR) function to
alleviate the intra-attribute imbalance problem. Experimental results on the
three benchmark datasets of PETA, RAP and PA100K illustrate the effectiveness
and competitiveness of the proposed approach and demonstrate that the
application of reinforcement learning to PAR is a valuable research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning deep learning models for stereo matching using results from semi-global matching. (arXiv:2205.14051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14051">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) methods are widely investigated for stereo image matching
tasks due to their reported high accuracies. However, their
transferability/generalization capabilities are limited by the instances seen
in the training data. With satellite images covering large-scale areas with
variances in locations, content, land covers, and spatial patterns, we expect
their performances to be impacted. Increasing the number and diversity of
training data is always an option, but with the ground-truth disparity being
limited in remote sensing due to its high cost, it is almost impossible to
obtain the ground-truth for all locations. Knowing that classical stereo
matching methods such as Census-based semi-global-matching (SGM) are widely
adopted to process different types of stereo data, we therefore, propose a
finetuning method that takes advantage of disparity maps derived from SGM on
target stereo data. Our proposed method adopts a simple scheme that uses the
energy map derived from the SGM algorithm to select high confidence disparity
measurements, at the same utilizing the images to limit these selected
disparity measurements on texture-rich regions. Our approach aims to
investigate the possibility of improving the transferability of current DL
methods to unseen target data without having their ground truth as a
requirement. To perform a comprehensive study, we select 20 study-sites around
the world to cover a variety of complexities and densities. We choose
well-established DL methods like geometric and context network (GCNet), pyramid
stereo matching network (PSMNet), and LEAStereo for evaluation. Our results
indicate an improvement in the transferability of the DL methods across
different regions visually and numerically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Harmonization with Region-wise Contrastive Learning. (arXiv:2205.14058v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14058">
<div class="article-summary-box-inner">
<span><p>Image harmonization task aims at harmonizing different composite foreground
regions according to specific background image. Previous methods would rather
focus on improving the reconstruction ability of the generator by some internal
enhancements such as attention, adaptive normalization and light adjustment,
$etc.$. However, they pay less attention to discriminating the foreground and
background appearance features within a restricted generator, which becomes a
new challenge in image harmonization task. In this paper, we propose a novel
image harmonization framework with external style fusion and region-wise
contrastive learning scheme. For the external style fusion, we leverage the
external background appearance from the encoder as the style reference to
generate harmonized foreground in the decoder. This approach enhances the
harmonization ability of the decoder by external background guidance. Moreover,
for the contrastive learning scheme, we design a region-wise contrastive loss
function for image harmonization task. Specifically, we first introduce a
straight-forward samples generation method that selects negative samples from
the output harmonized foreground region and selects positive samples from the
ground-truth background region. Our method attempts to bring together
corresponding positive and negative samples by maximizing the mutual
information between the foreground and background styles, which desirably makes
our harmonization network more robust to discriminate the foreground and
background style features when harmonizing composite images. Extensive
experiments on the benchmark datasets show that our method can achieve a clear
improvement in harmonization quality and demonstrate the good generalization
capability in real-scenario applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos. (arXiv:2205.14065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14065">
<div class="article-summary-box-inner">
<span><p>Unsupervised object-centric learning aims to represent the modular,
compositional, and causal structure of a scene as a set of object
representations and thereby promises to resolve many critical limitations of
traditional single-vector representations such as poor systematic
generalization. Although there have been many remarkable advances in recent
years, one of the most critical problems in this direction has been that
previous methods work only with simple and synthetic scenes but not with
complex and naturalistic images or videos. In this paper, we propose STEVE, an
unsupervised model for object-centric learning in videos. Our proposed model
makes a significant advancement by demonstrating its effectiveness on various
complex and naturalistic videos unprecedented in this line of research.
Interestingly, this is achieved by neither adding complexity to the model
architecture nor introducing a new objective or weak supervision. Rather, it is
achieved by a surprisingly simple architecture that uses a transformer-based
image decoder conditioned on slots and the learning objective is simply to
reconstruct the observation. Our experiment results on various complex and
naturalistic videos show significant improvements compared to the previous
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sharpness-Aware Training for Free. (arXiv:2205.14083v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14083">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks (DNNs) have achieved state-of-the-art
performances but are typically over-parameterized. The over-parameterization
may result in undesirably large generalization error in the absence of other
customized training strategies. Recently, a line of research under the name of
Sharpness-Aware Minimization (SAM) has shown that minimizing a sharpness
measure, which reflects the geometry of the loss landscape, can significantly
reduce the generalization error. However, SAM-like methods incur a two-fold
computational overhead of the given base optimizer (e.g. SGD) for approximating
the sharpness measure. In this paper, we propose Sharpness-Aware Training for
Free, or SAF, which mitigates the sharp landscape at almost zero additional
computational cost over the base optimizer. Intuitively, SAF achieves this by
avoiding sudden drops in the loss in the sharp local minima throughout the
trajectory of the updates of the weights. Specifically, we suggest a novel
trajectory loss, based on the KL-divergence between the outputs of DNNs with
the current weights and past weights, as a replacement of the SAM's sharpness
measure. This loss captures the rate of change of the training loss along the
model's update trajectory. By minimizing it, SAF ensures the convergence to a
flat minimum with improved generalization capabilities. Extensive empirical
results show that SAF minimizes the sharpness in the same way that SAM does,
yielding better results on the ImageNet dataset with essentially the same
computational cost as the base optimizer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenCalib: A multi-sensor calibration toolbox for autonomous driving. (arXiv:2205.14087v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14087">
<div class="article-summary-box-inner">
<span><p>Accurate sensor calibration is a prerequisite for multi-sensor perception and
localization systems for autonomous vehicles. The intrinsic parameter
calibration of the sensor is to obtain the mapping relationship inside the
sensor, and the extrinsic parameter calibration is to transform two or more
sensors into a unified spatial coordinate system. Most sensors need to be
calibrated after installation to ensure the accuracy of sensor measurements. To
this end, we present OpenCalib, a calibration toolbox that contains a rich set
of various sensor calibration methods. OpenCalib covers manual calibration
tools, automatic calibration tools, factory calibration tools, and online
calibration tools for different application scenarios. At the same time, to
evaluate the calibration accuracy and subsequently improve the accuracy of the
calibration algorithm, we released a corresponding benchmark dataset. This
paper introduces various features and calibration methods of this toolbox. To
our knowledge, this is the first open-sourced calibration codebase containing
the full set of autonomous-driving-related calibration approaches in this area.
We wish that the toolbox could be helpful to autonomous driving researchers. We
have open-sourced our code on GitHub to benefit the community. Code is
available at https://github.com/PJLab-ADG/SensorsCalibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIT: A Generative Image-to-text Transformer for Vision and Language. (arXiv:2205.14100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14100">
<div class="article-summary-box-inner">
<span><p>In this paper, we design and train a Generative Image-to-text Transformer,
GIT, to unify vision-language tasks such as image/video captioning and question
answering. While generative models provide a consistent network architecture
between pre-training and fine-tuning, existing work typically contains complex
structures (uni/multi-modal encoder/decoder) and depends on external modules
such as object detectors/taggers and optical character recognition (OCR). In
GIT, we simplify the architecture as one image encoder and one text decoder
under a single language modeling task. We also scale up the pre-training data
and the model size to boost the model performance. Without bells and whistles,
our GIT establishes new state of the arts on 12 challenging benchmarks with a
large margin. For instance, our model surpasses the human performance for the
first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a
new scheme of generation-based image classification and scene text recognition,
achieving decent performance on standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Interpretability via Polynomials. (arXiv:2205.14108v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14108">
<div class="article-summary-box-inner">
<span><p>Generalized Additive Models (GAMs) have quickly become the leading choice for
fully-interpretable machine learning. However, unlike uninterpretable methods
such as DNNs, they lack expressive power and easy scalability, and are hence
not a feasible alternative for real-world tasks. We present a new class of GAMs
that use tensor rank decompositions of polynomials to learn powerful,
$\textit{fully-interpretable}$ models. Our approach, titled Scalable Polynomial
Additive Models (SPAM) is effortlessly scalable and models $\textit{all}$
higher-order feature interactions without a combinatorial parameter explosion.
SPAM outperforms all current interpretable approaches, and matches DNN/XGBoost
performance on a series of real-world benchmarks with up to hundreds of
thousands of features. We demonstrate by human subject evaluations that SPAMs
are demonstrably more interpretable in practice, and are hence an effortless
replacement for DNNs for creating interpretable and high-performance systems
suitable for large-scale machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Road Segmentation in Challenging Domains Using Similar Place Priors. (arXiv:2205.14112v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14112">
<div class="article-summary-box-inner">
<span><p>Road segmentation in challenging domains, such as night, snow or rain, is a
difficult task. Most current approaches boost performance using fine-tuning,
domain adaptation, style transfer, or by referencing previously acquired
imagery. These approaches share one or more of three significant limitations: a
reliance on large amounts of annotated training data that can be costly to
obtain, both anticipation of and training data from the type of environmental
conditions expected at inference time, and/or imagery captured from a previous
visit to the location. In this research, we remove these restrictions by
improving road segmentation based on similar places. We use Visual Place
Recognition (VPR) to find similar but geographically distinct places, and fuse
segmentations for query images and these similar place priors using a Bayesian
approach and novel segmentation quality metric. Ablation studies show the need
to re-evaluate notions of VPR utility for this task. We demonstrate the system
achieving state-of-the-art road segmentation performance across multiple
challenging condition scenarios including night time and snow, without
requiring any prior training or previous access to the same geographical
locations. Furthermore, we show that this method is network agnostic, improves
multiple baseline techniques and is competitive against methods specialised for
road prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient textual explanations for complex road and traffic scenarios based on semantic segmentation. (arXiv:2205.14118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14118">
<div class="article-summary-box-inner">
<span><p>The complex driving environment brings great challenges to the visual
perception of autonomous vehicles. The accuracy of visual perception drops off
sharply under diverse weather conditions and uncertain traffic flow. Black box
model makes it difficult to interpret the mechanisms of visual perception. To
enhance the user acceptance and reliability of the visual perception system, a
textual explanation of the scene evolvement is essential. It analyzes the
geometry and topology structure in the complex environment and offers clues to
decision and control. However, the existing scene explanation has been
implemented as a separate model. It cannot detect comprehensive textual
information and requires a high computational load and time consumption. Thus,
this study proposed a comprehensive and efficient textual explanation model for
complex road and traffic scenarios. From 336k video frames of the driving
environment, critical images of complex road and traffic scenarios were
selected into a dataset. Through transfer learning, this study established an
accurate and efficient segmentation model to gain semantic information. Based
on the XGBoost algorithm, a comprehensive model was developed. The model
obtained textual information including road types, the motion of conflict
objects, and scenario complexity. The approach was verified on the real-world
road. It improved the perception accuracy of critical traffic elements to
78.8%. The time consumption reached 13 minutes for each epoch, which was 11.5
times more efficient compared with the pre-trained network. The textual
information analyzed from the model was also accordant with reality. The
findings explain how autonomous vehicle detects the driving environment, which
lays a foundation for subsequent decision and control. It can improve the
perception ability by enriching the prior knowledge and judgments for complex
traffic situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Basis Models for Interpretability. (arXiv:2205.14120v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14120">
<div class="article-summary-box-inner">
<span><p>Due to the widespread use of complex machine learning models in real-world
applications, it is becoming critical to explain model predictions. However,
these models are typically black-box deep neural networks, explained post-hoc
via methods with known faithfulness limitations. Generalized Additive Models
(GAMs) are an inherently interpretable class of models that address this
limitation by learning a non-linear shape function for each feature separately,
followed by a linear model on top. However, these models are typically
difficult to train, require numerous parameters, and are difficult to scale.
</p>
<p>We propose an entirely new subfamily of GAMs that utilizes basis
decomposition of shape functions. A small number of basis functions are shared
among all features, and are learned jointly for a given task, thus making our
model scale much better to large-scale data with high-dimensional features,
especially when features are sparse. We propose an architecture denoted as the
Neural Basis Model (NBM) which uses a single neural network to learn these
bases. On a variety of tabular and image datasets, we demonstrate that for
interpretable machine learning, NBMs are the state-of-the-art in accuracy,
model size, and, throughput and can easily model all higher-order feature
interactions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14141">
<div class="article-summary-box-inner">
<span><p>Masked image modeling (MIM) learns representations with remarkably good
fine-tuning performances, overshadowing previous prevalent pre-training
approaches such as image classification, instance contrastive learning, and
image-text alignment. In this paper, we show that the inferior fine-tuning
performance of these pre-training approaches can be significantly improved by a
simple post-processing in the form of feature distillation (FD). The feature
distillation converts the old representations to new representations that have
a few desirable properties just like those representations produced by MIM.
These properties, which we aggregately refer to as optimization friendliness,
are identified and analyzed by a set of attention- and optimization-related
diagnosis tools. With these properties, the new representations show strong
fine-tuning performance. Specifically, the contrastive self-supervised learning
methods are made as competitive in fine-tuning as the state-of-the-art masked
image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is
also significantly improved, with a CLIP ViT-L model reaching 89.0% top-1
accuracy on ImageNet-1K classification. More importantly, our work provides a
way for the future research to focus more effort on the generality and
scalability of the learnt representations without being pre-occupied with
optimization friendliness since it can be enhanced rather easily. The code will
be available at https://github.com/SwinTransformer/Feature-Distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09124">
<div class="article-summary-box-inner">
<span><p>While self-supervised representation learning (SSL) has received widespread
attention from the community, recent research argue that its performance will
suffer a cliff fall when the model size decreases. The current method mainly
relies on contrastive learning to train the network and in this work, we
propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease
the issue by a large margin. Specifically, we find the final embedding obtained
by the mainstream SSL methods contains the most fruitful information, and
propose to distill the final embedding to maximally transmit a teacher's
knowledge to a lightweight model by constraining the last embedding of the
student to be consistent with that of the teacher. In addition, in the
experiment, we find that there exists a phenomenon termed Distilling BottleNeck
and present to enlarge the embedding dimension to alleviate this problem. Our
method does not introduce any extra parameter to lightweight models during
deployment. Experimental results demonstrate that our method achieves the
state-of-the-art on all lightweight models. Particularly, when
ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear
result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,
but the number of parameters of EfficientNet-B0 is only 9.4\%/16.3\% of
ResNet-101/ResNet-50. Code is available at https://github.
com/Yuting-Gao/DisCo-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-CNN for Facial Micro- and Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame. (arXiv:2105.06340v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06340">
<div class="article-summary-box-inner">
<span><p>Facial expression spotting is the preliminary step for micro- and
macro-expression analysis. The task of reliably spotting such expressions in
video sequences is currently unsolved. The current best systems depend upon
optical flow methods to extract regional motion features, before categorisation
of that motion into a specific class of facial movement. Optical flow is
susceptible to drift error, which introduces a serious problem for motions with
long-term dependencies, such as high frame-rate macro-expression. We propose a
purely deep learning solution which, rather than tracking frame differential
motion, compares via a convolutional model, each frame with two temporally
local reference frames. Reference frames are sampled according to calculated
micro- and macro-expression duration. As baseline for MEGC2021 using
leave-one-subject-out evaluation method, we show that our solution achieves
F1-score of 0.105 in a high frame-rate (200 fps) SAMM long videos dataset
(SAMM-LV) and is competitive in a low frame-rate (30 fps) (CAS(ME)2) dataset.
On unseen MEGC2022 challenge dataset, the baseline results are 0.1176 on SAMM
Challenge dataset, 0.1739 on CAS(ME)3 and overall performance of 0.1531 on both
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06011">
<div class="article-summary-box-inner">
<span><p>In the context of visual navigation, the capacity to map a novel environment
is necessary for an agent to exploit its observation history in the considered
place and efficiently reach known goals. This ability can be associated with
spatial reasoning, where an agent is able to perceive spatial relationships and
regularities, and discover object characteristics. Recent work introduces
learnable policies parametrized by deep neural networks and trained with
Reinforcement Learning (RL). In classical RL setups, the capacity to map and
reason spatially is learned end-to-end, from reward alone. In this setting, we
introduce supplementary supervision in the form of auxiliary tasks designed to
favor the emergence of spatial perception capabilities in agents trained for a
goal-reaching downstream objective. We show that learning to estimate metrics
quantifying the spatial relationships between an agent at a given location and
a goal to reach has a high positive impact in Multi-Object Navigation settings.
Our method significantly improves the performance of different baseline agents,
that either build an explicit or implicit representation of the environment,
even matching the performance of incomparable oracle agents taking ground-truth
maps as input. A learning-based agent from the literature trained with the
proposed auxiliary losses was the winning entry to the Multi-Object Navigation
Challenge, part of the CVPR 2021 Embodied AI Workshop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11926">
<div class="article-summary-box-inner">
<span><p>Thanks to their ability to learn flexible data-driven losses, Generative
Adversarial Networks (GANs) are an integral part of many semi- and
weakly-supervised methods for medical image segmentation. GANs jointly optimise
a generator and an adversarial discriminator on a set of training data. After
training is complete, the discriminator is usually discarded, and only the
generator is used for inference. But should we discard discriminators? In this
work, we argue that training stable discriminators produces expressive loss
functions that we can re-use at inference to detect and \textit{correct}
segmentation mistakes. First, we identify key challenges and suggest possible
solutions to make discriminators re-usable at inference. Then, we show that we
can combine discriminators with image reconstruction costs (via decoders) to
endow a causal perspective to test-time training and further improve the model.
Our method is simple and improves the test-time performance of pre-trained
GANs. Moreover, we show that it is compatible with standard post-processing
techniques and it has the potential to be used for Online Continual Learning.
With our work, we open new research avenues for re-using adversarial
discriminators at inference. Our code is available at
https://vios-s.github.io/adversarial-test-time-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfAnFace: Bridging the infant-adult domain gap in facial landmark estimation in the wild. (arXiv:2110.08935v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08935">
<div class="article-summary-box-inner">
<span><p>We lay the groundwork for research in the algorithmic comprehension of infant
faces, in anticipation of applications from healthcare to psychology,
especially in the early prediction of developmental disorders. Specifically, we
introduce the first-ever dataset of infant faces annotated with facial landmark
coordinates and pose attributes, demonstrate the inadequacies of existing
facial landmark estimation algorithms in the infant domain, and train new
state-of-the-art models that significantly improve upon those algorithms using
domain adaptation techniques. We touch on the closely related task of facial
detection for infants, and also on a challenging case study of infrared baby
monitor images gathered by our lab as part of in-field research into the
aforementioned developmental issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IMPROVE Visiolinguistic Performance with Re-Query. (arXiv:2110.10206v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10206">
<div class="article-summary-box-inner">
<span><p>We humans regularly ask for clarification if we are confused when discussing
the visual world, yet the commonplace requirement in visiolinguistic problems
like Visual Dialog, VQA, and Referring Expression Comprehension is to force a
decision based on a single, static language input. Since this assumption does
not match human practice, we relax it and allow our model to request new
language inputs to refine the prediction for a task. Through the exemplar task
of referring expression comprehension, we formalize and motivate the problem,
introduce an evaluation method, and propose \textit{Iterative Multiplication of
Probabilities for Re-query Of Verbal Expressions} (IMPROVE) -- a re-query
method that updates the model's prediction across multiple queries. We
demonstrate IMPROVE on two different referring expression comprehension models
and show it can improve accuracy by up to 6.23% without additional training or
modification to the model's architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Flows as a General Purpose Solution for Inverse Problems. (arXiv:2110.13285v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13285">
<div class="article-summary-box-inner">
<span><p>Due to the success of generative flows to model data distributions, they have
been explored in inverse problems. Given a pre-trained generative flow,
previous work proposed to minimize the 2-norm of the latent variables as a
regularization term. The intuition behind it was to ensure high likelihood
latent variables that produce the closest restoration. However, high-likelihood
latent variables may generate unrealistic samples as we show in our
experiments. We therefore propose a solver to directly produce high-likelihood
reconstructions. We hypothesize that our approach could make generative flows a
general purpose solver for inverse problems. Furthermore, we propose 1 x 1
coupling functions to introduce permutations in a generative flow. It has the
advantage that its inverse does not require to be calculated in the generation
process. Finally, we evaluate our method for denoising, deblurring, inpainting,
and colorization. We observe a compelling improvement of our method over prior
works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts. (arXiv:2111.02358v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02358">
<div class="article-summary-box-inner">
<span><p>We present a unified Vision-Language pretrained Model (VLMo) that jointly
learns a dual encoder and a fusion encoder with a modular Transformer network.
Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer,
where each block contains a pool of modality-specific experts and a shared
self-attention layer. Because of the modeling flexibility of MoME, pretrained
VLMo can be fine-tuned as a fusion encoder for vision-language classification
tasks, or used as a dual encoder for efficient image-text retrieval. Moreover,
we propose a stagewise pre-training strategy, which effectively leverages
large-scale image-only and text-only data besides image-text pairs.
Experimental results show that VLMo achieves state-of-the-art results on
various vision-language tasks, including VQA, NLVR2 and image-text retrieval.
The code and pretrained models are available at https://aka.ms/vlmo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Domain Generalization in Real World: New Benchmark and Strong Baseline. (arXiv:2111.10221v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10221">
<div class="article-summary-box-inner">
<span><p>Conventional domain generalization aims to learn domain invariant
representation from multiple domains, which requires accurate annotations. In
realistic application scenarios, however, it is too cumbersome or even
infeasible to collect and annotate the large mass of data. Yet, web data
provides a free lunch to access a huge amount of unlabeled data with rich style
information that can be harnessed to augment domain generalization ability. In
this paper, we introduce a novel task, termed as semi-supervised domain
generalization, to study how to interact the labeled and unlabeled domains, and
establish two benchmarks including a web-crawled dataset, which poses a novel
yet realistic challenge to push the limits of existing technologies. To tackle
this task, a straightforward solution is to propagate the class information
from the labeled to the unlabeled domains via pseudo labeling in conjunction
with domain confusion training. Considering narrowing domain gap can improve
the quality of pseudo labels and further advance domain invariant feature
learning for generalization, we propose a cycle learning framework to encourage
the positive feedback between label propagation and domain generalization, in
favor of an evolving intermediate domain bridging the labeled and unlabeled
domains in a curriculum learning manner. Experiments are conducted to validate
the effectiveness of our framework. It is worth highlighting that web-crawled
data benefits domain generalization as demonstrated in our results. Our code
will be available later.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks. (arXiv:2111.12965v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12965">
<div class="article-summary-box-inner">
<span><p>One major goal of the AI security community is to securely and reliably
produce and deploy deep learning models for real-world applications. To this
end, data poisoning based backdoor attacks on deep neural networks (DNNs) in
the production stage (or training stage) and corresponding defenses are
extensively explored in recent years. Ironically, backdoor attacks in the
deployment stage, which can often happen in unprofessional users' devices and
are thus arguably far more threatening in real-world scenarios, draw much less
attention of the community. We attribute this imbalance of vigilance to the
weak practicality of existing deployment-stage backdoor attack algorithms and
the insufficiency of real-world attack demonstrations. To fill the blank, in
this work, we study the realistic threat of deployment-stage backdoor attacks
on DNNs. We base our study on a commonly used deployment-stage attack paradigm
-- adversarial weight attack, where adversaries selectively modify model
weights to embed backdoor into deployed DNNs. To approach realistic
practicality, we propose the first gray-box and physically realizable weights
attack algorithm for backdoor injection, namely subnet replacement attack
(SRA), which only requires architecture information of the victim model and can
support physical triggers in the real world. Extensive experimental simulations
and system-level real-world attack demonstrations are conducted. Our results
not only suggest the effectiveness and practicality of the proposed attack
algorithm, but also reveal the practical risk of a novel type of computer virus
that may widely spread and stealthily inject backdoor into DNN models in user
devices. By our study, we call for more attention to the vulnerability of DNNs
in the deployment stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anonymization for Skeleton Action Recognition. (arXiv:2111.15129v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15129">
<div class="article-summary-box-inner">
<span><p>Skeleton-based action recognition attracts practitioners and researchers due
to the lightweight, compact nature of datasets. Compared with RGB-video-based
action recognition, skeleton-based action recognition is a safer way to protect
the privacy of subjects while having competitive recognition performance.
However, due to improvements in skeleton estimation algorithms as well as
motion- and depth-sensors, more details of motion characteristics can be
preserved in the skeleton dataset, leading to potential privacy leakage. To
investigate the potential privacy leakage from skeleton datasets, we first
train a classifier to categorize sensitive private information from
trajectories of joints. Our preliminary experiments show that the gender
classifier achieves 87% accuracy on average and the re-identification task
achieves 80% accuracy on average for three baseline models: Shift-GCN, MS-G3D,
and 2s-AGCN. We propose an adversarial anonymization algorithm to protect
potential privacy leakage from the skeleton dataset. Experimental results show
that an anonymized dataset can reduce the risk of privacy leakage while having
marginal effects on action recognition performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding the Task-Optimal Low-Bit Sub-Distribution in Deep Neural Networks. (arXiv:2112.15139v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15139">
<div class="article-summary-box-inner">
<span><p>Quantized neural networks typically require smaller memory footprints and
lower computation complexity, which is crucial for efficient deployment.
However, quantization inevitably leads to a distribution divergence from the
original network, which generally degrades the performance. To tackle this
issue, massive efforts have been made, but most existing approaches lack
statistical considerations and depend on several manual configurations. In this
paper, we present an adaptive-mapping quantization method to learn an optimal
latent sub-distribution that is inherent within models and smoothly
approximated with a concrete Gaussian Mixture (GM). In particular, the network
weights are projected in compliance with the GM-approximated sub-distribution.
This sub-distribution evolves along with the weight update in a co-tuning
schema guided by the direct task-objective optimization. Sufficient experiments
on image classification and object detection over various modern architectures
demonstrate the effectiveness, generalization property, and transferability of
the proposed method. Besides, an efficient deployment flow for the mobile CPU
is developed, achieving up to 7.46$\times$ inference acceleration on an
octa-core ARM CPU. Our codes have been publicly released at
\url{https://github.com/RunpeiDong/DGMS}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis. (arXiv:2202.02444v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02444">
<div class="article-summary-box-inner">
<span><p>Neural implicit representations, which encode a surface as the level set of a
neural network applied to spatial coordinates, have proven to be remarkably
effective for optimizing, compressing, and generating 3D geometry. Although
these representations are easy to fit, it is not clear how to best evaluate
geometric queries on the shape, such as intersecting against a ray or finding a
closest point. The predominant approach is to encourage the network to have a
signed distance property. However, this property typically holds only
approximately, leading to robustness issues, and holds only at the conclusion
of training, inhibiting the use of queries in loss functions. Instead, this
work presents a new approach to perform queries directly on general neural
implicit functions for a wide range of existing architectures. Our key tool is
the application of range analysis to neural networks, using automatic
arithmetic rules to bound the output of a network over a region; we conduct a
study of range analysis on neural networks, and identify variants of affine
arithmetic which are highly effective. We use the resulting bounds to develop
geometric queries including ray casting, intersection testing, constructing
spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating
bulk properties, and more. Our queries can be efficiently evaluated on GPUs,
and offer concrete accuracy guarantees even on randomly-initialized networks,
enabling their use in training objectives and beyond. We also show a
preliminary application to inverse rendering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Walks for Adversarial Meshes. (arXiv:2202.07453v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07453">
<div class="article-summary-box-inner">
<span><p>A polygonal mesh is the most-commonly used representation of surfaces in
computer graphics. Therefore, it is not surprising that a number of mesh
classification networks have recently been proposed. However, while adversarial
attacks are wildly researched in 2D, the field of adversarial meshes is under
explored. This paper proposes a novel, unified, and general adversarial attack,
which leads to misclassification of several state-of-the-art mesh
classification neural networks. Our attack approach is black-box, i.e. it has
access only to the network's predictions, but not to the network's full
architecture or gradients. The key idea is to train a network to imitate a
given classification network. This is done by utilizing random walks along the
mesh surface, which gather geometric information. These walks provide insight
onto the regions of the mesh that are important for the correct prediction of
the given classification network. These mesh regions are then modified more
than other regions in order to attack the network in a manner that is barely
visible to the naked eye.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Universal Rotation Equivariant Point-cloud Network. (arXiv:2203.01216v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01216">
<div class="article-summary-box-inner">
<span><p>Equivariance to permutations and rigid motions is an important inductive bias
for various 3D learning problems. Recently it has been shown that the
equivariant Tensor Field Network architecture is universal -- it can
approximate any equivariant function. In this paper we suggest a much simpler
architecture, prove that it enjoys the same universality guarantees and
evaluate its performance on Modelnet40. The code to reproduce our experiments
is available at \url{https://github.com/simpleinvariance/UniversalNetwork}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03937">
<div class="article-summary-box-inner">
<span><p>Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by each query
attending to all keys/values, various methods have constrained the range of
attention within local regions, where each query only attends to keys/values
within a hand-crafted window. However, these hand-crafted window partition
mechanisms are data-agnostic and ignore their input content, so it is likely
that one query maybe attends to irrelevant keys/values. To address this issue,
we propose a Dynamic Group Attention (DG-Attention), which dynamically divides
all queries into multiple groups and selects the most relevant keys/values for
each group. Our DG-Attention can flexibly model more relevant dependencies
without any spatial constraint that is used in hand-crafted window based
attention. Built on the DG-Attention, we develop a general vision transformer
backbone named Dynamic Group Transformer (DGT). Extensive experiments show that
our models can outperform the state-of-the-art methods on multiple common
vision tasks, including image classification, semantic segmentation, object
detection, and instance segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4. (arXiv:2203.06649v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06649">
<div class="article-summary-box-inner">
<span><p>Modern high-scoring models of vision in the brain score competition do not
stem from Vision Transformers. However, in this paper, we provide evidence
against the unexpected trend of Vision Transformers (ViT) being not
perceptually aligned with human visual representations by showing how a
dual-stream Transformer, a CrossViT$~\textit{a la}$ Chen et al. (2021), under a
joint rotationally-invariant and adversarial optimization procedure yields 2nd
place in the aggregate Brain-Score 2022 competition(Schrimpf et al., 2020b)
averaged across all visual categories, and at the time of the competition held
1st place for the highest explainable variance of area V4. In addition, our
current Transformer-based model also achieves greater explainable variance for
areas V4, IT and Behaviour than a biologically-inspired CNN (ResNet50) that
integrates a frontal V1-like computation module (Dapello et al.,2020). To
assess the contribution of the optimization scheme with respect to the CrossViT
architecture, we perform several additional experiments on differently
optimized CrossViT's regarding adversarial robustness, common corruption
benchmarks, mid-ventral stimuli interpretation and feature inversion. Against
our initial expectations, our family of results provides tentative support for
an $\textit{"All roads lead to Rome"}$ argument enforced via a joint
optimization rule even for non biologically-motivated models of vision such as
Vision Transformers. Code is available at
https://github.com/williamberrios/BrainScore-Transformers
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Two-Stage Federated Transfer Learning Framework in Medical Images Classification on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12803">
<div class="article-summary-box-inner">
<span><p>COVID-19 pandemic has spread rapidly and caused a shortage of global medical
resources. The efficiency of COVID-19 diagnosis has become highly significant.
As deep learning and convolutional neural network (CNN) has been widely
utilized and been verified in analyzing medical images, it has become a
powerful tool for computer-assisted diagnosis. However, there are two most
significant challenges in medical image classification with the help of deep
learning and neural networks, one of them is the difficulty of acquiring enough
samples, which may lead to model overfitting. Privacy concerns mainly bring the
other challenge since medical-related records are often deemed patients'
private information and protected by laws such as GDPR and HIPPA. Federated
learning can ensure the model training is decentralized on different devices
and no data is shared among them, which guarantees privacy. However, with data
located on different devices, the accessible data of each device could be
limited. Since transfer learning has been verified in dealing with limited data
with good performance, therefore, in this paper, We made a trial to implement
federated learning and transfer learning techniques using CNNs to classify
COVID-19 using lung CT scans. We also explored the impact of dataset
distribution at the client-side in federated learning and the number of
training epochs a model is trained. Finally, we obtained very high performance
with federated learning, demonstrating our success in leveraging accuracy and
privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap. (arXiv:2203.13457v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13457">
<div class="article-summary-box-inner">
<span><p>Recently, contrastive learning has risen to be a promising approach for
large-scale self-supervised learning. However, theoretical understanding of how
it works is still unclear. In this paper, we propose a new guarantee on the
downstream performance without resorting to the conditional independence
assumption that is widely adopted in previous work but hardly holds in
practice. Our new theory hinges on the insight that the support of different
intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Based on this augmentation overlap perspective, theoretically, we
obtain asymptotically closed bounds for downstream performance under weaker
assumptions, and empirically, we propose an unsupervised model selection metric
ARC that aligns well with downstream accuracy. Our theory suggests an
alternative understanding of contrastive learning: the role of aligning
positive samples is more like a surrogate task than an ultimate goal, and the
overlapped augmented views (i.e., the chaos) create a ladder for contrastive
learning to gradually learn class-separated representations. The code for
computing ARC is available at https://github.com/zhangq327/ARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset. (arXiv:2203.14057v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14057">
<div class="article-summary-box-inner">
<span><p>We present FaceVerse, a fine-grained 3D Neural Face Model, which is built
from hybrid East Asian face datasets containing 60K fused RGB-D images and 2K
high-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed
to take better advantage of our hybrid dataset. In the coarse module, we
generate a base parametric model from large-scale RGB-D images, which is able
to predict accurate rough 3D face models in different genders, ages, etc. Then
in the fine module, a conditional StyleGAN architecture trained with
high-fidelity scan models is introduced to enrich elaborate facial geometric
and texture details. Note that different from previous methods, our base and
detailed modules are both changeable, which enables an innovative application
of adjusting both the basic attributes and the facial details of 3D face
models. Furthermore, we propose a single-image fitting framework based on
differentiable rendering. Rich experiments show that our method outperforms the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UV Volumes for Real-time Rendering of Editable Free-view Human Performance. (arXiv:2203.14402v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14402">
<div class="article-summary-box-inner">
<span><p>Neural volume rendering enables photo-realistic renderings of a human
performer in free-view, a critical task in immersive VR/AR applications. But
the practice is severely limited by high computational costs in the rendering
process. To solve this problem, we propose the UV Volumes, a new approach that
can render an editable free-view video of a human performer in realtime. It
separates the high-frequency (i.e., non-smooth) human appearance from the 3D
volume, and encodes them into 2D neural texture stacks (NTS). The smooth UV
volumes allow much smaller and shallower neural networks to obtain densities
and texture coordinates in 3D while capturing detailed appearance in 2D NTS.
For editability, the mapping between the parameterized human model and the
smooth texture coordinates allows us a better generalization on novel poses and
shapes. Furthermore, the use of NTS enables interesting applications, e.g.,
retexturing. Extensive experiments on CMU Panoptic, ZJU Mocap, and H36M
datasets show that our model can render 960 * 540 images in 30FPS on average
with comparable photo-realism to state-of-the-art methods. The project and
supplementary materials are available at https://fanegg.github.io/UV-Volumes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00598">
<div class="article-summary-box-inner">
<span><p>Large pretrained (e.g., "foundation") models exhibit distinct capabilities
depending on the domain of data they are trained on. While these domains are
generic, they may only barely overlap. For example, visual-language models
(VLMs) are trained on Internet-scale image captions, but large language models
(LMs) are further trained on Internet-scale text with no images (e.g.,
spreadsheets, SAT questions, code). As a result, these models store different
forms of commonsense knowledge across different domains. In this work, we show
that this diversity is symbiotic, and can be leveraged through Socratic Models
(SMs): a modular framework in which multiple pretrained models may be composed
zero-shot i.e., via multimodal-informed prompting, to exchange information with
each other and capture new multimodal capabilities, without requiring
finetuning. With minimal engineering, SMs are not only competitive with
state-of-the-art zero-shot image captioning and video-to-text retrieval, but
also enable new applications such as (i) answering free-form questions about
egocentric video, (ii) engaging in multimodal assistive dialogue with people
(e.g., for cooking recipes) by interfacing with external APIs and databases
(e.g., web search), and (iii) robot perception and planning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E^2TAD: An Energy-Efficient Tracking-based Action Detector. (arXiv:2204.04416v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04416">
<div class="article-summary-box-inner">
<span><p>Video action detection (spatio-temporal action localization) is usually the
starting point for human-centric intelligent analysis of videos nowadays. It
has high practical impacts for many applications across robotics, security,
healthcare, etc. The two-stage paradigm of Faster R-CNN inspires a standard
paradigm of video action detection in object detection, i.e., firstly
generating person proposals and then classifying their actions. However, none
of the existing solutions could provide fine-grained action detection to the
"who-when-where-what" level. This paper presents a tracking-based solution to
accurately and efficiently localize predefined key actions spatially (by
predicting the associated target IDs and locations) and temporally (by
predicting the time in exact frame indices). This solution won first place in
the UAV-Video Track of 2021 Low-Power Computer Vision Challenge (LPCVC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Multi-grid Methods for Minimizing Curvature Energy. (arXiv:2204.07921v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07921">
<div class="article-summary-box-inner">
<span><p>The geometric high-order regularization methods such as mean curvature and
Gaussian curvature, have been intensively studied during the last decades due
to their abilities in preserving geometric properties including image edges,
corners, and image contrast. However, the dilemma between restoration quality
and computational efficiency is an essential roadblock for high-order methods.
In this paper, we propose fast multi-grid algorithms for minimizing both mean
curvature and Gaussian curvature energy functionals without sacrificing the
accuracy for efficiency. Unlike the existing approaches based on operator
splitting and the Augmented Lagrangian method (ALM), no artificial parameters
are introduced in our formulation, which guarantees the robustness of the
proposed algorithm. Meanwhile, we adopt the domain decomposition method to
promote parallel computing and use the fine-to-coarse structure to accelerate
the convergence. Numerical experiments are presented on both image denoising
and CT reconstruction problem to demonstrate the ability to recover image
texture and the efficiency of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast. (arXiv:2204.14057v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14057">
<div class="article-summary-box-inner">
<span><p>We present an approach to learn voice-face representations from the talking
face videos, without any identity labels. Previous works employ cross-modal
instance discrimination tasks to establish the correlation of voice and face.
These methods neglect the semantic content of different videos, introducing
false-negative pairs as training noise. Furthermore, the positive pairs are
constructed based on the natural correlation between audio clips and visual
frames. However, this correlation might be weak or inaccurate in a large amount
of real-world data, which leads to deviating positives into the contrastive
paradigm. To address these issues, we propose the cross-modal prototype
contrastive learning (CMPC), which takes advantage of contrastive methods and
resists adverse effects of false negatives and deviate positives. On one hand,
CMPC could learn the intra-class invariance by constructing semantic-wise
positives via unsupervised clustering in different modalities. On the other
hand, by comparing the similarities of cross-modal instances from that of
cross-modal prototypes, we dynamically recalibrate the unlearnable instances'
contribution to overall loss. Experiments show that the proposed approach
outperforms state-of-the-art unsupervised methods on various voice-face
association evaluation protocols. Additionally, in the low-shot supervision
setting, our method also has a significant improvement compared to previous
instance-wise contrastive learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers. (arXiv:2204.14217v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14217">
<div class="article-summary-box-inner">
<span><p>The development of the transformer-based text-to-image models are impeded by
its slow generation and complexity for high-resolution images. In this work, we
put forward a solution based on hierarchical transformers and local parallel
auto-regressive generation. We pretrain a 6B-parameter transformer with a
simple and flexible self-supervised task, Cross-modal general language model
(CogLM), and finetune it for fast super-resolution. The new text-to-image
system, CogView2, shows very competitive generation compared to concurrent
state-of-the-art DALL-E-2, and naturally supports interactive text-guided
editing on images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations. (arXiv:2205.05167v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05167">
<div class="article-summary-box-inner">
<span><p>Recent neural network architectures have claimed to explain data from the
human visual cortex. Their demonstrated performance is however still limited by
the dependence on exploiting low-level features for solving visual tasks. This
strategy limits their performance in case of out-of-distribution/adversarial
data. Humans, meanwhile learn abstract concepts and are mostly unaffected by
even extreme image distortions. Humans and networks employ strikingly different
strategies to solve visual tasks. To probe this, we introduce a novel set of
image transforms and evaluate humans and networks on an object recognition
task. We found performance for a few common networks quickly decreases while
humans are able to recognize objects with a high accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Teaching Independent Parts Separately" (TIPSy-GAN) : Improving Accuracy and Stability in Unsupervised Adversarial 2D to 3D Pose Estimation. (arXiv:2205.05980v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05980">
<div class="article-summary-box-inner">
<span><p>We present TIPSy-GAN, a new approach to improve the accuracy and stability in
unsupervised adversarial 2D to 3D human pose estimation. In our work we
demonstrate that the human kinematic skeleton should not be assumed as a single
spatially codependent structure; in fact, we posit when a full 2D pose is
provided during training, there is an inherent bias learned where the 3D
coordinate of a keypoint is spatially codependent on the 2D coordinates of all
other keypoints. To investigate our hypothesis we follow previous adversarial
approaches but train two generators on spatially independent parts of the
kinematic skeleton, the torso and the legs. We find that improving the
self-consistency cycle is key to lowering the evaluation error and therefore
introduce new consistency constraints during training. A TIPSy model is
produced via knowledge distillation from these generators which can predict the
3D ordinates for the entire 2D pose with improved results. Furthermore, we
address an unanswered question in prior work of how long to train in a truly
unsupervised scenario. We show that for two independent generators training
adversarially has improved stability than that of a solo generator which
collapses. TIPSy decreases the average error by 17\% when compared to that of a
baseline solo generator on the Human3.6M dataset. TIPSy improves upon other
unsupervised approaches while also performing strongly against supervised and
weakly-supervised approaches during evaluation on both the Human3.6M and
MPI-INF-3DHP datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Attention Composition for Temporal Action Localization. (arXiv:2205.09956v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09956">
<div class="article-summary-box-inner">
<span><p>Temporal action localization aims at localizing action instances from
untrimmed videos. Existing works have designed various effective modules to
precisely localize action instances based on appearance and motion features.
However, by treating these two kinds of features with equal importance,
previous works cannot take full advantage of each modality feature, making the
learned model still sub-optimal. To tackle this issue, we make an early effort
to study temporal action localization from the perspective of multi-modality
feature learning, based on the observation that different actions exhibit
specific preferences to appearance or motion modality. Specifically, we build a
novel structured attention composition module. Unlike conventional attention,
the proposed module would not infer frame attention and modality attention
independently. Instead, by casting the relationship between the modality
attention and the frame attention as an attention assignment process, the
structured attention composition module learns to encode the frame-modality
structure and uses it to regularize the inferred frame attention and modality
attention, respectively, upon the optimal transport theory. The final
frame-modality attention is obtained by the composition of the two individual
attentions. The proposed structured attention composition module can be
deployed as a plug-and-play module into existing action localization
frameworks. Extensive experiments on two widely used benchmarks show that the
proposed structured attention composition consistently improves four
state-of-the-art temporal action localization methods and builds new
state-of-the-art performance on THUMOS14. Code is availabel at
https://github.com/VividLe/Structured-Attention-Composition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes. (arXiv:2205.10337v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10337">
<div class="article-summary-box-inner">
<span><p>We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13326">
<div class="article-summary-box-inner">
<span><p>This paper describes the methods submitted for evaluation to the SHREC 2022
track on pothole and crack detection in the road pavement. A total of 7
different runs for the semantic segmentation of the road surface are compared,
6 from the participants plus a baseline method. All methods exploit Deep
Learning techniques and their performance is tested using the same environment
(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic
segmentation image/mask pairs and 797 RGB-D video clips collected with the
latest depth cameras was made available to the participants. The methods are
then evaluated on the 496 image/mask pairs in the validation set, on the 504
pairs in the test set and finally on 8 video clips. The analysis of the results
is based on quantitative metrics for image segmentation and qualitative
analysis of the video clips. The participation and the results show that the
scenario is of great interest and that the use of RGB-D data is still
challenging in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransBoost: Improving the Best ImageNet Performance using Deep Transduction. (arXiv:2205.13331v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13331">
<div class="article-summary-box-inner">
<span><p>This paper deals with deep transductive learning, and proposes TransBoost as
a procedure for fine-tuning any deep neural model to improve its performance on
any (unlabeled) test set provided at training time. TransBoost is inspired by a
large margin principle and is efficient and simple to use. The ImageNet
classification performance is consistently and significantly improved with
TransBoost on many architectures such as ResNets, MobileNetV3-L,
EfficientNetB0, ViT-S, and ConvNext-T. Additionally we show that TransBoost is
effective on a wide variety of image classification datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing the Dark Secrets of Masked Image Modeling. (arXiv:2205.13543v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13543">
<div class="article-summary-box-inner">
<span><p>Masked image modeling (MIM) as pre-training is shown to be effective for
numerous vision downstream tasks, but how and where MIM works remain unclear.
In this paper, we compare MIM with the long-dominant supervised pre-trained
models from two perspectives, the visualizations and the experiments, to
uncover their key representational differences. From the visualizations, we
find that MIM brings locality inductive bias to all layers of the trained
models, but supervised models tend to focus locally at lower layers but more
globally at higher layers. That may be the reason why MIM helps Vision
Transformers that have a very large receptive field to optimize. Using MIM, the
model can maintain a large diversity on attention heads in all layers. But for
supervised models, the diversity on attention heads almost disappears from the
last three layers and less diversity harms the fine-tuning performance. From
the experiments, we find that MIM models can perform significantly better on
geometric and motion tasks with weak semantics or fine-grained classification
tasks, than their supervised counterparts. Without bells and whistles, a
standard MIM pre-trained SwinV2-L could achieve state-of-the-art performance on
pose estimation (78.9 AP on COCO test-dev and 78.0 AP on CrowdPose), depth
estimation (0.287 RMSE on NYUv2 and 1.966 RMSE on KITTI), and video object
tracking (70.7 SUC on LaSOT). For the semantic understanding datasets where the
categories are sufficiently covered by the supervised pre-training, MIM models
can still achieve highly competitive transfer performance. With a deeper
understanding of MIM, we hope that our work can inspire new and solid research
in this direction.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-30 23:09:15.250189516 UTC">2022-05-30 23:09:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>