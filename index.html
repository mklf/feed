<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-29T01:30:00Z">04-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Explanations for Natural Language Interfaces. (arXiv:2204.13192v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13192">
<div class="article-summary-box-inner">
<span><p>A key challenge facing natural language interfaces is enabling users to
understand the capabilities of the underlying system. We propose a novel
approach for generating explanations of a natural language interface based on
semantic parsing. We focus on counterfactual explanations, which are post-hoc
explanations that describe to the user how they could have minimally modified
their utterance to achieve their desired goal. In particular, the user provides
an utterance along with a demonstration of their desired goal; then, our
algorithm synthesizes a paraphrase of their utterance that is guaranteed to
achieve their goal. In two user studies, we demonstrate that our approach
substantially improves user performance, and that it generates explanations
that more closely match the user's intent compared to two ablations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data. (arXiv:2204.13243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13243">
<div class="article-summary-box-inner">
<span><p>A pressing challenge in current dialogue systems is to successfully converse
with users on topics with information distributed across different modalities.
Previous work in multiturn dialogue systems has primarily focused on either
text or table information. In more realistic scenarios, having a joint
understanding of both is critical as knowledge is typically distributed over
both unstructured and structured forms. We present a new dialogue dataset,
HybriDialogue, which consists of crowdsourced natural conversations grounded on
both Wikipedia text and tables. The conversations are created through the
decomposition of complex multihop questions into simple, realistic multiturn
dialogue interactions. We propose retrieval, system state tracking, and
dialogue response generation tasks for our dataset and conduct baseline
experiments for each. Our results show that there is still ample opportunity
for improvement, demonstrating the importance of building stronger dialogue
systems that can reason over the complex setting of information-seeking
dialogue grounded on tables and text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Memory Networks for Radiology Report Generation. (arXiv:2204.13258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13258">
<div class="article-summary-box-inner">
<span><p>Medical imaging plays a significant role in clinical practice of medical
diagnosis, where the text reports of the images are essential in understanding
them and facilitating later treatments. By generating the reports
automatically, it is beneficial to help lighten the burden of radiologists and
significantly promote clinical automation, which already attracts much
attention in applying artificial intelligence to medical domain. Previous
studies mainly follow the encoder-decoder paradigm and focus on the aspect of
text generation, with few studies considering the importance of cross-modal
mappings and explicitly exploit such mappings to facilitate radiology report
generation. In this paper, we propose a cross-modal memory networks (CMN) to
enhance the encoder-decoder framework for radiology report generation, where a
shared memory is designed to record the alignment between images and texts so
as to facilitate the interaction and generation across modalities. Experimental
results illustrate the effectiveness of our proposed model, where
state-of-the-art performance is achieved on two widely used benchmark datasets,
i.e., IU X-Ray and MIMIC-CXR. Further analyses also prove that our model is
able to better align information from radiology images and texts so as to help
generating more accurate reports in terms of clinical indicators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving robustness of language models from a geometry-aware perspective. (arXiv:2204.13309v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13309">
<div class="article-summary-box-inner">
<span><p>Recent studies have found that removing the norm-bounded projection and
increasing search steps in adversarial training can significantly improve
robustness. However, we observe that a too large number of search steps can
hurt accuracy. We aim to obtain strong robustness efficiently using fewer
steps. Through a toy experiment, we find that perturbing the clean data to the
decision boundary but not crossing it does not degrade the test accuracy.
Inspired by this, we propose friendly adversarial data augmentation (FADA) to
generate friendly adversarial data. On top of FADA, we propose geometry-aware
adversarial training (GAT) to perform adversarial training on friendly
adversarial data so that we can save a large number of search steps.
Comprehensive experiments across two widely used datasets and three pre-trained
language models demonstrate that GAT can obtain stronger robustness via fewer
steps. In addition, we provide extensive empirical results and in-depth
analyses on robustness to facilitate future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Copenhagen Corpus of Eye Tracking Recordings from Natural Reading of Danish Texts. (arXiv:2204.13311v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13311">
<div class="article-summary-box-inner">
<span><p>Eye movement recordings from reading are one of the richest signals of human
language processing. Corpora of eye movements during reading of contextualized
running text is a way of making such records available for natural language
processing purposes. Such corpora already exist in some languages. We present
CopCo, the Copenhagen Corpus of eye tracking recordings from natural reading of
Danish texts. It is the first eye tracking corpus of its kind for the Danish
language. CopCo includes 1,832 sentences with 34,897 tokens of Danish text
extracted from a collection of speech manuscripts. This first release of the
corpus contains eye tracking data from 22 participants. It will be extended
continuously with more participants and texts from other genres. We assess the
data quality of the recorded eye movements and find that the extracted features
are in line with related research. The dataset available here:
https://osf.io/ud8s5/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniTE: Unified Translation Evaluation. (arXiv:2204.13346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13346">
<div class="article-summary-box-inner">
<span><p>Translation quality evaluation plays a crucial role in machine translation.
According to the input format, it is mainly separated into three tasks, i.e.,
reference-only, source-only and source-reference-combined. Recent methods,
despite their promising results, are specifically designed and optimized on one
of them. This limits the convenience of these methods, and overlooks the
commonalities among tasks. In this paper, we propose UniTE, which is the first
unified framework engaged with abilities to handle all three evaluation tasks.
Concretely, we propose monotonic regional attention to control the interaction
among input segments, and unified pretraining to better adapt multi-task
learning. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality
Estimation benchmarks. Extensive analyses show that our \textit{single model}
can universally surpass various state-of-the-art or winner methods across
tasks. Both source code and associated models are available at
https://github.com/NLP2CT/UniTE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoBLEURT Submission for the WMT2021 Metrics Task. (arXiv:2204.13352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13352">
<div class="article-summary-box-inner">
<span><p>In this paper, we present our submission to Shared Metrics Task: RoBLEURT
(Robustly Optimizing the training of BLEURT). After investigating the recent
advances of trainable metrics, we conclude several aspects of vital importance
to obtain a well-performed metric model by: 1) jointly leveraging the
advantages of source-included model and reference-only model, 2) continuously
pre-training the model with massive synthetic data pairs, and 3) fine-tuning
the model with data denoising strategy. Experimental results show that our
model reaching state-of-the-art correlations with the WMT2020 human annotations
upon 8 out of 10 to-English language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Mechanism with Energy-Friendly Operations. (arXiv:2204.13353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13353">
<div class="article-summary-box-inner">
<span><p>Attention mechanism has become the dominant module in natural language
processing models. It is computationally intensive and depends on massive
power-hungry multiplications. In this paper, we rethink variants of attention
mechanism from the energy consumption aspects. After reaching the conclusion
that the energy costs of several energy-friendly operations are far less than
their multiplication counterparts, we build a novel attention model by
replacing multiplications with either selective operations or additions.
Empirical results on three machine translation tasks demonstrate that the
proposed model, against the vanilla one, achieves competitable accuracy while
saving 99\% and 66\% energy during alignment calculation and the whole
attention procedure. Code is available at: https://github.com/NLP2CT/E-Att.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints. (arXiv:2204.13355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13355">
<div class="article-summary-box-inner">
<span><p>However, current autoregressive approaches suffer from high latency. In this
paper, we focus on non-autoregressive translation (NAT) for this problem for
its efficiency advantage. We identify that current constrained NAT models,
which are based on iterative editing, do not handle low-frequency constraints
well. To this end, we propose a plug-in algorithm for this line of work, i.e.,
Aligned Constrained Training (ACT), which alleviates this problem by
familiarizing the model with the source-side context of the constraints.
Experiments on the general and domain datasets show that our model improves
over the backbone constrained NAT model in constraint preservation and
translation quality, especially for rare constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation. (arXiv:2204.13362v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13362">
<div class="article-summary-box-inner">
<span><p>Attribute-based Controlled Text Generation (CTG) refers to generating
sentences that satisfy desirable attributes (e.g., emotions and topics).
Existing works often utilize fine-tuning or resort to extra attribute
classifiers, yet suffer from storage and inference time increases. To address
these concerns, we explore attribute-based CTG in a prompt-based manner. In
short, the proposed Tailor represents each attribute as a pre-trained
continuous vector (i.e., single-attribute prompt) and guides the generation of
a fixed PLM switch to a pre-specified attribute. We experimentally find that
these prompts can be simply concatenated as a whole to multi-attribute CTG
without any re-training, yet raises problems of fluency decrease and position
sensitivity. To this end, Tailor provides a multi-attribute prompt mask and a
re-indexing position-ids sequence to bridge the gap between the training (one
prompt for each task) and testing stage (concatenating more than one prompt).
To further enhance such single-attribute prompt combinations, Tailor also
introduces a trainable prompt connector, which can be concatenated with any two
single-attribute prompts to multi-attribute text generation. Experiments on 11
attribute-specific generation tasks demonstrate strong performances of Tailor
on both single-attribute and multi-attribute CTG, with 0.08\% training
parameters of a GPT-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D3: A Massive Dataset of Scholarly Metadata for Analyzing the State of Computer Science Research. (arXiv:2204.13384v1 [cs.DL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13384">
<div class="article-summary-box-inner">
<span><p>DBLP is the largest open-access repository of scientific articles on computer
science and provides metadata associated with publications, authors, and
venues. We retrieved more than 6 million publications from DBLP and extracted
pertinent metadata (e.g., abstracts, author affiliations, citations) from the
publication texts to create the DBLP Discovery Dataset (D3). D3 can be used to
identify trends in research activity, productivity, focus, bias, accessibility,
and impact of computer science research. We present an initial analysis focused
on the volume of computer science research (e.g., number of papers, authors,
research activity), trends in topics of interest, and citation patterns. Our
findings show that computer science is a growing research field (approx. 15%
annually), with an active and collaborative researcher community. While papers
in recent years present more bibliographical entries in comparison to previous
decades, the average number of citations has been declining. Investigating
papers' abstracts reveals that recent topic trends are clearly reflected in D3.
Finally, we list further applications of D3 and pose supplemental research
questions. The D3 dataset, our findings, and source code are publicly available
for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Placing M-Phasis on the Plurality of Hate: A Feature-Based Corpus of Hate Online. (arXiv:2204.13400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13400">
<div class="article-summary-box-inner">
<span><p>Even though hate speech (HS) online has been an important object of research
in the last decade, most HS-related corpora over-simplify the phenomenon of
hate by attempting to label user comments as "hate" or "neutral". This ignores
the complex and subjective nature of HS, which limits the real-life
applicability of classifiers trained on these corpora. In this study, we
present the M-Phasis corpus, a corpus of ~9k German and French user comments
collected from migration-related news articles. It goes beyond the
"hate"-"neutral" dichotomy and is instead annotated with 23 features, which in
combination become descriptors of various types of speech, ranging from
critical comments to implicit and explicit expressions of hate. The annotations
are performed by 4 native speakers per language and achieve high (0.77 &lt;= k &lt;=
1) inter-annotator agreements. Besides describing the corpus creation and
presenting insights from a content, error and domain analysis, we explore its
data characteristics by training several classification baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeaNF: Weak Supervision with Normalizing Flows. (arXiv:2204.13409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13409">
<div class="article-summary-box-inner">
<span><p>A popular approach to decrease the need for costly manual annotation of large
data sets is weak supervision, which introduces problems of noisy labels,
coverage and bias. Methods for overcoming these problems have either relied on
discriminative models, trained with cost functions specific to weak
supervision, and more recently, generative models, trying to model the output
of the automatic annotation process. In this work, we explore a novel direction
of generative modeling for weak supervision: Instead of modeling the output of
the annotation process (the labeling function matches), we generatively model
the input-side data distributions (the feature space) covered by labeling
functions. Specifically, we estimate a density for each weak labeling source,
or labeling function, by using normalizing flows. An integral part of our
method is the flow-based modeling of multiple simultaneously matching labeling
functions, and therefore phenomena such as labeling function overlap and
correlations are captured. We analyze the effectiveness and modeling
capabilities on various commonly used weak supervision data sets, and show that
weakly supervised normalizing flows compare favorably to standard weak
supervision baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification. (arXiv:2204.13413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13413">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex label hierarchy. Recently, the
pretrained language models (PLM) have been widely adopted in HTC through a
fine-tuning paradigm. However, in this paradigm, there exists a huge gap
between the classification tasks with sophisticated label hierarchy and the
masked language model (MLM) pretraining tasks of PLMs and thus the potentials
of PLMs can not be fully tapped. To bridge the gap, in this paper, we propose
HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label
MLM perspective. Specifically, we construct dynamic virtual template and label
words which take the form of soft prompts to fuse the label hierarchy knowledge
and introduce a zero-bounded multi-label cross entropy loss to harmonize the
objectives of HTC and MLM. Extensive experiments show HPT achieves the
state-of-the-art performances on 3 popular HTC datasets and is adept at
handling the imbalance and low resource situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simplifying Multilingual News Clustering Through Projection From a Shared Space. (arXiv:2204.13418v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13418">
<div class="article-summary-box-inner">
<span><p>The task of organizing and clustering multilingual news articles for media
monitoring is essential to follow news stories in real time. Most approaches to
this task focus on high-resource languages (mostly English), with low-resource
languages being disregarded. With that in mind, we present a much simpler
online system that is able to cluster an incoming stream of documents without
depending on language-specific features. We empirically demonstrate that the
use of multilingual contextual embeddings as the document representation
significantly improves clustering quality. We challenge previous crosslingual
approaches by removing the precondition of building monolingual clusters. We
model the clustering process as a set of linear classifiers to aggregate
similar documents, and correct closely-related multilingual clusters through
merging in an online fashion. Our system achieves state-of-the-art results on a
multilingual news stream clustering dataset, and we introduce a new evaluation
for zero-shot news clustering in multiple languages. We make our code available
as open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVI: Multilingual Spoken Dialogue Tasks and Dataset for Knowledge-Based Enrolment, Verification, and Identification. (arXiv:2204.13496v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13496">
<div class="article-summary-box-inner">
<span><p>Knowledge-based authentication is crucial for task-oriented spoken dialogue
systems that offer personalised and privacy-focused services. Such systems
should be able to enrol (E), verify (V), and identify (I) new and recurring
users based on their personal information, e.g. postcode, name, and date of
birth. In this work, we formalise the three authentication tasks and their
evaluation protocols, and we present EVI, a challenging spoken multilingual
dataset with 5,506 dialogues in English, Polish, and French. Our proposed
models set the first competitive benchmarks, explore the challenges of
multilingual natural language processing of spoken dialogue, and set directions
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-Training Dialogue Summarization using Pseudo-Paraphrasing. (arXiv:2204.13498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13498">
<div class="article-summary-box-inner">
<span><p>Previous dialogue summarization techniques adapt large language models
pretrained on the narrative text by injecting dialogue-specific features into
the models. These features either require additional knowledge to recognize or
make the resulting models harder to tune. To bridge the format gap between
dialogues and narrative summaries in dialogue summarization tasks, we propose
to post-train pretrained language models (PLMs) to rephrase from dialogue to
narratives. After that, the model is fine-tuned for dialogue summarization as
usual. Comprehensive experiments show that our approach significantly improves
vanilla PLMs on dialogue summarization and outperforms other SOTA models by the
summary quality and implementation costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model. (arXiv:2204.13509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13509">
<div class="article-summary-box-inner">
<span><p>Many recent studies on large-scale language models have reported successful
in-context zero- and few-shot learning ability. However, the in-depth analysis
of when in-context learning occurs is still lacking. For example, it is unknown
how in-context learning performance changes as the training corpus varies.
Here, we investigate the effects of the source and size of the pretraining
corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From
our in-depth investigation, we introduce the following observations: (1)
in-context learning performance heavily depends on the corpus domain source,
and the size of the pretraining corpus does not necessarily determine the
emergence of in-context learning, (2) in-context learning ability can emerge
when a language model is trained on a combination of multiple corpora, even
when each corpus does not result in in-context learning on its own, (3)
pretraining with a corpus related to a downstream task does not always
guarantee the competitive in-context learning performance of the downstream
task, especially in the few-shot setting, and (4) the relationship between
language modeling (measured in perplexity) and in-context learning does not
always correlate: e.g., low perplexity does not always imply high in-context
few-shot learning performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobBERTje: a Distilled Dutch BERT Model. (arXiv:2204.13511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13511">
<div class="article-summary-box-inner">
<span><p>Pre-trained large-scale language models such as BERT have gained a lot of
attention thanks to their outstanding performance on a wide range of natural
language tasks. However, due to their large number of parameters, they are
resource-intensive both to deploy and to fine-tune. Researchers have created
several methods for distilling language models into smaller ones to increase
efficiency, with a small performance trade-off. In this paper, we create
several different distilled versions of the state-of-the-art Dutch RobBERT
model and call them RobBERTje. The distillations differ in their distillation
corpus, namely whether or not they are shuffled and whether they are merged
with subsequent sentences. We found that the performance of the models using
the shuffled versus non-shuffled datasets is similar for most tasks and that
randomly merging subsequent sentences in a corpus creates models that train
faster and perform better on tasks with long sequences. Upon comparing
distillation architectures, we found that the larger DistilBERT architecture
worked significantly better than the Bort hyperparametrization. Interestingly,
we also found that the distilled models exhibit less gender-stereotypical bias
than its teacher model. Since smaller architectures decrease the time to
fine-tune, these models allow for more efficient training and more lightweight
deployment of many Dutch downstream language tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization. (arXiv:2204.13512v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13512">
<div class="article-summary-box-inner">
<span><p>In zero-shot multilingual extractive text summarization, a model is typically
trained on English summarization dataset and then applied on summarization
datasets of other languages. Given English gold summaries and documents,
sentence-level labels for extractive summarization are usually generated using
heuristics. However, these monolingual labels created on English datasets may
not be optimal on datasets of other languages, for that there is the syntactic
or semantic discrepancy between different languages. In this way, it is
possible to translate the English dataset to other languages and obtain
different sets of labels again using heuristics. To fully leverage the
information of these different sets of labels, we propose NLSSum (Neural Label
Search for Summarization), which jointly learns hierarchical weights for these
different sets of labels together with our summarization model. We conduct
multilingual zero-shot summarization experiments on MLSUM and WikiLingua
datasets, and we achieve state-of-the-art results using both human and
automatic evaluations across these two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UM6P-CS at SemEval-2022 Task 11: Enhancing Multilingual and Code-Mixed Complex Named Entity Recognition via Pseudo Labels using Multilingual Transformer. (arXiv:2204.13515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13515">
<div class="article-summary-box-inner">
<span><p>Building real-world complex Named Entity Recognition (NER) systems is a
challenging task. This is due to the complexity and ambiguity of named entities
that appear in various contexts such as short input sentences, emerging
entities, and complex entities. Besides, real-world queries are mostly
malformed, as they can be code-mixed or multilingual, among other scenarios. In
this paper, we introduce our submitted system to the Multilingual Complex Named
Entity Recognition (MultiCoNER) shared task. We approach the complex NER for
multilingual and code-mixed queries, by relying on the contextualized
representation provided by the multilingual Transformer XLM-RoBERTa. In
addition to the CRF-based token classification layer, we incorporate a span
classification loss to recognize named entities spans. Furthermore, we use a
self-training mechanism to generate weakly-annotated data from a large
unlabeled dataset. Our proposed system is ranked 6th and 8th in the
multilingual and code-mixed MultiCoNER's tracks respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification. (arXiv:2204.13516v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13516">
<div class="article-summary-box-inner">
<span><p>Over the last five years, research on Relation Extraction (RE) witnessed
extensive progress with many new dataset releases. At the same time, setup
clarity has decreased, contributing to increased difficulty of reliable
empirical evaluation (Taill\'e et al., 2020). In this paper, we provide a
comprehensive survey of RE datasets, and revisit the task definition and its
adoption by the community. We find that cross-dataset and cross-domain setups
are particularly lacking. We present an empirical study on scientific Relation
Classification across two datasets. Despite large data overlap, our analysis
reveals substantial discrepancies in annotation. Annotation discrepancies
strongly impact Relation Classification performance, explaining large drops in
cross-dataset evaluations. Variation within further sub-domains exists but
impacts Relation Classification only to limited degrees. Overall, our study
calls for more rigour in reporting setups in RE and evaluation across multiple
test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning for Violence Risk Assessment Using Dutch Clinical Notes. (arXiv:2204.13535v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13535">
<div class="article-summary-box-inner">
<span><p>Violence risk assessment in psychiatric institutions enables interventions to
avoid violence incidents. Clinical notes written by practitioners and available
in electronic health records are valuable resources capturing unique
information, but are seldom used to their full potential. We explore
conventional and deep machine learning methods to assess violence risk in
psychiatric patients using practitioner notes. The performance of our best
models is comparable to the currently used questionnaire-based method, with an
area under the Receiver Operating Characteristic curve of approximately 0.8. We
find that the deep-learning model BERTje performs worse than conventional
machine learning methods. We also evaluate our data and our classifiers to
understand the performance of our models better. This is particularly important
for the applicability of evaluated classifiers to new data, and is also of
great interest to practitioners, due to the increased availability of new data
in electronic format.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Life is not Always Depressing: Exploring the Happy Moments of People Diagnosed with Depression. (arXiv:2204.13569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13569">
<div class="article-summary-box-inner">
<span><p>In this work, we explore the relationship between depression and
manifestations of happiness in social media. While the majority of works
surrounding depression focus on symptoms, psychological research shows that
there is a strong link between seeking happiness and being diagnosed with
depression. We make use of Positive-Unlabeled learning paradigm to
automatically extract happy moments from social media posts of both controls
and users diagnosed with depression, and qualitatively analyze them with
linguistic tools such as LIWC and keyness information. We show that the life of
depressed individuals is not always bleak, with positive events related to
friends and family being more noteworthy to their lives compared to the more
mundane happy events reported by control users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeSHup: A Corpus for Full Text Biomedical Document Indexing. (arXiv:2204.13604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13604">
<div class="article-summary-box-inner">
<span><p>Medical Subject Heading (MeSH) indexing refers to the problem of assigning a
given biomedical document with the most relevant labels from an extremely large
set of MeSH terms. Currently, the vast number of biomedical articles in the
PubMed database are manually annotated by human curators, which is time
consuming and costly; therefore, a computational system that can assist the
indexing is highly valuable. When developing supervised MeSH indexing systems,
the availability of a large-scale annotated text corpus is desirable. A
publicly available, large corpus that permits robust evaluation and comparison
of various systems is important to the research community. We release a large
scale annotated MeSH indexing corpus, MeSHup, which contains 1,342,667 full
text articles in English, together with the associated MeSH labels and
metadata, authors, and publication venues that are collected from the MEDLINE
database. We train an end-to-end model that combines features from documents
and their associated labels on our corpus and report the new baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Russian Texts Detoxification with Levenshtein Editing. (arXiv:2204.13638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13638">
<div class="article-summary-box-inner">
<span><p>Text detoxification is a style transfer task of creating neutral versions of
toxic texts. In this paper, we use the concept of text editing to build a
two-step tagging-based detoxification model using a parallel corpus of Russian
texts. With this model, we achieved the best style transfer accuracy among all
models in the RUSSE Detox shared task, surpassing larger sequence-to-sequence
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures. (arXiv:2204.13692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13692">
<div class="article-summary-box-inner">
<span><p>Being able to rank the similarity of short text segments is an interesting
bonus feature of neural machine translation. Translation-based similarity
measures include direct and pivot translation probability, as well as
translation cross-likelihood, which has not been studied so far. We analyze
these measures in the common framework of multilingual NMT, releasing the
NMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared
to baselines such as sentence embeddings, translation-based measures prove
competitive in paraphrase identification and are more robust against
adversarial or multilingual input, especially if proper normalization is
applied. When used for reference-based evaluation of data-to-text generation in
2 tasks and 17 languages, translation-based measures show a relatively high
correlation to human judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Radiology Reports via Memory-driven Transformer. (arXiv:2010.16056v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16056">
<div class="article-summary-box-inner">
<span><p>Medical imaging is frequently used in clinical practice and trials for
diagnosis and treatment. Writing imaging reports is time-consuming and can be
error-prone for inexperienced radiologists. Therefore, automatically generating
radiology reports is highly desired to lighten the workload of radiologists and
accordingly promote clinical automation, which is an essential task to apply
artificial intelligence to the medical domain. In this paper, we propose to
generate radiology reports with memory-driven Transformer, where a relational
memory is designed to record key information of the generation process and a
memory-driven conditional layer normalization is applied to incorporating the
memory into the decoder of Transformer. Experimental results on two prevailing
radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed
approach outperforms previous models with respect to both language generation
metrics and clinical evaluations. Particularly, this is the first work
reporting the generation results on MIMIC-CXR to the best of our knowledge.
Further analyses also demonstrate that our approach is able to generate long
reports with necessary medical terms as well as meaningful image-text attention
mappings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleKQC: A Style-Variant Paraphrase Corpus for Korean Questions and Commands. (arXiv:2103.13439v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13439">
<div class="article-summary-box-inner">
<span><p>Paraphrasing is often performed with less concern for controlled style
conversion. Especially for questions and commands, style-variant paraphrasing
can be crucial in tone and manner, which also matters with industrial
applications such as dialog systems. In this paper, we attack this issue with a
corpus construction scheme that simultaneously considers the core content and
style of directives, namely intent and formality, for the Korean language.
Utilizing manually generated natural language queries on six daily topics, we
expand the corpus to formal and informal sentences by human rewriting and
transferring. We verify the validity and industrial applicability of our
approach by checking the adequate classification and inference performance that
fit with conventional fine-tuning approaches, at the same time proposing a
supervised formality transfer task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistency Training with Virtual Adversarial Discrete Perturbation. (arXiv:2104.07284v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07284">
<div class="article-summary-box-inner">
<span><p>Consistency training regularizes a model by enforcing predictions of original
and perturbed inputs to be similar. Previous studies have proposed various
augmentation methods for the perturbation but are limited in that they are
agnostic to the training model. Thus, the perturbed samples may not aid in
regularization due to their ease of classification from the model. In this
context, we propose an augmentation method of adding a discrete noise that
would incur the highest divergence between predictions. This virtual
adversarial discrete noise obtained by replacing a small portion of tokens
while keeping original semantics as much as possible efficiently pushes a
training model's decision boundary. Experimental results show that our proposed
method outperforms other consistency training baselines with text editing,
paraphrasing, or a continuous noise on semi-supervised text classification
tasks and a robustness benchmark
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Dialogue Summarization: Recent Advances and New Frontiers. (arXiv:2107.03175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03175">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization aims to condense the original dialogue into a shorter
version covering salient information, which is a crucial way to reduce dialogue
data overload. Recently, the promising achievements in both dialogue systems
and natural language generation techniques drastically lead this task to a new
landscape, which results in significant research attentions. However, there
still remains a lack of a comprehensive survey for this task. To this end, we
take the first step and present a thorough review of this research field
carefully and widely. In detail, we systematically organize the current works
according to the characteristics of each domain, covering meeting, chat, email
thread, customer service and medical dialogue. Additionally, we provide an
overview of publicly available research datasets as well as organize two
leaderboards under unified metrics. Furthermore, we discuss some future
directions, including faithfulness, multi-modal, multi-domain and multi-lingual
dialogue summarization, and give our thoughts respectively. We hope that this
first survey of dialogue summarization can provide the community with a quick
access and a general picture to this task and motivate future researches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03587">
<div class="article-summary-box-inner">
<span><p>Sarcasm employs ambivalence, where one says something positive but actually
means negative, and vice versa. The essence of sarcasm, which is also a
sufficient and necessary condition, is the conflict between literal and implied
sentiments expressed in one sentence. However, it is difficult to recognize
such sentiment conflict because the sentiments are mixed or even implicit. As a
result, the recognition of sophisticated and obscure sentiment brings in a
great challenge to sarcasm detection. In this paper, we propose a Dual-Channel
Framework by modeling both literal and implied sentiments separately. Based on
this dual-channel framework, we design the Dual-Channel Network~(DC-Net) to
recognize sentiment conflict. Experiments on political debates (i.e. IAC-V1 and
IAC-V2) and Twitter datasets show that our proposed DC-Net achieves
state-of-the-art performance on sarcasm recognition. Our code is released to
support research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04825">
<div class="article-summary-box-inner">
<span><p>The impressive capabilities of recent generative models to create texts that
are challenging to distinguish from the human-written ones can be misused for
generating fake news, product reviews, and even abusive content. Despite the
prominent performance of existing methods for artificial text detection, they
still lack interpretability and robustness towards unseen models. To this end,
we propose three novel types of interpretable topological features for this
task based on Topological Data Analysis (TDA) which is currently understudied
in the field of NLP. We empirically show that the features derived from the
BERT model outperform count- and neural-based baselines up to 10\% on three
common datasets, and tend to be the most robust towards unseen GPT-style
generation models as opposed to existing methods. The probing analysis of the
features reveals their sensitivity to the surface and syntactic properties. The
results demonstrate that TDA is a promising line with respect to NLP tasks,
specifically the ones that incorporate surface and structural information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01900">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech representation learning methods like wav2vec 2.0 and
Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and
offer good representations for numerous speech processing tasks. Despite the
success of these methods, they require large memory and high pre-training
costs, making them inaccessible for researchers in academia and small
companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task
learning framework to distill hidden representations from a HuBERT model
directly. This method reduces HuBERT's size by 75% and 73% faster while
retaining most performance in ten different tasks. Moreover, DistilHuBERT
required little training time and data, opening the possibilities of
pre-training personal and on-device SSL models for speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning. (arXiv:2111.04198v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.04198">
<div class="article-summary-box-inner">
<span><p>Masked language models (MLMs) such as BERT and RoBERTa have revolutionized
the field of Natural Language Understanding in the past few years. However,
existing pre-trained MLMs often output an anisotropic distribution of token
representations that occupies a narrow subset of the entire representation
space. Such token representations are not ideal, especially for tasks that
demand discriminative semantic meanings of distinct tokens. In this work, we
propose TaCL (Token-aware Contrastive Learning), a novel continual pre-training
approach that encourages BERT to learn an isotropic and discriminative
distribution of token representations. TaCL is fully unsupervised and requires
no additional data. We extensively test our approach on a wide range of English
and Chinese benchmarks. The results show that TaCL brings consistent and
notable improvements over the original BERT model. Furthermore, we conduct
detailed analysis to reveal the merits and inner-workings of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Learning for Unsupervised Knowledge Grounded Dialogs. (arXiv:2112.00653v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00653">
<div class="article-summary-box-inner">
<span><p>Recent methods for knowledge grounded dialogs generate responses by
incorporating information from an external textual document. These methods do
not require the exact document to be known during training and rely on the use
of a retrieval system to fetch relevant documents from a large index. The
documents used to generate the responses are modeled as latent variables whose
prior probabilities need to be estimated. Models such as RAG and REALM,
marginalize the document probabilities over the documents retrieved from the
index to define the log likelihood loss function which is optimized end-to-end.
</p>
<p>In this paper, we develop a variational approach to the above technique
wherein, we instead maximize the Evidence Lower bound (ELBO). Using a
collection of three publicly available open-conversation datasets, we
demonstrate how the posterior distribution, that has information from the
ground-truth response, allows for a better approximation of the objective
function during training. To overcome the challenges associated with sampling
over a large knowledge collection, we develop an efficient approach to
approximate the ELBO. To the best of our knowledge we are the first to apply
variational training for open-scale unsupervised knowledge grounded dialog
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06223">
<div class="article-summary-box-inner">
<span><p>Code-switching is a speech phenomenon occurring when a speaker switches
language during a conversation. Despite the spontaneous nature of
code-switching in conversational spoken language, most existing works collect
code-switching data from read speech instead of spontaneous speech. ASCEND (A
Spontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English
code-switching corpus built on spontaneous multi-turn conversational dialogue
sources collected in Hong Kong. We report ASCEND's design and procedure for
collecting the speech data, including annotations. ASCEND consists of 10.62
hours of clean speech, collected from 23 bilingual speakers of Chinese and
English. Furthermore, we conduct baseline experiments using pre-trained wav2vec
2.0 models, achieving a best performance of 22.69\% character error rate and
27.05% mixed error rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Human Evaluation for Relative Model Comparisons. (arXiv:2112.08048v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08048">
<div class="article-summary-box-inner">
<span><p>Collecting human judgements is currently the most reliable evaluation method
for natural language generation systems. Automatic metrics have reported flaws
when applied to measure quality aspects of generated text and have been shown
to correlate poorly with human judgements. However, human evaluation is time
and cost-intensive, and we lack consensus on designing and conducting human
evaluation experiments. Thus there is a need for streamlined approaches for
efficient collection of human judgements when evaluating natural language
generation systems. Therefore, we present a dynamic approach to measure the
required number of human annotations when evaluating generated outputs in
relative comparison settings. We propose an agent-based framework of human
evaluation to assess multiple labelling strategies and methods to decide the
better model in a simulation and a crowdsourcing case study. The main results
indicate that a decision about the superior model can be made with high
probability across different labelling strategies, where assigning a single
random worker per task requires the least overall labelling effort and thus the
least cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DREAM: Improving Situational QA by First Elaborating the Situation. (arXiv:2112.08656v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08656">
<div class="article-summary-box-inner">
<span><p>When people answer questions about a specific situation, e.g., "I cheated on
my mid-term exam last week. Was that wrong?", cognitive science suggests that
they form a mental picture of that situation before answering. While we do not
know how language models (LMs) answer such questions, we conjecture that they
may answer more accurately if they are also provided with additional details
about the question situation, elaborating the "scene". To test this conjecture,
we train a new model, DREAM, to answer questions that elaborate the scenes that
situated questions are about, and then provide those elaborations as additional
context to a question-answering (QA) model. We find that DREAM is able to
create better scene elaborations (more accurate, useful, and consistent) than a
representative state-of-the-art, zero-shot model (Macaw). We also find that
using the scene elaborations as additional context improves the answer accuracy
of a downstream QA system, including beyond that obtainable by simply further
finetuning the QA system on DREAM's training data. These results suggest that
adding focused elaborations about a situation can improve a system's reasoning
about it, and may serve as an effective way of injecting new scenario based
knowledge into QA models. Finally, our approach is dataset-neutral; we observe
improved QA performance across different models, with even bigger gains on
models with fewer parameters. We make our dataset and model publicly available
at https://github.com/allenai/dream.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Contextual Embeddings and their Extraction Layers for Depression Assessment. (arXiv:2112.13795v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13795">
<div class="article-summary-box-inner">
<span><p>Recent works have demonstrated ability to assess aspects of mental health
from personal discourse. At the same time, pre-trained contextual word
embedding models have grown to dominate much of NLP but little is known
empirically on how to best apply them for mental health assessment. Using
degree of depression as a case study, we do an empirical analysis on which
off-the-shelf language model, individual layers, and combinations of layers
seem most promising when applied to human-level NLP tasks. Notably, we find
RoBERTa most effective and, despite the standard in past work suggesting the
second-to-last or concatenation of the last 4 layers, we find layer 19
(sixth-to last) is at least as good as layer 23 when using 1 layer. Further,
when using multiple layers, distributing them across the second half (i.e.
Layers 12+), rather than last 4, of the 24 layers yielded the most accurate
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Pre-training Language Model for Semantic Network Completion. (arXiv:2201.04843v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04843">
<div class="article-summary-box-inner">
<span><p>Semantic networks, such as the knowledge graph, can represent the knowledge
leveraging the graph structure. Although the knowledge graph shows promising
values in natural language processing, it suffers from incompleteness. This
paper focuses on knowledge graph completion by predicting linkage between
entities, which is a fundamental yet critical task. Semantic matching is a
potential solution as it can deal with unseen entities, which the translational
distance based methods struggle with. However, to achieve competitive
performance as translational distance based methods, semantic matching based
methods require large-scale datasets for the training purpose, which are
typically unavailable in practical settings. Therefore, we employ the language
model and introduce a novel knowledge graph architecture named LP-BERT, which
contains two main stages: multi-task pre-training and knowledge graph
fine-tuning. In the pre-training phase, three tasks are taken to drive the
model to learn the relationship from triples by predicting either entities or
relations. While in the fine-tuning phase, inspired by contrastive learning, we
design a triple-style negative sampling in a batch, which greatly increases the
proportion of negative sampling while keeping the training time almost
unchanged. Furthermore, we propose a new data augmentation method utilizing the
inverse relationship of triples to improve the performance and robustness of
the model. To demonstrate the effectiveness of our method, we conduct extensive
experiments on three widely-used datasets, WN18RR, FB15k-237, and UMLS. The
experimental results demonstrate the superiority of our methods, and our
approach achieves state-of-the-art results on WN18RR and FB15k-237 datasets.
Significantly, Hits@10 indicator is improved by 5% from previous
state-of-the-art result on the WN18RR dataset while reaching 100% on the UMLS
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NorDiaChange: Diachronic Semantic Change Dataset for Norwegian. (arXiv:2201.05123v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05123">
<div class="article-summary-box-inner">
<span><p>We describe NorDiaChange: the first diachronic semantic change dataset for
Norwegian. NorDiaChange comprises two novel subsets, covering about 80
Norwegian nouns manually annotated with graded semantic change over time. Both
datasets follow the same annotation procedure and can be used interchangeably
as train and test splits for each other. NorDiaChange covers the time periods
related to pre- and post-war events, oil and gas discovery in Norway, and
technological developments. The annotation was done using the DURel framework
and two large historical Norwegian corpora. NorDiaChange is published in full
under a permissive licence, complete with raw annotation data and inferred
diachronic word usage graphs (DWUGs).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pruning Learns Compact and Accurate Models. (arXiv:2204.00408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00408">
<div class="article-summary-box-inner">
<span><p>The growing size of neural language models has led to increased attention in
model compression. The two predominant approaches are pruning, which gradually
removes weights from a pre-trained model, and distillation, which trains a
smaller compact model to match a larger one. Pruning methods can significantly
reduce the model size but hardly achieve large speedups as distillation.
However, distillation methods require large amounts of unlabeled data and are
expensive to train. In this work, we propose a task-specific structured pruning
method CoFi (Coarse- and Fine-grained Pruning), which delivers highly
parallelizable subnetworks and matches the distillation methods in both
accuracy and latency, without resorting to any unlabeled data. Our key insight
is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads
and hidden units) modules, which controls the pruning decision of each
parameter with masks of different granularity. We also devise a layerwise
distillation strategy to transfer knowledge from unpruned to pruned models
during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi
yields models with over 10x speedups with a small accuracy drop, showing its
effectiveness and efficiency compared to previous pruning and distillation
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10496">
<div class="article-summary-box-inner">
<span><p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with
carefully curated vision-language datasets. While these datasets reach an order
of 10 million samples, the labor cost is prohibitive to scale further.
Conversely, unimodal encoders are pretrained with simpler annotations that are
less cost-prohibitive, achieving scales of hundreds of millions to billions. As
a result, unimodal encoders have achieved state-of-art (SOTA) on many
downstream tasks. However, challenges remain when applying to VL tasks. The
pretraining data is not optimal for cross-modal architectures and requires
heavy computational resources. In addition, unimodal architectures lack
cross-modal interactions that have demonstrated significant benefits for VL
tasks. Therefore, how to best leverage pretrained unimodal encoders for VL
tasks is still an area of active research. In this work, we propose a method to
leverage unimodal vision and text encoders for VL tasks that augment existing
VL approaches while conserving computational complexity. Specifically, we
propose Multimodal Adaptive Distillation (MAD), which adaptively distills
useful knowledge from pretrained encoders to cross-modal VL encoders. Second,
to better capture nuanced impacts on VL task performance, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data constraints and conditions of domain shift. Experiments demonstrate that
MAD leads to consistent gains in the low-shot, domain-shifted, and
fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA
performance on VCR compared to other single models pretrained with image-text
data. Finally, MAD outperforms concurrent works utilizing pretrained vision
encoder from CLIP. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10939">
<div class="article-summary-box-inner">
<span><p>Document intelligence automates the extraction of information from documents
and supports many business applications. Recent self-supervised learning
methods on large-scale unlabeled document datasets have opened up promising
directions towards reducing annotation efforts by training models with
self-supervised objectives. However, most of the existing document pretraining
methods are still language-dominated. We present UDoc, a new unified
pretraining framework for document understanding. UDoc is designed to support
most document understanding tasks, extending the Transformer to take multimodal
embeddings as input. Each input element is composed of words and visual
features from a semantic region of the input document image. An important
feature of UDoc is that it learns a generic representation by making use of
three self-supervised losses, encouraging the representation to model
sentences, learn similarities, and align modalities. Extensive empirical
analysis demonstrates that the pretraining procedure learns better joint
representations and leads to improvements in downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Dialogue Summarization System for Sales Calls. (arXiv:2204.12951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12951">
<div class="article-summary-box-inner">
<span><p>Summarizing sales calls is a routine task performed manually by salespeople.
We present a production system which combines generative models fine-tuned for
customer-agent setting, with a human-in-the-loop user experience for an
interactive summary curation process. We address challenging aspects of
dialogue summarization task in a real-world setting including long input
dialogues, content validation, lack of labeled data and quality evaluation. We
show how GPT-3 can be leveraged as an offline data labeler to handle training
data scarcity and accommodate privacy constraints in an industrial setting.
Experiments show significant improvements by our models in tackling the
summarization and content validation tasks on public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2204.13021v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13021">
<div class="article-summary-box-inner">
<span><p>We present NLU++, a novel dataset for natural language understanding (NLU) in
task-oriented dialogue (ToD) systems, with the aim to provide a much more
challenging evaluation environment for dialogue NLU models, up to date with the
current application and industry requirements. NLU++ is divided into two
domains (BANKING and HOTELS) and brings several crucial improvements over
current commonly used NLU datasets. 1) NLU++ provides fine-grained domain
ontologies with a large set of challenging multi-intent sentences, introducing
and validating the idea of intent modules that can be combined into complex
intents that convey complex user goals, combined with finer-grained and thus
more challenging slot sets. 2) The ontology is divided into domain-specific and
generic (i.e., domain-universal) intent modules that overlap across domains,
promoting cross-domain reusability of annotated examples. 3) The dataset design
has been inspired by the problems observed in industrial ToD systems, and 4) it
has been collected, filtered and carefully annotated by dialogue NLU experts,
yielding high-quality annotated data. Finally, we benchmark a series of current
state-of-the-art NLU models on NLU++; the results demonstrate the challenging
nature of the dataset, especially in low-data regimes, the validity of `intent
modularisation', and call for further research on ToD NLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Borrow -- Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion. (arXiv:2204.13097v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13097">
<div class="article-summary-box-inner">
<span><p>Prior work on integrating text corpora with knowledge graphs (KGs) to improve
Knowledge Graph Embedding (KGE) have obtained good performance for entities
that co-occur in sentences in text corpora. Such sentences (textual mentions of
entity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between
two entities. However, it is not possible to represent relations between
entities that do not co-occur in a single sentence using LDPs. In this paper,
we propose and evaluate several methods to address this problem, where we
borrow LDPs from the entity pairs that co-occur in sentences in the corpus
(i.e. with mention entity pairs) to represent entity pairs that do not co-occur
in any sentence in the corpus (i.e. without mention entity pairs). We propose a
supervised borrowing method, SuperBorrow, that learns to score the suitability
of an LDP to represent a without-mention entity pair using pre-trained entity
embeddings and contextualised LDP representations. Experimental results show
that SuperBorrow improves the link prediction performance of multiple
widely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation. (arXiv:2204.13132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13132">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to adapt a model trained on the
source domain (e.g. synthetic data) to the target domain (e.g. real-world data)
without requiring further annotations on the target domain. This work focuses
on UDA for semantic segmentation as real-world pixel-wise annotations are
particularly expensive to acquire. As UDA methods for semantic segmentation are
usually GPU memory intensive, most previous methods operate only on downscaled
images. We question this design as low-resolution predictions often fail to
preserve fine details. The alternative of training with random crops of
high-resolution images alleviates this problem but falls short in capturing
long-range, domain-robust context information. Therefore, we propose HRDA, a
multi-resolution training approach for UDA, that combines the strengths of
small high-resolution crops to preserve fine segmentation details and large
low-resolution crops to capture long-range context dependencies with a learned
scale attention, while maintaining a manageable GPU memory footprint. HRDA
enables adapting small objects and preserving fine segmentation details. It
significantly improves the state-of-the-art performance by 5.5 mIoU for
GTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in
unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available
at https://github.com/lhoyer/HRDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Nearest Neighbour Classifier. (arXiv:2204.13141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13141">
<div class="article-summary-box-inner">
<span><p>A windowed version of the Nearest Neighbour (WNN) classifier for images is
described. While its construction is inspired by the architecture of Artificial
Neural Networks, the underlying theoretical framework is based on approximation
theory. We illustrate WNN on the datasets MNIST and EMNIST of images of
handwritten digits. In order to calibrate the parameters of WNN, we first study
it on the classical MNIST dataset. We then apply WNN with these parameters to
the challenging EMNIST dataset. It is demonstrated that WNN misclassifies 0.42%
of the images of EMNIST and therefore significantly outperforms predictions by
humans and shallow ANNs that both have more than 1.3% of errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks. (arXiv:2204.13153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13153">
<div class="article-summary-box-inner">
<span><p>This paper follows cognitive studies to investigate a graph representation
for sketches, where the information of strokes, i.e., parts of a sketch, are
encoded on vertices and information of inter-stroke on edges. The resultant
graph representation facilitates the training of a Graph Neural Networks for
classification tasks, and achieves accuracy and robustness comparable to the
state-of-the-art against translation and rotation attacks, as well as stronger
attacks on graph vertices and topologies, i.e., modifications and addition of
strokes, all without resorting to adversarial training. Prior studies on
sketches, e.g., graph transformers, encode control points of stroke on
vertices, which are not invariant to spatial transformations. In contrary, we
encode vertices and edges using pairwise distances among control points to
achieve invariance. Compared with existing generative sketch model for one-shot
classification, our method does not rely on run-time statistical inference.
Lastly, the proposed representation enables generation of novel sketches that
are structurally similar to while separable from the existing dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Person Re-Identification. (arXiv:2204.13158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13158">
<div class="article-summary-box-inner">
<span><p>Person Re-Identification (Re-ID) is an important problem in computer
vision-based surveillance applications, in which one aims to identify a person
across different surveillance photographs taken from different cameras having
varying orientations and field of views. Due to the increasing demand for
intelligent video surveillance, Re-ID has gained significant interest in the
computer vision community. In this work, we experiment on some existing Re-ID
methods that obtain state of the art performance in some open benchmarks. We
qualitatively and quantitaively analyse their performance on a provided
dataset, and then propose methods to improve the results. This work was the
report submitted for COL780 final project at IIT Delhi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13170">
<div class="article-summary-box-inner">
<span><p>In Federated Learning a number of clients collaborate to train a model
without sharing their data. Client models are optimized locally and are
communicated through a central hub called server. A major challenge is to deal
with heterogeneity among clients' data which causes the local optimization to
drift away with respect to the global objective. In order to estimate and
therefore remove this drift, variance reduction techniques have been
incorporated into Federated Learning optimization recently. However, the
existing solutions propagate the error of their estimations, throughout the
optimization trajectory which leads to inaccurate approximations of the
clients' drift and ultimately failure to remove them properly. In this paper,
we address this issue by introducing an adaptive algorithm that efficiently
reduces clients' drift. Compared to the previous works on adapting variance
reduction to Federated Learning, our approach uses less or the same level of
communication bandwidth, computation or memory. Additionally, it addresses the
instability problem--prevalent in prior work, caused by increasing norm of the
estimates which makes our approach a much more practical solution for large
scale Federated Learning settings. Our experimental results demonstrate that
the proposed algorithm converges significantly faster and achieves higher
accuracy compared to the baselines in an extensive set of Federated Learning
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Graph Convolutional Network of Multi-Modality Brain Imaging for Alzheimer's Disease Diagnosis. (arXiv:2204.13188v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13188">
<div class="article-summary-box-inner">
<span><p>Identification of brain regions related to the specific neurological
disorders are of great importance for biomarker and diagnostic studies. In this
paper, we propose an interpretable Graph Convolutional Network (GCN) framework
for the identification and classification of Alzheimer's disease (AD) using
multi-modality brain imaging data. Specifically, we extended the Gradient Class
Activation Mapping (Grad-CAM) technique to quantify the most discriminative
features identified by GCN from brain connectivity patterns. We then utilized
them to find signature regions of interest (ROIs) by detecting the difference
of features between regions in healthy control (HC), mild cognitive impairment
(MCI), and AD groups. We conducted the experiments on the ADNI database with
imaging data from three modalities, including VBM-MRI, FDG-PET, and AV45-PET,
and showed that the ROI features learned by our method were effective for
enhancing the performances of both clinical score prediction and disease status
identification. It also successfully identified biomarkers associated with AD
and MCI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Use All The Labels: A Hierarchical Multi-Label Contrastive Learning Framework. (arXiv:2204.13207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13207">
<div class="article-summary-box-inner">
<span><p>Current contrastive learning frameworks focus on leveraging a single
supervisory signal to learn representations, which limits the efficacy on
unseen data and downstream tasks. In this paper, we present a hierarchical
multi-label representation learning framework that can leverage all available
labels and preserve the hierarchical relationship between classes. We introduce
novel hierarchy preserving losses, which jointly apply a hierarchical penalty
to the contrastive loss, and enforce the hierarchy constraint. The loss
function is data driven and automatically adapts to arbitrary multi-label
structures. Experiments on several datasets show that our
relationship-preserving embedding performs well on a variety of tasks and
outperform the baseline supervised and self-supervised approaches. Code is
available at https://github.com/salesforce/hierarchicalContrastiveLearning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offline Visual Representation Learning for Embodied Navigation. (arXiv:2204.13226v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13226">
<div class="article-summary-box-inner">
<span><p>How should we learn visual representations for embodied agents that must see
and move? The status quo is tabula rasa in vivo, i.e. learning visual
representations from scratch while also learning to move, potentially augmented
with auxiliary tasks (e.g. predicting the action taken between two successive
observations). In this paper, we show that an alternative 2-stage strategy is
far more effective: (1) offline pretraining of visual representations with
self-supervised learning (SSL) using large-scale pre-rendered images of indoor
environments (Omnidata), and (2) online finetuning of visuomotor
representations on specific tasks with image augmentations under long learning
schedules. We call this method Offline Visual Representation Learning (OVRL).
We conduct large-scale experiments - on 3 different 3D datasets (Gibson, HM3D,
MP3D), 2 tasks (ImageNav, ObjectNav), and 2 policy learning algorithms (RL, IL)
- and find that the OVRL representations lead to significant across-the-board
improvements in state of art, on ImageNav from 29.2% to 54.2% (+25% absolute,
86% relative) and on ObjectNav from 18.1% to 23.2% (+5.1% absolute, 28%
relative). Importantly, both results were achieved by the same visual encoder
generalizing to datasets that were not seen during pretraining. While the
benefits of pretraining sometimes diminish (or entirely disappear) with long
finetuning schedules, we find that OVRL's performance gains continue to
increase (not decrease) as the agent is trained for 2 billion frames of
experience.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Fine-tune with Dynamically Regulated Adversary. (arXiv:2204.13232v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13232">
<div class="article-summary-box-inner">
<span><p>Adversarial training is an effective method to boost model robustness to
malicious, adversarial attacks. However, such improvement in model robustness
often leads to a significant sacrifice of standard performance on clean images.
In many real-world applications such as health diagnosis and autonomous
surgical robotics, the standard performance is more valued over model
robustness against such extremely malicious attacks. This leads to the
question: To what extent we can boost model robustness without sacrificing
standard performance? This work tackles this problem and proposes a simple yet
effective transfer learning-based adversarial training strategy that
disentangles the negative effects of adversarial samples on model's standard
performance. In addition, we introduce a training-friendly adversarial attack
algorithm, which facilitates the boost of adversarial robustness without
introducing significant training complexity. Extensive experimentation
indicates that the proposed method outperforms previous adversarial training
algorithms towards the target: to improve model robustness while preserving
model's standard performance on clean data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Detection and Classification of Symbols in Engineering Drawings. (arXiv:2204.13277v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13277">
<div class="article-summary-box-inner">
<span><p>A method of finding and classifying various components and objects in a
design diagram, drawing, or planning layout is proposed. The method
automatically finds the objects present in a legend table and finds their
position, count and related information with the help of multiple deep neural
networks. The method is pre-trained on several drawings or design templates to
learn the feature set that may help in representing the new templates. For a
template not seen before, it does not require any training with template
dataset. The proposed method may be useful in multiple industry applications
such as design validation, object count, connectivity of components, etc. The
method is generic and domain independent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resource-efficient domain adaptive pre-training for medical images. (arXiv:2204.13280v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13280">
<div class="article-summary-box-inner">
<span><p>The deep learning-based analysis of medical images suffers from data scarcity
because of high annotation costs and privacy concerns. Researchers in this
domain have used transfer learning to avoid overfitting when using complex
architectures. However, the domain differences between pre-training and
downstream data hamper the performance of the downstream task. Some recent
studies have successfully used domain-adaptive pre-training (DAPT) to address
this issue. In DAPT, models are initialized with the generic dataset
pre-trained weights, and further pre-training is performed using a moderately
sized in-domain dataset (medical images). Although this technique achieved good
results for the downstream tasks in terms of accuracy and robustness, it is
computationally expensive even when the datasets for DAPT are moderately sized.
These compute-intensive techniques and models impact the environment negatively
and create an uneven playing field for researchers with limited resources. This
study proposed computationally efficient DAPT without compromising the
downstream accuracy and robustness. This study proposes three techniques for
this purpose, where the first (partial DAPT) performs DAPT on a subset of
layers. The second one adopts a hybrid strategy (hybrid DAPT) by performing
partial DAPT for a few epochs and then full DAPT for the remaining epochs. The
third technique performs DAPT on simplified variants of the base architecture.
The results showed that compared to the standard DAPT (full DAPT), the hybrid
DAPT technique achieved better performance on the development and external
datasets. In contrast, simplified architectures (after DAPT) achieved the best
robustness while achieving modest performance on the development dataset .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer. (arXiv:2204.13286v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13286">
<div class="article-summary-box-inner">
<span><p>Single-image super-resolution (SISR) has achieved significant breakthroughs
with the development of deep learning. However, these methods are difficult to
be applied in real-world scenarios since they are inevitably accompanied by the
problems of computational and memory costs caused by the complex operations. To
solve this issue, we propose a Lightweight Bimodal Network (LBNet) for SISR.
Specifically, an effective Symmetric CNN is designed for local feature
extraction and coarse image reconstruction. Meanwhile, we propose a Recursive
Transformer to fully learn the long-term dependence of images thus the global
information can be fully used to further refine texture details. Studies show
that the hybrid of CNN and Transformer can build a more efficient model.
Extensive experiments have proved that our LBNet achieves more prominent
performance than other state-of-the-art methods with a relatively low
computational cost and memory consumption. The code is available at
https://github.com/IVIPLab/LBNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-level Contrastive and Consistency Learning for Semi-Supervised Semantic Segmentation. (arXiv:2204.13314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13314">
<div class="article-summary-box-inner">
<span><p>Current semi-supervised semantic segmentation methods mainly focus on
designing pixel-level consistency and contrastive regularization. However,
pixel-level regularization is sensitive to noise from pixels with incorrect
predictions, and pixel-level contrastive regularization has memory and
computational cost with O(pixel_num^2). To address the issues, we propose a
novel region-level contrastive and consistency learning framework (RC^2L) for
semi-supervised semantic segmentation. Specifically, we first propose a Region
Mask Contrastive (RMC) loss and a Region Feature Contrastive (RFC) loss to
accomplish region-level contrastive property. Furthermore, Region Class
Consistency (RCC) loss and Semantic Mask Consistency (SMC) loss are proposed
for achieving region-level consistency. Based on the proposed region-level
contrastive and consistency regularization, we develop a region-level
contrastive and consistency learning framework (RC^2L) for semi-supervised
semantic segmentation, and evaluate our RC$^2$L on two challenging benchmarks
(PASCAL VOC 2012 and Cityscapes), outperforming the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMRotate: A Rotated Object Detection Benchmark using Pytorch. (arXiv:2204.13317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13317">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Decades of Colorization and Decolorization for Images and Videos. (arXiv:2204.13322v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13322">
<div class="article-summary-box-inner">
<span><p>Colorization is a computer-aided process, which aims to give color to a gray
image or video. It can be used to enhance black-and-white images, including
black-and-white photos, old-fashioned films, and scientific imaging results. On
the contrary, decolorization is to convert a color image or video into a
grayscale one. A grayscale image or video refers to an image or video with only
brightness information without color information. It is the basis of some
downstream image processing applications such as pattern recognition, image
segmentation, and image enhancement. Different from image decolorization, video
decolorization should not only consider the image contrast preservation in each
video frame, but also respect the temporal and spatial consistency between
video frames. Researchers were devoted to develop decolorization methods by
balancing spatial-temporal consistency and algorithm efficiency. With the
prevalance of the digital cameras and mobile phones, image and video
colorization and decolorization have been paid more and more attention by
researchers. This paper gives an overview of the progress of image and video
colorization and decolorization methods in the last two decades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminative-Region Attention and Orthogonal-View Generation Model for Vehicle Re-Identification. (arXiv:2204.13323v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13323">
<div class="article-summary-box-inner">
<span><p>Vehicle re-identification (Re-ID) is urgently demanded to alleviate
thepressure caused by the increasingly onerous task of urban traffic
management. Multiple challenges hamper the applications of vision-based vehicle
Re-ID methods: (1) The appearances of different vehicles of the same
brand/model are often similar; However, (2) the appearances of the same vehicle
differ significantly from different viewpoints. Previous methods mainly use
manually annotated multi-attribute datasets to assist the network in getting
detailed cues and in inferencing multi-view to improve the vehicle Re-ID
performance. However, finely labeled vehicle datasets are usually unattainable
in real application scenarios. Hence, we propose a Discriminative-Region
Attention and Orthogonal-View Generation (DRA-OVG) model, which only requires
identity (ID) labels to conquer the multiple challenges of vehicle Re-ID.The
proposed DRA model can automatically extract the discriminative region
features, which can distinguish similar vehicles. And the OVG model can
generate multi-view features based on the input view features to reduce the
impact of viewpoint mismatches. Finally, the distance between vehicle
appearances is presented by the discriminative region features and multi-view
features together. Therefore, the significance of pairwise distance measure
between vehicles is enhanced in acomplete feature space. Extensive experiments
substantiate the effectiveness of each proposed ingredient, and experimental
results indicate that our approach achieves remarkable improvements over the
state- of-the-art vehicle Re-ID methods on VehicleID and VeRi-776 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Image Captioning. (arXiv:2204.13324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13324">
<div class="article-summary-box-inner">
<span><p>State-of-the-art image captioners can generate accurate sentences to describe
images in a sequence to sequence manner without considering the controllability
and interpretability. This, however, is far from making image captioning widely
used as an image can be interpreted in infinite ways depending on the target
and the context at hand. Achieving controllability is important especially when
the image captioner is used by different people with different way of
interpreting the images. In this paper, we introduce a novel framework for
image captioning which can generate diverse descriptions by capturing the
co-dependence between Part-Of-Speech tags and semantics. Our model decouples
direct dependence between successive variables. In this way, it allows the
decoder to exhaustively search through the latent Part-Of-Speech choices, while
keeping decoding speed proportional to the size of the POS vocabulary. Given a
control signal in the form of a sequence of Part-Of-Speech tags, we propose a
method to generate captions through a Transformer network, which predicts words
based on the input Part-Of-Speech tag sequences. Experiments on publicly
available datasets show that our model significantly outperforms
state-of-the-art methods on generating diverse image captions with high
qualities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Color Transfer and Style Transfer for Images and Videos. (arXiv:2204.13339v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13339">
<div class="article-summary-box-inner">
<span><p>Image or video appearance features (e.g., color, texture, tone, illumination,
and so on) reflect one's visual perception and direct impression of an image or
video. Given a source image (video) and a target image (video), the image
(video) color transfer technique aims to process the color of the source image
or video (note that the source image or video is also referred to the reference
image or video in some literature) to make it look like that of the target
image or video, i.e., transferring the appearance of the target image or video
to that of the source image or video, which can thereby change one's perception
of the source image or video. As an extension of color transfer, style transfer
refers to rendering the content of a target image or video in the style of an
artist with either a style sample or a set of images through a style transfer
model. As an emerging field, the study of style transfer has attracted the
attention of a large number of researchers. After decades of development, it
has become a highly interdisciplinary research with a variety of artistic
expression styles can be achieved. This paper provides an overview of color
transfer and style transfer methods over the past years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Progressive Attention for Early Action Prediction. (arXiv:2204.13340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13340">
<div class="article-summary-box-inner">
<span><p>Early action prediction deals with inferring the ongoing action from
partially-observed videos, typically at the outset of the video. We propose a
bottleneck-based attention model that captures the evolution of the action,
through progressive sampling over fine-to-coarse scales. Our proposed Temporal
Progressive (TemPr) model is composed of multiple attention towers, one for
each scale. The predicted action label is based on the collective agreement
considering confidences of these attention towers. Extensive experiments over
three video datasets showcase state-of-the-art performance on the task of Early
Action Prediction across a range of backbone architectures. We demonstrate the
effectiveness and consistency of TemPr through detailed ablations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAGNet: Bidirectional Aware Guidance Network for Malignant Breast lesions Segmentation. (arXiv:2204.13342v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13342">
<div class="article-summary-box-inner">
<span><p>Breast lesions segmentation is an important step of computer-aided diagnosis
system, and it has attracted much attention. However, accurate segmentation of
malignant breast lesions is a challenging task due to the effects of
heterogeneous structure and similar intensity distributions. In this paper, a
novel bidirectional aware guidance network (BAGNet) is proposed to segment the
malignant lesion from breast ultrasound images. Specifically, the bidirectional
aware guidance network is used to capture the context between global
(low-level) and local (high-level) features from the input coarse saliency map.
The introduction of the global feature map can reduce the interference of
surrounding tissue (background) on the lesion regions. To evaluate the
segmentation performance of the network, we compared with several
state-of-the-art medical image segmentation methods on the public breast
ultrasound dataset using six commonly used evaluation metrics. Extensive
experimental results indicate that our method achieves the most competitive
segmentation results on malignant breast ultrasound images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Branch Classifiers of Multi-exit Architectures. (arXiv:2204.13347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13347">
<div class="article-summary-box-inner">
<span><p>Multi-exit architectures consist of a backbone and branch classifiers that
offer shortened inference pathways to reduce the run-time of deep neural
networks. In this paper, we analyze different branching patterns that vary in
their allocation of computational complexity for the branch classifiers.
Constant-complexity branching keeps all branches the same, while
complexity-increasing and complexity-decreasing branching place more complex
branches later or earlier in the backbone respectively. Through extensive
experimentation on multiple backbones and datasets, we find that
complexity-decreasing branches are more effective than constant-complexity or
complexity-increasing branches, which achieve the best accuracy-cost trade-off.
We investigate a cause by using knowledge consistency to probe the effect of
adding branches onto a backbone. Our findings show that complexity-decreasing
branching yields the least disruption to the feature abstraction hierarchy of
the backbone, which explains the effectiveness of the branching patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Generalized Unfolding Networks for Image Restoration. (arXiv:2204.13348v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13348">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNN) have achieved great success in image restoration.
However, most DNN methods are designed as a black box, lacking transparency and
interpretability. Although some methods are proposed to combine traditional
optimization algorithms with DNN, they usually demand pre-defined degradation
processes or handcrafted assumptions, making it difficult to deal with complex
and real-world applications. In this paper, we propose a Deep Generalized
Unfolding Network (DGUNet) for image restoration. Concretely, without loss of
interpretability, we integrate a gradient estimation strategy into the gradient
descent step of the Proximal Gradient Descent (PGD) algorithm, driving it to
deal with complex and real-world image degradation. In addition, we design
inter-stage information pathways across proximal mapping in different PGD
iterations to rectify the intrinsic information loss in most deep unfolding
networks (DUN) through a multi-scale and spatial-adaptive way. By integrating
the flexible gradient descent and informative proximal mapping, we unfold the
iterative PGD algorithm into a trainable DNN. Extensive experiments on various
image restoration tasks demonstrate the superiority of our method in terms of
state-of-the-art performance, interpretability, and generalizability. The
source code is available at
https://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning with Bayesian Model based on a Fixed Pre-trained Feature Extractor. (arXiv:2204.13349v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13349">
<div class="article-summary-box-inner">
<span><p>Deep learning has shown its human-level performance in various applications.
However, current deep learning models are characterised by catastrophic
forgetting of old knowledge when learning new classes. This poses a challenge
particularly in intelligent diagnosis systems where initially only training
data of a limited number of diseases are available. In this case, updating the
intelligent system with data of new diseases would inevitably downgrade its
performance on previously learned diseases. Inspired by the process of learning
new knowledge in human brains, we propose a Bayesian generative model for
continual learning built on a fixed pre-trained feature extractor. In this
model, knowledge of each old class can be compactly represented by a collection
of statistical distributions, e.g. with Gaussian mixture models, and naturally
kept from forgetting in continual learning over time. Unlike existing
class-incremental learning methods, the proposed approach is not sensitive to
the continual learning process and can be additionally well applied to the
data-incremental learning scenario. Experiments on multiple medical and natural
image classification tasks showed that the proposed approach outperforms
state-of-the-art approaches which even keep some images of old classes during
continual learning of new classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poly-CAM: High resolution class activation map for convolutional neural networks. (arXiv:2204.13359v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13359">
<div class="article-summary-box-inner">
<span><p>The need for Explainable AI is increasing with the development of deep
learning. The saliency maps derived from convolutional neural networks
generally fail in localizing with accuracy the image features justifying the
network prediction. This is because those maps are either low-resolution as for
CAM [Zhou et al., 2016], or smooth as for perturbation-based methods [Zeiler
and Fergus, 2014], or do correspond to a large number of widespread peaky spots
as for gradient-based approaches [Sundararajan et al., 2017, Smilkov et al.,
2017]. In contrast, our work proposes to combine the information from earlier
network layers with the one from later layers to produce a high resolution
Class Activation Map that is competitive with the previous art in term of
insertion-deletion faithfulness metrics, while outperforming it in term of
precision of class-specific features localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Role of Field of View for Occlusion Removal with Airborne Optical Sectioning. (arXiv:2204.13371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13371">
<div class="article-summary-box-inner">
<span><p>Occlusion caused by vegetation is an essential problem for remote sensing
applications in areas, such as search and rescue, wildfire detection, wildlife
observation, surveillance, border control, and others. Airborne Optical
Sectioning (AOS) is an optical, wavelength-independent synthetic aperture
imaging technique that supports computational occlusion removal in real-time.
It can be applied with manned or unmanned aircrafts, such as drones. In this
article, we demonstrate a relationship between forest density and field of view
(FOV) of applied imaging systems. This finding was made with the help of a
simulated procedural forest model which offers the consideration of more
realistic occlusion properties than our previous statistical model. While AOS
has been explored with automatic and autonomous research prototypes in the
past, we present a free AOS integration for DJI systems. It enables bluelight
organizations and others to use and explore AOS with compatible, manually
operated, off-the-shelf drones. The (digitally cropped) default FOV for this
implementation was chosen based on our new finding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphing Attack Potential. (arXiv:2204.13374v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13374">
<div class="article-summary-box-inner">
<span><p>In security systems the risk assessment in the sense of common criteria
testing is a very relevant topic; this requires quantifying the attack
potential in terms of the expertise of the attacker, his knowledge about the
target and access to equipment. Contrary to those attacks, the recently
revealed morphing attacks against Face Recognition Systems (FRSs) can not be
assessed by any of the above criteria. But not all morphing techniques pose the
same risk for an operational face recognition system. This paper introduces
with the Morphing Attack Potential (MAP) a consistent methodology, that can
quantify the risk, which a certain morphing attack creates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keep the Caption Information: Preventing Shortcut Learning in Contrastive Image-Caption Retrieval. (arXiv:2204.13382v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13382">
<div class="article-summary-box-inner">
<span><p>To train image-caption retrieval (ICR) methods, contrastive loss functions
are a common choice for optimization functions. Unfortunately, contrastive ICR
methods are vulnerable to learning shortcuts: decision rules that perform well
on the training data but fail to transfer to other testing conditions. We
introduce an approach to reduce shortcut feature representations for the ICR
task: latent target decoding (LTD). We add an additional decoder to the
learning framework to reconstruct the input caption, which prevents the image
and caption encoder from learning shortcut features. Instead of reconstructing
input captions in the input space, we decode the semantics of the caption in a
latent space. We implement the LTD objective as an optimization constraint, to
ensure that the reconstruction loss is below a threshold value while primarily
optimizing for the contrastive loss. Importantly, LTD does not depend on
additional training data or expensive (hard) negative mining strategies. Our
experiments show that, unlike reconstructing the input caption, LTD reduces
shortcut learning and improves generalizability by obtaining higher recall@k
and r-precision scores. Additionally, we show that the evaluation scores
benefit from implementing LTD as an optimization constraint instead of a dual
loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-Visual Contrastive Learning for Self-supervised Action Recognition. (arXiv:2204.13386v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13386">
<div class="article-summary-box-inner">
<span><p>The underlying correlation between audio and visual modalities within videos
can be utilized to learn supervised information for unlabeled videos. In this
paper, we present an end-to-end self-supervised framework named Audio-Visual
Contrastive Learning (AVCL), to learn discriminative audio-visual
representations for action recognition. Specifically, we design an attention
based multi-modal fusion module (AMFM) to fuse audio and visual modalities. To
align heterogeneous audio-visual modalities, we construct a novel
co-correlation guided representation alignment module (CGRA). To learn
supervised information from unlabeled videos, we propose a novel
self-supervised contrastive learning module (SelfCL). Furthermore, to expand
the existing audio-visual action recognition datasets and better evaluate our
framework AVCL, we build a new audio-visual action recognition dataset named
Kinetics-Sounds100. Experimental results on Kinetics-Sounds32 and
Kinetics-Sounds100 datasets demonstrate the superiority of our AVCL over the
state-of-the-art methods on large-scale action recognition benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">List-Mode PET Image Reconstruction Using Deep Image Prior. (arXiv:2204.13404v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13404">
<div class="article-summary-box-inner">
<span><p>List-mode positron emission tomography (PET) image reconstruction is an
important tool for PET scanners with many lines-of-response (LORs) and
additional information such as time-of-flight and depth-of-interaction. Deep
learning is one possible solution to enhance the quality of PET image
reconstruction. However, the application of deep learning techniques to
list-mode PET image reconstruction have not been progressed because list data
is a sequence of bit codes and unsuitable for processing by convolutional
neural networks (CNN). In this study, we propose a novel list-mode PET image
reconstruction method using an unsupervised CNN called deep image prior (DIP)
and a framework of alternating direction method of multipliers. The proposed
list-mode DIP reconstruction (LM-DIPRecon) method alternatively iterates
regularized list-mode dynamic row action maximum likelihood algorithm
(LM-DRAMA) and magnetic resonance imaging conditioned DIP (MR-DIP). We
evaluated LM-DIPRecon using both simulation and clinical data, and it achieved
sharper images and better tradeoff curves between contrast and noise than the
LM-DRAMA and MR-DIP. These results indicated that the LM-DIPRecon is useful for
quantitative PET imaging with limited events. In addition, as list data has
finer temporal information than dynamic sinograms, list-mode deep image prior
reconstruction is expected to be useful for 4D PET imaging and motion
correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-MoreGAN: A New Semi-supervised Generative Adversarial Network for Mixture of Rain Removal. (arXiv:2204.13420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13420">
<div class="article-summary-box-inner">
<span><p>Rain is one of the most common weather which can completely degrade the image
quality and interfere with the performance of many computer vision tasks,
especially under heavy rain conditions. We observe that: (i) rain is a mixture
of rain streaks and rainy haze; (ii) the scene depth determines the intensity
of rain streaks and the transformation into the rainy haze; (iii) most existing
deraining methods are only trained on synthetic rainy images, and hence
generalize poorly to the real-world scenes. Motivated by these observations, we
propose a new SEMI-supervised Mixture Of rain REmoval Generative Adversarial
Network (Semi-MoreGAN), which consists of four key modules: (I) a novel
attentional depth prediction network to provide precise depth estimation; (ii)
a context feature prediction network composed of several well-designed detailed
residual blocks to produce detailed image context features; (iii) a pyramid
depth-guided non-local network to effectively integrate the image context with
the depth information, and produce the final rain-free images; and (iv) a
comprehensive semi-supervised loss function to make the model not limited to
synthetic datasets but generalize smoothly to real-world heavy rainy scenes.
Extensive experiments show clear improvements of our approach over twenty
representative state-of-the-arts on both synthetic and real-world rainy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Relation Guided Set Matching for Few-shot Action Recognition. (arXiv:2204.13423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13423">
<div class="article-summary-box-inner">
<span><p>Current few-shot action recognition methods reach impressive performance by
learning discriminative features for each video via episodic training and
designing various temporal alignment strategies. Nevertheless, they are limited
in that (a) learning individual features without considering the entire task
may lose the most relevant information in the current episode, and (b) these
alignment strategies may fail in misaligned instances. To overcome the two
limitations, we propose a novel Hybrid Relation guided Set Matching (HyRSM)
approach that incorporates two key components: hybrid relation module and set
matching metric. The purpose of the hybrid relation module is to learn
task-specific embeddings by fully exploiting associated relations within and
cross videos in an episode. Built upon the task-specific features, we
reformulate distance measure between query and support videos as a set matching
problem and further design a bidirectional Mean Hausdorff Metric to improve the
resilience to misaligned instances. By this means, the proposed HyRSM can be
highly informative and flexible to predict query categories under the few-shot
settings. We evaluate HyRSM on six challenging benchmarks, and the experimental
results show its superiority over the state-of-the-art methods by a convincing
margin. Project page: https://hyrsm-cvpr2022.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AE-NeRF: Auto-Encoding Neural Radiance Fields for 3D-Aware Object Manipulation. (arXiv:2204.13426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13426">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework for 3D-aware object manipulation, called
Auto-Encoding Neural Radiance Fields (AE-NeRF). Our model, which is formulated
in an auto-encoder architecture, extracts disentangled 3D attributes such as 3D
shape, appearance, and camera pose from an image, and a high-quality image is
rendered from the attributes through disentangled generative Neural Radiance
Fields (NeRF). To improve the disentanglement ability, we present two losses,
global-local attribute consistency loss defined between input and output, and
swapped-attribute classification loss. Since training such auto-encoding
networks from scratch without ground-truth shape and appearance information is
non-trivial, we present a stage-wise training scheme, which dramatically helps
to boost the performance. We conduct experiments to demonstrate the
effectiveness of the proposed model over the latest methods and provide
extensive ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Orientation-Aware Functional Maps: Tackling Symmetry Issues in Shape Matching. (arXiv:2204.13453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13453">
<div class="article-summary-box-inner">
<span><p>State-of-the-art fully intrinsic networks for non-rigid shape matching often
struggle to disambiguate the symmetries of the shapes leading to unstable
correspondence predictions. Meanwhile, recent advances in the functional map
framework allow to enforce orientation preservation using a functional
representation for tangent vector field transfer, through so-called complex
functional maps. Using this representation, we propose a new deep learning
approach to learn orientation-aware features in a fully unsupervised setting.
Our architecture is built on top of DiffusionNet, making it robust to
discretization changes. Additionally, we introduce a vector field-based loss,
which promotes orientation preservation without using (often unstable)
extrinsic descriptors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Pixel-Level Noisy Label : A New Perspective for Light Field Saliency Detection. (arXiv:2204.13456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13456">
<div class="article-summary-box-inner">
<span><p>Saliency detection with light field images is becoming attractive given the
abundant cues available, however, this comes at the expense of large-scale
pixel level annotated data which is expensive to generate. In this paper, we
propose to learn light field saliency from pixel-level noisy labels obtained
from unsupervised hand crafted featured based saliency methods. Given this
goal, a natural question is: can we efficiently incorporate the relationships
among light field cues while identifying clean labels in a unified framework?
We address this question by formulating the learning as a joint optimization of
intra light field features fusion stream and inter scenes correlation stream to
generate the predictions. Specially, we first introduce a pixel forgetting
guided fusion module to mutually enhance the light field features and exploit
pixel consistency across iterations to identify noisy pixels. Next, we
introduce a cross scene noise penalty loss for better reflecting latent
structures of training data and enabling the learning to be invariant to noise.
Extensive experiments on multiple benchmark datasets demonstrate the
superiority of our framework showing that it learns saliency prediction
comparable to state-of-the-art fully supervised light field saliency methods.
Our code is available at https://github.com/OLobbCode/NoiseLF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13483">
<div class="article-summary-box-inner">
<span><p>The new generation of 4D high-resolution imaging radar provides not only a
huge amount of point cloud but also additional elevation measurement, which has
a great potential of 3D sensing in autonomous driving. In this paper, we
introduce an autonomous driving dataset named TJ4DRadSet, including multi-modal
sensors that are 4D radar, lidar, camera and GNSS, with about 40K frames in
total. 7757 frames within 44 consecutive sequences in various driving scenarios
are well annotated with 3D bounding boxes and track id. We provide a 4D
radar-based 3D object detection baseline for our dataset to demonstrate the
effectiveness of deep learning methods for 4D radar point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Multiscale Deep Equilibrium Models. (arXiv:2204.13492v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13492">
<div class="article-summary-box-inner">
<span><p>We present StreamDEQ, a method that infers frame-wise representations on
videos with minimal per-frame computation. In contrast to conventional methods
where compute time grows at least linearly with the network depth, we aim to
update the representations in a continuous manner. For this purpose, we
leverage the recently emerging implicit layer model which infers the
representation of an image by solving a fixed-point problem. Our main insight
is to leverage the slowly changing nature of videos and use the previous frame
representation as an initial condition on each frame. This scheme effectively
recycles the recent inference computations and greatly reduces the needed
processing time. Through extensive experimental analysis, we show that
StreamDEQ is able to recover near-optimal representations in a few frames time,
and maintain an up-to-date representation throughout the video duration. Our
experiments on video semantic segmentation and video object detection show that
StreamDEQ achieves on par accuracy with the baseline (standard MDEQ) while
being more than $3\times$ faster. The project page is available at:
https://ufukertenli.github.io/streamdeq/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Spatial-spectral Hyperspectral Image Reconstruction and Clustering with Diffusion Geometry. (arXiv:2204.13497v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13497">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images, which store a hundred or more spectral bands of
reflectance, have become an important data source in natural and social
sciences. Hyperspectral images are often generated in large quantities at a
relatively coarse spatial resolution. As such, unsupervised machine learning
algorithms incorporating known structure in hyperspectral imagery are needed to
analyze these images automatically. This work introduces the Spatial-Spectral
Image Reconstruction and Clustering with Diffusion Geometry (DSIRC) algorithm
for partitioning highly mixed hyperspectral images. DSIRC reduces measurement
noise through a shape-adaptive reconstruction procedure. In particular, for
each pixel, DSIRC locates spectrally correlated pixels within a data-adaptive
spatial neighborhood and reconstructs that pixel's spectral signature using
those of its neighbors. DSIRC then locates high-density, high-purity pixels far
in diffusion distance (a data-dependent distance metric) from other
high-density, high-purity pixels and treats these as cluster exemplars, giving
each a unique label. Non-modal pixels are assigned the label of their diffusion
distance-nearest neighbor of higher density and purity that is already labeled.
Strong numerical results indicate that incorporating spatial information
through image reconstruction substantially improves the performance of
pixel-wise clustering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse-Designed Meta-Optics with Spectral-Spatial Engineered Response to Mimic Color Perception. (arXiv:2204.13520v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13520">
<div class="article-summary-box-inner">
<span><p>Meta-optics have rapidly become a major research field within the optics and
photonics community, strongly driven by the seemingly limitless opportunities
made possible by controlling optical wavefronts through interaction with arrays
of sub-wavelength scatterers. As more and more modalities are explored, the
design strategies to achieve desired functionalities become increasingly
demanding, necessitating more advanced design techniques. Herein, the
inverse-design approach is utilized to create a set of single-layer meta-optics
that simultaneously focus light and shape the spectra of focused light without
using any filters. Thus, both spatial and spectral properties of the
meta-optics are optimized, resulting in spectra that mimic the color matching
functions of the CIE 1931 XYZ color space, which links the distributions of
wavelengths in light and the color perception of a human eye. Experimental
demonstrations of these meta-optics show qualitative agreement with the
theoretical predictions and help elucidate the focusing mechanism of these
devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tragedy Plus Time: Capturing Unintended Human Activities from Weakly-labeled Videos. (arXiv:2204.13548v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13548">
<div class="article-summary-box-inner">
<span><p>In videos that contain actions performed unintentionally, agents do not
achieve their desired goals. In such videos, it is challenging for computer
vision systems to understand high-level concepts such as goal-directed
behavior, an ability present in humans from a very early age. Inculcating this
ability in artificially intelligent agents would make them better social
learners by allowing them to evaluate human action under a teleological lens.
To validate the ability of deep learning models to perform this task, we curate
the W-Oops dataset, built upon the Oops dataset [15]. W-Oops consists of 2,100
unintentional human action videos, with 44 goal-directed and 30 unintentional
video-level activity labels collected through human annotations. Due to the
expensive segment annotation procedure, we propose a weakly supervised
algorithm for localizing the goal-directed as well as unintentional temporal
regions in the video leveraging solely video-level labels. In particular, we
employ an attention mechanism-based strategy that predicts the temporal regions
which contribute the most to a classification task. Meanwhile, our designed
overlap regularization allows the model to focus on distinct portions of the
video for inferring the goal-directed and unintentional activity while
guaranteeing their temporal ordering. Extensive quantitative experiments verify
the validity of our localization method. We further conduct a video captioning
experiment which demonstrates that the proposed localization module does indeed
assist teleological action understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixup-based Deep Metric Learning Approaches for Incomplete Supervision. (arXiv:2204.13572v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13572">
<div class="article-summary-box-inner">
<span><p>Deep learning architectures have achieved promising results in different
areas (e.g., medicine, agriculture, and security). However, using those
powerful techniques in many real applications becomes challenging due to the
large labeled collections required during training. Several works have pursued
solutions to overcome it by proposing strategies that can learn more for less,
e.g., weakly and semi-supervised learning approaches. As these approaches do
not usually address memorization and sensitivity to adversarial examples, this
paper presents three deep metric learning approaches combined with Mixup for
incomplete-supervision scenarios. We show that some state-of-the-art approaches
in metric learning might not work well in such scenarios. Moreover, the
proposed approaches outperform most of them in different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symmetric Transformer-based Network for Unsupervised Image Registration. (arXiv:2204.13575v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13575">
<div class="article-summary-box-inner">
<span><p>Medical image registration is a fundamental and critical task in medical
image analysis. With the rapid development of deep learning, convolutional
neural networks (CNN) have dominated the medical image registration field. Due
to the disadvantage of the local receptive field of CNN, some recent
registration methods have focused on using transformers for non-local
registration. However, the standard Transformer has a vast number of parameters
and high computational complexity, which causes Transformer can only be applied
at the bottom of the registration models. As a result, only coarse information
is available at the lowest resolution, limiting the contribution of Transformer
in their models. To address these challenges, we propose a convolution-based
efficient multi-head self-attention (CEMSA) block, which reduces the parameters
of the traditional Transformer and captures local spatial context information
for reducing semantic ambiguity in the attention mechanism. Based on the
proposed CEMSA, we present a novel Symmetric Transformer-based model
(SymTrans). SymTrans employs the Transformer blocks in the encoder and the
decoder respectively to model the long-range spatial cross-image relevance. We
apply SymTrans to the displacement field and diffeomorphic registration.
Experimental results show that our proposed method achieves state-of-the-art
performance in image registration. Our code is publicly available at
\url{https://github.com/MingR-Ma/SymTrans}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Sleeping Quality using Convolutional Neural Networks. (arXiv:2204.13584v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13584">
<div class="article-summary-box-inner">
<span><p>Identifying sleep stages and patterns is an essential part of diagnosing and
treating sleep disorders. With the advancement of smart technologies, sensor
data related to sleeping patterns can be captured easily. In this paper, we
propose a Convolution Neural Network (CNN) architecture that improves the
classification performance. In particular, we benchmark the classification
performance from different methods, including traditional machine learning
methods such as Logistic Regression (LR), Decision Trees (DT), k-Nearest
Neighbour (k-NN), Naive Bayes (NB) and Support Vector Machine (SVM), on 3
publicly available sleep datasets. The accuracy, sensitivity, specificity,
precision, recall, and F-score are reported and will serve as a baseline to
simulate the research in this direction in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Close Look into Human Activity Recognition Models using Deep Learning. (arXiv:2204.13589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13589">
<div class="article-summary-box-inner">
<span><p>Human activity recognition using deep learning techniques has become
increasing popular because of its high effectivity with recognizing complex
tasks, as well as being relatively low in costs compared to more traditional
machine learning techniques. This paper surveys some state-of-the-art human
activity recognition models that are based on deep learning architecture and
has layers containing Convolution Neural Networks (CNN), Long Short-Term Memory
(LSTM), or a mix of more than one type for a hybrid system. The analysis
outlines how the models are implemented to maximize its effectivity and some of
the potential limitations it faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision for Road Imaging and Pothole Detection: A State-of-the-Art Review of Systems and Algorithms. (arXiv:2204.13590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13590">
<div class="article-summary-box-inner">
<span><p>Computer vision algorithms have been prevalently utilized for 3-D road
imaging and pothole detection for over two decades. Nonetheless, there is a
lack of systematic survey articles on state-of-the-art (SoTA) computer vision
techniques, especially deep learning models, developed to tackle these
problems. This article first introduces the sensing systems employed for 2-D
and 3-D road data acquisition, including camera(s), laser scanners, and
Microsoft Kinect. Afterward, it thoroughly and comprehensively reviews the SoTA
computer vision algorithms, including (1) classical 2-D image processing, (2)
3-D point cloud modeling and segmentation, and (3) machine/deep learning,
developed for road pothole detection. This article also discusses the existing
challenges and future development trends of computer vision-based road pothole
detection approaches: classical 2-D image processing-based and 3-D point cloud
modeling and segmentation-based approaches have already become history; and
Convolutional neural networks (CNNs) have demonstrated compelling road pothole
detection results and are promising to break the bottleneck with the future
advances in self/un-supervised learning for multi-modal semantic segmentation.
We believe that this survey can serve as practical guidance for developing the
next-generation road condition assessment systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Networks for Image Super-Resolution: A Survey. (arXiv:2204.13620v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13620">
<div class="article-summary-box-inner">
<span><p>Single image super-resolution (SISR) has played an important role in the
field of image processing. Recent generative adversarial networks (GANs) can
achieve excellent results on low-resolution images with small samples. However,
there are little literatures summarizing different GANs in SISR. In this paper,
we conduct a comparative study of GANs from different perspectives. We first
take a look at developments of GANs. Second, we present popular architectures
for GANs in big and small samples for image applications. Then, we analyze
motivations, implementations and differences of GANs based optimization methods
and discriminative learning for image super-resolution in terms of supervised,
semi-supervised and unsupervised manners. Next, we compare performance of these
popular GANs on public datasets via quantitative and qualitative analysis in
SISR. Finally, we highlight challenges of GANs and potential research points
for SISR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotationally Equivariant 3D Object Detection. (arXiv:2204.13630v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13630">
<div class="article-summary-box-inner">
<span><p>Rotation equivariance has recently become a strongly desired property in the
3D deep learning community. Yet most existing methods focus on equivariance
regarding a global input rotation while ignoring the fact that rotation
symmetry has its own spatial support. Specifically, we consider the object
detection problem in 3D scenes, where an object bounding box should be
equivariant regarding the object pose, independent of the scene motion. This
suggests a new desired property we call object-level rotation equivariance. To
incorporate object-level rotation equivariance into 3D object detectors, we
need a mechanism to extract equivariant features with local object-level
spatial support while being able to model cross-object context information. To
this end, we propose Equivariant Object detection Network (EON) with a rotation
equivariance suspension design to achieve object-level equivariance. EON can be
applied to modern point cloud object detectors, such as VoteNet and PointRCNN,
enabling them to exploit object rotation symmetry in scene-scale inputs. Our
experiments on both indoor scene and autonomous driving datasets show that
significant improvements are obtained by plugging our EON design into existing
state-of-the-art 3D object detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly. (arXiv:2204.13631v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13631">
<div class="article-summary-box-inner">
<span><p>Machine learning has advanced dramatically, narrowing the accuracy gap to
humans in multimodal tasks like visual question answering (VQA). However, while
humans can say "I don't know" when they are uncertain (i.e., abstain from
answering a question), such ability has been largely neglected in multimodal
research, despite the importance of this problem to the usage of VQA in real
settings. In this work, we promote a problem formulation for reliable VQA,
where we prefer abstention over providing an incorrect answer. We first enable
abstention capabilities for several VQA models, and analyze both their
coverage, the portion of questions answered, and risk, the error on that
portion. For that we explore several abstention approaches. We find that
although the best performing models achieve over 71% accuracy on the VQA v2
dataset, introducing the option to abstain by directly using a model's softmax
scores limits them to answering less than 8% of the questions to achieve a low
risk of error (i.e., 1%). This motivates us to utilize a multimodal selection
function to directly estimate the correctness of the predicted answers, which
we show can triple the coverage from, for example, 5.0% to 16.7% at 1% risk.
While it is important to analyze both coverage and risk, these metrics have a
trade-off which makes comparing VQA models challenging. To address this, we
also propose an Effective Reliability metric for VQA that places a larger cost
on incorrect answers compared to abstentions. This new problem formulation,
metric, and analysis for VQA provide the groundwork for building effective and
reliable VQA models that have the self-awareness to abstain if and only if they
don't know the answer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemAttNet: Towards Attention-based Semantic Aware Guided Depth Completion. (arXiv:2204.13635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13635">
<div class="article-summary-box-inner">
<span><p>Depth completion involves recovering a dense depth map from a sparse map and
an RGB image. Recent approaches focus on utilizing color images as guidance
images to recover depth at invalid pixels. However, color images alone are not
enough to provide the necessary semantic understanding of the scene.
Consequently, the depth completion task suffers from sudden illumination
changes in RGB images (e.g., shadows). In this paper, we propose a novel
three-branch backbone comprising color-guided, semantic-guided, and
depth-guided branches. Specifically, the color-guided branch takes a sparse
depth map and RGB image as an input and generates color depth which includes
color cues (e.g., object boundaries) of the scene. The predicted dense depth
map of color-guided branch along-with semantic image and sparse depth map is
passed as input to semantic-guided branch for estimating semantic depth. The
depth-guided branch takes sparse, color, and semantic depths to generate the
dense depth map. The color depth, semantic depth, and guided depth are
adaptively fused to produce the output of our proposed three-branch backbone.
In addition, we also propose to apply semantic-aware multi-modal
attention-based fusion block (SAMMAFB) to fuse features between all three
branches. We further use CSPN++ with Atrous convolutions to refine the dense
depth map produced by our three-branch backbone. Extensive experiments show
that our model achieves state-of-the-art performance in the KITTI depth
completion benchmark at the time of submission.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Extract Building Footprints from Off-Nadir Aerial Images. (arXiv:2204.13637v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13637">
<div class="article-summary-box-inner">
<span><p>Extracting building footprints from aerial images is essential for precise
urban mapping with photogrammetric computer vision technologies. Existing
approaches mainly assume that the roof and footprint of a building are well
overlapped, which may not hold in off-nadir aerial images as there is often a
big offset between them. In this paper, we propose an offset vector learning
scheme, which turns the building footprint extraction problem in off-nadir
images into an instance-level joint prediction problem of the building roof and
its corresponding "roof to footprint" offset vector. Thus the footprint can be
estimated by translating the predicted roof mask according to the predicted
offset vector. We further propose a simple but effective feature-level offset
augmentation module, which can significantly refine the offset vector
prediction by introducing little extra cost. Moreover, a new dataset, Buildings
in Off-Nadir Aerial Images (BONAI), is created and released in this paper. It
contains 268,958 building instances across 3,300 aerial images with fully
annotated instance-level roof, footprint, and corresponding offset vector for
each building. Experiments on the BONAI dataset demonstrate that our method
achieves the state-of-the-art, outperforming other competitors by 3.37 to 7.39
points in F1-score. The codes, datasets, and trained models are available at
https://github.com/jwwangchn/BONAI.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking High-Accuracy Differentially Private Image Classification through Scale. (arXiv:2204.13650v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13650">
<div class="article-summary-box-inner">
<span><p>Differential Privacy (DP) provides a formal privacy guarantee preventing
adversaries with access to a machine learning model from extracting information
about individual training points. Differentially Private Stochastic Gradient
Descent (DP-SGD), the most popular DP training method, realizes this protection
by injecting noise during training. However previous works have found that
DP-SGD often leads to a significant degradation in performance on standard
image classification benchmarks. Furthermore, some authors have postulated that
DP-SGD inherently performs poorly on large models, since the norm of the noise
required to preserve privacy is proportional to the model dimension. In
contrast, we demonstrate that DP-SGD on over-parameterized models can perform
significantly better than previously thought. Combining careful hyper-parameter
tuning with simple techniques to ensure signal propagation and improve the
convergence rate, we obtain a new SOTA on CIFAR-10 of 81.4% under (8,
10^{-5})-DP using a 40-layer Wide-ResNet, improving over the previous SOTA of
71.7%. When fine-tuning a pre-trained 200-layer Normalizer-Free ResNet, we
achieve a remarkable 77.1% top-1 accuracy on ImageNet under (1, 8*10^{-7})-DP,
and achieve 81.1% under (8, 8*10^{-7})-DP. This markedly exceeds the previous
SOTA of 47.9% under a larger privacy budget of (10, 10^{-6})-DP. We believe our
results are a significant step towards closing the accuracy gap between private
and non-private image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRIT: General Robust Image Task Benchmark. (arXiv:2204.13653v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13653">
<div class="article-summary-box-inner">
<span><p>Computer vision models excel at making predictions when the test distribution
closely resembles the training distribution. Such models have yet to match the
ability of biological vision to learn from multiple sources and generalize to
new data sources and tasks. To facilitate the development and evaluation of
more general vision systems, we introduce the General Robust Image Task (GRIT)
benchmark. GRIT evaluates the performance, robustness, and calibration of a
vision system across a variety of image prediction tasks, concepts, and data
sources. The seven tasks in GRIT are selected to cover a range of visual
skills: object categorization, object localization, referring expression
grounding, visual question answering, segmentation, human keypoint detection,
and surface normal estimation. GRIT is carefully designed to enable the
evaluation of robustness under image perturbations, image source distribution
shift, and concept distribution shift. By providing a unified platform for
thorough assessment of skills and concepts learned by a vision model, we hope
GRIT catalyzes the development of performant and robust general purpose vision
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation. (arXiv:2204.13656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13656">
<div class="article-summary-box-inner">
<span><p>In clinical practice, well-aligned multi-modal images, such as Magnetic
Resonance (MR) and Computed Tomography (CT), together can provide complementary
information for image-guided therapies. Multi-modal image registration is
essential for the accurate alignment of these multi-modal images. However, it
remains a very challenging task due to complicated and unknown spatial
correspondence between different modalities. In this paper, we propose a novel
translation-based unsupervised deformable image registration approach to
convert the multi-modal registration problem to a mono-modal one. Specifically,
our approach incorporates a discriminator-free translation network to
facilitate the training of the registration network and a patchwise contrastive
loss to encourage the translation network to preserve object shapes.
Furthermore, we propose to replace an adversarial loss, that is widely used in
previous multi-modal image registration methods, with a pixel loss in order to
integrate the output of translation into the target modality. This leads to an
unsupervised method requiring no ground-truth deformation or pairs of aligned
images for training. We evaluate four variants of our approach on the public
Learn2Reg 2021 datasets \cite{hering2021learn2reg}. The experimental results
demonstrate that the proposed architecture achieves state-of-the-art
performance. Our code is available at https://github.com/heyblackC/DFMIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Articulated Objects in Free-form Hand Interaction. (arXiv:2204.13662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13662">
<div class="article-summary-box-inner">
<span><p>We use our hands to interact with and to manipulate objects. Articulated
objects are especially interesting since they often require the full dexterity
of human hands to manipulate them. To understand, model, and synthesize such
interactions, automatic and robust methods that reconstruct hands and
articulated objects in 3D from a color image are needed. Existing methods for
estimating 3D hand and object pose from images focus on rigid objects. In part,
because such methods rely on training data and no dataset of articulated object
manipulation exists. Consequently, we introduce ARCTIC - the first dataset of
free-form interactions of hands and articulated objects. ARCTIC has 1.2M images
paired with accurate 3D meshes for both hands and for objects that move and
deform over time. The dataset also provides hand-object contact information. To
show the value of our dataset, we perform two novel tasks on ARCTIC: (1) 3D
reconstruction of two hands and an articulated object in interaction; (2) an
estimation of dense hand-object relative distances, which we call interaction
field estimation. For the first task, we present ArcticNet, a baseline method
for the task of jointly reconstructing two hands and an articulated object from
an RGB image. For interaction field estimation, we predict the relative
distances from each hand vertex to the object surface, and vice versa. We
introduce InterField, the first method that estimates such distances from a
single RGB image. We provide qualitative and quantitative experiments for both
tasks, and provide detailed analysis on the data. Code and data will be
available at https://arctic.is.tue.mpg.de.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Simulation, Perception, and Generation of Human Behavior. (arXiv:2204.13678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13678">
<div class="article-summary-box-inner">
<span><p>Understanding and modeling human behavior is fundamental to almost any
computer vision and robotics applications that involve humans. In this thesis,
we take a holistic approach to human behavior modeling and tackle its three
essential aspects -- simulation, perception, and generation. Throughout the
thesis, we show how the three aspects are deeply connected and how utilizing
and improving one aspect can greatly benefit the other aspects. We also discuss
the lessons learned and our vision for what is next for human behavior
modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KING: Generating Safety-Critical Driving Scenarios for Robust Imitation via Kinematics Gradients. (arXiv:2204.13683v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13683">
<div class="article-summary-box-inner">
<span><p>Simulators offer the possibility of safe, low-cost development of
self-driving systems. However, current driving simulators exhibit na\"ive
behavior models for background traffic. Hand-tuned scenarios are typically
added during simulation to induce safety-critical situations. An alternative
approach is to adversarially perturb the background traffic trajectories. In
this paper, we study this approach to safety-critical driving scenario
generation using the CARLA simulator. We use a kinematic bicycle model as a
proxy to the simulator's true dynamics and observe that gradients through this
proxy model are sufficient for optimizing the background traffic trajectories.
Based on this finding, we propose KING, which generates safety-critical driving
scenarios with a 20% higher success rate than black-box optimization. By
solving the scenarios generated by KING using a privileged rule-based expert
algorithm, we obtain training data for an imitation learning policy. After
fine-tuning on this new data, we show that the policy becomes better at
avoiding collisions. Importantly, our generated data leads to reduced
collisions on both held-out scenarios generated via KING as well as traditional
hand-crafted scenarios, demonstrating improved robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling. (arXiv:2204.13686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13686">
<div class="article-summary-box-inner">
<span><p>4D human sensing and modeling are fundamental tasks in vision and graphics
with numerous applications. With the advances of new sensors and algorithms,
there is an increasing demand for more versatile datasets. In this work, we
contribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human
subjects, 400k sequences and 60M frames. HuMMan has several appealing
properties: 1) multi-modal data and annotations including color images, point
clouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile
device is included in the sensor suite; 3) a set of 500 actions, designed to
cover fundamental movements; 4) multiple tasks such as action recognition, pose
estimation, parametric human recovery, and textured mesh reconstruction are
supported and evaluated. Extensive experiments on HuMMan voice the need for
further study on challenges such as fine-grained action recognition, dynamic
human mesh reconstruction, point cloud-based parametric human recovery, and
cross-device domain gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeurMiPs: Neural Mixture of Planar Experts for View Synthesis. (arXiv:2204.13696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13696">
<div class="article-summary-box-inner">
<span><p>We present Neural Mixtures of Planar Experts (NeurMiPs), a novel planar-based
scene representation for modeling geometry and appearance. NeurMiPs leverages a
collection of local planar experts in 3D space as the scene representation.
Each planar expert consists of the parameters of the local rectangular shape
representing geometry and a neural radiance field modeling the color and
opacity. We render novel views by calculating ray-plane intersections and
composite output colors and densities at intersected points to the image.
NeurMiPs blends the efficiency of explicit mesh rendering and flexibility of
the neural radiance field. Experiments demonstrate superior performance and
speed of our proposed method, compared to other 3D representations in novel
view synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing. (arXiv:2004.13316v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13316">
<div class="article-summary-box-inner">
<span><p>Small and cluttered objects are common in real-world which are challenging
for detection. The difficulty is further pronounced when the objects are
rotated, as traditional detectors often routinely locate the objects in
horizontal bounding box such that the region of interest is contaminated with
background or nearby interleaved objects. In this paper, we first innovatively
introduce the idea of denoising to object detection. Instance-level denoising
on the feature map is performed to enhance the detection to small and cluttered
objects. To handle the rotation variation, we also add a novel IoU constant
factor to the smooth L1 loss to address the long standing boundary problem,
which to our analysis, is mainly caused by the periodicity of angular (PoA) and
exchangeability of edges (EoE). By combing these two features, our proposed
detector is termed as SCRDet++. Extensive experiments are performed on large
aerial images public datasets DOTA, DIOR, UCAS-AOD as well as natural image
dataset COCO, scene text dataset ICDAR2015, small traffic light dataset BSTLD
and our released S$^2$TLD by this paper. The results show the effectiveness of
our approach. The released dataset S2TLD is made public available, which
contains 5,786 images with 14,130 traffic light instances across five
categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Machine Learning for Particle Track Identification in the CLAS12 Detector. (arXiv:2008.12860v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12860">
<div class="article-summary-box-inner">
<span><p>Particle track reconstruction is the most computationally intensive process
in nuclear physics experiments. Traditional algorithms use a combinatorial
approach that exhaustively tests track measurements ("hits") to identify those
that form an actual particle trajectory. In this article, we describe the
development of four machine learning (ML) models that assist the tracking
algorithm by identifying valid track candidates from the measurements in drift
chambers. Several types of machine learning models were tested, including:
Convolutional Neural Networks (CNN), Multi-Layer Perceptrons (MLP), Extremely
Randomized Trees (ERT) and Recurrent Neural Networks (RNN). As a result of this
work, an MLP network classifier was implemented as part of the CLAS12
reconstruction software to provide the tracking code with recommended track
candidates. The resulting software achieved accuracy of greater than 99\% and
resulted in an end-to-end speedup of 35\% compared to existing algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Forensics via An Adversarial Game. (arXiv:2103.13567v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13567">
<div class="article-summary-box-inner">
<span><p>With the progress in AI-based facial forgery (i.e., deepfake), people are
increasingly concerned about its abuse. Albeit effort has been made for
training classification (also known as deepfake detection) models to recognize
such forgeries, existing models suffer from poor generalization to unseen
forgery technologies and high sensitivity to changes in image/video quality. In
this paper, we advocate adversarial training for improving the generalization
ability to both unseen facial forgeries and unseen image/video qualities. We
believe training with samples that are adversarially crafted to attack the
classification models improves the generalization ability considerably.
Considering that AI-based face manipulation often leads to high-frequency
artifacts that can be easily spotted by models yet difficult to generalize, we
further propose a new adversarial training method that attempts to blur out
these specific artifacts, by introducing pixel-wise Gaussian blurring models.
With adversarial training, the classification models are forced to learn more
discriminative and generalizable features, and the effectiveness of our method
can be verified by plenty of empirical evidence. Our code will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransCenter: Transformers with Dense Representations for Multiple-Object Tracking. (arXiv:2103.15145v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15145">
<div class="article-summary-box-inner">
<span><p>Transformers have proven superior performance for a wide variety of tasks
since they were introduced, which has drawn in recent years the attention of
the vision community where efforts were made such as image classification and
object detection. Despite this wave, building an accurate and efficient
multiple-object tracking (MOT) method with transformers is not a trivial task.
We argue that the direct application of a transformer architecture with
quadratic complexity and insufficient noise-initialized sparse queries -- is
not optimal for MOT. Inspired by recent research, we propose TransCenter, a
transformer-based MOT architecture with dense representations for accurately
tracking all the objects while keeping a reasonable runtime. Methodologically,
we propose the use of dense image-related multi-scale detection queries
produced by an efficient transformer architecture. The queries allow inferring
targets' locations globally and robustly from dense heatmap outputs. In
parallel, a set of efficient sparse tracking queries interacting with image
features in the TransCenter Decoder to associate object positions through time.
TransCenter exhibits remarkable performance improvements and outperforms by a
large margin the current state-of-the-art in two standard MOT benchmarks with
two tracking (public/private) settings. The proposed efficient and accurate
transformer architecture for MOT is proven with an extensive ablation study,
demonstrating its advantage compared to more naive alternatives and concurrent
works. The code will be made publicly available at
https://github.com/yihongxu/transcenter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00394">
<div class="article-summary-box-inner">
<span><p>Graph matching is an important problem that has received widespread
attention, especially in the field of computer vision. Recently,
state-of-the-art methods seek to incorporate graph matching with deep learning.
However, there is no research to explain what role the graph matching algorithm
plays in the model. Therefore, we propose an approach integrating a MILP
formulation of the graph matching problem. This formulation is solved to
optimal and it provides inherent baseline. Meanwhile, similar approaches are
derived by releasing the optimal guarantee of the graph matching solver and by
introducing a quality level. This quality level controls the quality of the
solutions provided by the graph matching solver. In addition, several
relaxations of the graph matching problem are put to the test. Our experimental
evaluation gives several theoretical insights and guides the direction of deep
graph matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Transport for Unsupervised Denoising Learning. (arXiv:2108.02574v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02574">
<div class="article-summary-box-inner">
<span><p>Recently, much progress has been made in unsupervised denoising learning.
However, existing methods more or less rely on some assumptions on the signal
and/or degradation model, which limits their practical performance. How to
construct an optimal criterion for unsupervised denoising learning without any
prior knowledge on the degradation model is still an open question. Toward
answering this question, this work proposes a criterion for unsupervised
denoising learning based on the optimal transport theory. This criterion has
favorable properties, e.g., approximately maximal preservation of the
information of the signal, whilst achieving perceptual reconstruction.
Furthermore, though a relaxed unconstrained formulation is used in practical
implementation, we prove that the relaxed formulation in theory has the same
solution as the original constrained formulation. Experiments on synthetic and
real-world data, including realistic photographic, microscopy, depth, and raw
depth images, demonstrate that the proposed method even compares favorably with
supervised methods, e.g., approaching the PSNR of supervised methods while
having better perceptual quality. Particularly, for spatially correlated noise
and realistic microscopy images, the proposed method not only achieves better
perceptual quality but also has higher PSNR than supervised methods. Besides,
it shows remarkable superiority in harsh practical conditions with complex
noise, e.g., raw depth images. Code is available at
https://github.com/wangweiSJTU/OTUR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NPBDREG: Uncertainty Assessment in Diffeomorphic Brain MRI Registration using a Non-parametric Bayesian Deep-Learning Based Approach. (arXiv:2108.06771v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06771">
<div class="article-summary-box-inner">
<span><p>Quantification of uncertainty in deep-neural-networks (DNN) based image
registration algorithms plays a critical role in the deployment of image
registration algorithms for clinical applications such as surgical planning,
intraoperative guidance, and longitudinal monitoring of disease progression or
treatment efficacy as well as in research-oriented processing pipelines.
Currently available approaches for uncertainty estimation in DNN-based image
registration algorithms may result in sub-optimal clinical decision making due
to potentially inaccurate estimation of the uncertainty of the registration
stems for the assumed parametric distribution of the registration latent space.
We introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty
estimation in DNN-based deformable image registration by combining an Adam
optimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the
underlying posterior distribution through posterior sampling. Thus, it has the
potential to provide uncertainty estimates that are highly correlated with the
presence of out of distribution data. We demonstrated the added-value of
NPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on
brain MRI image registration using $390$ image pairs from four publicly
available databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a
better correlation of the predicted uncertainty with out-of-distribution data
($r&gt;0.95$ vs. $r&lt;0.5$) as well as a 7.3%improvement in the registration
accuracy (Dice score, $0.74$ vs. $0.69$, $p \ll 0.01$), and 18% improvement in
registration smoothness (percentage of folds in the deformation field, 0.014
vs. 0.017, $p \ll 0.01$). Finally, NPBDREG demonstrated a better generalization
capability for data corrupted by a mixed structure noise (Dice score of $0.73$
vs. $0.69$, $p \ll 0.01$) compared to the baseline PrVXM approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Image Segmentation with 3D Convolutional Neural Networks: A Survey. (arXiv:2108.08467v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08467">
<div class="article-summary-box-inner">
<span><p>Computer-aided medical image analysis plays a significant role in assisting
medical practitioners for expert clinical diagnosis and deciding the optimal
treatment plan. At present, convolutional neural networks (CNN) are the
preferred choice for medical image analysis. In addition, with the rapid
advancements in three-dimensional (3D) imaging systems and the availability of
excellent hardware and software support to process large volumes of data, 3D
deep learning methods are gaining popularity in medical image analysis. Here,
we present an extensive review of the recently evolved 3D deep learning methods
in medical image segmentation. Furthermore, the research gaps and future
directions in 3D medical image segmentation are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11845">
<div class="article-summary-box-inner">
<span><p>This letter is concerned with image classification with deep convolutional
neural networks (CNNs). The focus is on the following question: given a set of
candidate CNN models, how to select the right one with the best generalization
property for the current task? Present model selection methods require access
to a batch of labeled data for computing a pre-specified performance metric,
such as the cross-entropy loss, the classification error rate, the negative
log-likelihood. In many practical cases, labels are not available in time as
labeling itself is a time-consuming and expensive task. To this end, this
letter presents an approach to CNN model selection using only unlabeled data.
This method is developed based on a principle termed consistent relative
confidence. The effectiveness and efficiency of the proposed method are
demonstrated by experiments using benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control. (arXiv:2110.01052v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01052">
<div class="article-summary-box-inner">
<span><p>We introduce a framework for calibrating machine learning models so that
their predictions satisfy explicit, finite-sample statistical guarantees. Our
calibration algorithm works with any underlying model and (unknown)
data-generating distribution and does not require model refitting. The
framework addresses, among other examples, false discovery rate control in
multi-label classification, intersection-over-union control in instance
segmentation, and the simultaneous control of the type-1 error of outlier
detection and confidence set coverage in classification or regression. Our main
insight is to reframe the risk-control problem as multiple hypothesis testing,
enabling techniques and mathematical arguments different from those in the
previous literature. We use our framework to provide new calibration methods
for several core machine learning tasks with detailed worked examples in
computer vision and tabular medical data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond). (arXiv:2110.05922v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05922">
<div class="article-summary-box-inner">
<span><p>"The power of a generalization system follows directly from its biases"
(Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems --
but to what degree have we understood how their inductive bias influences model
decisions? We here attempt to disentangle the various aspects that determine
how a model decides. In particular, we ask: what makes one model decide
differently from another? In a meticulously controlled setting, we find that
(1.) irrespective of the network architecture or objective (e.g.
self-supervised, semi-supervised, vision transformers, recurrent models) all
models end up with a similar decision boundary. (2.) To understand these
findings, we analysed model decisions on the ImageNet validation set from epoch
to epoch and image by image. We find that the ImageNet validation set, among
others, suffers from dichotomous data difficulty (DDD): For the range of
investigated models and their accuracies, it is dominated by 46.0% "trivial"
and 11.5% "impossible" images (beyond label errors). Only 42.5% of the images
could possibly be responsible for the differences between two models' decision
boundaries. (3.) Only removing the "impossible" and "trivial" images allows us
to see pronounced differences between models. (4.) Humans are highly accurate
at predicting which images are "trivial" and "impossible" for CNNs (81.4%).
This implies that in future comparisons of brains, machines and behaviour, much
may be gained from investigating the decisive role of images and the
distribution of their difficulties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Expressivity and Trainability of Quadratic Networks. (arXiv:2110.06081v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06081">
<div class="article-summary-box-inner">
<span><p>Inspired by the diversity of biological neurons, quadratic artificial neurons
can play an important role in deep learning models. The type of quadratic
neurons of our interest replaces the inner-product operation in the
conventional neuron with a quadratic function. Despite promising results so far
achieved by networks of quadratic neurons, there are important issues not well
addressed. Theoretically, the superior expressivity of a quadratic network over
either a conventional network or a conventional network via quadratic
activation is not fully elucidated, which makes the use of quadratic networks
not well grounded. Practically, although a quadratic network can be trained via
generic backpropagation, it can be subject to a higher risk of collapse than
the conventional counterpart. To address these issues, we first apply the
spline theory and a measure from algebraic geometry to give two theorems that
demonstrate better model expressivity of a quadratic network than the
conventional counterpart with or without quadratic activation. Then, we propose
an effective and efficient training strategy referred to as ReLinear to
stabilize the training process of a quadratic network, thereby unleashing the
full potential in its associated machine learning tasks. Comprehensive
experiments on popular datasets are performed to support our findings and
evaluate the performance of quadratic deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPTS: Single-Point Text Spotting. (arXiv:2112.07917v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07917">
<div class="article-summary-box-inner">
<span><p>Existing scene text spotting (i.e., end-to-end text detection and
recognition) methods rely on costly bounding box annotations (e.g., text-line,
word-level, or character-level bounding boxes). For the first time, we
demonstrate that training scene text spotting models can be achieved with an
extremely low-cost annotation of a single-point for each instance. We propose
an end-to-end scene text spotting method that tackles scene text spotting as a
sequence prediction task. Given an image as input, we formulate the desired
detection and recognition results as a sequence of discrete tokens and use an
auto-regressive Transformer to predict the sequence. The proposed method is
simple yet effective, which can achieve state-of-the-art results on widely used
benchmarks. Most significantly, we show that the performance is not very
sensitive to the positions of the point annotation, meaning that it can be much
easier to be annotated or even be automatically generated than the bounding box
that requires precise positions. We believe that such a pioneer attempt
indicates a significant opportunity for scene text spotting applications of a
much larger scale than previously possible. The code will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Geometry-aware 3D Generative Adversarial Networks. (arXiv:2112.07945v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07945">
<div class="article-summary-box-inner">
<span><p>Unsupervised generation of high-quality multi-view-consistent images and 3D
shapes using only collections of single-view 2D photographs has been a
long-standing challenge. Existing 3D GANs are either compute-intensive or make
approximations that are not 3D-consistent; the former limits quality and
resolution of the generated images and the latter adversely affects multi-view
consistency and shape quality. In this work, we improve the computational
efficiency and image quality of 3D GANs without overly relying on these
approximations. We introduce an expressive hybrid explicit-implicit network
architecture that, together with other design choices, synthesizes not only
high-resolution multi-view-consistent images in real time but also produces
high-quality 3D geometry. By decoupling feature generation and neural
rendering, our framework is able to leverage state-of-the-art 2D CNN
generators, such as StyleGAN2, and inherit their efficiency and expressiveness.
We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats,
among other experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2112.14025v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14025">
<div class="article-summary-box-inner">
<span><p>Clustering-based unsupervised domain adaptive (UDA) person re-identification
(ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature
embedding and imperfect clustering, pseudo labels for target domain data
inherently contain an unknown proportion of wrong ones, which would mislead
feature learning. In this paper, we propose an approach named probabilistic
uncertainty guided progressive label refinery (P$^2$LR) for domain adaptive
person re-identification. First, we propose to model the labeling uncertainty
with the probabilistic distance along with ideal single-peak distributions. A
quantitative criterion is established to measure the uncertainty of pseudo
labels and facilitate the network training. Second, we explore a progressive
strategy for refining pseudo labels. With the uncertainty-guided alternative
optimization, we balance between the exploration of target domain data and the
negative effects of noisy labeling. On top of a strong baseline, we obtain
significant improvements and achieve the state-of-the-art performance on four
UDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5%
mAP on the Duke2Market task, while surpassing the state-of-the-art method by
2.5% mAP on the Market2MSMT task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDS-Net: A Multi-scale Depth Stratification Based Monocular 3D Object Detection Algorithm. (arXiv:2201.04341v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04341">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is very challenging in autonomous driving due
to the lack of depth information. This paper proposes a one-stage monocular 3D
object detection algorithm based on multi-scale depth stratification, which
uses the anchor-free method to detect 3D objects in a per-pixel prediction. In
the proposed MDS-Net, a novel depth-based stratification structure is developed
to improve the network's ability of depth prediction by establishing
mathematical models between depth and image size of objects. A new angle loss
function is then developed to further improve the accuracy of the angle
prediction and increase the convergence speed of training. An optimized
soft-NMS is finally applied in the post-processing stage to adjust the
confidence of candidate boxes. Experiments on the KITTI benchmark show that the
MDS-Net outperforms the existing monocular 3D detection methods in 3D detection
and BEV detection tasks while fulfilling real-time requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention. (arXiv:2203.03937v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03937">
<div class="article-summary-box-inner">
<span><p>Recently, Transformers have shown promising performance in various vision
tasks. To reduce the quadratic computation complexity caused by each query
attending to all keys/values, various methods have constrained the range of
attention within local regions, where each query only attends to keys/values
within a hand-crafted window. However, these hand-crafted window partition
mechanisms are data-agnostic and ignore their input content, so it is likely
that one query maybe attends to irrelevant keys/values. To address this issue,
we propose a Dynamic Group Attention (DG-Attention), which dynamically divides
all queries into multiple groups and selects the most relevant keys/values for
each group. Our DG-Attention can flexibly model more relevant dependencies
without any spatial constraint that is used in hand-crafted window based
attention. Built on the DG-Attention, we develop a general vision transformer
backbone named Dynamic Group Transformer (DGT). Extensive experiments show that
our models can outperform the state-of-the-art methods on multiple common
vision tasks, including image classification, semantic segmentation, object
detection, and instance segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evolutionary Neural Cascade Search across Supernetworks. (arXiv:2203.04011v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04011">
<div class="article-summary-box-inner">
<span><p>To achieve excellent performance with modern neural networks, having the
right network architecture is important. Neural Architecture Search (NAS)
concerns the automatic discovery of task-specific network architectures. Modern
NAS approaches leverage supernetworks whose subnetworks encode candidate neural
network architectures. These subnetworks can be trained simultaneously,
removing the need to train each network from scratch, thereby increasing the
efficiency of NAS. A recent method called Neural Architecture Transfer (NAT)
further improves the efficiency of NAS for computer vision tasks by using a
multi-objective evolutionary algorithm to find high-quality subnetworks of a
supernetwork pretrained on ImageNet. Building upon NAT, we introduce ENCAS -
Evolutionary Neural Cascade Search. ENCAS can be used to search over multiple
pretrained supernetworks to achieve a trade-off front of cascades of different
neural network architectures, maximizing accuracy while minimizing FLOPs count.
We test ENCAS on common computer vision benchmarks (CIFAR-10, CIFAR-100,
ImageNet) and achieve Pareto dominance over previous state-of-the-art NAS
models up to 1.5 GFLOPs. Additionally, applying ENCAS to a pool of 518 publicly
available ImageNet classifiers leads to Pareto dominance in all computation
regimes and to increasing the maximum accuracy from 88.6% to 89.0%, accompanied
by an 18\% decrease in computation effort from 362 to 296 GFLOPs. Our code is
available at https://github.com/AwesomeLemon/ENCAS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP). (arXiv:2203.04180v3 [math.NA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04180">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance Imaging (MRI) has excellent soft tissue contrast but is
hindered by an inherently slow data acquisition process. Compressed sensing,
which reconstructs sparse signals from incoherently sampled data, has been
widely applied to accelerate MRI acquisitions. Compressed sensing MRI requires
one or more model parameters to be tuned, which is usually done by hand, giving
sub-optimal tuning in general. To address this issue, we build on previous work
by the authors on the single-coil Variable Density Approximate Message Passing
(VDAMP) algorithm, extending the framework to multiple receiver coils to
propose the Parallel VDAMP (P-VDAMP) algorithm. For Bernoulli random variable
density sampling, P-VDAMP obeys a "state evolution", where the intermediate
per-iteration image estimate is distributed according to the ground truth
corrupted by a zero-mean Gaussian vector with approximately known covariance.
To our knowledge, P-VDAMP is the first algorithm for multi-coil MRI data that
obeys a state evolution with accurately tracked parameters. We leverage state
evolution to automatically tune sparse parameters on-the-fly with Stein's
Unbiased Risk Estimate (SURE). P-VDAMP is evaluated on brain, knee and
angiogram datasets and compared with four variants of the Fast Iterative
Shrinkage-Thresholding algorithm (FISTA), including two tuning-free variants
from the literature. The proposed method is found to have a similar
reconstruction quality and time to convergence as FISTA with an optimally tuned
sparse weighting and offers substantial robustness and reconstruction quality
improvements over competing tuning-free methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Bracket High Dynamic Range Imaging with Event Cameras. (arXiv:2203.06622v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06622">
<div class="article-summary-box-inner">
<span><p>Modern high dynamic range (HDR) imaging pipelines align and fuse multiple low
dynamic range (LDR) images captured at different exposure times. While these
methods work well in static scenes, dynamic scenes remain a challenge since the
LDR images still suffer from saturation and noise. In such scenarios, event
cameras would be a valid complement, thanks to their higher temporal resolution
and dynamic range. In this paper, we propose the first multi-bracket HDR
pipeline combining a standard camera with an event camera. Our results show
better overall robustness when using events, with improvements in PSNR by up to
5dB on synthetic data and up to 0.7dB on real-world data. We also introduce a
new dataset containing bracketed LDR images with aligned events and HDR ground
truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partitioning Image Representation in Contrastive Learning. (arXiv:2203.10454v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10454">
<div class="article-summary-box-inner">
<span><p>In contrastive learning in the image domain, the anchor and positive samples
are forced to have as close representations as possible. However, forcing the
two samples to have the same representation could be misleading because the
data augmentation techniques make the two samples different. In this paper, we
introduce a new representation, partitioned representation, which can learn
both common and unique features of the anchor and positive samples in
contrastive learning. The partitioned representation consists of two parts: the
content part and the style part. The content part represents common features of
the class, and the style part represents the own features of each sample, which
can lead to the representation of the data augmentation method. We can achieve
the partitioned representation simply by decomposing a loss function of
contrastive learning into two terms on the two separate representations,
respectively. To evaluate our representation with two parts, we take two
framework models: Variational AutoEncoder (VAE) and BootstrapYour Own
Latent(BYOL) to show the separability of content and style, and to confirm the
generalization ability in classification, respectively. Based on the
experiments, we show that our approach can separate two types of information in
the VAE framework and outperforms the conventional BYOL in linear separability
and a few-shot learning task as downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient-VDVAE: Less is more. (arXiv:2203.13751v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13751">
<div class="article-summary-box-inner">
<span><p>Hierarchical VAEs have emerged in recent years as a reliable option for
maximum likelihood estimation. However, instability issues and demanding
computational requirements have hindered research progress in the area. We
present simple modifications to the Very Deep VAE to make it converge up to
$2.6\times$ faster, save up to $20\times$ in memory load and improve stability
during training. Despite these changes, our models achieve comparable or better
negative log-likelihood performance than current state-of-the-art models on all
$7$ commonly used image datasets we evaluated on. We also make an argument
against using 5-bit benchmarks as a way to measure hierarchical VAE's
performance due to undesirable biases caused by the 5-bit quantization.
Additionally, we empirically demonstrate that roughly $3\%$ of the hierarchical
VAE's latent space dimensions is sufficient to encode most of the image
information, without loss of performance, opening up the doors to efficiently
leverage the hierarchical VAEs' latent space in downstream tasks. We release
our source code and models at https://github.com/Rayhane-mamah/Efficient-VDVAE .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Sentinel-2 multi-year, multi-country benchmark dataset for crop classification and segmentation with deep learning. (arXiv:2204.00951v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00951">
<div class="article-summary-box-inner">
<span><p>In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi
country benchmark dataset, tailored for agricultural monitoring applications
with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer
declarations collected via the Land Parcel Identification System (LPIS) for
harmonizing country wide labels. These declarations have only recently been
made available as open data, allowing for the first time the labeling of
satellite imagery from ground truth data. We proceed to propose and standardise
a new crop type taxonomy across Europe that address Common Agriculture Policy
(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative
Crop Classification scheme. Sen4AgriNet is the only multi-country, multi-year
dataset that includes all spectral information. It is constructed to cover the
period 2016-2020 for Catalonia and France, while it can be extended to include
additional countries. Currently, it contains 42.5 million parcels, which makes
it significantly larger than other available archives. We extract two
sub-datasets to highlight its value for diverse Deep Learning applications; the
Object Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD
capitalizes zonal statistics of each parcel, thus creating a powerful
label-to-features instance for classification algorithms. On the other hand,
PAD structure generalizes the classification problem to parcel extraction and
semantic segmentation and labeling. The PAD and OAD are examined under three
different scenarios to showcase and model the effects of spatial and temporal
variability across different years and different countries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refining time-space traffic diagrams: A multiple linear regression model. (arXiv:2204.04457v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04457">
<div class="article-summary-box-inner">
<span><p>A time-space traffic (TS) diagram, which presents traffic states in
time-space cells with color, is an important traffic analysis and visualization
tool. Despite its importance for transportation research and engineering, most
TS diagrams that have already existed or are being produced are too coarse to
exhibit detailed traffic dynamics due to the limitations of existing
information technology and traffic infrastructure investment. To increase the
resolution of a TS diagram and enable it to present ample traffic details, this
paper introduces the TS diagram refinement problem and proposes a multiple
linear regression-based model to solve the problem. Two tests, which attempt to
increase the resolution of a TS diagram 4 and 16 times, are carried out to
evaluate the performance of the proposed model. Data collected at different
times, in different locations and even in different countries are employed to
thoroughly evaluate the accuracy and transferability of the proposed model.
Strict tests with diverse data show that the proposed model, despite its
simplicity, is able to refine a TS diagram with promising accuracy and reliable
transferability. The proposed refinement model will "save" widely existing TS
diagrams from their blurry "faces" and enable TS diagrams to show more traffic
details.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration. (arXiv:2204.08058v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08058">
<div class="article-summary-box-inner">
<span><p>Multimodal video-audio-text understanding and generation can benefit from
datasets that are narrow but rich. The narrowness allows bite-sized challenges
that the research community can make progress on. The richness ensures we are
making progress along the core challenges. To this end, we present a
large-scale video-audio-text dataset MUGEN, collected using the open-sourced
platform game CoinRun [11]. We made substantial modifications to make the game
richer by introducing audio and enabling new interactions. We trained RL agents
with different objectives to navigate the game and interact with 13 objects and
characters. This allows us to automatically extract a large collection of
diverse videos and associated audio. We sample 375K video clips (3.2s each) and
collect text descriptions from human annotators. Each video has additional
annotations that are extracted automatically from the game engine, such as
accurate semantic maps for each frame and templated textual descriptions.
Altogether, MUGEN can help progress research in many tasks in multimodal
understanding and generation. We benchmark representative approaches on tasks
involving video-audio-text retrieval and generation. Our dataset and code are
released at: https://mugen-org.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09826">
<div class="article-summary-box-inner">
<span><p>Recent advances in skeleton-based person re-identification (re-ID) obtain
impressive performance via either hand-crafted skeleton descriptors or skeleton
representation learning with deep learning paradigms. However, they typically
require skeletal pre-modeling and label information for training, which leads
to limited applicability of these methods. In this paper, we focus on
unsupervised skeleton-based person re-ID, and present a generic Simple Masked
Contrastive learning (SimMC) framework to learn effective representations from
unlabeled 3D skeletons for person re-ID. Specifically, to fully exploit
skeleton features within each skeleton sequence, we first devise a masked
prototype contrastive learning (MPC) scheme to cluster the most typical
skeleton features (skeleton prototypes) from different subsequences randomly
masked from raw sequences, and contrast the inherent similarity between
skeleton features and different prototypes to learn discriminative skeleton
representations without using any label. Then, considering that different
subsequences within the same sequence usually enjoy strong correlations due to
the nature of motion continuity, we propose the masked intra-sequence
contrastive learning (MIC) to capture intra-sequence pattern consistency
between subsequences, so as to encourage learning more effective skeleton
representations for person re-ID. Extensive experiments validate that the
proposed SimMC outperforms most state-of-the-art skeleton-based methods. We
further show its scalability and efficiency in enhancing the performance of
existing models. Our codes are available at https://github.com/Kali-Hac/SimMC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10496">
<div class="article-summary-box-inner">
<span><p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with
carefully curated vision-language datasets. While these datasets reach an order
of 10 million samples, the labor cost is prohibitive to scale further.
Conversely, unimodal encoders are pretrained with simpler annotations that are
less cost-prohibitive, achieving scales of hundreds of millions to billions. As
a result, unimodal encoders have achieved state-of-art (SOTA) on many
downstream tasks. However, challenges remain when applying to VL tasks. The
pretraining data is not optimal for cross-modal architectures and requires
heavy computational resources. In addition, unimodal architectures lack
cross-modal interactions that have demonstrated significant benefits for VL
tasks. Therefore, how to best leverage pretrained unimodal encoders for VL
tasks is still an area of active research. In this work, we propose a method to
leverage unimodal vision and text encoders for VL tasks that augment existing
VL approaches while conserving computational complexity. Specifically, we
propose Multimodal Adaptive Distillation (MAD), which adaptively distills
useful knowledge from pretrained encoders to cross-modal VL encoders. Second,
to better capture nuanced impacts on VL task performance, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data constraints and conditions of domain shift. Experiments demonstrate that
MAD leads to consistent gains in the low-shot, domain-shifted, and
fully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA
performance on VCR compared to other single models pretrained with image-text
data. Finally, MAD outperforms concurrent works utilizing pretrained vision
encoder from CLIP. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Pretraining Framework for Document Understanding. (arXiv:2204.10939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10939">
<div class="article-summary-box-inner">
<span><p>Document intelligence automates the extraction of information from documents
and supports many business applications. Recent self-supervised learning
methods on large-scale unlabeled document datasets have opened up promising
directions towards reducing annotation efforts by training models with
self-supervised objectives. However, most of the existing document pretraining
methods are still language-dominated. We present UDoc, a new unified
pretraining framework for document understanding. UDoc is designed to support
most document understanding tasks, extending the Transformer to take multimodal
embeddings as input. Each input element is composed of words and visual
features from a semantic region of the input document image. An important
feature of UDoc is that it learns a generic representation by making use of
three self-supervised losses, encouraging the representation to model
sentences, learn similarities, and align modalities. Extensive empirical
analysis demonstrates that the pretraining procedure learns better joint
representations and leads to improvements in downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Unsupervised Industrial Anomaly Detection Algorithms. (arXiv:2204.11161v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11161">
<div class="article-summary-box-inner">
<span><p>In line with the development of Industry 4.0, more and more attention is
attracted to the field of surface defect detection. Improving efficiency as
well as saving labor costs has steadily become a matter of great concern in
industry field, where deep learning-based algorithms performs better than
traditional vision inspection methods in recent years. While existing deep
learning-based algorithms are biased towards supervised learning, which not
only necessitates a huge amount of labeled data and a significant amount of
labor, but it is also inefficient and has certain limitations. In contrast,
recent research shows that unsupervised learning has great potential in
tackling above disadvantages for visual anomaly detection. In this survey, we
summarize current challenges and provide a thorough overview of recently
proposed unsupervised algorithms for visual anomaly detection covering five
categories, whose innovation points and frameworks are described in detail.
Meanwhile, information on publicly available datasets containing surface image
samples are provided. By comparing different classes of methods, the advantages
and disadvantages of anomaly detection algorithms are summarized. It is
expected to assist both the research community and industry in developing a
broader and cross-domain perspective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11479">
<div class="article-summary-box-inner">
<span><p>While efficient architectures and a plethora of augmentations for end-to-end
image classification tasks have been suggested and heavily investigated,
state-of-the-art techniques for audio classifications still rely on numerous
representations of the audio signal together with large architectures,
fine-tuned from large datasets. By utilizing the inherited lightweight nature
of audio and novel audio augmentations, we were able to present an efficient
end-to-end network with strong generalization ability. Experiments on a variety
of sound classification sets demonstrate the effectiveness and robustness of
our approach, by achieving state-of-the-art results in various settings. Public
code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint-Modal Label Denoising for Weakly-Supervised Audio-Visual Video Parsing. (arXiv:2204.11573v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11573">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the weakly-supervised audio-visual video parsing task,
which aims to recognize all events belonging to each modality and localize
their temporal boundaries. This task is challenging because only overall labels
indicating the video events are provided for training. However, an event might
be labeled but not appear in one of the modalities, which results in a
modality-specific noisy label problem. Motivated by two observations that
networks tend to learn clean samples first and that a labeled event would
appear in at least one modality, we propose a training strategy to identify and
remove modality-specific noisy labels dynamically. Specifically, we sort the
losses of all instances within a mini-batch individually in each modality, then
select noisy samples according to relationships between intra-modal and
inter-modal losses. Besides, we also propose a simple but valid noise ratio
estimation method by calculating the proportion of instances whose confidence
is below a preset threshold. Our method makes large improvements over the
previous state of the arts (e.g., from 60.0% to 63.8% in segment-level visual
metric), which demonstrates the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Logit Adjustment. (arXiv:2204.11822v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11822">
<div class="article-summary-box-inner">
<span><p>Semantic-descriptor-based Generalized Zero-Shot Learning (GZSL) poses
challenges in recognizing novel classes in the test phase. The development of
generative models enables current GZSL techniques to probe further into the
semantic-visual link, culminating in a two-stage form that includes a generator
and a classifier. However, existing generation-based methods focus on enhancing
the generator's effect while neglecting the improvement of the classifier. In
this paper, we first analyze of two properties of the generated pseudo unseen
samples: bias and homogeneity. Then, we perform variational Bayesian inference
to back-derive the evaluation metrics, which reflects the balance of the seen
and unseen classes. As a consequence of our derivation, the aforementioned two
properties are incorporated into the classifier training as seen-unseen priors
via logit adjustment. The Zero-Shot Logit Adjustment further puts
semantic-based classifiers into effect in generation-based GZSL. Our
experiments demonstrate that the proposed technique achieves state-of-the-art
when combined with the basic generator, and it can improve various generative
zero-shot learning frameworks. Our codes are available on
https://github.com/cdb342/IJCAI-2022-ZLA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers. (arXiv:2204.12997v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12997">
<div class="article-summary-box-inner">
<span><p>Transformers are successfully applied to computer vision due to their
powerful modeling capacity with self-attention. However, the excellent
performance of transformers heavily depends on enormous training images. Thus,
a data-efficient transformer solution is urgently needed. In this work, we
propose an early knowledge distillation framework, which is termed as DearKD,
to improve the data efficiency required by transformers. Our DearKD is a
two-stage framework that first distills the inductive biases from the early
intermediate layers of a CNN and then gives the transformer full play by
training without distillation. Further, our DearKD can be readily applied to
the extreme data-free case where no real images are available. In this case, we
propose a boundary-preserving intra-divergence loss based on DeepInversion to
further close the performance gap against the full-data counterpart. Extensive
experiments on ImageNet, partial ImageNet, data-free setting and other
downstream tasks prove the superiority of DearKD over its baselines and
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-29 23:08:26.509455109 UTC">2022-04-29 23:08:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>