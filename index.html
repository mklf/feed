<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-21T01:30:00Z">01-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech. (arXiv:2201.07876v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07876">
<div class="article-summary-box-inner">
<span><p>The prediction of valence from speech is an important, but challenging
problem. The externalization of valence in speech has speaker-dependent cues,
which contribute to performances that are often significantly lower than the
prediction of other emotional attributes such as arousal and dominance. A
practical approach to improve valence prediction from speech is to adapt the
models to the target speakers in the test set. Adapting a speech emotion
recognition (SER) system to a particular speaker is a hard problem, especially
with deep neural networks (DNNs), since it requires optimizing millions of
parameters. This study proposes an unsupervised approach to address this
problem by searching for speakers in the train set with similar acoustic
patterns as the speaker in the test set. Speech samples from the selected
speakers are used to create the adaptation set. This approach leverages
transfer learning using pre-trained models, which are adapted with these speech
samples. We propose three alternative adaptation strategies: unique speaker,
oversampling and weighting approaches. These methods differ on the use of the
adaptation set in the personalization of the valence models. The results
demonstrate that a valence prediction model can be efficiently personalized
with these unsupervised approaches, leading to relative improvements as high as
13.52%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP). (arXiv:2201.07899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07899">
<div class="article-summary-box-inner">
<span><p>The American Sign Language Linguistic Research Project (ASLLRP) provides
Internet access to high-quality ASL video data, generally including front and
side views and a close-up of the face. The manual and non-manual components of
the signing have been linguistically annotated using SignStream(R). The
recently expanded video corpora can be browsed and searched through the Data
Access Interface (DAI 2) we have designed; it is possible to carry out complex
searches. The data from our corpora can also be downloaded; annotations are
available in an XML export format. We have also developed the ASLLRP Sign Bank,
which contains almost 6,000 sign entries for lexical signs, with distinct
English-based glosses, with a total of 41,830 examples of lexical signs (in
addition to about 300 gestures, over 1,000 fingerspelled signs, and 475
classifier examples). The Sign Bank is likewise accessible and searchable on
the Internet; it can also be accessed from within SignStream(R) (software to
facilitate linguistic annotation and analysis of visual language data) to make
annotations more accurate and efficient. Here we describe the available
resources. These data have been used for many types of research in linguistics
and in computer-based sign language recognition from video; examples of such
research are provided in the latter part of this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Machine Common Sense via Cloze Testing. (arXiv:2201.07902v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07902">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) show state of the art performance for common sense (CS)
question answering, but whether this ability implies a human-level mastery of
CS remains an open question. Understanding the limitations and strengths of LMs
can help researchers improve these models, potentially by developing novel ways
of integrating external CS knowledge. We devise a series of tests and
measurements to systematically quantify their performance on different aspects
of CS. We propose the use of cloze testing combined with word embeddings to
measure the LM's robustness and confidence. Our results show than although
language models tend to achieve human-like accuracy, their confidence is
subpar. Future work can leverage this information to build more complex
systems, such as an ensemble of symbolic and distributed knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPTAM: Constituency Parse Tree Aggregation Method. (arXiv:2201.07905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07905">
<div class="article-summary-box-inner">
<span><p>Diverse Natural Language Processing tasks employ constituency parsing to
understand the syntactic structure of a sentence according to a phrase
structure grammar. Many state-of-the-art constituency parsers are proposed, but
they may provide different results for the same sentences, especially for
corpora outside their training domains. This paper adopts the truth discovery
idea to aggregate constituency parse trees from different parsers by estimating
their reliability in the absence of ground truth. Our goal is to consistently
obtain high-quality aggregated constituency parse trees. We formulate the
constituency parse tree aggregation problem in two steps, structure aggregation
and constituent label aggregation. Specifically, we propose the first truth
discovery solution for tree structures by minimizing the weighted sum of
Robinson-Foulds (RF) distances, a classic symmetric distance metric between two
trees. Extensive experiments are conducted on benchmark datasets in different
languages and domains. The experimental results show that our method, CPTAM,
outperforms the state-of-the-art aggregation baselines. We also demonstrate
that the weights estimated by CPTAM can adequately evaluate constituency
parsers in the absence of ground truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis: Predicting Yelp Scores. (arXiv:2201.07999v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07999">
<div class="article-summary-box-inner">
<span><p>In this work, we predict the sentiment of restaurant reviews based on a
subset of the Yelp Open Dataset. We utilize the meta features and text
available in the dataset and evaluate several machine learning and
state-of-the-art deep learning approaches for the prediction task. Through
several qualitative experiments, we show the success of the deep models with
attention mechanism in learning a balanced model for reviews across different
restaurants. Finally, we propose a novel Multi-tasked joint BERT model that
improves the overall classification performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction of a Quality Estimation Dataset for Automatic Evaluation of Japanese Grammatical Error Correction. (arXiv:2201.08038v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08038">
<div class="article-summary-box-inner">
<span><p>In grammatical error correction (GEC), automatic evaluation is an important
factor for research and development of GEC systems. Previous studies on
automatic evaluation have demonstrated that quality estimation models built
from datasets with manual evaluation can achieve high performance in automatic
evaluation of English GEC without using reference sentences.. However, quality
estimation models have not yet been studied in Japanese, because there are no
datasets for constructing quality estimation models. Therefore, in this study,
we created a quality estimation dataset with manual evaluation to build an
automatic evaluation model for Japanese GEC. Moreover, we conducted a
meta-evaluation to verify the dataset's usefulness in building the Japanese
quality estimation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VISA: An Ambiguous Subtitles Dataset for Visual Scene-Aware Machine Translation. (arXiv:2201.08054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08054">
<div class="article-summary-box-inner">
<span><p>Existing multimodal machine translation (MMT) datasets consist of images and
video captions or general subtitles, which rarely contain linguistic ambiguity,
making visual information not so effective to generate appropriate
translations. We introduce VISA, a new dataset that consists of 40k
Japanese-English parallel sentence pairs and corresponding video clips with the
following key features: (1) the parallel sentences are subtitles from movies
and TV episodes; (2) the source subtitles are ambiguous, which means they have
multiple possible translations with different meanings; (3) we divide the
dataset into Polysemy and Omission according to the cause of ambiguity. We show
that VISA is challenging for the latest MMT system, and we hope that the
dataset can facilitate MMT research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistically-driven Multi-task Pre-training for Low-resource Neural Machine Translation. (arXiv:2201.08070v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08070">
<div class="article-summary-box-inner">
<span><p>In the present study, we propose novel sequence-to-sequence pre-training
objectives for low-resource machine translation (NMT): Japanese-specific
sequence to sequence (JASS) for language pairs involving Japanese as the source
or target language, and English-specific sequence to sequence (ENSS) for
language pairs involving English. JASS focuses on masking and reordering
Japanese linguistic units known as bunsetsu, whereas ENSS is proposed based on
phrase structure masking and reordering tasks. Experiments on ASPEC
Japanese--English &amp; Japanese--Chinese, Wikipedia Japanese--Chinese, News
English--Korean corpora demonstrate that JASS and ENSS outperform MASS and
other existing language-agnostic pre-training methods by up to +2.9 BLEU points
for the Japanese--English tasks, up to +7.0 BLEU points for the
Japanese--Chinese tasks and up to +1.3 BLEU points for English--Korean tasks.
Empirical analysis, which focuses on the relationship between individual parts
in JASS and ENSS, reveals the complementary nature of the subtasks of JASS and
ENSS. Adequacy evaluation using LASER, human evaluation, and case studies
reveals that our proposed methods significantly outperform pre-training methods
without injected linguistic knowledge and they have a larger positive impact on
the adequacy as compared to the fluency. We release codes here:
https://github.com/Mao-KU/JASS/tree/master/linguistically-driven-pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08071">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding in videos (TSGV), a.k.a., natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate methods in different categories with their strengths
and weaknesses. Lastly, we discuss issues with the current TSGV research and
share our insights about promising research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training. (arXiv:2201.08081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08081">
<div class="article-summary-box-inner">
<span><p>Language-based environment manipulation requires agents to manipulate the
environment following natural language instructions, which is challenging due
to the huge space of the environments. To address this challenge, various
approaches have been proposed in recent work. Although these approaches work
well for their intended environments, they are difficult to generalize across
environments. In this work, we propose LEMON, a general framework for
language-based environment manipulation tasks. Specifically, we first propose a
unified approach to deal with various environments using the same generative
language model. Then we propose an execution-guided pre-training strategy to
inject prior knowledge of environments to the language model with a pure
synthetic pre-training corpus. Experimental results on tasks including Alchemy,
Scene, Tangrams and ProPara demonstrate the effectiveness of LEMON: it achieves
new state-of-the-art results on Alchemy, Scene and ProPara, and the
execution-guided pre-training strategy brings remarkable improvements on all
experimental tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Why Did You Not Compare With That? Identifying Papers for Use as Baselines. (arXiv:2201.08089v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08089">
<div class="article-summary-box-inner">
<span><p>We propose the task of automatically identifying papers used as baselines in
a scientific article. We frame the problem as a binary classification task
where all the references in a paper are to be classified as either baselines or
non-baselines. This is a challenging problem due to the numerous ways in which
a baseline reference can appear in a paper. We develop a dataset of $2,075$
papers from ACL anthology corpus with all their references manually annotated
as one of the two classes. We develop a multi-module attention-based neural
classifier for the baseline classification task that outperforms four
state-of-the-art citation role classification methods when applied to the
baseline classification task. We also present an analysis of the errors made by
the proposed classifier, eliciting the challenges that make baseline
identification a challenging problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Question Answering Leaderboard: A Community Resource to Prevent a Replication Crisis. (arXiv:2201.08174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08174">
<div class="article-summary-box-inner">
<span><p>Data-driven systems need to be evaluated to establish trust in the scientific
approach and its applicability. In particular, this is true for Knowledge Graph
(KG) Question Answering (QA), where complex data structures are made accessible
via natural-language interfaces. Evaluating the capabilities of these systems
has been a driver for the community for more than ten years while establishing
different KGQA benchmark datasets. However, comparing different approaches is
cumbersome. The lack of existing and curated leaderboards leads to a missing
global view over the research field and could inject mistrust into the results.
In particular, the latest and most-used datasets in the KGQA community, LC-QuAD
and QALD, miss providing central and up-to-date points of trust. In this paper,
we survey and analyze a wide range of evaluation results with significant
coverage of 100 publications and 98 systems from the last decade. We provide a
new central and open leaderboard for any KGQA benchmark dataset as a focal
point for the community - https://kgqa.github.io/leaderboard. Our analysis
highlights existing problems during the evaluation of KGQA systems. Thus, we
will point to possible improvements for future evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-based Hybrid Local Search for the Hard-label Textual Attack. (arXiv:2201.08193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08193">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to adversarial examples in Natural
Language Processing. However, existing textual adversarial attacks usually
utilize the gradient or prediction confidence to generate adversarial examples,
making it hard to be deployed in real-world applications. To this end, we
consider a rarely investigated but more rigorous setting, namely hard-label
attack, in which the attacker could only access the prediction label. In
particular, we find that the changes on prediction label caused by word
substitutions on the adversarial example could precisely reflect the importance
of different words. Based on this observation, we propose a novel hard-label
attack, called Learning-based Hybrid Local Search (LHLS) algorithm, which
effectively estimates word importance with the prediction label from the attack
history and integrate such information into hybrid local search algorithm to
optimize the adversarial perturbation. Extensive evaluations for text
classification and textual entailment using various datasets and models show
that our LHLS significantly outperforms existing hard-label attacks regarding
the attack performance as well as adversary quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Latent-Variable Model for Intrinsic Probing. (arXiv:2201.08214v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08214">
<div class="article-summary-box-inner">
<span><p>The success of pre-trained contextualized representations has prompted
researchers to analyze them for the presence of linguistic information. Indeed,
it is natural to assume that these pre-trained representations do encode some
level of linguistic knowledge as they have brought about large empirical
improvements on a wide variety of NLP tasks, which suggests they are learning
true linguistic generalization. In this work, we focus on intrinsic probing, an
analysis technique where the goal is not only to identify whether a
representation encodes a linguistic attribute, but also to pinpoint where this
attribute is encoded. We propose a novel latent-variable formulation for
constructing intrinsic probes and derive a tractable variational approximation
to the log-likelihood. Our results show that our model is versatile and yields
tighter mutual information estimates than two intrinsic probes previously
proposed in the literature. Finally, we find empirical evidence that
pre-trained representations develop a cross-lingually entangled notion of
morphosyntax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMDA: Language Models for Dialog Applications. (arXiv:2201.08239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08239">
<div class="article-summary-box-inner">
<span><p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08264">
<div class="article-summary-box-inner">
<span><p>Recent video and language pretraining frameworks lack the ability to generate
sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new
pretraining framework for learning from unlabelled videos which can be
effectively used for generative tasks such as multimodal video captioning.
Unlike recent video-language pretraining frameworks, our framework trains both
a multimodal video encoder and a sentence decoder jointly. To overcome the lack
of captions in unlabelled videos, we leverage the future utterance as an
additional text source and propose a bidirectional generation objective -- we
generate future utterances given the present mulitmodal context, and also the
present utterance given future observations. With this objective, we train an
encoder-decoder model end-to-end to generate a caption from raw pixels and
transcribed speech directly. Our model achieves state-of-the-art performance
for multimodal video captioning on four standard benchmarks, as well as for
other video understanding tasks such as VideoQA, video retrieval and action
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis. (arXiv:2201.08277v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08277">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is one of the most widely studied applications in NLP, but
most work focuses on languages with large amounts of data. We introduce the
first large-scale human-annotated Twitter sentiment dataset for the four most
widely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and Yoruba)
consisting of around 30,000 annotated tweets per language (except for
Nigerian-Pidgin), including a significant fraction of code-mixed tweets. We
propose text collection, filtering, processing, and labelling methods that
enable us to create datasets for these low-resource languages. We evaluate a
range of pre-trained models and transfer strategies on the dataset. We find
that language-specific models and language-adaptive fine-tuning generally
perform best. We release the datasets, trained models, sentiment lexicons, and
code to incentivize research on sentiment analysis in under-represented
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cheating Automatic Short Answer Grading: On the Adversarial Usage of Adjectives and Adverbs. (arXiv:2201.08318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08318">
<div class="article-summary-box-inner">
<span><p>Automatic grading models are valued for the time and effort saved during the
instruction of large student bodies. Especially with the increasing
digitization of education and interest in large-scale standardized testing, the
popularity of automatic grading has risen to the point where commercial
solutions are widely available and used. However, for short answer formats,
automatic grading is challenging due to natural language ambiguity and
versatility. While automatic short answer grading models are beginning to
compare to human performance on some datasets, their robustness, especially to
adversarially manipulated data, is questionable. Exploitable vulnerabilities in
grading models can have far-reaching consequences ranging from cheating
students receiving undeserved credit to undermining automatic grading
altogether - even when most predictions are valid. In this paper, we devise a
black-box adversarial attack tailored to the educational short answer grading
scenario to investigate the grading models' robustness. In our attack, we
insert adjectives and adverbs into natural places of incorrect student answers,
fooling the model into predicting them as correct. We observed a loss of
prediction accuracy between 10 and 22 percentage points using the
state-of-the-art models BERT and T5. While our attack made answers appear less
natural to humans in our experiments, it did not significantly increase the
graders' suspicions of cheating. Based on our experiments, we provide
recommendations for utilizing automatic grading systems more safely in
practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signature Entrenchment and Conceptual Changes in Automated Theory Repair. (arXiv:2201.08340v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08340">
<div class="article-summary-box-inner">
<span><p>Human beliefs change, but so do the concepts that underpin them. The recent
Abduction, Belief Revision and Conceptual Change (ABC) repair system combines
several methods from automated theory repair to expand, contract, or reform
logical structures representing conceptual knowledge in artificial agents. In
this paper we focus on conceptual change: repair not only of the membership of
logical concepts, such as what animals can fly, but also concepts themselves,
such that birds may be divided into flightless and flying birds, by changing
the signature of the logical theory used to represent them. We offer a method
for automatically evaluating entrenchment in the signature of a Datalog theory,
in order to constrain automated theory repair to succinct and intuitive
outcomes. Formally, signature entrenchment measures the inferential
contributions of every logical language element used to express conceptual
knowledge, i.e., predicates and the arguments, ranking possible repairs to
retain valuable logical concepts and reject redundant or implausible
alternatives. This quantitative measurement of signature entrenchment offers a
guide to the plausibility of conceptual changes, which we aim to contrast with
human judgements of concept entrenchment in future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Model to Measure the Spread Power of Rumors. (arXiv:2002.07563v4 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.07563">
<div class="article-summary-box-inner">
<span><p>With technologies that have democratized the production and reproduction of
information, a significant portion of daily interacted posts in social media
has been infected by rumors. Despite the extensive research on rumor detection
and verification, so far, the problem of calculating the spread power of rumors
has not been considered. To address this research gap, the present study seeks
a model to calculate the Spread Power of Rumor (SPR) as the function of
content-based features in two categories: False Rumor (FR) and True Rumor (TR).
For this purpose, the theory of Allport and Postman will be adopted, which it
claims that importance and ambiguity are the key variables in rumor-mongering
and the power of rumor. Totally 42 content features in two categories
"importance" (28 features) and "ambiguity" (14 features) are introduced to
compute SPR. The proposed model is evaluated on two datasets, Twitter and
Telegram. The results showed that (i) the spread power of False Rumor documents
is rarely more than True Rumors. (ii) there is a significant difference between
the SPR means of two groups False Rumor and True Rumor. (iii) SPR as a
criterion can have a positive impact on distinguishing False Rumors and True
Rumors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Hyperbolic Geometry for FG-NET over Distantly Supervised data. (arXiv:2101.11212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11212">
<div class="article-summary-box-inner">
<span><p>Fine-Grained Named Entity Typing (\FGNET{}) classifies an entity mention into
a fine range of entity types. A large number of entity types make it difficult
to manually label the training data, thus distant supervision is used to
automatically acquire the training data. Distant supervision incurs a lot of
training noise which hinders the performance improvement of the FG-NET systems.
In this paper, we propose to use hyperbolic geometry for FG-NET with the hope
that it can help overcoming the noise incurred by distant supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review. (arXiv:2105.01044v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01044">
<div class="article-summary-box-inner">
<span><p>Technology-assisted review (TAR) refers to iterative active learning
workflows for document review in high recall retrieval (HRR) tasks. TAR
research and most commercial TAR software have applied linear models such as
logistic regression to lexical features. Transformer-based models with
supervised tuning are known to improve effectiveness on many text
classification tasks, suggesting their use in TAR. We indeed find that the
pre-trained BERT model reduces review cost by 10% to 15% in TAR workflows
simulated on the RCV1-v2 newswire collection. In contrast, we likewise
determined that linear models outperform BERT for simulated legal discovery
topics on the Jeb Bush e-mail collection. This suggests the match between
transformer pre-training corpora and the task domain is of greater significance
than generally appreciated. Additionally, we show that just-right language
model fine-tuning on the task collection before starting active learning is
critical. Too little or too much fine-tuning hinders performance, worse than
that of linear models, even for a favorable corpus such as RCV1-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management. (arXiv:2106.11796v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11796">
<div class="article-summary-box-inner">
<span><p>Current task-oriented dialog (TOD) systems mostly manage structured knowledge
(e.g. databases and tables) to guide the goal-oriented conversations. However,
they fall short of handling dialogs which also involve unstructured knowledge
(e.g. reviews and documents). In this paper, we formulate a task of modeling
TOD grounded on a fusion of structured and unstructured knowledge. To address
this task, we propose a TOD system with semi-structured knowledge management,
SeKnow, which extends the belief state to manage knowledge with both structured
and unstructured contents. Furthermore, we introduce two implementations of
SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained
language model, respectively. Both implementations use the end-to-end manner to
jointly optimize dialog modeling grounded on structured and unstructured
knowledge. We conduct experiments on a modified version of MultiWOZ 2.1
dataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve
semi-structured knowledge. Experimental results show that SeKnow has strong
performances in both end-to-end dialog and intermediate knowledge management,
compared to existing TOD systems and their extensions with pipeline knowledge
management schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04137">
<div class="article-summary-box-inner">
<span><p>The goal of building intelligent dialogue systems has largely been
\textit{separately} pursued under two paradigms: task-oriented dialogue (TOD)
systems, which perform goal-oriented functions, and open-domain dialogue (ODD)
systems, which focus on non-goal-oriented chitchat. The two dialogue modes can
potentially be intertwined together seamlessly in the same conversation, as
easily done by a friendly human assistant. Such ability is desirable in
conversational agents, as the integration makes them more accessible and
useful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn
dialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset
FusedChat, by rewriting the existing TOD turns and adding new ODD turns. This
procedure constructs conversation sessions containing exchanges from both
dialogue modes. It features inter-mode contextual dependency, i.e., the
dialogue turns from the two modes depend on each other. Rich dependency
patterns including co-reference and ellipsis are features. The new dataset,
with 60k new human-written ODD turns and 5k re-written TOD turns, offers a
benchmark to test a dialogue model's ability to perform inter-mode
conversations. This is a more challenging task since the model has to determine
the appropriate dialogue mode and generate the response based on the inter-mode
context. But such models would better mimic human-level conversation
capabilities. We evaluate baseline models on this task, including
\textit{classification-based} two-stage models and \textit{two-in-one} fused
models. We publicly release FusedChat and the baselines to propel future work
on inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can depth-adaptive BERT perform better on binary classification tasks. (arXiv:2111.10951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10951">
<div class="article-summary-box-inner">
<span><p>In light of the success of transferring language models into NLP tasks, we
ask whether the full BERT model is always the best and does it exist a simple
but effective method to find the winning ticket in state-of-the-art deep neural
networks without complex calculations. We construct a series of BERT-based
models with different size and compare their predictions on 8 binary
classification tasks. The results show there truly exist smaller sub-networks
performing better than the full model. Then we present a further study and
propose a simple method to shrink BERT appropriately before fine-tuning. Some
extended experiments indicate that our method could save time and storage
overhead extraordinarily with little even no accuracy loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. (arXiv:2201.05966v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05966">
<div class="article-summary-box-inner">
<span><p>Structured knowledge grounding (SKG) leverages structured knowledge to
complete user requests, such as semantic parsing over databases and question
answering over knowledge bases. Since the inputs and outputs of SKG tasks are
heterogeneous, they have been studied separately by different communities,
which limits systematic and compatible research on SKG. In this paper, we
overcome this limitation by proposing the SKG framework, which unifies 21 SKG
tasks into a text-to-text format, aiming to promote systematic SKG research,
instead of being exclusive to a single task, domain, or dataset. We use
UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple
modifications when necessary, achieves state-of-the-art performance on almost
all of the 21 tasks. We further demonstrate that multi-task prefix-tuning
improves the performance on most tasks, largely improving the overall
performance. UnifiedSKG also facilitates the investigation of zero-shot and
few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot
and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of
controlled experiments on structured knowledge encoding variants across SKG
tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at
https://github.com/hkunlp/unifiedskg Latest collections at
https://unifiedskg.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural Machine Translation by Denoising Training. (arXiv:2201.07365v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07365">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective pretraining strategy {D}en{o}ising
{T}raining DoT for neural machine translation. Specifically, we update the
model parameters with source- and target-side denoising tasks at the early
stage and then tune the model normally. Notably, our approach does not increase
any parameters or training steps, requiring the parallel data merely.
Experiments show that DoT consistently improves the neural machine translation
performance across 12 bilingual and 16 multilingual directions (data size
ranges from 80K to 20M). In addition, we show that DoT can complement existing
data manipulation strategies, i.e. curriculum learning, knowledge distillation,
data diversification, bidirectional training, and back-translation.
Encouragingly, we found that DoT outperforms costly pretrained model mBART in
high-resource settings. Analyses show DoT is a novel in-domain cross-lingual
pretraining strategy and could offer further improvements with task-relevant
self-supervisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07423">
<div class="article-summary-box-inner">
<span><p>Loneliness has been associated with negative outcomes for physical and mental
health. Understanding how people express and cope with various forms of
loneliness is critical for early screening and targeted interventions to reduce
loneliness, particularly among vulnerable groups such as young adults. To
examine how different forms of loneliness and coping strategies manifest in
loneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained
Loneliness) by using Reddit posts in two young adult-focused forums and two
loneliness related forums consisting of a diverse age group. We provide
annotations by trained human annotators for binary and fine-grained loneliness
classifications of the posts. Trained on FIG-Loneliness, two BERT-based models
were used to understand loneliness forms and authors' coping strategies in
these forums. Our binary loneliness classification archived an accuracy above
97%, and fine-grained loneliness category classification reached an average
accuracy of 77% across all labeled categories. With FIG-Loneliness and model
predictions, we found that loneliness expressions in the young adult related
forums are distinct from other forums. Those in young adult-focused forums are
more likely to express concerns pertaining to peer relationship, and are
potentially more sensitive to geographical isolation impacted by the COVID-19
pandemic lockdown. Also, we show that different forms of loneliness have
differential use in coping strategies.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">BLINC: Lightweight Bimodal Learning for Low-Complexity VVC Intra Coding. (arXiv:2201.07823v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07823">
<div class="article-summary-box-inner">
<span><p>The latest video coding standard, Versatile Video Coding (VVC), achieves
almost twice coding efficiency compared to its predecessor, the High Efficiency
Video Coding (HEVC). However, achieving this efficiency (for intra coding)
requires 31x computational complexity compared to HEVC, making it challenging
for low power and real-time applications. This paper, proposes a novel machine
learning approach that jointly and separately employs two modalities of
features, to simplify the intra coding decision. First a set of features are
extracted that use the existing DCT core of VVC, to assess the texture
characteristics, and forms the first modality of data. This produces high
quality features with almost no overhead. The distribution of intra modes at
the neighboring blocks is also used to form the second modality of data, which
provides statistical information about the frame. Second, a two-step feature
reduction method is designed that reduces the size of feature set, such that a
lightweight model with a limited number of parameters can be used to learn the
intra mode decision task. Third, three separate training strategies are
proposed (1) an offline training strategy using the first (single) modality of
data, (2) an online training strategy that uses the second (single) modality,
and (3) a mixed online-offline strategy that uses bimodal learning. Finally, a
low-complexity encoding algorithms is proposed based on the proposed learning
strategies. Extensive experimental results show that the proposed methods can
reduce up to 24% of encoding time, with a negligible loss of coding efficiency.
Moreover, it is demonstrated how a bimodal learning strategy can boost the
performance of learning. Lastly, the proposed method has a very low
computational overhead (0.2%), and uses existing components of a VVC encoder,
which makes it much more practical compared to competing solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROS georegistration: Aerial Multi-spectral Image Simulator for the Robot Operating System. (arXiv:2201.07863v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07863">
<div class="article-summary-box-inner">
<span><p>This article describes a software package called ROS georegistration intended
for use with the Robot Operating System (ROS) and the Gazebo 3D simulation
environment. ROSgeoregistration provides tools for the simulation, test and
deployment of aerial georegistration algorithms and is made available with a
link provided in the paper. A model creation package is provided which
downloads multi-spectral images from the Google Earth Engine database and, if
necessary, incorporates these images into a single, possibly very large,
reference image. Additionally a Gazebo plugin which uses the real-time sensor
pose and image formation model to generate simulated imagery using the
specified reference image is provided along with related plugins for UAV
relevant data. The novelty of this work is threefold: (1) this is the first
system to link the massive multi-spectral imaging database of Google's Earth
Engine to the Gazebo simulator, (2) this is the first example of a system that
can simulate geospatially and radiometrically accurate imagery from multiple
sensor views of the same terrain region, and (3) integration with other UAS
tools creates a new holistic UAS simulation environment to support UAS system
and subsystem development where real-world testing would generally be
prohibitive. Sensed imagery and ground truth registration information is
published to client applications which can receive imagery synchronously with
telemetry from other payload sensors, e.g., IMU, GPS/GNSS, barometer, and
windspeed sensor data. To highlight functionality, we demonstrate
ROSgeoregistration for simulating Electro-Optical (EO) and Synthetic Aperture
Radar (SAR) image sensors and an example use case for developing and evaluating
image-based UAS position feedback, i.e., pose for image-based Guidance
Navigation and Control (GNC) applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automated Robotic Arm: A Machine Learning Approach. (arXiv:2201.07882v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07882">
<div class="article-summary-box-inner">
<span><p>The term robot generally refers to a machine that looks and works in a way
similar to a human. The modern industry is rapidly shifting from manual control
of systems to automation, in order to increase productivity and to deliver
quality products. Computer-based systems, though feasible for improving quality
and productivity, are inflexible to work with, and the cost of such systems is
significantly high. This led to the swift adoption of automated systems to
perform industrial tasks. One such task of industrial significance is of
picking and placing objects from one place to another. The implementation of
automation in pick and place tasks helps to improve efficiency of system and
also the performance. In this paper, we propose to demonstrate the designing
and working of an automated robotic arm with the Machine Learning approach. The
work uses Machine Learning approach for object identification detection and
traversal, which is adopted with Tensor flow package for better and accurate
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Networks for Spherical Signal Processing via Spherical Haar Tight Framelets. (arXiv:2201.07890v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07890">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop a general theoretical framework for constructing
Haar-type tight framelets on any compact set with a hierarchical partition. In
particular, we construct a novel area-regular hierarchical partition on the
2-sphere and establish its corresponding spherical Haar tight framelets with
directionality. We conclude by evaluating and illustrating the effectiveness of
our area-regular spherical Haar tight framelets in several denoising
experiments. Furthermore, we propose a convolutional neural network (CNN) model
for spherical signal denoising which employs the fast framelet decomposition
and reconstruction algorithms. Experiment results show that our proposed CNN
model outperforms threshold methods, and processes strong generalization and
robustness properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homogenization of Existing Inertial-Based Datasets to Support Human Activity Recognition. (arXiv:2201.07891v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07891">
<div class="article-summary-box-inner">
<span><p>Several techniques have been proposed to address the problem of recognizing
activities of daily living from signals. Deep learning techniques applied to
inertial signals have proven to be effective, achieving significant
classification accuracy. Recently, research in human activity recognition (HAR)
models has been almost totally model-centric. It has been proven that the
number of training samples and their quality are critical for obtaining deep
learning models that both perform well independently of their architecture, and
that are more robust to intraclass variability and interclass similarity.
Unfortunately, publicly available datasets do not always contain hight quality
data and a sufficiently large and diverse number of samples (e.g., number of
subjects, type of activity performed, and duration of trials). Furthermore,
datasets are heterogeneous among them and therefore cannot be trivially
combined to obtain a larger set. The final aim of our work is the definition
and implementation of a platform that integrates datasets of inertial signals
in order to make available to the scientific community large datasets of
homogeneous signals, enriched, when possible, with context information (e.g.,
characteristics of the subjects and device position). The main focus of our
platform is to emphasise data quality, which is essential for training
efficient models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Performance of Pre-Trained Networks by Matched Augmentation Distributions. (arXiv:2201.07894v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07894">
<div class="article-summary-box-inner">
<span><p>There exists a distribution discrepancy between training and testing, in the
way images are fed to modern CNNs. Recent work tried to bridge this gap either
by fine-tuning or re-training the network at different resolutions. However
re-training a network is rarely cheap and not always viable. To this end, we
propose a simple solution to address the train-test distributional shift and
enhance the performance of pre-trained models -- which commonly ship as a
package with deep learning platforms \eg, PyTorch. Specifically, we demonstrate
that running inference on the center crop of an image is not always the best as
important discriminatory information may be cropped-off. Instead we propose to
combine results for multiple random crops for a test image. This not only
matches the train time augmentation but also provides the full coverage of the
input image. We explore combining representation of random crops through
averaging at different levels \ie, deep feature level, logit level, and softmax
level. We demonstrate that, for various families of modern deep networks, such
averaging results in better validation accuracy compared to using a single
central crop per image. The softmax averaging results in the best performance
for various pre-trained networks without requiring any re-training or
fine-tuning whatsoever. On modern GPUs with batch processing, the paper's
approach to inference of pre-trained networks, is essentially free as all
images in a batch can all be processed at once.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASL Video Corpora & Sign Bank: Resources Available through the American Sign Language Linguistic Research Project (ASLLRP). (arXiv:2201.07899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07899">
<div class="article-summary-box-inner">
<span><p>The American Sign Language Linguistic Research Project (ASLLRP) provides
Internet access to high-quality ASL video data, generally including front and
side views and a close-up of the face. The manual and non-manual components of
the signing have been linguistically annotated using SignStream(R). The
recently expanded video corpora can be browsed and searched through the Data
Access Interface (DAI 2) we have designed; it is possible to carry out complex
searches. The data from our corpora can also be downloaded; annotations are
available in an XML export format. We have also developed the ASLLRP Sign Bank,
which contains almost 6,000 sign entries for lexical signs, with distinct
English-based glosses, with a total of 41,830 examples of lexical signs (in
addition to about 300 gestures, over 1,000 fingerspelled signs, and 475
classifier examples). The Sign Bank is likewise accessible and searchable on
the Internet; it can also be accessed from within SignStream(R) (software to
facilitate linguistic annotation and analysis of visual language data) to make
annotations more accurate and efficient. Here we describe the available
resources. These data have been used for many types of research in linguistics
and in computer-based sign language recognition from video; examples of such
research are provided in the latter part of this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Facial Expressions and Emotion in ASL. (arXiv:2201.07906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07906">
<div class="article-summary-box-inner">
<span><p>There is little prior work on quantifying the relationships between facial
expressions and emotionality in American Sign Language. In this final report,
we provide two methods for studying these relationships through probability and
prediction. Using a large corpus of natural signing manually annotated with
facial features paired with lexical emotion datasets, we find that there exist
many relationships between emotionality and the face, and that a simple
classifier can predict what someone is saying in terms of broad emotional
categories only by looking at the face.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-by-Novel-View-Synthesis for Full-Face Appearance-based 3D Gaze Estimation. (arXiv:2201.07927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07927">
<div class="article-summary-box-inner">
<span><p>Despite recent advances in appearance-based gaze estimation techniques, the
need for training data that covers the target head pose and gaze distribution
remains a crucial challenge for practical deployment. This work examines a
novel approach for synthesizing gaze estimation training data based on
monocular 3D face reconstruction. Unlike prior works using multi-view
reconstruction, photo-realistic CG models, or generative neural networks, our
approach can manipulate and extend the head pose range of existing training
data without any additional requirements. We introduce a projective matching
procedure to align the reconstructed 3D facial mesh to the camera coordinate
system and synthesize face images with accurate gaze labels. We also propose a
mask-guided gaze estimation model and data augmentation strategies to further
improve the estimation accuracy by taking advantage of the synthetic training
data. Experiments using multiple public datasets show that our approach can
significantly improve the estimation performance on challenging cross-dataset
settings with non-overlapping gaze distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Egocentric 3D Human Pose in the Wild with External Weak Supervision. (arXiv:2201.07929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07929">
<div class="article-summary-box-inner">
<span><p>Egocentric 3D human pose estimation with a single fisheye camera has drawn a
significant amount of attention recently. However, existing methods struggle
with pose estimation from in-the-wild images, because they can only be trained
on synthetic data due to the unavailability of large-scale in-the-wild
egocentric datasets. Furthermore, these methods easily fail when the body parts
are occluded by or interacting with the surrounding scene. To address the
shortage of in-the-wild data, we collect a large-scale in-the-wild egocentric
dataset called Egocentric Poses in the Wild (EgoPW). This dataset is captured
by a head-mounted fisheye camera and an auxiliary external camera, which
provides an additional observation of the human body from a third-person
perspective during training. We present a new egocentric pose estimation
method, which can be trained on the new dataset with weak external supervision.
Specifically, we first generate pseudo labels for the EgoPW dataset with a
spatio-temporal optimization method by incorporating the external-view
supervision. The pseudo labels are then used to train an egocentric pose
estimation network. To facilitate the network training, we propose a novel
learning strategy to supervise the egocentric features with the high-quality
features extracted by a pretrained external-view pose estimation model. The
experiments show that our method predicts accurate 3D poses from a single
in-the-wild egocentric image and outperforms the state-of-the-art methods both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Experimental Large-Scale Jet Flames' Geometrical Features Extraction for Risk Management Using Infrared Images and Deep Learning Segmentation Methods. (arXiv:2201.07931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07931">
<div class="article-summary-box-inner">
<span><p>Jet fires are relatively small and have the least severe effects among the
diverse fire accidents that can occur in industrial plants; however, they are
usually involved in a process known as the domino effect, that leads to more
severe events, such as explosions or the initiation of another fire, making the
analysis of such fires an important part of risk analysis. This research work
explores the application of deep learning models in an alternative approach
that uses the semantic segmentation of jet fires flames to extract main
geometrical attributes, relevant for fire risk assessments. A comparison is
made between traditional image processing methods and some state-of-the-art
deep learning models. It is found that the best approach is a deep learning
architecture known as UNet, along with its two improvements, Attention UNet and
UNet++. The models are then used to segment a group of vertical jet flames of
varying pipe outlet diameters to extract their main geometrical
characteristics. Attention UNet obtained the best general performance in the
approximation of both height and area of the flames, while also showing a
statistically significant difference between it and UNet++. UNet obtained the
best overall performance for the approximation of the lift-off distances;
however, there is not enough data to prove a statistically significant
difference between Attention UNet and UNet++. The only instance where UNet++
outperformed the other models, was while obtaining the lift-off distances of
the jet flames with 0.01275 m pipe outlet diameter. In general, the explored
models show good agreement between the experimental and predicted values for
relatively large turbulent propane jet flames, released in sonic and subsonic
regimes; thus, making these radiation zones segmentation models, a suitable
approach for different jet flame risk management scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images. (arXiv:2201.07935v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07935">
<div class="article-summary-box-inner">
<span><p>Developing innovative informatics approaches aimed to enhance fetal
monitoring is a burgeoning field of study in reproductive medicine. Several
reviews have been conducted regarding Artificial intelligence (AI) techniques
to improve pregnancy outcomes. They are limited by focusing on specific data
such as mother's care during pregnancy. This systematic survey aims to explore
how artificial intelligence (AI) can assist with fetal growth monitoring via
Ultrasound (US) image. We used eight medical and computer science bibliographic
databases, including PubMed, Embase, PsycINFO, ScienceDirect, IEEE explore, ACM
Library, Google Scholar, and the Web of Science. We retrieved studies published
between 2010 to 2021. Data extracted from studies were synthesized using a
narrative approach. Out of 1269 retrieved studies, we included 107 distinct
studies from queries that were relevant to the topic in the survey. We found
that 2D ultrasound images were more popular (n=88) than 3D and 4D ultrasound
images (n=19). Classification is the most used method (n=42), followed by
segmentation (n=31), classification integrated with segmentation (n=16) and
other miscellaneous such as object-detection, regression and reinforcement
learning (n=18). The most common areas within the pregnancy domain were the
fetus head (n=43), then fetus body (n=31), fetus heart (n=13), fetus abdomen
(n=10), and lastly the fetus face (n=10). In the most recent studies, deep
learning techniques were primarily used (n=81), followed by machine learning
(n=16), artificial neural network (n=7), and reinforcement learning (n=2). AI
techniques played a crucial role in predicting fetal diseases and identifying
fetus anatomy structures during pregnancy. More research is required to
validate this technology from a physician's perspective, such as pilot studies
and randomized controlled trials on AI and its applications in a hospital
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GASCN: Graph Attention Shape Completion Network. (arXiv:2201.07937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07937">
<div class="article-summary-box-inner">
<span><p>Shape completion, the problem of inferring the complete geometry of an object
given a partial point cloud, is an important problem in robotics and computer
vision. This paper proposes the Graph Attention Shape Completion Network
(GASCN), a novel neural network model that solves this problem. This model
combines a graph-based model for encoding local point cloud information with an
MLP-based architecture for encoding global information. For each completed
point, our model infers the normal and extent of the local surface patch which
is used to produce dense yet precise shape completions. We report experiments
that demonstrate that GASCN outperforms standard shape completion methods on a
standard benchmark drawn from the Shapenet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Video Representation Learning with Cascade Positive Retrieval. (arXiv:2201.07989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07989">
<div class="article-summary-box-inner">
<span><p>Self-supervised video representation learning has been shown to effectively
improve downstream tasks such as video retrieval and action recognition.In this
paper, we present the Cascade Positive Retrieval (CPR) that successively mines
positive examples w.r.t. the query for contrastive learning in a cascade of
stages. Specifically, CPR exploits multiple views of a query example in
different modalities, where an alternative view may help find another positive
example dissimilar in the query view. We explore the effects of possible CPR
configurations in ablations including the number of mining stages, the top
similar example selection ratio in each stage, and progressive training with an
incremental number of the final Top-k selection. The overall mining quality is
measured to reflect the recall across training set classes. CPR reaches a
median class mining recall of 83.3%, outperforming previous work by 5.5%.
Implementation-wise, CPR is complementary to pretext tasks and can be easily
applied to previous work. In the evaluation of pretraining on UCF101, CPR
consistently improves existing work and even achieves state-of-the-art R@1 of
56.7% and 24.4% in video retrieval as well as 83.8% and 54.8% in action
recognition on UCF101 and HMDB51. For transfer from large video dataset
Kinetics400 to UCF101 and HDMB, CPR benefits existing work, showing competitive
Top-1 accuracies of 85.1% and 57.4% despite pretraining at a lower resolution
and frame sampling rate. The code will be released soon for reproducing the
results. The code is available at https://github.com/necla-ml/CPR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CELESTIAL: Classification Enabled via Labelless Embeddings with Self-supervised Telescope Image Analysis Learning. (arXiv:2201.08001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08001">
<div class="article-summary-box-inner">
<span><p>A common class of problems in remote sensing is scene classification, a
fundamentally important task for natural hazards identification, geographic
image retrieval, and environment monitoring. Recent developments in this field
rely label-dependent supervised learning techniques which is antithetical to
the 35 petabytes of unlabelled satellite imagery in NASA GIBS. To solve this
problem, we establish CELESTIAL-a self-supervised learning pipeline for
effectively leveraging sparsely-labeled satellite imagery. This pipeline
successfully adapts SimCLR, an algorithm that first learns image
representations on unlabelled data and then fine-tunes this knowledge on the
provided labels. Our results show CELESTIAL requires only a third of the labels
that the supervised method needs to attain the same accuracy on an experimental
dataset. The first unsupervised tier can enable applications such as reverse
image search for NASA Worldview (i.e. searching similar atmospheric phenomenon
over years of unlabelled data with minimal samples) and the second supervised
tier can lower the necessity of expensive data annotation significantly. In the
future, we hope we can generalize the CELESTIAL pipeline to other data types,
algorithms, and applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRMI: A Dataset of Minirhizotron Images for Diverse Plant Root Study. (arXiv:2201.08002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08002">
<div class="article-summary-box-inner">
<span><p>Understanding a plant's root system architecture (RSA) is crucial for a
variety of plant science problem domains including sustainability and climate
adaptation. Minirhizotron (MR) technology is a widely-used approach for
phenotyping RSA non-destructively by capturing root imagery over time.
Precisely segmenting roots from the soil in MR imagery is a critical step in
studying RSA features. In this paper, we introduce a large-scale dataset of
plant root images captured by MR technology. In total, there are over 72K RGB
root images across six different species including cotton, papaya, peanut,
sesame, sunflower, and switchgrass in the dataset. The images span a variety of
conditions including varied root age, root structures, soil types, and depths
under the soil surface. All of the images have been annotated with weak
image-level labels indicating whether each image contains roots or not. The
image-level labels can be used to support weakly supervised learning in plant
root segmentation tasks. In addition, 63K images have been manually annotated
to generate pixel-level binary masks indicating whether each pixel corresponds
to root or not. These pixel-level binary masks can be used as ground truth for
supervised learning in semantic segmentation tasks. By introducing this
dataset, we aim to facilitate the automatic segmentation of roots and the
research of RSA with deep learning and other image analysis algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Morphological Profiles and Patch Tensor Change Detection for Hyperspectral Imagery. (arXiv:2201.08027v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08027">
<div class="article-summary-box-inner">
<span><p>Multi-temporal hyperspectral images can be used to detect changed
information, which has gradually attracted researchers' attention. However,
traditional change detection algorithms have not deeply explored the relevance
of spatial and spectral changed features, which leads to low detection
accuracy. To better excavate both spectral and spatial information of changed
features, a joint morphology and patch-tensor change detection (JMPT) method is
proposed. Initially, a patch-based tensor strategy is adopted to exploit
similar property of spatial structure, where the non-overlapping local patch
image is reshaped into a new tensor cube, and then three-order Tucker
decompositon and image reconstruction strategies are adopted to obtain more
robust multi-temporal hyperspectral datasets. Meanwhile, multiple morphological
profiles including max-tree and min-tree are applied to extract different
attributes of multi-temporal images. Finally, these results are fused to
general a final change detection map. Experiments conducted on two real
hyperspectral datasets demonstrate that the proposed detector achieves better
detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Frequency-based Feature Disentanglement and Interaction. (arXiv:2201.08029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08029">
<div class="article-summary-box-inner">
<span><p>Data out-of-distribution is a meta-challenge for all statistical learning
algorithms that strongly rely on the i.i.d. assumption. It leads to unavoidable
labor costs and confidence crises in realistic applications. For that, domain
generalization aims at mining domain-irrelevant knowledge from multiple source
domains that can generalize to unseen target domains with unknown
distributions. In this paper, leveraging the image frequency domain, we
uniquely work with two key observations: (i) the high-frequency information of
images depict object edge structure, which is naturally consistent across
different domains, and (ii) the low-frequency component retains object smooth
structure but are much more domain-specific. Motivated by these insights, we
introduce (i) an encoder-decoder structure for high-frequency and low-frequency
feature disentangling, (ii) an information interaction mechanism that ensures
helpful knowledge from both two parts can cooperate effectively, and (iii) a
novel data augmentation technique that works on the frequency domain for
encouraging robustness of the network. The proposed method obtains
state-of-the-art results on three widely used domain generalization benchmarks
(Digit-DG, Office-Home, and PACS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Salient Object Detection in Optical Remote Sensing Images via Feature Correlation. (arXiv:2201.08049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08049">
<div class="article-summary-box-inner">
<span><p>Salient object detection in optical remote sensing images (ORSI-SOD) has been
widely explored for understanding ORSIs. However, previous methods focus mainly
on improving the detection accuracy while neglecting the cost in memory and
computation, which may hinder their real-world applications. In this paper, we
propose a novel lightweight ORSI-SOD solution, named CorrNet, to address these
issues. In CorrNet, we first lighten the backbone (VGG-16) and build a
lightweight subnet for feature extraction. Then, following the coarse-to-fine
strategy, we generate an initial coarse saliency map from high-level semantic
features in a Correlation Module (CorrM). The coarse saliency map serves as the
location guidance for low-level features. In CorrM, we mine the object location
information between high-level semantic features through the cross-layer
correlation operation. Finally, based on low-level detailed features, we refine
the coarse saliency map in the refinement subnet equipped with Dense
Lightweight Refinement Blocks, and produce the final fine saliency map. By
reducing the parameters and computations of each component, CorrNet ends up
having only 4.09M parameters and running with 21.09G FLOPs. Experimental
results on two public datasets demonstrate that our lightweight CorrNet
achieves competitive or even better performance compared with 26
state-of-the-art methods (including 16 large CNN-based methods and 2
lightweight methods), and meanwhile enjoys the clear memory and run time
efficiency. The code and results of our method are available at
https://github.com/MathLee/CorrNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TerViT: An Efficient Ternary Vision Transformer. (arXiv:2201.08050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08050">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have demonstrated great potential in various
visual tasks, but suffer from expensive computational and memory cost problems
when deployed on resource-constrained devices. In this paper, we introduce a
ternary vision transformer (TerViT) to ternarize the weights in ViTs, which are
challenged by the large loss surface gap between real-valued and ternary
parameters. To address the issue, we introduce a progressive training scheme by
first training 8-bit transformers and then TerViT, and achieve a better
optimization than conventional methods. Furthermore, we introduce channel-wise
ternarization, by partitioning each matrix to different channels, each of which
is with an unique distribution and ternarization interval. We apply our methods
to popular DeiT and Swin backbones, and extensive results show that we can
achieve competitive performance. For example, TerViT can quantize Swin-S to
13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Vegetation Stratum Occupancy from Airborne LiDAR Data with Deep Learning. (arXiv:2201.08051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08051">
<div class="article-summary-box-inner">
<span><p>We propose a new deep learning-based method for estimating the occupancy of
vegetation strata from airborne 3D LiDAR point clouds. Our model predicts
rasterized occupancy maps for three vegetation strata corresponding to lower,
medium, and higher cover. Our weakly-supervised training scheme allows our
network to only be supervised with vegetation occupancy values aggregated over
cylindrical plots containing thousands of points. Such ground truth is easier
to produce than pixel-wise or point-wise annotations. Our method outperforms
handcrafted and deep learning baselines in terms of precision by up to 30%,
while simultaneously providing visual and interpretable predictions. We provide
an open-source implementation along with a dataset of 199 agricultural plots to
train and evaluate weakly supervised occupancy regression algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Elements of Temporal Sentence Grounding in Videos: A Survey and Future Directions. (arXiv:2201.08071v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08071">
<div class="article-summary-box-inner">
<span><p>Temporal sentence grounding in videos (TSGV), a.k.a., natural language video
localization (NLVL) or video moment retrieval (VMR), aims to retrieve a
temporal moment that semantically corresponds to a language query from an
untrimmed video. Connecting computer vision and natural language, TSGV has
drawn significant attention from researchers in both communities. This survey
attempts to provide a summary of fundamental concepts in TSGV and current
research status, as well as future research directions. As the background, we
present a common structure of functional components in TSGV, in a tutorial
style: from feature extraction from raw video and language query, to answer
prediction of the target moment. Then we review the techniques for multimodal
understanding and interaction, which is the key focus of TSGV for effective
alignment between the two modalities. We construct a taxonomy of TSGV
techniques and elaborate methods in different categories with their strengths
and weaknesses. Lastly, we discuss issues with the current TSGV research and
share our insights about promising research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirPose: Multi-View Fusion Network for Aerial 3D Human Pose and Shape Estimation. (arXiv:2201.08093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08093">
<div class="article-summary-box-inner">
<span><p>In this letter, we present a novel markerless 3D human motion capture (MoCap)
system for unstructured, outdoor environments that uses a team of autonomous
unmanned aerial vehicles (UAVs) with on-board RGB cameras and computation.
Existing methods are limited by calibrated cameras and off-line processing.
Thus, we present the first method (AirPose) to estimate human pose and shape
using images captured by multiple extrinsically uncalibrated flying cameras.
AirPose itself calibrates the cameras relative to the person instead of relying
on any pre-calibration. It uses distributed neural networks running on each UAV
that communicate viewpoint-independent information with each other about the
person (i.e., their 3D shape and articulated pose). The person's shape and pose
are parameterized using the SMPL-X body model, resulting in a compact
representation, that minimizes communication between the UAVs. The network is
trained using synthetic images of realistic virtual environments, and
fine-tuned on a small set of real images. We also introduce an
optimization-based post-processing method (AirPose$^{+}$) for offline
applications that require higher MoCap quality. We make our method's code and
data available for research at
https://github.com/robot-perception-group/AirPose. A video describing the
approach and results is available at https://youtu.be/xLYe1TNHsfs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What can we learn from misclassified ImageNet images?. (arXiv:2201.08098v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08098">
<div class="article-summary-box-inner">
<span><p>Understanding the patterns of misclassified ImageNet images is particularly
important, as it could guide us to design deep neural networks (DNN) that
generalize better. However, the richness of ImageNet imposes difficulties for
researchers to visually find any useful patterns of misclassification. Here, to
help find these patterns, we propose "Superclassing ImageNet dataset". It is a
subset of ImageNet which consists of 10 superclasses, each containing 7-116
related subclasses (e.g., 52 bird types, 116 dog types). By training neural
networks on this dataset, we found that: (i) Misclassifications are rarely
across superclasses, but mainly among subclasses within a superclass. (ii)
Ensemble networks trained each only on subclasses of a given superclass perform
better than the same network trained on all subclasses of all superclasses.
Hence, we propose a two-stage Super-Sub framework, and demonstrate that: (i)
The framework improves overall classification performance by 3.3%, by first
inferring a superclass using a generalist superclass-level network, and then
using a specialized network for final subclass-level classification. (ii)
Although the total parameter storage cost increases to a factor N+1 for N
superclasses compared to using a single network, with finetuning, delta and
quantization aware training techniques this can be reduced to 0.2N+1. Another
advantage of this efficient implementation is that the memory cost on the GPU
during inference is equivalent to using only one network. The reason is we
initiate each subclass-level network through addition of small parameter
variations (deltas) to the superclass-level network. (iii) Finally, our
framework promises to be more scalable and generalizable than the common
alternative of simply scaling up a vanilla network in size, since very large
networks often suffer from overfitting and gradient vanishing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Model for Machine Thinking. (arXiv:2201.08122v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08122">
<div class="article-summary-box-inner">
<span><p>A machine thinking model is proposed in this report based on recent advances
of computer vision and the recent results of neuroscience devoted to brain
understanding. We deliver the result of machine thinking in the form of
sentences of natural-language or drawn sketches either informative or
decisional. This result is obtained from a reasoning performed on new acquired
data and memorized data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Unsupervised Contrastive Hashing for Large-Scale Cross-Modal Text-Image Retrieval in Remote Sensing. (arXiv:2201.08125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08125">
<div class="article-summary-box-inner">
<span><p>Due to the availability of large-scale multi-modal data (e.g., satellite
images acquired by different sensors, text sentences, etc) archives, the
development of cross-modal retrieval systems that can search and retrieve
semantically relevant data across different modalities based on a query in any
modality has attracted great attention in RS. In this paper, we focus our
attention on cross-modal text-image retrieval, where queries from one modality
(e.g., text) can be matched to archive entries from another (e.g., image). Most
of the existing cross-modal text-image retrieval systems require a high number
of labeled training samples and also do not allow fast and memory-efficient
retrieval due to their intrinsic characteristics. These issues limit the
applicability of the existing cross-modal retrieval systems for large-scale
applications in RS. To address this problem, in this paper we introduce a novel
deep unsupervised cross-modal contrastive hashing (DUCH) method for RS
text-image retrieval. The proposed DUCH is made up of two main modules: 1)
feature extraction module (which extracts deep representations of the
text-image modalities); and 2) hashing module (which learns to generate
cross-modal binary hash codes from the extracted representations). Within the
hashing module, we introduce a novel multi-objective loss function including:
i) contrastive objectives that enable similarity preservation in both intra-
and inter-modal similarities; ii) an adversarial objective that is enforced
across two modalities for cross-modal representation consistency; iii)
binarization objectives for generating representative hash codes. Experimental
results show that the proposed DUCH outperforms state-of-the-art unsupervised
cross-modal hashing methods on two multi-modal (image and text) benchmark
archives in RS. Our code is publicly available at
https://git.tu-berlin.de/rsim/duch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoFill: Reference-Based Image Inpainting of Scenes with Complex Geometry. (arXiv:2201.08131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08131">
<div class="article-summary-box-inner">
<span><p>Reference-guided image inpainting restores image pixels by leveraging the
content from another reference image. The previous state-of-the-art, TransFill,
warps the source image with multiple homographies, and fuses them together for
hole filling. Inspired by structure from motion pipelines and recent progress
in monocular depth estimation, we propose a more principled approach that does
not require heuristic planar assumptions. We leverage a monocular depth
estimate and predict relative pose between cameras, then align the reference
image to the target by a differentiable 3D reprojection and a joint
optimization of relative pose and depth map scale and offset. Our approach
achieves state-of-the-art performance on both RealEstate10K and
MannequinChallenge dataset with large baselines, complex geometry and extreme
camera motions. We experimentally verify our approach is also better at
handling large holes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPAMs: Structured Implicit Parametric Models. (arXiv:2201.08141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08141">
<div class="article-summary-box-inner">
<span><p>Parametric 3D models have formed a fundamental role in modeling deformable
objects, such as human bodies, faces, and hands; however, the construction of
such parametric models requires significant manual intervention and domain
expertise. Recently, neural implicit 3D representations have shown great
expressibility in capturing 3D shape geometry. We observe that deformable
object motion is often semantically structured, and thus propose to learn
Structured-implicit PArametric Models (SPAMs) as a deformable object
representation that structurally decomposes non-rigid object motion into
part-based disentangled representations of shape and pose, with each being
represented by deep implicit functions. This enables a structured
characterization of object movement, with part decomposition characterizing a
lower-dimensional space in which we can establish coarse motion correspondence.
In particular, we can leverage the part decompositions at test time to fit to
new depth sequences of unobserved shapes, by establishing part correspondences
between the input observation and our learned part spaces; this guides a robust
joint optimization between the shape and pose of all parts, even under dramatic
motion sequences. Experiments demonstrate that our part-aware shape and pose
understanding lead to state-of-the-art performance in reconstruction and
tracking of depth sequences of complex deforming object motion. We plan to
release models to the public at https://pablopalafox.github.io/spams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically Embodied Deep Image Optimisation. (arXiv:2201.08142v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08142">
<div class="article-summary-box-inner">
<span><p>Physical sketches are created by learning programs to control a drawing
robot. A differentiable rasteriser is used to optimise sets of drawing strokes
to match an input image, using deep networks to provide an encoding for which
we can compute a loss. The optimised drawing primitives can then be translated
into G-code commands which command a robot to draw the image using drawing
instruments such as pens and pencils on a physical support medium.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WPPNets: Unsupervised CNN Training with Wasserstein Patch Priors for Image Superresolution. (arXiv:2201.08157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08157">
<div class="article-summary-box-inner">
<span><p>We introduce WPPNets, which are CNNs trained by a new unsupervised loss
function for image superresolution of materials microstructures. Instead of
requiring access to a large database of registered high- and low-resolution
images, we only assume to know a large database of low resolution images, the
forward operator and one high-resolution reference image. Then, we propose a
loss function based on the Wasserstein patch prior which measures the
Wasserstein-2 distance between the patch distributions of the predictions and
the reference image. We demonstrate by numerical examples that WPPNets
outperform other methods with similar assumptions. In particular, we show that
WPPNets are much more stable under inaccurate knowledge or perturbations of the
forward operator. This enables us to use them in real-world applications, where
neither a large database of registered data nor the exact forward operator are
given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HumanIBR: High Quality Image-based Rendering of Challenging Human Performers using Sparse Views. (arXiv:2201.08158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08158">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce HumanIBR, a method that addresses the challenge
of novel view rendering of human performers that wear clothes with complex
patterns using a sparse set of camera views. Some recent works have achieved
remarkable rendering quality on humans that wear pure clothes using sparse
views, but if the clothes have complex color patterns, the rendering quality is
still very low. To this end, the proposed HumanIBR uses a human reconstruction
net with pixel-aligned spatial transformer and a render net that uses
geometry-guided pixel-wise feature integration to achieve to goal of high
quality human reconstruction and rendering. The designed pixel-aligned spatial
transformer calculates the correlations between the input views, producing
human reconstruction results with high-frequency details presented in the input
views. Based on the reconstruction, the geometry-guided pixel-wise visibility
reasoning provides a guidance for multi-view feature integration, enabling the
render net to render high quality images on novel views. Unlike previous neural
rendering works that always need to train or fine-tune a separate network for
each scene or human, our method is a general framework that is able to
generalize to novel humans. Experiments show that our approach outperforms all
the prior general or human-specific works on both synthetic data and real-world
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CP-Net: Contour-Perturbed Reconstruction Network for Self-Supervised Point Cloud Learning. (arXiv:2201.08215v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08215">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has not been fully explored for point cloud
analysis. Current frameworks are mainly based on point cloud reconstruction.
Given only 3D coordinates, such approaches tend to learn local geometric
structures and contours, while failing in understanding high level semantic
content. Consequently, they achieve unsatisfactory performance in downstream
tasks such as classification, segmentation, etc. To fill this gap, we propose a
generic Contour-Perturbed Reconstruction Network (CP-Net), which can
effectively guide self-supervised reconstruction to learn semantic content in
the point cloud, and thus promote discriminative power of point cloud
representation. First, we introduce a concise contour-perturbed augmentation
module for point cloud reconstruction. With guidance of geometry disentangling,
we divide point cloud into contour and content components. Subsequently, we
perturb the contour components and preserve the content components on the point
cloud. As a result, self supervisor can effectively focus on semantic content,
by reconstructing the original point cloud from such perturbed one. Second, we
use this perturbed reconstruction as an assistant branch, to guide the learning
of basic reconstruction branch via a distinct dual-branch consistency loss. In
this case, our CP-Net not only captures structural contour but also learn
semantic content for discriminative downstream tasks. Finally, we perform
extensive experiments on a number of point cloud benchmarks. Part segmentation
results demonstrate that our CP-Net (81.5% of mIoU) outperforms the previous
self-supervised models, and narrows the gap with the fully-supervised methods.
For classification, we get a competitive result with the fully-supervised
methods on ModelNet40 (92.5% accuracy) and ScanObjectNN (87.9% accuracy). The
codes and models will be released afterwards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watermarking Pre-trained Encoders in Contrastive Learning. (arXiv:2201.08217v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08217">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has become a popular technique to pre-train image
encoders, which could be used to build various downstream classification models
in an efficient way. This process requires a large amount of data and
computation resources. Hence, the pre-trained encoders are an important
intellectual property that needs to be carefully protected. It is challenging
to migrate existing watermarking techniques from the classification tasks to
the contrastive learning scenario, as the owner of the encoder lacks the
knowledge of the downstream tasks which will be developed from the encoder in
the future. We propose the \textit{first} watermarking methodology for the
pre-trained encoders. We introduce a task-agnostic loss function to effectively
embed into the encoder a backdoor as the watermark. This backdoor can still
exist in any downstream models transferred from the encoder. Extensive
evaluations over different contrastive learning algorithms, datasets, and
downstream tasks indicate our watermarks exhibit high effectiveness and
robustness against different adversarial operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Generative Pretraining for Multimodal Video Captioning. (arXiv:2201.08264v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08264">
<div class="article-summary-box-inner">
<span><p>Recent video and language pretraining frameworks lack the ability to generate
sentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new
pretraining framework for learning from unlabelled videos which can be
effectively used for generative tasks such as multimodal video captioning.
Unlike recent video-language pretraining frameworks, our framework trains both
a multimodal video encoder and a sentence decoder jointly. To overcome the lack
of captions in unlabelled videos, we leverage the future utterance as an
additional text source and propose a bidirectional generation objective -- we
generate future utterances given the present mulitmodal context, and also the
present utterance given future observations. With this objective, we train an
encoder-decoder model end-to-end to generate a caption from raw pixels and
transcribed speech directly. Our model achieves state-of-the-art performance
for multimodal video captioning on four standard benchmarks, as well as for
other video understanding tasks such as VideoQA, video retrieval and action
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Rendering for Integral Imaging Light Field Displays Based on a Voxel-Pixel Lookup Table. (arXiv:2201.08266v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08266">
<div class="article-summary-box-inner">
<span><p>A real-time elemental image array (EIA) generation method which does not
sacrifice accuracy nor rely on high-performance hardware is developed, through
raytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both
offline and online working flow, experiments verified the effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling and hexahedral meshing of arterial networks from centerlines. (arXiv:2201.08279v1 [cs.CG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08279">
<div class="article-summary-box-inner">
<span><p>Computational fluid dynamics (CFD) simulation provides valuable information
on blood flow from the vascular geometry. However, it requires to extract
accurate models of arteries from low resolution medical images, which remains
challenging. Centerline-based representation is widely used to model large
vascular networks with small vessels, as it enables manual editing and encodes
the topological information. In this work, we propose an automatic method to
generate an hexahedral mesh suitable for CFD directly from centerlines. The
proposed method is an improvement of the state-of-the-art in terms of
robustness, mesh quality and reproductibility.
</p>
<p>Both the modeling and meshing tasks are addressed. A new vessel model based
on penalized splines is proposed to overcome the limitations inherent to the
centerline representation, such as noise and sparsity. Bifurcations are
reconstructed using a physiologically accurate parametric model that we
extended to planar n-furcations. Finally, a volume mesh with structured,
hexahedral and flow oriented cells is produced from the proposed vascular
network model.
</p>
<p>The proposed method offers a better robustness and mesh quality than the
state-of-the-art methods. As it combines both modeling and meshing techniques,
it can be applied to edit the geometry and topology of vascular models
effortlessly to study the impact on hemodynamics. We demonstrate the efficiency
of our method by entirely meshing a dataset of 60 cerebral vascular networks.
92\% of the vessels and 83\% of the bifurcations where mesh without defects
needing manual intervention, despite the challenging aspect of the input data.
The source code will be released publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIVA-DAF: A Deep Learning Framework for Historical Document Image Analysis. (arXiv:2201.08295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08295">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a new deep learning framework called DIVA-DAF. We
have developed this framework to support our research on historical document
image analysis tasks and to develop techniques to reduce the need for
manually-labeled ground truth. We want to apply self-supervised learning
techniques and use different kinds of training data. Our new framework aids us
in performing rapid prototyping and reproducible experiments. We present a
first semantic segmentation experiment on DIVA-HisDB using our framework,
achieving state-of-the-art results. The DIVA-DAF framework is open-source, and
we encourage other research groups to use it for their experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stitch it in Time: GAN-Based Facial Editing of Real Videos. (arXiv:2201.08361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08361">
<div class="article-summary-box-inner">
<span><p>The ability of Generative Adversarial Networks to encode rich semantics
within their latent space has been widely adopted for facial image editing.
However, replicating their success with videos has proven challenging. Sets of
high-quality facial videos are lacking, and working with videos introduces a
fundamental barrier to overcome - temporal coherency. We propose that this
barrier is largely artificial. The source video is already temporally coherent,
and deviations from this state arise in part due to careless treatment of
individual components in the editing pipeline. We leverage the natural
alignment of StyleGAN and the tendency of neural networks to learn low
frequency functions, and demonstrate that they provide a strongly consistent
prior. We draw on these insights and propose a framework for semantic editing
of faces in videos, demonstrating significant improvements over the current
state-of-the-art. Our method produces meaningful face manipulations, maintains
a higher degree of temporal consistency, and can be applied to challenging,
high quality, talking head videos which current methods struggle with.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Weakly Supervised Pre-Training of Visual Perception Models. (arXiv:2201.08371v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08371">
<div class="article-summary-box-inner">
<span><p>Model pre-training is a cornerstone of modern visual recognition systems.
Although fully supervised pre-training on datasets like ImageNet is still the
de-facto standard, recent studies suggest that large-scale weakly supervised
pre-training can outperform fully supervised approaches. This paper revisits
weakly-supervised pre-training of models using hashtag supervision with modern
versions of residual networks and the largest-ever dataset of images and
corresponding hashtags. We study the performance of the resulting models in
various transfer-learning settings including zero-shot transfer. We also
compare our models with those obtained via large-scale self-supervised
learning. We find our weakly-supervised models to be very competitive across
all settings, and find they substantially outperform their self-supervised
counterparts. We also include an investigation into whether our models learned
potentially troubling associations or stereotypes. Overall, our results provide
a compelling argument for the use of weakly supervised learning in the
development of visual recognition systems. Our models, Supervised Weakly
through hashtAGs (SWAG), are available publicly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omnivore: A Single Model for Many Visual Modalities. (arXiv:2201.08377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08377">
<div class="article-summary-box-inner">
<span><p>Prior work has studied different visual modalities in isolation and developed
separate architectures for recognition of images, videos, and 3D data. Instead,
in this paper, we propose a single model which excels at classifying images,
videos, and single-view 3D data using exactly the same model parameters. Our
'Omnivore' model leverages the flexibility of transformer-based architectures
and is trained jointly on classification tasks from different modalities.
Omnivore is simple to train, uses off-the-shelf standard datasets, and performs
at-par or better than modality-specific models of the same size. A single
Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN
RGB-D. After finetuning, our models outperform prior work on a variety of
vision tasks and generalize across modalities. Omnivore's shared visual
representation naturally enables cross-modal recognition without access to
correspondences between modalities. We hope our results motivate researchers to
model visual modalities together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Pixel Trajectories with Multiscale Contrastive Random Walks. (arXiv:2201.08379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08379">
<div class="article-summary-box-inner">
<span><p>A range of video modeling tasks, from optical flow to multiple object
tracking, share the same fundamental challenge: establishing space-time
correspondence. Yet, approaches that dominate each space differ. We take a step
towards bridging this gap by extending the recent contrastive random walk
formulation to much denser, pixel-level space-time graphs. The main
contribution is introducing hierarchy into the search problem by computing the
transition matrix between two frames in a coarse-to-fine manner, forming a
multiscale contrastive random walk when extended in time. This establishes a
unified technique for self-supervised learning of optical flow, keypoint
tracking, and video object segmentation. Experiments demonstrate that, for each
of these tasks, the unified model achieves performance competitive with strong
self-supervised approaches specific to that task. Project site:
https://jasonbian97.github.io/flowwalk
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition. (arXiv:2201.08383v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08383">
<div class="article-summary-box-inner">
<span><p>While today's video recognition systems parse snapshots or short clips
accurately, they cannot connect the dots and reason across a longer range of
time yet. Most existing video architectures can only process &lt;5 seconds of a
video without hitting the computation or memory bottlenecks.
</p>
<p>In this paper, we propose a new strategy to overcome this challenge. Instead
of trying to process more frames at once like most existing methods, we propose
to process videos in an online fashion and cache "memory" at each iteration.
Through the memory, the model can reference prior context for long-term
modeling, with only a marginal cost. Based on this idea, we build MeMViT, a
Memory-augmented Multiscale Vision Transformer, that has a temporal support 30x
longer than existing models with only 4.5% more compute; traditional methods
need &gt;3,000% more compute to do the same. On a wide range of settings, the
increased temporal support enabled by MeMViT brings large gains in recognition
accuracy consistently. MeMViT obtains state-of-the-art results on the AVA,
EPIC-Kitchens-100 action classification, and action anticipation datasets. Code
and models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing the train-test resolution discrepancy. (arXiv:1906.06423v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.06423">
<div class="article-summary-box-inner">
<span><p>Data-augmentation is key to the training of neural networks for image
classification. This paper first shows that existing augmentations induce a
significant discrepancy between the typical size of the objects seen by the
classifier at train and test time. We experimentally validate that, for a
target test resolution, using a lower train resolution offers better
classification at test time.
</p>
<p>We then propose a simple yet effective and efficient strategy to optimize the
classifier performance when the train and test resolutions differ. It involves
only a computationally cheap fine-tuning of the network at the test resolution.
This enables training strong classifiers using small training images. For
instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained
on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if
we use extra training data we get 82.5% with the ResNet-50 train with 224x224
images.
</p>
<p>Conversely, when training a ResNeXt-101 32x48d pre-trained in
weakly-supervised fashion on 940 million public images at resolution 224x224
and further optimizing for test resolution 320x320, we obtain a test top-1
accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge
this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dimensionality reduction to maximize prediction generalization capability. (arXiv:2003.00470v2 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00470">
<div class="article-summary-box-inner">
<span><p>Generalization of time series prediction remains an important open issue in
machine learning, wherein earlier methods have either large generalization
error or local minima. We develop an analytically solvable, unsupervised
learning scheme that extracts the most informative components for predicting
future inputs, termed predictive principal component analysis (PredPCA). Our
scheme can effectively remove unpredictable noise and minimize test prediction
error through convex optimization. Mathematical analyses demonstrate that,
provided with sufficient training samples and sufficiently high-dimensional
observations, PredPCA can asymptotically identify hidden states, system
parameters, and dimensionalities of canonical nonlinear generative processes,
with a global convergence guarantee. We demonstrate the performance of PredPCA
using sequential visual inputs comprising hand-digits, rotating 3D objects, and
natural scenes. It reliably estimates distinct hidden states and predicts
future outcomes of previously unseen test input data, based exclusively on
noisy observations. The simple architecture and low computational cost of
PredPCA are highly desirable for neuromorphic hardware.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Bounding-box Regression with Distance-IoU Loss for Visual Tracking. (arXiv:2007.01864v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01864">
<div class="article-summary-box-inner">
<span><p>Most existing trackers are based on using a classifier and multi-scale
estimation to estimate the target state. Consequently, and as expected,
trackers have become more stable while tracking accuracy has stagnated. While
trackers adopt a maximum overlap method based on an intersection-over-union
(IoU) loss to mitigate this problem, there are defects in the IoU loss itself,
that make it impossible to continue to optimize the objective function when a
given bounding box is completely contained within/without another bounding box;
this makes it very challenging to accurately estimate the target state.
Accordingly, in this paper, we address the above-mentioned problem by proposing
a novel tracking method based on a distance-IoU (DIoU) loss, such that the
proposed tracker consists of target estimation and target classification. The
target estimation part is trained to predict the DIoU score between the target
ground-truth bounding-box and the estimated bounding-box. The DIoU loss can
maintain the advantage provided by the IoU loss while minimizing the distance
between the center points of two bounding boxes, thereby making the target
estimation more accurate. Moreover, we introduce a classification part that is
trained online and optimized with a Conjugate-Gradient-based strategy to
guarantee real-time tracking speed. Comprehensive experimental results
demonstrate that the proposed method achieves competitive tracking accuracy
when compared to state-of-the-art trackers while with a real-time tracking
speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision and Normalizing Flow-Based Defect Detection. (arXiv:2012.06737v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06737">
<div class="article-summary-box-inner">
<span><p>Visual defect detection is critical to ensure the quality of most products.
However, majority of small medium manufactures still rely on tedious and
error-prune human manual inspection. The main reasons include: 1) the existing
automated visual defect detection systems require altering production assembly
lines, which is time consuming and expensive 2) the existing systems require
manually collecting defective samples and labeling them for a comparison-based
algorithm or training a machine learning model. This introduces heavy burden
for Small and Medium-sized Enterprise (SME) manufactures as defects do not
happen often and are difficult and time-consuming to collect. Furthermore, we
cannot exhaustively collect or define all defect types as any new deviation
from acceptable products are defects. In this paper, we overcome these
challenges and design a three-stage plug-and-play fully automated unsupervised
360-degree defect detection system. In our system, products are freely placed
on an unaltered assembly line and receive 360 degree visual inspection with
multiple cameras from different angles. As such, the images collected from
real-world product assembly lines contain lots of background noise. The
products face different angles. The product sizes vary due to the distance to
cameras. All these make defect detection much more difficult. Our system use
object detection, background subtraction and unsupervised normalizing
flow-based defect detection techniques to tackle these difficulty. Experiments
show our system can achieve 0.90 AUROC in a real-world non-altered drink ware
production assembly line.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance and Panoptic Segmentation Using Conditional Convolutions. (arXiv:2102.03026v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03026">
<div class="article-summary-box-inner">
<span><p>We propose a simple yet effective framework for instance and panoptic
segmentation, termed CondInst (conditional convolutions for instance and
panoptic segmentation). In the literature, top-performing instance segmentation
methods typically follow the paradigm of Mask R-CNN and rely on ROI operations
(typically ROIAlign) to attend to each instance. In contrast, we propose to
attend to the instances with dynamic conditional convolutions. Instead of using
instance-wise ROIs as inputs to the instance mask head of fixed weights, we
design dynamic instance-aware mask heads, conditioned on the instances to be
predicted. CondInst enjoys three advantages: 1.) Instance and panoptic
segmentation are unified into a fully convolutional network, eliminating the
need for ROI cropping and feature alignment. 2.) The elimination of the ROI
cropping also significantly improves the output instance mask resolution. 3.)
Due to the much improved capacity of dynamically-generated conditional
convolutions, the mask head can be very compact (e.g., 3 conv. layers, each
having only 8 channels), leading to significantly faster inference time per
instance and making the overall inference time almost constant, irrelevant to
the number of instances. We demonstrate a simpler method that can achieve
improved accuracy and inference speed on both instance and panoptic
segmentation tasks. On the COCO dataset, we outperform a few state-of-the-art
methods. We hope that CondInst can be a strong baseline for instance and
panoptic segmentation. Code is available at: https://git.io/AdelaiDet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Single Image Generation with Controllable Global Structure. (arXiv:2102.04780v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04780">
<div class="article-summary-box-inner">
<span><p>Image generation from a single image using generative adversarial networks is
quite interesting due to the realism of generated images. However, recent
approaches need improvement for such realistic and diverse image generation,
when the global context of the image is important such as in face, animal, and
architectural image generation. This is mainly due to the use of fewer
convolutional layers for mainly capturing the patch statistics and, thereby,
not being able to capture global statistics very well. We solve this problem by
using attention blocks at selected scales and feeding a random Gaussian blurred
image to the discriminator for training. Our results are visually better than
the state-of-the-art particularly in generating images that require global
context. The diversity of our image generation, measured using the average
standard deviation of pixels, is also better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Head-Position Prediction in First-Person View by Considering Head Pose for Human-Robot Eye Contact. (arXiv:2103.06417v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06417">
<div class="article-summary-box-inner">
<span><p>For a humanoid robot to make eye contact and initiate communication with a
person, it is necessary to estimate the person's head position. However, eye
contact becomes difficult due to the mechanical delay of the robot when the
person is moving. Owing to these issues, it is important to conduct a
head-position prediction to mitigate the effect of the delay in the robot
motion. Based on the fact that humans turn their heads before changing
direction while walking, we hypothesized that the accuracy of three-dimensional
(3D) head-position prediction from a first-person view can be improved by
considering the head pose. We compared our method with a conventional Kalman
filter-based approach, and found our method to be more accurate. The experiment
results show that considering the head pose helps improve the accuracy of 3D
head-position prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duplex Contextual Relation Network for Polyp Segmentation. (arXiv:2103.06725v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06725">
<div class="article-summary-box-inner">
<span><p>Polyp segmentation is of great importance in the early diagnosis and
treatment of colorectal cancer. Since polyps vary in their shape, size, color,
and texture, accurate polyp segmentation is very challenging. One promising way
to mitigate the diversity of polyps is to model the contextual relation for
each pixel such as using attention mechanism. However, previous methods only
focus on learning the dependencies between the position within an individual
image and ignore the contextual relation across different images. In this
paper, we propose Duplex Contextual Relation Network (DCRNet) to capture both
within-image and cross-image contextual relations. Specifically, we first
design Interior Contextual-Relation Module to estimate the similarity between
each position and all the positions within the same image. Then Exterior
Contextual-Relation Module is incorporated to estimate the similarity between
each position and the positions across different images. Based on the above two
types of similarity, the feature at one position can be further enhanced by the
contextual region embedding within and across images. To store the
characteristic region embedding from all the images, a memory bank is designed
and operates as a queue. Therefore, the proposed method can relate similar
features even though they come from different images. We evaluate the proposed
method on the EndoScene, Kvasir-SEG and the recently released large-scale
PICCOLO dataset. Experimental results show that the proposed DCRNet outperforms
the state-of-the-art methods in terms of the widely-used evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MBAPose: Mask and Bounding-Box Aware Pose Estimation of Surgical Instruments with Photorealistic Domain Randomization. (arXiv:2103.08105v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08105">
<div class="article-summary-box-inner">
<span><p>Surgical robots are usually controlled using a priori models based on the
robots' geometric parameters, which are calibrated before the surgical
procedure. One of the challenges in using robots in real surgical settings is
that those parameters can change over time, consequently deteriorating control
accuracy. In this context, our group has been investigating online calibration
strategies without added sensors. In one step toward that goal, we have
developed an algorithm to estimate the pose of the instruments' shafts in
endoscopic images. In this study, we build upon that earlier work and propose a
new framework to more precisely estimate the pose of a rigid surgical
instrument. Our strategy is based on a novel pose estimation model called
MBAPose and the use of synthetic training data. Our experiments demonstrated an
improvement of 21 % for translation error and 26 % for orientation error on
synthetic test data with respect to our previous work. Results with real test
data provide a baseline for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Spatio-Temporal Interaction Learning for Video Deraining: A Faster and Better Framework. (arXiv:2103.12318v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12318">
<div class="article-summary-box-inner">
<span><p>Video deraining is an important task in computer vision as the unwanted rain
hampers the visibility of videos and deteriorates the robustness of most
outdoor vision systems. Despite the significant success which has been achieved
for video deraining recently, two major challenges remain: 1) how to exploit
the vast information among continuous frames to extract powerful
spatio-temporal features across both the spatial and temporal domains, and 2)
how to restore high-quality derained videos with a high-speed approach. In this
paper, we present a new end-to-end video deraining framework, named Enhanced
Spatio-Temporal Interaction Network (ESTINet), which considerably boosts
current state-of-the-art video deraining quality and speed. The ESTINet takes
the advantage of deep residual networks and convolutional long short-term
memory, which can capture the spatial features and temporal correlations among
continuing frames at the cost of very little computational source. Extensive
experiments on three public datasets show that the proposed ESTINet can achieve
faster speed than the competitors, while maintaining better performance than
the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption. (arXiv:2103.16201v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16201">
<div class="article-summary-box-inner">
<span><p>An unresolved problem in Deep Learning is the ability of neural networks to
cope with domain shifts during test-time, imposed by commonly fixing network
parameters after training. Our proposed method Meta Test-Time Training (MT3),
however, breaks this paradigm and enables adaption at test-time. We combine
meta-learning, self-supervision and test-time training to learn to adapt to
unseen test distributions. By minimizing the self-supervised loss, we learn
task-specific model parameters for different tasks. A meta-model is optimized
such that its adaption to the different task-specific models leads to higher
performance on those tasks. During test-time a single unlabeled image is
sufficient to adapt the meta-model parameters. This is achieved by minimizing
only the self-supervised loss component resulting in a better prediction for
that image. Our approach significantly improves the state-of-the-art results on
the CIFAR-10-Corrupted image classification benchmark. Our implementation is
available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Aliased Resizing and Surprising Subtleties in GAN Evaluation. (arXiv:2104.11222v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11222">
<div class="article-summary-box-inner">
<span><p>Metrics for evaluating generative models aim to measure the discrepancy
between real and generated images. The often-used Frechet Inception Distance
(FID) metric, for example, extracts "high-level" features using a deep network
from the two sets. However, we find that the differences in "low-level"
preprocessing, specifically image resizing and compression, can induce large
variations and have unforeseen consequences. For instance, when resizing an
image, e.g., with a bilinear or bicubic kernel, signal processing principles
mandate adjusting prefilter width depending on the downsampling factor, to
antialias to the appropriate bandwidth. However, commonly-used implementations
use a fixed-width prefilter, resulting in aliasing artifacts. Such aliasing
leads to corruptions in the feature extraction downstream. Next, lossy
compression, such as JPEG, is commonly used to reduce the file size of an
image. Although designed to minimally degrade the perceptual quality of an
image, the operation also produces variations downstream. Furthermore, we show
that if compression is used on real training images, FID can actually improve
if the generated images are also subsequently compressed. This paper shows that
choices in low-level image processing have been an underappreciated aspect of
generative modeling. We identify and characterize variations in generative
modeling development pipelines, provide recommendations based on signal
processing principles, and release a reference implementation to facilitate
future comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. (arXiv:2106.11944v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11944">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to create generalizable and controllable neural signed
distance fields (SDFs) that represent clothed humans from monocular depth
observations. Recent advances in deep learning, especially neural implicit
representations, have enabled human shape reconstruction and controllable
avatar generation from different sensor inputs. However, to generate realistic
cloth deformations from novel input poses, watertight meshes or dense full-body
scans are usually needed as inputs. Furthermore, due to the difficulty of
effectively modeling pose-dependent cloth deformations for diverse body shapes
and cloth types, existing approaches resort to per-subject/cloth-type
optimization from scratch, which is computationally expensive. In contrast, we
propose an approach that can quickly generate realistic clothed human avatars,
represented as controllable neural SDFs, given only monocular depth images. We
achieve this by using meta-learning to learn an initialization of a
hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is
conditioned on human poses and represents a clothed neural avatar that deforms
non-rigidly according to the input poses. Meanwhile, it is meta-learned to
effectively incorporate priors of diverse body shapes and cloth types and thus
can be much faster to fine-tune, compared to models trained from scratch. We
qualitatively and quantitatively show that our approach outperforms
state-of-the-art approaches that require complete meshes as inputs while our
approach requires only depth frames as inputs and runs orders of magnitudes
faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very
robust, being the first to generate avatars with realistic dynamic cloth
deformations given as few as 8 monocular depth frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense Top-View Understanding. (arXiv:2107.00346v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00346">
<div class="article-summary-box-inner">
<span><p>At the heart of all automated driving systems is the ability to sense the
surroundings, e.g., through semantic segmentation of LiDAR sequences, which
experienced a remarkable progress due to the release of large datasets such as
SemanticKITTI and nuScenes-LidarSeg. While most previous works focus on sparse
segmentation of the LiDAR input, dense output masks provide self-driving cars
with almost complete environment information. In this paper, we introduce MASS
- a Multi-Attentional Semantic Segmentation model specifically built for dense
top-view understanding of the driving scenes. Our framework operates on pillar-
and occupancy features and comprises three attention-based building blocks: (1)
a keypoint-driven graph attention, (2) an LSTM-based attention computed from a
vector embedding of the spatial input, and (3) a pillar-based attention,
resulting in a dense 360-degree segmentation mask. With extensive experiments
on both, SemanticKITTI and nuScenes-LidarSeg, we quantitatively demonstrate the
effectiveness of our model, outperforming the state of the art by 19.0% on
SemanticKITTI and reaching 30.4% in mIoU on nuScenes-LidarSeg, where MASS is
the first work addressing the dense segmentation task. Furthermore, our
multi-attention model is shown to be very effective for 3D object detection
validated on the KITTI-3D dataset, showcasing its high generalizability to
other tasks related to 3D vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining EfficientNet and Vision Transformers for Video Deepfake Detection. (arXiv:2107.02612v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02612">
<div class="article-summary-box-inner">
<span><p>Deepfakes are the result of digital manipulation to forge realistic yet fake
imagery. With the astonishing advances in deep generative models, fake images
or videos are nowadays obtained using variational autoencoders (VAEs) or
Generative Adversarial Networks (GANs). These technologies are becoming more
accessible and accurate, resulting in fake videos that are very difficult to be
detected. Traditionally, Convolutional Neural Networks (CNNs) have been used to
perform video deepfake detection, with the best results obtained using methods
based on EfficientNet B7. In this study, we focus on video deep fake detection
on faces, given that most methods are becoming extremely accurate in the
generation of realistic human faces. Specifically, we combine various types of
Vision Transformers with a convolutional EfficientNet B0 used as a feature
extractor, obtaining comparable results with some very recent methods that use
Vision Transformers. Differently from the state-of-the-art approaches, we use
neither distillation nor ensemble methods. Furthermore, we present a
straightforward inference procedure based on a simple voting scheme for
handling multiple faces in the same video shot. The best model achieved an AUC
of 0.951 and an F1 score of 88.0%, very close to the state-of-the-art on the
DeepFake Detection Challenge (DFDC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MitoDet: Simple and robust mitosis detection. (arXiv:2109.01485v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01485">
<div class="article-summary-box-inner">
<span><p>Mitotic figure detection is a challenging task in digital pathology that has
a direct impact on therapeutic decisions. While automated methods often achieve
acceptable results under laboratory conditions, they frequently fail in the
clinical deployment phase. This problem can be mainly attributed to a
phenomenon called domain shift. An important source of a domain shift is
introduced by different microscopes and their camera systems, which noticeably
change the color representation of digitized images. In this method description
we present our submitted algorithm for the Mitosis Domain Generalization
Challenge, which employs a RetinaNet trained with strong data augmentation and
achieves an F1 score of 0.7138 on the preliminary test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03082">
<div class="article-summary-box-inner">
<span><p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach
with recurrent conditional GAN. In our approach, we employ the recurrent
auto-encoder-based compression network as the generator, and the most
importantly, we propose a recurrent conditional discriminator, which judges raw
and compressed video conditioned on both spatial and temporal features,
including the latent representation, temporal motion and hidden states in
recurrent cells. This way, in the adversarial training, it pushes the generated
video to be not only spatially photo-realistic but also temporally consistent
with groundtruth and coherent among video frames. The experimental results show
that the proposed PLVC model learns to compress video towards good perceptual
quality at low bit-rate, and outperforms the official HEVC test model (HM
16.20) and the previous learned approaches on several perceptual quality
metrics and user studies. The codes will be released at the project page:
https://github.com/RenYang-home/PLVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04797">
<div class="article-summary-box-inner">
<span><p>Computational fluid dynamics (CFD) is a valuable tool for personalised,
non-invasive evaluation of hemodynamics in arteries, but its complexity and
time-consuming nature prohibit large-scale use in practice. Recently, the use
of deep learning for rapid estimation of CFD parameters like wall shear stress
(WSS) on surface meshes has been investigated. However, existing approaches
typically depend on a hand-crafted re-parametrisation of the surface mesh to
match convolutional neural network architectures. In this work, we propose to
instead use mesh convolutional neural networks that directly operate on the
same finite-element surface mesh as used in CFD. We train and evaluate our
method on two datasets of synthetic coronary artery models with and without
bifurcation, using a ground truth obtained from CFD simulation. We show that
our flexible deep learning model can accurately predict 3D WSS vectors on this
surface mesh. Our method processes new meshes in less than 5 [s], consistently
achieves a normalised mean absolute error of $\leq$ 1.6 [%], and peaks at 90.5
[%] median approximation accuracy over the held-out test set, comparing
favourably to previously published work. This demonstrates the feasibility of
CFD surrogate modelling using mesh convolutional neural networks for
hemodynamic parameter estimation in artery models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S-Extension Patch: A simple and efficient way to extend an object detection model. (arXiv:2110.02670v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02670">
<div class="article-summary-box-inner">
<span><p>While building convolutional network-based systems, the toll it takes to
train the network is something that cannot be ignored. In cases where we need
to append additional capabilities to the existing model, the attention
immediately goes towards retraining techniques. In this paper, I show how to
leverage knowledge about the dataset to append the class faster while
maintaining the speed of inference as well as the accuracies; while reducing
the amount of time and data required. The method can extend a class in the
existing object detection model in 1/10th of the time compared to the other
existing methods. S-Extension patch not only offers faster training but also
speed and ease of adaptation, as it can be appended to any existing system,
given it fulfills the similarity threshold condition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Depth Completion for Active Stereo. (arXiv:2110.03234v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03234">
<div class="article-summary-box-inner">
<span><p>Active stereo systems are used in many robotic applications that require 3D
information. These depth sensors, however, suffer from stereo artefacts and do
not provide dense depth estimates.In this work, we present the first
self-supervised depth completion method for active stereo systems that predicts
accurate dense depth maps. Our system leverages a feature-based visual inertial
SLAM system to produce motion estimates and accurate (but sparse) 3D landmarks.
The 3D landmarks are used both as model input and as supervision during
training. The motion estimates are used in our novel reconstruction loss that
relies on a combination of passive and active stereo frames, resulting in
significant improvements in textureless areas that are common in indoor
environments. Due to the nonexistence of publicly available active stereo
datasets, we release a real dataset together with additional information for a
publicly available synthetic dataset (TartanAir [42]) needed for active depth
completion and prediction. Through rigorous evaluations we show that our method
outperforms state of the art on both datasets. Additionally we show how our
method obtains more complete, and therefore safer, 3D maps when used in a
robotic platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Structural Index: Leveraging a machine's perspective towards visual data. (arXiv:2110.04070v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04070">
<div class="article-summary-box-inner">
<span><p>With advances in vision and perception architectures, we have realized that
working with data is equally crucial, if not more, than the algorithms. Till
today, we have trained machines based on our knowledge and perspective of the
world. The entire concept of Dataset Structural Index(DSI) revolves around
understanding a machine`s perspective of the dataset. With DSI, I show two meta
values with which we can get more information over a visual dataset and use it
to optimize data, create better architectures, and have an ability to guess
which model would work best. These two values are the Variety contribution
ratio and Similarity matrix. In the paper, I show many applications of DSI, one
of which is how the same level of accuracy can be achieved with the same model
architectures trained over less amount of data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shared Visual Representations of Drawing for Communication: How do different biases affect human interpretability and intent?. (arXiv:2110.08203v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08203">
<div class="article-summary-box-inner">
<span><p>We present an investigation into how representational losses can affect the
drawings produced by artificial agents playing a communication game. Building
upon recent advances, we show that a combination of powerful pretrained encoder
networks, with appropriate inductive biases, can lead to agents that draw
recognisable sketches, whilst still communicating well. Further, we start to
develop an approach to help automatically analyse the semantic content being
conveyed by a sketch and demonstrate that current approaches to inducing
perceptual biases lead to a notion of objectness being a key feature despite
the agent training being self-supervised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BINAS: Bilinear Interpretable Neural Architecture Search. (arXiv:2110.12399v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12399">
<div class="article-summary-box-inner">
<span><p>Practical use of neural networks often involves requirements on latency,
energy and memory among others. A popular approach to find networks under such
requirements is through constrained Neural Architecture Search (NAS). However,
previous methods use complicated predictors for the accuracy of the network.
Those predictors are hard to interpret and sensitive to many hyperparameters to
be tuned, hence, the resulting accuracy of the generated models is often
harmed. In this work we resolve this by introducing Bilinear Interpretable
Neural Architecture Search (BINAS), that is based on an accurate and simple
bilinear formulation of both an accuracy estimator and the expected resource
requirement, together with a scalable search method with theoretical
guarantees. The simplicity of our proposed estimator together with the
intuitive way it is constructed bring interpretability through many insights
about the contribution of different design choices. For example, we find that
in the examined search space, adding depth and width is more effective at
deeper stages of the network and at the beginning of each resolution stage. Our
experiments show that BINAS generates comparable to or better architectures
than other state-of-the-art NAS methods within a reduced marginal search cost,
while strictly satisfying the resource constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Transformers Excel at Class-agnostic Object Detection. (arXiv:2111.11430v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11430">
<div class="article-summary-box-inner">
<span><p>What constitutes an object? This has been a long-standing question in
computer vision. Towards this goal, numerous learning-free and learning-based
approaches have been developed to score objectness. However, they generally do
not scale well across new domains and for unseen objects. In this paper, we
advocate that existing methods lack a top-down supervision signal governed by
human-understandable semantics. To bridge this gap, we explore recent
Multi-modal Vision Transformers (MViT) that have been trained with aligned
image-text pairs. Our extensive experiments across various domains and novel
objects show the state-of-the-art performance of MViTs to localize generic
objects in images. Based on these findings, we develop an efficient and
flexible MViT architecture using multi-scale feature processing and deformable
self-attention that can adaptively generate proposals given a specific language
query. We show the significance of MViT proposals in a diverse range of
applications including open-world object detection, salient and camouflage
object detection, supervised and self-supervised detection tasks. Further,
MViTs offer enhanced interactability with intelligible text queries. Code:
https://git.io/J1HPY.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector. (arXiv:2112.06102v2 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06102">
<div class="article-summary-box-inner">
<span><p>Vertebrate retinas are highly-efficient in processing trivial visual tasks
such as detecting moving objects, yet a complex task for modern computers. The
detection of object motion is done by specialised retinal ganglion cells named
Object-motion-sensitive ganglion cells (OMS-GC). OMS-GC process continuous
signals and generate spike patterns that are post-processed by the Visual
Cortex. The Neuromorphic Hybrid Spiking Motion Detector (NeuroHSMD) proposed in
this work accelerates the HSMD algorithm using Field-Programmable Gate Arrays
(FPGAs). The Hybrid Spiking Motion Detector (HSMD) algorithm was the first
hybrid algorithm to enhance dynamic background subtraction (DBS) algorithms
with a customised 3-layer spiking neural network (SNN) that generates OMS-GC
spiking-like responses. The NeuroHSMD algorithm was compared against the HSMD
algorithm, using the same 2012 change detection (CDnet2012) and 2014 change
detection (CDnet2014) benchmark datasets. The results show that the NeuroHSMD
has produced the same results as the HSMD algorithm in real-time without
degradation of quality. Moreover, the NeuroHSMD proposed in this paper was
completely implemented in Open Computer Language (OpenCL) and therefore is
easily replicated in other devices such as Graphical Processor Units (GPUs) and
clusters of Central Processor Units (CPUs).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PrintsGAN: Synthetic Fingerprint Generator. (arXiv:2201.03674v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03674">
<div class="article-summary-box-inner">
<span><p>A major impediment to researchers working in the area of fingerprint
recognition is the lack of publicly available, large-scale, fingerprint
datasets. The publicly available datasets that do exist contain very few
identities and impressions per finger. This limits research on a number of
topics, including e.g., using deep networks to learn fixed length fingerprint
embeddings. Therefore, we propose PrintsGAN, a synthetic fingerprint generator
capable of generating unique fingerprints along with multiple impressions for a
given fingerprint. Using PrintsGAN, we synthesize a database of 525k
fingerprints (35K distinct fingers, each with 15 impressions). Next, we show
the utility of the PrintsGAN generated dataset by training a deep network to
extract a fixed-length embedding from a fingerprint. In particular, an
embedding model trained on our synthetic fingerprints and fine-tuned on a small
number of publicly available real fingerprints (25K prints from NIST SD302)
obtains a TAR of 87.03% @ FAR=0.01% on the NIST SD4 database (a boost from
TAR=73.37% when only trained on NIST SD302). Prevailing synthetic fingerprint
generation methods do not enable such performance gains due to i) lack of
realism or ii) inability to generate multiple impressions per finger. We plan
to release our database of synthetic fingerprints to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Transformers for Video Recognition. (arXiv:2201.04288v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04288">
<div class="article-summary-box-inner">
<span><p>Video understanding requires reasoning at multiple spatiotemporal resolutions
-- from short fine-grained motions to events taking place over longer
durations. Although transformer architectures have recently advanced the
state-of-the-art, they have not explicitly modelled different spatiotemporal
resolutions. To this end, we present Multiview Transformers for Video
Recognition (MTV). Our model consists of separate encoders to represent
different views of the input video with lateral connections to fuse information
across views. We present thorough ablation studies of our model and show that
MTV consistently performs better than single-view counterparts in terms of
accuracy and computational cost across a range of model sizes. Furthermore, we
achieve state-of-the-art results on five standard datasets, and improve even
further with large-scale pretraining. We will release code and pretrained
checkpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Get your Foes Fooled: Proximal Gradient Split Learning for Defense against Model Inversion Attacks on IoMT data. (arXiv:2201.04569v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04569">
<div class="article-summary-box-inner">
<span><p>The past decade has seen a rapid adoption of Artificial Intelligence (AI),
specifically the deep learning networks, in Internet of Medical Things (IoMT)
ecosystem. However, it has been shown recently that the deep learning networks
can be exploited by adversarial attacks that not only make IoMT vulnerable to
the data theft but also to the manipulation of medical diagnosis. The existing
studies consider adding noise to the raw IoMT data or model parameters which
not only reduces the overall performance concerning medical inferences but also
is ineffective to the likes of deep leakage from gradients method. In this
work, we propose proximal gradient split learning (PSGL) method for defense
against the model inversion attacks. The proposed method intentionally attacks
the IoMT data when undergoing the deep neural network training process at
client side. We propose the use of proximal gradient method to recover gradient
maps and a decision-level fusion strategy to improve the recognition
performance. Extensive analysis show that the PGSL not only provides effective
defense mechanism against the model inversion attacks but also helps in
improving the recognition performance on publicly available datasets. We report
17.9$\%$ and 36.9$\%$ gains in accuracy over reconstructed and adversarial
attacked images, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Cross-Modality Retinal Vessel Segmentation via Disentangling Representation Style Transfer and Collaborative Consistency Learning. (arXiv:2201.04812v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04812">
<div class="article-summary-box-inner">
<span><p>Various deep learning models have been developed to segment anatomical
structures from medical images, but they typically have poor performance when
tested on another target domain with different data distribution. Recently,
unsupervised domain adaptation methods have been proposed to alleviate this
so-called domain shift issue, but most of them are designed for scenarios with
relatively small domain shifts and are likely to fail when encountering a large
domain gap. In this paper, we propose DCDA, a novel cross-modality unsupervised
domain adaptation framework for tasks with large domain shifts, e.g.,
segmenting retinal vessels from OCTA and OCT images. DCDA mainly consists of a
disentangling representation style transfer (DRST) module and a collaborative
consistency learning (CCL) module. DRST decomposes images into content
components and style codes and performs style transfer and image
reconstruction. CCL contains two segmentation models, one for source domain and
the other for target domain. The two models use labeled data (together with the
corresponding transferred images) for supervised learning and perform
collaborative consistency learning on unlabeled data. Each model focuses on the
corresponding single domain and aims to yield an expertized domain-specific
segmentation model. Through extensive experiments on retinal vessel
segmentation, our framework achieves Dice scores close to target-trained oracle
both from OCTA to OCT and from OCT to OCTA, significantly outperforming other
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Landscape of Neural Architecture Search across sensors: how much do they differ ?. (arXiv:2201.06321v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06321">
<div class="article-summary-box-inner">
<span><p>With the rapid rise of neural architecture search, the ability to understand
its complexity from the perspective of a search algorithm is desirable.
Recently, Traor\'e et al. have proposed the framework of Fitness Landscape
Footprint to help describe and compare neural architecture search problems. It
attempts at describing why a search strategy might be successful, struggle or
fail on a target task. Our study leverages this methodology in the context of
searching across sensors, including sensor data fusion. In particular, we apply
the Fitness Landscape Footprint to the real-world image classification problem
of So2Sat LCZ42, in order to identify the most beneficial sensor to our neural
network hyper-parameter optimization problem. From the perspective of
distributions of fitness, our findings indicate a similar behaviour of the
search space for all sensors: the longer the training time, the larger the
overall fitness, and more flatness in the landscapes (less ruggedness and
deviation). Regarding sensors, the better the fitness they enable (Sentinel-2),
the better the search trajectories (smoother, higher persistence). Results also
indicate very similar search behaviour for sensors that can be decently fitted
by the search space (Sentinel-2 and fusion).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?. (arXiv:2201.06346v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06346">
<div class="article-summary-box-inner">
<span><p>Even though image generation with Generative Adversarial Networks has been
showing remarkable ability to generate high-quality images, GANs do not always
guarantee photorealistic images will be generated. Sometimes they generate
images that have defective or unnatural objects, which are referred to as
'artifacts'. Research to determine why the artifacts emerge and how they can be
detected and removed has not been sufficiently carried out. To analyze this, we
first hypothesize that rarely activated neurons and frequently activated
neurons have different purposes and responsibilities for the progress of
generating images. By analyzing the statistics and the roles for those neurons,
we empirically show that rarely activated neurons are related to failed results
of making diverse objects and lead to artifacts. In addition, we suggest a
correction method, called 'sequential ablation', to repair the defective part
of the generated images without complex computational cost and manual efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-fidelity 3D Model Compression based on Key Spheres. (arXiv:2201.07486v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07486">
<div class="article-summary-box-inner">
<span><p>In recent years, neural signed distance function (SDF) has become one of the
most effective representation methods for 3D models. By learning continuous
SDFs in 3D space, neural networks can predict the distance from a given query
space point to its closest object surface,whose positive and negative signs
denote inside and outside of the object, respectively. Training a specific
network for each 3D model, which individually embeds its shape, can realize
compressed representation of objects by storing fewer network (and possibly
latent) parameters. Consequently, reconstruction through network inference and
surface recovery can be achieved. In this paper, we propose an SDF prediction
network using explicit key spheres as input. Key spheres are extracted from the
internal space of objects, whose centers either have relatively larger SDF
values (sphere radii), or are located at essential positions. By inputting the
spatial information of multiple spheres which imply different local shapes, the
proposed method can significantly improve the reconstruction accuracy with a
negligible storage cost. Compared to previous works, our method achieves the
high-fidelity and high-compression 3D object coding and reconstruction.
Experiments conducted on three datasets verify the superior performance of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation. (arXiv:2201.07779v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07779">
<div class="article-summary-box-inner">
<span><p>Learning to solve precision-based manipulation tasks from visual feedback
using Reinforcement Learning (RL) could drastically reduce the engineering
efforts required by traditional robot systems. However, performing fine-grained
motor control from visual inputs alone is challenging, especially with a static
third-person camera as often used in previous work. We propose a setting for
robotic manipulation in which the agent receives visual feedback from both a
third-person camera and an egocentric camera mounted on the robot's wrist.
While the third-person camera is static, the egocentric camera enables the
robot to actively control its vision to aid in precise manipulation. To fuse
visual information from both cameras effectively, we additionally propose to
use Transformers with a cross-view attention mechanism that models spatial
attention from one view to another (and vice-versa), and use the learned
features as input to an RL policy. Our method improves learning over strong
single-view and multi-view baselines, and successfully transfers to a set of
challenging manipulation tasks on a real robot with uncalibrated cameras, no
access to state information, and a high degree of task variability. In a hammer
manipulation task, our method succeeds in 75% of trials versus 38% and 13% for
multi-view and single-view baselines, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fantastic Data and How to Query Them. (arXiv:2201.05026v1 [cs.AI] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05026">
<div class="article-summary-box-inner">
<span><p>It is commonly acknowledged that the availability of the huge amount of
(training) data is one of the most important factors for many recent advances
in Artificial Intelligence (AI). However, datasets are often designed for
specific tasks in narrow AI sub areas and there is no unified way to manage and
access them. This not only creates unnecessary overheads when training or
deploying Machine Learning models but also limits the understanding of the
data, which is very important for data-centric AI. In this paper, we present
our vision about a unified framework for different datasets so that they can be
integrated and queried easily, e.g., using standard query languages. We
demonstrate this in our ongoing work to create a framework for datasets in
Computer Vision and show its advantages in different scenarios. Our
demonstration is available at https://vision.semkg.org.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-23 23:06:26.480472582 UTC">2022-01-23 23:06:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>