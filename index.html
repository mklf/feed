<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-31T01:30:00Z">05-31</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Who is we? Disambiguating the referents of first person plural pronouns in parliamentary debates. (arXiv:2205.14182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14182">
<div class="article-summary-box-inner">
<span><p>This paper investigates the use of first person plural pronouns as a
rhetorical device in political speeches. We present an annotation schema for
disambiguating pronoun references and use our schema to create an annotated
corpus of debates from the German Bundestag. We then use our corpus to learn to
automatically resolve pronoun referents in parliamentary debates. We explore
the use of data augmentation with weak supervision to further expand our corpus
and report preliminary results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion-LM Improves Controllable Text Generation. (arXiv:2205.14217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14217">
<div class="article-summary-box-inner">
<span><p>Controlling the behavior of language models (LMs) without re-training is a
major open problem in natural language generation. While recent works have
demonstrated successes on controlling simple sentence attributes (e.g.,
sentiment), there has been little progress on complex, fine-grained controls
(e.g., syntactic structure). To address this challenge, we develop a new
non-autoregressive language model based on continuous diffusions that we call
Diffusion-LM. Building upon the recent successes of diffusion models in
continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian
vectors into word vectors, yielding a sequence of intermediate latent
variables. The continuous, hierarchical nature of these intermediate variables
enables a simple gradient-based algorithm to perform complex, controllable
generation tasks. We demonstrate successful control of Diffusion-LM for six
challenging fine-grained control tasks, significantly outperforming prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Text Generation with Neurally-Decomposed Oracle. (arXiv:2205.14219v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14219">
<div class="article-summary-box-inner">
<span><p>We propose a general and efficient framework to control auto-regressive
generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained
base language model and a sequence-level boolean oracle function, we propose to
decompose the oracle function into token-level guidance to steer the base model
in text generation. Specifically, the token-level guidance is approximated by a
neural model trained with examples sampled from the base model, demanding no
additional auxiliary labeled data. We present the closed-form optimal solution
to incorporate the token-level guidance into the base model for controllable
generation. We further provide a theoretical analysis of how the approximation
quality of NADO affects the controllable generation results. Experiments
conducted on two applications: (1) text generation with lexical constraints and
(2) machine translation with formality control demonstrate that our framework
efficiently guides the base model towards the given oracle while maintaining
high generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Light-Weight Answer Text Retrieval in Dialogue Systems. (arXiv:2205.14226v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14226">
<div class="article-summary-box-inner">
<span><p>Dialogue systems can benefit from being able to search through a corpus of
text to find information relevant to user requests, especially when
encountering a request for which no manually curated response is available. The
state-of-the-art technology for neural dense retrieval or re-ranking involves
deep learning models with hundreds of millions of parameters. However, it is
difficult and expensive to get such models to operate at an industrial scale,
especially for cloud services that often need to support a big number of
individually customized dialogue systems, each with its own text corpus. We
report our work on enabling advanced neural dense retrieval systems to operate
effectively at scale on relatively inexpensive hardware. We compare with
leading alternative industrial solutions and show that we can provide a
solution that is effective, fast, and cost-efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition. (arXiv:2205.14228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14228">
<div class="article-summary-box-inner">
<span><p>Weakly supervised named entity recognition methods train label models to
aggregate the token annotations of multiple noisy labeling functions (LFs)
without seeing any manually annotated labels. To work well, the label model
needs to contextually identify and emphasize well-performed LFs while
down-weighting the under-performers. However, evaluating the LFs is challenging
due to the lack of ground truths. To address this issue, we propose the sparse
conditional hidden Markov model (Sparse-CHMM). Instead of predicting the entire
emission matrix as other HMM-based methods, Sparse-CHMM focuses on estimating
its diagonal elements, which are considered as the reliability scores of the
LFs. The sparse scores are then expanded to the full-fledged emission matrix
with pre-defined expansion functions. We also augment the emission with
weighted XOR scores, which track the probabilities of an LF observing incorrect
entities. Sparse-CHMM is optimized through unsupervised learning with a
three-stage training pipeline that reduces the training difficulty and prevents
the model from falling into local optima. Compared with the baselines in the
Wrench benchmark, Sparse-CHMM achieves a 3.01 average F1 score improvement on
five comprehensive datasets. Experiments show that each component of
Sparse-CHMM is effective, and the estimated LF reliabilities strongly correlate
with true LF F1 scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Against Stealthy Backdoor Attacks. (arXiv:2205.14246v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14246">
<div class="article-summary-box-inner">
<span><p>Defenses against security threats have been an interest of recent studies.
Recent works have shown that it is not difficult to attack a natural language
processing (NLP) model while defending against them is still a cat-mouse game.
Backdoor attacks are one such attack where a neural network is made to perform
in a certain way on specific samples containing some triggers while achieving
normal results on other samples. In this work, we present a few defense
strategies that can be useful to counter against such an attack. We show that
our defense methodologies significantly decrease the performance on the
attacked inputs while maintaining similar performance on benign inputs. We also
show that some of our defenses have very less runtime and also maintain
similarity with the original inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised models of audio effectively explain human cortical responses to speech. (arXiv:2205.14252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14252">
<div class="article-summary-box-inner">
<span><p>Self-supervised language models are very effective at predicting high-level
cortical responses during language comprehension. However, the best current
models of lower-level auditory processing in the human brain rely on either
hand-constructed acoustic filters or representations from supervised audio
neural networks. In this work, we capitalize on the progress of self-supervised
speech representation learning (SSL) to create new state-of-the-art models of
the human auditory system. Compared against acoustic baselines, phonemic
features, and supervised models, representations from the middle layers of
self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently
yield the best prediction performance for fMRI recordings within the auditory
cortex (AC). Brain areas involved in low-level auditory processing exhibit a
preference for earlier SSL model layers, whereas higher-level semantic areas
prefer later layers. We show that these trends are due to the models' ability
to encode information at multiple linguistic levels (acoustic, phonetic, and
lexical) along their representation depth. Overall, these results show that
self-supervised models effectively capture the hierarchy of information
relevant to different stages of speech processing in human cortex.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Subgoal Planning with Language Models. (arXiv:2205.14288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14288">
<div class="article-summary-box-inner">
<span><p>Pre-trained large language models have shown successful progress in many
language understanding benchmarks. This work explores the capability of these
models to predict actionable plans in real-world environments. Given a text
instruction, we show that language priors encoded in pre-trained language
models allow us to infer fine-grained subgoal sequences. In contrast to recent
methods which make strong assumptions about subgoal supervision, our
experiments show that language models can infer detailed subgoal sequences from
few training sequences without any fine-tuning. We further propose a simple
strategy to re-rank language model predictions based on interaction and
feedback from the environment. Combined with pre-trained navigation and visual
reasoning components, our approach demonstrates competitive performance on
subgoal prediction and task completion in the ALFRED benchmark compared to
prior methods that assume more subgoal supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximate Conditional Coverage via Neural Model Approximations. (arXiv:2205.14310v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14310">
<div class="article-summary-box-inner">
<span><p>Constructing reliable prediction sets is an obstacle for applications of
neural models: Distribution-free conditional coverage is theoretically
impossible, and the exchangeability assumption underpinning the coverage
guarantees of standard split-conformal approaches is violated on domain shifts.
Given these challenges, we propose and analyze a data-driven procedure for
obtaining empirically reliable approximate conditional coverage, calculating
unique quantile thresholds for each label for each test point. We achieve this
via the strong signals for prediction reliability from KNN-based model
approximations over the training set and approximations over constrained
samples from the held-out calibration set. We demonstrate the potential for
substantial (and otherwise unknowable) under-coverage with split-conformal
alternatives with marginal coverage guarantees when not taking these distances
and constraints into account with protein secondary structure prediction,
grammatical error detection, sentiment classification, and fact verification,
covering supervised sequence labeling, zero-shot sequence labeling (i.e.,
feature detection), document classification (with sparsity/interpretability
constraints), and retrieval-classification, including class-imbalanced and
domain-shifted settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Activation Network For Low Resource Multilingual Speech Recognition. (arXiv:2205.14326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14326">
<div class="article-summary-box-inner">
<span><p>Low resource automatic speech recognition (ASR) is a useful but thorny task,
since deep learning ASR models usually need huge amounts of training data. The
existing models mostly established a bottleneck (BN) layer by pre-training on a
large source language, and transferring to the low resource target language. In
this work, we introduced an adaptive activation network to the upper layers of
ASR model, and applied different activation functions to different languages.
We also proposed two approaches to train the model: (1) cross-lingual learning,
replacing the activation function from source language to target language, (2)
multilingual learning, jointly training the Connectionist Temporal
Classification (CTC) loss of each language and the relevance of different
languages. Our experiments on IARPA Babel datasets demonstrated that our
approaches outperform the from-scratch training and traditional bottleneck
feature based methods. In addition, combining the cross-lingual learning and
multilingual learning together could further improve the performance of
multilingual speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Augmentation Based Unsupervised Learning for Keyword Spotting. (arXiv:2205.14329v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14329">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigated a speech augmentation based unsupervised
learning approach for keyword spotting (KWS) task. KWS is a useful speech
application, yet also heavily depends on the labeled data. We designed a
CNN-Attention architecture to conduct the KWS task. CNN layers focus on the
local acoustic features, and attention layers model the long-time dependency.
To improve the robustness of KWS model, we also proposed an unsupervised
learning method. The unsupervised loss is based on the similarity between the
original and augmented speech features, as well as the audio reconstructing
information. Two speech augmentation methods are explored in the unsupervised
learning: speed and intensity. The experiments on Google Speech Commands V2
Dataset demonstrated that our CNN-Attention model has competitive results.
Moreover, the augmentation based unsupervised learning could further improve
the classification accuracy of KWS task. In our experiments, with augmentation
based unsupervised learning, our KWS model achieves better performance than
other unsupervised methods, such as CPC, APC, and MPC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Reference Is Not Enough: Diverse Distillation with Reference Selection for Non-Autoregressive Translation. (arXiv:2205.14333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14333">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive neural machine translation (NAT) suffers from the
multi-modality problem: the source sentence may have multiple correct
translations, but the loss function is calculated only according to the
reference sentence. Sequence-level knowledge distillation makes the target more
deterministic by replacing the target with the output from an autoregressive
model. However, the multi-modality problem in the distilled dataset is still
nonnegligible. Furthermore, learning from a specific teacher limits the upper
bound of the model capability, restricting the potential of NAT models. In this
paper, we argue that one reference is not enough and propose diverse
distillation with reference selection (DDRS) for NAT. Specifically, we first
propose a method called SeedDiv for diverse machine translation, which enables
us to generate a dataset containing multiple high-quality reference
translations for each source sentence. During the training, we compare the NAT
output with all references and select the one that best fits the NAT output to
train the model. Experiments on widely-used machine translation benchmarks
demonstrate the effectiveness of DDRS, which achieves 29.82 BLEU with only one
decoding pass on WMT14 En-De, improving the state-of-the-art performance for
NAT by over 1 BLEU. Source code: https://github.com/ictnlp/DDRS-NAT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Models to Express Their Uncertainty in Words. (arXiv:2205.14334v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14334">
<div class="article-summary-box-inner">
<span><p>We show that a GPT-3 model can learn to express uncertainty about its own
answers in natural language -- without use of model logits. When given a
question, the model generates both an answer and a level of confidence (e.g.
"90% confidence" or "high confidence"). These levels map to probabilities that
are well calibrated. The model also remains moderately calibrated under
distribution shift, and is sensitive to uncertainty in its own answers, rather
than imitating human examples. To our knowledge, this is the first time a model
has been shown to express calibrated uncertainty about its own answers in
natural language. For testing calibration, we introduce the CalibratedMath
suite of tasks. We compare the calibration of uncertainty expressed in words
("verbalized probability") to uncertainty extracted from model logits. Both
kinds of uncertainty are capable of generalizing calibration under distribution
shift. We also provide evidence that GPT-3's ability to generalize calibration
depends on pre-trained latent representations that correlate with epistemic
uncertainty over its answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-Specific Attentions over Entity Mentions for Enhanced Document-Level Relation Extraction. (arXiv:2205.14393v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14393">
<div class="article-summary-box-inner">
<span><p>Compared with traditional sentence-level relation extraction, document-level
relation extraction is a more challenging task where an entity in a document
may be mentioned multiple times and associated with multiple relations.
However, most methods of document-level relation extraction do not distinguish
between mention-level features and entity-level features, and just apply simple
pooling operation for aggregating mention-level features into entity-level
features. As a result, the distinct semantics between the different mentions of
an entity are overlooked. To address this problem, we propose RSMAN in this
paper which performs selective attentions over different entity mentions with
respect to candidate relations. In this manner, the flexible and
relation-specific representations of entities are obtained which indeed benefit
relation classification. Our extensive experiments upon two benchmark datasets
show that our RSMAN can bring significant improvements for some backbone models
to achieve state-of-the-art performance, especially when an entity have
multiple mentions in the document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAN-Cap: A Multi-Purpose English-Bangla Image Descriptions Dataset. (arXiv:2205.14462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14462">
<div class="article-summary-box-inner">
<span><p>As computers have become efficient at understanding visual information and
transforming it into a written representation, research interest in tasks like
automatic image captioning has seen a significant leap over the last few years.
While most of the research attention is given to the English language in a
monolingual setting, resource-constrained languages like Bangla remain out of
focus, predominantly due to a lack of standard datasets. Addressing this issue,
we present a new dataset BAN-Cap following the widely used Flickr8k dataset,
where we collect Bangla captions of the images provided by qualified
annotators. Our dataset represents a wider variety of image caption styles
annotated by trained people from different backgrounds. We present a
quantitative and qualitative analysis of the dataset and the baseline
evaluation of the recent models in Bangla image captioning. We investigate the
effect of text augmentation and demonstrate that an adaptive attention-based
model combined with text augmentation using Contextualized Word Replacement
(CWR) outperforms all state-of-the-art models for Bangla image captioning. We
also present this dataset's multipurpose nature, especially on machine
translation for Bangla-English and English-Bangla. This dataset and all the
models will be useful for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization. (arXiv:2205.14521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14521">
<div class="article-summary-box-inner">
<span><p>Text summarization aims to generate a short summary for an input text. In
this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS)
approach, which does not require parallel data for training. Our NAUS first
performs edit-based search towards a heuristically defined score, and generates
a summary as pseudo-groundtruth. Then, we train an encoder-only
non-autoregressive Transformer based on the search result. We also propose a
dynamic programming approach for length-control decoding, which is important
for the summarization task. Experiments on two datasets show that NAUS achieves
state-of-the-art performance for unsupervised summarization, yet largely
improving inference efficiency. Further, our algorithm is able to perform
explicit length-transfer summary generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Character-Level Length-Control Algorithm for Non-Autoregressive Sentence Summarization. (arXiv:2205.14522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14522">
<div class="article-summary-box-inner">
<span><p>Sentence summarization aims at compressing a long sentence into a short one
that keeps the main gist, and has extensive real-world applications such as
headline generation. In previous work, researchers have developed various
approaches to improve the ROUGE score, which is the main evaluation metric for
summarization, whereas controlling the summary length has not drawn much
attention. In our work, we address a new problem of explicit character-level
length control for summarization, and propose a dynamic programming algorithm
based on the Connectionist Temporal Classification (CTC) model. Results show
that our approach not only achieves higher ROUGE scores but also yields more
complete sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoDisc: Automatic Distillation Schedule for Large Language Model Compression. (arXiv:2205.14570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14570">
<div class="article-summary-box-inner">
<span><p>Driven by the teacher-student paradigm, knowledge distillation is one of the
de facto ways for language model compression. Recent studies have uncovered
that conventional distillation is less effective when facing a large capacity
gap between the teacher and the student, and introduced teacher assistant-based
distillation to bridge the gap. As a connection, the scale and the performance
of the teacher assistant is crucial for transferring the knowledge from the
teacher to the student. However, existing teacher assistant-based methods
manually select the scale of the teacher assistant, which fails to identify the
teacher assistant with the optimal scale-performance tradeoff. To this end, we
propose an Automatic Distillation Schedule (AutoDisc) for large language model
compression. In particular, AutoDisc first specifies a set of teacher assistant
candidates at different scales with gridding and pruning, and then optimizes
all candidates in an once-for-all optimization with two approximations. The
best teacher assistant scale is automatically selected according to the
scale-performance tradeoff. AutoDisc is evaluated with an extensive set of
experiments on a language understanding benchmark GLUE. Experimental results
demonstrate the improved performance and applicability of our AutoDisc. We
further apply AutoDisc on a language model with over one billion parameters and
show the scalability of AutoDisc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Locality and Isotropy in Dialogue Modeling. (arXiv:2205.14583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14583">
<div class="article-summary-box-inner">
<span><p>Existing dialogue modeling methods have achieved promising performance on
various dialogue tasks with the aid of Transformer and the large-scale
pre-trained language models. However, some recent studies revealed that the
context representations produced by these methods suffer the problem of
anisotropy. In this paper, we find that the generated representations are also
not conversational, losing the conversation structure information during the
context modeling stage. To this end, we identify two properties in dialogue
modeling, i.e., locality and isotropy, and present a simple method for dialogue
representation calibration, namely SimDRC, to build isotropic and
conversational feature spaces. Experimental results show that our approach
significantly outperforms the current state-of-the-art models on three dialogue
tasks across the automatic and human evaluation metrics. More in-depth analyses
further confirm the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor Prediction: A Topic Modeling Approach. (arXiv:2205.14631v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14631">
<div class="article-summary-box-inner">
<span><p>Networks of documents connected by hyperlinks, such as Wikipedia, are
ubiquitous. Hyperlinks are inserted by the authors to enrich the text and
facilitate the navigation through the network. However, authors tend to insert
only a fraction of the relevant hyperlinks, mainly because this is a time
consuming task. In this paper we address an annotation, which we refer to as
anchor prediction. Even though it is conceptually close to link prediction or
entity linking, it is a different task that require developing a specific
method to solve it. Given a source document and a target document, this task
consists in automatically identifying anchors in the source document, i.e words
or terms that should carry a hyperlink pointing towards the target document. We
propose a contextualized relational topic model, CRTM, that models directed
links between documents as a function of the local context of the anchor in the
source document and the whole content of the target document. The model can be
used to predict anchors in a source document, given the target document,
without relying on a dictionary of previously seen mention or title, nor any
external knowledge graph. Authors can benefit from CRTM, by letting it
automatically suggest hyperlinks, given a new document and the set of target
document to connect to. It can also benefit to readers, by dynamically
inserting hyperlinks between the documents they're reading. Experiments
conducted on several Wikipedia corpora (in English, Italian and German)
highlight the practical usefulness of anchor prediction and demonstrate the
relevancy of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SFE-AI at SemEval-2022 Task 11: Low-Resource Named Entity Recognition using Large Pre-trained Language Models. (arXiv:2205.14660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14660">
<div class="article-summary-box-inner">
<span><p>Large scale pre-training models have been widely used in named entity
recognition (NER) tasks. However, model ensemble through parameter averaging or
voting can not give full play to the differentiation advantages of different
models, especially in the open domain. This paper describes our NER system in
the SemEval 2022 task11: MultiCoNER. We proposed an effective system to
adaptively ensemble pre-trained language models by a Transformer layer. By
assigning different weights to each model for different inputs, we adopted the
Transformer layer to integrate the advantages of diverse models effectively.
Experimental results show that our method achieves superior performances in
Farsi and Dutch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoNT: Contrastive Neural Text Generation. (arXiv:2205.14690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14690">
<div class="article-summary-box-inner">
<span><p>Recently, contrastive learning attracts increasing interests in neural text
generation as a new solution to alleviate the exposure bias problem. It
introduces a sequence-level training signal which is crucial to generation
tasks that always rely on auto-regressive decoding. However, previous methods
using contrastive learning in neural text generation usually lead to inferior
performance. In this paper, we analyse the underlying reasons and propose a new
Contrastive Neural Text generation framework, CoNT. CoNT addresses bottlenecks
that prevent contrastive learning from being widely adopted in generation tasks
from three aspects -- the construction of contrastive examples, the choice of
the contrastive loss, and the strategy in decoding. We validate CoNT on five
generation tasks with ten benchmarks, including machine translation,
summarization, code comment generation, data-to-text generation and commonsense
generation. Experimental results show that CoNT clearly outperforms the
conventional training framework on all the ten benchmarks with a convincing
margin. Especially, CoNT surpasses previous the most competitive contrastive
learning method for text generation, by 1.50 BLEU on machine translation and
1.77 ROUGE-1 on summarization, respectively. It achieves new state-of-the-art
on summarization, code comment generation (without external data) and
data-to-text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VD-PCR: Improving Visual Dialog with Pronoun Coreference Resolution. (arXiv:2205.14693v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14693">
<div class="article-summary-box-inner">
<span><p>The visual dialog task requires an AI agent to interact with humans in
multi-round dialogs based on a visual environment. As a common linguistic
phenomenon, pronouns are often used in dialogs to improve the communication
efficiency. As a result, resolving pronouns (i.e., grounding pronouns to the
noun phrases they refer to) is an essential step towards understanding dialogs.
In this paper, we propose VD-PCR, a novel framework to improve Visual Dialog
understanding with Pronoun Coreference Resolution in both implicit and explicit
ways. First, to implicitly help models understand pronouns, we design novel
methods to perform the joint training of the pronoun coreference resolution and
visual dialog tasks. Second, after observing that the coreference relationship
of pronouns and their referents indicates the relevance between dialog rounds,
we propose to explicitly prune the irrelevant history rounds in visual dialog
models' input. With pruned input, the models can focus on relevant dialog
history and ignore the distraction in the irrelevant one. With the proposed
implicit and explicit methods, VD-PCR achieves state-of-the-art experimental
results on the VisDial dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14704">
<div class="article-summary-box-inner">
<span><p>Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What are People Talking about in #BackLivesMatter and #StopAsianHate? Exploring and Categorizing Twitter Topics Emerging in Online Social Movements through the Latent Dirichlet Allocation Model. (arXiv:2205.14725v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14725">
<div class="article-summary-box-inner">
<span><p>Minority groups have been using social media to organize social movements
that create profound social impacts. Black Lives Matter (BLM) and Stop Asian
Hate (SAH) are two successful social movements that have spread on Twitter that
promote protests and activities against racism and increase the public's
awareness of other social challenges that minority groups face. However,
previous studies have mostly conducted qualitative analyses of tweets or
interviews with users, which may not comprehensively and validly represent all
tweets. Very few studies have explored the Twitter topics within BLM and SAH
dialogs in a rigorous, quantified and data-centered approach. Therefore, in
this research, we adopted a mixed-methods approach to comprehensively analyze
BLM and SAH Twitter topics. We implemented (1) the latent Dirichlet allocation
model to understand the top high-level words and topics and (2) open-coding
analysis to identify specific themes across the tweets. We collected more than
one million tweets with the #blacklivesmatter and #stopasianhate hashtags and
compared their topics. Our findings revealed that the tweets discussed a
variety of influential topics in depth, and social justice, social movements,
and emotional sentiments were common topics in both movements, though with
unique subtopics for each movement. Our study contributes to the topic analysis
of social movements on social media platforms in particular and the literature
on the interplay of AI, ethics, and society in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI. (arXiv:2205.14727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14727">
<div class="article-summary-box-inner">
<span><p>Human language expression is based on the subjective construal of the
situation instead of the objective truth conditions, which means that speakers'
personalities and emotions after cognitive processing have an important
influence on conversation. However, most existing datasets for conversational
AI ignore human personalities and emotions, or only consider part of them. It's
difficult for dialogue systems to understand speakers' personalities and
emotions although large-scale pre-training language models have been widely
used. In order to consider both personalities and emotions in the process of
conversation generation, we propose CPED, a large-scale Chinese personalized
and emotional dialogue dataset, which consists of multi-source knowledge
related to empathy and personal characteristic. These knowledge covers gender,
Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED
contains more than 12K dialogues of 392 speakers from 40 TV shows. We release
the textual dataset with audio features and video features according to the
copyright claims, privacy issues, terms of service of video platforms. We
provide detailed description of the CPED construction process and introduce
three tasks for conversational AI, including personality recognition, emotion
recognition in conversations as well as personalized and emotional conversation
generation. Finally, we provide baseline systems for these tasks and consider
the function of speakers' personalities and emotions on conversation. Our
motivation is to propose a dataset to be widely adopted by the NLP community as
a new open benchmark for conversational AI research. The full dataset is
available at https://github.com/scutcyr/CPED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L3Cube-MahaNLP: Marathi Natural Language Processing Datasets, Models, and Library. (arXiv:2205.14728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14728">
<div class="article-summary-box-inner">
<span><p>Despite being the third most popular language in India, the Marathi language
lacks useful NLP resources. Moreover, popular NLP libraries do not have support
for the Marathi language. With L3Cube-MahaNLP, we aim to build resources and a
library for Marathi natural language processing. We present datasets and
transformer models for supervised tasks like sentiment analysis, named entity
recognition, and hate speech detection. We have also published a monolingual
Marathi corpus for unsupervised language modeling tasks. Overall we present
MahaCorpus, MahaSent, MahaNER, and MahaHate datasets and their corresponding
MahaBERT models fine-tuned on these datasets. We aim to move ahead of benchmark
datasets and prepare useful resources for Marathi. The resources are available
at https://github.com/l3cube-pune/MarathiNLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning as Conversation: Dialogue Systems Reinforced for Information Acquisition. (arXiv:2205.14748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14748">
<div class="article-summary-box-inner">
<span><p>We propose novel AI-empowered chat bots for learning as conversation where a
user does not read a passage but gains information and knowledge through
conversation with a teacher bot. Our information-acquisition-oriented dialogue
system employs a novel adaptation of reinforced self-play so that the system
can be transferred to various domains without in-domain dialogue data, and can
carry out conversations both informative and attentive to users. Our extensive
subjective and objective evaluations on three large public data corpora
demonstrate the effectiveness of our system to deliver knowledge-intensive and
attentive conversations and help end users substantially gain knowledge without
reading passages. Our code and datasets are publicly available for follow-up
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation. (arXiv:1906.11455v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.11455">
<div class="article-summary-box-inner">
<span><p>Chinese word segmentation (CWS) is a fundamental step of Chinese natural
language processing. In this paper, we build a new toolkit, named PKUSEG, for
multi-domain word segmentation. Unlike existing single-model toolkits, PKUSEG
targets multi-domain word segmentation and provides separate models for
different domains, such as web, medicine, and tourism. Besides, due to the lack
of labeled data in many domains, we propose a domain adaptation paradigm to
introduce cross-domain semantic knowledge via a translation system. Through
this method, we generate synthetic data using a large amount of unlabeled data
in the target domain and then obtain a word segmentation model for the target
domain. We also further refine the performance of the default model with the
help of synthetic data. Experiments show that PKUSEG achieves high performance
on multiple domains. The new toolkit also supports POS tagging and model
training to adapt to various application scenarios. The toolkit is now freely
and publicly available for the usage of research and industry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model. (arXiv:2102.11008v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11008">
<div class="article-summary-box-inner">
<span><p>We propose InsNet, an expressive insertion-based text generator with
efficient training and flexible decoding (parallel or sequential). Unlike most
existing insertion-based text generation works that require re-encoding of the
context after each insertion operation and thus are inefficient to train,
InsNet only requires one pass of context encoding for the entire insertion
sequence during training by introducing a novel insertion-oriented position
encoding to enable computation sharing. Experiments on two unsupervised
lexically constrained text generation datasets and three machine translation
datasets demonstrate InsNet's advantages over previous insertion-based methods
in terms of training speed, inference efficiency, and generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling the dynamics of language change: logistic regression, Piotrowski's law, and a handful of examples in Polish. (arXiv:2104.06324v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06324">
<div class="article-summary-box-inner">
<span><p>The study discusses modeling diachronic processes by logistic regression. The
phenomenon of nonlinear changes in language was first observed by Raimund
Piotrowski (hence labelled as Piotrowski's law), even if actual linguistic
evidence usually speaks against using the notion of a "law" in this context. In
our study, we apply logistic regression models to 9 changes which occurred
between 15th and 18th century in the Polish language. The attested course of
the majority of these changes closely follow the expected values, which proves
that the language change might indeed resemble a nonlinear phase change
scenario. We also extend the original Piotrowski's approach by proposing
polynomial logistic regression for these cases which can hardly be described by
its standard version. Also, we propose to consider individual language change
cases jointly, in order to inspect their possible collinearity or, more likely,
their different dynamics in the function of time. Last but not least, we
evaluate our results by testing the influence of the subcorpus size on the
model's goodness-of-fit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Answers with Entailment Trees. (arXiv:2104.08661v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08661">
<div class="article-summary-box-inner">
<span><p>Our goal, in the context of open-domain textual question-answering (QA), is
to explain answers by showing the line of reasoning from what is known to the
answer, rather than simply showing a fragment of textual evidence (a
"rationale'"). If this could be done, new opportunities for understanding and
debugging the system's reasoning become possible. Our approach is to generate
explanations in the form of entailment trees, namely a tree of multipremise
entailment steps from facts that are known, through intermediate conclusions,
to the hypothesis of interest (namely the question + answer). To train a model
with this skill, we created ENTAILMENTBANK, the first dataset to contain
multistep entailment trees. Given a hypothesis (question + answer), we define
three increasingly difficult explanation tasks: generate a valid entailment
tree given (a) all relevant sentences (b) all relevant and some irrelevant
sentences, or (c) a corpus. We show that a strong language model can partially
solve these tasks, in particular when the relevant sentences are included in
the input (e.g., 35% of trees for (a) are perfect), and with indications of
generalization to other domains. This work is significant as it provides a new
type of dataset (multistep entailments) and baselines, offering a new avenue
for the community to generate richer, more systematic explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing and Mitigating Interference in Neural Architecture Search. (arXiv:2108.12821v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12821">
<div class="article-summary-box-inner">
<span><p>Weight sharing is a popular approach to reduce the cost of neural
architecture search (NAS) by reusing the weights of shared operators from
previously trained child models. However, the rank correlation between the
estimated accuracy and ground truth accuracy of those child models is low due
to the interference among different child models caused by weight sharing. In
this paper, we investigate the interference issue by sampling different child
models and calculating the gradient similarity of shared operators, and
observe: 1) the interference on a shared operator between two child models is
positively correlated with the number of different operators; 2) the
interference is smaller when the inputs and outputs of the shared operator are
more similar. Inspired by these two observations, we propose two approaches to
mitigate the interference: 1) MAGIC-T: rather than randomly sampling child
models for optimization, we propose a gradual modification scheme by modifying
one operator between adjacent optimization steps to minimize the interference
on the shared operators; 2) MAGIC-A: forcing the inputs and outputs of the
operator across all child models to be similar to reduce the interference.
Experiments on a BERT search space verify that mitigating interference via each
of our proposed methods improves the rank correlation of super-pet and
combining both methods can achieve better results. Our discovered architecture
outperforms RoBERTa$_{\rm base}$ by 1.1 and 0.6 points and ELECTRA$_{\rm base}$
by 1.6 and 1.1 points on the dev and test set of GLUE benchmark. Extensive
results on the BERT compression, reading comprehension and ImageNet task
demonstrate the effectiveness and generality of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paradigm Shift in Natural Language Processing. (arXiv:2109.12575v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12575">
<div class="article-summary-box-inner">
<span><p>In the era of deep learning, modeling for most NLP tasks has converged to
several mainstream paradigms. For example, we usually adopt the sequence
labeling paradigm to solve a bundle of tasks such as POS-tagging, NER,
Chunking, and adopt the classification paradigm to solve tasks like sentiment
analysis. With the rapid progress of pre-trained language models, recent years
have observed a rising trend of Paradigm Shift, which is solving one NLP task
by reformulating it as another one. Paradigm shift has achieved great success
on many tasks, becoming a promising way to improve model performance. Moreover,
some of these paradigms have shown great potential to unify a large number of
NLP tasks, making it possible to build a single model to handle diverse tasks.
In this paper, we review such phenomenon of paradigm shifts in recent years,
highlighting several paradigms that have the potential to solve different NLP
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00269">
<div class="article-summary-box-inner">
<span><p>Pre-trained models learn informative representations on large-scale training
data through a self-supervised or supervised learning method, which has
achieved promising performance in natural language processing (NLP), computer
vision (CV), and cross-modal fields after fine-tuning. These models, however,
suffer from poor robustness and lack of interpretability. Pre-trained models
with knowledge injection, which we call knowledge enhanced pre-trained models
(KEPTMs), possess deep understanding and logical reasoning and introduce
interpretability. In this survey, we provide a comprehensive overview of KEPTMs
in NLP and CV. We first introduce the progress of pre-trained models and
knowledge representation learning. Then we systematically categorize existing
KEPTMs from three different perspectives. Finally, we outline some potential
directions of KEPTMs for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v12 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devising aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Semantic Parsing with Language Models Trained On Code. (arXiv:2112.08696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08696">
<div class="article-summary-box-inner">
<span><p>Large language models can perform semantic parsing with little training data,
when prompted with in-context examples. It has been shown that this can be
improved by formulating the problem as paraphrasing into canonical utterances,
which casts the underlying meaning representation into a controlled natural
language-like representation. Intuitively, such models can more easily output
canonical utterances as they are closer to the natural language used for
pre-training. Recently, models also pre-trained on code, like OpenAI Codex,
have risen in prominence. For semantic parsing tasks where we map natural
language into code, such models may prove more adept at it. In this paper, we
test this hypothesis and find that Codex performs better on such tasks than
equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that
unlike GPT-3, Codex performs similarly when targeting meaning representations
directly, perhaps because meaning representations are structured similar to
code in these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Dense Information Retrieval with Contrastive Learning. (arXiv:2112.09118v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09118">
<div class="article-summary-box-inner">
<span><p>Recently, information retrieval has seen the emergence of dense retrievers,
based on neural networks, as an alternative to classical sparse methods based
on term-frequency. These models have obtained state-of-the-art results on
datasets and tasks where large training sets are available. However, they do
not transfer well to new applications with no training data, and are
outperformed by unsupervised term-frequency methods such as BM25. In this work,
we explore the limits of contrastive learning as a way to train unsupervised
dense retrievers and show that it leads to strong performance in various
retrieval settings. On the BEIR benchmark our unsupervised model outperforms
BM25 on 11 out of 15 datasets for the Recall@100 metric. When used as
pre-training before fine-tuning, either on a few thousands in-domain examples
or on the large MS MARCO dataset, our contrastive model leads to improvements
on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual
retrieval, where training data is even scarcer than for English, and show that
our approach leads to strong unsupervised performance. Our model also exhibits
strong cross-lingual transfer when fine-tuned on supervised English data only
and evaluated on low resources language such as Swahili. We show that our
unsupervised models can perform cross-lingual retrieval between different
scripts, such as retrieving English documents from Arabic queries, which would
not be possible with term matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OntoProtein: Protein Pretraining With Gene Ontology Embedding. (arXiv:2201.11147v5 [q-bio.BM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11147">
<div class="article-summary-box-inner">
<span><p>Self-supervised protein language models have proved their effectiveness in
learning the proteins representations. With the increasing computational power,
current protein language models pre-trained with millions of diverse sequences
can advance the parameter scale from million-level to billion-level and achieve
remarkable improvement. However, those prevailing approaches rarely consider
incorporating knowledge graphs (KGs), which can provide rich structured
knowledge facts for better protein representations. We argue that informative
biology knowledge in KGs can enhance protein representation with external
knowledge. In this work, we propose OntoProtein, the first general framework
that makes use of structure in GO (Gene Ontology) into protein pre-training
models. We construct a novel large-scale knowledge graph that consists of GO
and its related proteins, and gene annotation texts or protein sequences
describe all nodes in the graph. We propose novel contrastive learning with
knowledge-aware negative sampling to jointly optimize the knowledge graph and
protein embedding during pre-training. Experimental results show that
OntoProtein can surpass state-of-the-art methods with pre-trained protein
language models in TAPE benchmark and yield better performance compared with
baselines in protein-protein interaction and protein function prediction. Code
and datasets are available in https://github.com/zjunlp/OntoProtein.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Contrastive Framework for Neural Text Generation. (arXiv:2202.06417v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06417">
<div class="article-summary-box-inner">
<span><p>Text generation is of great importance to many natural language processing
applications. However, maximization-based decoding methods (e.g. beam search)
of neural language models often lead to degenerate solutions -- the generated
text is unnatural and contains undesirable repetitions. Existing approaches
introduce stochasticity via sampling or modify training objectives to decrease
probabilities of certain tokens (e.g., unlikelihood training). However, they
often lead to solutions that lack coherence. In this work, we show that an
underlying reason for model degeneration is the anisotropic distribution of
token representations. We present a contrastive solution: (i) SimCTG, a
contrastive training objective to calibrate the model's representation space,
and (ii) a decoding method -- contrastive search -- to encourage diversity
while maintaining coherence in the generated text. Extensive experiments and
analyses on three benchmarks from two languages demonstrate that our proposed
approach outperforms state-of-the-art text generation methods as evaluated by
both human and automatic metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\rm{C {\small IS}}^2$: A Simplified Commonsense Inference Evaluation for Story Prose. (arXiv:2202.07880v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07880">
<div class="article-summary-box-inner">
<span><p>Transformers have been showing near-human performance on a variety of tasks,
but they are not without their limitations. We discuss the issue of conflating
results of transformers that are instructed to do multiple tasks
simultaneously. In particular, we focus on the domain of commonsense reasoning
within story prose, which we call contextual commonsense inference (CCI). We
look at the GLUCOSE (Mostafazadeh et al. 2020) dataset and task for predicting
implicit commonsense inferences between story sentences. Since the GLUCOSE task
simultaneously generates sentences and predicts the CCI relation, there is a
conflation in the results. Is the model really measuring CCI or is its ability
to generate grammatical text carrying the results? In this paper, we introduce
the task contextual commonsense inference in sentence selection ($\rm{C {\small
IS}}^2$), a simplified task that avoids conflation by eliminating language
generation altogether. Our findings emphasize the necessity of future work to
disentangle language generation from the desired NLP tasks at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition. (arXiv:2203.06925v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06925">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition task is one of the core tasks of information
extraction.Word ambiguity and word abbreviation are important reasons for the
low recognition rate of named entities. In this paper, we propose a novel named
entity recognition model WCL-BBCD (Word Contrastive Learning with
BERT-BiLSTM-CRF-DBpedia) incorporating the idea of contrastive learning. The
model first trains the sentence pairs in the text, calculate similarity between
words in sentence pairs by cosine similarity, and fine-tunes the BERT model
used for the named entity recognition task through the similarity, so as to
alleviate word ambiguity. Then, the fine-tuned BERT model is combined with the
BiLSTM-CRF model to perform the named entity recognition task. Finally, the
recognition results are corrected in combination with prior knowledge such as
knowledge graphs, so as to alleviate the recognition caused by word
abbreviations low-rate problem. Experimental results show that our model
outperforms other similar model methods on the CoNLL-2003 English dataset and
OntoNotes V5 English dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Semantic Aware Pre-training for Few-shot Text Classification. (arXiv:2204.07128v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07128">
<div class="article-summary-box-inner">
<span><p>In text classification tasks, useful information is encoded in the label
names. Label semantic aware systems have leveraged this information for
improved text classification performance during fine-tuning and prediction.
However, use of label-semantics during pre-training has not been extensively
explored. We therefore propose Label Semantic Aware Pre-training (LSAP) to
improve the generalization and data efficiency of text classification systems.
LSAP incorporates label semantics into pre-trained generative models (T5 in our
case) by performing secondary pre-training on labeled sentences from a variety
of domains. As domain-general pre-training requires large amounts of data, we
develop a filtering and labeling pipeline to automatically create
sentence-label pairs from unlabeled text. We perform experiments on intent
(ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP
obtains significant accuracy improvements over state-of-the-art models for
few-shot text classification while maintaining performance comparable to state
of the art in high-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding. (arXiv:2204.10050v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10050">
<div class="article-summary-box-inner">
<span><p>This paper presents the shared task on Multilingual Idiomaticity Detection
and Sentence Embedding, which consists of two subtasks: (a) a binary
classification task aimed at identifying whether a sentence contains an
idiomatic expression, and (b) a task based on semantic text similarity which
requires the model to adequately represent potentially idiomatic expressions in
context. Each subtask includes different settings regarding the amount of
training data. Besides the task description, this paper introduces the datasets
in English, Portuguese, and Galician and their annotation procedure, the
evaluation metrics, and a summary of the participant systems and their results.
The task had close to 100 registered participants organised into twenty five
teams making over 650 and 150 submissions in the practice and evaluation phases
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Win Lottery Tickets in BERT Transfer via Task-agnostic Mask Training. (arXiv:2204.11218v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11218">
<div class="article-summary-box-inner">
<span><p>Recent studies on the lottery ticket hypothesis (LTH) show that pre-trained
language models (PLMs) like BERT contain matching subnetworks that have similar
transfer learning performance as the original PLM. These subnetworks are found
using magnitude-based pruning. In this paper, we find that the BERT subnetworks
have even more potential than these studies have shown. Firstly, we discover
that the success of magnitude pruning can be attributed to the preserved
pre-training performance, which correlates with the downstream transferability.
Inspired by this, we propose to directly optimize the subnetwork structure
towards the pre-training objectives, which can better preserve the pre-training
performance. Specifically, we train binary masks over model weights on the
pre-training tasks, with the aim of preserving the universal transferability of
the subnetwork, which is agnostic to any specific downstream tasks. We then
fine-tune the subnetworks on the GLUE benchmark and the SQuAD dataset. The
results show that, compared with magnitude pruning, mask training can
effectively find BERT subnetworks with improved overall performance on
downstream tasks. Moreover, our method is also more efficient in searching
subnetworks and more advantageous when fine-tuning within a certain range of
data scarcity. Our code is available at https://github.com/llyx97/TAMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02357">
<div class="article-summary-box-inner">
<span><p>Multimodal Knowledge Graphs (MKGs), which organize visual-text factual
knowledge, have recently been successfully applied to tasks such as information
retrieval, question answering, and recommendation system. Since most MKGs are
far from complete, extensive knowledge graph completion studies have been
proposed focusing on the multimodal entity, relation extraction and link
prediction. However, different tasks and modalities require changes to the
model architecture, and not all images/objects are relevant to text input,
which hinders the applicability to diverse real-world scenarios. In this paper,
we propose a hybrid transformer with multi-level fusion to address those
issues. Specifically, we leverage a hybrid transformer architecture with
unified input-output for diverse multimodal knowledge graph completion tasks.
Moreover, we propose multi-level fusion, which integrates visual and text
representation via coarse-grained prefix-guided interaction and fine-grained
correlation-aware fusion modules. We conduct extensive experiments to validate
that our MKGformer can obtain SOTA performance on four datasets of multimodal
link prediction, multimodal RE, and multimodal NER. Code is available in
https://github.com/zjunlp/MKGformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Understanding Harmful Memes: A Survey. (arXiv:2205.04274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04274">
<div class="article-summary-box-inner">
<span><p>The automatic identification of harmful content online is of major concern
for social media platforms, policymakers, and society. Researchers have studied
textual, visual, and audio content, but typically in isolation. Yet, harmful
content often combines multiple modalities, as in the case of memes, which are
of particular interest due to their viral nature. With this in mind, here we
offer a comprehensive survey with a focus on harmful memes. Based on a
systematic analysis of recent literature, we first propose a new typology of
harmful memes, and then we highlight and summarize the relevant state of the
art. One interesting finding is that many types of harmful memes are not really
studied, e.g., such featuring self-harm and extremism, partly due to the lack
of suitable datasets. We further find that existing datasets mostly capture
multi-class scenarios, which are not inclusive of the affective spectrum that
memes can represent. Another observation is that memes can propagate globally
through repackaging in different languages and that they can also be
multilingual, blending different cultures. We conclude by highlighting several
challenges related to multimodal semiotics, technological constraints, and
non-trivial social engagement, and we present several open-ended aspects such
as delineating online harm and empirically examining related frameworks and
assistive interventions, which we believe will motivate and drive future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling Extra-Textual Attributes about Dialogue Participants -- A Case Study of English-to-Polish Neural Machine Translation. (arXiv:2205.04747v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04747">
<div class="article-summary-box-inner">
<span><p>Unlike English, morphologically rich languages can reveal characteristics of
speakers or their conversational partners, such as gender and number, via
pronouns, morphological endings of words and syntax. When translating from
English to such languages, a machine translation model needs to opt for a
certain interpretation of textual context, which may lead to serious
translation errors if extra-textual information is unavailable. We investigate
this challenge in the English-to-Polish language direction. We focus on the
underresearched problem of utilising external metadata in automatic translation
of TV dialogue, proposing a case study where a wide range of approaches for
controlling attributes in translation is employed in a multi-attribute
scenario. The best model achieves an improvement of +5.81 chrF++/+6.03 BLEU,
with other models achieving competitive performance. We additionally contribute
a novel attribute-annotated dataset of Polish TV dialogue and a morphological
analysis script used to evaluate attribute control in models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Distributional Properties Drive Emergent In-Context Learning in Transformers. (arXiv:2205.05055v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05055">
<div class="article-summary-box-inner">
<span><p>Large transformer-based models are able to perform in-context few-shot
learning, without being explicitly trained for it. This observation raises the
question: what aspects of the training regime lead to this emergent behavior?
Here, we show that this behavior is driven by the distributions of the training
data itself. In-context learning emerges when the training data exhibits
particular distributional properties such as burstiness (items appear in
clusters rather than being uniformly distributed over time) and having large
numbers of rarely occurring classes. In-context learning also emerges more
strongly when item meanings or interpretations are dynamic rather than fixed.
These properties are exemplified by natural language, but are also inherent to
naturalistic data in a wide range of other domains. They also depart
significantly from the uniform, i.i.d. training distributions typically used
for standard supervised learning. In our initial experiments, we found that
in-context learning traded off against more conventional weight-based learning,
and models were unable to achieve both simultaneously. However, our later
experiments uncovered that the two modes of learning could co-exist in a single
model when it was trained on data following a skewed Zipfian distribution --
another common property of naturalistic data, including language. In further
experiments, we found that naturalistic data distributions were only able to
elicit in-context learning in transformers, and not in recurrent models. In
sum, our findings indicate how the transformer architecture works together with
particular properties of the training data to drive the intriguing emergent
in-context learning behaviour of large language models, and how future work
might encourage both in-context and in-weights learning in domains beyond
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Tips from Song Reviews: A New Dataset and Framework. (arXiv:2205.06985v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06985">
<div class="article-summary-box-inner">
<span><p>Reviews of songs play an important role in online music service platforms.
Prior research shows that users can make quicker and more informed decisions
when presented with meaningful song reviews. However, reviews of music songs
are generally long in length and most of them are non-informative for users. It
is difficult for users to efficiently grasp meaningful messages for making
decisions. To solve this problem, one practical strategy is to provide tips,
i.e., short, concise, empathetic, and self-contained descriptions about songs.
Tips are produced from song reviews and should express non-trivial insights
about the songs. To the best of our knowledge, no prior studies have explored
the tip generation task in music domain. In this paper, we create a dataset
named MTips for the task and propose a framework named GENTMS for automatically
generating tips from song reviews. The dataset involves 8,003 Chinese
tips/non-tips from 128 songs which are distributed in five different song
genres. Experimental results show that GENTMS achieves top-10 precision at
85.56%, outperforming the baseline models by at least 3.34%. Besides, to
simulate the practical usage of our proposed framework, we also experiment with
previously-unseen songs, during which GENTMS also achieves the best performance
with top-10 precision at 78.89% on average. The results demonstrate the
effectiveness of the proposed framework in tip generation of the music domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Plagiarism in Introductory Programming Course Assignments. (arXiv:2205.08520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08520">
<div class="article-summary-box-inner">
<span><p>Measuring plagiarism in programming assignments is an essential task to the
educational procedure. This paper discusses the methods of plagiarism and its
detection in introductory programming course assignments written in C++. A
small corpus of assignments is made publically available. A general framework
to compute the similarity between a solution pair is developed that uses the
three token-based similarity methods as features and predicts if the solution
is plagiarized. The importance of each feature is also measured, which in
return ranks the effectiveness of each method in use. Finally, the artificially
generated dataset improves the results compared to the original data. We
achieved an F1 score of 0.955 and 0.971 on original and synthetic datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling. (arXiv:2205.12986v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12986">
<div class="article-summary-box-inner">
<span><p>Sentence scoring aims at measuring the likelihood score of a sentence and is
widely used in many natural language processing scenarios, like reranking,
which is to select the best sentence from multiple candidates. Previous works
on sentence scoring mainly adopted either causal language modeling (CLM) like
GPT or masked language modeling (MLM) like BERT, which have some limitations:
1) CLM only utilizes unidirectional information for the probability estimation
of a sentence without considering bidirectional context, which affects the
scoring quality; 2) MLM can only estimate the probability of partial tokens at
a time and thus requires multiple forward passes to estimate the probability of
the whole sentence, which incurs large computation and time cost. In this
paper, we propose \textit{Transcormer} -- a Transformer model with a novel
\textit{sliding language modeling} (SLM) for sentence scoring. Specifically,
our SLM adopts a triple-stream self-attention mechanism to estimate the
probability of all tokens in a sentence with bidirectional context and only
requires a single forward pass. SLM can avoid the limitations of CLM (only
unidirectional context) and MLM (multiple forward passes) and inherit their
advantages, and thus achieve high effectiveness and efficiency in scoring.
Experimental results on multiple tasks demonstrate that our method achieves
better performance than other language modelings.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Voxel Based Decoding For Enhanced Natural Image Reconstruction From Brain Activity. (arXiv:2205.14177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14177">
<div class="article-summary-box-inner">
<span><p>Reconstructing perceived images from human brain activity monitored by
functional magnetic resonance imaging (fMRI) is hard, especially for natural
images. Existing methods often result in blurry and unintelligible
reconstructions with low fidelity. In this study, we present a novel approach
for enhanced image reconstruction, in which existing methods for object
decoding and image reconstruction are merged together. This is achieved by
conditioning the reconstructed image to its decoded image category using a
class-conditional generative adversarial network and neural style transfer. The
results indicate that our approach improves the semantic similarity of the
reconstructed images and can be used as a general framework for enhanced image
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised learning of features and object boundaries from local prediction. (arXiv:2205.14195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14195">
<div class="article-summary-box-inner">
<span><p>A visual system has to learn both which features to extract from images and
how to group locations into (proto-)objects. Those two aspects are usually
dealt with separately, although predictability is discussed as a cue for both.
To incorporate features and boundaries into the same model, we model a layer of
feature maps with a pairwise Markov random field model in which each factor is
paired with an additional binary variable, which switches the factor on or off.
Using one of two contrastive learning objectives, we can learn both the
features and the parameters of the Markov random field factors from images
without further supervision signals. The features learned by shallow neural
networks based on this loss are local averages, opponent colors, and Gabor-like
stripe patterns. Furthermore, we can infer connectivity between locations by
inferring the switch variables. Contours inferred from this connectivity
perform quite well on the Berkeley segmentation database (BSDS500) without any
training on contours. Thus, computing predictions across space aids both
segmentation and feature learning, and models trained to optimize these
predictions show similarities to the human visual system. We speculate that
retinotopic visual cortex might implement such predictions over space through
lateral connections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Masked Autoencoders Learn Transferable Representations. (arXiv:2205.14204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14204">
<div class="article-summary-box-inner">
<span><p>Building scalable models to learn from diverse, multimodal data remains an
open challenge. For vision-language data, the dominant approaches are based on
contrastive learning objectives that train a separate encoder for each
modality. While effective, contrastive learning approaches introduce sampling
bias depending on the data augmentations used, which can degrade performance on
downstream tasks. Moreover, these methods are limited to paired image-text
data, and cannot leverage widely-available unpaired data. In this paper, we
investigate whether a large multimodal model trained purely via masked token
prediction, without using modality-specific encoders or contrastive learning,
can learn transferable representations for downstream tasks. We propose a
simple and scalable network architecture, the Multimodal Masked Autoencoder
(M3AE), which learns a unified encoder for both vision and language data via
masked token prediction. We provide an empirical study of M3AE trained on a
large-scale image-text dataset, and find that M3AE is able to learn
generalizable representations that transfer well to downstream tasks.
Surprisingly, we find that M3AE benefits from a higher text mask ratio
(50-90%), in contrast to BERT whose standard masking ratio is 15%, due to the
joint training of two data modalities. We also provide qualitative analysis
showing that the learned representation incorporates meaningful information
from both image and language. Lastly, we demonstrate the scalability of M3AE
with larger model size and training time, and its flexibility to train on both
paired image-text data as well as unpaired data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exemplar Free Class Agnostic Counting. (arXiv:2205.14212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14212">
<div class="article-summary-box-inner">
<span><p>We tackle the task of Class Agnostic Counting, which aims to count objects in
a novel object category at test time without any access to labeled training
data for that category. All previous class agnostic counting methods cannot
work in a fully automated setting, and require computationally expensive test
time adaptation. To address these challenges, we propose a visual counter which
operates in a fully automated setting and does not require any test time
adaptation. Our proposed approach first identifies exemplars from repeating
objects in an image, and then counts the repeating objects. We propose a novel
region proposal network for identifying the exemplars. After identifying the
exemplars, we obtain the corresponding count by using a density estimation
based Visual Counter. We evaluate our proposed approach on FSC-147 dataset, and
show that it achieves superior performance compared to the existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction. (arXiv:2205.14230v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14230">
<div class="article-summary-box-inner">
<span><p>Predicting the trajectories of surrounding objects is a critical task in
self-driving and many other autonomous systems. Recent works demonstrate that
adversarial attacks on trajectory prediction, where small crafted perturbations
are introduced to history trajectories, may significantly mislead the
prediction of future trajectories and ultimately induce unsafe planning.
However, few works have addressed enhancing the robustness of this important
safety-critical task. In this paper, we present the first adversarial training
method for trajectory prediction. Compared with typical adversarial training on
image tasks, our work is challenged by more random inputs with rich context,
and a lack of class labels. To address these challenges, we propose a method
based on a semi-supervised adversarial autoencoder that models disentangled
semantic features with domain knowledge and provides additional latent labels
for the adversarial training. Extensive experiments with different types of
attacks demonstrate that our semi-supervised semantics-guided adversarial
training method can effectively mitigate the impact of adversarial attacks and
generally improve the system's adversarial robustness to a variety of attacks,
including unseen ones. We believe that such semantics-guided architecture and
advancement in robust generalization is an important step for developing robust
prediction models and enabling safe decision making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Keypoint Matching using Graph Neural Networks. (arXiv:2205.14275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14275">
<div class="article-summary-box-inner">
<span><p>Image matching is a key component of many tasks in computer vision and its
main objective is to find correspondences between features extracted from
different natural images. When images are represented as graphs, image matching
boils down to the problem of graph matching which has been studied intensively
in the past. In recent years, graph neural networks have shown great potential
in the graph matching task, and have also been applied to image matching. In
this paper, we propose a graph neural network for the problem of image
matching. The proposed method first generates initial soft correspondences
between keypoints using localized node embeddings and then iteratively refines
the initial correspondences using a series of graph neural network layers. We
evaluate our method on natural image datasets with keypoint annotations and
show that, in comparison to a state-of-the-art model, our method speeds up
inference times without sacrificing prediction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Object Placement Assessment. (arXiv:2205.14280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14280">
<div class="article-summary-box-inner">
<span><p>Object placement assessment (OPA) aims to predict the rationality score of a
composite image in terms of the placement (e.g., scale, location) of inserted
foreground object. However, given a pair of scaled foreground and background,
to enumerate all the reasonable locations, existing OPA model needs to place
the foreground at each location on the background and pass the obtained
composite image through the model one at a time, which is very time-consuming.
In this work, we investigate a new task named as fast OPA. Specifically,
provided with a scaled foreground and a background, we only pass them through
the model once and predict the rationality scores for all locations. To
accomplish this task, we propose a pioneering fast OPA model with several
innovations (i.e., foreground dynamic filter, background prior transfer, and
composite feature mimicking) to bridge the performance gap between slow OPA
model and fast OPA model. Extensive experiments on OPA dataset show that our
proposed fast OPA model performs on par with slow OPA model but runs
significantly faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Lip Region-of-Interest Sufficient for Lipreading?. (arXiv:2205.14295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14295">
<div class="article-summary-box-inner">
<span><p>Lip region-of-interest (ROI) is conventionally used for visual input in the
lipreading task. Few works have adopted the entire face as visual input because
lip-excluded parts of the face are usually considered to be redundant and
irrelevant to visual speech recognition. However, faces contain much more
detailed information than lips, such as speakers' head pose, emotion, identity
etc. We argue that such information might benefit visual speech recognition if
a powerful feature extractor employing the entire face is trained. In this
work, we propose to adopt the entire face for lipreading with self-supervised
learning. AV-HuBERT, an audio-visual multi-modal self-supervised learning
framework, was adopted in our experiments. Our experimental results showed that
adopting the entire face achieved 16% relative word error rate (WER) reduction
on the lipreading task, compared with the baseline method using lip as visual
input. Without self-supervised pretraining, the model with face input achieved
a higher WER than that using lip input in the case of limited training data (30
hours), while a slightly lower WER when using large amount of training data
(433 hours).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake It Till You Make It: Near-Distribution Novelty Detection by Score-Based Generative Models. (arXiv:2205.14297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14297">
<div class="article-summary-box-inner">
<span><p>We aim for image-based novelty detection. Despite considerable progress,
existing models either fail or face a dramatic drop under the so-called
``near-distribution" setting, where the differences between normal and
anomalous samples are subtle. We first demonstrate existing methods experience
up to 20\% decrease in performance in the near-distribution setting. Next, we
propose to exploit a score-based generative model to produce synthetic
near-distribution anomalous data. Our model is then fine-tuned to distinguish
such data from the normal samples. We provide a quantitative as well as
qualitative evaluation of this strategy, and compare the results with a variety
of GAN-based models. Effectiveness of our method for both the near-distribution
and standard novelty detection is assessed through extensive experiments on
datasets in diverse applications such as medical images, object classification,
and quality control. This reveals that our method considerably improves over
existing models, and consistently decreases the gap between the
near-distribution and standard novelty detection performance. Overall, our
method improves the near-distribution novelty detection by 6% and passes the
state-of-the-art by 1% to 5% across nine novelty detection benchmarks. The code
repository is available at https://github.com/rohban-lab/FITYMI
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning with Label Noise: A Hierarchical Approach. (arXiv:2205.14299v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14299">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are susceptible to label noise. Existing methods to
improve robustness, such as meta-learning and regularization, usually require
significant change to the network architecture or careful tuning of the
optimization procedure. In this work, we propose a simple hierarchical approach
that incorporates a label hierarchy when training the deep learning models. Our
approach requires no change of the network architecture or the optimization
procedure. We investigate our hierarchical network through a wide range of
simulated and real datasets and various label noise types. Our hierarchical
approach improves upon regular deep neural networks in learning with label
noise. Combining our hierarchical approach with pre-trained models achieves
state-of-the-art performance in real-world noisy datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Fake News Detection via CLIP-Guided Learning. (arXiv:2205.14304v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14304">
<div class="article-summary-box-inner">
<span><p>Multimodal fake news detection has attracted many research interests in
social forensics. Many existing approaches introduce tailored attention
mechanisms to guide the fusion of unimodal features. However, how the
similarity of these features is calculated and how it will affect the
decision-making process in FND are still open questions. Besides, the potential
of pretrained multi-modal feature learning models in fake news detection has
not been well exploited. This paper proposes a FND-CLIP framework, i.e., a
multimodal Fake News Detection network based on Contrastive Language-Image
Pretraining (CLIP). Given a targeted multimodal news, we extract the deep
representations from the image and text using a ResNet-based encoder, a
BERT-based encoder and two pair-wise CLIP encoders. The multimodal feature is a
concatenation of the CLIP-generated features weighted by the standardized
cross-modal similarity of the two modalities. The extracted features are
further processed for redundancy reduction before feeding them into the final
classifier. We introduce a modality-wise attention module to adaptively
reweight and aggregate the features. We have conducted extensive experiments on
typical fake news datasets. The results indicate that the proposed framework
has a better capability in mining crucial features for fake news detection. The
proposed FND-CLIP can achieve better performances than previous works, i.e.,
0.7\%, 6.8\% and 1.3\% improvements in overall accuracy on Weibo, Politifact
and Gossipcop, respectively. Besides, we justify that CLIP-based learning can
allow better flexibility on multimodal feature selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Molecular Image Recognition: A Graph Generation Approach. (arXiv:2205.14311v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14311">
<div class="article-summary-box-inner">
<span><p>Molecular image recognition is a fundamental task in information extraction
from chemistry literature. Previous data-driven models formulate it as an
image-to-sequence task, to generate a sequential representation of the molecule
(e.g. SMILES string) from its graphical representation. Although they perform
adequately on certain benchmarks, these models are not robust in real-world
situations, where molecular images differ in style, quality, and chemical
patterns. In this paper, we propose a novel graph generation approach that
explicitly predicts atoms and bonds, along with their geometric layouts, to
construct the molecular graph. We develop data augmentation strategies for
molecules and images to increase the robustness of our model against domain
shifts. Our model is flexible to incorporate chemistry constraints, and
produces more interpretable predictions than SMILES. In experiments on both
synthetic and realistic molecular images, our model significantly outperforms
previous models, achieving 84-93% accuracy on five benchmarks. We also conduct
human evaluation and show that our model reduces the time for a chemist to
extract molecular structures from images by roughly 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WT-MVSNet: Window-based Transformers for Multi-view Stereo. (arXiv:2205.14319v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14319">
<div class="article-summary-box-inner">
<span><p>Recently, Transformers were shown to enhance the performance of multi-view
stereo by enabling long-range feature interaction. In this work, we propose
Window-based Transformers (WT) for local feature matching and global feature
aggregation in multi-view stereo. We introduce a Window-based Epipolar
Transformer (WET) which reduces matching redundancy by using epipolar
constraints. Since point-to-line matching is sensitive to erroneous camera pose
and calibration, we match windows near the epipolar lines. A second Shifted WT
is employed for aggregating global information within cost volume. We present a
novel Cost Transformer (CT) to replace 3D convolutions for cost volume
regularization. In order to better constrain the estimated depth maps from
multiple views, we further design a novel geometric consistency loss (Geo Loss)
which punishes unreliable areas where multi-view consistency is not satisfied.
Our WT multi-view stereo method (WT-MVSNet) achieves state-of-the-art
performance across multiple datasets and ranks $1^{st}$ on Tanks and Temples
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo. (arXiv:2205.14320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14320">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a learning-based approach for multi-view stereo
(MVS), i.e., estimate the depth map of a reference frame using posed multi-view
images. Our core idea lies in leveraging a "learning-to-optimize" paradigm to
iteratively index a plane-sweeping cost volume and regress the depth map via a
convolutional Gated Recurrent Unit (GRU). Since the cost volume plays a
paramount role in encoding the multi-view geometry, we aim to improve its
construction both in pixel- and frame- levels. In the pixel level, we propose
to break the symmetry of the Siamese network (which is typically used in MVS to
extract image features) by introducing a transformer block to the reference
image (but not to the source images). Such an asymmetric volume allows the
network to extract global features from the reference image to predict its
depth map. In view of the inaccuracy of poses between reference and source
images, we propose to incorporate a residual pose network to make corrections
to the relative poses, which essentially rectifies the cost volume in the
frame-level. We conduct extensive experiments on real-world MVS datasets and
show that our method achieves state-of-the-art performance in terms of both
within-dataset evaluation and cross-dataset generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point RCNN: An Angle-Free Framework for Rotated Object Detection. (arXiv:2205.14328v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14328">
<div class="article-summary-box-inner">
<span><p>Rotated object detection in aerial images is still challenging due to
arbitrary orientations, large scale and aspect ratio variations, and extreme
density of objects. Existing state-of-the-art rotated object detection methods
mainly rely on angle-based detectors. However, angle regression can easily
suffer from the long-standing boundary problem. To tackle this problem, we
propose a purely angle-free framework for rotated object detection, called
Point RCNN, which mainly consists of PointRPN and PointReg. In particular,
PointRPN generates accurate rotated RoIs (RRoIs) by converting the learned
representative points with a coarse-to-fine manner, which is motivated by
RepPoints. Based on the learned RRoIs, PointReg performs corner points
refinement for more accurate detection. In addition, aerial images are often
severely unbalanced in categories, and existing methods almost ignore this
issue. In this paper, we also experimentally verify that re-sampling the images
of the rare categories will stabilize training and further improve the
detection performance. Experiments demonstrate that our Point RCNN achieves the
new state-of-the-art detection performance on commonly used aerial datasets,
including DOTA-v1.0, DOTA-v1.5, and HRSC2016.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Point-Based Radiance Fields for Efficient View Synthesis. (arXiv:2205.14330v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14330">
<div class="article-summary-box-inner">
<span><p>We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V4D: Voxel for 4D Novel View Synthesis. (arXiv:2205.14332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14332">
<div class="article-summary-box-inner">
<span><p>Neural radiance fields have made a remarkable breakthrough in the novel view
synthesis task at the 3D static scene. However, for the 4D circumstance (e.g.,
dynamic scene), the performance of the existing method is still limited by the
capacity of the neural network, typically in a multilayer perceptron network
(MLP). In this paper, we present the method to model the 4D neural radiance
field by the 3D voxel, short as V4D, where the 3D voxel has two formats. The
first one is to regularly model the bounded 3D space and then use the sampled
local 3D feature with the time index to model the density field and the texture
field. The second one is in look-up tables (LUTs) format that is for the
pixel-level refinement, where the pseudo-surface produced by the volume
rendering is utilized as the guidance information to learn a 2D pixel-level
refinement mapping. The proposed LUTs-based refinement module achieves the
performance gain with a little computational cost and could serve as the
plug-and-play module in the novel view synthesis task. Moreover, we propose a
more effective conditional positional encoding toward the 4D data that achieves
performance gain with negligible computational burdens. Extensive experiments
demonstrate that the proposed method achieves state-of-the-art performance by a
large margin. At last, the proposed V4D is also a computational-friendly method
in both the training and testing phase, where we achieve 2 times faster in the
training phase and 10 times faster in the inference phase compared with the
state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-wise Masked Autoencoders for Fast Pre-training. (arXiv:2205.14338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14338">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training for images without labels has recently achieved
promising performance in image classification. The success of transformer-based
methods, ViT and MAE, draws the community's attention to the design of backbone
architecture and self-supervised task. In this work, we show that current
masked image encoding models learn the underlying relationship between all
objects in the whole scene, instead of a single object representation.
Therefore, those methods bring a lot of compute time for self-supervised
pre-training. To solve this issue, we introduce a novel object selection and
division strategy to drop non-object patches for learning object-wise
representations by selective reconstruction with interested region masks. We
refer to this method ObjMAE. Extensive experiments on four commonly-used
datasets demonstrate the effectiveness of our model in reducing the compute
cost by 72% while achieving competitive performance. Furthermore, we
investigate the inter-object and intra-object relationship and find that the
latter is crucial for self-supervised pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimation of 3D Body Shape and Clothing Measurements from Frontal- and Side-view Images. (arXiv:2205.14347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14347">
<div class="article-summary-box-inner">
<span><p>The estimation of 3D human body shape and clothing measurements is crucial
for virtual try-on and size recommendation problems in the fashion industry but
has always been a challenging problem due to several conditions, such as lack
of publicly available realistic datasets, ambiguity in multiple camera
resolutions, and the undefinable human shape space. Existing works proposed
various solutions to these problems but could not succeed in the industry
adaptation because of complexity and restrictions. To solve the complexity and
challenges, in this paper, we propose a simple yet effective architecture to
estimate both shape and measures from frontal- and side-view images. We utilize
silhouette segmentation from the two multi-view images and implement an
auto-encoder network to learn low-dimensional features from segmented
silhouettes. Then, we adopt a kernel-based regularized regression module to
estimate the body shape and measurements. The experimental results show that
the proposed method provides competitive results on the synthetic dataset,
NOMO-3d-400-scans Dataset, and RGB Images of humans captured in different
cameras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning with Multi-query Transformer for Dense Prediction. (arXiv:2205.14354v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14354">
<div class="article-summary-box-inner">
<span><p>Previous multi-task dense prediction studies developed complex pipelines such
as multi-modal distillations in multiple stages or searching for task
relational contexts for each task. The core insight beyond these methods is to
maximize the mutual effects between each task. Inspired by the recent
query-based Transformers, we propose a simpler pipeline named Multi-Query
Transformer (MQTransformer) that is equipped with multiple queries from
different tasks to facilitate the reasoning among multiple tasks and simplify
the cross task pipeline. Instead of modeling the dense per-pixel context among
different tasks, we seek a task-specific proxy to perform cross-task reasoning
via multiple queries where each query encodes the task-related context. The
MQTransformer is composed of three key components: shared encoder, cross task
attention and shared decoder. We first model each task with a task-relevant and
scale-aware query, and then both the image feature output by the feature
extractor and the task-relevant query feature are fed into the shared encoder,
thus encoding the query feature from the image feature. Secondly, we design a
cross task attention module to reason the dependencies among multiple tasks and
feature scales from two perspectives including different tasks of the same
scale and different scales of the same task. Then we use a shared decoder to
gradually refine the image features with the reasoned query features from
different tasks. Extensive experiment results on two dense prediction datasets
(NYUD-v2 and PASCAL-Context) show that the proposed method is an effective
approach and achieves the state-of-the-art result. Code will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Facial Expression Recognition by A Semi-Supervised Progressive Teacher. (arXiv:2205.14361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14361">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to improve the performance of in-the-wild Facial
Expression Recognition (FER) by exploiting semi-supervised learning.
Large-scale labeled data and deep learning methods have greatly improved the
performance of image recognition. However, the performance of FER is still not
ideal due to the lack of training data and incorrect annotations (e.g., label
noises). Among existing in-the-wild FER datasets, reliable ones contain
insufficient data to train robust deep models while large-scale ones are
annotated in lower quality. To address this problem, we propose a
semi-supervised learning algorithm named Progressive Teacher (PT) to utilize
reliable FER datasets as well as large-scale unlabeled expression images for
effective training. On the one hand, PT introduces semi-supervised learning
method to relieve the shortage of data in FER. On the other hand, it selects
useful labeled training samples automatically and progressively to alleviate
label noise. PT uses selected clean labeled data for computing the supervised
classification loss and unlabeled data for unsupervised consistency loss.
Experiments on widely-used databases RAF-DB and FERPlus validate the
effectiveness of our method, which achieves state-of-the-art performance with
accuracy of 89.57% on RAF-DB. Additionally, when the synthetic noise rate
reaches even 30%, the performance of our PT algorithm only degrades by 4.37%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WaveMix-Lite: A Resource-efficient Neural Network for Image Analysis. (arXiv:2205.14375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14375">
<div class="article-summary-box-inner">
<span><p>Gains in the ability to generalize on image analysis tasks for neural
networks have come at the cost of increased number of parameters and layers,
dataset sizes, training and test computations, and GPU RAM. We introduce a new
architecture -- WaveMix-Lite -- that can generalize on par with contemporary
transformers and convolutional neural networks (CNNs) while needing fewer
resources. WaveMix-Lite uses 2D-discrete wavelet transform to efficiently mix
spatial information from pixels. WaveMix-Lite seems to be a versatile and
scalable architectural framework that can be used for multiple vision tasks,
such as image classification and semantic segmentation, without requiring
significant architectural changes, unlike transformers and CNNs. It is able to
meet or exceed several accuracy benchmarks while training on a single GPU. For
instance, it achieves state-of-the-art accuracy on five EMNIST datasets,
outperforms CNNs and transformers in ImageNet-1K (64$\times$64 images), and
achieves an mIoU of 75.32 % on Cityscapes validation set, while using less than
one-fifth the number parameters and half the GPU RAM of comparable CNNs or
transformers. Our experiments show that while the convolutional elements of
neural architectures exploit the shift-invariance property of images, new types
of layers (e.g., wavelet transform) can exploit additional properties of
images, such as scale-invariance and finite spatial extents of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Quality of Pose-varied Face Restoration with Local Weak Feature Sensing and GAN Prior. (arXiv:2205.14377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14377">
<div class="article-summary-box-inner">
<span><p>Facial semantic guidance (facial landmarks, facial parsing maps, facial
heatmaps, etc.) and facial generative adversarial networks (GAN) prior have
been widely used in blind face restoration (BFR) in recent years. Although
existing BFR methods have achieved good performance in ordinary cases, these
solutions have limited resilience when applied to face images with serious
degradation and pose-varied (look up, look down, laugh, etc.) in real-world
scenarios. In this work, we propose a well-designed blind face restoration
network with generative facial prior. The proposed network is mainly comprised
of an asymmetric codec and StyleGAN2 prior network. In the asymmetric codec, we
adopt a mixed multi-path residual block (MMRB) to gradually extract weak
texture features of input images, which can improve the texture integrity and
authenticity of our networks. Furthermore, the MMRB block can also be
plug-and-play in any other network. Besides, a novel self-supervised training
strategy is specially designed for face restoration tasks to fit the
distribution closer to the target and maintain training stability. Extensive
experiments over synthetic and real-world datasets demonstrate that our model
achieves superior performance to the prior art for face restoration and face
super-resolution tasks and can tackle seriously degraded face images in diverse
poses and expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training. (arXiv:2205.14401v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14401">
<div class="article-summary-box-inner">
<span><p>Masked Autoencoders (MAE) have shown great potentials in self-supervised
pre-training for language and 2D image transformers. However, it still remains
an open question on how to exploit masked autoencoding for learning 3D
representations of irregular point clouds. In this paper, we propose
Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical
self-supervised learning of 3D point clouds. Unlike the standard transformer in
MAE, we modify the encoder and decoder into pyramid architectures to
progressively model spatial geometries and capture both fine-grained and
high-level semantics of 3D shapes. For the encoder that downsamples point
tokens by stages, we design a multi-scale masking strategy to generate
consistent visible regions across scales, and adopt a local spatial
self-attention mechanism to focus on neighboring patterns. By multi-scale token
propagation, the lightweight decoder gradually upsamples point tokens with
complementary skip connections from the encoder, which further promotes the
reconstruction from a global-to-local perspective. Extensive experiments
demonstrate the state-of-the-art performance of Point-M2AE for 3D
representation learning. With a frozen encoder after pre-training, Point-M2AE
achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some
fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves
86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely
benefits the few-shot classification, part segmentation and 3D object detection
with the hierarchical pre-training scheme. Code will be available at
https://github.com/ZrrSkywalker/Point-M2AE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strengthening Skeletal Action Recognizers via Leveraging Temporal Patterns. (arXiv:2205.14405v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14405">
<div class="article-summary-box-inner">
<span><p>Skeleton sequences are compact and lightweight. Numerous skeleton-based
action recognizers have been proposed to classify human behaviors. In this
work, we aim to incorporate components that are compatible with existing models
and further improve their accuracy. To this end, we design two temporal
accessories: discrete cosine encoding (DCE) and chronological loss (CRL). DCE
facilitates models to analyze motion patterns from the frequency domain and
meanwhile alleviates the influence of signal noise. CRL guides networks to
explicitly capture the sequence's chronological order. These two components
consistently endow many recently-proposed action recognizers with accuracy
boosts, achieving new state-of-the-art (SOTA) accuracy on two large benchmark
datasets (NTU60 and NTU120).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Generation for Satellite Image Classification Using Self-Supervised Representation Learning. (arXiv:2205.14418v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14418">
<div class="article-summary-box-inner">
<span><p>Supervised deep neural networks are the-state-of-the-art for many tasks in
the remote sensing domain, against the fact that such techniques require the
dataset consisting of pairs of input and label, which are rare and expensive to
collect in term of both manpower and resources. On the other hand, there are
abundance of raw satellite images available both for commercial and academic
purposes. Hence, in this work, we tackle the insufficient labeled data problem
in satellite image classification task by introducing the process based on the
self-supervised learning technique to create the synthetic labels for satellite
image patches. These synthetic labels can be used as the training dataset for
the existing supervised learning techniques. In our experiments, we show that
the models trained on the synthetic labels give similar performance to the
models trained on the real labels. And in the process of creating the synthetic
labels, we also obtain the visual representation vectors that are versatile and
knowledge transferable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looks Like Magic: Transfer Learning in GANs to Generate New Card Illustrations. (arXiv:2205.14442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14442">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose MAGICSTYLEGAN and MAGICSTYLEGAN-ADA - both
incarnations of the state-of-the-art models StyleGan2 and StyleGan2 ADA - to
experiment with their capacity of transfer learning into a rather different
domain: creating new illustrations for the vast universe of the game "Magic:
The Gathering" cards. This is a challenging task especially due to the variety
of elements present in these illustrations, such as humans, creatures,
artifacts, and landscapes - not to mention the plethora of art styles of the
images made by various artists throughout the years. To solve the task at hand,
we introduced a novel dataset, named MTG, with thousands of illustration from
diverse card types and rich in metadata. The resulting set is a dataset
composed by a myriad of both realistic and fantasy-like illustrations.
Although, to investigate effects of diversity we also introduced subsets that
contain specific types of concepts, such as forests, islands, faces, and
humans. We show that simpler models, such as DCGANs, are not able to learn to
generate proper illustrations in any setting. On the other side, we train
instances of MAGICSTYLEGAN using all proposed subsets, being able to generate
high quality illustrations. We perform experiments to understand how well
pre-trained features from StyleGan2 can be transferred towards the target
domain. We show that in well trained models we can find particular instances of
noise vector that realistically represent real images from the dataset.
Moreover, we provide both quantitative and qualitative studies to support our
claims, and that demonstrate that MAGICSTYLEGAN is the state-of-the-art
approach for generating Magic illustrations. Finally, this paper highlights
some emerging properties regarding transfer learning in GANs, which is still a
somehow under-explored field in generative learning research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Self-supervised Lightweight Vision Transformers. (arXiv:2205.14443v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14443">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning on large-scale Vision Transformers (ViTs) as
pre-training methods has achieved promising downstream performance. Yet, how
such pre-training paradigms promote lightweight ViTs' performance is
considerably less studied. In this work, we mainly produce recipes for
pre-training high-performance lightweight ViTs using
masked-image-modeling-based MAE, namely MAE-lite, which achieves 78.4% top-1
accuracy on ImageNet with ViT-Tiny (5.7M). Furthermore, we develop and
benchmark other fully-supervised and self-supervised pre-training counterparts,
e.g., contrastive-learning-based MoCo-v3, on both ImageNet and other
classification tasks. We analyze and clearly show the effect of such
pre-training, and reveal that properly-learned lower layers of the pre-trained
models matter more than higher ones in data-sufficient downstream tasks.
Finally, by further comparing with the pre-trained representations of the
up-scaled models, a distillation strategy during pre-training is developed to
improve the pre-trained representations as well, leading to further downstream
performance improvement. The code and models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Superordinate Abstraction for Robust Concept Learning. (arXiv:2205.14444v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14444">
<div class="article-summary-box-inner">
<span><p>Concept learning constructs visual representations that are connected to
linguistic semantics, which is fundamental to vision-language tasks. Although
promising progress has been made, existing concept learners are still
vulnerable to attribute perturbations and out-of-distribution compositions
during inference. We ascribe the bottleneck to a failure of exploring the
intrinsic semantic hierarchy of visual concepts, e.g. \{red, blue,...\} $\in$
`color' subspace yet cube $\in$ `shape'. In this paper, we propose a visual
superordinate abstraction framework for explicitly modeling semantic-aware
visual subspaces (i.e. visual superordinates). With only natural visual
question answering data, our model first acquires the semantic hierarchy from a
linguistic view, and then explores mutually exclusive visual superordinates
under the guidance of linguistic hierarchy. In addition, a quasi-center visual
concept clustering and a superordinate shortcut learning schemes are proposed
to enhance the discrimination and independence of concepts within each visual
superordinate. Experiments demonstrate the superiority of the proposed
framework under diverse settings, which increases the overall answering
accuracy relatively by 7.5\% on reasoning with perturbations and 15.6\% on
compositional generalization tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Transformer: A Framework Beyond the Trade-off between Accuracy and Diversity for Image Captioning. (arXiv:2205.14458v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14458">
<div class="article-summary-box-inner">
<span><p>Accuracy and Diversity are two essential metrizable manifestations in
generating natural and semantically correct captions. Many efforts have been
made to enhance one of them with another decayed due to the trade-off gap.
However, compromise does not make the progress. Decayed diversity makes the
captioner a repeater, and decayed accuracy makes it a fake advisor. In this
work, we exploit a novel Variational Transformer framework to improve accuracy
and diversity simultaneously. To ensure accuracy, we introduce the "Invisible
Information Prior" along with the "Auto-selectable GMM" to instruct the encoder
to learn the precise language information and object relation in different
scenes. To ensure diversity, we propose the "Range-Median Reward" baseline to
retain more diverse candidates with higher rewards during the RL-based training
process. Experiments show that our method achieves the simultaneous promotion
of accuracy (CIDEr) and diversity (self-CIDEr), up to 1.1 and 4.8 percent,
compared with the baseline. Also, our method outperforms others under the newly
proposed measurement of the trade-off gap, with at least 3.55 percent
promotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CyCLIP: Cyclic Contrastive Language-Image Pretraining. (arXiv:2205.14459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14459">
<div class="article-summary-box-inner">
<span><p>Recent advances in contrastive representation learning over paired image-text
data have led to models such as CLIP that achieve state-of-the-art performance
for zero-shot classification and distributional robustness. Such models
typically require joint reasoning in the image and text representation spaces
for downstream inference tasks. Contrary to prior beliefs, we demonstrate that
the image and text representations learned via a standard contrastive objective
are not interchangeable and can lead to inconsistent downstream predictions. To
mitigate this issue, we formalize consistency and propose CyCLIP, a framework
for contrastive representation learning that explicitly optimizes for the
learned representations to be geometrically consistent in the image and text
space. In particular, we show that consistent representations can be learned by
explicitly symmetrizing (a) the similarity between the two mismatched
image-text pairs (cross-modal consistency); and (b) the similarity between the
image-image pair and the text-text pair (in-modal consistency). Empirically, we
show that the improved consistency in CyCLIP translates to significant gains
over CLIP, with gains ranging from 10%-24% for zero-shot classification
accuracy on standard benchmarks (CIFAR-10, CIFAR-100, ImageNet1K) and 10%-27%
for robustness to various natural distribution shifts. The code is available at
https://github.com/goel-shashank/CyCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors. (arXiv:2205.14467v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14467">
<div class="article-summary-box-inner">
<span><p>Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an
unlabeled target domain supervised by a black-box predictor trained on a source
domain. It does not require access to both the source-domain data and the
predictor parameters, thus addressing the data privacy and portability issues
of standard domain adaptation. Existing DABP approaches mostly rely on model
distillation from the black-box predictor, \emph{i.e.}, training the model with
its noisy target-domain predictions, which however inevitably introduces the
confirmation bias accumulated from the prediction noises. To mitigate such
bias, we propose a new method, named BETA, to incorporate knowledge
distillation and noisy label learning into one coherent framework. This is
enabled by a new divide-to-adapt strategy. BETA divides the target domain into
an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain. Then
it deploys mutually-teaching twin networks to filter the predictor errors for
each other and improve them progressively, from the easy to hard subdomains. As
such, BETA effectively purifies the noisy labels and reduces error
accumulation. We theoretically show that the target error of BETA is minimized
by decreasing the noise ratio of the subdomains. Extensive experiments
demonstrate BETA outperforms existing methods on all DABP benchmarks, and is
even comparable with the standard domain adaptation methods that use the
source-domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptually Optimized Color Selection for Visualization. (arXiv:2205.14472v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14472">
<div class="article-summary-box-inner">
<span><p>We propose an approach, called the Equilibrium Distribution Model (EDM), for
automatically selecting colors with optimum perceptual contrast for scientific
visualization. Given any number of features that need to be emphasized in a
visualization task, our approach derives evenly distributed points in the
CIELAB color space to assign colors to the features so that the minimum
Euclidean Distance among the colors are optimized. Our approach can assign
colors with high perceptual contrast even for very high numbers of features,
where other color selection methods typically fail. We compare our approach
with the widely used Harmonic color selection scheme and demonstrate that while
the harmonic scheme can achieve reasonable color contrast for visualizing up to
20 different features, our Equilibrium scheme provides significantly better
contrast and achieves perceptible contrast for visualizing even up to 100
unique features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepRM: Deep Recurrent Matching for 6D Pose Refinement. (arXiv:2205.14474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14474">
<div class="article-summary-box-inner">
<span><p>Precise 6D pose estimation of rigid objects from RGB images is a critical but
challenging task in robotics and augmented reality. To address this problem, we
propose DeepRM, a novel recurrent network architecture for 6D pose refinement.
DeepRM leverages initial coarse pose estimates to render synthetic images of
target objects. The rendered images are then matched with the observed images
to predict a rigid transform for updating the previous pose estimate. This
process is repeated to incrementally refine the estimate at each iteration.
LSTM units are used to propagate information through each refinement step,
significantly improving overall performance. In contrast to many 2-stage
Perspective-n-Point based solutions, DeepRM is trained end-to-end, and uses a
scalable backbone that can be tuned via a single parameter for accuracy and
efficiency. During training, a multi-scale optical flow head is added to
predict the optical flow between the observed and synthetic images. Optical
flow prediction stabilizes the training process, and enforces the learning of
features that are relevant to the task of pose estimation. Our results
demonstrate that DeepRM achieves state-of-the-art performance on two widely
accepted challenging datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDMLP: Image Classification from Scratch on Small Datasets with MLP. (arXiv:2205.14477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14477">
<div class="article-summary-box-inner">
<span><p>The attention mechanism has become a go-to technique for natural language
processing and computer vision tasks. Recently, the MLP-Mixer and other
MLP-based architectures, based simply on multi-layer perceptrons (MLPs), are
also powerful compared to CNNs and attention techniques and raises a new
research direction. However, the high capability of the MLP-based networks
severely relies on large volume of training data, and lacks of explanation
ability compared to the Vision Transformer (ViT) or ConvNets. When trained on
small datasets, they usually achieved inferior results than ConvNets. To
resolve it, we present (i) multi-dimensional MLP (MDMLP), a conceptually simple
and lightweight MLP-based architecture yet achieves SOTA when training from
scratch on small-size datasets; (ii) multi-dimension MLP Attention Tool
(MDAttnTool), a novel and efficient attention mechanism based on MLPs. Even
without strong data augmentation, MDMLP achieves 90.90% accuracy on CIFAR10
with only 0.3M parameters, while the well-known MLP-Mixer achieves 85.45% with
17.1M parameters. In addition, the lightweight MDAttnTool highlights objects in
images, indicating its explanation power. Our code is available at
https://github.com/Amoza-Theodore/MDMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New High-Performance Approach to Approximate Pattern-Matching for Plagiarism Detection in Blockchain-Based Non-Fungible Tokens (NFTs). (arXiv:2205.14492v1 [cs.FL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14492">
<div class="article-summary-box-inner">
<span><p>We are presenting a fast and innovative approach to performing approximate
pattern-matching for plagiarism detection, using an NDFA-based approach that
significantly enhances performance compared to other existing similarity
measures. We outline the advantages of our approach in the context of
blockchain-based non-fungible tokens (NFTs). We present, formalize, discuss and
test our proposed approach in several real-world scenarios and with different
similarity measures commonly used in plagiarism detection, and observe
significant throughput enhancements throughout the entire spectrum of tests,
with little to no compromises on the accuracy of the detection process overall.
We conclude that our approach is suitable and adequate to perform approximate
pattern-matching for plagiarism detection, and outline research directions for
future improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BadDet: Backdoor Attacks on Object Detection. (arXiv:2205.14497v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14497">
<div class="article-summary-box-inner">
<span><p>Deep learning models have been deployed in numerous real-world applications
such as autonomous driving and surveillance. However, these models are
vulnerable in adversarial environments. Backdoor attack is emerging as a severe
security threat which injects a backdoor trigger into a small portion of
training data such that the trained model behaves normally on benign inputs but
gives incorrect predictions when the specific trigger appears. While most
research in backdoor attacks focuses on image classification, backdoor attacks
on object detection have not been explored but are of equal importance. Object
detection has been adopted as an important module in various security-sensitive
applications such as autonomous driving. Therefore, backdoor attacks on object
detection could pose severe threats to human lives and properties. We propose
four kinds of backdoor attacks for object detection task: 1) Object Generation
Attack: a trigger can falsely generate an object of the target class; 2)
Regional Misclassification Attack: a trigger can change the prediction of a
surrounding object to the target class; 3) Global Misclassification Attack: a
single trigger can change the predictions of all objects in an image to the
target class; and 4) Object Disappearance Attack: a trigger can make the
detector fail to detect the object of the target class. We develop appropriate
metrics to evaluate the four backdoor attacks on object detection. We perform
experiments using two typical object detection models -- Faster-RCNN and YOLOv3
on different datasets. More crucially, we demonstrate that even fine-tuning on
another benign dataset cannot remove the backdoor hidden in the object
detection model. To defend against these backdoor attacks, we propose Detector
Cleanse, an entropy-based run-time detection framework to identify poisoned
testing samples for any deployed object detector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14540">
<div class="article-summary-box-inner">
<span><p>Self-supervised Masked Autoencoders (MAE) are emerging as a new pre-training
paradigm in computer vision. MAE learns semantics implicitly via reconstructing
local patches, requiring thousands of pre-training epochs to achieve favorable
performance. This paper incorporates explicit supervision, i.e., golden labels,
into the MAE framework. The proposed Supervised MAE (SupMAE) only exploits a
visible subset of image patches for classification, unlike the standard
supervised pre-training where all image patches are used. SupMAE is efficient
and can achieve comparable performance with MAE using only 30% compute when
evaluated on ImageNet with the ViT-B/16 model. Detailed ablation studies are
conducted to verify the proposed components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Missing Invariance Principle Found -- the Reciprocal Twin of Invariant Risk Minimization. (arXiv:2205.14546v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14546">
<div class="article-summary-box-inner">
<span><p>Machine learning models often generalize poorly to out-of-distribution (OOD)
data as a result of relying on features that are spuriously correlated with the
label during training. Recently, the technique of Invariant Risk Minimization
(IRM) was proposed to learn predictors that only use invariant features by
conserving the feature-conditioned class expectation $\mathbb{E}_e[y|f(x)]$
across environments. However, more recent studies have demonstrated that IRM
can fail in various task settings. Here, we identify a fundamental flaw of IRM
formulation that causes the failure. We then introduce a complementary notion
of invariance, MRI, that is based on conserving the class-conditioned feature
expectation $\mathbb{E}_e[f(x)|y]$ across environments, that corrects for the
flaw in IRM. Further, we introduce a simplified, practical version of the MRI
formulation called as MRI-v1. We note that this constraint is convex which
confers it with an advantage over the practical version of IRM, IRM-v1, which
imposes non-convex constraints. We prove that in a general linear problem
setting, MRI-v1 can guarantee invariant predictors given sufficient
environments. We also empirically demonstrate that MRI strongly out-performs
IRM and consistently achieves near-optimal OOD generalization in image-based
nonlinear problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Super-resolution with An Enhanced Group Convolutional Neural Network. (arXiv:2205.14548v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14548">
<div class="article-summary-box-inner">
<span><p>CNNs with strong learning abilities are widely chosen to resolve
super-resolution problem. However, CNNs depend on deeper network architectures
to improve performance of image super-resolution, which may increase
computational cost in general. In this paper, we present an enhanced
super-resolution group CNN (ESRGCNN) with a shallow architecture by fully
fusing deep and wide channel features to extract more accurate low-frequency
information in terms of correlations of different channels in single image
super-resolution (SISR). Also, a signal enhancement operation in the ESRGCNN is
useful to inherit more long-distance contextual information for resolving
long-term dependency. An adaptive up-sampling operation is gathered into a CNN
to obtain an image super-resolution model with low-resolution images of
different sizes. Extensive experiments report that our ESRGCNN surpasses the
state-of-the-arts in terms of SISR performance, complexity, execution speed,
image quality evaluation and visual effect in SISR. Code is found at
https://github.com/hellloxiaotian/ESRGCNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProxyMix: Proxy-based Mixup Training with Label Refinery for Source-Free Domain Adaptation. (arXiv:2205.14566v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14566">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Owing to privacy concerns
and heavy data transmission, source-free UDA, exploiting the pre-trained source
models instead of the raw source data for target learning, has been gaining
popularity in recent years. Some works attempt to recover unseen source domains
with generative models, however introducing additional network parameters.
Other works propose to fine-tune the source model by pseudo labels, while noisy
pseudo labels may misguide the decision boundary, leading to unsatisfied
results. To tackle these issues, we propose an effective method named
Proxy-based Mixup training with label refinery (ProxyMix). First of all, to
avoid additional parameters and explore the information in the source model,
ProxyMix defines the weights of the classifier as the class prototypes and then
constructs a class-balanced proxy source domain by the nearest neighbors of the
prototypes to bridge the unseen source domain and the target domain. To improve
the reliability of pseudo labels, we further propose the frequency-weighted
aggregation strategy to generate soft pseudo labels for unlabeled target data.
The proposed strategy exploits the internal structure of target features, pulls
target features to their semantic neighbors, and increases the weights of
low-frequency classes samples during gradient updating. With the proxy domain
and the reliable pseudo labels, we employ two kinds of mixup regularization,
i.e., inter- and intra-domain mixup, in our framework, to align the proxy and
the target domain, enforcing the consistency of predictions, thereby further
mitigating the negative impacts of noisy labels. Experiments on three 2D image
and one 3D point cloud object recognition benchmarks demonstrate that ProxyMix
yields state-of-the-art performance for source-free UDA tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ComplexGen: CAD Reconstruction by B-Rep Chain Complex Generation. (arXiv:2205.14573v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14573">
<div class="article-summary-box-inner">
<span><p>We view the reconstruction of CAD models in the boundary representation
(B-Rep) as the detection of geometric primitives of different orders, i.e.
vertices, edges and surface patches, and the correspondence of primitives,
which are holistically modeled as a chain complex, and show that by modeling
such comprehensive structures more complete and regularized reconstructions can
be achieved. We solve the complex generation problem in two steps. First, we
propose a novel neural framework that consists of a sparse CNN encoder for
input point cloud processing and a tri-path transformer decoder for generating
geometric primitives and their mutual relationships with estimated
probabilities. Second, given the probabilistic structure predicted by the
neural network, we recover a definite B-Rep chain complex by solving a global
optimization maximizing the likelihood under structural validness constraints
and applying geometric refinements. Extensive tests on large scale CAD datasets
demonstrate that the modeling of B-Rep chain complex structure enables more
accurate detection for learning and more constrained reconstruction for
optimization, leading to structurally more faithful and complete CAD B-Rep
models than previous results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-Aligned Video Raindrop Removal with Temporal Constraints. (arXiv:2205.14574v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14574">
<div class="article-summary-box-inner">
<span><p>Existing adherent raindrop removal methods focus on the detection of the
raindrop locations, and then use inpainting techniques or generative networks
to recover the background behind raindrops. Yet, as adherent raindrops are
diverse in sizes and appearances, the detection is challenging for both single
image and video. Moreover, unlike rain streaks, adherent raindrops tend to
cover the same area in several frames. Addressing these problems, our method
employs a two-stage video-based raindrop removal method. The first stage is the
single image module, which generates initial clean results. The second stage is
the multiple frame module, which further refines the initial results using
temporal constraints, namely, by utilizing multiple input frames in our process
and applying temporal consistency between adjacent output frames. Our single
image module employs a raindrop removal network to generate initial raindrop
removal results, and create a mask representing the differences between the
input and initial output. Once the masks and initial results for consecutive
frames are obtained, our multiple-frame module aligns the frames in both the
image and feature levels and then obtains the clean background. Our method
initially employs optical flow to align the frames, and then utilizes
deformable convolution layers further to achieve feature-level frame alignment.
To remove small raindrops and recover correct backgrounds, a target frame is
predicted from adjacent frames. A series of unsupervised losses are proposed so
that our second stage, which is the video raindrop removal module, can
self-learn from video data without ground truths. Experimental results on real
videos demonstrate the state-of-art performance of our method both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-C2FT: Coarse-to-fine Transformer for Multi-view 3D Reconstruction. (arXiv:2205.14575v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14575">
<div class="article-summary-box-inner">
<span><p>Recently, the transformer model has been successfully employed for the
multi-view 3D reconstruction problem. However, challenges remain on designing
an attention mechanism to explore the multiview features and exploit their
relations for reinforcing the encoding-decoding modules. This paper proposes a
new model, namely 3D coarse-to-fine transformer (3D-C2FT), by introducing a
novel coarse-to-fine(C2F) attention mechanism for encoding multi-view features
and rectifying defective 3D objects. C2F attention mechanism enables the model
to learn multi-view information flow and synthesize 3D surface correction in a
coarse to fine-grained manner. The proposed model is evaluated by ShapeNet and
Multi-view Real-life datasets. Experimental results show that 3D-C2FT achieves
notable results and outperforms several competing models on these datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards an unsupervised large-scale 2D and 3D building mapping with LiDAR. (arXiv:2205.14585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14585">
<div class="article-summary-box-inner">
<span><p>A 2D and 3D building map provides invaluable information for understanding
human activities and their impacts on Earth and its environment. Despite
enormous efforts to improve the quality of building maps, current large-scale
building maps have lots of errors and are limited to providing only 2D building
information. This study presents a state-of-the-art 2D and 3D building
extraction algorithm with airborne LiDAR data that is suitable for large-scale
building mapping. Our algorithm operates in a fully unsupervised manner and
does not require either any training label or training procedure. Our algorithm
requires only simple operations of morphological filtering and planarity-based
filtering but can produce an accurate 2D and 3D building map. A quantitative
and qualitative evaluation in a large-scale dataset (-550 sqkm) of Denver and
New York City showed that our algorithm outperforms the deep learning-based
Microsoft's building mapping algorithm even without any parameter tuning. More
extensive evaluations in different conditions of landscapes confirmed that our
algorithm is scalable and can be improved further with appropriate parameter
selection. Our algorithm is more advantageous than other image-based building
extraction algorithms in that it is more computationally efficient, more
accurate, and more explainable. Our proposed algorithm that can produce an
accurate large-scale 2D and 3D building map provides a great potential towards
a global-scale 2D and 3D building mapping with airborne LiDAR data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Distillation with Receptive Tokens. (arXiv:2205.14589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14589">
<div class="article-summary-box-inner">
<span><p>Distilling from the feature maps can be fairly effective for dense prediction
tasks since both the feature discriminability and localization priors can be
well transferred. However, not every pixel contributes equally to the
performance, and a good student should learn from what really matters to the
teacher. In this paper, we introduce a learnable embedding dubbed receptive
token to localize those pixels of interests (PoIs) in the feature map, with a
distillation mask generated via pixel-wise attention. Then the distillation
will be performed on the mask via pixel-wise reconstruction. In this way, a
distillation mask actually indicates a pattern of pixel dependencies within
feature maps of teacher. We thus adopt multiple receptive tokens to investigate
more sophisticated and informative pixel dependencies to further enhance the
distillation. To obtain a group of masks, the receptive tokens are learned via
the regular task loss but with teacher fixed, and we also leverage a Dice loss
to enrich the diversity of learned masks. Our method dubbed MasKD is simple and
practical, and needs no priors of tasks in application. Experiments show that
our MasKD can achieve state-of-the-art performance consistently on object
detection and semantic segmentation benchmarks. Code is available at:
https://github.com/hunto/MasKD .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiasEnsemble: Revisiting the Importance of Amplifying Bias for Debiasing. (arXiv:2205.14594v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14594">
<div class="article-summary-box-inner">
<span><p>In image classification, "debiasing" aims to train a classifier to be less
susceptible to dataset bias, the strong correlation between peripheral
attributes of data samples and a target class. For example, even if the frog
class in the dataset mainly consists of frog images with a swamp background
(i.e., bias-aligned samples), a debiased classifier should be able to correctly
classify a frog at a beach (i.e., bias-conflicting samples). Recent debiasing
approaches commonly use two components for debiasing, a biased model $f_B$ and
a debiased model $f_D$. $f_B$ is trained to focus on bias-aligned samples while
$f_D$ is mainly trained with bias-conflicting samples by concentrating on
samples which $f_B$ fails to learn, leading $f_D$ to be less susceptible to the
dataset bias. While the state-of-the-art debiasing techniques have aimed to
better train $f_D$, we focus on training $f_B$, an overlooked component until
now. Our empirical analysis reveals that removing the bias-conflicting samples
from the training set for $f_B$ is important for improving the debiasing
performance of $f_D$. This is due to the fact that the bias-conflicting samples
work as noisy samples for amplifying the bias for $f_B$. To this end, we
propose a novel biased sample selection method BiasEnsemble which removes the
bias-conflicting samples via leveraging additional biased models to construct a
bias-amplified dataset for training $f_B$. Our simple yet effective approach
can be directly applied to existing reweighting-based debiasing approaches,
obtaining consistent performance boost and achieving the state-of-the-art
performance on both synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A General Multiple Data Augmentation Based Framework for Training Deep Neural Networks. (arXiv:2205.14606v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14606">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) often rely on massive labelled data for training,
which is inaccessible in many applications. Data augmentation (DA) tackles data
scarcity by creating new labelled data from available ones. Different DA
methods have different mechanisms and therefore using their generated labelled
data for DNN training may help improving DNN's generalisation to different
degrees. Combining multiple DA methods, namely multi-DA, for DNN training,
provides a way to boost generalisation. Among existing multi-DA based DNN
training methods, those relying on knowledge distillation (KD) have received
great attention. They leverage knowledge transfer to utilise the labelled data
sets created by multiple DA methods instead of directly combining them for
training DNNs. However, existing KD-based methods can only utilise certain
types of DA methods, incapable of utilising the advantages of arbitrary DA
methods. We propose a general multi-DA based DNN training framework capable to
use arbitrary DA methods. To train a DNN, our framework replicates a certain
portion in the latter part of the DNN into multiple copies, leading to multiple
DNNs with shared blocks in their former parts and independent blocks in their
latter parts. Each of these DNNs is associated with a unique DA and a newly
devised loss that allows comprehensively learning from the data generated by
all DA methods and the outputs from all DNNs in an online and adaptive way. The
overall loss, i.e., the sum of each DNN's loss, is used for training the DNN.
Eventually, one of the DNNs with the best validation performance is chosen for
inference. We implement the proposed framework by using three distinct DA
methods and apply it for training representative DNNs. Experiments on the
popular benchmarks of image classification demonstrate the superiority of our
method to several existing single-DA and multi-DA based training methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IFRNet: Intermediate Feature Refine Network for Efficient Frame Interpolation. (arXiv:2205.14620v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14620">
<div class="article-summary-box-inner">
<span><p>Prevailing video frame interpolation algorithms, that generate the
intermediate frames from consecutive inputs, typically rely on complex model
architectures with heavy parameters or large delay, hindering them from diverse
real-time applications. In this work, we devise an efficient encoder-decoder
based network, termed IFRNet, for fast intermediate frame synthesizing. It
first extracts pyramid features from given inputs, and then refines the
bilateral intermediate flow fields together with a powerful intermediate
feature until generating the desired output. The gradually refined intermediate
feature can not only facilitate intermediate flow estimation, but also
compensate for contextual details, making IFRNet do not need additional
synthesis or refinement module. To fully release its potential, we further
propose a novel task-oriented optical flow distillation loss to focus on
learning the useful teacher knowledge towards frame synthesizing. Meanwhile, a
new geometry consistency regularization term is imposed on the gradually
refined intermediate features to keep better structure layout. Experiments on
various benchmarks demonstrate the excellent performance and fast inference
speed of proposed approaches. Code is available at
https://github.com/ltkong218/IFRNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SKFlow: Learning Optical Flow with Super Kernels. (arXiv:2205.14623v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14623">
<div class="article-summary-box-inner">
<span><p>Optical flow estimation is a classical yet challenging task in computer
vision. One of the essential factors in accurately predicting optical flow is
to alleviate occlusions between frames. However, it is still a thorny problem
for current top-performing optical flow estimation methods due to insufficient
local evidence to model occluded areas. In this paper, we propose Super Kernel
Flow Network (SKFlow), a CNN architecture to ameliorate the impacts of
occlusions on optical flow estimation. SKFlow benefits from the super kernels
which bring enlarged receptive fields to complement the absent matching
information and recover the occluded motions. We present efficient super kernel
designs by utilizing conical connections and hybrid depth-wise convolutions.
Extensive experiments demonstrate the effectiveness of SKFlow on multiple
benchmarks, especially in the occluded areas. Without pre-trained backbones on
ImageNet and with modest increase in computation, SKFlow achieves compelling
performance and ranks $\textbf{1st}$ among current published methods on Sintel
benchmark. On the challenging Sintel final pass test set, SKFlow attains the
average end-point error of $2.23$, which surpasses the best published result
$2.47$ by $9.72\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cervical Glandular Cell Detection from Whole Slide Image with Out-Of-Distribution Data. (arXiv:2205.14625v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14625">
<div class="article-summary-box-inner">
<span><p>Cervical glandular cell (GC) detection is a key step in computer-aided
diagnosis for cervical adenocarcinomas screening. It is challenging to
accurately recognize GCs in cervical smears in which squamous cells are the
major. Widely existing Out-Of-Distribution (OOD) data in the entire smear leads
decreasing reliability of machine learning system for GC detection. Although,
the State-Of-The-Art (SOTA) deep learning model can outperform pathologists in
preselected regions of interest, the mass False Positive (FP) prediction with
high probability is still unsolved when facing such gigapixel whole slide
image. This paper proposed a novel PolarNet based on the morphological prior
knowledge of GC trying to solve the FP problem via a self-attention mechanism
in eight-neighbor. It estimates the polar orientation of nucleus of GC. As a
plugin module, PolarNet can guide the deep feature and predicted confidence of
general object detection models. In experiments, we discovered that general
models based on four different frameworks can reject FP in small image set and
increase the mean of average precision (mAP) by $\text{0.007}\sim\text{0.015}$
in average, where the highest exceeds the recent cervical cell detection model
0.037. By plugging PolarNet, the deployed C++ program improved by 8.8\% on
accuracy of top-20 GC detection from external WSIs, while sacrificing 14.4 s of
computational time. Code is available in
\href{https://github.com/Chrisa142857/PolarNet-GCdet}{https://github.com/Chrisa142857/PolarNet-GCdet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superclass Adversarial Attack. (arXiv:2205.14629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14629">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks have only focused on changing the predictions of the
classifier, but their danger greatly depends on how the class is mistaken. For
example, when an automatic driving system mistakes a Persian cat for a Siamese
cat, it is hardly a problem. However, if it mistakes a cat for a 120km/h
minimum speed sign, serious problems can arise. As a stepping stone to more
threatening adversarial attacks, we consider the superclass adversarial attack,
which causes misclassification of not only fine classes, but also superclasses.
We conducted the first comprehensive analysis of superclass adversarial attacks
(an existing and 19 new methods) in terms of accuracy, speed, and stability,
and identified several strategies to achieve better performance. Although this
study is aimed at superclass misclassification, the findings can be applied to
other problem settings involving multiple classes, such as top-k and
multi-label classification attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceiving the Invisible: Proposal-Free Amodal Panoptic Segmentation. (arXiv:2205.14637v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14637">
<div class="article-summary-box-inner">
<span><p>Amodal panoptic segmentation aims to connect the perception of the world to
its cognitive understanding. It entails simultaneously predicting the semantic
labels of visible scene regions and the entire shape of traffic participant
instances, including regions that may be occluded. In this work, we formulate a
proposal-free framework that tackles this task as a multi-label and multi-class
problem by first assigning the amodal masks to different layers according to
their relative occlusion order and then employing amodal instance regression on
each layer independently while learning background semantics. We propose the
\net architecture that incorporates a shared backbone and an asymmetrical
dual-decoder consisting of several modules to facilitate within-scale and
cross-scale feature aggregations, bilateral feature propagation between
decoders, and integration of global instance-level and local pixel-level
occlusion reasoning. Further, we propose the amodal mask refiner that resolves
the ambiguity in complex occlusion scenarios by explicitly leveraging the
embedding of unoccluded instance masks. Extensive evaluation on the BDD100K-APS
and KITTI-360-APS datasets demonstrate that our approach set the new
state-of-the-art on both benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Micro-Expression Recognition Based on Attribute Information Embedding and Cross-modal Contrastive Learning. (arXiv:2205.14643v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14643">
<div class="article-summary-box-inner">
<span><p>Facial micro-expressions recognition has attracted much attention recently.
Micro-expressions have the characteristics of short duration and low intensity,
and it is difficult to train a high-performance classifier with the limited
number of existing micro-expressions. Therefore, recognizing micro-expressions
is a challenge task. In this paper, we propose a micro-expression recognition
method based on attribute information embedding and cross-modal contrastive
learning. We use 3D CNN to extract RGB features and FLOW features of
micro-expression sequences and fuse them, and use BERT network to extract text
information in Facial Action Coding System. Through cross-modal contrastive
loss, we embed attribute information in the visual network, thereby improving
the representation ability of micro-expression recognition in the case of
limited samples. We conduct extensive experiments in CASME II and MMEW
databases, and the accuracy is 77.82% and 71.04%, respectively. The comparative
experiments show that this method has better recognition effect than other
methods for micro-expression recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COFS: Controllable Furniture layout Synthesis. (arXiv:2205.14657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14657">
<div class="article-summary-box-inner">
<span><p>Scalable generation of furniture layouts is essential for many applications
in virtual reality, augmented reality, game development and synthetic data
generation. Many existing methods tackle this problem as a sequence generation
problem which imposes a specific ordering on the elements of the layout making
such methods impractical for interactive editing or scene completion.
Additionally, most methods focus on generating layouts unconditionally and
offer minimal control over the generated layouts. We propose COFS, an
architecture based on standard transformer architecture blocks from language
modeling. The proposed model is invariant to object order by design, removing
the unnatural requirement of specifying an object generation order.
Furthermore, the model allows for user interaction at multiple levels enabling
fine grained control over the generation process. Our model consistently
outperforms other methods which we verify by performing quantitative
evaluations. Our method is also faster to train and sample from, compared to
existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Glance to Count: Learning to Rank with Anchors for Weakly-supervised Crowd Counting. (arXiv:2205.14659v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14659">
<div class="article-summary-box-inner">
<span><p>Crowd image is arguably one of the most laborious data to annotate. In this
paper, we devote to reduce the massive demand of densely labeled crowd data,
and propose a novel weakly-supervised setting, in which we leverage the binary
ranking of two images with high-contrast crowd counts as training guidance. To
enable training under this new setting, we convert the crowd count regression
problem to a ranking potential prediction problem. In particular, we tailor a
Siamese Ranking Network that predicts the potential scores of two images
indicating the ordering of the counts. Hence, the ultimate goal is to assign
appropriate potentials for all the crowd images to ensure their orderings obey
the ranking labels. On the other hand, potentials reveal the relative crowd
sizes but cannot yield an exact crowd count. We resolve this problem by
introducing "anchors" during the inference stage. Concretely, anchors are a few
images with count labels used for referencing the corresponding counts from
potential scores by a simple linear mapping function. We conduct extensive
experiments to study various combinations of supervision, and we show that the
proposed method outperforms existing weakly-supervised methods without
additional labeling effort by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency Map Based Data Augmentation. (arXiv:2205.14686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14686">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a commonly applied technique with two seemingly related
advantages. With this method one can increase the size of the training set
generating new samples and also increase the invariance of the network against
the applied transformations. Unfortunately all images contain both relevant and
irrelevant features for classification therefore this invariance has to be
class specific. In this paper we will present a new method which uses saliency
maps to restrict the invariance of neural networks to certain regions,
providing higher test accuracy in classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition. (arXiv:2205.14756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14756">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) has achieved remarkable performance in many vision
tasks. However, ViT is inferior to convolutional neural networks (CNNs) when
targeting high-resolution mobile vision applications. The key computational
bottleneck of ViT is the softmax attention module which has quadratic
computational complexity with the input resolution. It is essential to reduce
the cost of ViT to deploy it on edge devices. Existing methods (e.g., Swin,
PVT) restrict the softmax attention within local windows or reduce the
resolution of key/value tensors to reduce the cost, which sacrifices ViT's core
advantages on global feature extractions. In this work, we present
EfficientViT, an efficient ViT architecture for high-resolution low-computation
visual recognition. Instead of restricting the softmax attention, we propose to
replace softmax attention with linear attention while enhancing its local
feature extraction ability with depthwise convolution. EfficientViT maintains
global and local feature extraction capability while enjoying linear
computational complexity. Extensive experiments on COCO object detection and
Cityscapes semantic segmentation demonstrate the effectiveness of our method.
On the COCO dataset, EfficientViT achieves 42.6 AP with 4.4G MACs, surpassing
EfficientDet-D1 by 2.4 AP while having 27.9% fewer MACs. On Cityscapes,
EfficientViT reaches 78.7 mIoU with 19.1G MACs, outperforming SegFormer by 2.5
mIoU while requiring less than 1/3 the computational cost. On Qualcomm
Snapdragon 855 CPU, EfficientViT is 3x faster than EfficientNet while achieving
higher ImageNet accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Uncertainty Quantification for Segmentation with Multiple Annotations. (arXiv:1907.01949v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.01949">
<div class="article-summary-box-inner">
<span><p>The accurate estimation of predictive uncertainty carries importance in
medical scenarios such as lung node segmentation. Unfortunately, most existing
works on predictive uncertainty do not return calibrated uncertainty estimates,
which could be used in practice. In this work we exploit multi-grader
annotation variability as a source of 'groundtruth' aleatoric uncertainty,
which can be treated as a target in a supervised learning problem. We combine
this groundtruth uncertainty with a Probabilistic U-Net and test on the
LIDC-IDRI lung nodule CT dataset and MICCAI2012 prostate MRI dataset. We find
that we are able to improve predictive uncertainty estimates. We also find that
we can improve sample accuracy and sample diversity. In real-world
applications, our method could inform doctors about the confidence of the
segmentation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovery of Future Data via Convolution Nuclear Norm Minimization. (arXiv:1909.03889v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.03889">
<div class="article-summary-box-inner">
<span><p>This paper studies the problem of time series forecasting (TSF) from the
perspective of compressed sensing. First of all, we convert TSF into a more
inclusive problem called tensor completion with arbitrary sampling (TCAS),
which is to restore a tensor from a subset of its entries sampled in an
arbitrary manner. While it is known that, in the framework of Tucker
low-rankness, it is theoretically impossible to identify the target tensor
based on some arbitrarily selected entries, in this work we shall show that
TCAS is indeed tackleable in the light of a new concept called convolutional
low-rankness, which is a generalization of the well-known Fourier sparsity.
Then we introduce a convex program termed Convolution Nuclear Norm Minimization
(CNNM), and we prove that CNNM succeeds in solving TCAS as long as a sampling
condition--which depends on the convolution rank of the target tensor--is
obeyed. This theory provides a meaningful answer to the fundamental question of
what is the minimum sampling size needed for making a given number of
forecasts. Experiments on univariate time series, images and videos show
encouraging results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L6DNet: Light 6 DoF Network for Robust and Precise Object Pose Estimation with Small Datasets. (arXiv:2002.00911v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.00911">
<div class="article-summary-box-inner">
<span><p>Estimating the 3D pose of an object is a challenging task that can be
considered within augmented reality or robotic applications. In this paper, we
propose a novel approach to perform 6 DoF object pose estimation from a single
RGB-D image. We adopt a hybrid pipeline in two stages: data-driven and
geometric respectively. The data-driven step consists of a classification CNN
to estimate the object 2D location in the image from local patches, followed by
a regression CNN trained to predict the 3D location of a set of keypoints in
the camera coordinate system. To extract the pose information, the geometric
step consists in aligning the 3D points in the camera coordinate system with
the corresponding 3D points in world coordinate system by minimizing a
registration error, thus computing the pose. Our experiments on the standard
dataset LineMod show that our approach is more robust and accurate than
state-of-the-art methods. The approach is also validated to achieve a 6 DoF
positioning task by visual servoing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bayesian approach to tissue-fraction estimation for oncological PET segmentation. (arXiv:2003.00317v3 [physics.med-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00317">
<div class="article-summary-box-inner">
<span><p>Tumor segmentation in oncological PET is challenging, a major reason being
the partial-volume effects that arise due to low system resolution and finite
voxel size. The latter results in tissue-fraction effects, i.e. voxels contain
a mixture of tissue classes. Conventional segmentation methods are typically
designed to assign each voxel in the image as belonging to a certain tissue
class. Thus, these methods are inherently limited in modeling tissue-fraction
effects. To address the challenge of accounting for partial-volume effects, and
in particular, tissue-fraction effects, we propose a Bayesian approach to
tissue-fraction estimation for oncological PET segmentation. Specifically, this
Bayesian approach estimates the posterior mean of fractional volume that the
tumor occupies within each voxel of the image. The proposed method, implemented
using a deep-learning-based technique, was first evaluated using clinically
realistic 2-D simulation studies with known ground truth, in the context of
segmenting the primary tumor in PET images of patients with lung cancer. The
evaluation studies demonstrated that the method accurately estimated the
tumor-fraction areas and significantly outperformed widely used conventional
PET segmentation methods, including a U-net-based method, on the task of
segmenting the tumor. In addition, the proposed method was relatively
insensitive to partial-volume effects and yielded reliable tumor segmentation
for different clinical-scanner configurations. The method was then evaluated
using clinical images of patients with stage IIB/III non-small cell lung cancer
from ACRIN 6668/RTOG 0235 multi-center clinical trial. Here, the results showed
that the proposed method significantly outperformed all other considered
methods and yielded accurate tumor segmentation on patient images with Dice
similarity coefficient (DSC) of 0.82 (95 % CI: [0.78, 0.86]).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedBoosting: Federated Learning with Gradient Protected Boosting for Text Recognition. (arXiv:2007.07296v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.07296">
<div class="article-summary-box-inner">
<span><p>Typical machine learning approaches require centralized data for model
training, which may not be possible where restrictions on data sharing are in
place due to, for instance, privacy and gradient protection. The recently
proposed Federated Learning (FL) framework allows learning a shared model
collaboratively without data being centralized or shared among data owners.
However, we show in this paper that the generalization ability of the joint
model is poor on Non-Independent and Non-Identically Distributed (Non-IID)
data, particularly when the Federated Averaging (FedAvg) strategy is used due
to the weight divergence phenomenon. Hence, we propose a novel boosting
algorithm for FL to address both the generalization and gradient leakage
issues, as well as achieve faster convergence in gradient-based optimization.
In addition, a secure gradient sharing protocol using Homomorphic Encryption
(HE) and Differential Privacy (DP) is introduced to defend against gradient
leakage attack and avoid pairwise encryption that is not scalable. We
demonstrate the proposed Federated Boosting (FedBoosting) method achieves
noticeable improvements in both prediction accuracy and run-time efficiency in
a visual text recognition task on public benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation. (arXiv:2008.05440v4 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05440">
<div class="article-summary-box-inner">
<span><p>D shape generation is a fundamental operation in computer graphics. While
significant progress has been made, especially with recent deep generative
models, it remains a challenge to synthesize high-quality shapes with rich
geometric details and complex structure, in a controllable manner. To tackle
this, we introduce DSG-Net, a deep neural network that learns a disentangled
structured and geometric mesh representation for 3D shapes, where two key
aspects of shapes, geometry, and structure, are encoded in a synergistic manner
to ensure plausibility of the generated shapes, while also being disentangled
as much as possible. This supports a range of novel shape generation
applications with disentangled control, such as interpolation of structure
(geometry) while keeping geometry (structure) unchanged. To achieve this, we
simultaneously learn structure and geometry through variational autoencoders
(VAEs) in a hierarchical manner for both, with bijective mappings at each
level. In this manner, we effectively encode geometry and structure in separate
latent spaces, while ensuring their compatibility: the structure is used to
guide the geometry and vice versa. At the leaf level, the part geometry is
represented using a conditional part VAE, to encode high-quality geometric
details, guided by the structure context as the condition. Our method not only
supports controllable generation applications but also produces high-quality
synthesized shapes, outperforming state-of-the-art methods. The code has been
released at https://github.com/IGLICT/DSG-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Few-Shot Generalization with Attributes. (arXiv:2012.05895v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05895">
<div class="article-summary-box-inner">
<span><p>Despite impressive progress in deep learning, generalizing far beyond the
training distribution is an important open challenge. In this work, we consider
few-shot classification, and aim to shed light on what makes some novel classes
easier to learn than others, and what types of learned representations
generalize better. To this end, we define a new paradigm in terms of attributes
-- simple building blocks of which concepts are formed -- as a means of
quantifying the degree of relatedness of different concepts. Our empirical
analysis reveals that supervised learning generalizes poorly to new attributes,
but a combination of self-supervised pretraining with supervised finetuning
leads to stronger generalization. The benefit of self-supervised pretraining
and supervised finetuning is further investigated through controlled
experiments using random splits of the attribute space, and we find that
predictability of test attributes provides an informative estimate of a model's
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Momentum-Contrastive Pre-Training. (arXiv:2012.13154v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13154">
<div class="article-summary-box-inner">
<span><p>Recently proposed adversarial self-supervised learning methods usually
require big batches and long training epochs to extract robust features, which
will bring heavy computational overhead on platforms with limited resources. In
order to help the network learn more powerful feature representations in
smaller batches and fewer epochs, this paper proposes a novel adversarial
momentum contrastive learning method, which introduces two memory banks
corresponding to clean samples and adversarial samples, respectively. These
memory banks can be dynamically incorporated into the training process to track
invariant features among historical mini-batches. Compared with the previous
adversarial pre-training model, our method achieves superior performance with
smaller batch size and less training epochs. In addition, the model outperforms
some state-of-the-art supervised defensive methods on multiple benchmark
datasets after being fine-tuned on downstream classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-point dimensionality reduction to improve projection layout reliability. (arXiv:2101.06224v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06224">
<div class="article-summary-box-inner">
<span><p>In ordinary Dimensionality Reduction (DR), each data instance in a high
dimensional space (original space), or on a distance matrix denoting original
space distances, is mapped to (projected onto) one point in a low dimensional
space (visual space), building a layout of projected points trying to preserve
as much as possible some property of data such as distances, neighbourhood
relationships, and/or topology structures, with the ultimate goal of
approximating semantic properties of data with preserved geometric properties
or topology structures in visual space. In this paper, the concept of
Multi-point Dimensionality Reduction is elaborated on where each data instance
can be mapped to (projected onto) possibly more than one point in visual space
by providing the first general solution (algorithm) for it as a move in the
direction of improving reliablity, usability and interpretability of
dimensionality reduction. Furthermore by allowing the points in visual space to
be split into two layers while maintaining the possibility of having more than
one projection (mapping) per data instance , the benefit of separating more
reliable points from less reliable points is dicussed notwithstanding the
effort to improve less reliable points. The proposed solution (algorithm) in
this paper, named Layered Vertex Splitting Data Embedding (LVSDE), is built
upon and extends a combination of ordinary DR and graph drawing techniques. On
the empirical side, this paper shows that the particular proposed algorithm
(LVSDE) practically outperforms popular ordinary DR methods visually
(semantics, group separation, subgroup detection or combinational group
detection) in a way that is easily explainable and performs in close proximity
to top on most of the data sets studied in the paper in a quantitative analysis
based on KNN classification accuracy. [Abstract truncated for length]
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DivSwapper: Towards Diversified Patch-based Arbitrary Style Transfer. (arXiv:2101.06381v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06381">
<div class="article-summary-box-inner">
<span><p>Gram-based and patch-based approaches are two important research lines of
style transfer. Recent diversified Gram-based methods have been able to produce
multiple and diverse stylized outputs for the same content and style images.
However, as another widespread research interest, the diversity of patch-based
methods remains challenging due to the stereotyped style swapping process based
on nearest patch matching. To resolve this dilemma, in this paper, we dive into
the crux of existing patch-based methods and propose a universal and efficient
module, termed DivSwapper, for diversified patch-based arbitrary style
transfer. The key insight is to use an essential intuition that neural patches
with higher activation values could contribute more to diversity. Our
DivSwapper is plug-and-play and can be easily integrated into existing
patch-based and Gram-based methods to generate diverse results for arbitrary
styles. We conduct theoretical analyses and extensive experiments to
demonstrate the effectiveness of our method, and compared with state-of-the-art
algorithms, it shows superiority in diversity, quality, and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepRA: Predicting Joint Damage From Radiographs Using CNN with Attention. (arXiv:2102.06982v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06982">
<div class="article-summary-box-inner">
<span><p>Joint damage in Rheumatoid Arthritis (RA) is assessed by manually inspecting
and grading radiographs of hands and feet. This is a tedious task which
requires trained experts whose subjective assessment leads to low inter-rater
agreement. An algorithm which can automatically predict the joint level damage
in hands and feet can help optimize this process, which will eventually aid the
doctors in better patient care and research. In this paper, we propose a
two-staged approach which amalgamates object detection and convolution neural
networks with attention which can efficiently and accurately predict the
overall and joint level narrowing and erosion from patients radiographs. This
approach has been evaluated on hands and feet radiographs of patients suffering
from RA and has achieved a weighted root mean squared error (RMSE) of 1.358 and
1.404 in predicting joint level narrowing and erosion Sharp van der Heijde
(SvH) scores which is 31% and 19% improvement with respect to the baseline SvH
scores, respectively. The proposed approach achieved a weighted absolute error
of 1.456 in predicting the overall damage in hands and feet radiographs for the
patients which is a 79% improvement as compared to the baseline. Our method
also provides an inherent capability to provide explanations for model
predictions using attention weights, which is essential given the black box
nature of deep learning models. The proposed approach was developed during the
RA2 Dream Challenge hosted by Dream Challenges and secured 4th and 8th position
in predicting overall and joint level narrowing and erosion SvH scores from
radiographs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Image Generation by Conditioning Variational Auto-Encoders. (arXiv:2102.12037v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12037">
<div class="article-summary-box-inner">
<span><p>We present a conditional variational auto-encoder (VAE) which, to avoid the
substantial cost of training from scratch, uses an architecture and training
objective capable of leveraging a foundation model in the form of a pretrained
unconditional VAE. To train the conditional VAE, we only need to train an
artifact to perform amortized inference over the unconditional VAE's latent
variables given a conditioning input. We demonstrate our approach on tasks
including image inpainting, for which it outperforms state-of-the-art GAN-based
approaches at faithfully representing the inherent uncertainty. We conclude by
describing a possible application of our inpainting model, in which it is used
to perform Bayesian experimental design for the purpose of guiding a sensor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09124">
<div class="article-summary-box-inner">
<span><p>While self-supervised representation learning (SSL) has received widespread
attention from the community, recent research argue that its performance will
suffer a cliff fall when the model size decreases. The current method mainly
relies on contrastive learning to train the network and in this work, we
propose a simple yet effective Distilled Contrastive Learning (DisCo) to ease
the issue by a large margin. Specifically, we find the final embedding obtained
by the mainstream SSL methods contains the most fruitful information, and
propose to distill the final embedding to maximally transmit a teacher's
knowledge to a lightweight model by constraining the last embedding of the
student to be consistent with that of the teacher. In addition, in the
experiment, we find that there exists a phenomenon termed Distilling BottleNeck
and present to enlarge the embedding dimension to alleviate this problem. Our
method does not introduce any extra parameter to lightweight models during
deployment. Experimental results demonstrate that our method achieves the
state-of-the-art on all lightweight models. Particularly, when
ResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear
result of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,
but the number of parameters of EfficientNet-B0 is only 9.4\%/16.3\% of
ResNet-101/ResNet-50. Code is available at https://github.
com/Yuting-Gao/DisCo-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00689">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel approach to map-based navigation system for
unmanned aircraft. The proposed system attempts label-to-label matching, not
image-to-image matching, between aerial images and a map database. The ground
objects can be labelled by deep learning approaches and the configuration of
the objects is used to find the corresponding location in the map database. The
use of the deep learning technique as a tool for extracting high-level features
reduces the image-based localization problem to a pattern matching problem.
This paper proposes a pattern matching algorithm that does not require altitude
information or a camera model to estimate the absolute horizontal position. The
feasibility analysis with simulated images shows the proposed map-based
navigation can be realized with the proposed pattern matching algorithm and it
is able to provide positions given the labelled objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11845">
<div class="article-summary-box-inner">
<span><p>In this paper, we are concerned with image classification with deep
convolutional neural networks (CNNs). We focus on the following question: given
a set of candidate CNN models, how to select the right one with the best
generalization property for the current task? Current model selection methods
all require access to a batch of labeled data for computing a pre-specified
performance metric, such as the cross-entropy loss, the classification error
rate and the negative log-likelihood. In many practical cases, labels are not
available in time as labeling itself is a time-consuming and expensive task. To
this end, we propose an approach to CNN model selection using only unlabeled
data. We develop this method based on a principle termed consistent relative
confidence. Experimental results on benchmark datasets demonstrate the
effectiveness and efficiency of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite Relaxations and Scalable Global Optimization. (arXiv:2109.03349v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03349">
<div class="article-summary-box-inner">
<span><p>We propose the first general and scalable framework to design certifiable
algorithms for robust geometric perception in the presence of outliers. Our
first contribution is to show that estimation using common robust costs, such
as truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey's
biweight, among others, can be reformulated as polynomial optimization problems
(POPs). By focusing on the TLS cost, our second contribution is to exploit
sparsity in the POP and propose a sparse semidefinite programming (SDP)
relaxation that is much smaller than the standard Lasserre's hierarchy while
preserving empirical exactness, i.e., the SDP recovers the optimizer of the
nonconvex POP with an optimality certificate. Our third contribution is to
solve the SDP relaxations at an unprecedented scale and accuracy by presenting
STRIDE, a solver that blends global descent on the convex SDP with fast local
search on the nonconvex POP. Our fourth contribution is an evaluation of the
proposed framework on six geometric perception problems including single and
multiple rotation averaging, point cloud and mesh registration, absolute pose
estimation, and category-level object pose and shape estimation. Our
experiments demonstrate that (i) our sparse SDP relaxation is empirically exact
with up to 60%-90% outliers across applications; (ii) while still being far
from real-time, STRIDE is up to 100 times faster than existing SDP solvers on
medium-scale problems, and is the only solver that can solve large-scale SDPs
with hundreds of thousands of constraints to high accuracy; (iii) STRIDE
safeguards existing fast heuristics for robust estimation (e.g., RANSAC or
Graduated Non-Convexity), i.e., it certifies global optimality if the heuristic
estimates are optimal, or detects and allows escaping local optima when the
heuristic estimates are suboptimal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?. (arXiv:2109.05422v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05422">
<div class="article-summary-box-inner">
<span><p>Transformers have sprung up in the field of computer vision. In this work, we
explore whether the core self-attention module in Transformer is the key to
achieving excellent performance in image recognition. To this end, we build an
attention-free network called sMLPNet based on the existing MLP-based vision
models. Specifically, we replace the MLP module in the token-mixing step with a
novel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along
the axial directions and the parameters are shared among rows or columns. By
sparse connection and weight sharing, sMLP module significantly reduces the
number of model parameters and computational complexity, avoiding the common
over-fitting problem that plagues the performance of MLP-like models. When only
trained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1
accuracy with only 24M parameters, which is much better than most CNNs and
vision Transformers under the same model size constraint. When scaling up to
66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the
state-of-the-art Swin Transformer. The success of sMLPNet suggests that the
self-attention mechanism is not necessarily a silver bullet in computer vision.
The code and models are publicly available at
https://github.com/microsoft/SPACH
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Unsupervised Learning of Visual Representations and Categories. (arXiv:2109.05675v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05675">
<div class="article-summary-box-inner">
<span><p>Real world learning scenarios involve a nonstationary distribution of classes
with sequential dependencies among the samples, in contrast to the standard
machine learning formulation of drawing samples independently from a fixed,
typically uniform distribution. Furthermore, real world interactions demand
learning on-the-fly from few or no class labels. In this work, we propose an
unsupervised model that simultaneously performs online visual representation
learning and few-shot learning of new categories without relying on any class
labels. Our model is a prototype-based memory network with a control component
that determines when to form a new class prototype. We formulate it as an
online mixture model, where components are created with only a single new
example, and assignments do not have to be balanced, which permits an
approximation to natural imbalanced distributions from uncurated raw data.
Learning includes a contrastive loss that encourages different views of the
same image to be assigned to the same prototype. The result is a mechanism that
forms categorical representations of objects in nonstationary environments.
Experiments show that our method can learn from an online stream of visual
input data and its learned representations are significantly better at category
recognition compared to state-of-the-art self-supervised learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection. (arXiv:2109.06148v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06148">
<div class="article-summary-box-inner">
<span><p>We present DAFNe, a Dense one-stage Anchor-Free deep Network for oriented
object detection. As a one-stage model, it performs bounding box predictions on
a dense grid over the input image, being architecturally simpler in design, as
well as easier to optimize than its two-stage counterparts. Furthermore, as an
anchor-free model, it reduces the prediction complexity by refraining from
employing bounding box anchors. With DAFNe we introduce an orientation-aware
generalization of the center-ness function for arbitrarily oriented bounding
boxes to down-weight low-quality predictions and a center-to-corner bounding
box prediction strategy that improves object localization performance. Our
experiments show that DAFNe outperforms all previous one-stage anchor-free
models on DOTA 1.0, DOTA 1.5, and UCAS-AOD and is on par with the best models
on HRSC2016.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Dense Video Grounding via Parallel Regression. (arXiv:2109.11265v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11265">
<div class="article-summary-box-inner">
<span><p>Video grounding aims to localize the corresponding video moment in an
untrimmed video given a language query. Existing methods often address this
task in an indirect way, by casting it as a proposal-and-match or
fusion-and-detection problem. Solving these surrogate problems often requires
sophisticated label assignment during training and hand-crafted removal of
near-duplicate results. Meanwhile, existing works typically focus on sparse
video grounding with a single sentence as input, which could result in
ambiguous localization due to its unclear description. In this paper, we tackle
a new problem of dense video grounding, by simultaneously localizing multiple
moments with a paragraph as input. From a perspective on video grounding as
language conditioned regression, we present an end-to-end parallel decoding
paradigm by re-purposing a Transformer-alike architecture (PRVG). The key
design in our PRVG is to use languages as queries, and directly regress the
moment boundaries based on language-modulated visual representations. Thanks to
its simplicity in design, our PRVG framework can be applied in different
testing schemes (sparse or dense grounding) and allows for efficient inference
without any post-processing technique. In addition, we devise a robust
proposal-level attention loss to guide the training of PRVG, which is invariant
to moment duration and contributes to model convergence. We perform experiments
on two video grounding benchmarks of ActivityNet Captions and TACoS,
demonstrating that our PRVG can significantly outperform previous methods. We
also perform in-depth studies to investigate the effectiveness of parallel
regression paradigm on video grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Camera Human Motion Transfer by Time Series Analysis. (arXiv:2109.14174v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14174">
<div class="article-summary-box-inner">
<span><p>Along with advances in optical sensors is the increasingly common practice of
building an imaging system with heterogeneous cameras. While high-resolution
(HR) video acquisition and analysis benefit from hybrid sensors, the intrinsic
characteristics of multiple cameras lead to a challenging motion transfer
problem. In this paper, we propose an algorithm using time series analysis for
motion transfer among multiple cameras. Specifically, we first identify
seasonality in the motion data, and then build an additive time series model to
extract patterns that could be transferred across different cameras. Our
approach has a complete and clear mathematical formulation, and the algorithm
is also efficient and interpretable. Through the experiment on real-world data,
we demonstrate the effectiveness of our method. Furthermore, our motion
transfer algorithm could combine with and facilitate downstream tasks, e.g.,
enhancing pose estimation on low-resolution (LR) videos with inherent patterns
extracted from HR ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Sharpness-aware Minimization for Improved Training of Neural Networks. (arXiv:2110.03141v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03141">
<div class="article-summary-box-inner">
<span><p>Overparametrized Deep Neural Networks (DNNs) often achieve astounding
performances, but may potentially result in severe generalization error.
Recently, the relation between the sharpness of the loss landscape and the
generalization error has been established by Foret et al. (2020), in which the
Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the
generalization. Unfortunately, SAM s computational cost is roughly double that
of base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus
proposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM s
efficiency at no cost to its generalization performance. ESAM includes two
novel and efficient training strategies-StochasticWeight Perturbation and
Sharpness-Sensitive Data Selection. In the former, the sharpness measure is
approximated by perturbing a stochastically chosen set of weights in each
iteration; in the latter, the SAM loss is optimized using only a judiciously
selected subset of data that is sensitive to the sharpness. We provide
theoretical explanations as to why these strategies perform well. We also show,
via extensive experiments on the CIFAR and ImageNet datasets, that ESAM
enhances the efficiency over SAM from requiring 100% extra computations to 40%
vis-a-vis base optimizers, while test accuracies are preserved or even
improved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training Molecular Graph Representation with 3D Geometry. (arXiv:2110.07728v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07728">
<div class="article-summary-box-inner">
<span><p>Molecular graph representation learning is a fundamental problem in modern
drug and material discovery. Molecular graphs are typically modeled by their 2D
topological structures, but it has been recently discovered that 3D geometric
information plays a more vital role in predicting molecular functionalities.
However, the lack of 3D information in real-world scenarios has significantly
impeded the learning of geometric graph representation. To cope with this
challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework
where self-supervised learning (SSL) is performed by leveraging the
correspondence and consistency between 2D topological structures and 3D
geometric views. GraphMVP effectively learns a 2D molecular graph encoder that
is enhanced by richer and more discriminative 3D geometry. We further provide
theoretical insights to justify the effectiveness of GraphMVP. Finally,
comprehensive experiments show that GraphMVP can consistently outperform
existing graph SSL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-free unsupervised domain adaptation for cross-modality abdominal multi-organ segmentation. (arXiv:2111.12221v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12221">
<div class="article-summary-box-inner">
<span><p>Domain adaptation is crucial for transferring the knowledge from the source
labeled CT dataset to the target unlabeled MR dataset in abdominal multi-organ
segmentation. Meanwhile, it is highly desirable to avoid the high annotation
cost related to the target dataset and protect the source dataset privacy.
Therefore, we propose an effective source-free unsupervised domain adaptation
method for cross-modality abdominal multi-organ segmentation without source
dataset access. The proposed framework comprises two stages. In the first
stage, the feature map statistics-guided model adaptation combined with entropy
minimization is developed to help the top segmentation network reliably segment
the target images. The pseudo-labels output from the top segmentation network
are used to guide the style compensation network to generate source-like
images. The pseudo-labels output from the middle segmentation network is used
to supervise the learning progress of the desired model (bottom segmentation
network). In the second stage, the circular learning and pixel-adaptive mask
refinement are used to further improve the desired model performance. With this
approach, we achieved satisfactory abdominal multi-organ segmentation
performance, outperforming the existing state-of-the-art domain adaptation
methods. The proposed approach can be easily extended to situations in which
target annotation data exist. With only one labeled MR volume, the performance
can be levelled with that of supervised learning. Furthermore, the proposed
approach is proven to be effective for source-free unsupervised domain
adaptation in reverse direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hallucinated Neural Radiance Fields in the Wild. (arXiv:2111.15246v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15246">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) has recently gained popularity for its
impressive novel view synthesis ability. This paper studies the problem of
hallucinated NeRF: i.e., recovering a realistic NeRF at a different time of day
from a group of tourism images. Existing solutions adopt NeRF with a
controllable appearance embedding to render novel views under various
conditions, but they cannot render view-consistent images with an unseen
appearance. To solve this problem, we present an end-to-end framework for
constructing a hallucinated NeRF, dubbed as Ha-NeRF. Specifically, we propose
an appearance hallucination module to handle time-varying appearances and
transfer them to novel views. Considering the complex occlusions of tourism
images, we introduce an anti-occlusion module to decompose the static subjects
for visibility accurately. Experimental results on synthetic data and real
tourism photo collections demonstrate that our method can hallucinate the
desired appearances and render occlusion-free images from different views. The
project and supplementary materials are available at
https://rover-xingyu.github.io/Ha-NeRF/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trimap-guided Feature Mining and Fusion Network for Natural Image Matting. (arXiv:2112.00510v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00510">
<div class="article-summary-box-inner">
<span><p>Utilizing trimap guidance and fusing multi-level features are two important
issues for trimap-based matting with pixel-level prediction. To utilize trimap
guidance, most existing approaches simply concatenate trimaps and images
together to feed a deep network or apply an extra network to extract more
trimap guidance, which meets the conflict between efficiency and effectiveness.
For emerging content-based feature fusion, most existing matting methods only
focus on local features which lack the guidance of a global feature with strong
semantic information related to the interesting object. In this paper, we
propose a trimap-guided feature mining and fusion network consisting of our
trimap-guided non-background multi-scale pooling (TMP) module and global-local
context-aware fusion (GLF) modules. Considering that trimap provides strong
semantic guidance, our TMP module focuses effective feature mining on
interesting objects under the guidance of trimap without extra parameters.
Furthermore, our GLF modules use global semantic information of interesting
objects mined by our TMP module to guide an effective global-local
context-aware multi-level feature fusion. In addition, we build a common
interesting object matting (CIOM) dataset to advance high-quality image
matting. Particularly, results on the Composition-1k and our CIOM show that our
TMFNet achieves 13% and 25% relative improvement on SAD, respectively, against
a strong baseline with fewer parameters and 14% fewer FLOPs. Experimental
results on the Composition-1k test set, Alphamatting benchmark, and our CIOM
test set demonstrate that our method outperforms state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Retinex Fusion for Adaptive Infrared and Visible Image Super-resolution Fusion. (arXiv:2112.02869v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02869">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks have turned into an illustrious tool for image
fusion and super-resolution. However, their excellent performance cannot work
without large fixed-paired datasets; and additionally, these high-demanded
ground truth data always cannot be obtained easily in fusion tasks. In this
study, we show that, the structures of generative networks capture a great deal
of image feature priors, and then these priors are sufficient to reconstruct
high-quality fused super-resolution result using only low-resolution inputs. By
this way, we propose a novel self-supervised dataset-free method for adaptive
infrared (IR) and visible (VIS) image super-resolution fusion named Deep
Retinex Fusion (DRF). The key idea of DRF is first generating component priors
which are disentangled from physical model using our designed generative
networks ZipperNet, LightingNet and AdjustingNet, then combining these priors
which captured by networks via adaptive fusion loss functions based on Retinex
theory, and finally reconstructing the super-resolution fusion results.
Furthermore, in order to verify the effectiveness of our reported DRF, both
qualitative and quantitative experiments via comparing with other
state-of-the-art methods are performed using different test sets. These results
prove that, comparing with large datasets trained methods, DRF which works
without any dataset achieves the best super-resolution fusion performance; and
more importantly, DRF can adaptively balance IR and VIS information and has
good noise immunity. DRF codes are open source available at
https://github.com/GuYuanjie/Deep-Retinex-fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M-FasterSeg: An Efficient Semantic Segmentation Network Based on Neural Architecture Search. (arXiv:2112.07918v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07918">
<div class="article-summary-box-inner">
<span><p>Image semantic segmentation technology is one of the key technologies for
intelligent systems to understand natural scenes. As one of the important
research directions in the field of visual intelligence, this technology has
broad application scenarios in the fields of mobile robots, drones, smart
driving, and smart security. However, in the actual application of mobile
robots, problems such as inaccurate segmentation semantic label prediction and
loss of edge information of segmented objects and background may occur. This
paper proposes an improved structure of a semantic segmentation network based
on a deep learning network that combines self-attention neural network and
neural network architecture search methods. First, a neural network search
method NAS (Neural Architecture Search) is used to find a semantic segmentation
network with multiple resolution branches. In the search process, combine the
self-attention network structure module to adjust the searched neural network
structure, and then combine the semantic segmentation network searched by
different branches to form a fast semantic segmentation network structure, and
input the picture into the network structure to get the final forecast result.
The experimental results on the Cityscapes dataset show that the accuracy of
the algorithm is 69.8%, and the segmentation speed is 48/s. It achieves a good
balance between real-time and accuracy, can optimize edge segmentation, and has
a better performance in complex scenes. Good robustness is suitable for
practical application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point2Cyl: Reverse Engineering 3D Objects from Point Clouds to Extrusion Cylinders. (arXiv:2112.09329v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09329">
<div class="article-summary-box-inner">
<span><p>We propose Point2Cyl, a supervised network transforming a raw 3D point cloud
to a set of extrusion cylinders. Reverse engineering from a raw geometry to a
CAD model is an essential task to enable manipulation of the 3D data in shape
editing software and thus expand their usages in many downstream applications.
Particularly, the form of CAD models having a sequence of extrusion cylinders
-- a 2D sketch plus an extrusion axis and range -- and their boolean
combinations is not only widely used in the CAD community/software but also has
great expressivity of shapes, compared to having limited types of primitives
(e.g., planes, spheres, and cylinders). In this work, we introduce a neural
network that solves the extrusion cylinder decomposition problem in a
geometry-grounded way by first learning underlying geometric proxies.
Precisely, our approach first predicts per-point segmentation, base/barrel
labels and normals, then estimates for the underlying extrusion parameters in
differentiable and closed-form formulations. Our experiments show that our
approach demonstrates the best performance on two recent CAD datasets, Fusion
Gallery and DeepCAD, and we further showcase our approach on reverse
engineering and editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flow-Guided Sparse Transformer for Video Deblurring. (arXiv:2201.01893v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01893">
<div class="article-summary-box-inner">
<span><p>Exploiting similar and sharper scene patches in spatio-temporal neighborhoods
is critical for video deblurring. However, CNN-based methods show limitations
in capturing long-range dependencies and modeling non-local self-similarity. In
this paper, we propose a novel framework, Flow-Guided Sparse Transformer
(FGST), for video deblurring. In FGST, we customize a self-attention module,
Flow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each
$query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of
the estimated optical flow to globally sample spatially sparse yet highly
related $key$ elements corresponding to the same scene patch in neighboring
frames. Besides, we present a Recurrent Embedding (RE) mechanism to transfer
information from past frames and strengthen long-range temporal dependencies.
Comprehensive experiments demonstrate that our proposed FGST outperforms
state-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields
more visually pleasing results in real video deblurring. Code and pre-trained
models are publicly available at https://github.com/linjing7/VR-Baseline
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object Detectors in the Physical World. (arXiv:2201.08619v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08619">
<div class="article-summary-box-inner">
<span><p>Deep learning models have been shown to be vulnerable to recent backdoor
attacks. A backdoored model behaves normally for inputs containing no
attacker-secretly-chosen trigger and maliciously for inputs with the trigger.
To date, backdoor attacks and countermeasures mainly focus on image
classification tasks. And most of them are implemented in the digital world
with digital triggers. Besides the classification tasks, object detection
systems are also considered as one of the basic foundations of computer vision
tasks. However, there is no investigation and understanding of the backdoor
vulnerability of the object detector, even in the digital world with digital
triggers. For the first time, this work demonstrates that existing object
detectors are inherently susceptible to physical backdoor attacks. We use a
natural T-shirt bought from a market as a trigger to enable the cloaking
effect--the person bounding-box disappears in front of the object detector. We
show that such a backdoor can be implanted from two exploitable attack
scenarios into the object detector, which is outsourced or fine-tuned through a
pretrained model. We have extensively evaluated three popular object detection
algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building
upon 19 videos shot in real-world scenes, we confirm that the backdoor attack
is robust against various factors: movement, distance, angle, non-rigid
deformation, and lighting. Specifically, the attack success rate (ASR) in most
videos is 100% or close to it, while the clean data accuracy of the backdoored
model is the same as its clean counterpart. The latter implies that it is
infeasible to detect the backdoor behavior merely through a validation set. The
averaged ASR still remains sufficiently high to be 78% in the transfer learning
attack scenarios evaluated on CenterNet. See the demo video on
https://youtu.be/Q3HOF4OobbY.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Sketch Based Image Retrieval using Graph Transformer. (arXiv:2201.10185v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10185">
<div class="article-summary-box-inner">
<span><p>The performance of a zero-shot sketch-based image retrieval (ZS-SBIR) task is
primarily affected by two challenges. The substantial domain gap between image
and sketch features needs to be bridged, while at the same time the side
information has to be chosen tactfully. Existing literature has shown that
varying the semantic side information greatly affects the performance of
ZS-SBIR. To this end, we propose a novel graph transformer based zero-shot
sketch-based image retrieval (GTZSR) framework for solving ZS-SBIR tasks which
uses a novel graph transformer to preserve the topology of the classes in the
semantic space and propagates the context-graph of the classes within the
embedding features of the visual space. To bridge the domain gap between the
visual features, we propose minimizing the Wasserstein distance between images
and sketches in a learned domain-shared space. We also propose a novel
compatibility loss that further aligns the two visual domains by bridging the
domain gap of one class with respect to the domain gap of all other classes in
the training set. Experimental results obtained on the extended Sketchy,
TU-Berlin, and QuickDraw datasets exhibit sharp improvements over the existing
state-of-the-art methods in both ZS-SBIR and generalized ZS-SBIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Image Deblurring: A Survey. (arXiv:2201.10700v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10700">
<div class="article-summary-box-inner">
<span><p>Image deblurring is a classic problem in low-level computer vision with the
aim to recover a sharp image from a blurred input image. Advances in deep
learning have led to significant progress in solving this problem, and a large
number of deblurring networks have been proposed. This paper presents a
comprehensive and timely survey of recently published deep-learning based image
deblurring approaches, aiming to serve the community as a useful literature
review. We start by discussing common causes of image blur, introduce benchmark
datasets and performance metrics, and summarize different problem formulations.
Next, we present a taxonomy of methods using convolutional neural networks
(CNN) based on architecture, loss function, and application, offering a
detailed review and comparison. In addition, we discuss some domain-specific
deblurring applications including face images, text, and stereo image pairs. We
conclude by discussing key challenges and future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Tasks Siamese Transformer Framework for Building Damage Assessment. (arXiv:2201.10953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10953">
<div class="article-summary-box-inner">
<span><p>Accurate and fine-grained information about the extent of damage to buildings
is essential for humanitarian relief and disaster response. However, as the
most commonly used architecture in remote sensing interpretation tasks,
Convolutional Neural Networks (CNNs) have limited ability to model the
non-local relationship between pixels. Recently, Transformer architecture first
proposed for modeling long-range dependency in natural language processing has
shown promising results in computer vision tasks. Considering the frontier
advances of Transformer architecture in the computer vision field, in this
paper, we present the first attempt at designing a Transformer-based damage
assessment architecture (DamFormer). In DamFormer, a siamese Transformer
encoder is first constructed to extract non-local and representative deep
features from input multitemporal image-pairs. Then, a multitemporal fusion
module is designed to fuse information for downstream tasks. Finally, a
lightweight dual-tasks decoder aggregates multi-level features for final
prediction. To the best of our knowledge, it is the first time that such a deep
Transformer-based network is proposed for multitemporal remote sensing
interpretation tasks. The experimental results on the large-scale damage
assessment dataset xBD demonstrate the potential of the Transformer-based
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compose Diversified Prompts for Image Emotion Classification. (arXiv:2201.10963v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10963">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pre-training (CLIP) represents the latest
incarnation of pre-trained vision-language models. Although CLIP has recently
shown its superior power on a wide range of downstream vision-language tasks
like Visual Question Answering, it is still underexplored for Image Emotion
Classification (IEC). Adapting CLIP to the IEC task has three significant
challenges, tremendous training objective gap between pretraining and IEC,
shared suboptimal and invariant prompts for all instances. In this paper, we
propose a general framework that shows how CLIP can be effectively applied to
IEC. We first introduce a prompt tuning method that mimics the pretraining
objective of CLIP and thus can leverage the rich image and text semantics
entailed in CLIP. Then we automatically compose instance-specific prompts by
conditioning them on the categories and image contents of instances,
diversifying prompts and avoiding suboptimal problems. Evaluations on six
widely-used affective datasets demonstrate that our proposed method outperforms
the state-of-the-art methods to a large margin (i.e., up to 9.29% accuracy gain
on EmotionROI dataset) on IEC tasks, with only a few parameters trained. Our
codes will be publicly available for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Invariant Representation Learning from EEG with Private Encoders. (arXiv:2201.11613v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11613">
<div class="article-summary-box-inner">
<span><p>Deep learning based electroencephalography (EEG) signal processing methods
are known to suffer from poor test-time generalization due to the changes in
data distribution. This becomes a more challenging problem when
privacy-preserving representation learning is of interest such as in clinical
settings. To that end, we propose a multi-source learning architecture where we
extract domain-invariant representations from dataset-specific private
encoders. Our model utilizes a maximum-mean-discrepancy (MMD) based domain
alignment approach to impose domain-invariance for encoded representations,
which outperforms state-of-the-art approaches in EEG-based emotion
classification. Furthermore, representations learned in our pipeline preserve
domain privacy as dataset-specific private encoding alleviates the need for
conventional, centralized EEG-based deep neural network training approaches
with shared parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The KFIoU Loss for Rotated Object Detection. (arXiv:2201.12558v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12558">
<div class="article-summary-box-inner">
<span><p>Differing from the well-developed horizontal object detection area whereby
the computing-friendly IoU based loss is readily adopted and well fits with the
detection metrics. In contrast, rotation detectors often involve a more
complicated loss based on SkewIoU which is unfriendly to gradient-based
training. In this paper, we argue that one effective alternative is to devise
an approximate loss who can achieve trend-level alignment with SkewIoU loss
instead of the strict value-level identity. Specifically, we model the objects
as Gaussian distribution and adopt Kalman filter to inherently mimic the
mechanism of SkewIoU by its definition, and show its alignment with the SkewIoU
at trend-level. This is in contrast to recent Gaussian modeling based rotation
detectors e.g. GWD, KLD that involves a human-specified distribution distance
metric which requires additional hyperparameter tuning. The resulting new loss
called KFIoU is easier to implement and works better compared with exact
SkewIoU, thanks to its full differentiability and ability to handle the
non-overlapping cases. We further extend our technique to the 3-D case which
also suffers from the same issues as 2-D detection. Extensive results on
various public datasets (2-D/3-D, aerial/text/face images) with different base
detectors show the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context Autoencoder for Self-Supervised Representation Learning. (arXiv:2202.03026v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03026">
<div class="article-summary-box-inner">
<span><p>We present a novel masked image modeling (MIM) approach, context autoencoder
(CAE), for self-supervised representation pretraining. The goal is to pretrain
an encoder by solving the pretext task: estimate the masked patches from the
visible patches in an image. Our approach first feeds the visible patches into
the encoder, extracting the representations. Then, we make predictions from
visible patches to masked patches in the encoded representation space. We
introduce an alignment constraint, encouraging that the representations for
masked patches, predicted from the encoded representations of visible patches,
are aligned with the masked patch presentations computed from the encoder. In
other words, the predicted representations are expected to lie in the encoded
representation space, which empirically shows the benefit to representation
learning. Last, the predicted masked patch representations are mapped to the
targets of the pretext task through a decoder. In comparison to previous MIM
methods (e.g., BEiT) that couple the encoding and pretext task completion
roles, our approach benefits the separation of the representation learning
(encoding) role and the pretext task completion role, improving the
representation learning capacity and accordingly helping more on downstream
tasks. In addition, we present the explanations about why contrastive
pretraining and supervised pretraining perform similarly and why MIM
potentially performs better. We demonstrate the effectiveness of our CAE
through superior transfer performance in downstream tasks: semantic
segmentation, and object detection and instance segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving. (arXiv:2202.07133v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07133">
<div class="article-summary-box-inner">
<span><p>While supervised detection and classification frameworks in autonomous
driving require large labelled datasets to converge, Unsupervised Domain
Adaptation (UDA) approaches, facilitated by synthetic data generated from
photo-real simulated environments, are considered low-cost and less
time-consuming solutions. In this paper, we propose UDA schemes using
adversarial discriminative and generative methods for lane detection and
classification applications in autonomous driving. We also present Simulanes
dataset generator to create a synthetic dataset that is naturalistic utilizing
CARLA's vast traffic scenarios and weather conditions. The proposed UDA
frameworks take the synthesized dataset with labels as the source domain,
whereas the target domain is the unlabelled real-world data. Using adversarial
generative and feature discriminators, the learnt models are tuned to predict
the lane location and class in the target domain. The proposed techniques are
evaluated using both real-world and our synthetic datasets. The results
manifest that the proposed methods have shown superiority over other baseline
schemes in terms of detection and classification accuracy and consistency. The
ablation study reveals that the size of the simulation dataset plays important
roles in the classification performance of the proposed methods. Our UDA
frameworks are available at https://github.com/anita-hu/sim2real-lane-detection
and our dataset generator is released at https://github.com/anita-hu/simulanes
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lie Point Symmetry Data Augmentation for Neural PDE Solvers. (arXiv:2202.07643v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07643">
<div class="article-summary-box-inner">
<span><p>Neural networks are increasingly being used to solve partial differential
equations (PDEs), replacing slower numerical solvers. However, a critical issue
is that neural PDE solvers require high-quality ground truth data, which
usually must come from the very solvers they are designed to replace. Thus, we
are presented with a proverbial chicken-and-egg problem. In this paper, we
present a method, which can partially alleviate this problem, by improving
neural PDE solver sample complexity -- Lie point symmetry data augmentation
(LPSDA). In the context of PDEs, it turns out that we are able to
quantitatively derive an exhaustive list of data transformations, based on the
Lie point symmetry group of the PDEs in question, something not possible in
other application areas. We present this framework and demonstrate how it can
easily be deployed to improve neural PDE solver sample complexity by an order
of magnitude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSegNet: Operational Segmentation Network for COVID-19 Detection using Chest X-ray Images. (arXiv:2202.10185v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10185">
<div class="article-summary-box-inner">
<span><p>Coronavirus disease 2019 (COVID-19) has been diagnosed automatically using
Machine Learning algorithms over chest X-ray (CXR) images. However, most of the
earlier studies used Deep Learning models over scarce datasets bearing the risk
of overfitting. Additionally, previous studies have revealed the fact that deep
networks are not reliable for classification since their decisions may
originate from irrelevant areas on the CXRs. Therefore, in this study, we
propose Operational Segmentation Network (OSegNet) that performs detection by
segmenting COVID-19 pneumonia for a reliable diagnosis. To address the data
scarcity encountered in training and especially in evaluation, this study
extends the largest COVID-19 CXR dataset: QaTa-COV19 with 121,378 CXRs
including 9258 COVID-19 samples with their corresponding ground-truth
segmentation masks that are publicly shared with the research community.
Consequently, OSegNet has achieved a detection performance with the highest
accuracy of 99.65% among the state-of-the-art deep models with 98.09%
precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TwistSLAM: Constrained SLAM in Dynamic Environment. (arXiv:2202.12384v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12384">
<div class="article-summary-box-inner">
<span><p>Classical visual simultaneous localization and mapping (SLAM) algorithms
usually assume the environment to be rigid. This assumption limits the
applicability of those algorithms as they are unable to accurately estimate the
camera poses and world structure in real life scenes containing moving objects
(e.g. cars, bikes, pedestrians, etc.). To tackle this issue, we propose
TwistSLAM: a semantic, dynamic and stereo SLAM system that can track dynamic
objects in the environment. Our algorithm creates clusters of points according
to their semantic class. Thanks to the definition of inter-cluster constraints
modeled by mechanical joints (function of the semantic class), a novel
constrained bundle adjustment is then able to jointly estimate both poses and
velocities of moving objects along with the classical world structure and
camera trajectory. We evaluate our approach on several sequences from the
public KITTI dataset and demonstrate quantitatively that it improves camera and
object tracking compared to state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set Methods. (arXiv:2203.02486v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02486">
<div class="article-summary-box-inner">
<span><p>In many object recognition applications, the set of possible categories is an
open set, and the deployed recognition system will encounter novel objects
belonging to categories unseen during training. Detecting such "novel category"
objects is usually formulated as an anomaly detection problem. Anomaly
detection algorithms for feature-vector data identify anomalies as outliers,
but outlier detection has not worked well in deep learning. Instead, methods
based on the computed logits of visual object classifiers give state-of-the-art
performance. This paper proposes the Familiarity Hypothesis that these methods
succeed because they are detecting the absence of familiar learned features
rather than the presence of novelty. The paper reviews evidence from the
literature and presents additional evidence from our own experiments that
provide strong support for this hypothesis. The paper concludes with a
discussion of whether familiarity detection is an inevitable consequence of
representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN. (arXiv:2203.02762v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02762">
<div class="article-summary-box-inner">
<span><p>The research topic of sketch-to-portrait generation has witnessed a boost of
progress with deep learning techniques. The recently proposed StyleGAN
architectures achieve state-of-the-art generation ability but the original
StyleGAN is not friendly for sketch-based creation due to its unconditional
generation nature. To address this issue, we propose a direct conditioning
strategy to better preserve the spatial information under the StyleGAN
framework. Specifically, we introduce Spatially Conditioned StyleGAN
(SC-StyleGAN for short), which explicitly injects spatial constraints to the
original StyleGAN generation process. We explore two input modalities, sketches
and semantic maps, which together allow users to express desired generation
results more precisely and easily. Based on SC-StyleGAN, we present
DrawingInStyles, a novel drawing interface for non-professional users to easily
produce high-quality, photo-realistic face images with precise control, either
from scratch or editing existing ones. Qualitative and quantitative evaluations
show the superior generation ability of our method to existing and alternative
solutions. The usability and expressiveness of our system are confirmed by a
user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do We Really Need a Learnable Classifier at the End of Deep Neural Network?. (arXiv:2203.09081v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09081">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks for classification usually jointly learn a
backbone for representation and a linear classifier to output the logit of each
class. A recent study has shown a phenomenon called neural collapse that the
within-class means of features and the classifier vectors converge to the
vertices of a simplex equiangular tight frame (ETF) at the terminal phase of
training on a balanced dataset. Since the ETF geometric structure maximally
separates the pair-wise angles of all classes in the classifier, it is natural
to raise the question, why do we spend an effort to learn a classifier when we
know its optimal geometric structure? In this paper, we study the potential of
learning a neural network for classification with the classifier randomly
initialized as an ETF and fixed during training. Our analytical work based on
the layer-peeled model indicates that the feature learning with a fixed ETF
classifier naturally leads to the neural collapse state even when the dataset
is imbalanced among classes. We further show that in this case the cross
entropy (CE) loss is not necessary and can be replaced by a simple squared loss
that shares the same global optimality but enjoys a better convergence
property. Our experimental results show that our method is able to bring
significant improvements with faster convergence on multiple imbalanced
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09481">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models are a promising new class of
generative models that mark a milestone in high-quality image generation. This
paper showcases their ability to sequentially generate video, surpassing prior
methods in perceptual and probabilistic forecasting metrics. We propose an
autoregressive, end-to-end optimized video diffusion model inspired by recent
advances in neural video compression. The model successively generates future
frames by correcting a deterministic next-frame prediction using a stochastic
residual generated by an inverse diffusion process. We compare this approach
against five baselines on four datasets involving natural and simulation-based
videos. We find significant improvements in terms of perceptual quality for all
datasets. Furthermore, by introducing a scalable version of the Continuous
Ranked Probability Score (CRPS) applicable to video, we show that our model
also outperforms existing approaches in their probabilistic frame forecasting
ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Inversion for Nonlinear Imaging Models using Deep Generative Priors. (arXiv:2203.10078v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10078">
<div class="article-summary-box-inner">
<span><p>Most modern imaging systems incorporate a computational pipeline to infer the
image of interest from acquired measurements. The Bayesian approach for solving
such ill-posed inverse problems involves the characterization of the posterior
distribution of the image. This depends on the model of the imaging system and
prior knowledge on the image of interest. In this work, we present a Bayesian
reconstruction framework for nonlinear imaging models, where the prior
knowledge on the image is specified by a deep generative model. We develop a
tractable posterior sampling scheme based on the Metropolis-adjusted Langevin
algorithm (MALA) for the class of nonlinear inverse problems where the forward
model has a neural-network-like structure. This class includes most practical
imaging modalities. We introduce the notion of augmented deep generative priors
in order to suitably handle quantitative image recovery. We illustrate the
advantages of our framework by applying it to two nonlinear imaging
modalities-phase retrieval and optical diffraction tomography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12023">
<div class="article-summary-box-inner">
<span><p>Many promising applications of supervised machine learning face hurdles in
the acquisition of labeled data in sufficient quantity and quality, creating an
expensive bottleneck. To overcome such limitations, techniques that do not
depend on ground truth labels have been studied, including weak supervision and
generative modeling. While these techniques would seem to be usable in concert,
improving one another, how to build an interface between them is not
well-understood. In this work, we propose a model fusing programmatic weak
supervision and generative adversarial networks and provide theoretical
justification motivating this fusion. The proposed approach captures discrete
latent variables in the data alongside the weak supervision derived label
estimate. Alignment of the two allows for better modeling of sample-dependent
accuracies of the weak supervision sources, improving the estimate of
unobserved labels. It is the first approach to enable data augmentation through
weakly supervised synthetic images and pseudolabels. Additionally, its learned
latent variables can be inspected qualitatively. The model outperforms baseline
weak supervision label models on a number of multiclass image classification
datasets, improves the quality of generated images, and further improves
end-model performance through data augmentation with synthetic samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction. (arXiv:2203.12997v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12997">
<div class="article-summary-box-inner">
<span><p>Dimensionality reduction is crucial both for visualization and preprocessing
high dimensional data for machine learning. We introduce a novel method based
on a hierarchy built on 1-nearest neighbor graphs in the original space which
is used to preserve the grouping properties of the data distribution on
multiple levels. The core of the proposal is an optimization-free projection
that is competitive with the latest versions of t-SNE and UMAP in performance
and visualization quality while being an order of magnitude faster in run-time.
Furthermore, its interpretable mechanics, the ability to project new data, and
the natural separation of data clusters in visualizations make it a general
purpose unsupervised dimension reduction technique. In the paper, we argue
about the soundness of the proposed method and evaluate it on a diverse
collection of datasets with sizes varying from 1K to 11M samples and dimensions
from 28 to 16K. We perform comparisons with other state-of-the-art methods on
multiple metrics and target dimensions highlighting its efficiency and
performance. Code is available at https://github.com/koulakis/h-nne
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13310">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection has long been a challenging task in autonomous
driving, which requires to decode 3D predictions solely from a single 2D image.
Most existing methods follow conventional 2D object detectors to localize
objects based on their centers, and predict 3D attributes by neighboring
features around the centers. However, only using local features is insufficient
to understand the scene-level 3D spatial structures and ignore the inter-object
depth relations from contextual cues. In this paper, we introduce a novel
framework for Monocular DEtection with a depth-guided TRansformer, named
MonoDETR. The vanilla transformer is modified to be depth-aware and the whole
detection process is then guided by depth. Specifically, we represent 3D object
candidates as a set of queries and adopt an attention-based depth encoder to
produce non-local depth embeddings of the input image. Then, we propose a
depth-guided decoder with depth cross-attention modules to conduct both
inter-query and query-scene depth feature interactions. In this way, each
object query estimates its 3D attributes adaptively from the depth-guided
regions from the image and is no longer constrained to use only neighboring
visual features. MonoDETR is an end-to-end network without extra data or NMS
post-processing and achieves state-of-the-art performance on KITTI benchmark
with significant gains. Extensive ablation studies demonstrate the
effectiveness of our approach and its potential to serve as a transformer
baseline for future monocular 3D object detection research. Code is available
at https://github.com/ZrrSkywalker/MonoDETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Min-Max Similarity: A Contrastive Learning Based Semi-Supervised Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15177">
<div class="article-summary-box-inner">
<span><p>Segmentation of images is a popular topic in medical AI. This is mainly due
to the difficulty to obtain a significant number of pixel-level annotated data
to train a neural network. To address this issue, we proposed a semi-supervised
segmentation network based on contrastive learning. In contrast to the previous
state-of-the-art, we introduce a contrastive learning form of dual-view
training by employing classifiers and projectors to build all-negative, and
positive and negative feature pairs respectively to formulate the learning
problem as solving min-max similarity problem. The all-negative pairs are used
to supervise the networks learning from different views and make sure to
capture general features, and the consistency of unlabeled predictions is
measured by pixel-wise contrastive loss between positive and negative pairs. To
quantitative and qualitative evaluate our proposed method, we test it on two
public endoscopy surgical tool segmentation datasets and one cochlear implant
surgery dataset which we manually annotate the cochlear implant in surgical
videos. The segmentation performance (dice coefficients) indicates that our
proposed method outperforms state-of-the-art semi-supervised and fully
supervised segmentation algorithms consistently. The code is publicly available
at: https://github.com/AngeLouCN/Min_Max_Similarity
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex-Valued Autoencoders for Object Discovery. (arXiv:2204.02075v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02075">
<div class="article-summary-box-inner">
<span><p>Object-centric representations form the basis of human perception, and enable
us to reason about the world and to systematically generalize to new settings.
Currently, most works on unsupervised object discovery focus on slot-based
approaches, which explicitly separate the latent representations of individual
objects. While the result is easily interpretable, it usually requires the
design of involved architectures. In contrast to this, we propose a
comparatively simple approach - the Complex AutoEncoder (CAE) - that creates
distributed object-centric representations. Following a coding scheme theorized
to underlie object representations in biological neurons, its complex-valued
activations represent two messages: their magnitudes express the presence of a
feature, while the relative phase differences between neurons express which
features should be bound together to create joint object representations. In
contrast to previous approaches using complex-valued activations for object
discovery, we present a fully unsupervised approach that is trained end-to-end
- resulting in significant improvements in performance and efficiency on simple
multi-object datasets. Further, we show that the CAE achieves competitive or
better unsupervised object discovery performance compared to a state-of-the-art
slot-based approach while being up to 100 times faster to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Different Losses for Deep Learning Image Colorization. (arXiv:2204.02980v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02980">
<div class="article-summary-box-inner">
<span><p>Image colorization aims to add color information to a grayscale image in a
realistic way. Recent methods mostly rely on deep learning strategies. While
learning to automatically colorize an image, one can define well-suited
objective functions related to the desired color output. Some of them are based
on a specific type of error between the predicted image and ground truth one,
while other losses rely on the comparison of perceptual properties. But, is the
choice of the objective function that crucial, i.e., does it play an important
role in the results? In this chapter, we aim to answer this question by
analyzing the impact of the loss function on the estimated colorization
results. To that goal, we review the different losses and evaluation metrics
that are used in the literature. We then train a baseline network with several
of the reviewed objective functions: classic L1 and L2 losses, as well as more
complex combinations such as Wasserstein GAN and VGG-based LPIPS loss.
Quantitative results show that the models trained with VGG-based LPIPS provide
overall slightly better results for most evaluation metrics. Qualitative
results exhibit more vivid colors when with Wasserstein GAN plus the L2 loss or
again with the VGG-based LPIPS. Finally, the convenience of quantitative user
studies is also discussed to overcome the difficulty of properly assessing on
colorized images, notably for the case of old archive photographs where no
ground truth is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pin the Memory: Learning to Generalize Semantic Segmentation. (arXiv:2204.03609v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03609">
<div class="article-summary-box-inner">
<span><p>The rise of deep neural networks has led to several breakthroughs for
semantic segmentation. In spite of this, a model trained on source domain often
fails to work properly in new challenging domains, that is directly concerned
with the generalization capability of the model. In this paper, we present a
novel memory-guided domain generalization method for semantic segmentation
based on meta-learning framework. Especially, our method abstracts the
conceptual knowledge of semantic classes into categorical memory which is
constant beyond the domains. Upon the meta-learning concept, we repeatedly
train memory-guided networks and simulate virtual test to 1) learn how to
memorize a domain-agnostic and distinct information of classes and 2) offer an
externally settled memory as a class-guidance to reduce the ambiguity of
representation in the test data of arbitrary unseen domain. To this end, we
also propose memory divergence and feature cohesion losses, which encourage to
learn memory reading and update processes for category-aware domain
generalization. Extensive experiments for semantic segmentation demonstrate the
superior generalization capability of our method over state-of-the-art works on
various benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transparent Shape from Single Polarization Images. (arXiv:2204.06331v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06331">
<div class="article-summary-box-inner">
<span><p>This paper presents a data-driven approach for transparent shape from
polarization. Due to the inherent high transmittance, the previous shape from
polarization(SfP) methods based on specular reflection model have difficulty in
estimating transparent shape, and the lack of datasets for transparent SfP also
limits the application of the data-driven approach. Hence, we construct the
transparent SfP dataset which consists of both synthetic and real-world
datasets. To determine the reliability of the physics-based reflection model,
we define the physics-based prior confidence by exploiting the inherent fault
of polarization information, then we propose a multi-branch fusion network to
embed the confidence. Experimental results show that our approach outperforms
other SfP methods. Compared with the previous method, the mean and median
angular error of our approach are reduced from $19.00^\circ$ and $14.91^\circ$
to $16.72^\circ$ and $13.36^\circ$, and the accuracy $11.25^\circ, 22.5^\circ,
30^\circ$ are improved from $38.36\%, 77.36\%, 87.48\%$ to $45.51\%, 78.86\%,
89.98\%$, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Prototype: Divide-and-conquer Proxies for Few-shot Segmentation. (arXiv:2204.09903v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09903">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation, which aims to segment unseen-class objects given only
a handful of densely labeled samples, has received widespread attention from
the community. Existing approaches typically follow the prototype learning
paradigm to perform meta-inference, which fails to fully exploit the underlying
information from support image-mask pairs, resulting in various segmentation
failures, e.g., incomplete objects, ambiguous boundaries, and distractor
activation. To this end, we propose a simple yet versatile framework in the
spirit of divide-and-conquer. Specifically, a novel self-reasoning scheme is
first implemented on the annotated support image, and then the coarse
segmentation mask is divided into multiple regions with different properties.
Leveraging effective masked average pooling operations, a series of
support-induced proxies are thus derived, each playing a specific role in
conquering the above challenges. Moreover, we devise a unique parallel decoder
structure that integrates proxies with similar attributes to boost the
discrimination power. Our proposed approach, named divide-and-conquer proxies
(DCP), allows for the development of appropriate and reliable information as a
guide at the "episode" level, not just about the object cues themselves.
Extensive experiments on PASCAL-5i and COCO-20i demonstrate the superiority of
DCP over conventional prototype-based approaches (up to 5~10% on average),
which also establishes a new state-of-the-art. Code is available at
github.com/chunbolang/DCP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Estimation with Simplified Transformer. (arXiv:2204.13791v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13791">
<div class="article-summary-box-inner">
<span><p>Transformer and its variants have shown state-of-the-art results in many
vision tasks recently, ranging from image classification to dense prediction.
Despite of their success, limited work has been reported on improving the model
efficiency for deployment in latency-critical applications, such as autonomous
driving and robotic navigation. In this paper, we aim at improving upon the
existing transformers in vision, and propose a method for self-supervised
monocular Depth Estimation with Simplified Transformer (DEST), which is
efficient and particularly suitable for deployment on GPU-based platforms.
Through strategic design choices, our model leads to significant reduction in
model size, complexity, as well as inference latency, while achieving superior
accuracy as compared to state-of-the-art. We also show that our design
generalize well to other dense prediction task without bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining. (arXiv:2204.14095v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14095">
<div class="article-summary-box-inner">
<span><p>Large-scale vision-language pre-training has achieved promising results on
downstream tasks. Existing methods highly rely on the assumption that the
image-text pairs crawled from the Internet are in perfect one-to-one
correspondence. However, in real scenarios, this assumption can be difficult to
hold: the text description, obtained by crawling the affiliated metadata of the
image, often suffers from the semantic mismatch and the mutual compatibility.
To address these issues, we introduce PyramidCLIP, which constructs an input
pyramid with different semantic levels for each modality, and aligns visual
elements and linguistic elements in the form of hierarchy via peer-level
semantics alignment and cross-level relation alignment. Furthermore, we soften
the loss of negative samples (unpaired samples) so as to weaken the strict
constraint during the pre-training stage, thus mitigating the risk of forcing
the model to distinguish compatible negative pairs. Experiments on five
downstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In
particular, with the same amount of 15 million pre-training image-text pairs,
PyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by
10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder
respectively. When scaling to larger datasets, PyramidCLIP achieves the
state-of-the-art results on several downstream tasks. In particular, the
results of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that
of CLIP using 400M data on ImageNet zero-shot classification task,
significantly improving the data efficiency of CLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks. (arXiv:2205.01355v3 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01355">
<div class="article-summary-box-inner">
<span><p>We present a learning algorithm that uses bone-driven motion networks to
predict the deformation of loose-fitting garment meshes at interactive rates.
Given a garment, we generate a simulation database and extract virtual bones
from simulated mesh sequences using skin decomposition. At runtime, we
separately compute low- and high-frequency deformations in a sequential manner.
The low-frequency deformations are predicted by transferring body motions to
virtual bones' motions, and the high-frequency deformations are estimated
leveraging the global information of virtual bones' motions and local
information extracted from low-frequency meshes. In addition, our method can
estimate garment deformations caused by variations of the simulation parameters
(e.g., fabric's bending stiffness) using an RBF kernel ensembling trained
networks for different sets of simulation parameters. Through extensive
comparisons, we show that our method outperforms state-of-the-art methods in
terms of prediction accuracy of mesh deformations by about 20% in RMSE and 10%
in Hausdorff distance and STED. The code and data are available at
https://github.com/non-void/VirtualBones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting and Understanding Harmful Memes: A Survey. (arXiv:2205.04274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04274">
<div class="article-summary-box-inner">
<span><p>The automatic identification of harmful content online is of major concern
for social media platforms, policymakers, and society. Researchers have studied
textual, visual, and audio content, but typically in isolation. Yet, harmful
content often combines multiple modalities, as in the case of memes, which are
of particular interest due to their viral nature. With this in mind, here we
offer a comprehensive survey with a focus on harmful memes. Based on a
systematic analysis of recent literature, we first propose a new typology of
harmful memes, and then we highlight and summarize the relevant state of the
art. One interesting finding is that many types of harmful memes are not really
studied, e.g., such featuring self-harm and extremism, partly due to the lack
of suitable datasets. We further find that existing datasets mostly capture
multi-class scenarios, which are not inclusive of the affective spectrum that
memes can represent. Another observation is that memes can propagate globally
through repackaging in different languages and that they can also be
multilingual, blending different cultures. We conclude by highlighting several
challenges related to multimodal semiotics, technological constraints, and
non-trivial social engagement, and we present several open-ended aspects such
as delineating online harm and empirically examining related frameworks and
assistive interventions, which we believe will motivate and drive future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-textures, a self-supervised hard clustering deep learning algorithm for satellite image segmentation. (arXiv:2205.08671v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08671">
<div class="article-summary-box-inner">
<span><p>Deep learning self-supervised algorithms that can segment an image in a fixed
number of hard labels such as the k-means algorithm and relying only on deep
learning techniques are still lacking. Here, we introduce the k-textures
algorithm which provides self-supervised segmentation of a 4-band image
(RGB-NIR) for a $k$ number of classes. An example of its application on high
resolution Planet satellite imagery is given. Our algorithm shows that discrete
search is feasible using convolutional neural networks (CNN) and gradient
descent. The model detects $k$ hard clustering classes represented in the model
as $k$ discrete binary masks and their associated $k$ independently generated
textures, that combined are a simulation of the original image. The similarity
loss is the mean squared error between the features of the original and the
simulated image, both extracted from the penultimate convolutional block of
Keras 'imagenet' pretrained VGG-16 model and a custom feature extractor made
with Planet data. The main advances of the k-textures model are: first, the $k$
discrete binary masks are obtained inside the model using gradient descent. The
model allows for the generation of discrete binary masks using a novel method
using a hard sigmoid activation function. Second, it provides hard clustering
classes -- each pixels has only one class. Finally, in comparison to k-means,
where each pixel is considered independently, here, contextual information is
also considered and each class is not associated only to similar values in the
color channels but also to a texture. Our approach is designed to ease the
production of training samples for satellite image segmentation and the
k-textures architecture could be adapted to support different number of bands
and for more complex tasks, such as object self-segmentation. The model codes
and weights are available at https://doi.org/10.5281/zenodo.6359859
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation. (arXiv:2205.09853v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09853">
<div class="article-summary-box-inner">
<span><p>Video prediction is a challenging task. The quality of video frames from
current state-of-the-art (SOTA) generative models tends to be poor and
generalization beyond the training data is difficult. Furthermore, existing
prediction frameworks are typically not capable of simultaneously handling
other video-related tasks such as unconditional generation or interpolation. In
this work, we devise a general-purpose framework called Masked Conditional
Video Diffusion (MCVD) for all of these video synthesis tasks using a
probabilistic conditional score-based denoising diffusion model, conditioned on
past and/or future frames. We train the model in a manner where we randomly and
independently mask all the past frames or all the future frames. This novel but
straightforward setup allows us to train a single model that is capable of
executing a broad range of video tasks, specifically: future/past prediction --
when only future/past frames are masked; unconditional generation -- when both
past and future frames are masked; and interpolation -- when neither past nor
future frames are masked. Our experiments show that this approach can generate
high-quality frames for diverse types of videos. Our MCVD models are built from
simple non-recurrent 2D-convolutional architectures, conditioning on blocks of
frames and generating blocks of frames. We generate videos of arbitrary lengths
autoregressively in a block-wise manner. Our approach yields SOTA results
across standard video prediction and interpolation benchmarks, with computation
times for training models measured in 1-12 days using $\le$ 4 GPUs. Project
page: https://mask-cond-video-diffusion.github.io ; Code :
https://github.com/voletiv/mcvd-pytorch
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer based Generative Adversarial Network for Liver Segmentation. (arXiv:2205.10663v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10663">
<div class="article-summary-box-inner">
<span><p>Automated liver segmentation from radiology scans (CT, MRI) can improve
surgery and therapy planning and follow-up assessment in addition to
conventional use for diagnosis and prognosis. Although convolutional neural
networks (CNNs) have become the standard image segmentation tasks, more
recently this has started to change towards Transformers based architectures
because Transformers are taking advantage of capturing long range dependence
modeling capability in signals, so called attention mechanism. In this study,
we propose a new segmentation approach using a hybrid approach combining the
Transformer(s) with the Generative Adversarial Network (GAN) approach. The
premise behind this choice is that the self-attention mechanism of the
Transformers allows the network to aggregate the high dimensional feature and
provide global information modeling. This mechanism provides better
segmentation performance compared with traditional methods. Furthermore, we
encode this generator into the GAN based architecture so that the discriminator
network in the GAN can classify the credibility of the generated segmentation
masks compared with the real masks coming from human (expert) annotations. This
allows us to extract the high dimensional topology information in the mask for
biomedical image segmentation and provide more reliable segmentation results.
Our model achieved a high dice coefficient of 0.9433, recall of 0.9515, and
precision of 0.9376 and outperformed other Transformer based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners. (arXiv:2205.10747v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10747">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to build flexible video-language models that can
generalize to various video-to-text tasks from few examples, such as
domain-specific captioning, question answering, and future event prediction.
Existing few-shot video-language learners focus exclusively on the encoder,
resulting in the absence of a video-to-text decoder to handle generative tasks.
Video captioners have been pretrained on large-scale video-language datasets,
but they rely heavily on finetuning and lack the ability to generate text for
unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language
Learner via Image and Language models, which demonstrates strong performance on
few-shot video-to-text tasks without the necessity of pretraining or finetuning
on any video datasets. We use the image-language models to translate the video
content into frame captions, object, attribute, and event phrases, and compose
them into a temporal structure template. We then instruct a language model,
with a prompt containing a few in-context examples, to generate a target output
from the composed content. The flexibility of prompting allows the model to
capture any form of text input, such as automatic speech recognition (ASR)
transcripts. Our experiments demonstrate the power of language models in
understanding videos on a wide variety of video-language tasks, including video
captioning, video question answering, video caption retrieval, and video future
event prediction. Especially, on video future event prediction, our few-shot
model significantly outperforms state-of-the-art supervised models trained on
large-scale video datasets. Code and resources are publicly available for
research purposes at https://github.com/MikeWangWZHL/VidIL .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Counting with Crowd-Sourced Supervision. (arXiv:2205.11398v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11398">
<div class="article-summary-box-inner">
<span><p>Crowd-sourcing is an increasingly popular tool for image analysis in animal
ecology. Computer vision methods that can utilize crowd-sourced annotations can
help scale up analysis further. In this work we study the potential to do so on
the challenging task of fine-grained counting. As opposed to the standard crowd
counting task, fine-grained counting also involves classifying attributes of
individuals in dense crowds. We introduce a new dataset from animal ecology to
enable this study that contains 1.7M crowd-sourced annotations of 8
fine-grained classes. It is the largest available dataset for fine-grained
counting and the first to enable the study of the task with crowd-sourced
annotations. We introduce methods for generating aggregate "ground truths" from
the collected annotations, as well as a counting method that can utilize the
aggregate information. Our method improves results by 8% over a comparable
baseline, indicating the potential for algorithms to learn fine-grained
counting using crowd-sourced supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11664">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection (Mono3D) has achieved tremendous improvements
with emerging large-scale autonomous driving datasets and the rapid development
of deep learning techniques. However, caused by severe domain gaps (e.g., the
field of view (FOV), pixel size, and object size among datasets), Mono3D
detectors have difficulty in generalization, leading to drastic performance
degradation on unseen domains. To solve these issues, we combine the
position-invariant transform and multi-scale training with the pixel-size depth
strategy to construct an effective unified camera-generalized paradigm (CGP).
It fully considers discrepancies in the FOV and pixel size of images captured
by different cameras. Moreover, we further investigate the obstacle in
quantitative metrics when cross-dataset inference through an exhaustive
systematic study. We discern that the size bias of prediction leads to a
colossal failure. Hence, we propose the 2D-3D geometry-consistent object
scaling strategy (GCOS) to bridge the gap via an instance-level augment. Our
method called DGMono3D achieves remarkable performance on all evaluated
datasets and surpasses the SoTA unsupervised domain adaptation scheme even
without utilizing data on the target domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet Feature Maps Compression for Image-to-Image CNNs. (arXiv:2205.12268v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12268">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) are known for requiring extensive
computational resources, and quantization is among the best and most common
methods for compressing them. While aggressive quantization (i.e., less than
4-bits) performs well for classification, it may cause severe performance
degradation in image-to-image tasks such as semantic segmentation and depth
estimation. In this paper, we propose Wavelet Compressed Convolution (WCC) -- a
novel approach for high-resolution activation maps compression integrated with
point-wise convolutions, which are the main computational cost of modern
architectures. To this end, we use an efficient and hardware-friendly
Haar-wavelet transform, known for its effectiveness in image compression, and
define the convolution on the compressed activation map. We experiment on
various tasks, that benefit from high-resolution input, and by combining WCC
with light quantization, we achieve compression rates equivalent to 1-4bit
activation quantization with relatively small and much more graceful
degradation in performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Diffusion Models via Early Stop of the Diffusion Process. (arXiv:2205.12524v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12524">
<div class="article-summary-box-inner">
<span><p>Denoising Diffusion Probabilistic Models (DDPMs) have achieved impressive
performance on various generation tasks. By modeling the reverse process of
gradually diffusing the data distribution into a Gaussian distribution,
generating a sample in DDPMs can be regarded as iteratively denoising a
randomly sampled Gaussian noise. However, in practice DDPMs often need hundreds
even thousands of denoising steps to obtain a high-quality sample from the
Gaussian noise, leading to extremely low inference efficiency. In this work, we
propose a principled acceleration strategy, referred to as Early-Stopped DDPM
(ES-DDPM), for DDPMs. The key idea is to stop the diffusion process early where
only the few initial diffusing steps are considered and the reverse denoising
process starts from a non-Gaussian distribution. By further adopting a powerful
pre-trained generative model, such as GAN and VAE, in ES-DDPM, sampling from
the target non-Gaussian distribution can be efficiently achieved by diffusing
samples obtained from the pre-trained generative model. In this way, the number
of required denoising steps is significantly reduced. In the meantime, the
sample quality of ES-DDPM also improves substantially, outperforming both the
vanilla DDPM and the adopted pre-trained generative model. On extensive
experiments across CIFAR-10, CelebA, ImageNet, LSUN-Bedroom and LSUN-Cat,
ES-DDPM obtains promising acceleration effect and performance improvement over
representative baseline methods. Moreover, ES-DDPM also demonstrates several
attractive properties, including being orthogonal to existing acceleration
methods, as well as simultaneously enabling both global semantic and local
pixel-level control in image generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12693">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has achieved a great success in the representation
learning of visual and textual data. However, the current methods are mainly
validated on the well-curated datasets, which do not exhibit the real-world
long-tailed distribution. Recent attempts to consider self-supervised
long-tailed learning are made by rebalancing in the loss perspective or the
model perspective, resembling the paradigms in the supervised long-tailed
learning. Nevertheless, without the aid of labels, these explorations have not
shown the expected significant promise due to the limitation in tail sample
discovery or the heuristic structure design. Different from previous works, we
explore this direction from an alternative perspective, i.e., the data
perspective, and propose a novel Boosted Contrastive Learning (BCL) method.
Specifically, BCL leverages the memorization effect of deep neural networks to
automatically drive the information discrepancy of the sample views in
contrastive learning, which is more efficient to enhance the long-tailed
learning in the label-unaware context. Extensive experiments on a range of
benchmark datasets demonstrate the effectiveness of BCL over several
state-of-the-art methods. Our code is available at
https://github.com/Zhihan-Zhou/Boosted-Contrastive-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing an Efficient End-to-end Machine Learning Pipeline for Real-time Empty-shelf Detection. (arXiv:2205.13060v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13060">
<div class="article-summary-box-inner">
<span><p>On-Shelf Availability (OSA) of products in retail stores is a critical
business criterion in the fast moving consumer goods and retails sector. When a
product is out-of-stock (OOS) and a customer cannot find it on its designed
shelf, this motivates the customer to store-switching or buying nothing, which
causes fall in future sales and demands. Retailers are employing several
approaches to detect empty shelves and ensure high OSA of products; however,
such methods are generally ineffective and infeasible since they are either
manual, expensive or less accurate. Recently machine learning based solutions
have been proposed, but they suffer from high computational cost and low
accuracy problem due to lack of large annotated datasets of on-shelf products.
Here, we present an elegant approach for designing an end-to-end machine
learning (ML) pipeline for real-time empty shelf detection. Considering the
strong dependency between the quality of ML models and the quality of data, we
focus on the importance of proper data collection, cleaning and correct data
annotation before delving into modeling. Since an empty-shelf detection
solution should be computationally-efficient for real-time predictions, we
explore different run-time optimizations to improve the model performance. Our
dataset contains 1000 images, collected and annotated by following well-defined
guidelines. Our low-latency model achieves a mean average F1-score of 68.5%,
and can process up to 67 images/s on Intel Xeon Gold and up to 860 images/s on
an A100 GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning. (arXiv:2205.13137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13137">
<div class="article-summary-box-inner">
<span><p>In this study, we propose Mixed and Masked Image Modeling (MixMIM), a simple
but efficient MIM method that is applicable to various hierarchical Vision
Transformers. Existing MIM methods replace a random subset of input tokens with
a special MASK symbol and aim at reconstructing original image tokens from the
corrupted image. However, we find that using the MASK symbol greatly slows down
the training and causes training-finetuning inconsistency, due to the large
masking ratio (e.g., 40% in BEiT). In contrast, we replace the masked tokens of
one image with visible tokens of another image, i.e., creating a mixed image.
We then conduct dual reconstruction to reconstruct the original two images from
the mixed input, which significantly improves efficiency. While MixMIM can be
applied to various architectures, this paper explores a simpler but stronger
hierarchical Transformer, and scales with MixMIM-B, -L, and -H. Empirical
results demonstrate that MixMIM can learn high-quality visual representations
efficiently. Notably, MixMIM-B with 88M parameters achieves 85.1% top-1
accuracy on ImageNet-1K by pretraining for 600 epochs, setting a new record for
neural networks with comparable model sizes (e.g., ViT-B) among MIM methods.
Besides, its transferring performances on the other 6 datasets show MixMIM has
better FLOPs / performance tradeoff than previous MIM methods. Code is
available at https://github.com/Sense-X/MixMIM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CA-UDA: Class-Aware Unsupervised Domain Adaptation with Optimal Assignment and Pseudo-Label Refinement. (arXiv:2205.13579v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13579">
<div class="article-summary-box-inner">
<span><p>Recent works on unsupervised domain adaptation (UDA) focus on the selection
of good pseudo-labels as surrogates for the missing labels in the target data.
However, source domain bias that deteriorates the pseudo-labels can still exist
since the shared network of the source and target domains are typically used
for the pseudo-label selections. The suboptimal feature space source-to-target
domain alignment can also result in unsatisfactory performance. In this paper,
we propose CA-UDA to improve the quality of the pseudo-labels and UDA results
with optimal assignment, a pseudo-label refinement strategy and class-aware
domain alignment. We use an auxiliary network to mitigate the source domain
bias for pseudo-label refinement. Our intuition is that the underlying
semantics in the target domain can be fully exploited to help refine the
pseudo-labels that are inferred from the source features under domain shift.
Furthermore, our optimal assignment can optimally align features in the
source-to-target domains and our class-aware domain alignment can
simultaneously close the domain gap while preserving the classification
decision boundaries. Extensive experiments on several benchmark datasets show
that our method can achieve state-of-the-art performance in the image
classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TraClets: Harnessing the power of computer vision for trajectory classification. (arXiv:2205.13880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13880">
<div class="article-summary-box-inner">
<span><p>Due to the advent of new mobile devices and tracking sensors in recent years,
huge amounts of data are being produced every day. Therefore, novel
methodologies need to emerge that dive through this vast sea of information and
generate insights and meaningful information. To this end, researchers have
developed several trajectory classification algorithms over the years that are
able to annotate tracking data. Similarly, in this research, a novel
methodology is presented that exploits image representations of trajectories,
called TraClets, in order to classify trajectories in an intuitive humans way,
through computer vision techniques. Several real-world datasets are used to
evaluate the proposed approach and compare its classification performance to
other state-of-the-art trajectory classification algorithms. Experimental
results demonstrate that TraClets achieves a classification performance that is
comparable to, or in most cases, better than the state-of-the-art, acting as a
universal, high-accuracy approach for trajectory classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sharpness-Aware Training for Free. (arXiv:2205.14083v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14083">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks (DNNs) have achieved state-of-the-art
performances but are typically over-parameterized. The over-parameterization
may result in undesirably large generalization error in the absence of other
customized training strategies. Recently, a line of research under the name of
Sharpness-Aware Minimization (SAM) has shown that minimizing a sharpness
measure, which reflects the geometry of the loss landscape, can significantly
reduce the generalization error. However, SAM-like methods incur a two-fold
computational overhead of the given base optimizer (e.g. SGD) for approximating
the sharpness measure. In this paper, we propose Sharpness-Aware Training for
Free, or SAF, which mitigates the sharp landscape at almost zero additional
computational cost over the base optimizer. Intuitively, SAF achieves this by
avoiding sudden drops in the loss in the sharp local minima throughout the
trajectory of the updates of the weights. Specifically, we suggest a novel
trajectory loss, based on the KL-divergence between the outputs of DNNs with
the current weights and past weights, as a replacement of the SAM's sharpness
measure. This loss captures the rate of change of the training loss along the
model's update trajectory. By minimizing it, SAF ensures the convergence to a
flat minimum with improved generalization capabilities. Extensive empirical
results show that SAF minimizes the sharpness in the same way that SAM does,
yielding better results on the ImageNet dataset with essentially the same
computational cost as the base optimizer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenCalib: A Multi-sensor Calibration Toolbox for Autonomous Driving. (arXiv:2205.14087v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14087">
<div class="article-summary-box-inner">
<span><p>Accurate sensor calibration is a prerequisite for multi-sensor perception and
localization systems for autonomous vehicles. The intrinsic parameter
calibration of the sensor is to obtain the mapping relationship inside the
sensor, and the extrinsic parameter calibration is to transform two or more
sensors into a unified spatial coordinate system. Most sensors need to be
calibrated after installation to ensure the accuracy of sensor measurements. To
this end, we present OpenCalib, a calibration toolbox that contains a rich set
of various sensor calibration methods. OpenCalib covers manual calibration
tools, automatic calibration tools, factory calibration tools, and online
calibration tools for different application scenarios. At the same time, to
evaluate the calibration accuracy and subsequently improve the accuracy of the
calibration algorithm, we released a corresponding benchmark dataset. This
paper introduces various features and calibration methods of this toolbox. To
our knowledge, this is the first open-sourced calibration codebase containing
the full set of autonomous-driving-related calibration approaches in this area.
We wish that the toolbox could be helpful to autonomous driving researchers. We
have open-sourced our code on GitHub to benefit the community. Code is
available at https://github.com/PJLab-ADG/SensorsCalibration.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-31 23:08:39.774911760 UTC">2022-05-31 23:08:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>