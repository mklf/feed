<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-13T01:30:00Z">09-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04500">
<div class="article-summary-box-inner">
<span><p>We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for
semantic parsing in task-oriented dialog. Our model consists of an encoder
network that incrementally builds the semantic parse tree by predicting the
non-terminal label and its positions in the linearized tree. At the generation
time, the model constructs the semantic parse tree by recursively inserting the
predicted non-terminal labels at the predicted positions until termination.
RINE achieves state-of-the-art exact match accuracy on low- and high-resource
versions of the conversational semantic parsing benchmark TOP (Gupta et al.,
2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and
transition-based parsers. We also show that our model design is applicable to
nested named entity recognition task, where it performs on par with
state-of-the-art approach designed for that task. Finally, we demonstrate that
our approach is 2-3.5 times faster than the sequence-to-sequence model at
inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04513">
<div class="article-summary-box-inner">
<span><p>We present models which complete missing text given transliterations of
ancient Mesopotamian documents, originally written on cuneiform clay tablets
(2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on
contextual cues to manually fill in missing parts in the text in a subjective
and time-consuming process. We identify that this challenge can be formulated
as a masked language modelling task, used mostly as a pretraining objective for
contextualized language models. Following, we develop several architectures
focusing on the Akkadian language, the lingua franca of the time. We find that
despite data scarcity (1M tokens) we can achieve state of the art performance
on missing tokens prediction (89% hit@5) using a greedy decoding scheme and
pretraining on data from other languages and different time periods. Finally,
we conduct human evaluations showing the applicability of our models in
assisting experts to transcribe texts in extinct languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04535">
<div class="article-summary-box-inner">
<span><p>Extracting moral sentiment from text is a vital component in understanding
public opinion, social movements, and policy decisions. The Moral Foundation
Theory identifies five moral foundations, each associated with a positive and
negative polarity. However, moral sentiment is often motivated by its targets,
which can correspond to individuals or collective entities. In this paper, we
introduce morality frames, a representation framework for organizing moral
attitudes directed at different entities, and come up with a novel and
high-quality annotated dataset of tweets written by US politicians. Then, we
propose a relational learning model to predict moral attitudes towards entities
and moral foundations jointly. We do qualitative and quantitative evaluations,
showing that moral sentiment towards entities differs highly across political
ideologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generic resources are what you need: Style transfer tasks without task-specific parallel training data. (arXiv:2109.04543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04543">
<div class="article-summary-box-inner">
<span><p>Style transfer aims to rewrite a source text in a different target style
while preserving its content. We propose a novel approach to this task that
leverages generic resources, and without using any task-specific parallel
(source-target) data outperforms existing unsupervised approaches on the two
most popular style transfer tasks: formality transfer and polarity swap. In
practice, we adopt a multi-step procedure which builds on a generic pre-trained
sequence-to-sequence model (BART). First, we strengthen the model's ability to
rewrite by further pre-training BART on both an existing collection of generic
paraphrases, as well as on synthetic pairs created using a general-purpose
lexical resource. Second, through an iterative back-translation approach, we
train two models, each in a transfer direction, so that they can provide each
other with synthetically generated pairs, dynamically in the training process.
Lastly, we let our best reresulting model generate static synthetic pairs to be
used in a supervised training regime. Besides methodology and state-of-the-art
results, a core contribution of this work is a reflection on the nature of the
two tasks we address, and how their differences are highlighted by their
response to our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints. (arXiv:2109.04546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04546">
<div class="article-summary-box-inner">
<span><p>We study the problem of generating arithmetic math word problems (MWPs) given
a math equation that specifies the mathematical computation and a context that
specifies the problem scenario. Existing approaches are prone to generating
MWPs that are either mathematically invalid or have unsatisfactory language
quality. They also either ignore the context or require manual specification of
a problem template, which compromises the diversity of the generated MWPs. In
this paper, we develop a novel MWP generation approach that leverages i)
pre-trained language models and a context keyword selection model to improve
the language quality of the generated MWPs and ii) an equation consistency
constraint for math equations to improve the mathematical validity of the
generated MWPs. Extensive quantitative and qualitative experiments on three
real-world MWP datasets demonstrate the superior performance of our approach
compared to various baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04550">
<div class="article-summary-box-inner">
<span><p>Temporal Knowledge Graphs store events in the form of subjects, relations,
objects, and timestamps which are often represented by dynamic heterogeneous
graphs. Event forecasting is a critical and challenging task in Temporal
Knowledge Graph reasoning that predicts the subject or object of an event in
the future. To obtain temporal embeddings multi-step away in the future,
existing methods learn generative models that capture the joint distribution of
the observed events. To reduce the high computation costs, these methods rely
on unrealistic assumptions of independence and approximations in training and
inference. In this work, we propose SeDyT, a discriminative framework that
performs sequence modeling on the dynamic entity embeddings to solve the
multi-step event forecasting problem. SeDyT consists of two components: a
Temporal Graph Neural Network that generates dynamic entity embeddings in the
past and a sequence model that predicts the entity embeddings in the future.
Compared with the generative models, SeDyT does not rely on any heuristic-based
probability model and has low computation complexity in both training and
inference. SeDyT is compatible with most Temporal Graph Neural Networks and
sequence models. We also design an efficient training method that trains the
two components in one gradient descent propagation. We evaluate the performance
of SeDyT on five popular datasets. By combining temporal Graph Neural Network
models and sequence models, SeDyT achieves an average of 2.4% MRR improvement
when not using the validation set and more than 10% MRR improvement when using
the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04552">
<div class="article-summary-box-inner">
<span><p>Selective rationalization aims to produce decisions along with rationales
(e.g., text highlights or word alignments between two sentences). Commonly,
rationales are modeled as stochastic binary masks, requiring sampling-based
gradient estimators, which complicates training and requires careful
hyperparameter tuning. Sparse attention mechanisms are a deterministic
alternative, but they lack a way to regularize the rationale extraction (e.g.,
to control the sparsity of a text highlight or the number of alignments). In
this paper, we present a unified framework for deterministic extraction of
structured explanations via constrained inference on a factor graph, forming a
differentiable layer. Our approach greatly eases training and rationale
regularization, generally outperforming previous work on what comes to
performance and plausibility of the extracted rationales. We further provide a
comparative study of stochastic and deterministic methods for rationale
extraction for classification and natural language inference tasks, jointly
assessing their predictive power, quality of the explanations, and model
variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subword Mapping and Anchoring across Languages. (arXiv:2109.04556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04556">
<div class="article-summary-box-inner">
<span><p>State-of-the-art multilingual systems rely on shared vocabularies that
sufficiently cover all considered languages. To this end, a simple and
frequently used approach makes use of subword vocabularies constructed jointly
over several languages. We hypothesize that such vocabularies are suboptimal
due to false positives (identical subwords with different meanings across
languages) and false negatives (different subwords with similar meanings). To
address these issues, we propose Subword Mapping and Anchoring across Languages
(SMALA), a method to construct bilingual subword vocabularies. SMALA extracts
subword alignments using an unsupervised state-of-the-art mapping technique and
uses them to create cross-lingual anchors based on subword similarities. We
demonstrate the benefits of SMALA for cross-lingual natural language inference
(XNLI), where it improves zero-shot transfer to an unseen language without
task-specific data, but only by sharing subword embeddings. Moreover, in neural
machine translation, we show that joint subword vocabularies obtained with
SMALA lead to higher BLEU scores on sentences that contain many false positives
and false negatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling. (arXiv:2109.04562v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04562">
<div class="article-summary-box-inner">
<span><p>Human conversations naturally evolve around different topics and fluently
move between them. In research on dialog systems, the ability to actively and
smoothly transition to new topics is often ignored. In this paper we introduce
TIAGE, a new topic-shift aware dialog benchmark constructed utilizing human
annotations on topic shifts. Based on TIAGE, we introduce three tasks to
investigate different scenarios of topic-shift modeling in dialog settings:
topic-shift detection, topic-shift triggered response generation and
topic-aware dialog generation. Experiments on these tasks show that the
topic-shift signals in TIAGE are useful for topic-shift response generation. On
the other hand, dialog systems still struggle to decide when to change topic.
This indicates further research is needed in topic-shift aware dialog modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speechformer: Reducing Information Loss in Direct Speech Translation. (arXiv:2109.04574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04574">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have gained increasing popularity achieving
state-of-the-art performance in many research fields including speech
translation. However, Transformer's quadratic complexity with respect to the
input sequence length prevents its adoption as is with audio signals, which are
typically represented by long sequences. Current solutions resort to an initial
sub-optimal compression based on a fixed sampling of raw audio features.
Therefore, potentially useful linguistic information is not accessible to
higher-level layers in the architecture. To solve this issue, we propose
Speechformer, an architecture that, thanks to reduced memory usage in the
attention layers, avoids the initial lossy compression and aggregates
information only at a higher level according to more informed linguistic
criteria. Experiments on three language pairs (en-&gt;de/es/nl) show the efficacy
of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and
of up to 4.0 BLEU in a low resource scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04587">
<div class="article-summary-box-inner">
<span><p>The dominant paradigm for semantic parsing in recent years is to formulate
parsing as a sequence-to-sequence task, generating predictions with
auto-regressive sequence decoders. In this work, we explore an alternative
paradigm. We formulate semantic parsing as a dependency parsing task, applying
graph-based decoding techniques developed for syntactic parsing. We compare
various decoding techniques given the same pre-trained Transformer encoder on
the TOP dataset, including settings where training data is limited or contains
only partially-annotated examples. We find that our graph-based approach is
competitive with sequence decoders on the standard setting, and offers
significant improvements in data efficiency and settings where
partially-annotated data is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation. (arXiv:2109.04588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04588">
<div class="article-summary-box-inner">
<span><p>The success of bidirectional encoders using masked language models, such as
BERT, on numerous natural language processing tasks has prompted researchers to
attempt to incorporate these pre-trained models into neural machine translation
(NMT) systems. However, proposed methods for incorporating pre-trained models
are non-trivial and mainly focus on BERT, which lacks a comparison of the
impact that other pre-trained models may have on translation performance. In
this paper, we demonstrate that simply using the output (contextualized
embeddings) of a tailored and suitable bilingual pre-trained language model
(dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art
translation performance. Moreover, we also propose a stochastic layer selection
approach and a concept of dual-directional translation model to ensure the
sufficient utilization of contextualized embeddings. In the case of without
using back translation, our best models achieve BLEU scores of 30.45 for En-&gt;De
and 38.61 for De-&gt;En on the IWSLT'14 dataset, and 31.26 for En-&gt;De and 34.94
for De-&gt;En on the WMT'14 dataset, which exceeds all published numbers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04593">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural machine translation (NMT) have pushed the quality
of machine translation systems to the point where they are becoming widely
adopted to build competitive systems. However, there is still a large number of
languages that are yet to reap the benefits of NMT. In this paper, we provide
the first large-scale case study of the practical application of MT in the
Turkic language family in order to realize the gains of NMT for Turkic
languages under high-resource to extremely low-resource scenarios. In addition
to presenting an extensive analysis that identifies the bottlenecks towards
building competitive systems to ameliorate data scarcity, our study has several
key contributions, including, i) a large parallel corpus covering 22 Turkic
languages consisting of common public datasets in combination with new datasets
of approximately 2 million parallel sentences, ii) bilingual baselines for 26
language pairs, iii) novel high-quality test sets in three different
translation domains and iv) human evaluation scores. All models, scripts, and
data will be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04600">
<div class="article-summary-box-inner">
<span><p>Temporal grounding aims to predict a time interval of a video clip
corresponding to a natural language query input. In this work, we present
EVOQUER, a temporal grounding framework incorporating an existing text-to-video
grounding model and a video-assisted query generation network. Given a query
and an untrimmed video, the temporal grounding model predicts the target
interval, and the predicted video clip is fed into a video translation task by
generating a simplified version of the input query. EVOQUER forms closed-loop
learning by incorporating loss functions from both temporal grounding and query
generation serving as feedback. Our experiments on two widely used datasets,
Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements
by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could
facilitate error analysis by explaining temporal grounding model behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations. (arXiv:2109.04602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04602">
<div class="article-summary-box-inner">
<span><p>Current language models are usually trained using a self-supervised scheme,
where the main focus is learning representations at the word or sentence level.
However, there has been limited progress in generating useful discourse-level
representations. In this work, we propose to use ideas from predictive coding
theory to augment BERT-style language models with a mechanism that allows them
to learn suitable discourse-level representations. As a result, our proposed
approach is able to predict future sentences using explicit top-down
connections that operate at the intermediate layers of the network. By
experimenting with benchmarks designed to evaluate discourse-related knowledge
using pre-trained sentence representations, we demonstrate that our approach
improves performance in 6 out of 11 tasks by excelling in discourse
relationship detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04604">
<div class="article-summary-box-inner">
<span><p>The general goal of text simplification (TS) is to reduce text complexity for
human consumption. This paper investigates another potential use of neural TS:
assisting machines performing natural language processing (NLP) tasks. We
evaluate the use of neural TS in two ways: simplifying input texts at
prediction time and augmenting data to provide machines with additional
information during training. We demonstrate that the latter scenario provides
positive effects on machine performance on two separate datasets. In
particular, the latter use of TS improves the performances of LSTM (1.82-1.98%)
and SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,
real-world relation extraction task. Further, the same setting yields
improvements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT
text classifier on MNLI, a practical natural language inference dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization. (arXiv:2109.04607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04607">
<div class="article-summary-box-inner">
<span><p>We present IndoBERTweet, the first large-scale pretrained model for
Indonesian Twitter that is trained by extending a monolingually-trained
Indonesian BERT model with additive domain-specific vocabulary. We focus in
particular on efficient model adaptation under vocabulary mismatch, and
benchmark different ways of initializing the BERT embedding layer for new word
types. We find that initializing with the average BERT subword embedding makes
pretraining five times faster, and is more effective than proposed methods for
vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploratory Study on Long Dialogue Summarization: What Works and What's Next. (arXiv:2109.04609v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04609">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization helps readers capture salient information from long
conversations in meetings, interviews, and TV series. However, real-world
dialogues pose a great challenge to current summarization models, as the
dialogue length typically exceeds the input limits imposed by recent
transformer-based pre-trained models, and the interactive nature of dialogues
makes relevant information more context-dependent and sparsely distributed than
news articles. In this work, we perform a comprehensive study on long dialogue
summarization by investigating three strategies to deal with the lengthy input
problem and locate relevant information: (1) extended transformer models such
as Longformer, (2) retrieve-then-summarize pipeline models with several
dialogue utterance retrieval methods, and (3) hierarchical dialogue encoding
models such as HMNet. Our experimental results on three long dialogue datasets
(QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline
models yield the best performance. We also demonstrate that the summary quality
can be further improved with a stronger retrieval model and pretraining on
proper external summarization datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04611">
<div class="article-summary-box-inner">
<span><p>Transformer-based rankers have shown state-of-the-art performance. However,
their self-attention operation is mostly unable to process long sequences. One
of the common approaches to train these rankers is to heuristically select some
segments of each document, such as the first segment, as training data.
However, these segments may not contain the query-related parts of documents.
To address this problem, we propose query-driven segment selection from long
documents to build training data. The segment selector provides relevant
samples with more accurate labels and non-relevant samples which are harder to
be predicted. The experimental results show that the basic BERT-based ranker
trained with the proposed segment selector significantly outperforms that
trained by the heuristically selected segments, and performs equally to the
state-of-the-art model with localized self-attention that can process longer
input sequences. Our findings open up new direction to design efficient
transformer-based rankers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04620">
<div class="article-summary-box-inner">
<span><p>Current approaches to incorporating terminology constraints in machine
translation (MT) typically assume that the constraint terms are provided in
their correct morphological forms. This limits their application to real-world
scenarios where constraint terms are provided as lemmas. In this paper, we
introduce a modular framework for incorporating lemma constraints in neural MT
(NMT) in which linguistic knowledge and diverse types of NMT models can be
flexibly applied. It is based on a novel cross-lingual inflection module that
inflects the target lemma constraints based on the source context. We explore
linguistically motivated rule-based and data-driven neural-based inflection
modules and design English-German health and English-Lithuanian news test
suites to evaluate them in domain adaptation and low-resource MT settings.
Results show that our rule-based inflection module helps NMT models incorporate
lemma constraints more accurately than a neural module and outperforms the
existing end-to-end approach with lower training costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04645">
<div class="article-summary-box-inner">
<span><p>As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema(definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5)is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. (arXiv:2109.04650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04650">
<div class="article-summary-box-inner">
<span><p>GPT-3 shows remarkable in-context learning ability of large-scale language
models (LMs) trained on hundreds of billion scale data. Here we address some
remaining issues less reported by the GPT-3 paper, such as a non-English LM,
the performances of different sized models, and the effect of recently
introduced prompt optimization on in-context learning. To achieve this, we
introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric
corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA
with our training configuration shows state-of-the-art in-context zero-shot and
few-shot learning performances on various downstream tasks in Korean. Also, we
show the performance benefits of prompt-based learning and demonstrate how it
can be integrated into the prompt engineering pipeline. Then we discuss the
possibility of materializing the No Code AI paradigm by providing AI
prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,
an interactive prompt engineering interface. Lastly, we demonstrate the
potential of our methods with three successful in-house applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining. (arXiv:2109.04652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04652">
<div class="article-summary-box-inner">
<span><p>Natural language relies on a finite lexicon to express an unbounded set of
emerging ideas. One result of this tension is the formation of new
compositions, such that existing linguistic units can be combined with emerging
items into novel expressions. We develop a framework that exploits the
cognitive mechanisms of chaining and multimodal knowledge to predict emergent
compositional expressions through time. We present the syntactic frame
extension model (SFEM) that draws on the theory of chaining and knowledge from
"percept", "concept", and "language" to infer how verbs extend their frames to
form new compositions with existing and novel nouns. We evaluate SFEM
rigorously on the 1) modalities of knowledge and 2) categorization models of
chaining, in a syntactically parsed English corpus over the past 150 years. We
show that multimodal SFEM predicts newly emerged verb syntax and arguments
substantially better than competing models using purely linguistic or unimodal
knowledge. We find support for an exemplar view of chaining as opposed to a
prototype view and reveal how the joint approach of multimodal chaining may be
fundamental to the creation of literal and figurative language uses including
metaphor and metonymy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation. (arXiv:2109.04653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04653">
<div class="article-summary-box-inner">
<span><p>Pre-trained language-vision models have shown remarkable performance on the
visual question answering (VQA) task. However, most pre-trained models are
trained by only considering monolingual learning, especially the resource-rich
language like English. Training such models for multilingual setups demand high
computing resources and multilingual language-vision dataset which hinders
their application in practice. To alleviate these challenges, we propose a
knowledge distillation approach to extend an English language-vision model
(teacher) into an equally effective multilingual and code-mixed model
(student). Unlike the existing knowledge distillation methods, which only use
the output from the last layer of the teacher network for distillation, our
student model learns and imitates the teacher from multiple intermediate layers
(language and vision encoders) with appropriately designed distillation
objectives for incremental knowledge extraction. We also create the large-scale
multilingual and code-mixed VQA dataset in eleven different language setups
considering the multiple Indian and European languages. Experimental results
and in-depth analysis show the effectiveness of the proposed VQA model over the
pre-trained language-vision models on eleven diverse language setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Dialogue State Tracking via Cross-Task Transfer. (arXiv:2109.04655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04655">
<div class="article-summary-box-inner">
<span><p>Zero-shot transfer learning for dialogue state tracking (DST) enables us to
handle a variety of task-oriented dialogue domains without the expense of
collecting in-domain data. In this work, we propose to transfer the
\textit{cross-task} knowledge from general question answering (QA) corpora for
the zero-shot DST task. Specifically, we propose TransferQA, a transferable
generative QA model that seamlessly combines extractive QA and multi-choice QA
via a text-to-text transformer framework, and tracks both categorical slots and
non-categorical slots in DST. In addition, we introduce two effective ways to
construct unanswerable questions, namely, negative question sampling and
context truncation, which enable our model to handle "none" value slots in the
zero-shot DST setting. The extensive experiments show that our approaches
substantially improve the existing zero-shot and few-shot results on MultiWoz.
Moreover, compared to the fully trained baseline on the Schema-Guided Dialogue
dataset, our approach shows better generalization ability in unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Euphemistic Phrase Detection by Masked Language Model. (arXiv:2109.04666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04666">
<div class="article-summary-box-inner">
<span><p>It is a well-known approach for fringe groups and organizations to use
euphemisms -- ordinary-sounding and innocent-looking words with a secret
meaning -- to conceal what they are discussing. For instance, drug dealers
often use "pot" for marijuana and "avocado" for heroin. From a social media
content moderation perspective, though recent advances in NLP have enabled the
automatic detection of such single-word euphemisms, no existing work is capable
of automatically detecting multi-word euphemisms, such as "blue dream"
(marijuana) and "black tar" (heroin). Our paper tackles the problem of
euphemistic phrase detection without human effort for the first time, as far as
we are aware. We first perform phrase mining on a raw text corpus (e.g., social
media posts) to extract quality phrases. Then, we utilize word embedding
similarities to select a set of euphemistic phrase candidates. Finally, we rank
those candidates by a masked language model -- SpanBERT. Compared to strong
baselines, we report 20-50% higher detection accuracies using our algorithm for
detecting euphemistic phrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04672">
<div class="article-summary-box-inner">
<span><p>The transformer-based pre-trained language models have been tremendously
successful in most of the conventional NLP tasks. But they often struggle in
those tasks where numerical understanding is required. Some possible reasons
can be the tokenizers and pre-training objectives which are not specifically
designed to learn and preserve numeracy. Here we investigate the ability of
text-to-text transfer learning model (T5), which has outperformed its
predecessors in the conventional NLP tasks, to learn numeracy. We consider four
numeracy tasks: numeration, magnitude order prediction, finding minimum and
maximum in a series, and sorting. We find that, although T5 models perform
reasonably well in the interpolation setting, they struggle considerably in the
extrapolation setting across all four tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization. (arXiv:2109.04673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04673">
<div class="article-summary-box-inner">
<span><p>Identifying relevant knowledge to be used in conversational systems that are
grounded in long documents is critical to effective response generation. We
introduce a knowledge identification model that leverages the document
structure to provide dialogue-contextualized passage encodings and better
locate knowledge relevant to the conversation. An auxiliary loss captures the
history of dialogue-document connections. We demonstrate the effectiveness of
our model on two document-grounded conversational datasets and provide analyses
showing generalization to unseen documents and long dialogue contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04689">
<div class="article-summary-box-inner">
<span><p>Motivated by suggested question generation in conversational news
recommendation systems, we propose a model for generating question-answer pairs
(QA pairs) with self-contained, summary-centric questions and
length-constrained, article-summarizing answers. We begin by collecting a new
dataset of news articles with questions as titles and pairing them with
summaries of varying length. This dataset is used to learn a QA pair generation
model producing summaries as answers that balance brevity with sufficiency
jointly with their corresponding questions. We then reinforce the QA pair
generation process with a differentiable reward function to mitigate exposure
bias, a common problem in natural language generation. Both automatic metrics
and human evaluation demonstrate these QA pairs successfully capture the
central gists of the articles and achieve high answer accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
<div class="article-summary-box-inner">
<span><p>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterogeneous Graph Neural Networks for Keyphrase Generation. (arXiv:2109.04703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04703">
<div class="article-summary-box-inner">
<span><p>The encoder-decoder framework achieves state-of-the-art results in keyphrase
generation (KG) tasks by predicting both present keyphrases that appear in the
source document and absent keyphrases that do not. However, relying solely on
the source document can result in generating uncontrollable and inaccurate
absent keyphrases. To address these problems, we propose a novel graph-based
method that can capture explicit knowledge from related references. Our model
first retrieves some document-keyphrases pairs similar to the source document
from a pre-defined index as references. Then a heterogeneous graph is
constructed to capture relationships of different granularities between the
source document and its references. To guide the decoding process, a
hierarchical attention and copy mechanism is introduced, which directly copies
appropriate words from both the source document and its references based on
their relevance and significance. The experimental results on multiple KG
benchmarks show that the proposed model achieves significant improvements
against other baseline models, especially with regard to the absent keyphrase
prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables. (arXiv:2109.04705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04705">
<div class="article-summary-box-inner">
<span><p>Zero-shot translation, directly translating between language pairs unseen in
training, is a promising capability of multilingual neural machine translation
(NMT). However, it usually suffers from capturing spurious correlations between
the output language and language invariant semantics due to the maximum
likelihood training objective, leading to poor transfer performance on
zero-shot translation. In this paper, we introduce a denoising autoencoder
objective based on pivot language into traditional training objective to
improve the translation accuracy on zero-shot directions. The theoretical
analysis from the perspective of latent variables shows that our approach
actually implicitly maximizes the probability distributions for zero-shot
directions. On two benchmark machine translation datasets, we demonstrate that
the proposed method is able to effectively eliminate the spurious correlations
and significantly outperforms state-of-the-art methods with a remarkable
performance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04707">
<div class="article-summary-box-inner">
<span><p>Meta-learning has achieved great success in leveraging the historical learned
knowledge to facilitate the learning process of the new task. However, merely
learning the knowledge from the historical tasks, adopted by current
meta-learning algorithms, may not generalize well to testing tasks when they
are not well-supported by training tasks. This paper studies a low-resource
text classification problem and bridges the gap between meta-training and
meta-testing tasks by leveraging the external knowledge bases. Specifically, we
propose KGML to introduce additional representation for each sentence learned
from the extracted sentence-specific knowledge graph. The extensive experiments
on three datasets demonstrate the effectiveness of KGML under both supervised
adaptation and unsupervised adaptation settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04708">
<div class="article-summary-box-inner">
<span><p>The majority of language domains require prudent use of terminology to ensure
clarity and adequacy of information conveyed. While the correct use of
terminology for some languages and domains can be achieved by adapting
general-purpose MT systems on large volumes of in-domain parallel data, such
quantities of domain-specific data are seldom available for less-resourced
languages and niche domains. Furthermore, as exemplified by COVID-19 recently,
no domain-specific parallel data is readily available for emerging domains.
However, the gravity of this recent calamity created a high demand for reliable
translation of critical information regarding pandemic and infection
prevention. This work is part of WMT2021 Shared Task: Machine Translation using
Terminologies, where we describe Tilde MT systems that are capable of dynamic
terminology integration at the time of translation. Our systems achieve up to
94% COVID-19 term use accuracy on the test set of the EN-FR language pair
without having access to any form of in-domain information during system
training. We conclude our work with a broader discussion considering the Shared
Task itself and terminology translation in MT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-train or Annotate? Domain Adaptation with a Constrained Budget. (arXiv:2109.04711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04711">
<div class="article-summary-box-inner">
<span><p>Recent work has demonstrated that pre-training in-domain language models can
boost performance when adapting to a new domain. However, the costs associated
with pre-training raise an important question: given a fixed budget, what steps
should an NLP practitioner take to maximize performance? In this paper, we
study domain adaptation under budget constraints, and approach it as a customer
choice problem between data annotation and pre-training. Specifically, we
measure the annotation cost of three procedural text datasets and the
pre-training cost of three in-domain language models. Then we evaluate the
utility of different combinations of pre-training and data annotation under
varying budget constraints to assess which combination strategy works best. We
find that, for small budgets, spending all funds on annotation leads to the
best performance; once the budget becomes large enough, a combination of data
annotation and in-domain pre-training works more optimally. We therefore
suggest that task-specific data annotation should be part of an economical
strategy when adapting an NLP model to a new domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04712">
<div class="article-summary-box-inner">
<span><p>Multi-label text classification is a challenging task because it requires
capturing label dependencies. It becomes even more challenging when class
distribution is long-tailed. Resampling and re-weighting are common approaches
used for addressing the class imbalance problem, however, they are not
effective when there is label dependency besides class imbalance because they
result in oversampling of common labels. Here, we introduce the application of
balancing loss functions for multi-label text classification. We perform
experiments on a general domain dataset with 90 labels (Reuters-21578) and a
domain-specific dataset from PubMed with 18211 labels. We find that a
distribution-balanced loss function, which inherently addresses both the class
imbalance and label linkage problems, outperforms commonly used loss functions.
Distribution balancing methods have been successfully used in the image
recognition field. Here, we show their effectiveness in natural language
processing. Source code is available at
https://github.com/blessu/BalancedLossNLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages. (arXiv:2109.04715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04715">
<div class="article-summary-box-inner">
<span><p>Reproducible benchmarks are crucial in driving progress of machine
translation research. However, existing machine translation benchmarks have
been mostly limited to high-resource or well-represented languages. Despite an
increasing interest in low-resource machine translation, there are no
standardized reproducible benchmarks for many African languages, many of which
are used by millions of speakers but have less digitized textual data. To
tackle these challenges, we propose AfroMT, a standardized, clean, and
reproducible machine translation benchmark for eight widely spoken African
languages. We also develop a suite of analysis tools for system diagnosis
taking into account the unique properties of these languages. Furthermore, we
explore the newly considered case of low-resource focused pretraining and
develop two novel data augmentation-based strategies, leveraging word-level
alignment information and pseudo-monolingual data for pretraining multilingual
sequence-to-sequence models. We demonstrate significant improvements when
pretraining on 11 languages, with gains of up to 2 BLEU points over strong
baselines. We also show gains of up to 12 BLEU points over cross-lingual
transfer baselines in data-constrained scenarios. All code and pretrained
models will be released as further steps towards larger reproducible benchmarks
for African languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04726">
<div class="article-summary-box-inner">
<span><p>Deep neural models for low-resource named entity recognition (NER) have shown
impressive results by leveraging distant super-vision or other meta-level
information (e.g. explanation). However, the costs of acquiring such additional
information are generally prohibitive, especially in domains where existing
resources (e.g. databases to be used for distant supervision) may not exist. In
this paper, we present a novel two-stage framework (AutoTriggER) to improve NER
performance by automatically generating and leveraging "entity triggers" which
are essentially human-readable clues in the text that can help guide the model
to make better decisions. Thus, the framework is able to both create and
leverage auxiliary supervision by itself. Through experiments on three
well-studied NER datasets, we show that our automatically extracted triggers
are well-matched to human triggers, and AutoTriggER improves performance over a
RoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a
low resource setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations. (arXiv:2109.04727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04727">
<div class="article-summary-box-inner">
<span><p>Language agnostic and semantic-language information isolation is an emerging
research direction for multilingual representations models. We explore this
problem from a novel angle of geometric algebra and semantic space. A simple
but highly effective method "Language Information Removal (LIR)" factors out
language identity information from semantic related components in multilingual
representations pre-trained on multi-monolingual data. A post-training and
model-agnostic method, LIR only uses simple linear operations, e.g. matrix
factorization and orthogonal projection. LIR reveals that for weak-alignment
multilingual systems, the principal components of semantic spaces primarily
encodes language identity information. We first evaluate the LIR on a
cross-lingual question answer retrieval task (LAReQA), which requires the
strong alignment for the multilingual embedding space. Experiment shows that
LIR is highly effectively on this task, yielding almost 100% relative
improvement in MAP for weak-alignment models. We then evaluate the LIR on
Amazon Reviews and XEVAL dataset, with the observation that removing language
information is able to improve the cross-lingual transfer performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing the Reliability of Word Embedding Gender Bias Measures. (arXiv:2109.04732v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04732">
<div class="article-summary-box-inner">
<span><p>Various measures have been proposed to quantify human-like social biases in
word embeddings. However, bias scores based on these measures can suffer from
measurement error. One indication of measurement quality is reliability,
concerning the extent to which a measure produces consistent results. In this
paper, we assess three types of reliability of word embedding gender bias
measures, namely test-retest reliability, inter-rater consistency and internal
consistency. Specifically, we investigate the consistency of bias scores across
different choices of random seeds, scoring rules and words. Furthermore, we
analyse the effects of various factors on these measures' reliability scores.
Our findings inform better design of word embedding gender bias measures.
Moreover, we urge researchers to be more critical about the application of such
measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Genre as Weak Supervision for Cross-lingual Dependency Parsing. (arXiv:2109.04733v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04733">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that monolingual masked language models learn to
represent data-driven notions of language variation which can be used for
domain-targeted training data selection. Dataset genre labels are already
frequently available, yet remain largely unexplored in cross-lingual setups. We
harness this genre metadata as a weak supervision signal for targeted data
selection in zero-shot dependency parsing. Specifically, we project
treebank-level genre information to the finer-grained sentence level, with the
goal to amplify information implicitly stored in unsupervised contextualized
representations. We demonstrate that genre is recoverable from multilingual
contextual embeddings and that it provides an effective signal for training
data selection in cross-lingual, zero-shot scenarios. For 12 low-resource
language treebanks, six of which are test-only, our genre-specific methods
significantly outperform competitive baselines as well as recent
embedding-based methods for data selection. Moreover, genre-based data
selection provides new state-of-the-art results for three of these target
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy. (arXiv:2109.04740v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04740">
<div class="article-summary-box-inner">
<span><p>It is widely accepted that fine-tuning pre-trained language models usually
brings about performance improvements in downstream tasks. However, there are
limited studies on the reasons behind this effectiveness, particularly from the
viewpoint of structural changes in the embedding space. Trying to fill this
gap, in this paper, we analyze the extent to which the isotropy of the
embedding space changes after fine-tuning. We demonstrate that, even though
isotropy is a desirable geometrical property, fine-tuning does not necessarily
result in isotropy enhancements. Moreover, local structures in pre-trained
contextual word representations (CWRs), such as those encoding token types or
frequency, undergo a massive change during fine-tuning. Our experiments show
dramatic growth in the number of elongated directions in the embedding space,
which, in contrast to pre-trained CWRs, carry the essential linguistic
knowledge in the fine-tuned embedding space, making existing isotropy
enhancement methods ineffective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04762">
<div class="article-summary-box-inner">
<span><p>Text classification systems based on contextual embeddings are not viable
options for many of the low resource languages. On the other hand, recently
introduced capsule networks have shown performance in par with these text
classification models. Thus, they could be considered as a viable alternative
for text classification for languages that do not have pre-trained contextual
embedding models. However, current capsule networks depend upon spatial
patterns without considering the sequential features of the text. They are also
sub-optimal in capturing the context-level information in longer sequences.
This paper presents a novel Dual-State Capsule (DS-Caps) network-based
technique for text classification, which is optimized to mitigate these issues.
Two varieties of states, namely sentence-level and word-level, are integrated
with capsule layers to capture deeper context-level information for language
modeling. The dynamic routing process among capsules was also optimized using
the context-level information obtained through sentence-level states. The
DS-Caps networks outperform the existing capsule network architectures for
multiple datasets, particularly for tasks with longer sequences of text. We
also demonstrate the superiority of DS-Caps in text classification for a low
resource language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Strong Baseline for Query Efficient Attacks in a Black Box Setting. (arXiv:2109.04775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04775">
<div class="article-summary-box-inner">
<span><p>Existing black box search methods have achieved high success rate in
generating adversarial attacks against NLP models. However, such search methods
are inefficient as they do not consider the amount of queries required to
generate adversarial attacks. Also, prior attacks do not maintain a consistent
search space while comparing different search methods. In this paper, we
propose a query efficient attack strategy to generate plausible adversarial
examples on text classification and entailment tasks. Our attack jointly
leverages attention mechanism and locality sensitive hashing (LSH) to reduce
the query count. We demonstrate the efficacy of our approach by comparing our
attack with four baselines across three different search spaces. Further, we
benchmark our results across the same search space used in prior attacks. In
comparison to attacks proposed, on an average, we are able to reduce the query
count by 75% across all datasets and target models. We also demonstrate that
our attack achieves a higher success rate when compared to prior attacks in a
limited query setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04778">
<div class="article-summary-box-inner">
<span><p>Multilingual Neural Machine Translation (NMT) enables one model to serve all
translation directions, including ones that are unseen during training, i.e.
zero-shot translation. Despite being theoretically attractive, current models
often produce low quality translations -- commonly failing to even produce
outputs in the right target language. In this work, we observe that off-target
translation is dominant even in strong multilingual systems, trained on massive
multilingual corpora. To address this issue, we propose a joint approach to
regularize NMT models at both representation-level and gradient-level. At the
representation level, we leverage an auxiliary target language prediction task
to regularize decoder outputs to retain information about the target language.
At the gradient level, we leverage a small amount of direct data (in thousands
of sentence pairs) to regularize model gradients. Our results demonstrate that
our approach is highly effective in both reducing off-target translation
occurrences and improving zero-shot translation performance by +5.59 and +10.38
BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our
method also works well when the small amount of direct data is not available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04780">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models, such as BERT, have achieved remarkable
results on machine reading comprehension. However, due to the constraint of
encoding length (e.g., 512 WordPiece tokens), a long document is usually split
into multiple chunks that are independently read. It results in the reading
field being limited to individual chunks without information collaboration for
long document machine reading comprehension. To address this problem, we
propose RoR, a read-over-read method, which expands the reading field from
chunk to document. Specifically, RoR includes a chunk reader and a document
reader. The former first predicts a set of regional answers for each chunk,
which are then compacted into a highly-condensed version of the original
document, guaranteeing to be encoded once. The latter further predicts the
global answers from this condensed document. Eventually, a voting strategy is
utilized to aggregate and rerank the regional and global answers for final
prediction. Extensive experiments on two benchmarks QuAC and TriviaQA
demonstrate the effectiveness of RoR for long document reading. Notably, RoR
ranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of
submission (May 17th, 2021).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04783">
<div class="article-summary-box-inner">
<span><p>When a sufficiently large far-field training data is presented, jointly
optimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech
Recognition (ASR) backend shows promising results. Recent literature has shown
traditional beamformer designs, such as MVDR (Minimum Variance Distortionless
Response) or fixed beamformers can be successfully integrated as the frontend
into an E2E ASR system with learnable parameters. In this work, we propose the
self-attention channel combinator (SACC) ASR frontend, which leverages the
self-attention mechanism to combine multichannel audio signals in the magnitude
spectral domain. Experiments conducted on a multichannel playback test data
shows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed
beamformer-based frontend, both jointly optimized with a ContextNet-based ASR
backend. We also demonstrate the connection between the SACC and the
traditional beamformers, and analyze the intermediate outputs of the SACC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exophoric Pronoun Resolution in Dialogues with Topic Regularization. (arXiv:2109.04787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04787">
<div class="article-summary-box-inner">
<span><p>Resolving pronouns to their referents has long been studied as a fundamental
natural language understanding problem. Previous works on pronoun coreference
resolution (PCR) mostly focus on resolving pronouns to mentions in text while
ignoring the exophoric scenario. Exophoric pronouns are common in daily
communications, where speakers may directly use pronouns to refer to some
objects present in the environment without introducing the objects first.
Although such objects are not mentioned in the dialogue text, they can often be
disambiguated by the general topics of the dialogue. Motivated by this, we
propose to jointly leverage the local context and global topics of dialogues to
solve the out-of-text PCR problem. Extensive experiments demonstrate the
effectiveness of adding topic regularization for resolving exophoric pronouns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT. (arXiv:2109.04810v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04810">
<div class="article-summary-box-inner">
<span><p>Infusing factual knowledge into pre-trained models is fundamental for many
knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions
(MoP), an infusion approach that can handle a very large knowledge graph (KG)
by partitioning it into smaller sub-graphs and infusing their specific
knowledge into various BERT models using lightweight adapters. To leverage the
overall factual knowledge for a target task, these sub-graph adapters are
further fine-tuned along with the underlying BERT through a mixture layer. We
evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on
six downstream tasks (inc. NLI, QA, Classification), and the results show that
our MoP consistently enhances the underlying BERTs in task performance, and
achieves new SOTA performances on five evaluated datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework. (arXiv:2109.04817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04817">
<div class="article-summary-box-inner">
<span><p>Style is an integral part of natural language. However, evaluation methods
for style measures are rare, often task-specific and usually do not control for
content. We propose the modular, fine-grained and content-controlled
similarity-based STyle EvaLuation framework (STEL) to test the performance of
any model that can compare two sentences on style. We illustrate STEL with two
general dimensions of style (formal/informal and simple/complex) as well as two
specific characteristics of style (contrac'tion and numb3r substitution). We
find that BERT-based methods outperform simple versions of commonly used style
measures like 3-grams, punctuation frequency and LIWC-based approaches. We
invite the addition of further tasks and task instances to STEL and hope to
facilitate the improvement of style-sensitive measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04825">
<div class="article-summary-box-inner">
<span><p>The impressive capabilities of recent generative models to create texts that
are challenging to distinguish from the human-written ones can be misused for
generating fake news, product reviews, and even abusive content. Despite the
prominent performance of existing methods for artificial text detection, they
still lack interpretability and robustness towards unseen models. To this end,
we propose three novel types of interpretable topological features for this
task based on Topological Data Analysis (TDA) which is currently understudied
in the field of NLP. We empirically show that the features derived from the
BERT model outperform count- and neural-based baselines up to 10\% on three
common datasets, and tend to be the most robust towards unseen GPT-style
generation models as opposed to existing methods. The probing analysis of the
features reveals their sensitivity to the surface and syntactic properties. The
results demonstrate that TDA is a promising line with respect to NLP tasks,
specifically the ones that incorporate surface and structural information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking It All: Generating Contextualized Questions for any Semantic Role. (arXiv:2109.04832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04832">
<div class="article-summary-box-inner">
<span><p>Asking questions about a situation is an inherent step towards understanding
it. To this end, we introduce the task of role question generation, which,
given a predicate mention and a passage, requires producing a set of questions
asking about all possible semantic roles of the predicate. We develop a
two-stage model for this task, which first produces a context-independent
question prototype for each role and then revises it to be contextually
appropriate for the passage. Unlike most existing approaches to question
generation, our approach does not require conditioning on existing answers in
the text. Instead, we condition on the type of information to inquire about,
regardless of whether the answer appears explicitly in the text, could be
inferred from it, or should be sought elsewhere. Our evaluation demonstrates
that we generate diverse and well-formed questions for a large, broad-coverage
ontology of predicates and roles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model. (arXiv:2109.04834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04834">
<div class="article-summary-box-inner">
<span><p>Multi-turn response selection models have recently shown comparable
performance to humans in several benchmark datasets. However, in the real
environment, these models often have weaknesses, such as making incorrect
predictions based heavily on superficial patterns without a comprehensive
understanding of the context. For example, these models often give a high score
to the wrong response candidate containing several keywords related to the
context but using the inconsistent tense. In this study, we analyze the
weaknesses of the open-domain Korean Multi-turn response selection models and
publish an adversarial dataset to evaluate these weaknesses. We also suggest a
strategy to build a robust model in this adversarial environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04835">
<div class="article-summary-box-inner">
<span><p>In recent years, with the expansion of the Internet and attractive social
media infrastructures, people prefer to follow the news through these media.
Despite the many advantages of these media in the news field, the lack of any
control and verification mechanism has led to the spread of fake news, as one
of the most important threats to democracy, economy, journalism and freedom of
expression. Designing and using automatic methods to detect fake news on social
media has become a significant challenge. In this paper, we examine the
publishers' role in detecting fake news on social media. We also suggest a high
accurate multi-modal framework, namely FR-Detect, using user-related and
content-related features with early detection capability. For this purpose, two
new user-related features, namely Activity Credibility and Influence, have been
introduced for publishers. Furthermore, a sentence-level convolutional neural
network is provided to combine these features with latent textual content
features properly. Experimental results have shown that the publishers'
features can improve the performance of content-based models by up to 13% and
29% in accuracy and F1-score, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04838">
<div class="article-summary-box-inner">
<span><p>Pre-training has improved model accuracy for both classification and
generation tasks at the cost of introducing much larger and slower models.
Pruning methods have proven to be an effective way of reducing model size,
whereas distillation methods are proven for speeding up inference. We introduce
a block pruning approach targeting both small and fast models. Our approach
extends structured methods by considering blocks of any size and integrates
this structure into the movement pruning paradigm for fine-tuning. We find that
this approach learns to prune out full components of the underlying model, such
as attention heads. Experiments consider classification and generation tasks,
yielding among other results a pruned model that is a 2.4x faster, 74% smaller
BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models
in speed and pruned models in size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04847">
<div class="article-summary-box-inner">
<span><p>Labeling data can be an expensive task as it is usually performed manually by
domain experts. This is cumbersome for deep learning, as it is dependent on
large labeled datasets. Active learning (AL) is a paradigm that aims to reduce
labeling effort by only using the data which the used model deems most
informative. Little research has been done on AL in a text classification
setting and next to none has involved the more recent, state-of-the-art NLP
models. Here, we present an empirical study that compares different
uncertainty-based algorithms with BERT$_{base}$ as the used classifier. We
evaluate the algorithms on two NLP classification datasets: Stanford Sentiment
Treebank and KvK-Frontpages. Additionally, we explore heuristics that aim to
solve presupposed problems of uncertainty-based AL; namely, that it is
unscalable and that it is prone to selecting outliers. Furthermore, we explore
the influence of the query-pool size on the performance of AL. Whereas it was
found that the proposed heuristics for AL did not improve performance of AL;
our results show that using uncertainty-based AL with BERT$_{base}$ outperforms
random sampling of data. This difference in performance can decrease as the
query-pool size gets larger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification. (arXiv:2109.04853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04853">
<div class="article-summary-box-inner">
<span><p>Large-Scale Multi-Label Text Classification (LMTC) includes tasks with
hierarchical label spaces, such as automatic assignment of ICD-9 codes to
discharge summaries. Performance of models in prior art is evaluated with
standard precision, recall, and F1 measures without regard for the rich
hierarchical structure. In this work we argue for hierarchical evaluation of
the predictions of neural LMTC models. With the example of the ICD-9 ontology
we describe a structural issue in the representation of the structured label
space in prior art, and propose an alternative representation based on the
depth of the ontology. We propose a set of metrics for hierarchical evaluation
using the depth-based representation. We compare the evaluation scores from the
proposed metrics with previously used metrics on prior art LMTC models for
ICD-9 coding in MIMIC-III. We also propose further avenues of research
involving the proposed ontological representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Studying word order through iterative shuffling. (arXiv:2109.04867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04867">
<div class="article-summary-box-inner">
<span><p>As neural language models approach human performance on NLP benchmark tasks,
their advances are widely seen as evidence of an increasingly complex
understanding of syntax. This view rests upon a hypothesis that has not yet
been empirically tested: that word order encodes meaning essential to
performing these tasks. We refute this hypothesis in many cases: in the GLUE
suite and in various genres of English text, the words in a sentence or phrase
can rarely be permuted to form a phrase carrying substantially different
information. Our surprising result relies on inference by iterative shuffling
(IBIS), a novel, efficient procedure that finds the ordering of a bag of words
having the highest likelihood under a fixed language model. IBIS can use any
black-box model without additional training and is superior to existing word
ordering algorithms. Coalescing our findings, we discuss how shuffling
inference procedures such as IBIS can benefit language modeling and constrained
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiAzterTest: a Multilingual Analyzer on Multiple Levels of Language for Readability Assessment. (arXiv:2109.04870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04870">
<div class="article-summary-box-inner">
<span><p>Readability assessment is the task of determining how difficult or easy a
text is or which level/grade it has. Traditionally, language dependent
readability formula have been used, but these formulae take few text
characteristics into account. However, Natural Language Processing (NLP) tools
that assess the complexity of texts are able to measure more different features
and can be adapted to different languages. In this paper, we present the
MultiAzterTest tool: (i) an open source NLP tool which analyzes texts on over
125 measures of cohesion,language, and readability for English, Spanish and
Basque, but whose architecture is designed to easily adapt other languages;
(ii) readability assessment classifiers that improve the performance of
Coh-Metrix in English, Coh-Metrix-Esp in Spanish and ErreXail in Basque; iii) a
web tool. MultiAzterTest obtains 90.09 % in accuracy when classifying into
three reading levels (elementary, intermediate, and advanced) in English and
95.50 % in Basque and 90 % in Spanish when classifying into two reading levels
(simple and complex) using a SMO classifier. Using cross-lingual features,
MultiAzterTest also obtains competitive results above all in a complex vs
simple distinction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04876">
<div class="article-summary-box-inner">
<span><p>The problem of representing the atomic elements of language in modern neural
learning systems is one of the central challenges of the field of natural
language processing. I present a survey of the distributional, compositional,
and relational approaches to addressing this task, and discuss various means of
integrating them into systems, with special emphasis on the word level and the
out-of-vocabulary phenomenon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Test Time Adapter Ensembling for Low-resource Language Varieties. (arXiv:2109.04877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04877">
<div class="article-summary-box-inner">
<span><p>Adapters are light-weight modules that allow parameter-efficient fine-tuning
of pretrained models. Specialized language and task adapters have recently been
proposed to facilitate cross-lingual transfer of multilingual pretrained models
(Pfeiffer et al., 2020b). However, this approach requires training a separate
language adapter for every language one wishes to support, which can be
impractical for languages with limited data. An intuitive solution is to use a
related language adapter for the new language variety, but we observe that this
solution can lead to sub-optimal performance. In this paper, we aim to improve
the robustness of language adapters to uncovered languages without training new
adapters. We find that ensembling multiple existing language adapters makes the
fine-tuned model significantly more robust to other language varieties not
included in these adapters. Building upon this observation, we propose Entropy
Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble
weights of the pretrained language adapters for each test sentence by
minimizing the entropy of its predictions. Experiments on three diverse groups
of language varieties show that our method leads to significant improvements on
both named entity recognition and part-of-speech tagging across all languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-level Entity-based Extraction as Template Generation. (arXiv:2109.04901v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04901">
<div class="article-summary-box-inner">
<span><p>Document-level entity-based extraction (EE), aiming at extracting
entity-centric information such as entity roles and entity relations, is key to
automatic knowledge acquisition from text corpora for various domains. Most
document-level EE systems build extractive models, which struggle to model
long-term dependencies among entities at the document level. To address this
issue, we propose a generative framework for two document-level EE tasks:
role-filler entity extraction (REE) and relation extraction (RE). We first
formulate them as a template generation problem, allowing models to efficiently
capture cross-entity dependencies, exploit label semantics, and avoid the
exponential computation complexity of identifying N-ary relations. A novel
cross-attention guided copy mechanism, TopK Copy, is incorporated into a
pre-trained sequence-to-sequence model to enhance the capabilities of
identifying key information in the input document. Experiments done on the
MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%),
binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04912">
<div class="article-summary-box-inner">
<span><p>We present ReasonBert, a pre-training method that augments language models
with the ability to reason over long-range relations and multiple, possibly
hybrid contexts. Unlike existing pre-training methods that only harvest
learning signals from local contexts of naturally occurring texts, we propose a
generalized notion of distant supervision to automatically connect multiple
pieces of text and tables to create pre-training examples that require
long-range reasoning. Different types of reasoning are simulated, including
intersecting multiple pieces of evidence, bridging from one piece of evidence
to another, and detecting unanswerable cases. We conduct a comprehensive
evaluation on a variety of extractive question answering datasets ranging from
single-hop to multi-hop and from text-only to table-only to hybrid that require
various reasoning capabilities and show that ReasonBert achieves remarkable
improvement over an array of strong baselines. Few-shot experiments further
demonstrate that our pre-training method substantially improves sample
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion in Task-Oriented Dialogue Systems. (arXiv:2109.04919v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04919">
<div class="article-summary-box-inner">
<span><p>The ability to recognise emotions lends a conversational artificial
intelligence a human touch. While emotions in chit-chat dialogues have received
substantial attention, emotions in task-oriented dialogues have been largely
overlooked despite having an equally important role, such as to signal failure
or success. Existing emotion-annotated task-oriented corpora are limited in
size, label richness, and public availability, creating a bottleneck for
downstream tasks. To lay a foundation for studies on emotions in task-oriented
dialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus
of task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain
task-oriented dialogue dataset. It contains more than 11K dialogues with more
than 83K emotion annotations of user utterances. In addition to Wizzard-of-Oz
dialogues from MultiWOZ, we collect human-machine dialogues within the same set
of domains to sufficiently cover the space of various emotions that can happen
during the lifetime of a data-driven dialogue system. To the best of our
knowledge, this is the first large-scale open-source corpus of its kind. We
propose a novel emotion labelling scheme, which is tailored to task-oriented
dialogues. We report a set of experimental results to show the usability of
this corpus for emotion recognition and state tracking in task-oriented
dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes. (arXiv:2109.04921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04921">
<div class="article-summary-box-inner">
<span><p>State-of-the-art contextual embeddings are obtained from large language
models available only for a few languages. For others, we need to learn
representations using a multilingual model. There is an ongoing debate on
whether multilingual embeddings can be aligned in a space shared across many
languages. The novel Orthogonal Structural Probe (Limisiewicz and Mare\v{c}ek,
2021) allows us to answer this question for specific linguistic features and
learn a projection based only on mono-lingual annotated datasets. We evaluate
syntactic (UD) and lexical (WordNet) structural information encoded inmBERT's
contextual representations for nine diverse languages. We observe that for
languages closely related to English, no transformation is needed. The
evaluated information is encoded in a shared cross-lingual embedding space. For
other languages, it is beneficial to apply orthogonal transformation learned
separately for each language. We successfully apply our findings to zero-shot
and few-shot cross-lingual parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers. (arXiv:2109.04922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04922">
<div class="article-summary-box-inner">
<span><p>As large-scale, pre-trained language models achieve human-level and
superhuman accuracy on existing language understanding tasks, statistical bias
in benchmark data and probing studies have recently called into question their
true capabilities. For a more informative evaluation than accuracy on text
classification tasks can offer, we propose evaluating systems through a novel
measure of prediction coherence. We apply our framework to two existing
language understanding benchmarks with different properties to demonstrate its
versatility. Our experimental results show that this evaluation framework,
although simple in ideas and implementation, is a quick, effective, and
versatile measure to provide insight into the coherence of machines'
predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04939">
<div class="article-summary-box-inner">
<span><p>In computational linguistics, it has been shown that hierarchical structures
make language models (LMs) more human-like. However, the previous literature
has been agnostic about a parsing strategy of the hierarchical models. In this
paper, we investigated whether hierarchical structures make LMs more
human-like, and if so, which parsing strategy is most cognitively plausible. In
order to address this question, we evaluated three LMs against human reading
times in Japanese with head-final left-branching structures: Long Short-Term
Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars
(RNNGs) with top-down and left-corner parsing strategies as hierarchical
models. Our computational modeling demonstrated that left-corner RNNGs
outperformed top-down RNNGs and LSTM, suggesting that hierarchical and
left-corner architectures are more cognitively plausible than top-down or
sequential architectures. In addition, the relationships between the cognitive
plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be
discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04947">
<div class="article-summary-box-inner">
<span><p>Large-scale, pre-trained language models (LMs) have achieved human-level
performance on a breadth of language understanding tasks. However, evaluations
only based on end task performance shed little light on machines' true ability
in language understanding and reasoning. In this paper, we highlight the
importance of evaluating the underlying reasoning process in addition to end
performance. Toward this goal, we introduce Tiered Reasoning for Intuitive
Physics (TRIP), a novel commonsense reasoning dataset with dense annotations
that enable multi-tiered evaluation of machines' reasoning process. Our
empirical results show that while large LMs can achieve high end performance,
they struggle to support their predictions with valid supporting evidence. The
TRIP dataset and our baseline results will motivate verifiable evaluation of
commonsense reasoning and facilitate future research toward developing better
language understanding and reasoning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">We went to look for meaning and all we got were these lousy representations: aspects of meaning representation for computational semantics. (arXiv:2109.04949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04949">
<div class="article-summary-box-inner">
<span><p>In this paper we examine different meaning representations that are commonly
used in different natural language applications today and discuss their limits,
both in terms of the aspects of the natural language meaning they are modelling
and in terms of the aspects of the application for which they are used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04953">
<div class="article-summary-box-inner">
<span><p>Pretraining techniques leveraging enormous datasets have driven recent
advances in text summarization. While folk explanations suggest that knowledge
transfer accounts for pretraining's benefits, little is known about why it
works or what makes a pretraining task or dataset suitable. In this paper, we
challenge the knowledge transfer story, showing that pretraining on documents
consisting of character n-grams selected at random, we can nearly match the
performance of models pretrained on real corpora. This work holds the promise
of eliminating upstream corpora, which may alleviate some concerns over
offensive language, bias, and copyright issues. To see whether the small
residual benefit of using real data could be accounted for by the structure of
the pretraining task, we design several tasks motivated by a qualitative study
of summarization corpora. However, these tasks confer no appreciable benefit,
leaving open the possibility of a small role for knowledge transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Neural Sentence-Level Reframing of News Articles. (arXiv:2109.04957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04957">
<div class="article-summary-box-inner">
<span><p>Framing a news article means to portray the reported event from a specific
perspective, e.g., from an economic or a health perspective. Reframing means to
change this perspective. Depending on the audience or the submessage, reframing
can become necessary to achieve the desired effect on the readers. Reframing is
related to adapting style and sentiment, which can be tackled with neural text
generation techniques. However, it is more challenging since changing a frame
requires rewriting entire sentences rather than single phrases. In this paper,
we study how to computationally reframe sentences in news articles while
maintaining their coherence to the context. We treat reframing as a
sentence-level fill-in-the-blank task for which we train neural models on an
existing media frame corpus. To guide the training, we propose three
strategies: framed-language pretraining, named-entity preservation, and
adversarial learning. We evaluate respective models automatically and manually
for topic consistency, coherence, and successful reframing. Our results
indicate that generating properly-framed text works well but with tradeoffs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04993">
<div class="article-summary-box-inner">
<span><p>Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04994">
<div class="article-summary-box-inner">
<span><p>Unlike well-structured text, such as news reports and encyclopedia articles,
dialogue content often comes from two or more interlocutors, exchanging
information with each other. In such a scenario, the topic of a conversation
can vary upon progression and the key information for a certain topic is often
scattered across multiple utterances of different speakers, which poses
challenges to abstractly summarize dialogues. To capture the various topic
information of a conversation and outline salient facts for the captured
topics, this work proposes two topic-aware contrastive learning objectives,
namely coherence detection and sub-summary generation objectives, which are
expected to implicitly model the topic change and handle information scattering
challenges for the dialogue summarization task. The proposed contrastive
objectives are framed as auxiliary tasks for the primary dialogue summarization
task, united via an alternative parameter updating strategy. Extensive
experiments on benchmark datasets demonstrate that the proposed simple method
significantly outperforms strong baselines and achieves new state-of-the-art
performance. The code and trained models are publicly available via
\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04997">
<div class="article-summary-box-inner">
<span><p>A major factor contributing to the success of modern representation learning
is the ease of performing various vector operations. Recently, objects with
geometric structures (eg. distributions, complex or hyperbolic vectors, or
regions such as cones, disks, or boxes) have been explored for their
alternative inductive biases and additional representational capacities. In
this work, we introduce Box Embeddings, a Python library that enables
researchers to easily apply and extend probabilistic box embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05003">
<div class="article-summary-box-inner">
<span><p>We study the problem of training named entity recognition (NER) models using
only distantly-labeled data, which can be automatically obtained by matching
entity mentions in the raw text with entity types in a knowledge base. The
biggest challenge of distantly-supervised NER is that the distant supervision
may induce incomplete and noisy labels, rendering the straightforward
application of supervised learning ineffective. In this paper, we propose (1) a
noise-robust learning scheme comprised of a new loss function and a noisy label
removal step, for training NER models on distantly-labeled data, and (2) a
self-training method that uses contextualized augmentations created by
pre-trained language models to improve the generalization ability of the NER
model. On three benchmark datasets, our method achieves superior performance,
outperforming existing distantly-supervised NER models by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiSECT: Learning to Split and Rephrase Sentences with Bitexts. (arXiv:2109.05006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05006">
<div class="article-summary-box-inner">
<span><p>An important task in NLP applications such as sentence simplification is the
ability to take a long, complex sentence and split it into shorter sentences,
rephrasing as necessary. We introduce a novel dataset and a new model for this
`split and rephrase' task. Our BiSECT training data consists of 1 million long
English sentences paired with shorter, meaning-equivalent English sentences. We
obtain these by extracting 1-2 sentence alignments in bilingual parallel
corpora and then using machine translation to convert both sides of the corpus
into the same language. BiSECT contains higher quality training examples than
previous Split and Rephrase corpora, with sentence splits that require more
significant modifications. We categorize examples in our corpus, and use these
categories in a novel model that allows us to target specific regions of the
input sentence to be split and edited. Moreover, we show that models trained on
BiSECT can perform a wider variety of split operations and improve upon
previous state-of-the-art approaches in automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation Quality and Post-Editing Performance. (arXiv:2109.05016v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05016">
<div class="article-summary-box-inner">
<span><p>We test the natural expectation that using MT in professional translation
saves human processing time. The last such study was carried out by
Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the
translation quality. In contrast, we focus on neural MT (NMT) of high quality,
which has become the state-of-the-art approach since then and also got adopted
by most translation companies.
</p>
<p>Through an experimental study involving over 30 professional translators for
English -&gt; Czech translation, we examine the relationship between NMT
performance and post-editing time and quality. Across all models, we found that
better MT systems indeed lead to fewer changes in the sentences in this
industry setting. The relation between system quality and post-editing time is
however not straightforward and, contrary to the results on phrase-based MT,
BLEU is definitely not a stable predictor of the time or final output quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dimensional Emotion Detection from Categorical Emotion. (arXiv:1911.02499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.02499">
<div class="article-summary-box-inner">
<span><p>We present a model to predict fine-grained emotions along the continuous
dimensions of valence, arousal, and dominance (VAD) with a corpus with
categorical emotion annotations. Our model is trained by minimizing the EMD
(Earth Mover's Distance) loss between the predicted VAD score distribution and
the categorical emotion distributions sorted along VAD, and it can
simultaneously classify the emotion categories and predict the VAD scores for a
given sentence. We use pre-trained RoBERTa-Large and fine-tune on three
different corpora with categorical labels and evaluate on EmoBank corpus with
VAD scores. We show that our approach reaches comparable performance to that of
the state-of-the-art classifiers in categorical emotion classification and
shows significant positive correlations with the ground truth VAD scores. Also,
further training with supervision of VAD labels leads to improved performance
especially when dataset is small. We also present examples of predictions of
appropriate emotion words that are not part of the original annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Surprising Variability in Word Embedding Stability Across Languages. (arXiv:2004.14876v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14876">
<div class="article-summary-box-inner">
<span><p>Word embeddings are powerful representations that form the foundation of many
natural language processing architectures, both in English and in other
languages. To gain further insight into word embeddings, we explore their
stability (e.g., overlap between the nearest neighbors of a word in different
embedding spaces) in diverse languages. We discuss linguistic properties that
are related to stability, drawing out insights about correlations with
affixing, language gender systems, and other features. This has implications
for embedding use, particularly in research that uses them to study language
trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms. (arXiv:2005.00782v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00782">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PTLMs) have achieved impressive performance on
commonsense inference benchmarks, but their ability to employ commonsense to
make robust inferences, which is crucial for effective communications with
humans, is debated. In the pursuit of advancing fluid human-AI communication,
we propose a new challenge, RICA: Robust Inference capability based on
Commonsense Axioms, that evaluates robust commonsense inference despite textual
perturbations. To generate data for this challenge, we develop a systematic and
scalable procedure using commonsense knowledge bases and probe PTLMs across two
different evaluation settings. Extensive experiments on our generated probe
sets with more than 10k statements show that PTLMs perform no better than
random guessing on the zero-shot setting, are heavily impacted by statistical
biases, and are not robust to perturbation attacks. We also find that
fine-tuning on similar statements offer limited gains, as PTLMs still fail to
generalize to unseen inferences. Our new large-scale benchmark exposes a
significant gap between PTLMs and human-level language understanding and offers
a new challenge for PTLMs to demonstrate commonsense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hard Retrieval Decoder Attention for Transformers. (arXiv:2009.14658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14658">
<div class="article-summary-box-inner">
<span><p>The Transformer translation model is based on the multi-head attention
mechanism, which can be parallelized easily. The multi-head attention network
performs the scaled dot-product attention function in parallel, empowering the
model by jointly attending to information from different representation
subspaces at different positions. In this paper, we present an approach to
learning a hard retrieval attention where an attention head only attends to one
token in the sentence rather than all tokens. The matrix multiplication between
attention probabilities and the value sequence in the standard scaled
dot-product attention can thus be replaced by a simple and efficient retrieval
operation. We show that our hard retrieval attention mechanism is 1.43 times
faster in decoding, while preserving translation quality on a wide range of
machine translation tasks when used in the decoder self- and cross-attention
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06127">
<div class="article-summary-box-inner">
<span><p>Transformers that are pre-trained on multilingual corpora, such as, mBERT and
XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In
the zero-shot transfer setting, only English training data is used, and the
fine-tuned model is evaluated on another target language. While this works
surprisingly well, substantial variance has been observed in target language
performance between different fine-tuning runs, and in the zero-shot setup, no
target-language development data is available to select among multiple
fine-tuned models. Prior work has relied on English dev data to select among
models that are fine-tuned with different learning rates, number of steps and
other hyperparameters, often resulting in suboptimal choices. In this paper, we
show that it is possible to select consistently better models when small
amounts of annotated data are available in auxiliary pivot languages. We
propose a machine learning approach to model selection that uses the fine-tuned
model's own internal representations to predict its cross-lingual capabilities.
In extensive experiments we find that this method consistently selects better
models than English validation data across twenty five languages (including
eight low-resource languages), and often achieves results that are comparable
to model selection using target language development data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09697">
<div class="article-summary-box-inner">
<span><p>The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such "saturated" networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational AI systems. (arXiv:2010.12251v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12251">
<div class="article-summary-box-inner">
<span><p>Natural Language Understanding (NLU) is an established component within a
conversational AI or digital assistant system, and it is responsible for
producing semantic understanding of a user request. We propose a scalable and
automatic approach for improving NLU in a large-scale conversational AI system
by leveraging implicit user feedback, with an insight that user interaction
data and dialog context have rich information embedded from which user
satisfaction and intention can be inferred. In particular, we propose a general
domain-agnostic framework for curating new supervision data for improving NLU
from live production traffic. With an extensive set of experiments, we show the
results of applying the framework and improving NLU for a large-scale
production system and show its impact across 10 domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Association Between Labels and Free-Text Rationales. (arXiv:2010.12762v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12762">
<div class="article-summary-box-inner">
<span><p>In interpretable NLP, we require faithful rationales that reflect the model's
decision-making process for an explained instance. While prior work focuses on
extractive rationales (a subset of the input words), we investigate their
less-studied counterpart: free-text natural language rationales. We demonstrate
that pipelines, existing models for faithful extractive rationalization on
information-extraction style tasks, do not extend as reliably to "reasoning"
tasks requiring free-text rationales. We turn to models that jointly predict
and rationalize, a class of widely used high-performance models for free-text
rationalization whose faithfulness is not yet established. We define
label-rationale association as a necessary property for faithfulness: the
internal mechanisms of the model producing the label and the rationale must be
meaningfully correlated. We propose two measurements to test this property:
robustness equivalence and feature importance agreement. We find that
state-of-the-art T5-based joint models exhibit both properties for
rationalizing commonsense question-answering and natural language inference,
indicating their potential for producing faithful free-text rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12800">
<div class="article-summary-box-inner">
<span><p>We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.
Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,
Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from
55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query
Bank and Relevance Set, where the former contains 1,236 human-paraphrased
queries while the latter contains ~32 human-annotated FAQ items for each query.
We analyze COUGH by testing different FAQ retrieval models built on top of BM25
and BERT, among which the best model achieves 48.8 under P@5, indicating a
great challenge presented by COUGH and encouraging future research for further
improvement. Our COUGH dataset is available at
https://github.com/sunlab-osu/covid-faq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16410">
<div class="article-summary-box-inner">
<span><p>To alleviate human efforts from obtaining large-scale annotations,
Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in
addition to learning from limited samples. Existing self-training methods
suffer from the gradual drift problem, where noisy pseudo labels on unlabeled
data are incorporated during training. To alleviate the noise in pseudo labels,
we propose a method called MetaSRE, where a Relation Label Generation Network
generates quality assessment on pseudo labels by (meta) learning from the
successful and failed attempts on Relation Classification Network as an
additional meta-objective. To reduce the influence of noisy pseudo labels,
MetaSRE adopts a pseudo label selection and exploitation scheme which assesses
pseudo label quality on unlabeled samples and only exploits high-quality pseudo
labels in a self-training fashion to incrementally augment labeled samples for
both robustness and accuracy. Experimental results on two public datasets
demonstrate the effectiveness of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07497">
<div class="article-summary-box-inner">
<span><p>Codifying commonsense knowledge in machines is a longstanding goal of
artificial intelligence. Recently, much progress toward this goal has been made
with automatic knowledge base (KB) construction techniques. However, such
techniques focus primarily on the acquisition of positive (true) KB statements,
even though negative (false) statements are often also important for
discriminative reasoning over commonsense KBs. As a first step toward the
latter, this paper proposes NegatER, a framework that ranks potential negatives
in commonsense KBs using a contextual language model (LM). Importantly, as most
KBs do not contain negatives, NegatER relies only on the positive knowledge in
the LM and does not require ground-truth negative examples. Experiments
demonstrate that, compared to multiple contrastive data augmentation
approaches, NegatER yields negatives that are more grammatical, coherent, and
informative -- leading to statistically significant accuracy improvements in a
challenging KB completion task and confirming that the positive knowledge in
LMs can be "re-purposed" to generate negative knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Sentence Representation Learning with Conditional Masked Language Model. (arXiv:2012.14388v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14388">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel training method, Conditional Masked Language
Modeling (CMLM), to effectively learn sentence representations on large scale
unlabeled corpora. CMLM integrates sentence representation learning into MLM
training by conditioning on the encoded vectors of adjacent sentences. Our
English CMLM model achieves state-of-the-art performance on SentEval, even
outperforming models learned using supervised signals. As a fully unsupervised
learning method, CMLM can be conveniently extended to a broad range of
languages and domains. We find that a multilingual CMLM model co-trained with
bitext retrieval (BR) and natural language inference (NLI) tasks outperforms
the previous state-of-the-art multilingual models by a large margin, e.g. 10%
improvement upon baseline models on cross-lingual semantic search. We explore
the same language bias of the learned representations, and propose a simple,
post-training and model agnostic approach to remove the language identifying
information from the representation while still retaining sentence semantics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. (arXiv:2012.15562v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15562">
<div class="article-summary-box-inner">
<span><p>Massively multilingual language models such as multilingual BERT offer
state-of-the-art cross-lingual transfer performance on a range of NLP tasks.
However, due to limited capacity and large differences in pretraining data
sizes, there is a profound performance gap between resource-rich and
resource-poor target languages. The ultimate challenge is dealing with
under-resourced languages not covered at all by the models and written in
scripts unseen during pretraining. In this work, we propose a series of novel
data-efficient methods that enable quick and effective adaptation of pretrained
multilingual models to such low-resource languages and unseen scripts. Relying
on matrix factorization, our methods capitalize on the existing latent
knowledge about multiple languages already available in the pretrained model's
embedding matrix. Furthermore, we show that learning of the new dedicated
embedding matrix in the target language can be improved by leveraging a small
number of vocabulary items (i.e., the so-called lexically overlapping tokens)
shared between mBERT's and target language vocabulary. Our adaptation
techniques offer substantial performance gains for languages with unseen
scripts. We also demonstrate that they can yield improvements for low-resource
languages written in scripts covered by the pretrained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Robust Neural Machine Translation: A Transformer Case Study. (arXiv:2012.15710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15710">
<div class="article-summary-box-inner">
<span><p>Transformers (Vaswani et al., 2017) have brought a remarkable improvement in
the performance of neural machine translation (NMT) systems but they could be
surprisingly vulnerable to noise. In this work, we try to investigate how noise
breaks Transformers and if there exist solutions to deal with such issues.
There is a large body of work in the NMT literature on analyzing the behavior
of conventional models for the problem of noise but Transformers are relatively
understudied in this context. Motivated by this, we introduce a novel
data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate
noise during training. This idea is comparable to the well-known fine-tuning
strategy. Moreover, we propose two other novel extensions to the original
Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that
modify the neural architecture as well as the training process to handle noise.
One important characteristic of our techniques is that they only impact the
training phase and do not impose any overhead at inference time. We evaluated
our techniques to translate the English--German pair in both directions and
observed that our models have a higher tolerance to noise. More specifically,
they perform with no deterioration where up to 10% of entire test words are
infected by noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15781">
<div class="article-summary-box-inner">
<span><p>Influence functions approximate the "influences" of training data-points for
test predictions and have a wide variety of applications. Despite the
popularity, their computational cost does not scale well with model and
training data size. We present FastIF, a set of simple modifications to
influence functions that significantly improves their run-time. We use
k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good
candidate data points, identify the configurations that best balance the
speed-quality trade-off in estimating the inverse Hessian-vector product, and
introduce a fast parallel variant. Our proposed method achieves about 80X
speedup while being highly correlated with the original influence values. With
the availability of the fast influence functions, we demonstrate their
usefulness in four applications. First, we examine whether influential
data-points can "explain" test time behavior using the framework of
simulatability. Second, we visualize the influence interactions between
training and test data-points. Third, we show that we can correct model errors
by additional fine-tuning on certain influential data-points, improving the
accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we
experiment with a similar setup but fine-tuning on datapoints not seen during
training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI
datasets respectively. Overall, our fast influence functions can be efficiently
applied to large models and datasets, and our experiments demonstrate the
potential of influence functions in model interpretation and correcting model
errors. Code is available at
https://github.com/salesforce/fast-influence-functions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Commonsense Emergence in Few-shot Knowledge Models. (arXiv:2101.00297v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00297">
<div class="article-summary-box-inner">
<span><p>Recently, commonsense knowledge models - pretrained language models (LM)
fine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of
commonsense knowledge can be encoded in the parameters of large language
models. However, as parallel studies show that LMs are poor hypothesizers of
declarative commonsense relationships on their own, it remains unclear whether
this knowledge is learned during pretraining or from fine-tuning on KG
examples. To investigate this question, we train commonsense knowledge models
in few-shot settings to study the emergence of their commonsense representation
abilities. Our results show that commonsense knowledge models can rapidly adapt
from limited examples, indicating that KG fine-tuning serves to learn an
interface to encoded knowledge learned during pretraining. Importantly, our
analysis of absolute, angular, and distributional parameter changes during
few-shot fine-tuning provides novel insights into how this interface is
learned.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00433">
<div class="article-summary-box-inner">
<span><p>Broader disclosive transparency$-$truth and clarity in communication
regarding the function of AI systems$-$is widely considered desirable.
Unfortunately, it is a nebulous concept, difficult to both define and quantify.
This is problematic, as previous work has demonstrated possible trade-offs and
negative consequences to disclosive transparency, such as a confusion effect,
where "too much information" clouds a reader's understanding of what a system
description means. Disclosive transparency's subjective nature has rendered
deep study into these problems and their remedies difficult. To improve this
state of affairs, We introduce neural language model-based probabilistic
metrics to directly model disclosive transparency, and demonstrate that they
correlate with user and expert opinions of system transparency, making them a
valid objective proxy. Finally, we demonstrate the use of these metrics in a
pilot study quantifying the relationships between transparency, confusion, and
user perceptions in a corpus of real NLP system descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Spoken Language Modeling from Raw Audio. (arXiv:2102.01192v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01192">
<div class="article-summary-box-inner">
<span><p>We introduce Generative Spoken Language Modeling, the task of learning the
acoustic and linguistic characteristics of a language from raw audio (no text,
no labels), and a set of metrics to automatically evaluate the learned
representations at acoustic and linguistic levels for both encoding and
generation. We set up baseline systems consisting of a discrete speech encoder
(returning pseudo-text units), a generative language model (trained on
pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all
trained without supervision and validate the proposed metrics with human
evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that
the number of discrete units (50, 100, or 200) matters in a task-dependent and
encoder-dependent way, and that some combinations approach text-based systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Creative Inspiration with Fine-Grained Functional Facets of Ideas. (arXiv:2102.09761v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09761">
<div class="article-summary-box-inner">
<span><p>Large repositories of products, patents and scientific papers offer an
opportunity for building systems that scour millions of ideas and help users
discover inspirations. However, idea descriptions are typically in the form of
unstructured text, lacking key structure that is required for supporting
creative innovation interactions. Prior work has explored idea representations
that were limited in expressivity, required significant manual effort from
users, or dependent on curated knowledge bases with poor coverage. We explore a
novel representation that automatically breaks up products into fine-grained
functional facets capturing the purposes and mechanisms of ideas, and use it to
support important creative innovation interactions: functional search for
ideas, and exploration of the design space around a focal problem by viewing
related problem perspectives pooled from across many products. In user studies,
our approach boosts the quality of creative search and inspirations,
outperforming strong baselines by 50-60%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01378">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations clarify why an event occurred in contrast to
another. They are more inherently intuitive to humans to both produce and
comprehend. We propose a methodology to produce contrastive explanations for
classification models by modifying the representation to disregard
non-contrastive information, and modifying model behavior to only be based on
contrastive reasoning. Our method is based on projecting model representation
to a latent space that captures only the features that are useful (to the
model) to differentiate two potential decisions. We demonstrate the value of
contrastive explanations by analyzing two different scenarios, using both
high-level abstract concept attribution and low-level input token/span
attribution, on two widely used text classification tasks. Specifically, we
produce explanations for answering: for which label, and against which
alternative label, is some aspect of the input useful? And which aspects of the
input are useful for and against particular decisions? Overall, our findings
shed light on the ability of label-contrastive explanations to provide a more
accurate and finer-grained interpretability of a model's decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07352">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation models are sensitive to noise in the input texts,
such as misspelled words and ungrammatical constructions. Existing robustness
techniques generally fail when faced with unseen types of noise and their
performance degrades on clean texts. In this paper, we focus on three types of
realistic noise that are commonly generated by humans and introduce the idea of
visual context to improve translation robustness for noisy texts. In addition,
we describe a novel error correction training regime that can be used as an
auxiliary task to further improve translation robustness. Experiments on
English-French and English-German translation show that both multimodal and
error correction components improve model robustness to noisy texts, while
still retaining translation quality on clean texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources. (arXiv:2103.11320v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11320">
<div class="article-summary-box-inner">
<span><p>Warning: this paper contains content that may be offensive or upsetting.
</p>
<p>Numerous natural language processing models have tried injecting commonsense
by using the ConceptNet knowledge base to improve performance on different
tasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect
human biases such as "lawyers are dishonest." It is important that these biases
are not conflated with the notion of commonsense. We study this missing yet
important problem by first defining and quantifying biases in ConceptNet as two
types of representational harms: overgeneralization of polarized perceptions
and representation disparity. We find that ConceptNet contains severe biases
and disparities across four demographic categories. In addition, we analyze two
downstream models that use ConceptNet as a source for commonsense knowledge and
find the existence of biases in those models as well. We further propose a
filtered-based bias-mitigation approach and examine its effectiveness. We show
that our mitigation approach can reduce the issues in both resource and models
but leads to a performance drop, leaving room for future work to build fairer
and stronger commonsense models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Morphosyntactic Well-formedness of Generated Texts. (arXiv:2103.16590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16590">
<div class="article-summary-box-inner">
<span><p>Text generation systems are ubiquitous in natural language processing
applications. However, evaluation of these systems remains a challenge,
especially in multilingual settings. In this paper, we propose L'AMBRE -- a
metric to evaluate the morphosyntactic well-formedness of text using its
dependency parse and morphosyntactic rules of the language. We present a way to
automatically extract various rules governing morphosyntax directly from
dependency treebanks. To tackle the noisy outputs from text generation systems,
we propose a simple methodology to train robust parsers. We show the
effectiveness of our metric on the task of machine translation through a
diachronic study of systems translating into morphologically-rich languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Keyword Spotting in Any Language. (arXiv:2104.01454v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01454">
<div class="article-summary-box-inner">
<span><p>We introduce a few-shot transfer learning method for keyword spotting in any
language. Leveraging open speech corpora in nine languages, we automate the
extraction of a large multilingual keyword bank and use it to train an
embedding model. With just five training examples, we fine-tune the embedding
model for keyword spotting and achieve an average F1 score of 0.75 on keyword
classification for 180 new keywords unseen by the embedding model in these nine
languages. This embedding model also generalizes to new languages. We achieve
an average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13
new languages unseen by the embedding model. We investigate streaming accuracy
for our 5-shot models in two contexts: keyword spotting and keyword search.
Across 440 keywords in 22 languages, we achieve an average streaming keyword
spotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe
promising initial results on keyword search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05094">
<div class="article-summary-box-inner">
<span><p>Contrastive Learning has emerged as a powerful representation learning method
and facilitates various downstream tasks especially when supervised data is
limited. How to construct efficient contrastive samples through data
augmentation is key to its success. Unlike vision tasks, the data augmentation
method for contrastive learning has not been investigated sufficiently in
language tasks. In this paper, we propose a novel approach to construct
contrastive samples for language tasks using text summarization. We use these
samples for supervised contrastive learning to gain better text representations
which greatly benefit text classification tasks with limited annotations. To
further improve the method, we mix up samples from different classes and add an
extra regularization, named Mixsum, in addition to the cross-entropy-loss.
Experiments on real-world text classification datasets (Amazon-5, Yelp-5, AG
News, and IMDb) demonstrate the effectiveness of the proposed contrastive
learning framework with summarization-based data augmentation and Mixsum
regularization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WHOSe Heritage: Classification of UNESCO World Heritage "Outstanding Universal Value" Documents with Soft Labels. (arXiv:2104.05547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05547">
<div class="article-summary-box-inner">
<span><p>The UNESCO World Heritage List (WHL) includes the exceptionally valuable
cultural and natural heritage to be preserved for mankind. Evaluating and
justifying the Outstanding Universal Value (OUV) is essential for each site
inscribed in the WHL, and yet a complex task, even for experts, since the
selection criteria of OUV are not mutually exclusive. Furthermore, manual
annotation of heritage values and attributes from multi-source textual data,
which is currently dominant in heritage studies, is knowledge-demanding and
time-consuming, impeding systematic analysis of such authoritative documents in
terms of their implications on heritage management. This study applies
state-of-the-art NLP models to build a classifier on a new dataset containing
Statements of OUV, seeking an explainable and scalable automation tool to
facilitate the nomination, evaluation, research, and monitoring processes of
World Heritage sites. Label smoothing is innovatively adapted to improve the
model performance by adding prior inter-class relationship knowledge to
generate soft labels. The study shows that the best models fine-tuned from BERT
and ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation
on the model prediction shows that the models are sufficiently generalizable.
The study is promising to be further developed and applied in heritage research
and practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05837">
<div class="article-summary-box-inner">
<span><p>Relational knowledge bases (KBs) are commonly used to represent world
knowledge in machines. However, while advantageous for their high degree of
precision and interpretability, KBs are usually organized according to
manually-defined schemas, which limit their expressiveness and require
significant human efforts to engineer and maintain. In this review, we take a
natural language processing perspective to these limitations, examining how
they may be addressed in part by training deep contextual language models (LMs)
to internalize and express relational knowledge in more flexible forms. We
propose to organize knowledge representation strategies in LMs by the level of
KB supervision provided, from no KB supervision at all to entity- and
relation-level supervision. Our contributions are threefold: (1) We provide a
high-level, extensible taxonomy for knowledge representation in LMs; (2) Within
our taxonomy, we highlight notable models, evaluation tasks, and findings, in
order to provide an up-to-date review of current knowledge representation
capabilities in LMs; and (3) We suggest future research directions that build
upon the complementary aspects of LMs and KBs as knowledge representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05845">
<div class="article-summary-box-inner">
<span><p>Understanding what sequence of steps are needed to complete a goal can help
artificial intelligence systems reason about human activities. Past work in NLP
has examined the task of goal-step inference for text. We introduce the visual
analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model
is given a textual goal and must choose which of four images represents a
plausible step towards that goal. With a new dataset harvested from wikiHow
consisting of 772,277 images representing human actions, we show that our task
is challenging for state-of-the-art multimodal models. Moreover, the multimodal
representation learned from our data can be effectively transferred to other
datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task
will facilitate multimodal reasoning about procedural events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06022">
<div class="article-summary-box-inner">
<span><p>We propose a parameter sharing method for Transformers (Vaswani et al.,
2017). The proposed approach relaxes a widely used technique, which shares
parameters for one layer with all layers such as Universal Transformers
(Dehghani et al., 2019), to increase the efficiency in the computational time.
We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign
parameters to each layer. Experimental results show that the proposed
strategies are efficient in the parameter size and computational time.
Moreover, we indicate that the proposed strategies are also effective in the
configuration where we use many training data such as the recent WMT
competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06644">
<div class="article-summary-box-inner">
<span><p>A possible explanation for the impressive performance of masked language
model (MLM) pre-training is that such models have learned to represent the
syntactic structures prevalent in classical NLP pipelines. In this paper, we
propose a different explanation: MLMs succeed on downstream tasks almost
entirely due to their ability to model higher-order word co-occurrence
statistics. To demonstrate this, we pre-train MLMs on sentences with randomly
shuffled word order, and show that these models still achieve high accuracy
after fine-tuning on many downstream tasks -- including on tasks specifically
designed to be challenging for models that ignore word order. Our models
perform surprisingly well according to some parametric syntactic probes,
indicating possible deficiencies in how we test representations for syntactic
information. Overall, our results show that purely distributional information
largely explains the success of pre-training, and underscore the importance of
curating challenging evaluation datasets that require deeper linguistic
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06979">
<div class="article-summary-box-inner">
<span><p>Learning sentence embeddings often requires a large amount of labeled data.
However, for most tasks and domains, labeled data is seldom available and
creating it is expensive. In this work, we present a new state-of-the-art
unsupervised method based on pre-trained Transformers and Sequential Denoising
Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.
It can achieve up to 93.1% of the performance of in-domain supervised
approaches. Further, we show that TSDAE is a strong domain adaptation and
pre-training method for sentence embeddings, significantly outperforming other
approaches like Masked Language Model.
</p>
<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work
mainly evaluates on the single task of Semantic Textual Similarity (STS), which
does not require any domain knowledge. It is unclear if these proposed methods
generalize to other domains and tasks. We fill this gap and evaluate TSDAE and
other recent approaches on four different datasets from heterogeneous domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution. (arXiv:2104.07425v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07425">
<div class="article-summary-box-inner">
<span><p>Masked language models (MLMs) have contributed to drastic performance
improvements with regard to zero anaphora resolution (ZAR). To further improve
this approach, in this study, we made two proposals. The first is a new
pretraining task that trains MLMs on anaphoric relations with explicit
supervision, and the second proposal is a new finetuning method that remedies a
notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese
ZAR demonstrated that our two proposals boost the state-of-the-art performance,
and our detailed analysis provides new insights on the remaining challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07637">
<div class="article-summary-box-inner">
<span><p>Natural languages display a trade-off among different strategies to convey
syntactic structure, such as word order or inflection. This trade-off, however,
has not appeared in recent simulations of iterated language learning with
neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in
light of three factors that play an important role in comparable experiments
from the Language Evolution field: (i) speaker bias towards efficient
messaging, (ii) non systematic input languages, and (iii) learning bottleneck.
Our simulations show that neural agents mainly strive to maintain the utterance
type distribution observed during learning, instead of developing a more
efficient or systematic language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07789">
<div class="article-summary-box-inner">
<span><p>A health outcome is a measurement or an observation used to capture and
assess the effect of a treatment. Automatic detection of health outcomes from
text would undoubtedly speed up access to evidence necessary in healthcare
decision making. Prior work on outcome detection has modelled this task as
either (a) a sequence labelling task, where the goal is to detect which text
spans describe health outcomes, or (b) a classification task, where the goal is
to classify a text into a pre-defined set of categories depending on an outcome
that is mentioned somewhere in that text. However, this decoupling of span
detection and classification is problematic from a modelling perspective and
ignores global structural correspondences between sentence-level and word-level
information present in a given text. To address this, we propose a method that
uses both word-level and sentence-level information to simultaneously perform
outcome span detection and outcome type classification. In addition to
injecting contextual information to hidden vectors, we use label attention to
appropriately weight both word and sentence level information. Experimental
results on several benchmark datasets for health outcome detection show that
our proposed method consistently outperforms decoupled methods, reporting
competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What to Pre-Train on? Efficient Intermediate Task Selection. (arXiv:2104.08247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08247">
<div class="article-summary-box-inner">
<span><p>Intermediate task fine-tuning has been shown to culminate in large transfer
gains across many NLP tasks. With an abundance of candidate datasets as well as
pre-trained language models, it has become infeasible to run the cross-product
of all combinations to find the best transfer setting. In this work we first
establish that similar sequential fine-tuning gains can be achieved in adapter
settings, and subsequently consolidate previously proposed methods that
efficiently identify beneficial tasks for intermediate transfer learning. We
experiment with a diverse set of 42 intermediate and 11 target English
classification, multiple choice, question answering, and sequence tagging
tasks. Our results show that efficient embedding based methods that rely solely
on the respective datasets outperform computational expensive few-shot
fine-tuning approaches. Our best methods achieve an average Regret@3 of less
than 1% across all target tasks, demonstrating that we are able to efficiently
identify the best datasets for intermediate training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Wikily" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08384">
<div class="article-summary-box-inner">
<span><p>We present a simple but effective approach for leveraging Wikipedia for
neural machine translation as well as cross-lingual tasks of image captioning
and dependency parsing without using any direct supervision from external
parallel data or supervised models in the target language. We show that first
sentences and titles of linked Wikipedia pages, as well as cross-lingual image
captions, are strong signals for a seed parallel data to extract bilingual
dictionaries and cross-lingual word embeddings for mining parallel text from
Wikipedia. Our final model achieves high BLEU scores that are close to or
sometimes higher than strong supervised baselines in low-resource languages;
e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.
Moreover, we tailor our wikily supervised translation models to unsupervised
image captioning, and cross-lingual dependency parser transfer. In image
captioning, we train a multi-tasking machine translation and image captioning
pipeline for Arabic and English from which the Arabic training data is a
translated version of the English captioning data, using our wikily-supervised
translation models. Our captioning results on Arabic are slightly better than
that of its supervised model. In dependency parsing, we translate a large
amount of monolingual text, and use it as artificial training data in an
annotation projection framework. We show that our model outperforms recent work
on cross-lingual transfer of dependency parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment. (arXiv:2104.08597v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08597">
<div class="article-summary-box-inner">
<span><p>Cross-lingual named-entity lexica are an important resource to multilingual
NLP tasks such as machine translation and cross-lingual wikification. While
knowledge bases contain a large number of entities in high-resource languages
such as English and French, corresponding entities for lower-resource languages
are often missing. To address this, we propose Lexical-Semantic-Phonetic Align
(LSP-Align), a technique to automatically mine cross-lingual entity lexica from
mined web data. We demonstrate LSP-Align outperforms baselines at extracting
cross-lingual entity pairs and mine 164 million entity pairs from 120 different
languages aligned with English. We release these cross-lingual entity pairs
along with the massively multilingual tagged named entity corpus as a resource
to the NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. (arXiv:2104.08645v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08645">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language encoders, such as multilingual BERT and
XLM-R, show great potential for zero-shot cross-lingual transfer. However,
these multilingual encoders do not precisely align words and phrases across
languages. Especially, learning alignments in the multilingual embedding space
usually requires sentence-level or word-level parallel corpora, which are
expensive to be obtained for low-resource languages. An alternative is to make
the multilingual encoders more robust; when fine-tuning the encoder using
downstream task, we train the encoder to tolerate noise in the contextual
embedding spaces such that even if the representations of different languages
are not aligned well, the model can still achieve good performance on zero-shot
cross-lingual transfer. In this work, we propose a learning strategy for
training robust models by drawing connections between adversarial examples and
the failure cases of zero-shot cross-lingual transfer. We adopt two widely used
robust training methods, adversarial training and randomized smoothing, to
train the desired robust model. The experimental results demonstrate that
robust training improves zero-shot cross-lingual transfer on text
classification tasks. The improvement is more significant in the generalized
cross-lingual transfer setting, where the pair of input sentences belong to two
different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08685">
<div class="article-summary-box-inner">
<span><p>Are pairs of words that tend to occur together also likely to stand in a
linguistic dependency? This empirical question is motivated by a long history
of literature in cognitive science, psycholinguistics, and NLP. In this work we
contribute an extensive analysis of the relationship between linguistic
dependencies and statistical dependence between words. Improving on previous
work, we introduce the use of large pretrained language models to compute
contextualized estimates of the pointwise mutual information between words
(CPMI). For multiple models and languages, we extract dependency trees which
maximize CPMI, and compare to gold standard linguistic dependencies. Overall,
we find that CPMI dependencies achieve an unlabelled undirected attachment
score of at most $\approx 0.5$. While far above chance, and consistently above
a non-contextualized PMI baseline, this score is generally comparable to a
simple baseline formed by connecting adjacent words. We analyze which kinds of
linguistic dependencies are best captured in CPMI dependencies, and also find
marked differences between the estimates of the large pretrained language
models, illustrating how their different training schemes affect the type of
dependencies they capture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning. (arXiv:2104.08799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08799">
<div class="article-summary-box-inner">
<span><p>Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a
classical task for capturing the central idea from a given document. Based on
Seq2Seq models, the previous reinforcement learning framework on KG tasks
utilizes the evaluation metrics to further improve the well-trained neural
models. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are
only aware of the exact correctness of predictions on phrase-level and ignore
the semantic similarities between similar predictions and targets, which
inhibits the model from learning deep linguistic patterns. In response to this
problem, we propose a new fine-grained evaluation metric to improve the RL
framework, which considers different granularities: token-level $F_1$ score,
edit distance, duplication, and prediction quantities. On the whole, the new
framework includes two reward functions: the fine-grained evaluation score and
the vanilla $F_1$ score. This framework helps the model identifying some
partial match phrases which can be further optimized as the exact match ones.
Experiments on KG benchmarks show that our proposed training framework
outperforms the previous RL training frameworks among all evaluation scores. In
addition, our method can effectively ease the synonym problem and generate a
higher quality prediction. The source code is available at
\url{https://github.com/xuyige/FGRL4KG}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08821">
<div class="article-summary-box-inner">
<span><p>This paper presents SimCSE, a simple contrastive learning framework that
greatly advances the state-of-the-art sentence embeddings. We first describe an
unsupervised approach, which takes an input sentence and predicts itself in a
contrastive objective, with only standard dropout used as noise. This simple
method works surprisingly well, performing on par with previous supervised
counterparts. We find that dropout acts as minimal data augmentation and
removing it leads to a representation collapse. Then, we propose a supervised
approach, which incorporates annotated pairs from natural language inference
datasets into our contrastive learning framework, by using "entailment" pairs
as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on
standard semantic textual similarity (STS) tasks, and our unsupervised and
supervised models using BERT base achieve an average of 76.3% and 81.6%
Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to
previous best results. We also show -- both theoretically and empirically --
that contrastive learning objective regularizes pre-trained embeddings'
anisotropic space to be more uniform, and it better aligns positive pairs when
supervised signals are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09574">
<div class="article-summary-box-inner">
<span><p>Humans use commonsense reasoning (CSR) implicitly to produce natural and
coherent responses in conversations. Aiming to close the gap between current
response generation (RG) models and human communication abilities, we want to
understand why RG models respond as they do by probing RG model's understanding
of commonsense reasoning that elicits proper responses. We formalize the
problem by framing commonsense as a latent variable in the RG task and using
explanations for responses as textual form of commonsense. We collect 6k
annotated explanations justifying responses from four dialogue datasets and ask
humans to verify them and propose two probing settings to evaluate RG models'
CSR capabilities. Probing results show that models fail to capture the logical
relations between commonsense explanations and responses and fine-tuning on
in-domain data and increasing model sizes do not lead to understanding of CSR
for RG. We hope our study motivates more research in making RG models emulate
the human reasoning process in pursuit of smooth human-AI communication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03432">
<div class="article-summary-box-inner">
<span><p>Concept-to-text Natural Language Generation is the task of expressing an
input meaning representation in natural language. Previous approaches in this
task have been able to generalise to rare or unseen instances by relying on a
delexicalisation of the input. However, this often requires that the input
appears verbatim in the output text. This poses challenges in multilingual
settings, where the task expands to generate the output text in multiple
languages given the same input. In this paper, we explore the application of
multilingual models in concept-to-text and propose Language Agnostic
Delexicalisation, a novel delexicalisation method that uses multilingual
pretrained embeddings, and employs a character-level post-editing model to
inflect words in their correct form during relexicalisation. Our experiments
across five datasets and five languages show that multilingual models
outperform monolingual models in concept-to-text and that our framework
outperforms previous approaches, especially for low resource languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03824">
<div class="article-summary-box-inner">
<span><p>We show that Transformer encoder architectures can be sped up, with limited
accuracy costs, by replacing the self-attention sublayers with simple linear
transformations that "mix" input tokens. These linear mixers, along with
standard nonlinearities in feed-forward layers, prove competent at modeling
semantic relationships in several text classification tasks. Most surprisingly,
we find that replacing the self-attention sublayer in a Transformer encoder
with a standard, unparameterized Fourier Transform achieves 92-97% of the
accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on
GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input
lengths, our FNet model is significantly faster: when compared to the
"efficient" Transformers on the Long Range Arena benchmark, FNet matches the
accuracy of the most accurate models, while outpacing the fastest models across
all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).
Finally, FNet has a light memory footprint and is particularly efficient at
smaller model sizes; for a fixed speed and accuracy budget, small FNet models
outperform Transformer counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01198">
<div class="article-summary-box-inner">
<span><p>In this work, we present to the NLP community, and to the wider research
community as a whole, an application for the diachronic analysis of research
corpora. We open source an easy-to-use tool coined: DRIFT, which allows
researchers to track research trends and development over the years. The
analysis methods are collated from well-cited research works, with a few of our
own methods added for good measure. Succinctly put, some of the analysis
methods are: keyword extraction, word clouds, predicting
declining/stagnant/growing trends using Productivity, tracking bi-grams using
Acceleration plots, finding the Semantic Drift of words, tracking trends using
similarity, etc. To demonstrate the utility and efficacy of our tool, we
perform a case study on the cs.CL corpus of the arXiv repository and draw
inferences from the analysis methods. The toolkit and the associated code are
available here: https://github.com/rajaswa/DRIFT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00270">
<div class="article-summary-box-inner">
<span><p>Opinion prediction is an emerging research area with diverse real-world
applications, such as market research and situational awareness. We identify
two lines of approaches to the problem of opinion prediction. One uses
topic-based sentiment analysis with time-series modeling, while the other uses
static embedding of text. The latter approaches seek user-specific solutions by
generating user fingerprints. Such approaches are useful in predicting user's
reactions to unseen content. In this work, we propose a novel dynamic
fingerprinting method that leverages contextual embedding of user's comments
conditioned on relevant user's reading history. We integrate BERT variants with
a recurrent neural network to generate predictions. The results show up to 13\%
improvement in micro F1-score compared to previous approaches. Experimental
results show novel insights that were previously unknown such as better
predictions for an increase in dynamic history length, the impact of the nature
of the article on performance, thereby laying the foundation for further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. (arXiv:2108.04539v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
<div class="article-summary-box-inner">
<span><p>Key information extraction (KIE) from document images requires understanding
the contextual and spatial semantics of texts in two-dimensional (2D) space.
Many recent studies try to solve the task by developing pre-training language
models focusing on combining visual features from document images with texts
and their layout. On the other hand, this paper tackles the problem by going
back to the basic: effective combination of text and layout. Specifically, we
propose a pre-trained language model, named BROS (BERT Relying On Spatiality),
that encodes relative positions of texts in 2D space and learns from unlabeled
documents with area-masking strategy. With this optimized training scheme for
understanding texts in 2D space, BROS shows comparable or better performance
compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and
SciTSR) without relying on visual features. This paper also reveals two
real-world challenges in KIE tasks--(1) minimizing the error from incorrect
text ordering and (2) efficient learning from fewer downstream examples--and
demonstrates the superiority of BROS over previous methods. Our code will be
open to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05669">
<div class="article-summary-box-inner">
<span><p>Isolated silos of scientific research and the growing challenge of
information overload limit awareness across the literature and hinder
innovation. Algorithmic curation and recommendation, which often prioritize
relevance, can further reinforce these informational "filter bubbles." In
response, we describe Bridger, a system for facilitating discovery of scholars
and their work, to explore design tradeoffs between relevant and novel
recommendations. We construct a faceted representation of authors with
information gleaned from their papers and inferred author personas, and use it
to develop an approach that locates commonalities ("bridges") and contrasts
between scientists -- retrieving partially similar authors rather than aiming
for strict similarity. In studies with computer science researchers, this
approach helps users discover authors considered useful for generating novel
research directions, outperforming a state-of-art neural model. In addition to
recommending new content, we also demonstrate an approach for displaying it in
a manner that boosts researchers' ability to understand the work of authors
with whom they are unfamiliar. Finally, our analysis reveals that Bridger
connects authors who have different citation profiles, publish in different
venues, and are more distant in social co-authorship networks, raising the
prospect of bridging diverse communities and facilitating discovery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07971">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a novel problem formulation for de-identification of
unstructured clinical text. We formulate the de-identification problem as a
sequence to sequence learning problem instead of a token classification
problem. Our approach is inspired by the recent state-of -the-art performance
of sequence to sequence learning models for named entity recognition. Early
experimentation of our proposed approach achieved 98.91% recall rate on i2b2
dataset. This performance is comparable to current state-of-the-art models for
unstructured clinical text de-identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11601">
<div class="article-summary-box-inner">
<span><p>Software developers write a lot of source code and documentation during
software development. Intrinsically, developers often recall parts of source
code or code summaries that they had written in the past while implementing
software or documenting them. To mimic developers' code or summary generation
behavior, we propose a retrieval augmented framework, REDCODER, that retrieves
relevant code or summaries from a retrieval database and provides them as a
supplement to code generation or summarization models. REDCODER has a couple of
uniqueness. First, it extends the state-of-the-art dense retrieval technique to
search for relevant code or summaries. Second, it can work with retrieval
databases that include unimodal (only code or natural language description) or
bimodal instances (code-description pairs). We conduct experiments and
extensive analysis on two benchmark datasets of code generation and
summarization in Java and Python, and the promising results endorse the
effectiveness of our proposed retrieval augmented framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications. For example, intent
classification in dialogue systems. The reason is that the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current methods can tackle such problems
reliably in a realistic scenario where zero OOD training data is available. In
this study, we propose ProtoInfoMax, a new architecture that extends
Prototypical Networks to simultaneously process in-domain and OOD sentences via
Mutual Information Maximization (InfoMax) objective. Experimental results show
that our proposed method can substantially improve performance up to 20% for
OOD detection in low resource settings of text classification. We also show
that ProtoInfoMax is less prone to typical overconfidence errors of Neural
Networks, leading to more reliable prediction results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12738">
<div class="article-summary-box-inner">
<span><p>Recent advances in summarization provide models that can generate summaries
of higher quality. Such models now exist for a number of summarization tasks,
including query-based summarization, dialogue summarization, and multi-document
summarization. While such models and tasks are rapidly growing in the research
field, it has also become challenging for non-experts to keep track of them. To
make summarization methods more accessible to a wider audience, we develop
SummerTime by rethinking the summarization task from the perspective of an NLP
non-expert. SummerTime is a complete toolkit for text summarization, including
various models, datasets and evaluation metrics, for a full spectrum of
summarization-related tasks. SummerTime integrates with libraries designed for
NLP researchers, and enables users with easy-to-use APIs. With SummerTime,
users can locate pipeline solutions and search for the best model with their
own data, and visualize the differences, all with a few lines of code. We also
provide explanations for models and evaluation metrics to help users understand
the model behaviors and select models that best suit their needs. Our library,
along with a notebook demo, is available at
https://github.com/Yale-LILY/SummerTime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12971">
<div class="article-summary-box-inner">
<span><p>A smart contract is a program executed on a blockchain, based on which many
cryptocurrencies are implemented, and is being used for automating
transactions. Due to the large amount of money that smart contracts deal with,
there is a surging demand for a method that can statically and formally verify
them.
</p>
<p>This article describes our type-based static verification tool HELMHOLTZ for
Michelson, which is a statically typed stack-based language for writing smart
contracts that are executed on the blockchain platform Tezos. HELMHOLTZ is
designed on top of our extension of Michelson's type system with refinement
types. HELMHOLTZ takes a Michelson program annotated with a user-defined
specification written in the form of a refinement type as input; it then
typechecks the program against the specification based on the refinement type
system, discharging the generated verification conditions with the SMT solver
Z3. We briefly introduce our refinement type system for the core calculus
Mini-Michelson of Michelson, which incorporates the characteristic features
such as compound datatypes (e.g., lists and pairs), higher-order functions, and
invocation of another contract. \HELMHOLTZ{} successfully verifies several
practical Michelson programs, including one that transfers money to an account
and that checks a digital signature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13751">
<div class="article-summary-box-inner">
<span><p>Keeping track of scientific challenges, advances and emerging directions is a
fundamental part of research. However, researchers face a flood of papers that
hinders discovery of important knowledge. In biomedicine, this directly impacts
human lives. To address this problem, we present a novel task of extraction and
search of scientific challenges and directions, to facilitate rapid knowledge
discovery. We construct and release an expert-annotated corpus of texts sampled
from full-length papers, labeled with novel semantic categories that generalize
across many types of challenges and directions. We focus on a large corpus of
interdisciplinary work relating to the COVID-19 pandemic, ranging from
biomedicine to areas such as AI and economics. We apply a model trained on our
data to identify challenges and directions across the corpus and build a
dedicated search engine. In experiments with 19 researchers and clinicians
using our system, we outperform a popular scientific search engine in assisting
knowledge discovery. Finally, we show that models trained on our resource
generalize to the wider biomedical domain and to AI papers, highlighting its
broad utility. We make our data, model and search engine publicly available.
https://challenges.apps.allenai.org/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01850">
<div class="article-summary-box-inner">
<span><p>As the digital news industry becomes the main channel of information
dissemination, the adverse impact of fake news is explosively magnified. The
credibility of a news report should not be considered in isolation. Rather,
previously published news articles on the similar event could be used to assess
the credibility of a news report. Inspired by this, we propose a BERT-based
multimodal unreliable news detection framework, which captures both textual and
visual information from unreliable articles utilising the contrastive learning
strategy. The contrastive learner interacts with the unreliable news classifier
to push similar credible news (or similar unreliable news) closer while moving
news articles with similar content but opposite credibility labels away from
each other in the multimodal embedding space. Experimental results on a
COVID-19 related dataset, ReCOVery, show that our model outperforms a number of
competitive baseline in unreliable news detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02972">
<div class="article-summary-box-inner">
<span><p>Despite constant improvements in machine translation quality, automatic
poetry translation remains a challenging problem due to the lack of
open-sourced parallel poetic corpora, and to the intrinsic complexities
involved in preserving the semantics, style, and figurative nature of poetry.
We present an empirical investigation for poetry translation along several
dimensions: 1) size and style of training data (poetic vs. non-poetic),
including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)
language-family-specific models vs. mixed-multilingual models. To accomplish
this, we contribute a parallel dataset of poetry translations for several
language pairs. Our results show that multilingual fine-tuning on poetic text
significantly outperforms multilingual fine-tuning on non-poetic text that is
35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and
human evaluation metrics such as faithfulness (meaning and poetic style).
Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual}
fine-tuning on poetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03158">
<div class="article-summary-box-inner">
<span><p>An individual's variation in writing style is often a function of both social
and personal attributes. While structured social variation has been extensively
studied, e.g., gender based variation, far less is known about how to
characterize individual styles due to their idiosyncratic nature. We introduce
a new approach to studying idiolects through a massive cross-author comparison
to identify and encode stylistic features. The neural model achieves strong
performance at authorship identification on short texts and through an
analogy-based probing task, showing that the learned representations exhibit
surprising regularities that encode qualitative and quantitative shifts of
idiolectal styles. Through text perturbation, we quantify the relative
contributions of different linguistic elements to idiolectal variation.
Furthermore, we provide a description of idiolects through measuring inter- and
intra-author variation, showing that variation in idiolects is often
distinctive yet consistent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03848">
<div class="article-summary-box-inner">
<span><p>Cyber intelligence is widely and abundantly available in numerous open online
sources with reports on vulnerabilities and incidents. This constant stream of
noisy information requires new tools and techniques if it is to be used for the
benefit of analysts and investigators in various organizations. In this paper
we present and implement a novel knowledge graph and knowledge mining framework
for extracting relevant information from free-form text about incidents in the
cyber domain. Our framework includes a machine learning based pipeline as well
as crawling methods for generating graphs of entities, attackers and the
related information with our non-technical cyber ontology. We test our
framework on publicly available cyber incident datasets to evaluate the
accuracy of our knowledge mining methods as well as the usefulness of the
framework in the use of cyber analysts. Our results show analyzing the
knowledge graph constructed using the novel framework, an analyst can infer
additional information from the current cyber landscape in terms of risk to
various entities and the propagation of risk between industries and countries.
Expanding the framework to accommodate more technical and operational level
information can increase the accuracy and explainability of trends and risk in
the knowledge graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03858">
<div class="article-summary-box-inner">
<span><p>Recent works have found evidence of gender bias in models of machine
translation and coreference resolution using mostly synthetic diagnostic
datasets. While these quantify bias in a controlled experiment, they often do
so on a small scale and consist mostly of artificial, out-of-distribution
sentences. In this work, we find grammatical patterns indicating stereotypical
and non-stereotypical gender-role assignments (e.g., female nurses versus male
dancers) in corpora from three domains, resulting in a first large-scale gender
bias dataset of 108K diverse real-world English sentences. We manually verify
the quality of our corpus and use it to evaluate gender bias in various
coreference resolution and machine translation models. We find that all tested
models tend to over-rely on gender stereotypes when presented with natural
inputs, which may be especially harmful when deployed in commercial systems.
Finally, we show that our dataset lends itself to finetuning a coreference
resolution model, finding it mitigates bias on a held out set. Our dataset and
models are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will
spur future research into gender bias evaluation mitigation techniques in
realistic settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03969">
<div class="article-summary-box-inner">
<span><p>Transformers have recently become very popular for sequence-to-sequence
applications such as machine translation and speech recognition. In this work,
we propose a multi-task learning-based transformer model for low-resource
multilingual speech recognition for Indian languages. Our proposed model
consists of a conformer [1] encoder and two parallel transformer decoders. We
use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme
decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme
recognition task as an auxiliary task for our multi-task learning framework. We
jointly optimize the network for both phoneme and grapheme recognition tasks
using Joint CTC-Attention [2] training. We use a conditional decoding scheme to
inject the language information into the model before predicting the grapheme
sequence. Our experiments show that our proposed approach can obtain
significant improvement over previous approaches [4]. We also show that our
conformer-based dual-decoder approach outperforms both the transformer-based
dual-decoder approach and single decoder approach. Finally, We compare
monolingual ASR models with our proposed multilingual ASR approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04400">
<div class="article-summary-box-inner">
<span><p>In cross-lingual text classification, it is required that task-specific
training data in high-resource source languages are available, where the task
is identical to that of a low-resource target language. However, collecting
such training data can be infeasible because of the labeling cost, task
characteristics, and privacy concerns. This paper proposes an alternative
solution that uses only task-independent word embeddings of high-resource
languages and bilingual dictionaries. First, we construct a dictionary-based
heterogeneous graph (DHG) from bilingual dictionaries. This opens the
possibility to use graph neural networks for cross-lingual transfer. The
remaining challenge is the heterogeneity of DHG because multiple languages are
considered. To address this challenge, we propose dictionary-based
heterogeneous graph neural network (DHGNet) that effectively handles the
heterogeneity of DHG by two-step aggregations, which are word-level and
language-level aggregations. Experimental results demonstrate that our method
outperforms pretrained models even though it does not access to large corpora.
Furthermore, it can perform well even though dictionaries contain many
incorrect translations. Its robustness allows the usage of a wider range of
dictionaries such as an automatically constructed dictionary and crowdsourced
dictionary, which are convenient for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization. (arXiv:2109.04527v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04527">
<div class="article-summary-box-inner">
<span><p>Visual localization is the problem of estimating the position and orientation
from which a given image (or a sequence of images) is taken in a known scene.
It is an important part of a wide range of computer vision and robotics
applications, from self-driving cars to augmented/virtual reality systems.
Visual localization techniques should work reliably and robustly under a wide
range of conditions, including seasonal, weather, illumination and man-made
changes. Recent benchmarking efforts model this by providing images under
different conditions, and the community has made rapid progress on these
datasets since their inception. However, they are limited to a few geographical
regions and often recorded with a single device. We propose a new benchmark for
visual localization in outdoor scenes, using crowd-sourced data to cover a wide
range of geographical regions and camera devices with a focus on the failure
cases of current algorithms. Experiments with state-of-the-art localization
approaches show that our dataset is very challenging, with all evaluated
methods failing on its hardest parts. As part of the dataset release, we
provide the tooling used to generate it, enabling efficient and effective 2D
correspondence annotation to obtain reference poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04553">
<div class="article-summary-box-inner">
<span><p>As an essential ingredient of modern deep learning, attention mechanism,
especially self-attention, plays a vital role in the global correlation
discovery. However, is hand-crafted attention irreplaceable when modeling the
global context? Our intriguing finding is that self-attention is not better
than the matrix decomposition (MD) model developed 20 years ago regarding the
performance and computational cost for encoding the long-distance dependencies.
We model the global context issue as a low-rank recovery problem and show that
its optimization algorithms can help design global information blocks. This
paper then proposes a series of Hamburgers, in which we employ the optimization
algorithms for solving MDs to factorize the input representations into
sub-matrices and reconstruct a low-rank embedding. Hamburgers with different
MDs can perform favorably against the popular global context module
self-attention when carefully coping with gradients back-propagated through
MDs. Comprehensive experiments are conducted in the vision tasks where it is
crucial to learn the global context, including semantic segmentation and image
generation, demonstrating significant improvements over self-attention and its
variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S3G-ARM: Highly Compressive Visual Self-localization from Sequential Semantic Scene Graph Using Absolute and Relative Measurements. (arXiv:2109.04569v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04569">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of image sequence-based
self-localization (ISS) from a new highly compressive scene representation
called sequential semantic scene graph (S3G). Recent developments in deep graph
convolutional neural networks (GCNs) have enabled a highly compressive visual
place classifier (VPC) that can use a scene graph as the input modality.
However, in such a highly compressive application, the amount of information
lost in the image-to-graph mapping is significant and can damage the
classification performance. To address this issue, we propose a pair of
similarity-preserving mappings, image-to-nodes and image-to-edges, such that
the nodes and edges act as absolute and relative features, respectively, that
complement each other. Moreover, the proposed GCN-VPC is applied to a new task
of viewpoint planning (VP) of the query image sequence, which contributes to
further improvement in the VPC performance. Experiments using the public NCLT
dataset validated the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object recognition for robotics from tactile time series data utilising different neural network architectures. (arXiv:2109.04573v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04573">
<div class="article-summary-box-inner">
<span><p>Robots need to exploit high-quality information on grasped objects to
interact with the physical environment. Haptic data can therefore be used for
supplementing the visual modality. This paper investigates the use of
Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) neural
network architectures for object classification on Spatio-temporal tactile
grasping data. Furthermore, we compared these methods using data from two
different fingertip sensors (namely the BioTac SP and WTS-FT) in the same
physical setup, allowing for a realistic comparison across methods and sensors
for the same tactile object classification dataset. Additionally, we propose a
way to create more training examples from the recorded data. The results show
that the proposed method improves the maximum accuracy from 82.4% (BioTac SP
fingertips) and 90.7% (WTS-FT fingertips) with complete time-series data to
about 94% for both sensor types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Portrait Video Matting via Context Motion Network. (arXiv:2109.04598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04598">
<div class="article-summary-box-inner">
<span><p>Our automatic portrait video matting method does not require extra inputs.
Most state-of-the-art matting methods rely on semantic segmentation methods to
automatically generate the trimap. Their performance is compromised due to the
lack of temporal information. Our method exploits semantic information as well
as temporal information from optical flow and produces high-quality results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04600">
<div class="article-summary-box-inner">
<span><p>Temporal grounding aims to predict a time interval of a video clip
corresponding to a natural language query input. In this work, we present
EVOQUER, a temporal grounding framework incorporating an existing text-to-video
grounding model and a video-assisted query generation network. Given a query
and an untrimmed video, the temporal grounding model predicts the target
interval, and the predicted video clip is fed into a video translation task by
generating a simplified version of the input query. EVOQUER forms closed-loop
learning by incorporating loss functions from both temporal grounding and query
generation serving as feedback. Our experiments on two widely used datasets,
Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements
by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could
facilitate error analysis by explaining temporal grounding model behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficiently Identifying Task Groupings for Multi-Task Learning. (arXiv:2109.04617v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04617">
<div class="article-summary-box-inner">
<span><p>Multi-task learning can leverage information learned by one task to benefit
the training of other tasks. Despite this capacity, naively training all tasks
together in one model often degrades performance, and exhaustively searching
through combinations of task groupings can be prohibitively expensive. As a
result, efficiently identifying the tasks that would benefit from co-training
remains a challenging design question without a clear solution. In this paper,
we suggest an approach to select which tasks should train together in
multi-task learning models. Our method determines task groupings in a single
training run by co-training all tasks together and quantifying the effect to
which one task's gradient would affect another task's loss. On the large-scale
Taskonomy computer vision dataset, we find this method can decrease test loss
by 10.0\% compared to simply training all tasks together while operating 11.6
times faster than a state-of-the-art task grouping method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACFNet: Adaptively-Cooperative Fusion Network for RGB-D Salient Object Detection. (arXiv:2109.04627v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04627">
<div class="article-summary-box-inner">
<span><p>The reasonable employment of RGB and depth data show great significance in
promoting the development of computer vision tasks and robot-environment
interaction. However, there are different advantages and disadvantages in the
early and late fusion of the two types of data. Besides, due to the diversity
of object information, using a single type of data in a specific scenario tends
to result in semantic misleading. Based on the above considerations, we propose
an adaptively-cooperative fusion network (ACFNet) with ResinRes structure for
salient object detection. This structure is designed to flexibly utilize the
advantages of feature fusion in early and late stages. Secondly, an
adaptively-cooperative semantic guidance (ACG) scheme is designed to suppress
inaccurate features in the guidance phase. Further, we proposed a type-based
attention module (TAM) to optimize the network and enhance the multi-scale
perception of different objects. For different objects, the features generated
by different types of convolution are enhanced or suppressed by the gated
mechanism for segmentation optimization. ACG and TAM optimize the transfer of
feature streams according to their data attributes and convolution attributes,
respectively. Sufficient experiments conducted on RGB-D SOD datasets illustrate
that the proposed network performs favorably against 18 state-of-the-art
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Per Garment Capture and Synthesis for Real-time Virtual Try-on. (arXiv:2109.04654v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04654">
<div class="article-summary-box-inner">
<span><p>Virtual try-on is a promising application of computer graphics and human
computer interaction that can have a profound real-world impact especially
during this pandemic. Existing image-based works try to synthesize a try-on
image from a single image of a target garment, but it inherently limits the
ability to react to possible interactions. It is difficult to reproduce the
change of wrinkles caused by pose and body size change, as well as pulling and
stretching of the garment by hand. In this paper, we propose an alternative per
garment capture and synthesis workflow to handle such rich interactions by
training the model with many systematically captured images. Our workflow is
composed of two parts: garment capturing and clothed person image synthesis. We
designed an actuated mannequin and an efficient capturing process that collects
the detailed deformations of the target garments under diverse body sizes and
poses. Furthermore, we proposed to use a custom-designed measurement garment,
and we captured paired images of the measurement garment and the target
garments. We then learn a mapping between the measurement garment and the
target garments using deep image-to-image translation. The customer can then
try on the target garments interactively during online shopping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIP: Physical Interaction Prediction via Mental Imagery with Span Selection. (arXiv:2109.04683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04683">
<div class="article-summary-box-inner">
<span><p>To align advanced artificial intelligence (AI) with human values and promote
safe AI, it is important for AI to predict the outcome of physical
interactions. Even with the ongoing debates on how humans predict the outcomes
of physical interactions among objects in the real world, there are works
attempting to tackle this task via cognitive-inspired AI approaches. However,
there is still a lack of AI approaches that mimic the mental imagery humans use
to predict physical interactions in the real world. In this work, we propose a
novel PIP scheme: Physical Interaction Prediction via Mental Imagery with Span
Selection. PIP utilizes a deep generative model to output future frames of
physical interactions among objects before extracting crucial information for
predicting physical interactions by focusing on salient frames using span
selection. To evaluate our model, we propose a large-scale SPACE+ dataset of
synthetic video frames, including three physical interaction events in a 3D
environment. Our experiments show that PIP outperforms baselines and human
performance in physical interaction prediction for both seen and unseen
objects. Furthermore, PIP's span selection scheme can effectively identify the
frames where physical interactions among objects occur within the generated
frames, allowing for added interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual 3D Scene Flow Learning with Context-Aware Feature Extraction. (arXiv:2109.04685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04685">
<div class="article-summary-box-inner">
<span><p>Scene flow estimation is the task to predict the point-wise 3D displacement
vector between two consecutive frames of point clouds, which has important
application in fields such as service robots and autonomous driving. Although
many previous works have explored greatly on scene flow estimation based on
point clouds, we point out two problems that have not been noticed or well
solved before: 1) Points of adjacent frames in repetitive patterns may be
wrongly associated due to similar spatial structure in their neighbourhoods; 2)
Scene flow between adjacent frames of point clouds with long-distance movement
may be inaccurately estimated. To solve the first problem, we propose a novel
context-aware set conv layer to exploit contextual structure information of
Euclidean space and learn soft aggregation weights for local point features.
Our design is inspired by human perception of contextual structure information
during scene understanding. We incorporate the context-aware set conv layer in
a context-aware point feature pyramid module of 3D point clouds for scene flow
estimation. For the second problem, we propose an explicit residual flow
learning structure in the residual flow refinement layer to cope with
long-distance movement. The experiments and ablation study on FlyingThings3D
and KITTI scene flow datasets demonstrate the effectiveness of each proposed
component and that we solve problem of ambiguous inter-frame association and
long-distance movement estimation. Quantitative results on both FlyingThings3D
and KITTI scene flow datasets show that our method achieves state-of-the-art
performance, surpassing all other previous works to the best of our knowledge
by at least 25%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face-NMS: A Core-set Selection Approach for Efficient Face Recognition. (arXiv:2109.04698v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04698">
<div class="article-summary-box-inner">
<span><p>Recently, face recognition in the wild has achieved remarkable success and
one key engine is the increasing size of training data. For example, the
largest face dataset, WebFace42M contains about 2 million identities and 42
million faces. However, a massive number of faces raise the constraints in
training time, computing resources, and memory cost. The current research on
this problem mainly focuses on designing an efficient Fully-connected layer
(FC) to reduce GPU memory consumption caused by a large number of identities.
In this work, we relax these constraints by resolving the redundancy problem of
the up-to-date face datasets caused by the greedily collecting operation (i.e.
the core-set selection perspective). As the first attempt in this perspective
on the face recognition problem, we find that existing methods are limited in
both performance and efficiency. For superior cost-efficiency, we contribute a
novel filtering strategy dubbed Face-NMS. Face-NMS works on feature space and
simultaneously considers the local and global sparsity in generating core sets.
In practice, Face-NMS is analogous to Non-Maximum Suppression (NMS) in the
object detection community. It ranks the faces by their potential contribution
to the overall sparsity and filters out the superfluous face in the pairs with
high similarity for local sparsity. With respect to the efficiency aspect,
Face-NMS accelerates the whole pipeline by applying a smaller but sufficient
proxy dataset in training the proxy model. As a result, with Face-NMS, we
successfully scale down the WebFace42M dataset to 60% while retaining its
performance on the main benchmarks, offering a 40% resource-saving and 1.64
times acceleration. The code is publicly available for reference at
https://github.com/HuangJunJie2017/Face-NMS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
<div class="article-summary-box-inner">
<span><p>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering. (arXiv:2109.04735v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04735">
<div class="article-summary-box-inner">
<span><p>Video question answering (VideoQA) is challenging given its multimodal
combination of visual understanding and natural language understanding. While
existing approaches seldom leverage the appearance-motion information in the
video at multiple temporal scales, the interaction between the question and the
visual information for textual semantics extraction is frequently ignored.
Targeting these issues, this paper proposes a novel Temporal Pyramid
Transformer (TPT) model with multimodal interaction for VideoQA. The TPT model
comprises two modules, namely Question-specific Transformer (QT) and Visual
Inference (VI). Given the temporal pyramid constructed from a video, QT builds
the question semantics from the coarse-to-fine multimodal co-occurrence between
each word and the visual content. Under the guidance of such question-specific
semantics, VI infers the visual clues from the local-to-global multi-level
interactions between the question and the video. Within each module, we
introduce a multimodal attention mechanism to aid the extraction of
question-video interactions, with residual connections adopted for the
information passing across different levels. Through extensive experiments on
three VideoQA datasets, we demonstrate better performances of the proposed
method in comparison with the state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization. (arXiv:2109.04753v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04753">
<div class="article-summary-box-inner">
<span><p>Along with feature points for image matching, line features provide
additional constraints to solve visual geometric problems in robotics and
computer vision (CV). Although recent convolutional neural network (CNN)-based
line descriptors are promising for viewpoint changes or dynamic environments,
we claim that the CNN architecture has innate disadvantages to abstract
variable line length into the fixed-dimensional descriptor. In this paper, we
effectively introduce Line-Transformers dealing with variable lines. Inspired
by natural language processing (NLP) tasks where sentences can be understood
and abstracted well in neural nets, we view a line segment as a sentence that
contains points (words). By attending to well-describable points on aline
dynamically, our descriptor performs excellently on variable line length. We
also propose line signature networks sharing the line's geometric attributes to
neighborhoods. Performing as group descriptors, the networks enhance line
descriptors by understanding lines' relative geometries. Finally, we present
the proposed line descriptor and matching in a Point and Line Localization
(PL-Loc). We show that the visual localization with feature points can be
improved using our line features. We validate the proposed method for
homography estimation and visual localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReconfigISP: Reconfigurable Camera Image Processing Pipeline. (arXiv:2109.04760v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04760">
<div class="article-summary-box-inner">
<span><p>Image Signal Processor (ISP) is a crucial component in digital cameras that
transforms sensor signals into images for us to perceive and understand.
Existing ISP designs always adopt a fixed architecture, e.g., several
sequential modules connected in a rigid order. Such a fixed ISP architecture
may be suboptimal for real-world applications, where camera sensors, scenes and
tasks are diverse. In this study, we propose a novel Reconfigurable ISP
(ReconfigISP) whose architecture and parameters can be automatically tailored
to specific data and tasks. In particular, we implement several ISP modules,
and enable backpropagation for each module by training a differentiable proxy,
hence allowing us to leverage the popular differentiable neural architecture
search and effectively search for the optimal ISP architecture. A proxy tuning
mechanism is adopted to maintain the accuracy of proxy networks in all cases.
Extensive experiments conducted on image restoration and object detection, with
different sensors, light conditions and efficiency constraints, validate the
effectiveness of ReconfigISP. Only hundreds of parameters need tuning for every
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04797">
<div class="article-summary-box-inner">
<span><p>Computational fluid dynamics (CFD) is a valuable tool for personalised,
non-invasive evaluation of hemodynamics in arteries, but its complexity and
time-consuming nature prohibit large-scale use in practice. Recently, the use
of deep learning for rapid estimation of CFD parameters like wall shear stress
(WSS) on surface meshes has been investigated. However, existing approaches
typically depend on a hand-crafted re-parametrisation of the surface mesh to
match convolutional neural network architectures. In this work, we propose to
instead use mesh convolutional neural networks that directly operate on the
same finite-element surface mesh as used in CFD. We train and evaluate our
method on two datasets of synthetic coronary artery models with and without
bifurcation, using a ground truth obtained from CFD simulation. We show that
our flexible deep learning model can accurately predict 3D WSS vectors on this
surface mesh. Our method processes new meshes in less than 5 [s], consistently
achieves a normalised mean absolute error of $\leq$ 1.6 [%], and peaks at 90.5
[%] median approximation accuracy over the held-out test set, comparing
favorably to previously published work. This shows the feasibility of CFD
surrogate modelling using mesh convolutional neural networks for hemodynamic
parameter estimation in artery models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TADA: Taxonomy Adaptive Domain Adaptation. (arXiv:2109.04813v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04813">
<div class="article-summary-box-inner">
<span><p>Traditional domain adaptation addresses the task of adapting a model to a
novel target domain under limited or no additional supervision. While tackling
the input domain gap, the standard domain adaptation settings assume no domain
change in the output space. In semantic prediction tasks, different datasets
are often labeled according to different semantic taxonomies. In many
real-world settings, the target domain task requires a different taxonomy than
the one imposed by the source domain. We therefore introduce the more general
taxonomy adaptive domain adaptation (TADA) problem, allowing for inconsistent
taxonomies between the two domains. We further propose an approach that jointly
addresses the image-level and label-level domain adaptation. On the
label-level, we employ a bilateral mixed sampling strategy to augment the
target domain, and a relabelling method to unify and align the label spaces. We
address the image-level domain gap by proposing an uncertainty-rectified
contrastive learning method, leading to more domain-invariant and class
discriminative features. We extensively evaluate the effectiveness of our
framework under different TADA settings: open taxonomy, coarse-to-fine
taxonomy, and partially-overlapping taxonomy. Our framework outperforms
previous state-of-the-art by a large margin, while capable of adapting to new
target domain taxonomies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporally Coherent Person Matting Trained on Fake-Motion Dataset. (arXiv:2109.04843v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04843">
<div class="article-summary-box-inner">
<span><p>We propose a novel neural-network-based method to perform matting of videos
depicting people that does not require additional user input such as trimaps.
Our architecture achieves temporal stability of the resulting alpha mattes by
using motion-estimation-based smoothing of image-segmentation algorithm
outputs, combined with convolutional-LSTM modules on U-Net skip connections.
</p>
<p>We also propose a fake-motion algorithm that generates training clips for the
video-matting network given photos with ground-truth alpha mattes and
background videos. We apply random motion to photos and their mattes to
simulate movement one would find in real videos and composite the result with
the background clips. It lets us train a deep neural network operating on
videos in an absence of a large annotated video dataset and provides
ground-truth training-clip foreground optical flow for use in loss functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emerging AI Security Threats for Autonomous Cars -- Case Studies. (arXiv:2109.04865v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04865">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence has made a significant contribution to autonomous
vehicles, from object detection to path planning. However, AI models require a
large amount of sensitive training data and are usually computationally
intensive to build. The commercial value of such models motivates attackers to
mount various attacks. Adversaries can launch model extraction attacks for
monetization purposes or step-ping-stone towards other attacks like model
evasion. In specific cases, it even results in destroying brand reputation,
differentiation, and value proposition. In addition, IP laws and AI-related
legalities are still evolving and are not uniform across countries. We discuss
model extraction attacks in detail with two use-cases and a generic kill-chain
that can compromise autonomous cars. It is essential to investigate strategies
to manage and mitigate the risk of model theft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation. (arXiv:2109.04871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04871">
<div class="article-summary-box-inner">
<span><p>Event camera has offered promising alternative for visual perception,
especially in high speed and high dynamic range scenes. Recently, many deep
learning methods have shown great success in providing model-free solutions to
many event-based problems, such as optical flow estimation. However, existing
deep learning methods did not address the importance of temporal information
well from the perspective of architecture design and cannot effectively extract
spatio-temporal features. Another line of research that utilizes Spiking Neural
Network suffers from training issues for deeper architecture. To address these
points, a novel input representation is proposed that captures the events
temporal distribution for signal enhancement. Moreover, we introduce a
spatio-temporal recurrent encoding-decoding neural network architecture for
event-based optical flow estimation, which utilizes Convolutional Gated
Recurrent Units to extract feature maps from a series of event images. Besides,
our architecture allows some traditional frame-based core modules, such as
correlation layer and iterative residual refine scheme, to be incorporated. The
network is end-to-end trained with self-supervised learning on the
Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms
all the existing state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding. (arXiv:2109.04872v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04872">
<div class="article-summary-box-inner">
<span><p>Temporal grounding aims to temporally localize a video moment in the video
whose semantics are related to a given natural language query. Existing methods
typically apply a detection or regression pipeline on the fused representation
with a focus on designing complicated heads and fusion strategies. Instead,
from a perspective on temporal grounding as a metric-learning problem, we
present a Dual Matching Network (DMN), to directly model the relations between
language queries and video moments in a joint embedding space. This new
metric-learning framework enables fully exploiting negative samples from two
new aspects: constructing negative cross-modal pairs from a dual matching
scheme and mining negative pairs across different videos. These new negative
samples could enhance the joint representation learning of two modalities via
cross-modal pair discrimination to maximize their mutual information.
Experiments show that DMN achieves highly competitive performance compared with
state-of-the-art methods on four video grounding benchmarks. Based on DMN, we
present a winner solution for STVG challenge of the 3rd PIC workshop. This
suggests that metric-learning is still a promising method for temporal
grounding via capturing the essential cross-modal correlation in a joint
embedding space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resolving gas bubbles ascending in liquid metal from low-SNR neutron radiography images. (arXiv:2109.04883v1 [physics.flu-dyn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04883">
<div class="article-summary-box-inner">
<span><p>We demonstrate a new image processing methodology for resolving gas bubbles
travelling through liquid metal from dynamic neutron radiography images with
intrinsically low signal-to-noise ratio. Image pre-processing, denoising and
bubble segmentation are described in detail, with practical recommendations.
Experimental validation is presented - stationary and moving reference bodies
with neutron-transparent cavities are radiographed with imaging conditions
similar to the cases with bubbles in liquid metal. The new methods are applied
to our experimental data from previous and recent imaging campaigns, and the
performance of the methods proposed in this paper is compared against our
previously developed methods. Significant improvements are observed as well as
the capacity to reliably extract physically meaningful information from
measurements performed under highly adverse imaging conditions. The showcased
image processing solution and separate elements thereof are readily extendable
beyond the present application, and have been made open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LibFewShot: A Comprehensive Library for Few-shot Learning. (arXiv:2109.04898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04898">
<div class="article-summary-box-inner">
<span><p>Few-shot learning, especially few-shot image classification, has received
increasing attention and witnessed significant advances in recent years. Some
recent studies implicitly show that many generic techniques or ``tricks'', such
as data augmentation, pre-training, knowledge distillation, and
self-supervision, may greatly boost the performance of a few-shot learning
method. Moreover, different works may employ different software platforms,
different training schedules, different backbone architectures and even
different input image sizes, making fair comparisons difficult and
practitioners struggle with reproducibility. To address these situations, we
propose a comprehensive library for few-shot learning (LibFewShot) by
re-implementing seventeen state-of-the-art few-shot learning methods in a
unified framework with the same single codebase in PyTorch. Furthermore, based
on LibFewShot, we provide comprehensive evaluations on multiple benchmark
datasets with multiple backbone architectures to evaluate common pitfalls and
effects of different training tricks. In addition, given the recent doubts on
the necessity of meta- or episodic-training mechanism, our evaluation results
show that such kind of mechanism is still necessary especially when combined
with pre-training. We hope our work can not only lower the barriers for
beginners to work on few-shot learning but also remove the effects of the
nontrivial tricks to facilitate intrinsic research on few-shot learning. The
source code is available from https://github.com/RL-VIG/LibFewShot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04954">
<div class="article-summary-box-inner">
<span><p>Artificial learning systems aspire to mimic human intelligence by continually
learning from a stream of tasks without forgetting past knowledge. One way to
enable such learning is to store past experiences in the form of input examples
in episodic memory and replay them when learning new tasks. However,
performance of such method suffers as the size of the memory becomes smaller.
In this paper, we propose a new approach for experience replay, where we select
the past experiences by looking at the saliency maps which provide visual
explanations for the model's decision. Guided by these saliency maps, we pack
the memory with only the parts or patches of the input images important for the
model's prediction. While learning a new task, we replay these memory patches
with appropriate zero-padding to remind the model about its past decisions. We
evaluate our algorithm on diverse image classification datasets and report
better performance than the state-of-the-art approaches. With qualitative and
quantitative analyses we show that our method captures richer summary of past
experiences without any memory increase, and hence performs well with small
episodic memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Displacement and Vibration Measurement in Laboratory Experiments with A Deep Learning Method. (arXiv:2109.04960v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04960">
<div class="article-summary-box-inner">
<span><p>This paper proposes a pipeline to automatically track and measure
displacement and vibration of structural specimens during laboratory
experiments. The latest Mask Regional Convolutional Neural Network (Mask R-CNN)
can locate the targets and monitor their movement from videos recorded by a
stationary camera. To improve precision and remove the noise, techniques such
as Scale-invariant Feature Transform (SIFT) and various filters for signal
processing are included. Experiments on three small-scale reinforced concrete
beams and a shaking table test are utilized to verify the proposed method.
Results show that the proposed deep learning method can achieve the goal to
automatically and precisely measure the motion of tested structural members
during laboratory experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">View Blind-spot as Inpainting: Self-Supervised Denoising with Mask Guided Residual Convolution. (arXiv:2109.04970v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04970">
<div class="article-summary-box-inner">
<span><p>In recent years, self-supervised denoising methods have shown impressive
performance, which circumvent painstaking collection procedure of noisy-clean
image pairs in supervised denoising methods and boost denoising applicability
in real world. One of well-known self-supervised denoising strategies is the
blind-spot training scheme. However, a few works attempt to improve blind-spot
based self-denoiser in the aspect of network architecture. In this paper, we
take an intuitive view of blind-spot strategy and consider its process of using
neighbor pixels to predict manipulated pixels as an inpainting process.
Therefore, we propose a novel Mask Guided Residual Convolution (MGRConv) into
common convolutional neural networks, e.g. U-Net, to promote blind-spot based
denoising. Our MGRConv can be regarded as soft partial convolution and find a
trade-off among partial convolution, learnable attention maps, and gated
convolution. It enables dynamic mask learning with appropriate mask constrain.
Different from partial convolution and gated convolution, it provides moderate
freedom for network learning. It also avoids leveraging external learnable
parameters for mask activation, unlike learnable attention maps. The
experiments show that our proposed plug-and-play MGRConv can assist blind-spot
based denoising network to reach promising results on both existing
single-image based and dataset-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic Narrative Grounding. (arXiv:2109.04988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04988">
<div class="article-summary-box-inner">
<span><p>This paper proposes Panoptic Narrative Grounding, a spatially fine and
general formulation of the natural language visual grounding problem. We
establish an experimental framework for the study of this new task, including
new ground truth and metrics, and we propose a strong baseline method to serve
as stepping stone for future work. We exploit the intrinsic semantic richness
in an image by including panoptic categories, and we approach visual grounding
at a fine-grained level by using segmentations. In terms of ground truth, we
propose an algorithm to automatically transfer Localized Narratives annotations
to specific regions in the panoptic segmentations of the MS COCO dataset. To
guarantee the quality of our annotations, we take advantage of the semantic
structure contained in WordNet to exclusively incorporate noun phrases that are
grounded to a meaningfully related panoptic segmentation region. The proposed
baseline achieves a performance of 55.4 absolute Average Recall points. This
result is a suitable foundation to push the envelope further in the development
of methods for Panoptic Narrative Grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Change Detection in Hyperspectral Images using Feature Fusion Deep Convolutional Autoencoders. (arXiv:2109.04990v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04990">
<div class="article-summary-box-inner">
<span><p>Binary change detection in bi-temporal co-registered hyperspectral images is
a challenging task due to a large number of spectral bands present in the data.
Researchers, therefore, try to handle it by reducing dimensions. The proposed
work aims to build a novel feature extraction system using a feature fusion
deep convolutional autoencoder for detecting changes between a pair of such
bi-temporal co-registered hyperspectral images. The feature fusion considers
features across successive levels and multiple receptive fields and therefore
adds a competitive edge over the existing feature extraction methods. The
change detection technique described is completely unsupervised and is much
more elegant than other supervised or semi-supervised methods which require
some amount of label information. Different methods have been applied to the
extracted features to find the changes in the two images and it is found that
the proposed method clearly outperformed the state of the art methods in
unsupervised change detection for all the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of GAN-synthesized street videos. (arXiv:2109.04991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04991">
<div class="article-summary-box-inner">
<span><p>Research on the detection of AI-generated videos has focused almost
exclusively on face videos, usually referred to as deepfakes. Manipulations
like face swapping, face reenactment and expression manipulation have been the
subject of an intense research with the development of a number of efficient
tools to distinguish artificial videos from genuine ones. Much less attention
has been paid to the detection of artificial non-facial videos. Yet, new tools
for the generation of such kind of videos are being developed at a fast pace
and will soon reach the quality level of deepfake videos. The goal of this
paper is to investigate the detectability of a new kind of AI-generated videos
framing driving street sequences (here referred to as DeepStreets videos),
which, by their nature, can not be analysed with the same tools used for facial
deepfakes. Specifically, we present a simple frame-based detector, achieving
very good performance on state-of-the-art DeepStreets videos generated by the
Vid2vid architecture. Noticeably, the detector retains very good performance on
compressed videos, even when the compression level used during training does
not match that used for the test videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04993">
<div class="article-summary-box-inner">
<span><p>Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA. (arXiv:2109.05014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05014">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (VQA) involves answering questions
that require external knowledge not present in the image. Existing methods
first retrieve knowledge from external resources, then reason over the selected
knowledge, the input image, and question for answer prediction. However, this
two-step approach could lead to mismatches that potentially limit the VQA
performance. For example, the retrieved knowledge might be noisy and irrelevant
to the question, and the re-embedded knowledge features during reasoning might
deviate from their original meanings in the knowledge base (KB). To address
this challenge, we propose PICa, a simple yet effective method that Prompts
GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by
GPT-3's power in knowledge retrieval and question answering, instead of using
structured KBs as in previous work, we treat GPT-3 as an implicit and
unstructured KB that can jointly acquire and process relevant knowledge.
Specifically, we first convert the image into captions (or tags) that GPT-3 can
understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just
providing a few in-context VQA examples. We further boost performance by
carefully investigating: (i) what text formats best describe the image content,
and (ii) how in-context examples can be better selected and used. PICa unlocks
the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa
surpasses the supervised state of the art by an absolute +8.6 points on the
OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent
few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition. (arXiv:2002.05809v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.05809">
<div class="article-summary-box-inner">
<span><p>Hidden Markov Models (HMMs) comprise a powerful generative approach for
modeling sequential data and time-series in general. However, the commonly
employed assumption of the dependence of the current time frame to a single or
multiple immediately preceding frames is unrealistic; more complicated dynamics
potentially exist in real world scenarios. This paper revisits conventional
sequential modeling approaches, aiming to address the problem of capturing
time-varying temporal dependency patterns. To this end, we propose a different
formulation of HMMs, whereby the dependence on past frames is dynamically
inferred from the data. Specifically, we introduce a hierarchical extension by
postulating an additional latent variable layer; therein, the (time-varying)
temporal dependence patterns are treated as latent variables over which
inference is performed. We leverage solid arguments from the Variational Bayes
framework and derive a tractable inference algorithm based on the
forward-backward algorithm. As we experimentally show, our approach can model
highly complex sequential data and can effectively handle data with missing
values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Lung Nodules Segmentation with Detailed Representation Transfer and Soft Mask Supervision. (arXiv:2007.14556v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14556">
<div class="article-summary-box-inner">
<span><p>Accurate lung nodules segmentation from Computed Tomography (CT) images is
crucial to the analysis and diagnosis of lung diseases such as COVID-19 and
lung cancer. However, due to the smallness and variety of lung nodules and the
lack of high-quality labeling, accurate lung nodule segmentation is still a
challenging problem. To address these issues, we propose a complete paradigm
for accurate lung nodules segmentation. First, we introduce a new segmentation
mask named Soft Mask which has richer and more accurate edge details
description and better visualization. Correspondingly, we develop a universal
semi-automatic Soft Mask annotation pipeline to deal with different datasets.
Second, a novel Network with Detailed representation transfer and Soft Mask
supervision (DSNet) is proposed to process the input low-resolution images of
lung nodules into high-quality segmentation results. In our DSNet, we design a
novel Selective Detailed Representation Fusion Module to reconstruct the
detailed representation to alleviate the small size of lung nodules images. In
addition, the adversarial training framework with Soft Mask is proposed to
further improve the accuracy of segmentation. Extensive experiments validate
that our DSNet outperforms the state-of-the-art methods for accurate lung
nodules segmentation. And our method also demonstrates competitive results in
other accurate medical segmentation tasks. Besides, we provide a new
challenging lung nodules segmentation dataset for further studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose Estimation for Robot Manipulators via Keypoint Optimization and Sim-to-Real Transfer. (arXiv:2010.08054v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08054">
<div class="article-summary-box-inner">
<span><p>Keypoint detection is an essential building block for many robotic
applications like motion capture and pose estimation. Historically, keypoints
are detected using uniquely engineered markers such as checkerboards or
fiducials. More recently, deep learning methods have been explored as they have
the ability to detect user-defined keypoints in a marker-less manner. However,
different manually selected keypoints can have uneven performance when it comes
to detection and localization. An example of this can be found on symmetric
robotic tools where DNN detectors cannot solve the correspondence problem
correctly. In this work, we propose a new and autonomous way to define the
keypoint locations that overcomes these challenges. The approach involves
finding the optimal set of keypoints on robotic manipulators for robust visual
detection and localization. Using a robotic simulator as a medium, our
algorithm utilizes synthetic data for DNN training, and the proposed algorithm
is used to optimize the selection of keypoints through an iterative approach.
The results show that when using the optimized keypoints, the detection
performance of the DNNs improved significantly. We further use the optimized
keypoints for real robotic applications by using domain randomization to bridge
the reality gap between the simulator and the physical world. The physical
world experiments show how the proposed method can be applied to the
wide-breadth of robotic applications that require visual feedback, such as
camera-to-robot calibration, robotic tool tracking, and end-effector pose
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose. (arXiv:2010.13321v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13321">
<div class="article-summary-box-inner">
<span><p>Recognition of human poses and actions is crucial for autonomous systems to
interact smoothly with people. However, cameras generally capture human poses
in 2D as images and videos, which can have significant appearance variations
across viewpoints that make the recognition tasks challenging. To address this,
we explore recognizing similarity in 3D human body poses from 2D information,
which has not been well-studied in existing works. Here, we propose an approach
to learning a compact view-invariant embedding space from 2D body joint
keypoints, without explicitly predicting 3D poses. Input ambiguities of 2D
poses from projection and occlusion are difficult to represent through a
deterministic mapping, and therefore we adopt a probabilistic formulation for
our embedding space. Experimental results show that our embedding model
achieves higher accuracy when retrieving similar poses across different camera
views, in comparison with 3D pose estimation models. We also show that by
training a simple temporal embedding model, we achieve superior performance on
pose sequence retrieval and largely reduce the embedding dimension from
stacking frame-based embeddings for efficient large-scale retrieval.
Furthermore, in order to enable our embeddings to work with partially visible
input, we further investigate different keypoint occlusion augmentation
strategies during training. We demonstrate that these occlusion augmentations
significantly improve retrieval performance on partial 2D input poses. Results
on action recognition and video alignment demonstrate that using our embeddings
without any additional training achieves competitive performance relative to
other models specifically trained for each task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Harnessing Transference in Multi-Task Learning. (arXiv:2010.15413v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.15413">
<div class="article-summary-box-inner">
<span><p>Multi-task learning can leverage information learned by one task to benefit
the training of other tasks. Despite this capacity, naive formulations often
degrade performance and in particular, identifying the tasks that would benefit
from co-training remains a challenging design question. In this paper, we
analyze the dynamics of information transfer, or transference, across tasks
throughout training. Specifically, we develop a similarity measure that can
quantify transference among tasks and use this quantity to both better
understand the optimization dynamics of multi-task learning as well as improve
overall learning performance. In the latter case, we propose two methods to
leverage our transference metric. The first operates at a macro-level by
selecting which tasks should train together while the second functions at a
micro-level by determining how to combine task gradients at each training step.
We find these methods can lead to significant improvement over prior work on
three supervised multi-task learning benchmarks and one multi-task
reinforcement learning paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Features Guidance Network for partial-to-partial point cloud registration. (arXiv:2011.12079v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12079">
<div class="article-summary-box-inner">
<span><p>To eliminate the problems of large dimensional differences, big semantic gap,
and mutual interference caused by hybrid features, in this paper, we propose a
novel Multi-Features Guidance Network for partial-to-partial point cloud
registration(MFG). The proposed network mainly includes four parts: keypoints'
feature extraction, correspondences searching, correspondences credibility
computation, and SVD, among which correspondences searching and correspondence
credibility computation are the cores of the network. Unlike the previous work,
we utilize the shape features and the spatial coordinates to guide
correspondences search independently and fusing the matching results to obtain
the final matching matrix. In the correspondences credibility computation
module, based on the conflicted relationship between the features matching
matrix and the coordinates matching matrix, we score the reliability for each
correspondence, which can reduce the impact of mismatched or non-matched
points. Experimental results show that our network outperforms the current
state-of-the-art while maintaining computational efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nerfies: Deformable Neural Radiance Fields. (arXiv:2011.12948v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12948">
<div class="article-summary-box-inner">
<span><p>We present the first method capable of photorealistically reconstructing
deformable scenes using photos/videos captured casually from mobile phones. Our
approach augments neural radiance fields (NeRF) by optimizing an additional
continuous volumetric deformation field that warps each observed point into a
canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone
to local minima, and propose a coarse-to-fine optimization method for
coordinate-based models that allows for more robust optimization. By adapting
principles from geometry processing and physical simulation to NeRF-like
models, we propose an elastic regularization of the deformation field that
further improves robustness. We show that our method can turn casually captured
selfie photos/videos into deformable NeRF models that allow for photorealistic
renderings of the subject from arbitrary viewpoints, which we dub "nerfies." We
evaluate our method by collecting time-synchronized data using a rig with two
mobile phones, yielding train/validation images of the same pose at different
viewpoints. We show that our method faithfully reconstructs non-rigidly
deforming scenes and reproduces unseen views with high fidelity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning normal appearance for fetal anomaly screening: Application to the unsupervised detection of Hypoplastic Left Heart Syndrome. (arXiv:2012.03679v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03679">
<div class="article-summary-box-inner">
<span><p>Congenital heart disease is considered as one the most common groups of
congenital malformations which affects $6-11$ per $1000$ newborns. In this
work, an automated framework for detection of cardiac anomalies during
ultrasound screening is proposed and evaluated on the example of Hypoplastic
Left Heart Syndrome (HLHS), a sub-category of congenital heart disease. We
propose an unsupervised approach that learns healthy anatomy exclusively from
clinically confirmed normal control patients. We evaluate a number of known
anomaly detection frameworks together with a model architecture based on the
$\alpha$-GAN network and find evidence that the proposed model performs
significantly better than the state-of-the-art in image-based anomaly
detection, yielding average $0.81$ AUC \emph{and} a better robustness towards
initialisation compared to previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept Generalization in Visual Representation Learning. (arXiv:2012.05649v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05649">
<div class="article-summary-box-inner">
<span><p>Measuring concept generalization, i.e., the extent to which models trained on
a set of (seen) visual concepts can be leveraged to recognize a new set of
(unseen) concepts, is a popular way of evaluating visual representations,
especially in a self-supervised learning framework. Nonetheless, the choice of
unseen concepts for such an evaluation is usually made arbitrarily, and
independently from the seen concepts used to train representations, thus
ignoring any semantic relationships between the two. In this paper, we argue
that the semantic relationships between seen and unseen concepts affect
generalization performance and propose ImageNet-CoG, a novel benchmark on the
ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in
a principled way. Our benchmark leverages expert knowledge that comes from
WordNet in order to define a sequence of unseen IN-21K concept sets that are
semantically more and more distant from the ImageNet-1K (IN-1K) subset, a
ubiquitous training set. This allows us to benchmark visual representations
learned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31
convolution and transformer-based models and show how different architectures,
levels of supervision, regularization techniques and use of web data impact the
concept generalization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISD: Self-Supervised Learning by Iterative Similarity Distillation. (arXiv:2012.09259v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09259">
<div class="article-summary-box-inner">
<span><p>Recently, contrastive learning has achieved great results in self-supervised
learning, where the main idea is to push two augmentations of an image
(positive pairs) closer compared to other random images (negative pairs). We
argue that not all random images are equal. Hence, we introduce a self
supervised learning algorithm where we use a soft similarity for the negative
images rather than a binary distinction between positive and negative pairs. We
iteratively distill a slowly evolving teacher model to the student model by
capturing the similarity of a query image to some random images and
transferring that knowledge to the student. We argue that our method is less
constrained compared to recent contrastive learning methods, so it can learn
better features. Specifically, our method should handle unbalanced and
unlabeled data better than existing contrastive learning methods, because the
randomly chosen negative set might include many samples that are semantically
similar to the query image. In this case, our method labels them as highly
similar while standard contrastive methods label them as negative pairs. Our
method achieves comparable results to the state-of-the-art models. We also show
that our method performs better in the settings where the unlabeled data is
unbalanced. Our code is available here: https://github.com/UMBCvision/ISD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solid Texture Synthesis using Generative Adversarial Networks. (arXiv:2102.03973v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03973">
<div class="article-summary-box-inner">
<span><p>Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a
3D solid volume, exhibits advantages in numerous application domains. However,
existing methods generally synthesize solid texture with specific features,
which may result in the failure of capturing diversified textural information.
In this paper, we propose a novel generative adversarial nets-based approach
(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our
multi-scale discriminators evaluate the similarity between patch from exemplar
and slice from the generated volume, promoting the generator to synthesize
realistic solid textures. Experimental results demonstrate that the proposed
method can generate high-quality solid textures with similar visual
characteristics to the exemplar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Modelling of BRDF Textures from Flash Images. (arXiv:2102.11861v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11861">
<div class="article-summary-box-inner">
<span><p>We learn a latent space for easy capture, consistent interpolation, and
efficient reproduction of visual material appearance. When users provide a
photo of a stationary natural material captured under flashlight illumination,
first it is converted into a latent material code. Then, in the second step,
conditioned on the material code, our method produces an infinite and diverse
spatial field of BRDF model parameters (diffuse albedo, normals, roughness,
specular albedo) that subsequently allows rendering in complex scenes and
illuminations, matching the appearance of the input photograph. Technically, we
jointly embed all flash images into a latent space using a convolutional
encoder, and -- conditioned on these latent codes -- convert random spatial
fields into fields of BRDF parameters using a convolutional neural network
(CNN). We condition these BRDF parameters to match the visual characteristics
(statistics and spectra of visual features) of the input under matching light.
A user study compares our approach favorably to previous work, even those with
access to BRDF supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08922">
<div class="article-summary-box-inner">
<span><p>Text line segmentation is one of the pre-stages of modern optical character
recognition systems. The algorithmic approach proposed by this paper has been
designed for this exact purpose. Its main characteristic is the combination of
two different techniques, morphological image operations and horizontal
histogram projections. The method was developed to be applied on a historic
data collection that commonly features quality issues, such as degraded paper,
blurred text, or presence of noise. For that reason, the segmenter in question
could be of particular interest for cultural institutions, that want access to
robust line bounding boxes for a given historic document. Because of the
promising segmentation results that are joined by low computational cost, the
algorithm was incorporated into the OCR pipeline of the National Library of
Luxembourg, in the context of the initiative of reprocessing their historic
newspaper collection. The general contribution of this paper is to outline the
approach and to evaluate the gains in terms of accuracy and speed, comparing it
to the segmentation algorithm bundled with the used open source OCR software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. (arXiv:2103.16874v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16874">
<div class="article-summary-box-inner">
<span><p>The task of image-based virtual try-on aims to transfer a target clothing
item onto the corresponding region of a person, which is commonly tackled by
fitting the item to the desired body part and fusing the warped item with the
person. While an increasing number of studies have been conducted, the
resolution of synthesized images is still limited to low (e.g., 256x192), which
acts as the critical limitation against satisfying online consumers. We argue
that the limitation stems from several challenges: as the resolution increases,
the artifacts in the misaligned areas between the warped clothes and the
desired clothing regions become noticeable in the final results; the
architectures used in existing methods have low performance in generating
high-quality body parts and maintaining the texture sharpness of the clothes.
To address the challenges, we propose a novel virtual try-on method called
VITON-HD that successfully synthesizes 1024x768 virtual try-on images.
Specifically, we first prepare the segmentation map to guide our virtual try-on
synthesis, and then roughly fit the target clothing item to a given person's
body. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS
generator to handle the misaligned areas and preserve the details of 1024x768
inputs. Through rigorous comparison with existing methods, we demonstrate that
VITON-HD highly surpasses the baselines in terms of synthesized image quality
both qualitatively and quantitatively. Code is available at
https://github.com/shadow2496/VITON-HD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AAformer: Auto-Aligned Transformer for Person Re-Identification. (arXiv:2104.00921v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00921">
<div class="article-summary-box-inner">
<span><p>In person re-identification, extracting part-level features from person
images has been verified to be crucial. Most of existing CNN-based methods only
locate the human parts coarsely, or rely on pre-trained human parsing models
and fail in locating the identifiable non-human parts (e.g., knapsack). In this
paper, we introduce an alignment scheme in Transformer architecture for the
first time and propose the Auto-Aligned Transformer (AAformer) to automatically
locate both the human parts and non-human ones at patch-level. We introduce the
"part tokens", which are learnable vectors, to extract part features in
Transformer. A part token only interacts with a local subset of patches in
self-attention and learns to be the part representation. To adaptively group
the image patches into different subsets, we design the Auto-Alignment.
Auto-Alignment employs a fast variant of Optimal Transport algorithm to online
cluster the patch embeddings into several groups with the part tokens as their
prototypes. We harmoniously integrate the part alignment into the
self-attention and the output part tokens can be directly used for retrieval.
Extensive experiments validate the effectiveness of part tokens and the
superiority of AAformer over various state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis. (arXiv:2104.04767v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04767">
<div class="article-summary-box-inner">
<span><p>In recent years, the use of Generative Adversarial Networks (GANs) has become
very popular in generative image modeling. While style-based GAN architectures
yield state-of-the-art results in high-fidelity image synthesis,
computationally, they are highly complex. In our work, we focus on the
performance optimization of style-based generative models. We analyze the most
computationally hard parts of StyleGAN2, and propose changes in the generator
network to make it possible to deploy style-based generative networks in the
edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer
parameters and is x9.5 less computationally complex than StyleGAN2, while
providing comparable quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05845">
<div class="article-summary-box-inner">
<span><p>Understanding what sequence of steps are needed to complete a goal can help
artificial intelligence systems reason about human activities. Past work in NLP
has examined the task of goal-step inference for text. We introduce the visual
analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model
is given a textual goal and must choose which of four images represents a
plausible step towards that goal. With a new dataset harvested from wikiHow
consisting of 772,277 images representing human actions, we show that our task
is challenging for state-of-the-art multimodal models. Moreover, the multimodal
representation learned from our data can be effectively transferred to other
datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task
will facilitate multimodal reasoning about procedural events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAM: A Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07566">
<div class="article-summary-box-inner">
<span><p>Recovering texture information from the aliasing regions has always been a
major challenge for Single Image Super Resolution (SISR) task. These regions
are often submerged in noise so that we have to restore texture details while
suppressing noise. To address this issue, we propose a Balanced Attention
Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and
Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to
suppress extreme noise in the large scale feature maps while MSAM preserves
high-frequency texture details. Thanks to the parallel structure, these two
modules not only conduct self-optimization, but also mutual optimization to
obtain the balance of noise reduction and high-frequency texture restoration
during the back propagation process, and the parallel structure makes the
inference faster. To verify the effectiveness and robustness of BAM, we applied
it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently
improve the networks performance, and for those originally with attention
mechanism, the substitution with BAM further reduces the amount of parameters
and increases the inference speed. Moreover, we present a dataset with rich
texture aliasing regions in real scenes, named realSR7. Experiments prove that
BAM achieves better super-resolution results on the aliasing area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Wikily" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08384">
<div class="article-summary-box-inner">
<span><p>We present a simple but effective approach for leveraging Wikipedia for
neural machine translation as well as cross-lingual tasks of image captioning
and dependency parsing without using any direct supervision from external
parallel data or supervised models in the target language. We show that first
sentences and titles of linked Wikipedia pages, as well as cross-lingual image
captions, are strong signals for a seed parallel data to extract bilingual
dictionaries and cross-lingual word embeddings for mining parallel text from
Wikipedia. Our final model achieves high BLEU scores that are close to or
sometimes higher than strong supervised baselines in low-resource languages;
e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.
Moreover, we tailor our wikily supervised translation models to unsupervised
image captioning, and cross-lingual dependency parser transfer. In image
captioning, we train a multi-tasking machine translation and image captioning
pipeline for Arabic and English from which the Arabic training data is a
translated version of the English captioning data, using our wikily-supervised
translation models. Our captioning results on Arabic are slightly better than
that of its supervised model. In dependency parsing, we translate a large
amount of monolingual text, and use it as artificial training data in an
annotation projection framework. We show that our model outperforms recent work
on cross-lingual transfer of dependency parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mean Shift for Self-Supervised Learning. (arXiv:2105.07269v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07269">
<div class="article-summary-box-inner">
<span><p>Most recent self-supervised learning (SSL) algorithms learn features by
contrasting between instances of images or by clustering the images and then
contrasting between the image clusters. We introduce a simple mean-shift
algorithm that learns representations by grouping images together without
contrasting between them or adopting much of prior on the structure of the
clusters. We simply "shift" the embedding of each image to be close to the
"mean" of its neighbors. Since in our setting, the closest neighbor is always
another augmentation of the same image, our model will be identical to BYOL
when using only one nearest neighbor instead of 5 as used in our experiments.
Our model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200
epochs outperforming BYOL. Our code is available here:
https://github.com/UMBCvision/MSF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FineAction: A Fine-Grained Video Dataset for Temporal Action Localization. (arXiv:2105.11107v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11107">
<div class="article-summary-box-inner">
<span><p>Temporal action localization (TAL) is an important and challenging problem in
video understanding. However, most existing TAL benchmarks are built upon the
coarse granularity of action classes, which exhibits two major limitations in
this task. First, coarse-level actions can make the localization models overfit
in high-level context information, and ignore the atomic action details in the
video. Second, the coarse action classes often lead to the ambiguous
annotations of temporal boundaries, which are inappropriate for temporal action
localization. To tackle these problems, we develop a novel large-scale and
fine-grained video dataset, coined as FineAction, for temporal action
localization. In total, FineAction contains 103K temporal instances of 106
action categories, annotated in 17K untrimmed videos. FineAction introduces new
opportunities and challenges for temporal action localization, thanks to its
distinct characteristics of fine action classes with rich diversity, dense
annotations of multiple instances, and co-occurring actions of different
classes. To benchmark FineAction, we systematically investigate the performance
of several popular temporal localization methods on it, and deeply analyze the
influence of short-duration and fine-grained instances in temporal action
localization. We believe that FineAction can advance research of temporal
action localization and beyond.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07806">
<div class="article-summary-box-inner">
<span><p>Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11858">
<div class="article-summary-box-inner">
<span><p>Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13228">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this "hyper-space". Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between "moments",
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for
interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
Additional videos, results, and visualizations are available at
https://hypernerf.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes for Hierarchical Vision Transformer?. (arXiv:2107.02174v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02174">
<div class="article-summary-box-inner">
<span><p>Recent studies indicate that hierarchical Vision Transformer with a macro
architecture of interleaved non-overlapped window-based self-attention \&amp;
shifted-window operation is able to achieve state-of-the-art performance in
various visual recognition tasks, and challenges the ubiquitous convolutional
neural networks (CNNs) using densely slid kernels. Most follow-up works attempt
to replace the shifted-window operation with other kinds of cross-window
communication paradigms, while treating self-attention as the de-facto standard
for window-based information aggregation. In this manuscript, we question
whether self-attention is the only choice for hierarchical Vision Transformer
to attain strong performance, and the effects of different kinds of
cross-window communication. To this end, we replace self-attention layers with
embarrassingly simple linear mapping layers, and the resulting proof-of-concept
architecture termed as LinMapper can achieve very strong performance in
ImageNet-1k image recognition. Moreover, we find that LinMapper is able to
better leverage the pre-trained representations from image recognition and
demonstrates excellent transfer learning properties on downstream dense
prediction tasks such as object detection and instance segmentation. We also
experiment with other alternatives to self-attention for content aggregation
inside each non-overlapped window under different cross-window communication
approaches, which all give similar competitive results. Our study reveals that
the \textbf{macro architecture} of Swin model families, other than specific
aggregation layers or specific means of cross-window communication, may be more
responsible for its strong performance and is the real challenger to the
ubiquitous CNN's dense sliding window paradigm. Code and models will be
publicly available to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. (arXiv:2108.01390v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01390">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have recently received explosive popularity, but
the huge computational cost is still a severe issue. Since the computation
complexity of ViT is quadratic with respect to the input sequence length, a
mainstream paradigm for computation reduction is to reduce the number of
tokens. Existing designs include structured spatial compression that uses a
progressive shrinking pyramid to reduce the computations of large feature maps,
and unstructured token pruning that dynamically drops redundant tokens.
However, the limitation of existing token pruning lies in two folds: 1) the
incomplete spatial structure caused by pruning is not compatible with
structured spatial compression that is commonly used in modern deep-narrow
transformers; 2) it usually requires a time-consuming pre-training procedure.
To tackle the limitations and expand the applicable scenario of token pruning,
we present Evo-ViT, a self-motivated slow-fast token evolution approach for
vision transformers. Specifically, we conduct unstructured instance-wise token
selection by taking advantage of the simple and effective global class
attention that is native to vision transformers. Then, we propose to update the
selected informative tokens and uninformative tokens with different computation
paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains
the spatial structure and information flow, Evo-ViT can accelerate vanilla
transformers of both flat and deep-narrow structures from the very beginning of
the training process. Experimental results demonstrate that our method
significantly reduces the computational cost of vision transformers while
maintaining comparable performance on image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach. (arXiv:2108.05060v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05060">
<div class="article-summary-box-inner">
<span><p>Multitask learning is a common approach in machine learning, which allows to
train multiple objectives with a shared architecture. It has been shown that by
training multiple tasks together inference time and compute resources can be
saved, while the objectives performance remains on a similar or even higher
level. However, in perception related multitask networks only closely related
tasks can be found, such as object detection, instance and semantic
segmentation or depth estimation. Multitask networks with diverse tasks and
their effects with respect to efficiency on one another are not well studied.
In this paper we augment the CenterNet anchor-free approach for training
multiple diverse perception related tasks together, including the task of
object detection and semantic segmentation as well as human pose estimation. We
refer to this DNN as Multitask-CenterNet (MCN). Additionally, we study
different MCN settings for efficiency. The MCN can perform several tasks at
once while maintaining, and in some cases even exceeding, the performance
values of its corresponding single task networks. More importantly, the MCN
architecture decreases inference time and reduces network size when compared to
a composition of single task networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-imaging real-time detection and tracking of fast-moving objects using a single-pixel detector. (arXiv:2108.06009v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06009">
<div class="article-summary-box-inner">
<span><p>Detection and tracking of fast-moving objects have widespread utility in many
fields. However, fulfilling this demand for fast and efficient detecting and
tracking using image-based techniques is problematic, owing to the complex
calculations and limited data processing capabilities. To tackle this problem,
we propose an image-free method to achieve real-time detection and tracking of
fast-moving objects. It employs the Hadamard pattern to illuminate the
fast-moving object by a spatial light modulator, in which the resulting light
signal is collected by a single-pixel detector. The single-pixel measurement
values are directly used to reconstruct the position information without image
reconstruction. Furthermore, a new sampling method is used to optimize the
pattern projection way for achieving an ultra-low sampling rate. Compared with
the state-of-the-art methods, our approach is not only capable of handling
real-time detection and tracking, but also it has a small amount of calculation
and high efficiency. We experimentally demonstrate that the proposed method,
using a 22kHz digital micro-mirror device, can implement a 105fps frame rate at
a 1.28% sampling rate when tracks. Our method breaks through the traditional
tracking ways, which can implement the object real-time tracking without image
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Backbone for Hyperspectral Image Reconstruction. (arXiv:2108.07739v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07739">
<div class="article-summary-box-inner">
<span><p>The study of 3D hyperspectral image (HSI) reconstruction refers to the
inverse process of snapshot compressive imaging, during which the optical
system, e.g., the coded aperture snapshot spectral imaging (CASSI) system,
captures the 3D spatial-spectral signal and encodes it to a 2D measurement.
While numerous sophisticated neural networks have been elaborated for
end-to-end reconstruction, trade-offs still need to be made among performance,
efficiency (training and inference time), and feasibility (the ability of
restoring high resolution HSI on limited GPU memory). This raises a challenge
to design a new baseline to conjointly meet the above requirements. In this
paper, we fill in this blank by proposing a Spatial/Spectral Invariant Residual
U-Net, namely SSI-ResU-Net. It differentiates with U-Net in three folds--1)
scale/spectral-invariant learning, 2) nested residual learning, and 3)
computational efficiency. Benefiting from these three modules, the proposed
SSI-ResU-Net outperforms the current state-of-the-art method TSA-Net by over 3
dB in PSNR and 0.036 in SSIM while only using 2.82% trainable parameters. To
the greatest extent, SSI-ResU-Net achieves competing performance with over
77.3% reduction in terms of floating-point operations (FLOPs), which for the
first time, makes high-resolution HSI reconstruction feasible under practical
application scenarios. Code and pre-trained models are made available at
https://github.com/Jiamian-Wang/HSI_baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Self-training for Image Classification through Self-supervision. (arXiv:2109.00778v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00778">
<div class="article-summary-box-inner">
<span><p>Self-training is a simple semi-supervised learning approach: Unlabelled
examples that attract high-confidence predictions are labelled with their
predictions and added to the training set, with this process being repeated
multiple times. Recently, self-supervision -- learning without manual
supervision by solving an automatically-generated pretext task -- has gained
prominence in deep learning. This paper investigates three different ways of
incorporating self-supervision into self-training to improve accuracy in image
classification: self-supervision as pretraining only, self-supervision
performed exclusively in the first iteration of self-training, and
self-supervision added to every iteration of self-training. Empirical results
on the SVHN, CIFAR-10, and PlantVillage datasets, using both training from
scratch, and Imagenet-pretrained weights, show that applying self-supervision
only in the first iteration of self-training can greatly improve accuracy, for
a modest increase in computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer of Pretrained Model Weights Substantially Improves Semi-Supervised Image Classification. (arXiv:2109.00788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00788">
<div class="article-summary-box-inner">
<span><p>Deep neural networks produce state-of-the-art results when trained on a large
number of labeled examples but tend to overfit when small amounts of labeled
examples are used for training. Creating a large number of labeled examples
requires considerable resources, time, and effort. If labeling new data is not
feasible, so-called semi-supervised learning can achieve better generalisation
than purely supervised learning by employing unlabeled instances as well as
labeled ones. The work presented in this paper is motivated by the observation
that transfer learning provides the opportunity to potentially further improve
performance by exploiting models pretrained on a similar domain. More
specifically, we explore the use of transfer learning when performing
semi-supervised learning using self-learning. The main contribution is an
empirical evaluation of transfer learning using different combinations of
similarity metric learning methods and label propagation algorithms in
semi-supervised learning. We find that transfer learning always substantially
improves the model's accuracy when few labeled examples are available,
regardless of the type of loss used for training the neural network. This
finding is obtained by performing extensive experiments on the SVHN, CIFAR10,
and Plant Village image classification datasets and applying pretrained weights
from Imagenet for transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning using Siamese Networks. (arXiv:2109.00794v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00794">
<div class="article-summary-box-inner">
<span><p>Neural networks have been successfully used as classification models yielding
state-of-the-art results when trained on a large number of labeled samples.
These models, however, are more difficult to train successfully for
semi-supervised problems where small amounts of labeled instances are available
along with a large number of unlabeled instances. This work explores a new
training method for semi-supervised learning that is based on similarity
function learning using a Siamese network to obtain a suitable embedding. The
learned representations are discriminative in Euclidean space, and hence can be
used for labeling unlabeled instances using a nearest-neighbor classifier.
Confident predictions of unlabeled instances are used as true labels for
retraining the Siamese network on the expanded training set. This process is
applied iteratively. We perform an empirical study of this iterative
self-training algorithm. For improving unlabeled predictions, local learning
with global consistency [22] is also evaluated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting isocitrate dehydrogenase mutation status in glioma using structural brain networks and graph neural networks. (arXiv:2109.01854v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01854">
<div class="article-summary-box-inner">
<span><p>Glioma is a common malignant brain tumor with distinct survival among
patients. The isocitrate dehydrogenase (IDH) gene mutation provides critical
diagnostic and prognostic value for glioma. It is of crucial significance to
non-invasively predict IDH mutation based on pre-treatment MRI. Machine
learning/deep learning models show reasonable performance in predicting IDH
mutation using MRI. However, most models neglect the systematic brain
alterations caused by tumor invasion, where widespread infiltration along white
matter tracts is a hallmark of glioma. Structural brain network provides an
effective tool to characterize brain organisation, which could be captured by
the graph neural networks (GNN) to more accurately predict IDH mutation.
</p>
<p>Here we propose a method to predict IDH mutation using GNN, based on the
structural brain network of patients. Specifically, we firstly construct a
network template of healthy subjects, consisting of atlases of edges (white
matter tracts) and nodes (cortical/subcortical brain regions) to provide
regions of interest (ROIs). Next, we employ autoencoders to extract the latent
multi-modal MRI features from the ROIs of edges and nodes in patients, to train
a GNN architecture for predicting IDH mutation. The results show that the
proposed method outperforms the baseline models using the 3D-CNN and
3D-DenseNet. In addition, model interpretation suggests its ability to identify
the tracts infiltrated by tumor, corresponding to clinical prior knowledge. In
conclusion, integrating brain networks with GNN offers a new avenue to study
brain lesions using computational neuroscience and computer vision approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02860">
<div class="article-summary-box-inner">
<span><p>Graph convolutional networks (GCNs) achieve promising performance for
skeleton-based action recognition. However, in most GCN-based methods, the
spatial-temporal graph convolution is strictly restricted by the graph topology
while only captures the short-term temporal context, thus lacking the
flexibility of feature extraction. In this work, we present a novel
architecture, named Graph Convolutional skeleton Transformer (GCsT), which
addresses limitations in GCNs by introducing Transformer. Our GCsT employs all
the benefits of Transformer (i.e. dynamical attention and global context) while
keeps the advantages of GCNs (i.e. hierarchy and local topology structure). In
GCsT, the spatial-temporal GCN forces the capture of local dependencies while
Transformer dynamically extracts global spatial-temporal relationships.
Furthermore, the proposed GCsT shows stronger expressive capability by adding
additional information present in skeleton sequences. Incorporating the
Transformer allows that information to be introduced into the model almost
effortlessly. We validate the proposed GCsT by conducting extensive
experiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU
RGB+D 120 and Northwestern-UCLA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02862">
<div class="article-summary-box-inner">
<span><p>Image classification is a major application domain for conventional deep
learning (DL). Quantum machine learning (QML) has the potential to
revolutionize image classification. In any typical DL-based image
classification, we use convolutional neural network (CNN) to extract features
from the image and multi-layer perceptron network (MLP) to create the actual
decision boundaries. On one hand, QML models can be useful in both of these
tasks. Convolution with parameterized quantum circuits (Quanvolution) can
extract rich features from the images. On the other hand, quantum neural
network (QNN) models can create complex decision boundaries. Therefore,
Quanvolution and QNN can be used to create an end-to-end QML model for image
classification. Alternatively, we can extract image features separately using
classical dimension reduction techniques such as, Principal Components Analysis
(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to
train a QNN. We review two proposals on quantum-classical hybrid ML models for
image classification namely, Quanvolutional Neural Network and dimension
reduction using a classical algorithm followed by QNN. Particularly, we make a
case for trainable filters in Quanvolution and CAE-based feature extraction for
image datasets (instead of dimension reduction using linear transformations
such as, PCA). We discuss various design choices, potential opportunities, and
drawbacks of these models. We also release a Python-based framework to create
and explore these hybrid models with a variety of design choices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03805">
<div class="article-summary-box-inner">
<span><p>Panoptic scene understanding and tracking of dynamic agents are essential for
robots and automated vehicles to navigate in urban environments. As LiDARs
provide accurate illumination-independent geometric depictions of the scene,
performing these tasks using LiDAR point clouds provides reliable predictions.
However, existing datasets lack diversity in the type of urban scenes and have
a limited number of dynamic object instances which hinders both learning of
these tasks as well as credible benchmarking of the developed methods. In this
paper, we introduce the large-scale Panoptic nuScenes benchmark dataset that
extends our popular nuScenes dataset with point-wise groundtruth annotations
for semantic segmentation, panoptic segmentation, and panoptic tracking tasks.
To facilitate comparison, we provide several strong baselines for each of these
tasks on our proposed dataset. Moreover, we analyze the drawbacks of the
existing metrics for panoptic tracking and propose the novel instance-centric
PAT metric that addresses the concerns. We present exhaustive experiments that
demonstrate the utility of Panoptic nuScenes compared to existing datasets and
make the online evaluation server available at nuScenes.org. We believe that
this extension will accelerate the research of novel methods for scene
understanding of dynamic urban environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Event-Line Constraint for Closed-Form Velocity Initialization. (arXiv:2109.04313v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04313">
<div class="article-summary-box-inner">
<span><p>Event cameras trigger events asynchronously and independently upon a
sufficient change of the logarithmic brightness level. The neuromorphic sensor
has several advantages over standard cameras including low latency, absence of
motion blur, and high dynamic range. Event cameras are particularly well suited
to sense motion dynamics in agile scenarios. We propose the continuous
event-line constraint, which relies on a constant-velocity motion assumption as
well as trifocal tensor geometry in order to express a relationship between
line observations given by event clusters as well as first-order camera
dynamics. Our core result is a closed-form solver for up-to-scale linear camera
velocity {with known angular velocity}. Nonlinear optimization is adopted to
improve the performance of the algorithm. The feasibility of the approach is
demonstrated through a careful analysis on both simulated and real data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IFBiD: Inference-Free Bias Detection. (arXiv:2109.04374v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04374">
<div class="article-summary-box-inner">
<span><p>This paper is the first to explore an automatic way to detect bias in deep
convolutional neural networks by simply looking at their weights. Furthermore,
it is also a step towards understanding neural networks and how they work. We
show that it is indeed possible to know if a model is biased or not simply by
looking at its weights, without the model inference for an specific input. We
analyze how bias is encoded in the weights of deep networks through a toy
example using the Colored MNIST database and we also provide a realistic case
study in gender detection from face images using state-of-the-art methods and
experimental resources. To do so, we generated two databases with 36K and 48K
biased models each. In the MNIST models we were able to detect whether they
presented a strong or low bias with more than 99% accuracy, and we were also
able to classify between four levels of bias with more than 70% accuracy. For
the face models, we achieved 90% accuracy in distinguishing between models
biased towards Asian, Black, or Caucasian ethnicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing and grounding narrated instructional videos in 3D. (arXiv:2109.04409v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04409">
<div class="article-summary-box-inner">
<span><p>Narrated instructional videos often show and describe manipulations of
similar objects, e.g., repairing a particular model of a car or laptop. In this
work we aim to reconstruct such objects and to localize associated narrations
in 3D. Contrary to the standard scenario of instance-level 3D reconstruction,
where identical objects or scenes are present in all views, objects in
different instructional videos may have large appearance variations given
varying conditions and versions of the same product. Narrations may also have
large variation in natural language expressions. We address these challenges by
three contributions. First, we propose an approach for correspondence
estimation combining learnt local features and dense flow. Second, we design a
two-step divide and conquer reconstruction approach where the initial 3D
reconstructions of individual videos are combined into a 3D alignment graph.
Finally, we propose an unsupervised approach to ground natural language in
obtained 3D reconstructions. We demonstrate the effectiveness of our approach
for the domain of car maintenance. Given raw instructional videos and no manual
supervision, our method successfully reconstructs engines of different car
models and associates textual descriptions with corresponding objects in 3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Web image search engine based on LSH index and CNN Resnet50. (arXiv:2108.13301v1 [cs.IR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13301">
<div class="article-summary-box-inner">
<span><p>To implement a good Content Based Image Retrieval (CBIR) system, it is
essential to adopt efficient search methods. One way to achieve this results is
by exploiting approximate search techniques. In fact, when we deal with very
large collections of data, using an exact search method makes the system very
slow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to
implement a CBIR system that allows us to perform fast similarity search on
deep features. Specifically, we exploit transfer learning techniques to extract
deep features from images; this phase is done using two famous Convolutional
Neural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both
pre-trained on ImageNet. Then we try out several fully connected deep neural
networks, built on top of both of the previously mentioned CNNs in order to
fine-tuned them on our dataset. In both of previous cases, we index the
features within our LSH index implementation and within a sequential scan, to
better understand how much the introduction of the index affects the results.
Finally, we carry out a performance analysis: we evaluate the relevance of the
result set, computing the mAP (mean Average Precision) value obtained during
the different experiments with respect to the number of done comparison and
varying the hyper-parameter values of the LSH index.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-13 23:02:36.020130892 UTC">2021-09-13 23:02:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>