<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-04T01:30:00Z">05-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01133">
<div class="article-summary-box-inner">
<span><p>Multi-modal Machine Translation (MMT) enables the use of visual information
to enhance the quality of translations. The visual information can serve as a
valuable piece of context information to decrease the ambiguity of input
sentences. Despite the increasing popularity of such a technique, good and
sizeable datasets are scarce, limiting the full extent of their potential.
Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It
is estimated that about 100 to 150 million people speak the language, with more
than 80 million indigenous speakers. This is more than any of the other Chadic
languages. Despite a large number of speakers, the Hausa language is considered
low-resource in natural language processing (NLP). This is due to the absence
of sufficient resources to implement most NLP tasks. While some datasets exist,
they are either scarce, machine-generated, or in the religious domain.
Therefore, there is a need to create training and evaluation data for
implementing machine learning tasks and bridging the research gap in the
language. This work presents the Hausa Visual Genome (HaVG), a dataset that
contains the description of an image or a section within the image in Hausa and
its equivalent in English. To prepare the dataset, we started by translating
the English description of the images in the Hindi Visual Genome (HVG) into
Hausa automatically. Afterward, the synthetic Hausa data was carefully
post-edited considering the respective images. The dataset comprises 32,923
images and their descriptions that are divided into training, development,
test, and challenge test set. The Hausa Visual Genome is the first dataset of
its kind and can be used for Hausa-English machine translation, multi-modal
research, and image description, among various other natural language
processing and generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Text Classification using Graph Convolutional Networks for Large-Scale Low Resource Language. (arXiv:2205.01204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01204">
<div class="article-summary-box-inner">
<span><p>Graph Convolutional Networks (GCN) have achieved state-of-art results on
single text classification tasks like sentiment analysis, emotion detection,
etc. However, the performance is achieved by testing and reporting on
resource-rich languages like English. Applying GCN for multi-task text
classification is an unexplored area. Moreover, training a GCN or adopting an
English GCN for Indian languages is often limited by data availability, rich
morphological variation, syntax, and semantic differences. In this paper, we
study the use of GCN for the Telugu language in single and multi-task settings
for four natural language processing (NLP) tasks, viz. sentiment analysis (SA),
emotion identification (EI), hate-speech (HS), and sarcasm detection (SAR). In
order to evaluate the performance of GCN with one of the Indian languages,
Telugu, we analyze the GCN based models with extensive experiments on four
downstream tasks. In addition, we created an annotated Telugu dataset, TEL-NLP,
for the four NLP tasks. Further, we propose a supervised graph reconstruction
method, Multi-Task Text GCN (MT-Text GCN) on the Telugu that leverages to
simultaneously (i) learn the low-dimensional word and sentence graph embeddings
from word-sentence graph reconstruction using graph autoencoder (GAE) and (ii)
perform multi-task text classification using these latent sentence graph
embeddings. We argue that our proposed MT-Text GCN achieves significant
improvements on TEL-NLP over existing Telugu pretrained word embeddings, and
multilingual pretrained Transformer models: mBERT, and XLM-R. On TEL-NLP, we
achieve a high F1-score for four NLP tasks: SA (0.84), EI (0.55), HS (0.83) and
SAR (0.66). Finally, we show our model's quantitative and qualitative analysis
on the four NLP tasks in Telugu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paragraph-based Transformer Pre-training for Multi-Sentence Inference. (arXiv:2205.01228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01228">
<div class="article-summary-box-inner">
<span><p>Inference tasks such as answer sentence selection (AS2) or fact verification
are typically solved by fine-tuning transformer-based models as individual
sentence-pair classifiers. Recent studies show that these tasks benefit from
modeling dependencies across multiple candidate sentences jointly. In this
paper, we first show that popular pre-trained transformers perform poorly when
used for fine-tuning on multi-candidate inference tasks. We then propose a new
pre-training objective that models the paragraph-level semantics across
multiple input sentences. Our evaluation on three AS2 and one fact verification
datasets demonstrates the superiority of our pre-training technique over the
traditional ones for transformers used as joint models for multi-candidate
inference tasks, as well as when used as cross-encoders for sentence-pair
formulations of these tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Enhanced Machine Learning. (arXiv:2205.01230v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01230">
<div class="article-summary-box-inner">
<span><p>Although information access systems have long supported people in
accomplishing a wide range of tasks, we propose broadening the scope of users
of information access systems to include task-driven machines, such as machine
learning models. In this way, the core principles of indexing, representation,
retrieval, and ranking can be applied and extended to substantially improve
model generalization, scalability, robustness, and interpretability. We
describe a generic retrieval-enhanced machine learning (REML) framework, which
includes a number of existing models as special cases. REML challenges
information retrieval conventions, presenting opportunities for novel advances
in core areas, including optimization. The REML research agenda lays a
foundation for a new style of information access research and paves a path
towards advancing machine learning and artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemAttack: Natural Textual Attacks via Different Semantic Spaces. (arXiv:2205.01287v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01287">
<div class="article-summary-box-inner">
<span><p>Recent studies show that pre-trained language models (LMs) are vulnerable to
textual adversarial attacks. However, existing attack methods either suffer
from low attack success rates or fail to search efficiently in the
exponentially large perturbation space. We propose an efficient and effective
framework SemAttack to generate natural adversarial text by constructing
different semantic perturbation functions. In particular, SemAttack optimizes
the generated perturbations constrained on generic semantic spaces, including
typo space, knowledge space (e.g., WordNet), contextualized semantic space
(e.g., the embedding space of BERT clusterings), or the combination of these
spaces. Thus, the generated adversarial texts are more semantically close to
the original inputs. Extensive experiments reveal that state-of-the-art (SOTA)
large-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are
still vulnerable to SemAttack. We further demonstrate that SemAttack is general
and able to generate natural adversarial texts for different languages (e.g.,
English and Chinese) with high attack success rates. Human evaluations also
confirm that our generated adversarial texts are natural and barely affect
human performance. Our code is publicly available at
https://github.com/AI-secure/SemAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding Hallucination for Few-Shot Language Fine-tuning. (arXiv:2205.01307v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01307">
<div class="article-summary-box-inner">
<span><p>Few-shot language learners adapt knowledge from a pre-trained model to
recognize novel classes from a few-labeled sentences. In such settings,
fine-tuning a pre-trained language model can cause severe over-fitting. In this
paper, we propose an Embedding Hallucination (EmbedHalluc) method, which
generates auxiliary embedding-label pairs to expand the fine-tuning dataset.
The hallucinator is trained by playing an adversarial game with the
discriminator, such that the hallucinated embedding is indiscriminative to the
real ones in the fine-tuning dataset. By training with the extended dataset,
the language learner effectively learns from the diverse hallucinated
embeddings to overcome the over-fitting issue. Experiments demonstrate that our
proposed method is effective in a wide range of language tasks, outperforming
current fine-tuning methods. Further, we show that EmbedHalluc outperforms
other methods that address this over-fitting problem, such as common data
augmentation, semi-supervised pseudo-labeling, and regularization. The code
will be made available at: https://github.com/yiren-jian/EmbedHalluc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Prompt-Based Few-Shot Language Learners. (arXiv:2205.01308v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01308">
<div class="article-summary-box-inner">
<span><p>The impressive performance of GPT-3 using natural language prompts and
in-context learning has inspired work on better fine-tuning of moderately-sized
models under this paradigm. Following this line of work, we present a
contrastive learning framework that clusters inputs from the same class for
better generality of models trained with only limited examples. Specifically,
we propose a supervised contrastive framework that clusters inputs from the
same class under different augmented "views" and repel the ones from different
classes. We create different "views" of an example by appending it with
different language prompts and contextual demonstrations. Combining a
contrastive loss with the standard masked language modeling (MLM) loss in
prompt-based few-shot learners, the experimental results show that our method
can improve over the state-of-the-art methods in a diverse set of 15 language
tasks. Our framework makes minimal assumptions on the task or the base model,
and can be applied to many recent methods with little modification. The code
will be made available at: https://github.com/yiren-jian/LM-SupCon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open vs Closed-ended questions in attitudinal surveys -- comparing, combining, and interpreting using natural language processing. (arXiv:2205.01317v1 [econ.GN])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01317">
<div class="article-summary-box-inner">
<span><p>To improve the traveling experience, researchers have been analyzing the role
of attitudes in travel behavior modeling. Although most researchers use
closed-ended surveys, the appropriate method to measure attitudes is debatable.
Topic Modeling could significantly reduce the time to extract information from
open-ended responses and eliminate subjective bias, thereby alleviating analyst
concerns. Our research uses Topic Modeling to extract information from
open-ended questions and compare its performance with closed-ended responses.
Furthermore, some respondents might prefer answering questions using their
preferred questionnaire type. So, we propose a modeling framework that allows
respondents to use their preferred questionnaire type to answer the survey and
enable analysts to use the modeling frameworks of their choice to predict
behavior. We demonstrate this using a dataset collected from the USA that
measures the intention to use Autonomous Vehicles for commute trips.
Respondents were presented with alternative questionnaire versions (open- and
closed- ended). Since our objective was also to compare the performance of
alternative questionnaire versions, the survey was designed to eliminate
influences resulting from statements, behavioral framework, and the choice
experiment. Results indicate the suitability of using Topic Modeling to extract
information from open-ended responses; however, the models estimated using the
closed-ended questions perform better compared to them. Besides, the proposed
model performs better compared to the models used currently. Furthermore, our
proposed framework will allow respondents to choose the questionnaire type to
answer, which could be particularly beneficial to them when using voice-based
surveys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Issue Types with seBERT. (arXiv:2205.01335v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01335">
<div class="article-summary-box-inner">
<span><p>Pre-trained transformer models are the current state-of-the-art for natural
language models processing. seBERT is such a model, that was developed based on
the BERT architecture, but trained from scratch with software engineering data.
We fine-tuned this model for the NLBSE challenge for the task of issue type
prediction. Our model dominates the baseline fastText for all three issue types
in both recall and precisio} to achieve an overall F1-score of 85.7%, which is
an increase of 4.1% over the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding patterns in Knowledge Attribution for Transformers. (arXiv:2205.01366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01366">
<div class="article-summary-box-inner">
<span><p>We analyze the Knowledge Neurons framework for the attribution of factual and
relational knowledge to particular neurons in the transformer network. We use a
12-layer multi-lingual BERT model for our experiments. Our study reveals
various interesting phenomena. We observe that mostly factual knowledge can be
attributed to middle and higher layers of the network($\ge 6$). Further
analysis reveals that the middle layers($6-9$) are mostly responsible for
relational information, which is further refined into actual factual knowledge
or the "correct answer" in the last few layers($10-12$). Our experiments also
show that the model handles prompts in different languages, but representing
the same fact, similarly, providing further evidence for effectiveness of
multi-lingual pre-training. Applying the attribution scheme for grammatical
knowledge, we find that grammatical knowledge is far more dispersed among the
neurons than factual knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden behind the obvious: misleading keywords and implicitly abusive language on social media. (arXiv:2205.01374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01374">
<div class="article-summary-box-inner">
<span><p>While social media offers freedom of self-expression, abusive language carry
significant negative social impact. Driven by the importance of the issue,
research in the automated detection of abusive language has witnessed growth
and improvement. However, these detection models display a reliance on strongly
indicative keywords, such as slurs and profanity. This means that they can
falsely (1a) miss abuse without such keywords or (1b) flag non-abuse with such
keywords, and that (2) they perform poorly on unseen data. Despite the
recognition of these problems, gaps and inconsistencies remain in the
literature. In this study, we analyse the impact of keywords from dataset
construction to model behaviour in detail, with a focus on how models make
mistakes on (1a) and (1b), and how (1a) and (1b) interact with (2). Through the
analysis, we provide suggestions for future research to address all three
problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning. (arXiv:2205.01376v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01376">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that NLP tasks such as Relation Extraction (RE) can be
recasted as Textual Entailment tasks using verbalizations, with strong
performance in zero-shot and few-shot settings thanks to pre-trained entailment
models. The fact that relations in current RE datasets are easily verbalized
casts doubts on whether entailment would be effective in more complex tasks. In
this work we show that entailment is also effective in Event Argument
Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE
and WikiEvents respectively, while achieving the same performance as with full
training. More importantly, we show that recasting EAE as entailment alleviates
the dependency on schemas, which has been a road-block for transferring
annotations between domains. Thanks to the entailment, the multi-source
transfer between ACE and WikiEvents further reduces annotation down to 10% and
5% (respectively) of the full training without transfer. Our analysis shows
that the key to good results is the use of several entailment datasets to
pre-train the entailment model. Similar to previous approaches, our method
requires a small amount of effort for manual verbalization: only less than 15
minutes per event argument type is needed, and comparable results can be
achieved with users with different level of expertise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kompetencer: Fine-grained Skill Classification in Danish Job Postings via Distant Supervision and Transfer Learning. (arXiv:2205.01381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01381">
<div class="article-summary-box-inner">
<span><p>Skill Classification (SC) is the task of classifying job competences from job
postings. This work is the first in SC applied to Danish job vacancy data. We
release the first Danish job posting dataset: Kompetencer (en: competences),
annotated for nested spans of competences. To improve upon coarse-grained
annotations, we make use of The European Skills, Competences, Qualifications
and Occupations (ESCO; le Vrang et al., 2014) taxonomy API to obtain
fine-grained labels via distant supervision. We study two setups: The zero-shot
and few-shot classification setting. We fine-tune English-based models and
RemBERT (Chung et al., 2020) and compare them to in-language Danish models. Our
results show RemBERT significantly outperforms all other models in both the
zero-shot and the few-shot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP). (arXiv:2205.01397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01397">
<div class="article-summary-box-inner">
<span><p>Contrastively trained image-text models such as CLIP, ALIGN, and BASIC have
demonstrated unprecedented robustness to multiple challenging natural
distribution shifts. Since these image-text models differ from previous
training approaches in several ways, an important question is what causes the
large robustness gains. We answer this question via a systematic experimental
investigation. Concretely, we study five different possible causes for the
robustness gains: (i) the training set size, (ii) the training distribution,
(iii) language supervision at training time, (iv) language supervision at test
time, and (v) the contrastive loss function. Our experiments show that the more
diverse training distribution is the main cause for the robustness gains, with
the other factors contributing little to no robustness. Beyond our experimental
results, we also introduce ImageNet-Captions, a version of ImageNet with
original text annotations from Flickr, to enable further controlled experiments
of language-image training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?. (arXiv:2205.01404v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01404">
<div class="article-summary-box-inner">
<span><p>Several popular Transformer based language models have been found to be
successful for text-driven brain encoding. However, existing literature
leverages only pretrained text Transformer models and has not explored the
efficacy of task-specific learned Transformer representations. In this work, we
explore transfer learning from representations learned for ten popular natural
language processing tasks (two syntactic and eight semantic) for predicting
brain responses from two diverse datasets: Pereira (subjects reading sentences
from paragraphs) and Narratives (subjects listening to the spoken stories).
Encoding models based on task features are used to predict activity in
different regions across the whole brain. Features from coreference resolution,
NER, and shallow syntax parsing explain greater variance for the reading
activity. On the other hand, for the listening activity, tasks such as
paraphrase generation, summarization, and natural language inference show
better encoding performance. Experiments across all 10 task representations
provide the following cognitive insights: (i) language left hemisphere has
higher predictive brain activity versus language right hemisphere, (ii)
posterior medial cortex, temporo-parieto-occipital junction, dorsal frontal
lobe have higher correlation versus early auditory and auditory association
cortex, (iii) syntactic and semantic tasks display a good predictive
performance across brain regions for reading and listening stimuli resp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exact Paired-Permutation Testing for Structured Test Statistics. (arXiv:2205.01416v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01416">
<div class="article-summary-box-inner">
<span><p>Significance testing -- especially the paired-permutation test -- has played
a vital role in developing NLP systems to provide confidence that the
difference in performance between two systems (i.e., the test statistic) is not
due to luck. However, practitioners rely on Monte Carlo approximation to
perform this test due to a lack of a suitable exact algorithm. In this paper,
we provide an efficient exact algorithm for the paired-permutation test for a
family of structured test statistics. Our algorithm runs in $\mathcal{O}(GN
(\log GN )(\log N ))$ time where $N$ is the dataset size and $G$ is the range
of the test statistic. We found that our exact algorithm was $10$x faster than
the Monte Carlo approximation with $20000$ samples on a common dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inducing and Using Alignments for Transition-based AMR Parsing. (arXiv:2205.01464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01464">
<div class="article-summary-box-inner">
<span><p>Transition-based parsers for Abstract Meaning Representation (AMR) rely on
node-to-word alignments. These alignments are learned separately from parser
training and require a complex pipeline of rule-based components,
pre-processing, and post-processing to satisfy domain-specific constraints.
Parsers also train on a point-estimate of the alignment pipeline, neglecting
the uncertainty due to the inherent ambiguity of alignment. In this work we
explore two avenues for overcoming these limitations. First, we propose a
neural aligner for AMR that learns node-to-word alignments without relying on
complex pipelines. We subsequently explore a tighter integration of aligner and
parser training by considering a distribution over oracle action sequences
arising from aligner uncertainty. Empirical results show this approach leads to
more accurate alignments and generalization better from the AMR2.0 to AMR3.0
corpora. We attain a new state-of-the art for gold-only trained models,
matching silver-trained performance without the need for beam search on AMR3.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Diversity in Dialogue with Natural Language Inference. (arXiv:2205.01497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01497">
<div class="article-summary-box-inner">
<span><p>Generating diverse, interesting responses to chitchat conversations is a
problem for neural conversational agents. This paper makes two substantial
contributions to improving diversity in dialogue generation. First, we propose
a novel metric which uses Natural Language Inference (NLI) to measure the
semantic diversity of a set of model responses for a conversation. We evaluate
this metric using an established framework (Tevet and Berant, 2021) and find
strong evidence indicating NLI Diversity is correlated with semantic diversity.
Specifically, we show that the contradiction relation is more useful than the
neutral relation for measuring this diversity and that incorporating the NLI
model's confidence achieves state-of-the-art results. Second, we demonstrate
how to iteratively improve the semantic diversity of a sampled set of responses
via a new generation procedure called Diversity Threshold Generation, which
results in an average 137% increase in NLI Diversity compared to standard
generation procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Learning for Natural Language Processing: A Survey. (arXiv:2205.01500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01500">
<div class="article-summary-box-inner">
<span><p>Deep learning has been the mainstream technique in natural language
processing (NLP) area. However, the techniques require many labeled data and
are less generalizable across domains. Meta-learning is an arising field in
machine learning studying approaches to learn better learning algorithms.
Approaches aim at improving algorithms in various aspects, including data
efficiency and generalizability. Efficacy of approaches has been shown in many
NLP tasks, but there is no systematic survey of these approaches in NLP, which
hinders more researchers from joining the field. Our goal with this survey
paper is to offer researchers pointers to relevant meta-learning works in NLP
and attract more attention from the NLP community to drive future innovation.
This paper first introduces the general concepts of meta-learning and the
common approaches. Then we summarize task construction settings and application
of meta-learning for various NLP problems and review the development of
meta-learning in NLP community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BasqueParl: A Bilingual Corpus of Basque Parliamentary Transcriptions. (arXiv:2205.01506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01506">
<div class="article-summary-box-inner">
<span><p>Parliamentary transcripts provide a valuable resource to understand the
reality and know about the most important facts that occur over time in our
societies. Furthermore, the political debates captured in these transcripts
facilitate research on political discourse from a computational social science
perspective. In this paper we release the first version of a newly compiled
corpus from Basque parliamentary transcripts. The corpus is characterized by
heavy Basque-Spanish code-switching, and represents an interesting resource to
study political discourse in contrasting languages such as Basque and Spanish.
We enrich the corpus with metadata related to relevant attributes of the
speakers and speeches (language, gender, party...) and process the text to
obtain named entities and lemmas. The obtained metadata is then used to perform
a detailed corpus analysis which provides interesting insights about the
language use of the Basque political representatives across time, parties and
gender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models. (arXiv:2205.01523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01523">
<div class="article-summary-box-inner">
<span><p>Nowadays, pretrained language models (PLMs) have dominated the majority of
NLP tasks. While, little research has been conducted on systematically
evaluating the language abilities of PLMs. In this paper, we present a
large-scale empirical study on general language ability evaluation of PLMs
(ElitePLM). In our study, we design four evaluation dimensions, i.e. memory,
comprehension, reasoning, and composition, to measure ten widely-used PLMs
within five categories. Our empirical results demonstrate that: (1) PLMs with
varying training objectives and strategies are good at different ability tests;
(2) fine-tuning PLMs in downstream tasks is usually sensitive to the data size
and distribution; (3) PLMs have excellent transferability between similar
tasks. Moreover, the prediction results of PLMs in our experiments are released
as an open resource for more deep and detailed analysis on the language
abilities of PLMs. This paper can guide the future work to select, apply, and
design PLMs for specific tasks. We have made all the details of experiments
publicly available at https://github.com/RUCAIBox/ElitePLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUBS: Subtree Substitution for Compositional Semantic Parsing. (arXiv:2205.01538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01538">
<div class="article-summary-box-inner">
<span><p>Although sequence-to-sequence models often achieve good performance in
semantic parsing for i.i.d. data, their performance is still inferior in
compositional generalization. Several data augmentation methods have been
proposed to alleviate this problem. However, prior work only leveraged
superficial grammar or rules for data augmentation, which resulted in limited
improvement. We propose to use subtree substitution for compositional data
augmentation, where we consider subtrees with similar semantic functions as
exchangeable. Our experiments showed that such augmented data led to
significantly better performance on SCAN and GeoQuery, and reached new SOTA on
compositional split of GeoQuery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Fine-Tuning of BERT Models on the Edge. (arXiv:2205.01541v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01541">
<div class="article-summary-box-inner">
<span><p>Resource-constrained devices are increasingly the deployment targets of
machine learning applications. Static models, however, do not always suffice
for dynamic environments. On-device training of models allows for quick
adaptability to new scenarios. With the increasing size of deep neural
networks, as noted with the likes of BERT and other natural language processing
models, comes increased resource requirements, namely memory, computation,
energy, and time. Furthermore, training is far more resource intensive than
inference. Resource-constrained on-device learning is thus doubly difficult,
especially with large BERT-like models. By reducing the memory usage of
fine-tuning, pre-trained BERT models can become efficient enough to fine-tune
on resource-constrained devices. We propose Freeze And Reconfigure (FAR), a
memory-efficient training regime for BERT-like models that reduces the memory
usage of activation maps during fine-tuning by avoiding unnecessary parameter
updates. FAR reduces fine-tuning time on the DistilBERT model and CoLA dataset
by 30%, and time spent on memory operations by 47%. More broadly, reductions in
metric performance on the GLUE and SQuAD datasets are around 1% on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Transfer Prompts for Text Generation. (arXiv:2205.01543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01543">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLMs) have made remarkable progress in text
generation tasks via fine-tuning. While, it is challenging to fine-tune PLMs in
a data-scarce situation. Therefore, it is non-trivial to develop a general and
lightweight model that can adapt to various text generation tasks based on
PLMs. To fulfill this purpose, the recent prompt-based learning offers a
potential solution. In this paper, we improve this technique and propose a
novel prompt-based method (PTG) for text generation in a transferable setting.
First, PTG learns a set of source prompts for various source generation tasks
and then transfers these prompts as target prompts to perform target generation
tasks. To consider both task- and instance-level information, we design an
adaptive attention mechanism to derive the target prompts. For each data
instance, PTG learns a specific target prompt by attending to highly relevant
source prompts. In extensive experiments, PTG yields competitive or better
results than fine-tuning methods. We release our source prompts as an open
resource, where users can add or reuse them to improve new text generation
tasks for future research. Code and data can be available at
https://github.com/RUCAIBox/Transfer-Prompts-for-Text-Generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptable Adapters. (arXiv:2205.01549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01549">
<div class="article-summary-box-inner">
<span><p>State-of-the-art pretrained NLP models contain a hundred million to trillion
parameters. Adapters provide a parameter-efficient alternative for the full
finetuning in which we can only finetune lightweight neural network layers on
top of pretrained weights. Adapter layers are initialized randomly. However,
existing work uses the same adapter architecture -- i.e., the same adapter
layer on top of each layer of the pretrained model -- for every dataset,
regardless of the properties of the dataset or the amount of available training
data. In this work, we introduce adaptable adapters that contain (1) learning
different activation functions for different layers and different input data,
and (2) a learnable switch to select and only use the beneficial adapter
layers. We show that adaptable adapters achieve on-par performances with the
standard adapter architecture while using a considerably smaller number of
adapter layers. In addition, we show that the selected adapter architecture by
adaptable adapters transfers well across different data settings and similar
tasks. We propose to use adaptable adapters for designing efficient and
effective adapter architectures. The resulting adapters (a) contain about 50%
of the learning parameters of the standard adapter and are therefore more
efficient at training and inference, and require less storage space, and (b)
achieve considerably higher performances in low-data settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Mixed-Domain Translation Models via Federated Learning. (arXiv:2205.01557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01557">
<div class="article-summary-box-inner">
<span><p>Training mixed-domain translation models is a complex task that demands
tailored architectures and costly data preparation techniques. In this work, we
leverage federated learning (FL) in order to tackle the problem. Our
investigation demonstrates that with slight modifications in the training
process, neural machine translation (NMT) engines can be easily adapted when an
FL-based aggregation is applied to fuse different domains. Experimental results
also show that engines built via FL are able to perform on par with
state-of-the-art baselines that rely on centralized training techniques. We
evaluate our hypothesis in the presence of five datasets with different sizes,
from different domains, to translate from German into English and discuss how
FL and NMT can mutually benefit from each other. In addition to providing
benchmarking results on the union of FL and NMT, we also propose a novel
technique to dynamically control the communication bandwidth by selecting
impactful parameters during FL updates. This is a significant achievement
considering the large size of NMT engines that need to be exchanged between FL
parties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SparCAssist: A Model Risk Assessment Assistant Based on Sparse Generated Counterfactuals. (arXiv:2205.01588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01588">
<div class="article-summary-box-inner">
<span><p>We introduce SparcAssist, a general-purpose risk assessment tool for the
machine learning models trained for language tasks. It evaluates models' risk
by inspecting their behavior on counterfactuals, namely out-of-distribution
instances generated based on the given data instance. The counterfactuals are
generated by replacing tokens in rational subsequences identified by ExPred,
while the replacements are retrieved using HotFlip or
Masked-Language-Model-based algorithms. The main purpose of our system is to
help the human annotators to assess the model's risk on deployment. The
counterfactual instances generated during the assessment are the by-product and
can be used to train more robust NLP models in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparison of Approaches for Imbalanced Classification Problems in the Context of Retrieving Relevant Documents for an Analysis. (arXiv:2205.01600v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01600">
<div class="article-summary-box-inner">
<span><p>One of the first steps in many text-based social science studies is to
retrieve documents that are relevant for the analysis from large corpora of
otherwise irrelevant documents. The conventional approach in social science to
address this retrieval task is to apply a set of keywords and to consider those
documents to be relevant that contain at least one of the keywords. But the
application of incomplete keyword lists risks drawing biased inferences. More
complex and costly methods such as query expansion techniques, topic
model-based classification rules, and active as well as passive supervised
learning could have the potential to more accurately separate relevant from
irrelevant documents and thereby reduce the potential size of bias. Yet,
whether applying these more expensive approaches increases retrieval
performance compared to keyword lists at all, and if so, by how much, is
unclear as a comparison of these approaches is lacking. This study closes this
gap by comparing these methods across three retrieval tasks associated with a
data set of German tweets (Linder, 2017), the Social Bias Inference Corpus
(SBIC) (Sap et al., 2020), and the Reuters-21578 corpus (Lewis, 1997). Results
show that query expansion techniques and topic model-based classification rules
in most studied settings tend to decrease rather than increase retrieval
performance. Active supervised learning, however, if applied on a not too small
set of labeled training instances (e.g. 1,000 documents), reaches a
substantially higher retrieval performance than keyword lists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTM -- A Model for Large-Scale Multi-View Tweet Topic Classification. (arXiv:2205.01603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01603">
<div class="article-summary-box-inner">
<span><p>Automatically associating social media posts with topics is an important
prerequisite for effective search and recommendation on many social media
platforms. However, topic classification of such posts is quite challenging
because of (a) a large topic space (b) short text with weak topical cues, and
(c) multiple topic associations per post. In contrast to most prior work which
only focuses on post classification into a small number of topics ($10$-$20$),
we consider the task of large-scale topic classification in the context of
Twitter where the topic space is $10$ times larger with potentially multiple
topic associations per Tweet. We address the challenges above by proposing a
novel neural model, CTM that (a) supports a large topic space of $300$ topics
and (b) takes a holistic approach to tweet content modeling -- leveraging
multi-modal content, author context, and deeper semantic cues in the Tweet. Our
method offers an effective way to classify Tweets into topics at scale by
yielding superior performance to other approaches (a relative lift of
$\mathbf{20}\%$ in median average precision score) and has been successfully
deployed in production at Twitter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniKnight: Multilingual Neural Machine Translation with Language-Specific Self-Distillation. (arXiv:2205.01620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01620">
<div class="article-summary-box-inner">
<span><p>Although all-in-one-model multilingual neural machine translation (MNMT) has
achieved remarkable progress in recent years, its selected best overall
checkpoint fails to achieve the best performance simultaneously in all language
pairs. It is because that the best checkpoints for each individual language
pair (i.e., language-specific best checkpoints) scatter in different epochs. In
this paper, we present a novel training strategy dubbed Language-Specific
Self-Distillation (LSSD) for bridging the gap between language-specific best
checkpoints and the overall best checkpoint. In detail, we regard each
language-specific best checkpoint as a teacher to distill the overall best
checkpoint. Moreover, we systematically explore three variants of our LSSD,
which perform distillation statically, selectively, and adaptively.
Experimental results on two widely-used benchmarks show that LSSD obtains
consistent improvements towards all language pairs and achieves the
state-of-the-art
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Training for High-Stakes Reliability. (arXiv:2205.01663v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01663">
<div class="article-summary-box-inner">
<span><p>In the future, powerful AI systems may be deployed in high-stakes settings,
where a single failure could be catastrophic. One technique for improving AI
safety in high-stakes settings is adversarial training, which uses an adversary
to generate examples to train on in order to achieve better worst-case
performance.
</p>
<p>In this work, we used a language generation task as a testbed for achieving
high reliability through adversarial training. We created a series of
adversarial training techniques -- including a tool that assists human
adversaries -- to find and eliminate failures in a classifier that filters text
completions suggested by a generator. In our simple "avoid injuries" task, we
determined that we can set very conservative classifier thresholds without
significantly impacting the quality of the filtered outputs. With our chosen
thresholds, filtering with our baseline classifier decreases the rate of unsafe
completions from about 2.4% to 0.003% on in-distribution data, which is near
the limit of our ability to measure. We found that adversarial training
significantly increased robustness to the adversarial attacks that we trained
on, without affecting in-distribution performance. We hope to see further work
in the high-stakes reliability setting, including more powerful tools for
enhancing human adversaries and better ways to measure high levels of
reliability, until we can confidently rule out the possibility of catastrophic
deployment-time failures of powerful models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Modeling Hierarchical and Horizontal Features for Relational Triple Extraction. (arXiv:1908.08672v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.08672">
<div class="article-summary-box-inner">
<span><p>Recent works on relational triple extraction have shown the superiority of
jointly extracting entities and relations over the pipelined extraction manner.
However, most existing joint models fail to balance the modeling of entity
features and the joint decoding strategy, and thus the interactions between the
entity level and triple level are not fully investigated. In this work, we
first introduce the hierarchical dependency and horizontal commonality between
the two levels, and then propose an entity-enhanced dual tagging framework that
enables the triple extraction (TE) task to utilize such interactions with
self-learned entity features through an auxiliary entity extraction (EE) task,
without breaking the joint decoding of relational triples. Specifically, we
align the EE and TE tasks in a position-wise manner by formulating them as two
sequence labeling problems with identical encoder-decoder structure. Moreover,
the two tasks are organized in a carefully designed parameter sharing setting
so that the learned entity features could be naturally shared via multi-task
learning. Empirical experiments on the NYT benchmark demonstrate the
effectiveness of the proposed framework compared to the state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v5 [cs.DS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03277">
<div class="article-summary-box-inner">
<span><p>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping
$\pi$ from vertices of a graph to distinct integers that minimizes
$\sum_{\{u,v\}\in E}|\pi(u) - \pi(v)|$. In that setting, vertices are often
assumed to lie on a horizontal line and edges are drawn as semicircles above
said line. For trees, various algorithms are available to solve the problem in
polynomial time in $n=|V|$. There exist variants of the MLA in which the
arrangements are constrained. Iordanskii, and later Hochberg and Stallmann
(HS), put forward $O(n)$-time algorithms that solve the problem when
arrangements are constrained to be planar (also known as one-page book
embeddings). We also consider linear arrangements of rooted trees that are
constrained to be projective (planar embeddings where the root is not covered
by any edge). Gildea and Temperley (GT) sketched an algorithm for projective
arrangements which they claimed runs in $O(n)$ but did not provide any
justification of its cost. In contrast, Park and Levy claimed that GT's
algorithm runs in $O(n \log d_{max})$ where $d_{max}$ is the maximum degree but
did not provide sufficient detail. Here we correct an error in HS's algorithm
for the planar case, show its relationship with the projective case, and derive
simple algorithms for the projective and planar cases that run without a doubt
in $O(n)$ time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extended Parallel Corpus for Amharic-English Machine Translation. (arXiv:2104.03543v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03543">
<div class="article-summary-box-inner">
<span><p>This paper describes the acquisition, preprocessing, segmentation, and
alignment of an Amharic-English parallel corpus. It will be helpful for machine
translation of a low-resource language, Amharic. We freely released the corpus
for research purposes. Furthermore, we developed baseline statistical and
neural machine translation systems; we trained statistical and neural machine
translation models using the corpus. In the experiments, we also used a large
monolingual corpus for the language model of statistical machine translation
and back-translation of neural machine translation. In the automatic
evaluation, neural machine translation models outperform statistical machine
translation models by approximately six to seven Bilingual Evaluation
Understudy (BLEU) points. Besides, among the neural machine translation models,
the subword models outperform the word-based models by three to four BLEU
points. Moreover, two other relevant automatic evaluation metrics, Translation
Edit Rate on Character Level and Better Evaluation as Ranking, reflect
corresponding differences among the trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Restoring Hebrew Diacritics Without a Dictionary. (arXiv:2105.05209v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05209">
<div class="article-summary-box-inner">
<span><p>We demonstrate that it is feasible to diacritize Hebrew script without any
human-curated resources other than plain diacritized text. We present NAKDIMON,
a two-layer character level LSTM, that performs on par with much more
complicated curation-dependent systems, across a diverse array of modern Hebrew
sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Should We Trust This Summary? Bayesian Abstractive Summarization to The Rescue. (arXiv:2105.10155v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10155">
<div class="article-summary-box-inner">
<span><p>We explore the notion of uncertainty in the context of modern abstractive
summarization models, using the tools of Bayesian Deep Learning. Our approach
approximates Bayesian inference by first extending state-of-the-art
summarization models with Monte Carlo dropout and then using them to perform
multiple stochastic forward passes. Based on Bayesian inference we are able to
effectively quantify uncertainty at prediction time. Having a reliable
uncertainty measure, we can improve the experience of the end user by filtering
out generated summaries of high uncertainty. Furthermore, uncertainty
estimation could be used as a criterion for selecting samples for annotation,
and can be paired nicely with active learning and human-in-the-loop approaches.
Finally, Bayesian inference enables us to find a Bayesian summary which
performs better than a deterministic one and is more robust to uncertainty. In
practice, we show that our Variational Bayesian equivalents of BART and PEGASUS
can outperform their deterministic counterparts on multiple benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Speech Recognition. (arXiv:2105.11084v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11084">
<div class="article-summary-box-inner">
<span><p>Despite rapid progress in the recent past, current speech recognition systems
still require labeled training data which limits this technology to a small
fraction of the languages spoken around the globe. This paper describes
wav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition
models without any labeled data. We leverage self-supervised speech
representations to segment unlabeled audio and learn a mapping from these
representations to phonemes via adversarial training. The right representations
are key to the success of our method. Compared to the best previous
unsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT
benchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,
wav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the
best published systems trained on 960 hours of labeled data from only two years
ago. We also experiment on nine other languages, including low-resource
languages such as Kyrgyz, Swahili and Tatar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Identification of Dementia from Transcripts using Transformer Networks. (arXiv:2109.06980v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06980">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) is the main cause of dementia which is accompanied
by loss of memory and may lead to severe consequences in peoples' everyday life
if not diagnosed on time. Very few works have exploited transformer-based
networks and despite the high accuracy achieved, little work has been done in
terms of model interpretability. In addition, although Mini-Mental State Exam
(MMSE) scores are inextricably linked with the identification of dementia,
research works face the task of dementia identification and the task of the
prediction of MMSE scores as two separate tasks. In order to address these
limitations, we employ several transformer-based models, with BERT achieving
the highest accuracy accounting for 87.50%. Concurrently, we propose an
interpretable method to detect AD patients based on siamese networks reaching
accuracy up to 83.75%. Next, we introduce two multi-task learning models, where
the main task refers to the identification of dementia (binary classification),
while the auxiliary one corresponds to the identification of the severity of
dementia (multiclass classification). Our model obtains accuracy equal to
86.25% on the detection of AD patients in the multi-task learning setting.
Finally, we present some new methods to identify the linguistic patterns used
by AD patients and non-AD ones, including text statistics, vocabulary
uniqueness, word usage, correlations via a detailed linguistic analysis, and
explainability techniques (LIME). Findings indicate significant differences in
language between AD and non-AD patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Training with Differentiable Teacher. (arXiv:2109.07049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07049">
<div class="article-summary-box-inner">
<span><p>Self-training achieves enormous success in various semi-supervised and
weakly-supervised learning tasks. The method can be interpreted as a
teacher-student framework, where the teacher generates pseudo-labels, and the
student makes predictions. The two models are updated alternatingly. However,
such a straightforward alternating update rule leads to training instability.
This is because a small change in the teacher may result in a significant
change in the student. To address this issue, we propose DRIFT, short for
differentiable self-training, that treats teacher-student as a Stackelberg
game. In this game, a leader is always in a more advantageous position than a
follower. In self-training, the student contributes to the prediction
performance, and the teacher controls the training process by generating
pseudo-labels. Therefore, we treat the student as the leader and the teacher as
the follower. The leader procures its advantage by acknowledging the follower's
strategy, which involves differentiable pseudo-labels and differentiable sample
weights. Consequently, the leader-follower interaction can be effectively
captured via Stackelberg gradient, obtained by differentiating the follower's
strategy. Experimental results on semi- and weakly-supervised classification
and named entity recognition tasks show that our model outperforms existing
approaches by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaICL: Learning to Learn In Context. (arXiv:2110.15943v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15943">
<div class="article-summary-box-inner">
<span><p>We introduce MetaICL (Meta-training for In-Context Learning), a new
meta-training framework for few-shot learning where a pretrained language model
is tuned to do in-context learning on a large set of training tasks. This
meta-training enables the model to more effectively learn a new task in context
at test time, by simply conditioning on a few training examples with no
parameter updates or task-specific templates. We experiment on a large, diverse
collection of tasks consisting of 142 NLP datasets including classification,
question answering, natural language inference, paraphrase detection and more,
across seven different meta-training/target splits. MetaICL outperforms a range
of baselines including in-context learning without meta-training and multi-task
learning followed by zero-shot transfer. We find that the gains are
particularly significant for target tasks that have domain shifts from the
meta-training tasks, and that using a diverse set of the meta-training tasks is
key to improvements. We also show that MetaICL approaches (and sometimes beats)
the performance of models fully finetuned on the target task, and outperforms
much bigger models with nearly 8x parameters. Finally, we show that MetaICL is
complementary to human-written instructions, and the best performance can be
achieved by combining both approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve-then-extract Based Knowledge Graph Querying Using Graph Neural Networks. (arXiv:2111.10541v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10541">
<div class="article-summary-box-inner">
<span><p>The abstract of Retrieve-then-extract Based Knowledge Graph Querying Using
Graph Neural Networks will be updated here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Conversational Data using Discourse Mutual Information Maximization. (arXiv:2112.05787v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05787">
<div class="article-summary-box-inner">
<span><p>Although many pretrained models exist for text or images, there have been
relatively fewer attempts to train representations specifically for dialog
understanding. Prior works usually relied on finetuned representations based on
generic text representation models like BERT or GPT-2. But such language
modeling pretraining objectives do not take the structural information of
conversational text into consideration. Although generative dialog models can
learn structural features too, we argue that the structure-unaware word-by-word
generation is not suitable for effective conversation modeling. We empirically
demonstrate that such representations do not perform consistently across
various dialog understanding tasks. Hence, we propose a structure-aware Mutual
Information based loss-function DMI (Discourse Mutual Information) for training
dialog-representation models, that additionally captures the inherent
uncertainty in response prediction. Extensive evaluation on nine diverse dialog
modeling tasks shows that our proposed DMI-based models outperform strong
baselines by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06223">
<div class="article-summary-box-inner">
<span><p>Code-switching is a speech phenomenon occurring when a speaker switches
language during a conversation. Despite the spontaneous nature of
code-switching in conversational spoken language, most existing works collect
code-switching data from read speech instead of spontaneous speech. ASCEND (A
Spontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English
code-switching corpus built on spontaneous multi-turn conversational dialogue
sources collected in Hong Kong. We report ASCEND's design and procedure for
collecting the speech data, including annotations. ASCEND consists of 10.62
hours of clean speech, collected from 23 bilingual speakers of Chinese and
English. Furthermore, we conduct baseline experiments using pre-trained wav2vec
2.0 models, achieving a best performance of 22.69\% character error rate and
27.05% mixed error rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Massive-scale Decoding for Text Generation using Lattices. (arXiv:2112.07660v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07660">
<div class="article-summary-box-inner">
<span><p>Conditional neural text generation models generate high-quality outputs, but
often concentrate around a mode when what we really want is a diverse set of
options. We present a search algorithm to construct lattices encoding a massive
number of generation options. First, we restructure decoding as a best-first
search, which explores the space differently than beam search and improves
efficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis
recombination: we can identify pairs of similar generation candidates during
search and merge them as an approximation. On both summarization and machine
translation, we show that our algorithm encodes thousands of diverse options
that remain grammatical and high-quality into one lattice. This algorithm
provides a foundation for building downstream generation applications on top of
massive-scale diverse outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maximum Bayes Smatch Ensemble Distillation for AMR Parsing. (arXiv:2112.07790v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07790">
<div class="article-summary-box-inner">
<span><p>AMR parsing has experienced an unprecendented increase in performance in the
last three years, due to a mixture of effects including architecture
improvements and transfer learning. Self-learning techniques have also played a
role in pushing performance forward. However, for most recent high performant
parsers, the effect of self-learning and silver data augmentation seems to be
fading. In this paper we propose to overcome this diminishing returns of silver
data by combining Smatch-based ensembling techniques with ensemble
distillation. In an extensive experimental setup, we push single model English
parser performance to a new state-of-the-art, 85.9 (AMR2.0) and 84.3 (AMR3.0),
and return to substantial gains from silver data augmentation. We also attain a
new state-of-the-art for cross-lingual AMR parsing for Chinese, German, Italian
and Spanish. Finally we explore the impact of the proposed technique on domain
adaptation, and show that it can produce gains rivaling those of human
annotated data for QALD-9 and achieve a new state-of-the-art for BioAMR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LongT5: Efficient Text-To-Text Transformer for Long Sequences. (arXiv:2112.07916v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07916">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that either (1) increasing the input length or (2)
increasing model size can improve the performance of Transformer-based neural
models. In this paper, we present a new model, called LongT5, with which we
explore the effects of scaling both the input length and model size at the same
time. Specifically, we integrated attention ideas from long-input transformers
(ETC), and adopted pre-training strategies from summarization pre-training
(PEGASUS) into the scalable T5 architecture. The result is a new attention
mechanism we call {\em Transient Global} (TGlobal), which mimics ETC's
local/global attention mechanism, but without requiring additional side-inputs.
We are able to achieve state-of-the-art results on several summarization tasks
and outperform the original T5 models on question answering tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation. (arXiv:2112.08594v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08594">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-context media, such as "mis-captioned" images on Twitter, is
a relevant problem, especially in domains of high public significance. In this
work we aim to develop defenses against such misinformation for the topics of
Climate Change, COVID-19, and Military Vehicles. We first present a large-scale
multimodal dataset with over 884k tweets relevant to these topics. Next, we
propose a detection method, based on the state-of-the-art CLIP model, that
leverages automatically generated hard image-text mismatches. While this
approach works well on our automatically constructed out-of-context tweets, we
aim to validate its usefulness on data representative of the real world. Thus,
we test it on a set of human-generated fakes created by mimicking in-the-wild
misinformation. We achieve an 11% detection improvement in a high precision
regime over a strong baseline. Finally, we share insights about our best model
design and analyze the challenges of this emerging threat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Hierarchical Domain Adaptation for Pretrained Language Models. (arXiv:2112.08786v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08786">
<div class="article-summary-box-inner">
<span><p>The remarkable success of large language models has been driven by dense
models trained on massive unlabeled, unstructured corpora. These corpora
typically contain text from diverse, heterogeneous sources, but information
about the source of the text is rarely used during training. Transferring their
knowledge to a target domain is typically done by continuing training
in-domain. In this paper, we introduce a method to permit domain adaptation to
many diverse domains using a computationally efficient adapter approach. Our
method is based on the observation that textual domains are partially
overlapping, and we represent domains as a hierarchical tree structure where
each node in the tree is associated with a set of adapter weights. When
combined with a frozen pretrained language model, this approach enables
parameter sharing among related domains, while avoiding negative interference
between unrelated ones. Experimental results with GPT-2 and a large fraction of
the 100 most represented websites in C4 show across-the-board improvements
in-domain. We additionally provide an inference time algorithm for a held-out
domain and show that averaging over multiple paths through the tree enables
further gains in generalization, while adding only a marginal cost to
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AcTune: Uncertainty-aware Active Self-Training for Semi-Supervised Active Learning with Pretrained Language Models. (arXiv:2112.08787v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08787">
<div class="article-summary-box-inner">
<span><p>While pre-trained language model (PLM) fine-tuning has achieved strong
performance in many NLP tasks, the fine-tuning stage can be still demanding in
labeled data. Recent works have resorted to active fine-tuning to improve the
label efficiency of PLM fine-tuning, but none of them investigate the potential
of unlabeled data. We propose {\ours}, a new framework that leverages unlabeled
data to improve the label efficiency of active PLM fine-tuning. AcTune switches
between data annotation and model self-training based on uncertainty: it
selects high-uncertainty unlabeled samples for active annotation and
low-uncertainty ones for model self-training. Under this framework, we design
(1) a region-aware sampling strategy that reduces redundancy when actively
querying for annotations and (2) a momentum-based memory bank that dynamically
aggregates the model's pseudo labels to suppress label noise in self-training.
Experiments on 6 text classification datasets show that AcTune outperforms the
strongest active learning and self-training baselines and improves the label
efficiency of PLM fine-tuning by 56.2\% on average. Our implementation will be
available at \url{https://github.com/yueyu1030/actune}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer. (arXiv:2112.08995v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08995">
<div class="article-summary-box-inner">
<span><p>Machines that can represent and describe environmental soundscapes have
practical potential, e.g., for audio tagging and captioning systems. Prevailing
learning paradigms have been relying on parallel audio-text data, which is,
however, scarcely available on the web. We propose VIP-ANT that induces
\textbf{A}udio-\textbf{T}ext alignment without using any parallel audio-text
data. Our key idea is to share the image modality between bi-modal image-text
representations and bi-modal image-audio representations; the image modality
functions as a pivot and connects audio and text in a tri-modal embedding space
implicitly.
</p>
<p>In a difficult zero-shot setting with no paired audio-text data, our model
demonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio
classification tasks, and even surpasses the supervised state of the art for
Clotho caption retrieval (with audio queries) by 2.2\% R@1. We further
investigate cases of minimal audio-text supervision, finding that, e.g., just a
few hundred supervised audio-text pairs increase the zero-shot audio
classification accuracy by 8\% on US8K. However, to match human parity on some
zero-shot tasks, our empirical scaling experiments suggest that we would need
about $2^{21} \approx 2M$ supervised audio-caption pairs. Our work opens up new
avenues for learning audio-text connections with little to no parallel
audio-text data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification. (arXiv:2201.00989v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00989">
<div class="article-summary-box-inner">
<span><p>In aspect-level sentiment classification (ASC), state-of-the-art models
encode either syntax graph or relation graph to capture the local syntactic
information or global relational information. Despite the advantages of syntax
and relation graphs, they have respective shortages which are neglected,
limiting the representation power in the graph modeling process. To resolve
their limitations, we design a novel local-global interactive graph, which
marries their advantages by stitching the two graphs via interactive edges. To
model this local-global interactive graph, we propose a novel neural network
termed DigNet, whose core module is the stacked local-global interactive (LGI)
layers performing two processes: intra-graph message passing and cross-graph
message passing. In this way, the local syntactic and global relational
information can be reconciled as a whole in understanding the aspect-level
sentiment. Concretely, we design two variants of local-global interactive
graphs with different kinds of interactive edges and three variants of LGI
layers. We conduct experiments on several public benchmark datasets and the
results show that we outperform previous best scores by 3\%, 2.32\%, and 6.33\%
in terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,
confirming the effectiveness and superiority of the proposed local-global
interactive graph and DigNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LARD: Large-scale Artificial Disfluency Generation. (arXiv:2201.05041v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05041">
<div class="article-summary-box-inner">
<span><p>Disfluency detection is a critical task in real-time dialogue systems.
However, despite its importance, it remains a relatively unexplored field,
mainly due to the lack of appropriate datasets. At the same time, existing
datasets suffer from various issues, including class imbalance issues, which
can significantly affect the performance of the model on rare classes, as it is
demonstrated in this paper. To this end, we propose LARD, a method for
generating complex and realistic artificial disfluencies with little effort.
The proposed method can handle three of the most common types of disfluencies:
repetitions, replacements and restarts. In addition, we release a new
large-scale dataset with disfluencies that can be used on four different tasks:
disfluency detection, classification, extraction and correction. Experimental
results on the LARD dataset demonstrate that the data produced by the proposed
method can be effectively used for detecting and removing disfluencies, while
also addressing limitations of existing datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text characterization based on recurrence networks. (arXiv:2201.06665v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06665">
<div class="article-summary-box-inner">
<span><p>Several complex systems are characterized by presenting intricate
characteristics taking place at several scales of time and space. These
multiscale characterizations are used in various applications, including better
understanding diseases, characterizing transportation systems, and comparison
between cities, among others. In particular, texts are also characterized by a
hierarchical structure that can be approached by using multi-scale concepts and
methods. The multiscale properties of texts constitute a subject worth further
investigation. In addition, more effective approaches to text characterization
and analysis can be obtained by emphasizing words with potentially more
informational content. The present work aims at developing these possibilities
while focusing on mesoscopic representations of networks. More specifically, we
adopt an extension to the mesoscopic approach to represent text narratives, in
which only the recurrent relationships among tagged parts of speech (subject,
verb and direct object) are considered to establish connections among
sequential pieces of text (e.g., paragraphs). The characterization of the texts
was then achieved by considering scale-dependent complementary methods:
accessibility, symmetry and recurrence signatures. In order to evaluate the
potential of these concepts and methods, we approached the problem of
distinguishing between literary genres (fiction and non-fiction). A set of 300
books organized into the two genres was considered and were compared by using
the aforementioned approaches. All the methods were capable of differentiating
to some extent between the two genres. The accessibility and symmetry reflected
the narrative asymmetries, while the recurrence signature provided a more
direct indication about the non-sequential semantic connections taking place
along the narrative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TableFormer: Robust Transformer Modeling for Table-Text Encoding. (arXiv:2203.00274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00274">
<div class="article-summary-box-inner">
<span><p>Understanding tables is an important aspect of natural language
understanding. Existing models for table understanding require linearization of
the table structure, where row or column order is encoded as an unwanted bias.
Such spurious biases make the model vulnerable to row and column order
perturbations. Additionally, prior work has not thoroughly modeled the table
structures or table-text alignments, hindering the table-text understanding
ability. In this work, we propose a robust and structurally aware table-text
encoding architecture TableFormer, where tabular structural biases are
incorporated completely through learnable attention biases. TableFormer is (1)
strictly invariant to row and column orders, and, (2) could understand tables
better due to its tabular inductive biases. Our evaluations showed that
TableFormer outperforms strong baselines in all settings on SQA, WTQ and
TabFact table reasoning datasets, and achieves state-of-the-art performance on
SQA, especially when facing answer-invariant row and column order perturbations
(6% improvement over the best baseline), because previous SOTA models'
performance drops by 4% - 6% when facing such perturbations while TableFormer
is not affected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Free Attentive Scoring for Speaker Verification. (arXiv:2203.05642v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05642">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel study of parameter-free attentive scoring for
speaker verification. Parameter-free scoring provides the flexibility of
comparing speaker representations without the need of an accompanying
parametric scoring model. Inspired by the attention component in Transformer
neural networks, we propose a variant of the scaled dot product attention
mechanism to compare enrollment and test segment representations. In addition,
this work explores the effect on performance of (i) different types of
normalization, (ii) independent versus tied query/key estimation, (iii) varying
the number of key-value pairs and (iv) pooling multiple enrollment utterance
statistics. Experimental results for a 4 task average show that a simple
parameter-free attentive scoring mechanism can improve the average EER by 10%
over the best cosine similarity baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09509">
<div class="article-summary-box-inner">
<span><p>Toxic language detection systems often falsely flag text that contains
minority group mentions as toxic, as those groups are often the targets of
online hate. Such over-reliance on spurious correlations also causes systems to
struggle with detecting implicitly toxic language. To help mitigate these
issues, we create ToxiGen, a new large-scale and machine-generated dataset of
274k toxic and benign statements about 13 minority groups. We develop a
demonstration-based prompting framework and an adversarial
classifier-in-the-loop decoding method to generate subtly toxic and benign text
with a massive pretrained language model. Controlling machine generation in
this way allows ToxiGen to cover implicitly toxic text at a larger scale, and
about more demographic groups, than previous resources of human-written text.
We conduct a human evaluation on a challenging subset of ToxiGen and find that
annotators struggle to distinguish machine-generated text from human-written
language. We also find that 94.5% of toxic examples are labeled as hate speech
by human annotators. Using three publicly-available datasets, we show that
finetuning a toxicity classifier on our data improves its performance on
human-written data substantially. We also demonstrate that ToxiGen can be used
to fight machine-generated toxicity as finetuning improves the classifier
significantly on our evaluation subset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations. (arXiv:2203.13602v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13602">
<div class="article-summary-box-inner">
<span><p>The current workflow for Information Extraction (IE) analysts involves the
definition of the entities/relations of interest and a training corpus with
annotated examples. In this demonstration we introduce a new workflow where the
analyst directly verbalizes the entities/relations, which are then used by a
Textual Entailment model to perform zero-shot IE. We present the design and
implementation of a toolkit with a user interface, as well as experiments on
four IE tasks that show that the system achieves very good performance at
zero-shot learning using only 5--15 minutes per type of a user's effort. Our
demonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A
demonstration video is available at https://vimeo.<a href="/abs/com/6761383">com/6761383</a>40 .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pruning Learns Compact and Accurate Models. (arXiv:2204.00408v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00408">
<div class="article-summary-box-inner">
<span><p>The growing size of neural language models has led to increased attention in
model compression. The two predominant approaches are pruning, which gradually
removes weights from a pre-trained model, and distillation, which trains a
smaller compact model to match a larger one. Pruning methods can significantly
reduce the model size but hardly achieve large speedups as distillation.
However, distillation methods require large amounts of unlabeled data and are
expensive to train. In this work, we propose a task-specific structured pruning
method CoFi (Coarse- and Fine-grained Pruning), which delivers highly
parallelizable subnetworks and matches the distillation methods in both
accuracy and latency, without resorting to any unlabeled data. Our key insight
is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads
and hidden units) modules, which controls the pruning decision of each
parameter with masks of different granularity. We also devise a layerwise
distillation strategy to transfer knowledge from unpruned to pruned models
during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi
yields models with over 10x speedups with a small accuracy drop, showing its
effectiveness and efficiency compared to previous pruning and distillation
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective. (arXiv:2204.04391v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04391">
<div class="article-summary-box-inner">
<span><p>NER model has achieved promising performance on standard NER benchmarks.
However, recent studies show that previous approaches may over-rely on entity
mention information, resulting in poor performance on out-of-vocabulary (OOV)
entity recognition. In this work, we propose MINER, a novel NER learning
framework, to remedy this issue from an information-theoretic perspective. The
proposed approach contains two mutual information-based training objectives: i)
generalizing information maximization, which enhances representation via deep
understanding of context and entity surface forms; ii) superfluous information
minimization, which discourages representation from rote memorizing entity
names or exploiting biased cues in data. Experiments on various settings and
datasets demonstrate that it achieves better performance in predicting OOV
entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuS: Neutral Multi-News Summarization for Mitigating Framing Bias. (arXiv:2204.04902v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04902">
<div class="article-summary-box-inner">
<span><p>Media news framing bias can increase political polarization and undermine
civil society. The need for automatic mitigation methods is therefore growing.
We propose a new task, a neutral summary generation from multiple news articles
of the varying political leanings to facilitate balanced and unbiased news
reading. In this paper, we first collect a new dataset, illustrate insights
about framing bias through a case study, and propose a new effective metric and
model (NeuS-TITLE) for the task. Based on our discovery that title provides a
good signal for framing bias, we present NeuS-TITLE that learns to neutralize
news content in hierarchical order from title to article. Our hierarchical
multi-task learning is achieved by formatting our hierarchical data pair
(title, article) sequentially with identifier-tokens ("TITLE=&gt;", "ARTICLE=&gt;")
and fine-tuning the auto-regressive decoder with the standard negative
log-likelihood objective. We then analyze and point out the remaining
challenges and future directions. One of the most interesting observations is
that neural NLG models can hallucinate not only factually inaccurate or
unverifiable content but also politically biased content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRUE: Re-evaluating Factual Consistency Evaluation. (arXiv:2204.04991v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04991">
<div class="article-summary-box-inner">
<span><p>Grounded text generation systems often generate text that contains factual
inconsistencies, hindering their real-world applicability. Automatic factual
consistency evaluation may help alleviate this limitation by accelerating
evaluation cycles, filtering inconsistent outputs and augmenting training data.
While attracting increasing attention, such evaluation metrics are usually
developed and evaluated in silo for a single task or dataset, slowing their
adoption. Moreover, previous meta-evaluation protocols focused on system-level
correlations with human annotations, which leave the example-level accuracy of
such metrics unclear. In this work, we introduce TRUE: a comprehensive survey
and assessment of factual consistency metrics on a standardized collection of
existing texts from diverse tasks, manually annotated for factual consistency.
Our standardization enables an example-level meta-evaluation protocol that is
more actionable and interpretable than previously reported correlations,
yielding clearer quality measures. Across diverse state-of-the-art metrics and
11 datasets we find that large-scale NLI and question
generation-and-answering-based approaches achieve strong and complementary
results. We recommend those methods as a starting point for model and metric
developers, and hope TRUE will foster progress towards even better evaluation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05991">
<div class="article-summary-box-inner">
<span><p>Training a referring expression comprehension (ReC) model for a new visual
domain requires collecting referring expressions, and potentially corresponding
bounding boxes, for images in the domain. While large-scale pre-trained models
are useful for image classification across domains, it remains unclear if they
can be applied in a zero-shot manner to more complex tasks like ReC. We present
ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a
state-of-the-art large-scale model, for ReC. Motivated by the close connection
between ReC and CLIP's contrastive pre-training objective, the first component
of ReCLIP is a region-scoring method that isolates object proposals via
cropping and blurring, and passes them to CLIP. However, through controlled
experiments on a synthetic dataset, we find that CLIP is largely incapable of
performing spatial reasoning off-the-shelf. Thus, the second component of
ReCLIP is a spatial relation resolver that handles several types of spatial
relations. We reduce the gap between zero-shot baselines from prior work and
supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game
imagery), ReCLIP's relative improvement over supervised ReC models trained on
real images is 8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Cluster-Based k-Nearest-Neighbor Machine Translation. (arXiv:2204.06175v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06175">
<div class="article-summary-box-inner">
<span><p>k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as
a non-parametric solution for domain adaptation in neural machine translation
(NMT). It aims to alleviate the performance degradation of advanced MT systems
in translating out-of-domain sentences by coordinating with an additional
token-level feature-based retrieval module constructed from in-domain data.
Previous studies have already demonstrated that non-parametric NMT is even
superior to models fine-tuned on out-of-domain data. In spite of this success,
kNN retrieval is at the expense of high latency, in particular for large
datastores. To make it practical, in this paper, we explore a more efficient
kNN-MT and propose to use clustering to improve the retrieval efficiency.
Concretely, we first propose a cluster-based Compact Network for feature
reduction in a contrastive learning manner to compress context features into
90+% lower dimensional vectors. We then suggest a cluster-based pruning
solution to filter out 10%-40% redundant nodes in large datastores while
retaining translation quality. Our proposed methods achieve better or
comparable performance while reducing up to 57% inference latency against the
advanced non-parametric MT model on several machine translation benchmarks.
Experimental results indicate that the proposed methods maintain the most
useful information of the original datastore and the Compact Network shows good
generalization on unseen domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding. (arXiv:2204.07316v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07316">
<div class="article-summary-box-inner">
<span><p>Transformer-based models are widely used in natural language understanding
(NLU) tasks, and multimodal transformers have been effective in visual-language
tasks. This study explores distilling visual information from pretrained
multimodal transformers to pretrained language encoders. Our framework is
inspired by cross-modal encoders' success in visual-language tasks while we
alter the learning objective to cater to the language-heavy characteristics of
NLU. After training with a small number of extra adapting steps and finetuned,
the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in
general language understanding evaluation (GLUE), situations with adversarial
generations (SWAG) benchmarks, and readability benchmarks. We analyze the
performance of XDBERT on GLUE to show that the improvement is likely visually
grounded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imagination-Augmented Natural Language Understanding. (arXiv:2204.08535v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08535">
<div class="article-summary-box-inner">
<span><p>Human brains integrate linguistic and perceptual information simultaneously
to understand natural language, and hold the critical ability to render
imaginations. Such abilities enable us to construct new abstract concepts or
concrete objects, and are essential in involving practical knowledge to solve
problems in low-resource scenarios. However, most existing methods for Natural
Language Understanding (NLU) are mainly focused on textual signals. They do not
simulate human visual imagination ability, which hinders models from inferring
and learning efficiently from limited data samples. Therefore, we introduce an
Imagination-Augmented Cross-modal Encoder (iACE) to solve natural language
understanding tasks from a novel learning perspective -- imagination-augmented
cross-modal understanding. iACE enables visual imagination with external
knowledge transferred from the powerful generative and pre-trained
vision-and-language models. Extensive experiments on GLUE and SWAG show that
iACE achieves consistent improvement over visually-supervised pre-trained
models. More importantly, results in extreme and normal few-shot settings
validate the effectiveness of iACE in low-resource natural language
understanding circumstances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns. (arXiv:2204.10281v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10281">
<div class="article-summary-box-inner">
<span><p>Gender-neutral pronouns have recently been introduced in many languages to a)
include non-binary people and b) as a generic singular. Recent results from
psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not
associated with human processing difficulties. This, we show, is in sharp
contrast with automated processing. We show that gender-neutral pronouns in
Danish, English, and Swedish are associated with higher perplexity, more
dispersed attention patterns, and worse downstream performance. We argue that
such conservativity in language models may limit widespread adoption of
gender-neutral pronouns and must therefore be resolved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Opponent Modeling in Negotiation Dialogues by Related Data Adaptation. (arXiv:2205.00344v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00344">
<div class="article-summary-box-inner">
<span><p>Opponent modeling is the task of inferring another party's mental state
within the context of social interactions. In a multi-issue negotiation, it
involves inferring the relative importance that the opponent assigns to each
issue under discussion, which is crucial for finding high-value deals. A
practical model for this task needs to infer these priorities of the opponent
on the fly based on partial dialogues as input, without needing additional
annotations for training. In this work, we propose a ranker for identifying
these priorities from negotiation dialogues. The model takes in a partial
dialogue as input and predicts the priority order of the opponent. We further
devise ways to adapt related data sources for this task to provide more
explicit supervision for incorporating the opponent's preferences and offers,
as a proxy to relying on granular utterance-level annotations. We show the
utility of our proposed approach through extensive experiments based on two
dialogue datasets. We find that the proposed data adaptations lead to strong
performance in zero-shot and few-shot scenarios. Moreover, they allow the model
to perform better than baselines while accessing fewer utterances from the
opponent. We release our code to support future work in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Cross-lingual Conversation Summarization Challenge. (arXiv:2205.00379v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00379">
<div class="article-summary-box-inner">
<span><p>We propose the shared task of cross-lingual conversation summarization,
\emph{ConvSumX Challenge}, opening new avenues for researchers to investigate
solutions that integrate conversation summarization and machine translation.
This task can be particularly useful due to the emergence of online meetings
and conferences. We construct a new benchmark, covering 2 real-world scenarios
and 3 language directions, including a low-resource language. We hope that
\emph{ConvSumX} can motivate researches to go beyond English and break the
barrier for non-English speakers to benefit from recent advances of
conversation summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">None Class Ranking Loss for Document-Level Relation Extraction. (arXiv:2205.00476v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00476">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (RE) aims at extracting relations among
entities expressed across multiple sentences, which can be viewed as a
multi-label classification problem. In a typical document, most entity pairs do
not express any pre-defined relation and are labeled as "none" or "no
relation". For good document-level RE performance, it is crucial to distinguish
such none class instances (entity pairs) from those of pre-defined classes
(relations). However, most existing methods only estimate the probability of
pre-defined relations independently without considering the probability of "no
relation". This ignores the context of entity pairs and the label correlations
between the none class and pre-defined classes, leading to sub-optimal
predictions. To address this problem, we propose a new multi-label loss that
encourages large margins of label confidence scores between each pre-defined
class and the none class, which enables captured label correlations and
context-dependent thresholding for label prediction. To gain further robustness
against positive-negative imbalance and mislabeled data that could appear in
real-world RE datasets, we propose a margin regularization and a margin
shifting technique. Experimental results demonstrate that our method
significantly outperforms existing multi-label losses for document-level RE and
works well in other multi-label tasks such as emotion classification when none
class instances are available for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Bias in Masked Language Models for Multiple Languages. (arXiv:2205.00551v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00551">
<div class="article-summary-box-inner">
<span><p>Masked Language Models (MLMs) pre-trained by predicting masked tokens on
large corpora have been used successfully in natural language processing tasks
for a variety of languages. Unfortunately, it was reported that MLMs also learn
discriminative biases regarding attributes such as gender and race. Because
most studies have focused on MLMs in English, the bias of MLMs in other
languages has rarely been investigated. Manual annotation of evaluation data
for languages other than English has been challenging due to the cost and
difficulty in recruiting annotators. Moreover, the existing bias evaluation
methods require the stereotypical sentence pairs consisting of the same context
with attribute words (e.g. He/She is a nurse). We propose Multilingual Bias
Evaluation (MBE) score, to evaluate bias in various languages using only
English attribute word lists and parallel corpora between the target language
and English without requiring manually annotated data. We evaluated MLMs in
eight languages using the MBE and confirmed that gender-related biases are
encoded in MLMs for all those languages. We manually created datasets for
gender bias in Japanese and Russian to evaluate the validity of the MBE. The
results show that the bias scores reported by the MBE significantly correlates
with that computed from the above manually created datasets and the existing
English datasets for gender bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Factors Should Paper-Reviewer Assignments Rely On? Community Perspectives on Issues and Ideals in Conference Peer-Review. (arXiv:2205.01005v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01005">
<div class="article-summary-box-inner">
<span><p>Both scientific progress and individual researcher careers depend on the
quality of peer review, which in turn depends on paper-reviewer matching.
Surprisingly, this problem has been mostly approached as an automated
recommendation problem rather than as a matter where different stakeholders
(area chairs, reviewers, authors) have accumulated experience worth taking into
account. We present the results of the first survey of the NLP community,
identifying common issues and perspectives on what factors should be considered
by paper-reviewer matching systems. This study contributes actionable
recommendations for improving future NLP conferences, and desiderata for
interpretable peer review assignments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPT: Open Pre-trained Transformer Language Models. (arXiv:2205.01068v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01068">
<div class="article-summary-box-inner">
<span><p>Large language models, which are often trained for hundreds of thousands of
compute days, have shown remarkable capabilities for zero- and few-shot
learning. Given their computational cost, these models are difficult to
replicate without significant capital. For the few that are available through
APIs, no access is granted to the full model weights, making them difficult to
study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only
pre-trained transformers ranging from 125M to 175B parameters, which we aim to
fully and responsibly share with interested researchers. We show that OPT-175B
is comparable to GPT-3, while requiring only 1/7th the carbon footprint to
develop. We are also releasing our logbook detailing the infrastructure
challenges we faced, along with code for experimenting with all of the released
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoBERTuito: a pre-trained language model for social media text in Spanish. (arXiv:2111.09453v2 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09453">
<div class="article-summary-box-inner">
<span><p>Since BERT appeared, Transformer language models and transfer learning have
become state-of-the-art for Natural Language Understanding tasks. Recently,
some works geared towards pre-training specially-crafted models for particular
domains, such as scientific papers, medical documents, user-generated texts,
among others. These domain-specific models have been shown to improve
performance significantly in most tasks. However, for languages other than
English such models are not widely available.
</p>
<p>In this work, we present RoBERTuito, a pre-trained language model for
user-generated text in Spanish, trained on over 500 million tweets. Experiments
on a benchmark of tasks involving user-generated text showed that RoBERTuito
outperformed other pre-trained language models in Spanish. In addition to this,
our model achieves top results for some English-Spanish tasks of the Linguistic
Code-Switching Evaluation benchmark (LinCE) and has also competitive
performance against monolingual models in English tasks. To facilitate further
research, we make RoBERTuito publicly available at the HuggingFace model hub
together with the dataset used to pre-train it.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation. (arXiv:2205.01133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01133">
<div class="article-summary-box-inner">
<span><p>Multi-modal Machine Translation (MMT) enables the use of visual information
to enhance the quality of translations. The visual information can serve as a
valuable piece of context information to decrease the ambiguity of input
sentences. Despite the increasing popularity of such a technique, good and
sizeable datasets are scarce, limiting the full extent of their potential.
Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It
is estimated that about 100 to 150 million people speak the language, with more
than 80 million indigenous speakers. This is more than any of the other Chadic
languages. Despite a large number of speakers, the Hausa language is considered
low-resource in natural language processing (NLP). This is due to the absence
of sufficient resources to implement most NLP tasks. While some datasets exist,
they are either scarce, machine-generated, or in the religious domain.
Therefore, there is a need to create training and evaluation data for
implementing machine learning tasks and bridging the research gap in the
language. This work presents the Hausa Visual Genome (HaVG), a dataset that
contains the description of an image or a section within the image in Hausa and
its equivalent in English. To prepare the dataset, we started by translating
the English description of the images in the Hindi Visual Genome (HVG) into
Hausa automatically. Afterward, the synthetic Hausa data was carefully
post-edited considering the respective images. The dataset comprises 32,923
images and their descriptions that are divided into training, development,
test, and challenge test set. The Hausa Visual Genome is the first dataset of
its kind and can be used for Hausa-English machine translation, multi-modal
research, and image description, among various other natural language
processing and generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-DPCC: Deep Dynamic Point Cloud Compression via 3D Motion Prediction. (arXiv:2205.01135v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01135">
<div class="article-summary-box-inner">
<span><p>The non-uniformly distributed nature of the 3D dynamic point cloud (DPC)
brings significant challenges to its high-efficient inter-frame compression.
This paper proposes a novel 3D sparse convolution-based Deep Dynamic Point
Cloud Compression (D-DPCC) network to compensate and compress the DPC geometry
with 3D motion estimation and motion compensation in the feature space. In the
proposed D-DPCC network, we design a {\it Multi-scale Motion Fusion} (MMF)
module to accurately estimate the 3D optical flow between the feature
representations of adjacent point cloud frames. Specifically, we utilize a 3D
sparse convolution-based encoder to obtain the latent representation for motion
estimation in the feature space and introduce the proposed MMF module for fused
3D motion embedding. Besides, for motion compensation, we propose a 3D {\it
Adaptively Weighted Interpolation} (3DAWI) algorithm with a penalty coefficient
to adaptively decrease the impact of distant neighbors. We compress the motion
embedding and the residual with a lossy autoencoder-based network. To our
knowledge, this paper is the first work proposing an end-to-end deep dynamic
point cloud compression framework. The experimental result shows that the
proposed D-DPCC framework achieves an average 76\% BD-Rate (Bjontegaard Delta
Rate) gains against state-of-the-art Video-based Point Cloud Compression
(V-PCC) v13 in inter mode.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cost-Aware Comparison of LiDAR-based 3D Object Detectors. (arXiv:2205.01142v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01142">
<div class="article-summary-box-inner">
<span><p>Considerable research efforts have been devoted to LiDAR-based 3D object
detection and its empirical performance has been significantly improved. While
the progress has been encouraging, we observe an overlooked issue: it is not
yet common practice to compare different 3D detectors under the same cost,
e.g., inference latency. This makes it difficult to quantify the true
performance gain brought by recently proposed architecture designs. The goal of
this work is to conduct a fair comparison of LiDAR-based 3D object detectors.
Specifically, we focus on SECOND, a simple grid-based one-stage detector, and
analyze its performance under different costs by scaling its original
architecture. Then we compare the family of scaled SECOND with recent 3D
detection methods, such as Voxel R-CNN and PV-RCNN++. The results are
surprising. We find that, if allowed to use the same latency, SECOND can match
the performance of PV-RCNN++, the current state-of-the-art method on the Waymo
Open Dataset. Scaled SECOND also easily outperforms many recent 3D detection
methods published during the past year. We recommend future research control
the inference cost in their empirical comparison and include the family of
scaled SECOND as a strong baseline when presenting novel 3D detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion-Controllable Generalized Talking Face Generation. (arXiv:2205.01155v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01155">
<div class="article-summary-box-inner">
<span><p>Despite the significant progress in recent years, very few of the AI-based
talking face generation methods attempt to render natural emotions. Moreover,
the scope of the methods is majorly limited to the characteristics of the
training dataset, hence they fail to generalize to arbitrary unseen faces. In
this paper, we propose a one-shot facial geometry-aware emotional talking face
generation method that can generalize to arbitrary faces. We propose a graph
convolutional neural network that uses speech content feature, along with an
independent emotion input to generate emotion and speech-induced motion on
facial geometry-aware landmark representation. This representation is further
used in our optical flow-guided texture generation network for producing the
texture. We propose a two-branch texture generation network, with motion and
texture branches designed to consider the motion and texture content
independently. Compared to the previous emotion talking face methods, our
method can adapt to arbitrary faces captured in-the-wild by fine-tuning with
only a single image of the target identity in neutral emotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels. (arXiv:2205.01156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01156">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are prone to overfitting noisy labels, resulting in poor
generalization performance. To overcome this problem, we present a simple and
effective method self-ensemble label correction (SELC) to progressively correct
noisy labels and refine the model. We look deeper into the memorization
behavior in training with noisy labels and observe that the network outputs are
reliable in the early stage. To retain this reliable knowledge, SELC uses
ensemble predictions formed by an exponential moving average of network outputs
to update the original noisy labels. We show that training with SELC refines
the model by gradually reducing supervision from noisy labels and increasing
supervision from ensemble predictions. Despite its simplicity, compared with
many state-of-the-art methods, SELC obtains more promising and stable results
in the presence of class-conditional, instance-dependent, and real-world label
noise. The code is available at https://github.com/MacLLL/SELC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency map using features derived from spiking neural networks of primate visual cortex. (arXiv:2205.01159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01159">
<div class="article-summary-box-inner">
<span><p>We propose a framework inspired by biological vision systems to produce
saliency maps of digital images. Well-known computational models for receptive
fields of areas in the visual cortex that are specialized for color and
orientation perception are used. To model the connectivity between these areas
we use the CARLsim library which is a spiking neural network(SNN) simulator.
The spikes generated by CARLsim, then serve as extracted features and input to
our saliency detection algorithm. This new method of saliency detection is
described and applied to benchmark images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Convolutional Neural Networks for Dendrite Segmentation Using Fine-Tuning and Hyperparameter Optimization. (arXiv:2205.01167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01167">
<div class="article-summary-box-inner">
<span><p>Dendritic microstructures are ubiquitous in nature and are the primary
solidification morphologies in metallic materials. Techniques such as x-ray
computed tomography (XCT) have provided new insights into dendritic phase
transformation phenomena. However, manual identification of dendritic
morphologies in microscopy data can be both labor intensive and potentially
ambiguous. The analysis of 3D datasets is particularly challenging due to their
large sizes (terabytes) and the presence of artifacts scattered within the
imaged volumes. In this study, we trained 3D convolutional neural networks
(CNNs) to segment 3D datasets. Three CNN architectures were investigated,
including a new 3D version of FCDense. We show that using hyperparameter
optimization (HPO) and fine-tuning techniques, both 2D and 3D CNN architectures
can be trained to outperform the previous state of the art. The 3D U-Net
architecture trained in this study produced the best segmentations according to
quantitative metrics (pixel-wise accuracy of 99.84% and a boundary displacement
error of 0.58 pixels), while 3D FCDense produced the smoothest boundaries and
best segmentations according to visual inspection. The trained 3D CNNs are able
to segment entire 852 x 852 x 250 voxel 3D volumes in only ~60 seconds, thus
hastening the progress towards a deeper understanding of phase transformation
phenomena such as dendritic solidification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Video Object Segmentation based on Scale Inconsistency. (arXiv:2205.01197v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01197">
<div class="article-summary-box-inner">
<span><p>We present a refinement framework to boost the performance of pre-trained
semi-supervised video object segmentation (VOS) models. Our work is based on
scale inconsistency, which is motivated by the observation that existing VOS
models generate inconsistent predictions from input frames with different
sizes. We use the scale inconsistency as a clue to devise a pixel-level
attention module that aggregates the advantages of the predictions from
different-size inputs. The scale inconsistency is also used to regularize the
training based on a pixel-level variance measured by an uncertainty estimation.
We further present a self-supervised online adaptation, tailored for test-time
optimization, that bootstraps the predictions without ground-truth masks based
on the scale inconsistency. Experiments on DAVIS 16 and DAVIS 17 datasets show
that our framework can be generically applied to various VOS models and improve
their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NHA12D: A New Pavement Crack Dataset and a Comparison Study Of Crack Detection Algorithms. (arXiv:2205.01198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01198">
<div class="article-summary-box-inner">
<span><p>Crack detection plays a key role in automated pavement inspection. Although a
large number of algorithms have been developed in recent years to further boost
performance, there are still remaining challenges in practice, due to the
complexity of pavement images. To further accelerate the development and
identify the remaining challenges, this paper conducts a comparison study to
evaluate the performance of the state of the art crack detection algorithms
quantitatively and objectively. A more comprehensive annotated pavement crack
dataset (NHA12D) that contains images with different viewpoints and pavements
types is proposed. In the comparison study, crack detection algorithms were
trained equally on the largest public crack dataset collected and evaluated on
the proposed dataset (NHA12D). Overall, the U-Net model with VGG-16 as backbone
has the best all-around performance, but models generally fail to distinguish
cracks from concrete joints, leading to a high false-positive rate. It also
found that detecting cracks from concrete pavement images still has huge room
for improvement. Dataset for concrete pavement images is also missing in the
literature. Future directions in this area include filling the gap for concrete
pavement images and using domain adaptation techniques to enhance the detection
results on unseen datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Defense Method against Adversarial Attacks on Traffic Sign Classifiers in Autonomous Vehicles. (arXiv:2205.01225v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01225">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks can make deep neural network (DNN) models predict
incorrect output labels, such as misclassified traffic signs, for autonomous
vehicle (AV) perception modules. Resilience against adversarial attacks can
help AVs navigate safely on the road by avoiding misclassication of signs or
objects. This DNN-based study develops a resilient traffic sign classifier for
AVs that uses a hybrid defense method. We use transfer learning to retrain the
Inception-V3 and Resnet-152 models as traffic sign classifiers. This method
also utilizes a combination of three different strategies: random filtering,
ensembling, and local feature mapping. We use the random cropping and resizing
technique for random filtering, plurality voting as ensembling strategy and an
optical character recognition model as a local feature mapper. This DNN-based
hybrid defense method has been tested for the no attack scenario and against
well-known untargeted adversarial attacks (e.g., Projected Gradient Descent or
PGD, Fast Gradient Sign Method or FGSM, Momentum Iterative Method or MIM
attack, and Carlini and Wagner or C&amp;W). We find that our hybrid defense method
achieves 99% average traffic sign classification accuracy for the no attack
scenario and 88% average traffic sign classification accuracy for all attack
scenarios. Moreover, the hybrid defense method, presented in this study,
improves the accuracy for traffic sign classification compared to the
traditional defense methods (i.e., JPEG filtering, feature squeezing, binary
filtering, and random filtering) up to 6%, 50%, and 55% for FGSM, MIM, and PGD
attacks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial attacks on an optical neural network. (arXiv:2205.01226v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01226">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks have been extensively investigated for machine learning
systems including deep learning in the digital domain. However, the adversarial
attacks on optical neural networks (ONN) have been seldom considered
previously. In this work, we first construct an accurate image classifier with
an ONN using a mesh of interconnected Mach-Zehnder interferometers (MZI). Then
a corresponding adversarial attack scheme is proposed for the first time. The
attacked images are visually very similar to the original ones but the ONN
system becomes malfunctioned and generates wrong classification results in most
time. The results indicate that adversarial attack is also a significant issue
for optical machine learning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Weird Trick to Improve Your Semi-Weakly Supervised Semantic Segmentation Model. (arXiv:2205.01233v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01233">
<div class="article-summary-box-inner">
<span><p>Semi-weakly supervised semantic segmentation (SWSSS) aims to train a model to
identify objects in images based on a small number of images with pixel-level
labels, and many more images with only image-level labels. Most existing SWSSS
algorithms extract pixel-level pseudo-labels from an image classifier - a very
difficult task to do well, hence requiring complicated architectures and
extensive hyperparameter tuning on fully-supervised validation sets. We propose
a method called prediction filtering, which instead of extracting
pseudo-labels, just uses the classifier as a classifier: it ignores any
segmentation predictions from classes which the classifier is confident are not
present. Adding this simple post-processing method to baselines gives results
competitive with or better than prior SWSSS algorithms. Moreover, it is
compatible with pseudo-label methods: adding prediction filtering to existing
SWSSS algorithms further improves segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Performance-Consistent and Computation-Efficient CNN System for High-Quality Automated Brain Tumor Segmentation. (arXiv:2205.01239v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01239">
<div class="article-summary-box-inner">
<span><p>The research on developing CNN-based fully-automated Brain-Tumor-Segmentation
systems has been progressed rapidly. For the systems to be applicable in
practice, a good The research on developing CNN-based fully-automated
Brain-Tumor-Segmentation systems has been progressed rapidly. For the systems
to be applicable in practice, a good processing quality and reliability are the
must. Moreover, for wide applications of such systems, a minimization of
computation complexity is desirable, which can also result in a minimization of
randomness in computation and, consequently, a better performance consistency.
To this end, the CNN in the proposed system has a unique structure with 2
distinguished characters. Firstly, the three paths of its feature extraction
block are designed to extract, from the multi-modality input, comprehensive
feature information of mono-modality, paired-modality and cross-modality data,
respectively. Also, it has a particular three-branch classification block to
identify the pixels of 4 classes. Each branch is trained separately so that the
parameters are updated specifically with the corresponding ground truth data of
a target tumor areas. The convolution layers of the system are custom-designed
with specific purposes, resulting in a very simple config of 61,843 parameters
in total. The proposed system is tested extensively with BraTS2018 and
BraTS2019 datasets. The mean Dice scores, obtained from the ten experiments on
BraTS2018 validation samples, are 0.787+0.003, 0.886+0.002, 0.801+0.007, for
enhancing tumor, whole tumor and tumor core, respectively, and 0.751+0.007,
0.885+0.002, 0.776+0.004 on BraTS2019. The test results demonstrate that the
proposed system is able to perform high-quality segmentation in a consistent
manner. Furthermore, its extremely low computation complexity will facilitate
its implementation/application in various environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01271">
<div class="article-summary-box-inner">
<span><p>Pose estimation plays a critical role in human-centered vision applications.
However, it is difficult to deploy state-of-the-art HRNet-based pose estimation
models on resource-constrained edge devices due to the high computational cost
(more than 150 GMACs per frame). In this paper, we study efficient architecture
design for real-time multi-person pose estimation on edge. We reveal that
HRNet's high-resolution branches are redundant for models at the
low-computation region via our gradual shrinking experiments. Removing them
improves both efficiency and performance. Inspired by this finding, we design
LitePose, an efficient single-branch architecture for pose estimation, and
introduce two simple approaches to enhance the capacity of LitePose, including
Fusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the
redundancy in high-resolution branches, allowing scale-aware feature fusion
with low overhead. Large Kernel Convs significantly improve the model's
capacity and receptive field while maintaining a low computational cost. With
only 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3
kernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the
latency by up to 5.0x without sacrificing performance, compared with prior
state-of-the-art efficient pose estimation models, pushing the frontier of
real-time multi-person pose estimation on edge. Our code and pre-trained models
are released at https://github.com/mit-han-lab/litepose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Domain Object Detection by Target-Perceived Dual Branch Distillation. (arXiv:2205.01291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01291">
<div class="article-summary-box-inner">
<span><p>Cross domain object detection is a realistic and challenging task in the
wild. It suffers from performance degradation due to large shift of data
distributions and lack of instance-level annotations in the target domain.
Existing approaches mainly focus on either of these two difficulties, even
though they are closely coupled in cross domain object detection. To solve this
problem, we propose a novel Target-perceived Dual-branch Distillation (TDD)
framework. By integrating detection branches of both source and target domains
in a unified teacher-student learning scheme, it can reduce domain shift and
generate reliable supervision effectively. In particular, we first introduce a
distinct Target Proposal Perceiver between two domains. It can adaptively
enhance source detector to perceive objects in a target image, by leveraging
target proposal contexts from iterative cross-attention. Afterwards, we design
a concise Dual Branch Self Distillation strategy for model training, which can
progressively integrate complementary object knowledge from different domains
via self-distillation in two branches. Finally, we conduct extensive
experiments on a number of widely-used scenarios in cross domain object
detection. The results show that our TDD significantly outperforms the
state-of-the-art methods on all the benchmarks. Our code and model will be
available at https://github.com/Feobi1999/TDD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RU-Net: Regularized Unrolling Network for Scene Graph Generation. (arXiv:2205.01297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01297">
<div class="article-summary-box-inner">
<span><p>Scene graph generation (SGG) aims to detect objects and predict the
relationships between each pair of objects. Existing SGG methods usually suffer
from several issues, including 1) ambiguous object representations, as graph
neural network-based message passing (GMP) modules are typically sensitive to
spurious inter-node correlations, and 2) low diversity in relationship
predictions due to severe class imbalance and a large number of missing
annotations. To address both problems, in this paper, we propose a regularized
unrolling network (RU-Net). We first study the relation between GMP and graph
Laplacian denoising (GLD) from the perspective of the unrolling technique,
determining that GMP can be formulated as a solver for GLD. Based on this
observation, we propose an unrolled message passing module and introduce an
$\ell_p$-based graph regularization to suppress spurious connections between
nodes. Second, we propose a group diversity enhancement module that promotes
the prediction diversity of relationships via rank maximization. Systematic
experiments demonstrate that RU-Net is effective under a variety of settings
and metrics. Furthermore, RU-Net achieves new state-of-the-arts on three
popular databases: VG, VRD, and OI. Code is available at
https://github.com/siml3/RU-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Governing Laws and Source Input for Dynamical Systems from Videos. (arXiv:2205.01314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01314">
<div class="article-summary-box-inner">
<span><p>Distilling interpretable physical laws from videos has led to expanded
interest in the computer vision community recently thanks to the advances in
deep learning, but still remains a great challenge. This paper introduces an
end-to-end unsupervised deep learning framework to uncover the explicit
governing equations of dynamics presented by moving object(s), based on
recorded videos. Instead in the pixel (spatial) coordinate system of image
space, the physical law is modeled in a regressed underlying physical
coordinate system where the physical states follow potential explicit governing
equations. A numerical integrator-based sparse regression module is designed
and serves as a physical constraint to the autoencoder and coordinate system
regression, and, in the meanwhile, uncover the parsimonious closed-form
governing equations from the learned physical states. Experiments on simulated
dynamical scenes show that the proposed method is able to distill closed-form
governing equations and simultaneously identify unknown excitation input for
several dynamical systems recorded by videos, which fills in the gap in
literature where no existing methods are available and applicable for solving
this type of problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HL-Net: Heterophily Learning Network for Scene Graph Generatio. (arXiv:2205.01316v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01316">
<div class="article-summary-box-inner">
<span><p>Scene graph generation (SGG) aims to detect objects and predict their
pairwise relationships within an image. Current SGG methods typically utilize
graph neural networks (GNNs) to acquire context information between
objects/relationships. Despite their effectiveness, however, current SGG
methods only assume scene graph homophily while ignoring heterophily.
Accordingly, in this paper, we propose a novel Heterophily Learning Network
(HL-Net) to comprehensively explore the homophily and heterophily between
objects/relationships in scene graphs. More specifically, HL-Net comprises the
following 1) an adaptive reweighting transformer module, which adaptively
integrates the information from different layers to exploit both the
heterophily and homophily in objects; 2) a relationship feature propagation
module that efficiently explores the connections between relationships by
considering heterophily in order to refine the relationship representation; 3)
a heterophily-aware message-passing scheme to further distinguish the
heterophily and homophily between objects/relationships, thereby facilitating
improved message passing in graphs. We conducted extensive experiments on two
public datasets: Visual Genome (VG) and Open Images (OI). The experimental
results demonstrate the superiority of our proposed HL-Net over existing
state-of-the-art approaches. In more detail, HL-Net outperforms the second-best
competitors by 2.1$\%$ on the VG dataset for scene graph classification and
1.2$\%$ on the IO dataset for the final score. Code is available at
https://github.com/siml3/HL-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioTouchPass: Handwritten Passwords for Touchscreen Biometrics. (arXiv:2205.01353v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01353">
<div class="article-summary-box-inner">
<span><p>This work enhances traditional authentication systems based on Personal
Identification Numbers (PIN) and One-Time Passwords (OTP) through the
incorporation of biometric information as a second level of user
authentication. In our proposed approach, users draw each digit of the password
on the touchscreen of the device instead of typing them as usual. A complete
analysis of our proposed biometric system is carried out regarding the
discriminative power of each handwritten digit and the robustness when
increasing the length of the password and the number of enrolment samples. The
new e-BioDigit database, which comprises on-line handwritten digits from 0 to
9, has been acquired using the finger as input on a mobile device. This
database is used in the experiments reported in this work and it is available
together with benchmark results in GitHub. Finally, we discuss specific details
for the deployment of our proposed approach on current PIN and OTP systems,
achieving results with Equal Error Rates (EERs) ca. 4.0% when the attacker
knows the password. These results encourage the deployment of our proposed
approach in comparison to traditional PIN and OTP systems where the attack
would have 100% success rate under the same impostor scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Loose-Fitting Garment Deformations Using Bone-Driven Motion Networks. (arXiv:2205.01355v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01355">
<div class="article-summary-box-inner">
<span><p>We present a learning algorithm that uses bone-driven motion networks to
predict the deformation of loose-fitting garment meshes at interactive rates.
Given a garment, we generate a simulation database and extract virtual bones
from simulated mesh sequences using skin decomposition. At runtime, we
separately compute low- and high-frequency deformations in a sequential manner.
The low-frequency deformations are predicted by transferring body motions to
virtual bones' motions, and the high-frequency deformations are estimated
leveraging the global information of virtual bones' motions and local
information extracted from low-frequency meshes. In addition, our method can
estimate garment deformations caused by variations of the simulation parameters
(e.g., fabric's bending stiffness) using an RBF kernel ensembling trained
networks for different sets of simulation parameters. Through extensive
comparisons, we show that our method outperforms state-of-the-art methods in
terms of prediction accuracy of mesh deformations by about 20% in RMSE and 10%
in Hausdorff distance and STED. The code and data are available at
https://github.com/non-void/VirtualBones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A hybrid multi-object segmentation framework with model-based B-splines for microbial single cell analysis. (arXiv:2205.01367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01367">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a hybrid approach for multi-object microbial cell
segmentation. The approach combines an ML-based detection with a geometry-aware
variational-based segmentation using B-splines that are parametrized based on a
geometric model of the cell shape. The detection is done first using YOLOv5. In
a second step, each detected cell is segmented individually. Thus, the
segmentation only needs to be done on a per-cell basis, which makes it amenable
to a variational approach that incorporates prior knowledge on the geometry.
Here, the contour of the segmentation is modelled as closed uniform cubic
B-spline, whose control points are parametrized using the known cell geometry.
Compared to purely ML-based segmentation approaches, which need accurate
segmentation maps as training data that are very laborious to produce, our
method just needs bounding boxes as training data. Still, the proposed method
performs on par with ML-based segmentation approaches usually used in this
context. We study the performance of the proposed method on time-lapse
microscopy data of Corynebacterium glutamicum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Copy Motion From One to Another: Fake Motion Video Generation. (arXiv:2205.01373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01373">
<div class="article-summary-box-inner">
<span><p>One compelling application of artificial intelligence is to generate a video
of a target person performing arbitrary desired motion (from a source person).
While the state-of-the-art methods are able to synthesize a video demonstrating
similar broad stroke motion details, they are generally lacking in texture
details. A pertinent manifestation appears as distorted face, feet, and hands,
and such flaws are very sensitively perceived by human observers. Furthermore,
current methods typically employ GANs with a L2 loss to assess the authenticity
of the generated videos, inherently requiring a large amount of training
samples to learn the texture details for adequate video generation. In this
work, we tackle these challenges from three aspects: 1) We disentangle each
video frame into foreground (the person) and background, focusing on generating
the foreground to reduce the underlying dimension of the network output. 2) We
propose a theoretically motivated Gromov-Wasserstein loss that facilitates
learning the mapping from a pose to a foreground image. 3) To enhance texture
details, we encode facial features with geometric guidance and employ local
GANs to refine the face, feet, and hands. Extensive experiments show that our
method is able to generate realistic target person videos, faithfully copying
complex motions from a source person. Our code and datasets are released at
https://github.com/Sifann/FakeMotion
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning in Multimodal Remote Sensing Data Fusion: A Comprehensive Review. (arXiv:2205.01380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01380">
<div class="article-summary-box-inner">
<span><p>With the extremely rapid advances in remote sensing (RS) technology, a great
quantity of Earth observation (EO) data featuring considerable and complicated
heterogeneity is readily available nowadays, which renders researchers an
opportunity to tackle current geoscience applications in a fresh way. With the
joint utilization of EO data, much research on multimodal RS data fusion has
made tremendous progress in recent years, yet these developed traditional
algorithms inevitably meet the performance bottleneck due to the lack of the
ability to comprehensively analyse and interpret these strongly heterogeneous
data. Hence, this non-negligible limitation further arouses an intense demand
for an alternative tool with powerful processing competence. Deep learning
(DL), as a cutting-edge technology, has witnessed remarkable breakthroughs in
numerous computer vision tasks owing to its impressive ability in data
representation and reconstruction. Naturally, it has been successfully applied
to the field of multimodal RS data fusion, yielding great improvement compared
with traditional methods. This survey aims to present a systematic overview in
DL-based multimodal RS data fusion. More specifically, some essential knowledge
about this topic is first given. Subsequently, a literature survey is conducted
to analyse the trends of this field. Some prevalent sub-fields in the
multimodal RS data fusion are then reviewed in terms of the to-be-fused data
modalities, i.e., spatiospectral, spatiotemporal, light detection and
ranging-optical, synthetic aperture radar-optical, and RS-Geospatial Big Data
fusion. Furthermore, We collect and summarize some valuable resources for the
sake of the development in multimodal RS data fusion. Finally, the remaining
challenges and potential future directions are highlighted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sampling-free obstacle gradients and reactive planning in Neural Radiance Fields (NeRF). (arXiv:2205.01389v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01389">
<div class="article-summary-box-inner">
<span><p>This work investigates the use of Neural implicit representations,
specifically Neural Radiance Fields (NeRF), for geometrical queries and motion
planning. We show that by adding the capacity to infer occupancy in a radius to
a pre-trained NeRF, we are effectively learning an approximation to a Euclidean
Signed Distance Field (ESDF). Using backward differentiation of the augmented
network, we obtain an obstacle gradient that is integrated into an obstacle
avoidance policy based on the Riemannian Motion Policies (RMP) framework. Thus,
our findings allow for very fast sampling-free obstacle avoidance planning in
the implicit representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP). (arXiv:2205.01397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01397">
<div class="article-summary-box-inner">
<span><p>Contrastively trained image-text models such as CLIP, ALIGN, and BASIC have
demonstrated unprecedented robustness to multiple challenging natural
distribution shifts. Since these image-text models differ from previous
training approaches in several ways, an important question is what causes the
large robustness gains. We answer this question via a systematic experimental
investigation. Concretely, we study five different possible causes for the
robustness gains: (i) the training set size, (ii) the training distribution,
(iii) language supervision at training time, (iv) language supervision at test
time, and (v) the contrastive loss function. Our experiments show that the more
diverse training distribution is the main cause for the robustness gains, with
the other factors contributing little to no robustness. Beyond our experimental
results, we also introduce ImageNet-Captions, a version of ImageNet with
original text annotations from Flickr, to enable further controlled experiments
of language-image training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Outdoor Monocular Depth Estimation: A Research Review. (arXiv:2205.01399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01399">
<div class="article-summary-box-inner">
<span><p>Depth estimation is an important task, applied in various methods and
applications of computer vision. While the traditional methods of estimating
depth are based on depth cues and require specific equipment such as stereo
cameras and configuring input according to the approach being used, the focus
at the current time is on a single source, or monocular, depth estimation. The
recent developments in Convolution Neural Networks along with the integration
of classical methods in these deep learning approaches have led to a lot of
advancements in the depth estimation problem. The problem of outdoor depth
estimation, or depth estimation in wild, is a very scarcely researched field of
study. In this paper, we give an overview of the available datasets, depth
estimation methods, research work, trends, challenges, and opportunities that
exist for open research. To our knowledge, no openly available survey work
provides a comprehensive collection of outdoor depth estimation techniques and
research scope, making our work an essential contribution for people looking to
enter this field of study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Detection of Unknown Objects on Roads for Autonomous Driving. (arXiv:2205.01414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01414">
<div class="article-summary-box-inner">
<span><p>Tremendous progress in deep learning over the last years has led towards a
future with autonomous vehicles on our roads. Nevertheless, the performance of
their perception systems is strongly dependent on the quality of the utilized
training data. As these usually only cover a fraction of all object classes an
autonomous driving system will face, such systems struggle with handling the
unexpected. In order to safely operate on public roads, the identification of
objects from unknown classes remains a crucial task. In this paper, we propose
a novel pipeline to detect unknown objects. Instead of focusing on a single
sensor modality, we make use of lidar and camera data by combining state-of-the
art detection models in a sequential manner. We evaluate our approach on the
Waymo Open Perception Dataset and point out current research gaps in anomaly
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Analysis of the Use of Real-Time Reachability for the Safety Assurance of Autonomous Vehicles. (arXiv:2205.01419v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01419">
<div class="article-summary-box-inner">
<span><p>Recent advances in machine learning technologies and sensing have paved the
way for the belief that safe, accessible, and convenient autonomous vehicles
may be realized in the near future. Despite tremendous advances within this
context, fundamental challenges around safety and reliability are limiting
their arrival and comprehensive adoption. Autonomous vehicles are often tasked
with operating in dynamic and uncertain environments. As a result, they often
make use of highly complex components, such as machine learning approaches, to
handle the nuances of sensing, actuation, and control. While these methods are
highly effective, they are notoriously difficult to assure. Moreover, within
uncertain and dynamic environments, design time assurance analyses may not be
sufficient to guarantee safety. Thus, it is critical to monitor the correctness
of these systems at runtime. One approach for providing runtime assurance of
systems with components that may not be amenable to formal analysis is the
simplex architecture, where an unverified component is wrapped with a safety
controller and a switching logic designed to prevent dangerous behavior. In
this paper, we propose using a real-time reachability algorithm for the
implementation of the simplex architecture to assure the safety of a 1/10 scale
open source autonomous vehicle platform known as F1/10. The reachability
algorithm that we leverage (a) provides provable guarantees of safety, and (b)
is used to detect potentially unsafe scenarios. In our approach, the need to
analyze an underlying controller is abstracted away, instead focusing on the
effects of the controller's decisions on the system's future states. We
demonstrate the efficacy of our architecture through a vast set of experiments
conducted both in simulation and on an embedded hardware platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-Selective Geometry Upsampling of Point Clouds. (arXiv:2205.01458v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01458">
<div class="article-summary-box-inner">
<span><p>The demand for high-resolution point clouds has increased throughout the last
years. However, capturing high-resolution point clouds is expensive and thus,
frequently replaced by upsampling of low-resolution data. Most state-of-the-art
methods are either restricted to a rastered grid, incorporate normal vectors,
or are trained for a single use case. We propose to use the frequency
selectivity principle, where a frequency model is estimated locally that
approximates the surface of the point cloud. Then, additional points are
inserted into the approximated surface. Our novel frequency-selective geometry
upsampling shows superior results in terms of subjective as well as objective
quality compared to state-of-the-art methods for scaling factors of 2 and 4. On
average, our proposed method shows a 4.4 times smaller point-to-point error
than the second best state-of-the-art PU-Net for a scale factor of 4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Semantic Scene Perception using Distributed Smart Edge Sensors. (arXiv:2205.01460v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01460">
<div class="article-summary-box-inner">
<span><p>We present a system for 3D semantic scene perception consisting of a network
of distributed smart edge sensors. The sensor nodes are based on an embedded
CNN inference accelerator and RGB-D and thermal cameras. Efficient vision CNN
models for object detection, semantic segmentation, and human pose estimation
run on-device in real time. 2D human keypoint estimations, augmented with the
RGB-D depth estimate, as well as semantically annotated point clouds are
streamed from the sensors to a central backend, where multiple viewpoints are
fused into an allocentric 3D semantic scene model. As the image interpretation
is computed locally, only semantic information is sent over the network. The
raw images remain on the sensor boards, significantly reducing the required
bandwidth, and mitigating privacy risks for the observed persons. We evaluate
the proposed system in challenging real-world multi-person scenes in our lab.
The proposed perception system provides a complete scene view containing
semantically annotated 3D geometry and estimates 3D poses of multiple persons
in real time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subspace Diffusion Generative Models. (arXiv:2205.01490v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01490">
<div class="article-summary-box-inner">
<span><p>Score-based models generate samples by mapping noise to data (and vice versa)
via a high-dimensional diffusion process. We question whether it is necessary
to run this entire process at high dimensionality and incur all the
inconveniences thereof. Instead, we restrict the diffusion via projections onto
subspaces as the data distribution evolves toward noise. When applied to
state-of-the-art models, our framework simultaneously improves sample quality
-- reaching an FID of 2.17 on unconditional CIFAR-10 -- and reduces the
computational cost of inference for the same number of denoising steps. Our
framework is fully compatible with continuous-time diffusion and retains its
flexible capabilities, including exact log-likelihoods and controllable
generation. Code is available at
https://github.com/bjing2016/subspace-diffusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey of Image Augmentation Techniques for Deep Learning. (arXiv:2205.01491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01491">
<div class="article-summary-box-inner">
<span><p>Deep learning has been achieving decent performance in computer vision
requiring a large volume of images, however, collecting images is expensive and
difficult in many scenarios. To alleviate this issue, many image augmentation
algorithms have been proposed as effective and efficient strategies.
Understanding current algorithms is essential to find suitable methods or
develop novel techniques for given tasks. In this paper, we perform a
comprehensive survey on image augmentation for deep learning with a novel
informative taxonomy. To get the basic idea why we need image augmentation, we
introduce the challenges in computer vision tasks and vicinity distribution.
Then, the algorithms are split into three categories; model-free, model-based,
and optimizing policy-based. The model-free category employs image processing
methods while the model-based method leverages trainable image generation
models. In contrast, the optimizing policy-based approach aims to find the
optimal operations or their combinations. Furthermore, we discuss the current
trend of common applications with two more active topics, leveraging different
ways to understand image augmentation, such as group and kernel theory, and
deploying image augmentation for unsupervised learning. Based on the analysis,
we believe that our survey gives a better understanding helpful to choose
suitable methods or design novel algorithms for practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compact Neural Networks via Stacking Designed Basic Units. (arXiv:2205.01508v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01508">
<div class="article-summary-box-inner">
<span><p>Unstructured pruning has the limitation of dealing with the sparse and
irregular weights. By contrast, structured pruning can help eliminate this
drawback but it requires complex criterion to determine which components to be
pruned. To this end, this paper presents a new method termed TissueNet, which
directly constructs compact neural networks with fewer weight parameters by
independently stacking designed basic units, without requiring additional
judgement criteria anymore. Given the basic units of various architectures,
they are combined and stacked in a certain form to build up compact neural
networks. We formulate TissueNet in diverse popular backbones for comparison
with the state-of-the-art pruning methods on different benchmark datasets.
Moreover, two new metrics are proposed to evaluate compression performance.
Experiment results show that TissueNet can achieve comparable classification
accuracy while saving up to around 80% FLOPs and 89.7% parameters. That is,
stacking basic units provides a new promising way for network compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MS Lesion Segmentation: Revisiting Weighting Mechanisms for Federated Learning. (arXiv:2205.01509v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01509">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) has been widely employed for medical image analysis
to facilitate multi-client collaborative learning without sharing raw data.
Despite great success, FL's performance is limited for multiple sclerosis (MS)
lesion segmentation tasks, due to variance in lesion characteristics imparted
by different scanners and acquisition parameters. In this work, we propose the
first FL MS lesion segmentation framework via two effective re-weighting
mechanisms. Specifically, a learnable weight is assigned to each local node
during the aggregation process, based on its segmentation performance. In
addition, the segmentation loss function in each client is also re-weighted
according to the lesion volume for the data during training. Comparison
experiments on two FL MS segmentation scenarios using public and clinical
datasets have demonstrated the effectiveness of the proposed method by
outperforming other FL methods significantly. Furthermore, the segmentation
performance of FL incorporating our proposed aggregation mechanism can exceed
centralised training with all the raw data. The extensive evaluation also
indicated the superiority of our method when estimating brain volume
differences estimation after lesion inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Network for Joint Object Detection, Semantic Segmentation and Human Pose Estimation in Vehicle Occupancy Monitoring. (arXiv:2205.01515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01515">
<div class="article-summary-box-inner">
<span><p>In order to ensure safe autonomous driving, precise information about the
conditions in and around the vehicle must be available. Accordingly, the
monitoring of occupants and objects inside the vehicle is crucial. In the
state-of-the-art, single or multiple deep neural networks are used for either
object recognition, semantic segmentation, or human pose estimation. In
contrast, we propose our Multitask Detection, Segmentation and Pose Estimation
Network (MDSP) -- the first multitask network solving all these three tasks
jointly in the area of occupancy monitoring. Due to the shared architecture,
memory and computing costs can be saved while achieving higher accuracy.
Furthermore, our architecture allows a flexible combination of the three
mentioned tasks during a simple end-to-end training. We perform comprehensive
evaluations on the public datasets SVIRO and TiCaM in order to demonstrate the
superior performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Generative Distillation. (arXiv:2205.01529v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01529">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation has been applied to various tasks successfully. The
current distillation algorithm usually improves students' performance by
imitating the output of the teacher. This paper shows that teachers can also
improve students' representation power by guiding students' feature recovery.
From this point of view, we propose Masked Generative Distillation (MGD), which
is simple: we mask random pixels of the student's feature and force it to
generate the teacher's full feature through a simple block. MGD is a truly
general feature-based distillation method, which can be utilized on various
tasks, including image classification, object detection, semantic segmentation
and instance segmentation. We experiment on different models with extensive
datasets and the results show that all the students achieve excellent
improvements. Notably, we boost ResNet-18 from 69.90% to 71.69% ImageNet top-1
accuracy, RetinaNet with ResNet-50 backbone from 37.4 to 41.0 Boundingbox mAP,
SOLO based on ResNet-50 from 33.1 to 36.2 Mask mAP and DeepLabV3 based on
ResNet-18 from 73.20 to 76.02 mIoU. Our codes are available at
https://github.com/yzd-v/MGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiOcularGAN: Bimodal Synthesis and Annotation of Ocular Images. (arXiv:2205.01536v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01536">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art segmentation techniques for ocular images are
critically dependent on large-scale annotated datasets, which are
labor-intensive to gather and often raise privacy concerns. In this paper, we
present a novel framework, called BiOcularGAN, capable of generating synthetic
large-scale datasets of photorealistic (visible light and near infrared) ocular
images, together with corresponding segmentation labels to address these
issues. At its core, the framework relies on a novel Dual-Branch StyleGAN2
(DB-StyleGAN2) model that facilitates bimodal image generation, and a Semantic
Mask Generator (SMG) that produces semantic annotations by exploiting
DB-StyleGAN2's feature space. We evaluate BiOcularGAN through extensive
experiments across five diverse ocular datasets and analyze the effects of
bimodal data generation on image quality and the produced annotations. Our
experimental results show that BiOcularGAN is able to produce high-quality
matching bimodal images and annotations (with minimal manual intervention) that
can be used to train highly competitive (deep) segmentation models that perform
well across multiple real-world datasets. The source code will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Scale Sparse Convolution Point Cloud Semantic Segmentation Neural Network. (arXiv:2205.01550v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01550">
<div class="article-summary-box-inner">
<span><p>Point clouds have the characteristics of disorder, unstructured and
sparseness.Aiming at the problem of the non-structural nature of point clouds,
thanks to the excellent performance of convolutional neural networks in image
processing, one of the solutions is to extract features from point clouds based
on two-dimensional convolutional neural networks. The three-dimensional
information carried in the point cloud can be converted to two-dimensional, and
then processed by a two-dimensional convolutional neural network, and finally
back-projected to three-dimensional.In the process of projecting 3D information
to 2D and back-projection, certain information loss will inevitably be caused
to the point cloud and category inconsistency will be introduced in the
back-projection stage;Another solution is the voxel-based point cloud
segmentation method, which divides the point cloud into small grids one by
one.However, the point cloud is sparse, and the direct use of 3D convolutional
neural network inevitably wastes computing resources. In this paper, we propose
a feature extraction module based on multi-scale ultra-sparse convolution and a
feature selection module based on channel attention, and build a point cloud
segmentation network framework based on this.By introducing multi-scale sparse
convolution, network could capture richer feature information based on
convolution kernels of different sizes, improving the segmentation result of
point cloud segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-View Cross-Scene Multi-View Crowd Counting. (arXiv:2205.01551v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01551">
<div class="article-summary-box-inner">
<span><p>Multi-view crowd counting has been previously proposed to utilize
multi-cameras to extend the field-of-view of a single camera, capturing more
people in the scene, and improve counting performance for occluded people or
those in low resolution. However, the current multi-view paradigm trains and
tests on the same single scene and camera-views, which limits its practical
application. In this paper, we propose a cross-view cross-scene (CVCS)
multi-view crowd counting paradigm, where the training and testing occur on
different scenes with arbitrary camera layouts. To dynamically handle the
challenge of optimal view fusion under scene and camera layout change and
non-correspondence noise due to camera calibration errors or erroneous
features, we propose a CVCS model that attentively selects and fuses multiple
views together using camera layout geometry, and a noise view regularization
method to train the model to handle non-correspondence errors. We also generate
a large synthetic multi-camera crowd counting dataset with a large number of
scenes and camera views to capture many possible variations, which avoids the
difficulty of collecting and annotating such a large real dataset. We then test
our trained CVCS model on real multi-view counting datasets, by using
unsupervised domain transfer. The proposed CVCS model trained on synthetic data
outperforms the same model trained only on real data, and achieves promising
performance compared to fully supervised methods that train and test on the
same single scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT-MSF: Self-Supervised Monocular Scene Flow using Recurrent Optimizer. (arXiv:2205.01568v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01568">
<div class="article-summary-box-inner">
<span><p>Learning scene flow from a monocular camera still remains a challenging task
due to its ill-posedness as well as lack of annotated data. Self-supervised
methods demonstrate learning scene flow estimation from unlabeled data, yet
their accuracy lags behind (semi-)supervised methods. In this paper, we
introduce a self-supervised monocular scene flow method that substantially
improves the accuracy over the previous approaches. Based on RAFT, a
state-of-the-art optical flow model, we design a new decoder to iteratively
update 3D motion fields and disparity maps simultaneously. Furthermore, we
propose an enhanced upsampling layer and a disparity initialization technique,
which overall further improves accuracy up to 7.2%. Our method achieves
state-of-the-art accuracy among all self-supervised monocular scene flow
methods, improving accuracy by 34.2%. Our fine-tuned model outperforms the best
previous semi-supervised method with 228 times faster runtime. Code will be
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RangeSeg: Range-Aware Real Time Segmentation of 3D LiDAR Point Clouds. (arXiv:2205.01570v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01570">
<div class="article-summary-box-inner">
<span><p>Semantic outdoor scene understanding based on 3D LiDAR point clouds is a
challenging task for autonomous driving due to the sparse and irregular data
structure. This paper takes advantages of the uneven range distribution of
different LiDAR laser beams to propose a range aware instance segmentation
network, RangeSeg. RangeSeg uses a shared encoder backbone with two range
dependent decoders. A heavy decoder only computes top of a range image where
the far and small objects locate to improve small object detection accuracy,
and a light decoder computes whole range image for low computational cost. The
results are further clustered by the DBSCAN method with a resolution weighted
distance function to get instance-level segmentation results. Experiments on
the KITTI dataset show that RangeSeg outperforms the state-of-the-art semantic
segmentation methods with enormous speedup and improves the instance-level
segmentation performance on small and far objects. The whole RangeSeg pipeline
meets the real time requirement on NVIDIA\textsuperscript{\textregistered}
JETSON AGX Xavier with 19 frames per second in average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real Time 1280x720 Object Detection Chip With 585MB/s Memory Traffic. (arXiv:2205.01571v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01571">
<div class="article-summary-box-inner">
<span><p>Memory bandwidth has become the real-time bottleneck of current deep learning
accelerators (DLA), particularly for high definition (HD) object detection.
Under resource constraints, this paper proposes a low memory traffic DLA chip
with joint hardware and software optimization. To maximize hardware utilization
under memory bandwidth, we morph and fuse the object detection model into a
group fusion-ready model to reduce intermediate data access. This reduces the
YOLOv2's feature memory traffic from 2.9 GB/s to 0.15 GB/s. To support group
fusion, our previous DLA based hardware employes a unified buffer with
write-masking for simple layer-by-layer processing in a fusion group. When
compared to our previous DLA with the same PE numbers, the chip implemented in
a TSMC 40nm process supports 1280x720@30FPS object detection and consumes 7.9X
less external DRAM access energy, from 2607 mJ to 327.6 mJ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better plain ViT baselines for ImageNet-1k. (arXiv:2205.01580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01580">
<div class="article-summary-box-inner">
<span><p>It is commonly accepted that the Vision Transformer model requires
sophisticated regularization techniques to excel at ImageNet-1k scale data.
Surprisingly, we find this is not the case and standard data augmentation is
sufficient. This note presents a few minor modifications to the original Vision
Transformer (ViT) vanilla training setting that dramatically improve the
performance of plain ViT models. Notably, 90 epochs of training surpass 76%
top-1 accuracy in under seven hours on a TPUv3-8, similar to the classic
ResNet50 baseline, and 300 epochs of training reach 80% in less than one day.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simpler is Better: off-the-shelf Continual Learning Through Pretrained Backbones. (arXiv:2205.01586v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01586">
<div class="article-summary-box-inner">
<span><p>In this short paper, we propose a baseline (off-the-shelf) for Continual
Learning of Computer Vision problems, by leveraging the power of pretrained
models. By doing so, we devise a simple approach achieving strong performance
for most of the common benchmarks. Our approach is fast since requires no
parameters updates and has minimal memory requirements (order of KBytes). In
particular, the "training" phase reorders data and exploit the power of
pretrained models to compute a class prototype and fill a memory bank. At
inference time we match the closest prototype through a knn-like approach,
providing us the prediction. We will see how this naive solution can act as an
off-the-shelf continual learning system. In order to better consolidate our
results, we compare the devised pipeline with common CNN models and show the
superiority of Vision Transformers, suggesting that such architectures have the
ability to produce features of higher quality. Moreover, this simple pipeline,
raises the same questions raised by previous works \cite{gdumb} on the
effective progresses made by the CL community especially in the dataset
considered and the usage of pretrained models. Code is live at
https://github.com/francesco-p/off-the-shelf-cl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bidirectional Conversion Network for Cross-Spectral Face Recognition. (arXiv:2205.01595v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01595">
<div class="article-summary-box-inner">
<span><p>Face recognition in the infrared (IR) band has become an important supplement
to visible light face recognition due to its advantages of independent
background light, strong penetration, ability of imaging under harsh
environments such as nighttime, rain and fog. However, cross-spectral face
recognition (i.e., VIS to IR) is very challenging due to the dramatic
difference between the visible light and IR imageries as well as the lack of
paired training data. This paper proposes a framework of bidirectional
cross-spectral conversion (BCSC-GAN) between the heterogeneous face images, and
designs an adaptive weighted fusion mechanism based on information fusion
theory. The network reduces the cross-spectral recognition problem into an
intra-spectral problem, and improves performance by fusing bidirectional
information. Specifically, a face identity retaining module (IRM) is introduced
with the ability to preserve identity features, and a new composite loss
function is designed to overcome the modal differences caused by different
spectral characteristics. Two datasets of TINDERS and CASIA were tested, where
performance metrics of FID, recognition rate, equal error rate and normalized
distance were compared. Results show that our proposed network is superior than
other state-of-the-art methods. Additionally, the proposed rule of Self
Adaptive Weighted Fusion (SAWF) is better than the recognition results of the
unfused case and other traditional fusion rules that are commonly used, which
further justifies the effectiveness and superiority of the proposed
bidirectional conversion approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Modeling Creative Processes for Algorithmic Painting. (arXiv:2205.01605v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01605">
<div class="article-summary-box-inner">
<span><p>This paper proposes a framework for computational modeling of artistic
painting algorithms, inspired by human creative practices. Based on examples
from expert artists and from the author's own experience, the paper argues that
creative processes often involve two important components: vague, high-level
goals (e.g., "make a good painting"), and exploratory processes for discovering
new ideas. This paper then sketches out possible computational mechanisms for
imitating those elements of the painting process, including underspecified loss
functions and iterative painting procedures with explicit task decompositions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Segmentation of Aircraft Dents in Point Clouds. (arXiv:2205.01614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01614">
<div class="article-summary-box-inner">
<span><p>Dents on the aircraft skin are frequent and may easily go undetected during
airworthiness checks, as their inspection process is tedious and extremely
subject to human factors and environmental conditions. Nowadays, 3D scanning
technologies are being proposed for more reliable, human-independent
measurements, yet the process of inspection and reporting remains laborious and
time consuming because data acquisition and validation are still carried out by
the engineer. For full automation of dent inspection, the acquired point cloud
data must be analysed via a reliable segmentation algorithm, releasing humans
from the search and evaluation of damage. This paper reports on two
developments towards automated dent inspection. The first is a method to
generate a synthetic dataset of dented surfaces to train a fully convolutional
neural network. The training of machine learning algorithms needs a substantial
volume of dent data, which is not readily available. Dents are thus simulated
in random positions and shapes, within criteria and definitions of a Boeing 737
structural repair manual. The noise distribution from the scanning apparatus is
then added to reflect the complete process of 3D point acquisition on the
training. The second proposition is a surface fitting strategy to convert 3D
point clouds to 2.5D. This allows higher resolution point clouds to be
processed with a small amount of memory compared with state-of-the-art methods
involving 3D sampling approaches. Simulations with available ground truth data
show that the proposed technique reaches an intersection-over-union of over
80%. Experiments over dent samples prove an effective detection of dents with a
speed of over 500 000 points per second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynopSet: Multiscale Visual Abstraction Set for Explanatory Analysis of DNA Nanotechnology Simulations. (arXiv:2205.01628v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01628">
<div class="article-summary-box-inner">
<span><p>We propose a new abstraction set (SynopSet) that has a continuum of visual
representations for the explanatory analysis of molecular dynamics simulations
(MDS) in the DNA nanotechnology domain. By re-purposing the commonly used
progress bar and designing novel visuals, as well as transforming the data from
the domain format to a format that better fits the newly designed visuals, we
compose this new set of representations. This set is also designed to be
capable of showing all spatial and temporal details, and all structural
complexity, or abstracting these to various degrees, enabling both the slow
playback of the simulation for detailed examinations or very fast playback for
an overview that helps to efficiently identify events of interest, as well as
several intermediate levels between these two extremes. For any pair of
successive representations, we demonstrate smooth, continuous transitions,
enabling users to keep track of relevant information from one representation to
the next. By providing multiple representations suited to different temporal
resolutions and connected by smooth transitions, we enable time-efficient
simulation analysis, giving users the opportunity to examine and present
important phases in great detail, or leverage abstract representations to go
over uneventful phases much faster. Domain experts can thus gain actionable
insight about their simulations and communicate it in a much shorter time.
Further, the novel representations are more intuitive and also enable
researchers unfamiliar with MDS analysis graphs to better understand the
simulation results. We assessed the effectiveness of SynopSet on 12 DNA
nanostructure simulations together with a domain expert. We have also shown
that our set of representations can be systematically located in a
visualization space, dubbed SynopSpace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Geometry: Correspondences Refinement Based on Algebraic Properties. (arXiv:2205.01634v1 [cs.CG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01634">
<div class="article-summary-box-inner">
<span><p>Correspondences estimation or feature matching is a key step in the
image-based 3D reconstruction problem. In this paper, we propose two algebraic
properties for correspondences. The first is a rank deficient matrix construct
from the correspondences of at least nine key-points on two images (two-view
correspondences) and the second is also another rank deficient matrix built
from the other correspondences of six key-points on at least five images
(multi-view correspondences). To our knowledge, there are no theoretical
results for multi-view correspondences prior to this paper. To obtain accurate
correspondences, multi-view correspondences seem to be more useful than
two-view correspondences. From these two algebraic properties, we propose an
refinement algorithm for correspondences. This algorithm is a combination of
correspondences refinement, outliers recognition and missing key-points
recovery. Real experiments from the project of reconstructing Buddha statue
show that the proposed refinement algorithm can reduce the average error from
77 pixels to 55 pixels on the correspondences estimation. This drop is
substantial and it validates our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Object Detection with Mean-Teacher Transformer. (arXiv:2205.01643v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01643">
<div class="article-summary-box-inner">
<span><p>Recently, DEtection TRansformer (DETR), an end-to-end object detection
pipeline, has achieved promising performance. However, it requires large-scale
labeled data and suffers from domain shift, especially when no labeled data is
available in the target domain. To solve this problem, we propose an end-to-end
cross-domain detection transformer based on the mean teacher knowledge transfer
(MTKT), which transfers knowledge between domains via pseudo labels. To improve
the quality of pseudo labels in the target domain, which is a crucial factor
for better domain adaptation, we design three levels of source-target feature
alignment strategies based on the architecture of the Transformer, including
domain query-based feature alignment (DQFA), bi-level-graph-based prototype
alignment (BGPA), and token-wise image feature alignment (TIFA). These three
levels of feature alignment match the global, local, and instance features
between source and target, respectively. With these strategies, more accurate
pseudo labels can be obtained, and knowledge can be better transferred from
source to target, thus improving the cross-domain capability of the detection
transformer. Extensive experiments demonstrate that our proposed method
achieves state-of-the-art performance on three domain adaptation scenarios,
especially the result of Sim10k to Cityscapes scenario is remarkably improved
from 52.6 mAP to 57.9 mAP. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Enriched Features for Fast Image Restoration and Enhancement. (arXiv:2205.01649v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01649">
<div class="article-summary-box-inner">
<span><p>Given a degraded input image, image restoration aims to recover the missing
high-quality image content. Numerous applications demand effective image
restoration, e.g., computational photography, surveillance, autonomous
vehicles, and remote sensing. Significant advances in image restoration have
been made in recent years, dominated by convolutional neural networks (CNNs).
The widely-used CNN-based methods typically operate either on full-resolution
or on progressively low-resolution representations. In the former case, spatial
details are preserved but the contextual information cannot be precisely
encoded. In the latter case, generated outputs are semantically reliable but
spatially less accurate. This paper presents a new architecture with a holistic
goal of maintaining spatially-precise high-resolution representations through
the entire network, and receiving complementary contextual information from the
low-resolution representations. The core of our approach is a multi-scale
residual block containing the following key elements: (a) parallel
multi-resolution convolution streams for extracting multi-scale features, (b)
information exchange across the multi-resolution streams, (c) non-local
attention mechanism for capturing contextual information, and (d) attention
based multi-scale feature aggregation. Our approach learns an enriched set of
features that combines contextual information from multiple scales, while
simultaneously preserving the high-resolution spatial details. Extensive
experiments on six real image benchmark datasets demonstrate that our method,
named as MIRNet-v2 , achieves state-of-the-art results for a variety of image
processing tasks, including defocus deblurring, image denoising,
super-resolution, and image enhancement. The source code and pre-trained models
are available at https://github.com/swz30/MIRNetv2
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Episodic Memory Question Answering. (arXiv:2205.01652v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01652">
<div class="article-summary-box-inner">
<span><p>Egocentric augmented reality devices such as wearable glasses passively
capture visual data as a human wearer tours a home environment. We envision a
scenario wherein the human communicates with an AI agent powering such a device
by asking questions (e.g., where did you last see my keys?). In order to
succeed at this task, the egocentric AI assistant must (1) construct
semantically rich and efficient scene memories that encode spatio-temporal
information about objects seen during the tour and (2) possess the ability to
understand the question and ground its answer into the semantic memory
representation. Towards that end, we introduce (1) a new task - Episodic Memory
Question Answering (EMQA) wherein an egocentric AI assistant is provided with a
video sequence (the tour) and a question as an input and is asked to localize
its answer to the question within the tour, (2) a dataset of grounded questions
designed to probe the agent's spatio-temporal understanding of the tour, and
(3) a model for the task that encodes the scene as an allocentric, top-down
semantic feature map and grounds the question into the map to localize the
answer. We show that our choice of episodic scene memory outperforms naive,
off-the-shelf solutions for the task as well as a host of very competitive
baselines and is robust to noise in depth, pose as well as camera jitter. The
project page can be found at: https://samyak-268.github.io/emqa .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoRefine: Self-Supervised Online Depth Refinement for Accurate Dense Mapping. (arXiv:2205.01656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01656">
<div class="article-summary-box-inner">
<span><p>We present a robust and accurate depth refinement system, named GeoRefine,
for geometrically-consistent dense mapping from monocular sequences. GeoRefine
consists of three modules: a hybrid SLAM module using learning-based priors, an
online depth refinement module leveraging self-supervision, and a global
mapping module via TSDF fusion. The proposed system is online by design and
achieves great robustness and accuracy via: (i) a robustified hybrid SLAM that
incorporates learning-based optical flow and/or depth; (ii) self-supervised
losses that leverage SLAM outputs and enforce long-term geometric consistency;
(iii) careful system design that avoids degenerate cases in online depth
refinement. We extensively evaluate GeoRefine on multiple public datasets and
reach as low as $5\%$ absolute relative depth errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Representation Learning for Zero-shot Action Recognition. (arXiv:2205.01657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01657">
<div class="article-summary-box-inner">
<span><p>We present a cross-modal Transformer-based framework, which jointly encodes
video data and text labels for zero-shot action recognition (ZSAR). Our model
employs a conceptually new pipeline by which visual representations are learned
in conjunction with visual-semantic associations in an end-to-end manner. The
model design provides a natural mechanism for visual and semantic
representations to be learned in a shared knowledge space, whereby it
encourages the learned visual embedding to be discriminative and more
semantically consistent. In zero-shot inference, we devise a simple semantic
transfer scheme that embeds semantic relatedness information between seen and
unseen classes to composite unseen visual prototypes. Accordingly, the
discriminative features in the visual structure could be preserved and
exploited to alleviate the typical zero-shot issues of information loss,
semantic gap, and the hubness problem. Under a rigorous zero-shot setting of
not pre-training on additional datasets, the experiment results show our model
considerably improves upon the state of the arts in ZSAR, reaching encouraging
top-1 accuracy on UCF101, HMDB51, and ActivityNet benchmark datasets. Code will
be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DANBO: Disentangled Articulated Neural Body Representations via Graph Neural Networks. (arXiv:2205.01666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01666">
<div class="article-summary-box-inner">
<span><p>Deep learning greatly improved the realism of animatable human models by
learning geometry and appearance from collections of 3D scans, template meshes,
and multi-view imagery. High-resolution models enable photo-realistic avatars
but at the cost of requiring studio settings not available to end users. Our
goal is to create avatars directly from raw images without relying on expensive
studio setups and surface tracking. While a few such approaches exist, those
have limited generalization capabilities and are prone to learning spurious
(chance) correlations between irrelevant body parts, resulting in implausible
deformations and missing body parts on unseen poses. We introduce a three-stage
method that induces two inductive biases to better disentangled pose-dependent
deformation. First, we model correlations of body parts explicitly with a graph
neural network. Second, to further reduce the effect of chance correlations, we
introduce localized per-bone features that use a factorized volumetric
representation and a new aggregation function. We demonstrate that our model
produces realistic body shapes under challenging unseen poses and shows
high-quality image synthesis. Our proposed representation strikes a better
trade-off between model capacity, expressiveness, and robustness than competing
methods. Project website: https://lemonatsu.github.io/danbo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Visual Editing with a Generatively Pre-Trained Artist. (arXiv:2205.01668v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01668">
<div class="article-summary-box-inner">
<span><p>We consider the targeted image editing problem: blending a region in a source
image with a driver image that specifies the desired change. Differently from
prior works, we solve this problem by learning a conditional probability
distribution of the edits, end-to-end. Training such a model requires
addressing a fundamental technical challenge: the lack of example edits for
training. To this end, we propose a self-supervised approach that simulates
edits by augmenting off-the-shelf images in a target domain. The benefits are
remarkable: implemented as a state-of-the-art auto-regressive transformer, our
approach is simple, sidesteps difficulties with previous methods based on
GAN-like priors, obtains significantly better edits, and is efficient.
Furthermore, we show that different blending effects can be learned by an
intuitive control of the augmentation process, with no other changes required
to the model architecture. We demonstrate the superiority of this approach
across several datasets in extensive quantitative and qualitative experiments,
including human studies, significantly outperforming prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation and Image Classification via Deep Conditional Adaptation Network. (arXiv:2006.07776v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07776">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation aims to generalize the supervised model
trained on a source domain to an unlabeled target domain. Marginal distribution
alignment of feature spaces is widely used to reduce the domain discrepancy
between the source and target domains. However, it assumes that the source and
target domains share the same label distribution, which limits their
application scope. In this paper, we consider a more general application
scenario where the label distributions of the source and target domains are not
the same. In this scenario, marginal distribution alignment-based methods will
be vulnerable to negative transfer. To address this issue, we propose a novel
unsupervised domain adaptation method, Deep Conditional Adaptation Network
(DCAN), based on conditional distribution alignment of feature spaces. To be
specific, we reduce the domain discrepancy by minimizing the Conditional
Maximum Mean Discrepancy between the conditional distributions of deep features
on the source and target domains, and extract the discriminant information from
target domain by maximizing the mutual information between samples and the
prediction labels. In addition, DCAN can be used to address a special scenario,
Partial unsupervised domain adaptation, where the target domain category is a
subset of the source domain category. Experiments on both unsupervised domain
adaptation and Partial unsupervised domain adaptation show that DCAN achieves
superior classification performance over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Learning of Multi-Object 3D Scene Decompositions Using Deep Shape Priors. (arXiv:2010.04030v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04030">
<div class="article-summary-box-inner">
<span><p>Representing scenes at the granularity of objects is a prerequisite for scene
understanding and decision making. We propose PriSMONet, a novel approach based
on Prior Shape knowledge for learning Multi-Object 3D scene decomposition and
representations from single images. Our approach learns to decompose images of
synthetic scenes with multiple objects on a planar surface into its constituent
scene objects and to infer their 3D properties from a single view. A recurrent
encoder regresses a latent representation of 3D shape, pose and texture of each
object from an input RGB image. By differentiable rendering, we train our model
to decompose scenes from RGB-D images in a self-supervised way. The 3D shapes
are represented continuously in function-space as signed distance functions
which we pre-train from example shapes in a supervised way. These shape priors
provide weak supervision signals to better condition the challenging overall
learning task. We evaluate the accuracy of our model in inferring 3D scene
layout, demonstrate its generative capabilities, assess its generalization to
real images, and point out benefits of the learned representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepCloth: Neural Garment Representation for Shape and Style Editing. (arXiv:2011.14619v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14619">
<div class="article-summary-box-inner">
<span><p>Garment representation, editing and animation are challenging topics in the
area of computer vision and graphics. It remains difficult for existing garment
representations to achieve smooth and plausible transitions between different
shapes and topologies. In this work, we introduce, DeepCloth, a unified
framework for garment representation, reconstruction, animation and editing.
Our unified framework contains 3 components: First, we represent the garment
geometry with a "topology-aware UV-position map", which allows for the unified
description of various garments with different shapes and topologies by
introducing an additional topology-aware UV-mask for the UV-position map.
Second, to further enable garment reconstruction and editing, we contribute a
method to embed the UV-based representations into a continuous feature space,
which enables garment shape reconstruction and editing by optimization and
control in the latent space, respectively. Finally, we propose a garment
animation method by unifying our neural garment representation with body shape
and pose, which achieves plausible garment animation results leveraging the
dynamic information encoded by our shape and style representation, even under
drastic garment editing operations. To conclude, with DeepCloth, we move a step
forward in establishing a more flexible and general 3D garment digitization
framework. Experiments demonstrate that our method can achieve state-of-the-art
garment representation performance compared with previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalising via Meta-Examples for Continual Learning in the Wild. (arXiv:2101.12081v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12081">
<div class="article-summary-box-inner">
<span><p>Future deep learning systems call for techniques that can deal with the
evolving nature of temporal data and scarcity of annotations when new problems
occur. As a step towards this goal, we present FUSION (Few-shot UnSupervIsed
cONtinual learning), a learning strategy that enables a neural network to learn
quickly and continually on streams of unlabelled data and unbalanced tasks. The
objective is to maximise the knowledge extracted from the unlabelled data
stream (unsupervised), favor the forward transfer of previously learnt tasks
and features (continual) and exploit as much as possible the supervised
information when available (few-shot). The core of FUSION is MEML -
Meta-Example Meta-Learning - that consolidates a meta-representation through
the use of a self-attention mechanism during a single inner loop in the
meta-optimisation stage. To further enhance the capability of MEML to
generalise from few data, we extend it by creating various augmented surrogate
tasks and by optimising over the hardest. An extensive experimental evaluation
on public computer vision benchmarks shows that FUSION outperforms existing
state-of-the-art solutions both in the few-shot and continual learning
experimental settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural 3D Video Synthesis from Multi-view Video. (arXiv:2103.02597v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02597">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach for 3D video synthesis that is able to represent
multi-view video recordings of a dynamic real-world scene in a compact, yet
expressive representation that enables high-quality view synthesis and motion
interpolation. Our approach takes the high quality and compactness of static
neural radiance fields in a new direction: to a model-free, dynamic setting. At
the core of our approach is a novel time-conditioned neural radiance field that
represents scene dynamics using a set of compact latent codes. We are able to
significantly boost the training speed and perceptual quality of the generated
imagery by a novel hierarchical training scheme in combination with ray
importance sampling. Our learned representation is highly compact and able to
represent a 10 second 30 FPS multiview video recording by 18 cameras with a
model size of only 28MB. We demonstrate that our method can render
high-fidelity wide-angle novel views at over 1K resolution, even for complex
and dynamic scenes. We perform an extensive qualitative and quantitative
evaluation that shows that our approach outperforms the state of the art.
Project website: https://neural-3d-video.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FetalNet: Multi-task Deep Learning Framework for Fetal Ultrasound Biometric Measurements. (arXiv:2107.06943v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06943">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an end-to-end multi-task neural network called
FetalNet with an attention mechanism and stacked module for spatio-temporal
fetal ultrasound scan video analysis. Fetal biometric measurement is a standard
examination during pregnancy used for the fetus growth monitoring and
estimation of gestational age and fetal weight. The main goal in fetal
ultrasound scan video analysis is to find proper standard planes to measure the
fetal head, abdomen and femur. Due to natural high speckle noise and shadows in
ultrasound data, medical expertise and sonographic experience are required to
find the appropriate acquisition plane and perform accurate measurements of the
fetus. In addition, existing computer-aided methods for fetal US biometric
measurement address only one single image frame without considering temporal
features. To address these shortcomings, we propose an end-to-end multi-task
neural network for spatio-temporal ultrasound scan video analysis to
simultaneously localize, classify and measure the fetal body parts. We propose
a new encoder-decoder segmentation architecture that incorporates a
classification branch. Additionally, we employ an attention mechanism with a
stacked module to learn salient maps to suppress irrelevant US regions and
efficient scan plane localization. We trained on the fetal ultrasound video
comes from routine examinations of 700 different patients. Our method called
FetalNet outperforms existing state-of-the-art methods in both classification
and segmentation in fetal ultrasound video recordings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04392">
<div class="article-summary-box-inner">
<span><p>Deep learning has the potential to automate many clinically useful tasks in
medical imaging. However translation of deep learning into clinical practice
has been hindered by issues such as lack of the transparency and
interpretability in these "black box" algorithms compared to traditional
statistical methods. Specifically, many clinical deep learning models lack
rigorous and robust techniques for conveying certainty (or lack thereof) in
their predictions -- ultimately limiting their appeal for extensive use in
medical decision-making. Furthermore, numerous demonstrations of algorithmic
bias have increased hesitancy towards deployment of deep learning for clinical
applications. To this end, we explore how conformal predictions can complement
existing deep learning approaches by providing an intuitive way of expressing
uncertainty while facilitating greater transparency to clinical users. In this
paper, we conduct field interviews with radiologists to assess possible
use-cases for conformal predictors. Using insights gathered from these
interviews, we devise two clinical use-cases and empirically evaluate several
methods of conformal predictions on a dermatology photography dataset for skin
lesion classification. We show how to modify conformal predictions to be more
adaptive to subgroup differences in patient skin tones through equalized
coverage. Finally, we compare conformal prediction against measures of
epistemic uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An optimised deep spiking neural network architecture without gradients. (arXiv:2109.12813v3 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12813">
<div class="article-summary-box-inner">
<span><p>We present an end-to-end trainable modular event-driven neural architecture
that uses local synaptic and threshold adaptation rules to perform
transformations between arbitrary spatio-temporal spike patterns. The
architecture represents a highly abstracted model of existing Spiking Neural
Network (SNN) architectures. The proposed Optimized Deep Event-driven Spiking
neural network Architecture (ODESA) can simultaneously learn hierarchical
spatio-temporal features at multiple arbitrary time scales. ODESA performs
online learning without the use of error back-propagation or the calculation of
gradients. Through the use of simple local adaptive selection thresholds at
each node, the network rapidly learns to appropriately allocate its neuronal
resources at each layer for any given problem without using a real-valued error
measure. These adaptive selection thresholds are the central feature of ODESA,
ensuring network stability and remarkable robustness to noise as well as to the
selection of initial system parameters. Network activations are inherently
sparse due to a hard Winner-Take-All (WTA) constraint at each layer. We
evaluate the architecture on existing spatio-temporal datasets, including the
spike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based
on International Morse Code that we created. These tests demonstrate the
hierarchical spatio-temporal learning capabilities of ODESA. Through these
tests, we demonstrate ODESA can optimally solve practical and highly
challenging hierarchical spatio-temporal learning tasks with the minimum
possible number of computing nodes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADOP: Approximate Differentiable One-Pixel Point Rendering. (arXiv:2110.06635v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06635">
<div class="article-summary-box-inner">
<span><p>In this paper we present ADOP, a novel point-based, differentiable neural
rendering pipeline. Like other neural renderers, our system takes as input
calibrated camera images and a proxy geometry of the scene, in our case a point
cloud. To generate a novel view, the point cloud is rasterized with learned
feature vectors as colors and a deep neural network fills the remaining holes
and shades each output pixel. The rasterizer renders points as one-pixel
splats, which makes it very fast and allows us to compute gradients with
respect to all relevant input parameters efficiently. Furthermore, our pipeline
contains a fully differentiable physically-based photometric camera model,
including exposure, white balance, and a camera response function. Following
the idea of inverse rendering, we use our renderer to refine its input in order
to reduce inconsistencies and optimize the quality of its output. In
particular, we can optimize structural parameters like the camera pose, lens
distortions, point positions and features, and a neural environment map, but
also photometric parameters like camera response function, vignetting, and
per-image exposure and white balance. Because our pipeline includes photometric
parameters, e.g.~exposure and camera response function, our system can smoothly
handle input images with varying exposure and white balance, and generates
high-dynamic range output. We show that due to the improved input, we can
achieve high render quality, also for difficult input, e.g. with imperfect
camera calibrations, inaccurate proxy geometry, or varying exposure. As a
result, a simpler and thus faster deep neural network is sufficient for
reconstruction. In combination with the fast point rasterization, ADOP achieves
real-time rendering rates even for models with well over 100M points.
https://github.com/darglein/ADOP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CeyMo: See More on Roads -- A Novel Benchmark Dataset for Road Marking Detection. (arXiv:2110.11867v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11867">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel road marking benchmark dataset for road
marking detection, addressing the limitations in the existing publicly
available datasets such as lack of challenging scenarios, prominence given to
lane markings, unavailability of an evaluation script, lack of annotation
formats and lower resolutions. Our dataset consists of 2887 total images with
4706 road marking instances belonging to 11 classes. The images have a high
resolution of 1920 x 1080 and capture a wide range of traffic, lighting and
weather conditions. We provide road marking annotations in polygons, bounding
boxes and pixel-level segmentation masks to facilitate a diverse range of road
marking detection algorithms. The evaluation metrics and the evaluation script
we provide, will further promote direct comparison of novel approaches for road
marking detection with existing methods. Furthermore, we evaluate the
effectiveness of using both instance segmentation and object detection based
approaches for the road marking detection task. Speed and accuracy scores for
two instance segmentation models and two object detector models are provided as
a performance baseline for our benchmark dataset. The dataset and the
evaluation script is publicly available at https://github.com/oshadajay/CeyMo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking. (arXiv:2111.11892v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11892">
<div class="article-summary-box-inner">
<span><p>Multi-Camera Multi-Object Tracking is currently drawing attention in the
computer vision field due to its superior performance in real-world
applications such as video surveillance in crowded scenes or in wide spaces. In
this work, we propose a mathematically elegant multi-camera multiple object
tracking approach based on a spatial-temporal lifted multicut formulation. Our
model utilizes state-of-the-art tracklets produced by single-camera trackers as
proposals. As these tracklets may contain ID-Switch errors, we refine them
through a novel pre-clustering obtained from 3D geometry projections. As a
result, we derive a better tracking graph without ID switches and more precise
affinity costs for the data association phase. Tracklets are then matched to
multi-camera trajectories by solving a global lifted multicut formulation that
incorporates short and long-range temporal interactions on tracklets located in
the same camera as well as inter-camera ones. Experimental results on the
WildTrack dataset yield near-perfect performance, outperforming
state-of-the-art trackers on Campus while being on par on the PETS-09 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-VField: Adversarial Augmentation of Point Clouds for Domain Generalization in 3D Object Detection. (arXiv:2112.04764v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04764">
<div class="article-summary-box-inner">
<span><p>As 3D object detection on point clouds relies on the geometrical
relationships between the points, non-standard object shapes can hinder a
method's detection capability. However, in safety-critical settings, robustness
to out-of-domain and long-tail samples is fundamental to circumvent dangerous
issues, such as the misdetection of damaged or rare cars. In this work, we
substantially improve the generalization of 3D object detectors to
out-of-domain data by deforming point clouds during training. We achieve this
with 3D-VField: a novel data augmentation method that plausibly deforms objects
via vector fields learned in an adversarial fashion. Our approach constrains 3D
points to slide along their sensor view rays while neither adding nor removing
any of them. The obtained vectors are transferable, sample-independent and
preserve shape and occlusions. Despite training only on a standard dataset,
such as KITTI, augmenting with our vector fields significantly improves the
generalization to differently shaped objects and scenes. Towards this end, we
propose and share CrashD: a synthetic dataset of realistic damaged and rare
cars, with a variety of crash scenarios. Extensive experiments on KITTI, Waymo,
our CrashD and SUN RGB-D show the generalizability of our techniques to
out-of-domain data, different models and sensors, namely LiDAR and ToF cameras,
for both indoor and outdoor scenes. Our CrashD dataset is available at
https://crashd-cars.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation. (arXiv:2112.08594v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08594">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-context media, such as "mis-captioned" images on Twitter, is
a relevant problem, especially in domains of high public significance. In this
work we aim to develop defenses against such misinformation for the topics of
Climate Change, COVID-19, and Military Vehicles. We first present a large-scale
multimodal dataset with over 884k tweets relevant to these topics. Next, we
propose a detection method, based on the state-of-the-art CLIP model, that
leverages automatically generated hard image-text mismatches. While this
approach works well on our automatically constructed out-of-context tweets, we
aim to validate its usefulness on data representative of the real world. Thus,
we test it on a set of human-generated fakes created by mimicking in-the-wild
misinformation. We achieve an 11% detection improvement in a high precision
regime over a strong baseline. Finally, we share insights about our best model
design and analyze the challenges of this emerging threat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer. (arXiv:2112.08995v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08995">
<div class="article-summary-box-inner">
<span><p>Machines that can represent and describe environmental soundscapes have
practical potential, e.g., for audio tagging and captioning systems. Prevailing
learning paradigms have been relying on parallel audio-text data, which is,
however, scarcely available on the web. We propose VIP-ANT that induces
\textbf{A}udio-\textbf{T}ext alignment without using any parallel audio-text
data. Our key idea is to share the image modality between bi-modal image-text
representations and bi-modal image-audio representations; the image modality
functions as a pivot and connects audio and text in a tri-modal embedding space
implicitly.
</p>
<p>In a difficult zero-shot setting with no paired audio-text data, our model
demonstrates state-of-the-art zero-shot performance on the ESC50 and US8K audio
classification tasks, and even surpasses the supervised state of the art for
Clotho caption retrieval (with audio queries) by 2.2\% R@1. We further
investigate cases of minimal audio-text supervision, finding that, e.g., just a
few hundred supervised audio-text pairs increase the zero-shot audio
classification accuracy by 8\% on US8K. However, to match human parity on some
zero-shot tasks, our empirical scaling experiments suggest that we would need
about $2^{21} \approx 2M$ supervised audio-caption pairs. Our work opens up new
avenues for learning audio-text connections with little to no parallel
audio-text data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifoldron: Direct Space Partition via Manifold Discovery. (arXiv:2201.05279v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05279">
<div class="article-summary-box-inner">
<span><p>A neural network with the widely-used ReLU activation has been shown to
partition the sample space into many convex polytopes for prediction. However,
the parameterized way a neural network and other machine learning models use to
partition the space has imperfections, \textit{e}.\textit{g}., the compromised
interpretability for complex models, the inflexibility in decision boundary
construction due to the generic character of the model, and the risk of being
trapped into shortcut solutions. In contrast, although the non-parameterized
models can adorably avoid or downplay these issues, they are usually
insufficiently powerful either due to over-simplification or the failure to
accommodate the manifold structures of data. In this context, we first propose
a new type of machine learning models referred to as Manifoldron that directly
derives decision boundaries from data and partitions the space via manifold
structure discovery. Then, we systematically analyze the key characteristics of
the Manifoldron such as manifold characterization capability and its link to
neural networks. The experimental results on 4 synthetic examples, 20 public
benchmark datasets, and 1 real-world application demonstrate that the proposed
Manifoldron performs competitively compared to the mainstream machine learning
models. We have shared our code in \url{https://github.com/wdayang/Manifoldron}
for free download and evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decision boundaries and convex hulls in the feature space that deep learning functions learn from images. (arXiv:2202.04052v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04052">
<div class="article-summary-box-inner">
<span><p>The success of deep neural networks in image classification and learning can
be partly attributed to the features they extract from images. It is often
speculated about the properties of a low-dimensional manifold that models
extract and learn from images. However, there is not sufficient understanding
about this low-dimensional space based on theory or empirical evidence. For
image classification models, their last hidden layer is the one where images of
each class is separated from other classes and it also has the least number of
features. Here, we develop methods and formulations to study that feature space
for any model. We study the partitioning of the domain in feature space,
identify regions guaranteed to have certain classifications, and investigate
its implications for the pixel space. We observe that geometric arrangements of
decision boundaries in feature space is significantly different compared to
pixel space, providing insights about adversarial vulnerabilities, image
morphing, extrapolation, ambiguity in classification, and the mathematical
understanding of image classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artemis: Articulated Neural Pets with Appearance and Motion synthesis. (arXiv:2202.05628v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05628">
<div class="article-summary-box-inner">
<span><p>We, humans, are entering into a virtual era and indeed want to bring animals
to the virtual world as well for companion. Yet, computer-generated (CGI) furry
animals are limited by tedious off-line rendering, let alone interactive motion
control. In this paper, we present ARTEMIS, a novel neural modeling and
rendering pipeline for generating ARTiculated neural pets with appEarance and
Motion synthesIS. Our ARTEMIS enables interactive motion control, real-time
animation, and photo-realistic rendering of furry animals. The core of our
ARTEMIS is a neural-generated (NGI) animal engine, which adopts an efficient
octree-based representation for animal animation and fur rendering. The
animation then becomes equivalent to voxel-level deformation based on explicit
skeletal warping. We further use a fast octree indexing and efficient
volumetric rendering scheme to generate appearance and density features maps.
Finally, we propose a novel shading network to generate high-fidelity details
of appearance and opacity under novel poses from appearance and density feature
maps. For the motion control module in ARTEMIS, we combine state-of-the-art
animal motion capture approach with recent neural character control scheme. We
introduce an effective optimization scheme to reconstruct the skeletal motion
of real animals captured by a multi-view RGB and Vicon camera array. We feed
all the captured motion into a neural character control scheme to generate
abstract control signals with motion styles. We further integrate ARTEMIS into
existing engines that support VR headsets, providing an unprecedented immersive
experience where a user can intimately interact with a variety of virtual
animals with vivid movements and photo-realistic appearance. We make available
our ARTEMIS model and dynamic furry animal dataset at
https://haiminluo.github.io/publication/artemis/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual attention analysis of pathologists examining whole slide images of Prostate cancer. (arXiv:2202.08437v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08437">
<div class="article-summary-box-inner">
<span><p>We study the attention of pathologists as they examine whole-slide images
(WSIs) of prostate cancer tissue using a digital microscope. To the best of our
knowledge, our study is the first to report in detail how pathologists navigate
WSIs of prostate cancer as they accumulate information for their diagnoses. We
collected slide navigation data (i.e., viewport location, magnification level,
and time) from 13 pathologists in 2 groups (5 genitourinary (GU) specialists
and 8 general pathologists) and generated visual attention heatmaps and
scanpaths. Each pathologist examined five WSIs from the TCGA PRAD dataset,
which were selected by a GU pathology specialist. We examined and analyzed the
distributions of visual attention for each group of pathologists after each WSI
was examined. To quantify the relationship between a pathologist's attention
and evidence for cancer in the WSI, we obtained tumor annotations from a
genitourinary specialist. We used these annotations to compute the overlap
between the distribution of visual attention and annotated tumor region to
identify strong correlations. Motivated by this analysis, we trained a deep
learning model to predict visual attention on unseen WSIs. We find that the
attention heatmaps predicted by our model correlate quite well with the ground
truth attention heatmap and tumor annotations on a test set of 17 WSIs by using
various spatial and temporal evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active learning with binary models for real time data labelling. (arXiv:2203.00439v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00439">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) and Deep Learning (DL) tasks primarily depend on data.
Most of the ML and DL applications involve supervised learning which requires
labelled data. In the initial phases of ML realm lack of data used to be a
problem, now we are in a new era of big data. The supervised ML algorithms
require data to be labelled and of good quality. Labelling task requires a
large amount of money and time investment. Data labelling require a skilled
person who will charge high for this task, consider the case of the medical
field or the data is in bulk that requires a lot of people assigned to label
it. The amount of data that is well enough for training needs to be known,
money and time can not be wasted to label the whole data. This paper mainly
aims to propose a strategy that helps in labelling the data along with oracle
in real-time. With balancing on model contribution for labelling is 89 and 81.1
for furniture type and intel scene image data sets respectively. Further with
balancing being kept off model contribution is found to be 83.47 and 78.71 for
furniture type and flower data sets respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACID: Action-Conditional Implicit Visual Dynamics for Deformable Object Manipulation. (arXiv:2203.06856v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06856">
<div class="article-summary-box-inner">
<span><p>Manipulating volumetric deformable objects in the real world, like plush toys
and pizza dough, bring substantial challenges due to infinite shape variations,
non-rigid motions, and partial observability. We introduce ACID, an
action-conditional visual dynamics model for volumetric deformable objects
based on structured implicit neural representations. ACID integrates two new
techniques: implicit representations for action-conditional dynamics and
geodesics-based contrastive learning. To represent deformable dynamics from
partial RGB-D observations, we learn implicit representations of occupancy and
flow-based forward dynamics. To accurately identify state change under large
non-rigid deformations, we learn a correspondence embedding field through a
novel geodesics-based contrastive loss. To evaluate our approach, we develop a
simulation framework for manipulating complex deformable shapes in realistic
scenes and a benchmark containing over 17,000 action trajectories with six
types of plush toys and 78 variants. Our model achieves the best performance in
geometry, correspondence, and dynamics predictions over existing approaches.
The ACID dynamics models are successfully employed to goal-conditioned
deformable manipulation tasks, resulting in a 30% increase in task success rate
over the strongest baseline. For more results and information, please visit
https://b0ku1.github.io/acid/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-set Recognition via Augmentation-based Similarity Learning. (arXiv:2203.13238v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13238">
<div class="article-summary-box-inner">
<span><p>The primary assumption of conventional supervised learning or classification
is that the test samples are drawn from the same distribution as the training
samples, which is called closed set learning or classification. In many
practical scenarios, this is not the case because there are unknowns or unseen
class samples in the test data, which is called the open set scenario, and the
unknowns need to be detected. This problem is referred to as the open set
recognition problem and is important in safety-critical applications. We
propose to detect unknowns (or unseen class samples) through learning pairwise
similarities. The proposed method works in two steps. It first learns a closed
set classifier using the seen classes that have appeared in training and then
learns how to compare seen classes with pseudo-unseen (automatically generated
unseen class samples). The pseudo-unseen generation is carried out by
performing distribution shifting augmentations on the seen or training samples.
We call our method OPG (Open set recognition based on Pseudo unseen data
Generation). The experimental evaluation shows that the learned
similarity-based features can successfully distinguish seen from unseen in
benchmark datasets for open set recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOS! Self-supervised Learning Over Sets Of Handled Objects In Egocentric Action Recognition. (arXiv:2204.04796v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04796">
<div class="article-summary-box-inner">
<span><p>Learning an egocentric action recognition model from video data is
challenging due to distractors (e.g., irrelevant objects) in the background.
Further integrating object information into an action model is hence
beneficial. Existing methods often leverage a generic object detector to
identify and represent the objects in the scene. However, several important
issues remain. Object class annotations of good quality for the target domain
(dataset) are still required for learning good object representation. Besides,
previous methods deeply couple the existing action models and need to retrain
them jointly with object representation, leading to costly and inflexible
integration. To overcome both limitations, we introduce Self-Supervised
Learning Over Sets (SOS), an approach to pre-train a generic Objects In Contact
(OIC) representation model from video object regions detected by an
off-the-shelf hand-object contact detector. Instead of augmenting object
regions individually as in conventional self-supervised learning, we view the
action process as a means of natural data transformations with unique
spatio-temporal continuity and exploit the inherent relationships among
per-video object sets. Extensive experiments on two datasets, EPIC-KITCHENS-100
and EGTEA, show that our OIC significantly boosts the performance of multiple
state-of-the-art video classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension. (arXiv:2204.05991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05991">
<div class="article-summary-box-inner">
<span><p>Training a referring expression comprehension (ReC) model for a new visual
domain requires collecting referring expressions, and potentially corresponding
bounding boxes, for images in the domain. While large-scale pre-trained models
are useful for image classification across domains, it remains unclear if they
can be applied in a zero-shot manner to more complex tasks like ReC. We present
ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a
state-of-the-art large-scale model, for ReC. Motivated by the close connection
between ReC and CLIP's contrastive pre-training objective, the first component
of ReCLIP is a region-scoring method that isolates object proposals via
cropping and blurring, and passes them to CLIP. However, through controlled
experiments on a synthetic dataset, we find that CLIP is largely incapable of
performing spatial reasoning off-the-shelf. Thus, the second component of
ReCLIP is a spatial relation resolver that handles several types of spatial
relations. We reduce the gap between zero-shot baselines from prior work and
supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game
imagery), ReCLIP's relative improvement over supervised ReC models trained on
real images is 8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proto2Proto: Can you recognize the car, the way I do?. (arXiv:2204.11830v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11830">
<div class="article-summary-box-inner">
<span><p>Prototypical methods have recently gained a lot of attention due to their
intrinsic interpretable nature, which is obtained through the prototypes. With
growing use cases of model reuse and distillation, there is a need to also
study transfer of interpretability from one model to another. We present
Proto2Proto, a novel method to transfer interpretability of one prototypical
part network to another via knowledge distillation. Our approach aims to add
interpretability to the "dark" knowledge transferred from the teacher to the
shallower student model. We propose two novel losses: "Global Explanation" loss
and "Patch-Prototype Correspondence" loss to facilitate such a transfer. Global
Explanation loss forces the student prototypes to be close to teacher
prototypes, and Patch-Prototype Correspondence loss enforces the local
representations of the student to be similar to that of the teacher. Further,
we propose three novel metrics to evaluate the student's proximity to the
teacher as measures of interpretability transfer in our settings. We
qualitatively and quantitatively demonstrate the effectiveness of our method on
CUB-200-2011 and Stanford Cars datasets. Our experiments show that the proposed
method indeed achieves interpretability transfer from teacher to student while
simultaneously exhibiting competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Power Bundle Adjustment for Large-Scale 3D Reconstruction. (arXiv:2204.12834v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12834">
<div class="article-summary-box-inner">
<span><p>We present the design and the implementation of a new expansion type
algorithm to solve large-scale bundle adjustment problems. Our approach --
called Power Bundle Adjustment -- is based on the power series expansion of the
inverse Schur complement. This initiates a new family of solvers that we call
inverse expansion methods. We show with the real-world BAL dataset that the
proposed solver challenges the traditional direct and iterative methods. The
solution of the normal equation is significantly accelerated, even for reaching
a very high accuracy. Last but not least, our solver can also complement a
recently presented distributed bundle adjustment framework. We demonstrate that
employing the proposed Power Bundle Adjustment as a sub-problem solver greatly
improves speed and accuracy of the distributed optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRIT: General Robust Image Task Benchmark. (arXiv:2204.13653v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13653">
<div class="article-summary-box-inner">
<span><p>Computer vision models excel at making predictions when the test distribution
closely resembles the training distribution. Such models have yet to match the
ability of biological vision to learn from multiple sources and generalize to
new data sources and tasks. To facilitate the development and evaluation of
more general vision systems, we introduce the General Robust Image Task (GRIT)
benchmark. GRIT evaluates the performance, robustness, and calibration of a
vision system across a variety of image prediction tasks, concepts, and data
sources. The seven tasks in GRIT are selected to cover a range of visual
skills: object categorization, object localization, referring expression
grounding, visual question answering, segmentation, human keypoint detection,
and surface normal estimation. GRIT is carefully designed to enable the
evaluation of robustness under image perturbations, image source distribution
shift, and concept distribution shift. By providing a unified platform for
thorough assessment of skills and concepts learned by a vision model, we hope
GRIT catalyzes the development of performant and robust general purpose vision
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A very preliminary analysis of DALL-E 2. (arXiv:2204.13807v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13807">
<div class="article-summary-box-inner">
<span><p>The DALL-E 2 system generates original synthetic images corresponding to an
input text as caption. We report here on the outcome of fourteen tests of this
system designed to assess its common sense, reasoning and ability to understand
complex texts. All of our prompts were intentionally much more challenging than
the typical ones that have been showcased in recent weeks. Nevertheless, for 5
out of the 14 prompts, at least one of the ten images fully satisfied our
requests. On the other hand, on no prompt did all of the ten images satisfy our
requests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source Domain Subset Sampling for Semi-Supervised Domain Adaptation in Semantic Segmentation. (arXiv:2205.00312v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00312">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce source domain subset sampling (SDSS) as a new
perspective of semi-supervised domain adaptation. We propose domain adaptation
by sampling and exploiting only a meaningful subset from source data for
training. Our key assumption is that the entire source domain data may contain
samples that are unhelpful for the adaptation. Therefore, the domain adaptation
can benefit from a subset of source data composed solely of helpful and
relevant samples. The proposed method effectively subsamples full source data
to generate a small-scale meaningful subset. Therefore, training time is
reduced, and performance is improved with our subsampled source data. To
further verify the scalability of our method, we construct a new dataset called
Ocean Ship, which comprises 500 real and 200K synthetic sample images with
ground-truth labels. The SDSS achieved a state-of-the-art performance when
applied on GTA5 to Cityscapes and SYNTHIA to Cityscapes public benchmark
datasets and a 9.13 mIoU improvement on our Ocean Ship dataset over a baseline
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog. (arXiv:2205.00423v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00423">
<div class="article-summary-box-inner">
<span><p>Visual Dialog aims to answer multi-round, interactive questions based on the
dialog history and image content. Existing methods either consider answer
ranking and generating individually or only weakly capture the relation across
the two tasks implicitly by two separate models. The research on a universal
framework that jointly learns to rank and generate answers in a single model is
seldom explored. In this paper, we propose a contrastive learning-based
framework UTC to unify and facilitate both discriminative and generative tasks
in visual dialog with a single model. Specifically, considering the inherent
limitation of the previous learning paradigm, we devise two inter-task
contrastive losses i.e., context contrastive loss and answer contrastive loss
to make the discriminative and generative tasks mutually reinforce each other.
These two complementary contrastive losses exploit dialog context and target
answer as anchor points to provide representation learning signals from
different perspectives. We evaluate our proposed UTC on the VisDial v1.0
dataset, where our method outperforms the state-of-the-art on both
discriminative and generative tasks and surpasses previous state-of-the-art
generative methods by more than 2 absolute points on Recall@1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00701">
<div class="article-summary-box-inner">
<span><p>Gravitational lensing is the relativistic effect generated by massive bodies,
which bend the space-time surrounding them. It is a deeply investigated topic
in astrophysics and allows validating theoretical relativistic results and
studying faint astrophysical objects that would not be visible otherwise. In
recent years Machine Learning methods have been applied to support the analysis
of the gravitational lensing phenomena by detecting lensing effects in data
sets consisting of images associated with brightness variation time series.
However, the state-of-art approaches either consider only images and neglect
time-series data or achieve relatively low accuracy on the most difficult data
sets. This paper introduces DeepGraviLens, a novel multi-modal network that
classifies spatio-temporal data belonging to one non-lensed system type and
three lensed system types. It surpasses the current state of the art accuracy
results by $\approx$ 19% to $\approx$ 43%, depending on the considered data
set. Such an improvement will enable the acceleration of the analysis of lensed
objects in upcoming astrophysical surveys, which will exploit the petabytes of
data collected, e.g., from the Vera C. Rubin Observatory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCLF: A Contrastive-Curiosity-Driven Learning Framework for Sample-Efficient Reinforcement Learning. (arXiv:2205.00943v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00943">
<div class="article-summary-box-inner">
<span><p>In reinforcement learning (RL), it is challenging to learn directly from
high-dimensional observations, where data augmentation has recently been shown
to remedy this via encoding invariances from raw pixels. Nevertheless, we
empirically find that not all samples are equally important and hence simply
injecting more augmented inputs may instead cause instability in Q-learning. In
this paper, we approach this problem systematically by developing a
model-agnostic Contrastive-Curiosity-Driven Learning Framework (CCLF), which
can fully exploit sample importance and improve learning efficiency in a
self-supervised manner. Facilitated by the proposed contrastive curiosity, CCLF
is capable of prioritizing the experience replay, selecting the most
informative augmented inputs, and more importantly regularizing the Q-function
as well as the encoder to concentrate more on under-learned data. Moreover, it
encourages the agent to explore with a curiosity-based reward. As a result, the
agent can focus on more informative samples and learn representation
invariances more efficiently, with significantly reduced augmented inputs. We
apply CCLF to several base RL algorithms and evaluate on the DeepMind Control
Suite, Atari, and MiniGrid benchmarks, where our approach demonstrates superior
sample efficiency and learning performances compared with other
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-stage deep architecture for summary generation of soccer videos. (arXiv:2205.00694v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00694">
<div class="article-summary-box-inner">
<span><p>Video content is present in an ever-increasing number of fields, both
scientific and commercial. Sports, particularly soccer, is one of the
industries that has invested the most in the field of video analytics, due to
the massive popularity of the game and the emergence of new markets. Previous
state-of-the-art methods on soccer matches video summarization rely on
handcrafted heuristics to generate summaries which are poorly generalizable,
but these works have yet proven that multiple modalities help detect the best
actions of the game. On the other hand, machine learning models with higher
generalization potential have entered the field of summarization of
general-purpose videos, offering several deep learning approaches. However,
most of them exploit content specificities that are not appropriate for sport
whole-match videos. Although video content has been for many years the main
source for automatizing knowledge extraction in soccer, the data that records
all the events happening on the field has become lately very important in
sports analytics, since this event data provides richer context information and
requires less processing. We propose a method to generate the summary of a
soccer match exploiting both the audio and the event metadata. The results show
that our method can detect the actions of the match, identify which of these
actions should belong to the summary and then propose multiple candidate
summaries which are similar enough but with relevant variability to provide
different options to the final editor. Furthermore, we show the generalization
capability of our work since it can transfer knowledge between datasets from
different broadcasting companies, different competitions, acquired in different
conditions, and corresponding to summaries of different lengths
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-04 23:08:39.447443405 UTC">2022-05-04 23:08:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>